{
    "syntactic": [
        "gpu",
        "cpu",
        "inference",
        "utility functions",
        "gpus",
        "weight information",
        "convolutional neural networks",
        "detection rates"
    ],
    "semantic": [
        "gpu",
        "detection algorithm",
        "random access memory",
        "cuda",
        "cpu",
        "inference",
        "utility functions",
        "gpus",
        "weight information",
        "probabilistic inference"
    ],
    "union": [
        "gpu",
        "detection algorithm",
        "weight information",
        "random access memory",
        "cuda",
        "inference",
        "utility functions",
        "detection rates",
        "gpus",
        "cpu",
        "convolutional neural networks",
        "probabilistic inference"
    ],
    "enhanced": [
        "program processors",
        "signal detection",
        "attribute weight",
        "random access storage",
        "parallel programming",
        "graphics processing unit",
        "inference engines",
        "game theory",
        "intrusion detection",
        "neural networks",
        "bayesian methods",
        "probability distributions",
        "parallel processing systems",
        "microprocessor chips",
        "signal processing",
        "multiple attribute decision making",
        "parallel algorithms",
        "parallel architectures",
        "computer programming languages",
        "artificial intelligence",
        "computer crime",
        "machine learning",
        "probability",
        "distributed systems",
        "computer hardware",
        "engineering",
        "illustrative examples",
        "computer programming",
        "computer science",
        "security of data",
        "mathematics",
        "distributed computer systems",
        "linear matrix inequalities",
        "computer security",
        "computer systems",
        "matrix algebra"
    ],
    "explanation": {
        "detection rates": [
            "detectron accuracy"
        ],
        "utility functions": [
            "utility function"
        ],
        "weight information": [
            "weight information"
        ],
        "convolutional neural networks": [
            "cnn"
        ],
        "gpu": [
            "gpu",
            "gpus",
            "cpu",
            "gpu cpu",
            "nvidia"
        ],
        "inference": [
            "inference notebook",
            "maskrcnn_benchmark inference",
            "batch inference",
            "inference",
            "inference line",
            "inference perform inference",
            "inference mix",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "write inference",
            "perform inference",
            "inference pipeline",
            "gpu inference",
            "inference time"
        ],
        "cpu": [
            "cpu"
        ],
        "gpus": [
            "gpu",
            "gpus"
        ],
        "probabilistic inference": [
            "inference notebook",
            "maskrcnn_benchmark inference",
            "batch inference",
            "inference",
            "inference line",
            "inference perform inference",
            "inference mix",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "write inference",
            "perform inference",
            "inference pipeline",
            "gpu inference",
            "inference time"
        ],
        "random access memory": [
            "experience memory",
            "memory nvidia",
            "detail memory",
            "gpu memory",
            "memory",
            "memory reason",
            "memory mmdetection",
            "memory error",
            "lot memory",
            "much memory"
        ],
        "cuda": [
            "gpu",
            "gpus",
            "nvidia"
        ],
        "detection algorithm": [
            "shoot detection",
            "detection",
            "lesion detection",
            "detection tag",
            "detection cheng",
            "detection segmentation"
        ],
        "program processors": [
            "gpu",
            "gpus",
            "cpu",
            "gpu cpu",
            "nvidia"
        ],
        "signal detection": [
            "shoot detection",
            "detection",
            "lesion detection",
            "detection tag",
            "detection cheng",
            "detection segmentation"
        ],
        "attribute weight": [
            "weight information"
        ],
        "random access storage": [
            "experience memory",
            "memory nvidia",
            "detail memory",
            "gpu memory",
            "memory",
            "memory reason",
            "memory mmdetection",
            "memory error",
            "lot memory",
            "much memory"
        ],
        "parallel programming": [
            "gpu",
            "gpus",
            "nvidia"
        ],
        "graphics processing unit": [
            "gpu",
            "gpus",
            "nvidia"
        ],
        "inference engines": [
            "inference notebook",
            "maskrcnn_benchmark inference",
            "batch inference",
            "inference",
            "inference line",
            "inference perform inference",
            "inference mix",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "write inference",
            "perform inference",
            "inference pipeline",
            "gpu inference",
            "inference time"
        ],
        "game theory": [
            "utility function"
        ],
        "intrusion detection": [
            "detectron accuracy"
        ],
        "neural networks": [
            "cnn"
        ],
        "bayesian methods": [
            "inference notebook",
            "maskrcnn_benchmark inference",
            "batch inference",
            "inference",
            "inference line",
            "inference perform inference",
            "inference mix",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "write inference",
            "perform inference",
            "inference pipeline",
            "gpu inference",
            "inference time"
        ],
        "probability distributions": [
            "inference notebook",
            "maskrcnn_benchmark inference",
            "batch inference",
            "inference",
            "inference line",
            "inference perform inference",
            "inference mix",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "write inference",
            "perform inference",
            "inference pipeline",
            "gpu inference",
            "inference time"
        ],
        "parallel processing systems": [
            "memory nvidia",
            "nvidia",
            "memory",
            "gpu cpu",
            "much memory",
            "gpu",
            "experience memory",
            "detail memory",
            "gpu memory",
            "memory reason",
            "memory mmdetection",
            "memory error",
            "gpus",
            "lot memory",
            "cpu"
        ],
        "microprocessor chips": [
            "memory nvidia",
            "nvidia",
            "memory",
            "gpu cpu",
            "much memory",
            "gpu",
            "experience memory",
            "detail memory",
            "gpu memory",
            "memory reason",
            "memory mmdetection",
            "memory error",
            "gpus",
            "lot memory",
            "cpu"
        ],
        "signal processing": [
            "detection",
            "detection tag",
            "detection cheng",
            "shoot detection",
            "lesion detection",
            "detection segmentation"
        ],
        "multiple attribute decision making": [
            "weight information"
        ],
        "parallel algorithms": [
            "gpu",
            "gpus",
            "nvidia"
        ],
        "parallel architectures": [
            "gpu",
            "gpus",
            "nvidia"
        ],
        "computer programming languages": [
            "gpu",
            "gpus",
            "nvidia"
        ],
        "artificial intelligence": [
            "inference notebook",
            "maskrcnn_benchmark inference",
            "batch inference",
            "inference",
            "inference line",
            "inference perform inference",
            "inference mix",
            "cnn",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "write inference",
            "perform inference",
            "inference pipeline",
            "gpu inference",
            "utility function",
            "inference time"
        ],
        "computer crime": [
            "detectron accuracy"
        ],
        "machine learning": [
            "cnn"
        ],
        "probability": [
            "inference notebook",
            "maskrcnn_benchmark inference",
            "batch inference",
            "inference",
            "inference line",
            "inference perform inference",
            "inference mix",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "write inference",
            "perform inference",
            "inference pipeline",
            "gpu inference",
            "inference time"
        ],
        "distributed systems": [
            "memory nvidia",
            "nvidia",
            "memory",
            "gpu cpu",
            "much memory",
            "gpu",
            "experience memory",
            "detail memory",
            "gpu memory",
            "memory reason",
            "memory mmdetection",
            "memory error",
            "gpus",
            "lot memory",
            "cpu"
        ],
        "computer hardware": [
            "memory nvidia",
            "memory",
            "nvidia",
            "gpu cpu",
            "much memory",
            "gpu",
            "experience memory",
            "detail memory",
            "gpu memory",
            "memory reason",
            "memory mmdetection",
            "memory error",
            "lot memory",
            "gpus",
            "cpu"
        ],
        "engineering": [
            "detection",
            "detection tag",
            "detection cheng",
            "shoot detection",
            "lesion detection",
            "detection segmentation"
        ],
        "illustrative examples": [
            "weight information"
        ],
        "computer programming": [
            "gpu",
            "gpus",
            "nvidia"
        ],
        "computer science": [
            "maskrcnn_benchmark inference",
            "memory nvidia",
            "memory",
            "inference",
            "inference line",
            "gpu cpu",
            "cnn",
            "gpu",
            "inference cpu",
            "support inference",
            "detail memory",
            "memory reason",
            "memory error",
            "perform inference",
            "gpus",
            "inference pipeline",
            "gpu inference",
            "inference notebook",
            "batch inference",
            "nvidia",
            "detectron accuracy",
            "inference perform inference",
            "inference mix",
            "much memory",
            "inference cpu inference",
            "cpu inference",
            "inference perform",
            "write inference",
            "experience memory",
            "gpu memory",
            "memory mmdetection",
            "lot memory",
            "cpu",
            "utility function",
            "inference time"
        ],
        "security of data": [
            "detectron accuracy"
        ],
        "mathematics": [
            "inference notebook",
            "maskrcnn_benchmark inference",
            "batch inference",
            "inference",
            "inference line",
            "inference perform inference",
            "inference mix",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "write inference",
            "inference pipeline",
            "perform inference",
            "weight information",
            "gpu inference",
            "inference time"
        ],
        "distributed computer systems": [
            "memory nvidia",
            "nvidia",
            "memory",
            "gpu cpu",
            "much memory",
            "gpu",
            "experience memory",
            "detail memory",
            "gpu memory",
            "memory reason",
            "memory mmdetection",
            "memory error",
            "gpus",
            "lot memory",
            "cpu"
        ],
        "linear matrix inequalities": [
            "weight information"
        ],
        "computer security": [
            "detectron accuracy"
        ],
        "computer systems": [
            "memory nvidia",
            "nvidia",
            "memory",
            "gpu cpu",
            "much memory",
            "gpu",
            "experience memory",
            "detail memory",
            "gpu memory",
            "memory reason",
            "memory mmdetection",
            "memory error",
            "gpus",
            "lot memory",
            "cpu"
        ],
        "matrix algebra": [
            "weight information"
        ]
    }
}