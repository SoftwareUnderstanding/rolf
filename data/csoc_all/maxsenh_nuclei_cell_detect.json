{
    "syntactic": [
        "gpu",
        "inference",
        "cpu",
        "convolutional neural networks",
        "detection rates"
    ],
    "semantic": [
        "gpu",
        "detection algorithm",
        "inference",
        "cpu",
        "probabilistic inference"
    ],
    "union": [
        "probabilistic inference",
        "gpu",
        "detection algorithm",
        "cpu",
        "convolutional neural networks",
        "detection rates",
        "inference"
    ],
    "enhanced": [
        "inference engines",
        "bayesian methods",
        "probability distributions",
        "program processors",
        "signal detection",
        "neural networks",
        "intrusion detection",
        "artificial intelligence",
        "probability",
        "parallel processing systems",
        "microprocessor chips",
        "signal processing",
        "machine learning",
        "computer crime",
        "computer science",
        "mathematics",
        "distributed systems",
        "computer hardware",
        "engineering",
        "security of data",
        "distributed computer systems",
        "computer security",
        "computer systems"
    ],
    "explanation": {
        "detection rates": [
            "detectron accuracy"
        ],
        "convolutional neural networks": [
            "cnn"
        ],
        "gpu": [
            "cpu",
            "gpu",
            "gpu cpu"
        ],
        "inference": [
            "inference prediction",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "batch inference",
            "inference",
            "inference batch",
            "inference batch inference",
            "perform inference",
            "inference perform inference",
            "gpu inference",
            "inference time"
        ],
        "cpu": [
            "cpu"
        ],
        "probabilistic inference": [
            "inference prediction",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "batch inference",
            "inference batch",
            "inference batch inference",
            "inference",
            "perform inference",
            "inference perform inference",
            "gpu inference",
            "inference time"
        ],
        "detection algorithm": [
            "detection nucleus",
            "detection",
            "cell detection",
            "detection fluorescence"
        ],
        "inference engines": [
            "batch inference",
            "inference batch",
            "inference",
            "inference perform inference",
            "inference prediction",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "inference batch inference",
            "perform inference",
            "gpu inference",
            "inference time"
        ],
        "bayesian methods": [
            "inference prediction",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "batch inference",
            "inference batch",
            "inference batch inference",
            "inference",
            "perform inference",
            "inference perform inference",
            "gpu inference",
            "inference time"
        ],
        "probability distributions": [
            "inference prediction",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "batch inference",
            "inference batch",
            "inference batch inference",
            "inference",
            "perform inference",
            "inference perform inference",
            "gpu inference",
            "inference time"
        ],
        "program processors": [
            "cpu",
            "gpu",
            "gpu cpu"
        ],
        "signal detection": [
            "detection nucleus",
            "detection",
            "cell detection",
            "detection fluorescence"
        ],
        "neural networks": [
            "cnn"
        ],
        "intrusion detection": [
            "detectron accuracy"
        ],
        "artificial intelligence": [
            "batch inference",
            "inference",
            "inference batch",
            "inference perform inference",
            "cnn",
            "inference prediction",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "inference batch inference",
            "perform inference",
            "gpu inference",
            "inference time"
        ],
        "probability": [
            "batch inference",
            "inference batch",
            "inference",
            "inference perform inference",
            "inference prediction",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "inference batch inference",
            "perform inference",
            "gpu inference",
            "inference time"
        ],
        "parallel processing systems": [
            "gpu",
            "cpu",
            "gpu cpu"
        ],
        "microprocessor chips": [
            "gpu",
            "cpu",
            "gpu cpu"
        ],
        "signal processing": [
            "detection",
            "detection nucleus",
            "cell detection",
            "detection fluorescence"
        ],
        "machine learning": [
            "cnn"
        ],
        "computer crime": [
            "detectron accuracy"
        ],
        "computer science": [
            "batch inference",
            "inference",
            "inference batch",
            "detectron accuracy",
            "inference perform inference",
            "gpu cpu",
            "cnn",
            "inference prediction",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "gpu",
            "inference batch inference",
            "perform inference",
            "cpu",
            "gpu inference",
            "inference time"
        ],
        "mathematics": [
            "batch inference",
            "inference batch",
            "inference",
            "inference perform inference",
            "inference prediction",
            "inference cpu",
            "inference cpu inference",
            "inference perform",
            "support inference",
            "cpu inference",
            "inference batch inference",
            "perform inference",
            "gpu inference",
            "inference time"
        ],
        "distributed systems": [
            "gpu",
            "cpu",
            "gpu cpu"
        ],
        "computer hardware": [
            "gpu",
            "cpu",
            "gpu cpu"
        ],
        "engineering": [
            "detection",
            "detection nucleus",
            "cell detection",
            "detection fluorescence"
        ],
        "security of data": [
            "detectron accuracy"
        ],
        "distributed computer systems": [
            "gpu",
            "cpu",
            "gpu cpu"
        ],
        "computer security": [
            "detectron accuracy"
        ],
        "computer systems": [
            "gpu",
            "cpu",
            "gpu cpu"
        ]
    }
}