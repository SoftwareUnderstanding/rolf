{
    "syntactic": [
        "gpu",
        "cpu",
        "object detection",
        "inference",
        "utility functions",
        "gpus",
        "weight information",
        "convolutional neural networks",
        "detection rates"
    ],
    "semantic": [
        "gpu",
        "cpu",
        "object detection",
        "inference",
        "utility functions",
        "gpus",
        "weight information",
        "probabilistic inference"
    ],
    "union": [
        "gpu",
        "weight information",
        "object detection",
        "inference",
        "utility functions",
        "detection rates",
        "gpus",
        "cpu",
        "convolutional neural networks",
        "probabilistic inference"
    ],
    "enhanced": [
        "program processors",
        "attribute weight",
        "object recognition",
        "inference engines",
        "game theory",
        "intrusion detection",
        "cuda",
        "neural networks",
        "bayesian methods",
        "probability distributions",
        "parallel processing systems",
        "microprocessor chips",
        "multiple attribute decision making",
        "pattern recognition",
        "image segmentation",
        "artificial intelligence",
        "computer crime",
        "parallel programming",
        "graphics processing unit",
        "machine learning",
        "probability",
        "distributed systems",
        "computer hardware",
        "illustrative examples",
        "image analysis",
        "image processing",
        "computer science",
        "security of data",
        "parallel algorithms",
        "parallel architectures",
        "computer programming languages",
        "mathematics",
        "distributed computer systems",
        "linear matrix inequalities",
        "computer imaging and vision",
        "computer security",
        "computer programming",
        "computer systems",
        "matrix algebra"
    ],
    "explanation": {
        "object detection": [
            "object detection"
        ],
        "detection rates": [
            "detectron accuracy"
        ],
        "utility functions": [
            "utility function"
        ],
        "weight information": [
            "weight information"
        ],
        "convolutional neural networks": [
            "cnn"
        ],
        "inference": [
            "inference notebook",
            "inference inference",
            "maskrcnn_benchmark inference",
            "format inference",
            "batch inference",
            "inference",
            "inference acknowledgement",
            "google inference",
            "inference line",
            "inference perform inference",
            "inference time",
            "inference mix",
            "inference cpu",
            "inference cpu inference",
            "cpu inference",
            "support inference",
            "inference perform",
            "write inference",
            "format inference inference",
            "perform inference",
            "inference pipeline",
            "inference image_bbox_extraction",
            "gpu inference",
            "inference inference acknowledgement"
        ],
        "gpu": [
            "gpu",
            "gpus",
            "cpu",
            "gpu cpu",
            "nvidia"
        ],
        "cpu": [
            "cpu"
        ],
        "gpus": [
            "gpu",
            "gpus"
        ],
        "probabilistic inference": [
            "inference notebook",
            "inference inference",
            "maskrcnn_benchmark inference",
            "format inference",
            "batch inference",
            "inference",
            "inference acknowledgement",
            "google inference",
            "inference line",
            "inference perform inference",
            "inference time",
            "inference mix",
            "inference cpu",
            "inference cpu inference",
            "cpu inference",
            "support inference",
            "inference perform",
            "write inference",
            "format inference inference",
            "perform inference",
            "inference pipeline",
            "inference image_bbox_extraction",
            "gpu inference",
            "inference inference acknowledgement"
        ],
        "program processors": [
            "gpu",
            "gpus",
            "cpu",
            "gpu cpu",
            "nvidia"
        ],
        "attribute weight": [
            "weight information"
        ],
        "object recognition": [
            "object detection"
        ],
        "inference engines": [
            "maskrcnn_benchmark inference",
            "inference",
            "inference acknowledgement",
            "google inference",
            "inference line",
            "inference cpu",
            "support inference",
            "format inference inference",
            "perform inference",
            "inference pipeline",
            "gpu inference",
            "inference inference acknowledgement",
            "inference notebook",
            "inference inference",
            "format inference",
            "batch inference",
            "inference perform inference",
            "inference mix",
            "inference cpu inference",
            "cpu inference",
            "inference perform",
            "write inference",
            "inference image_bbox_extraction",
            "inference time"
        ],
        "game theory": [
            "utility function"
        ],
        "intrusion detection": [
            "detectron accuracy"
        ],
        "cuda": [
            "gpu",
            "gpus"
        ],
        "neural networks": [
            "cnn"
        ],
        "bayesian methods": [
            "inference notebook",
            "inference inference",
            "maskrcnn_benchmark inference",
            "format inference",
            "batch inference",
            "inference",
            "inference acknowledgement",
            "google inference",
            "inference line",
            "inference perform inference",
            "inference time",
            "inference mix",
            "inference cpu",
            "inference cpu inference",
            "cpu inference",
            "support inference",
            "inference perform",
            "write inference",
            "format inference inference",
            "perform inference",
            "inference pipeline",
            "inference image_bbox_extraction",
            "gpu inference",
            "inference inference acknowledgement"
        ],
        "probability distributions": [
            "inference notebook",
            "inference inference",
            "maskrcnn_benchmark inference",
            "format inference",
            "batch inference",
            "inference",
            "inference acknowledgement",
            "google inference",
            "inference line",
            "inference perform inference",
            "inference time",
            "inference mix",
            "inference cpu",
            "inference cpu inference",
            "cpu inference",
            "support inference",
            "inference perform",
            "write inference",
            "format inference inference",
            "perform inference",
            "inference pipeline",
            "inference image_bbox_extraction",
            "gpu inference",
            "inference inference acknowledgement"
        ],
        "parallel processing systems": [
            "gpu",
            "nvidia",
            "gpus",
            "cpu",
            "gpu cpu"
        ],
        "microprocessor chips": [
            "gpu",
            "nvidia",
            "gpus",
            "cpu",
            "gpu cpu"
        ],
        "multiple attribute decision making": [
            "weight information"
        ],
        "pattern recognition": [
            "object detection"
        ],
        "image segmentation": [
            "object detection"
        ],
        "artificial intelligence": [
            "maskrcnn_benchmark inference",
            "inference",
            "inference acknowledgement",
            "google inference",
            "inference line",
            "cnn",
            "inference cpu",
            "support inference",
            "object detection",
            "format inference inference",
            "perform inference",
            "inference pipeline",
            "gpu inference",
            "inference inference acknowledgement",
            "inference notebook",
            "inference inference",
            "format inference",
            "batch inference",
            "inference perform inference",
            "inference mix",
            "inference cpu inference",
            "cpu inference",
            "inference perform",
            "write inference",
            "inference image_bbox_extraction",
            "utility function",
            "inference time"
        ],
        "computer crime": [
            "detectron accuracy"
        ],
        "parallel programming": [
            "gpu",
            "gpus"
        ],
        "graphics processing unit": [
            "gpu",
            "gpus"
        ],
        "machine learning": [
            "object detection",
            "cnn"
        ],
        "probability": [
            "maskrcnn_benchmark inference",
            "inference",
            "inference acknowledgement",
            "google inference",
            "inference line",
            "inference cpu",
            "support inference",
            "format inference inference",
            "perform inference",
            "inference pipeline",
            "gpu inference",
            "inference inference acknowledgement",
            "inference notebook",
            "inference inference",
            "format inference",
            "batch inference",
            "inference perform inference",
            "inference mix",
            "inference cpu inference",
            "cpu inference",
            "inference perform",
            "write inference",
            "inference image_bbox_extraction",
            "inference time"
        ],
        "distributed systems": [
            "gpu",
            "nvidia",
            "gpus",
            "cpu",
            "gpu cpu"
        ],
        "computer hardware": [
            "gpu",
            "nvidia",
            "gpus",
            "cpu",
            "gpu cpu"
        ],
        "illustrative examples": [
            "weight information"
        ],
        "image analysis": [
            "object detection"
        ],
        "image processing": [
            "object detection"
        ],
        "computer science": [
            "maskrcnn_benchmark inference",
            "inference",
            "inference acknowledgement",
            "google inference",
            "inference line",
            "gpu cpu",
            "cnn",
            "gpu",
            "inference cpu",
            "support inference",
            "object detection",
            "format inference inference",
            "perform inference",
            "gpus",
            "inference pipeline",
            "gpu inference",
            "inference inference acknowledgement",
            "inference notebook",
            "inference inference",
            "format inference",
            "batch inference",
            "nvidia",
            "detectron accuracy",
            "inference perform inference",
            "inference mix",
            "inference cpu inference",
            "cpu inference",
            "inference perform",
            "write inference",
            "cpu",
            "inference image_bbox_extraction",
            "utility function",
            "inference time"
        ],
        "security of data": [
            "detectron accuracy"
        ],
        "parallel algorithms": [
            "gpu",
            "gpus"
        ],
        "parallel architectures": [
            "gpu",
            "gpus"
        ],
        "computer programming languages": [
            "gpu",
            "gpus"
        ],
        "mathematics": [
            "maskrcnn_benchmark inference",
            "inference",
            "inference acknowledgement",
            "google inference",
            "inference line",
            "inference cpu",
            "support inference",
            "format inference inference",
            "perform inference",
            "inference pipeline",
            "gpu inference",
            "inference inference acknowledgement",
            "inference notebook",
            "inference inference",
            "format inference",
            "batch inference",
            "inference perform inference",
            "inference mix",
            "inference cpu inference",
            "cpu inference",
            "inference perform",
            "write inference",
            "weight information",
            "inference image_bbox_extraction",
            "inference time"
        ],
        "distributed computer systems": [
            "gpu",
            "nvidia",
            "gpus",
            "cpu",
            "gpu cpu"
        ],
        "linear matrix inequalities": [
            "weight information"
        ],
        "computer imaging and vision": [
            "object detection"
        ],
        "computer security": [
            "detectron accuracy"
        ],
        "computer programming": [
            "gpu",
            "gpus"
        ],
        "computer systems": [
            "gpu",
            "nvidia",
            "gpus",
            "cpu",
            "gpu cpu"
        ],
        "matrix algebra": [
            "weight information"
        ]
    }
}