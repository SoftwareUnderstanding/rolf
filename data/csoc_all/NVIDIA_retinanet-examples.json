{
    "syntactic": [
        "gpu",
        "engine",
        "video streams",
        "optimization",
        "object detection",
        "inference",
        "cache",
        "trade",
        "gpus",
        "jitter"
    ],
    "semantic": [
        "gpu",
        "engine",
        "video streams",
        "optimization",
        "object detection",
        "inference",
        "cache",
        "trade",
        "gpus",
        "probabilistic inference",
        "jitter"
    ],
    "union": [
        "gpu",
        "engine",
        "video streams",
        "optimization",
        "object detection",
        "inference",
        "cache",
        "trade",
        "gpus",
        "probabilistic inference",
        "jitter"
    ],
    "enhanced": [
        "program processors",
        "engineering",
        "video streaming",
        "mathematics",
        "object recognition",
        "inference engines",
        "operating systems",
        "buffer storage",
        "cache memory",
        "storage allocation (computer)",
        "commerce",
        "cuda",
        "bayesian methods",
        "probability distributions",
        "bandwidth",
        "parallel processing systems",
        "microprocessor chips",
        "image coding",
        "multimedia systems",
        "pattern recognition",
        "image segmentation",
        "artificial intelligence",
        "computer science",
        "telecommunication traffic",
        "data communication systems",
        "economics",
        "parallel programming",
        "graphics processing unit",
        "probability",
        "computer networks",
        "distributed systems",
        "computer hardware",
        "image processing",
        "internet",
        "machine learning",
        "image analysis",
        "telecommunication networks",
        "computer systems",
        "parallel algorithms",
        "parallel architectures",
        "computer programming languages",
        "distributed computer systems",
        "computer imaging and vision",
        "telecommunication systems",
        "computer programming"
    ],
    "explanation": {
        "object detection": [
            "object detection"
        ],
        "video streams": [
            "time video",
            "video",
            "video stream"
        ],
        "gpu": [
            "gpu",
            "gpus",
            "nvidia"
        ],
        "optimization": [
            "optimize",
            "optimization"
        ],
        "trade": [
            "trade"
        ],
        "inference": [
            "inference inference",
            "time inference",
            "range inference",
            "schedule inference",
            "performance inference",
            "inference",
            "usage inference",
            "size inference",
            "int8 inference",
            "inference latency",
            "inference datasets",
            "fast inference",
            "optimize inference",
            "coco inference",
            "inference evaluation",
            "accuracy inference",
            "inference nvidia",
            "inference export",
            "inference documentation",
            "detection inference inference",
            "inference measure",
            "inference int8",
            "detection inference",
            "inference inference documentation"
        ],
        "engine": [
            "engine"
        ],
        "jitter": [
            "jitter"
        ],
        "cache": [
            "cache"
        ],
        "gpus": [
            "gpu",
            "gpus"
        ],
        "probabilistic inference": [
            "inference inference",
            "time inference",
            "range inference",
            "schedule inference",
            "performance inference",
            "inference",
            "usage inference",
            "size inference",
            "int8 inference",
            "inference latency",
            "inference datasets",
            "fast inference",
            "optimize inference",
            "coco inference",
            "inference evaluation",
            "accuracy inference",
            "inference nvidia",
            "inference export",
            "inference documentation",
            "detection inference inference",
            "inference measure",
            "inference int8",
            "detection inference",
            "inference inference documentation"
        ],
        "program processors": [
            "gpu",
            "gpus",
            "nvidia"
        ],
        "engineering": [
            "engine"
        ],
        "video streaming": [
            "time video",
            "video",
            "video stream"
        ],
        "mathematics": [
            "inference inference",
            "optimization",
            "time inference",
            "range inference",
            "schedule inference",
            "performance inference",
            "inference",
            "usage inference",
            "size inference",
            "int8 inference",
            "inference latency",
            "inference datasets",
            "fast inference",
            "optimize inference",
            "coco inference",
            "inference evaluation",
            "accuracy inference",
            "inference nvidia",
            "inference export",
            "inference documentation",
            "detection inference inference",
            "inference measure",
            "optimize",
            "inference int8",
            "detection inference",
            "inference inference documentation"
        ],
        "object recognition": [
            "object detection"
        ],
        "inference engines": [
            "time inference",
            "schedule inference",
            "inference",
            "usage inference",
            "int8 inference",
            "inference latency",
            "optimize inference",
            "coco inference",
            "inference evaluation",
            "accuracy inference",
            "inference export",
            "detection inference inference",
            "inference measure",
            "detection inference",
            "inference inference",
            "range inference",
            "performance inference",
            "size inference",
            "inference datasets",
            "fast inference",
            "inference nvidia",
            "inference documentation",
            "inference int8",
            "inference inference documentation"
        ],
        "operating systems": [
            "cache"
        ],
        "buffer storage": [
            "cache"
        ],
        "cache memory": [
            "cache"
        ],
        "storage allocation (computer)": [
            "cache"
        ],
        "commerce": [
            "trade"
        ],
        "cuda": [
            "gpu",
            "gpus"
        ],
        "bayesian methods": [
            "inference inference",
            "time inference",
            "range inference",
            "schedule inference",
            "performance inference",
            "inference",
            "usage inference",
            "size inference",
            "int8 inference",
            "inference latency",
            "inference datasets",
            "fast inference",
            "optimize inference",
            "coco inference",
            "inference evaluation",
            "accuracy inference",
            "inference nvidia",
            "inference export",
            "inference documentation",
            "detection inference inference",
            "inference measure",
            "inference int8",
            "detection inference",
            "inference inference documentation"
        ],
        "probability distributions": [
            "inference inference",
            "time inference",
            "range inference",
            "schedule inference",
            "performance inference",
            "inference",
            "usage inference",
            "size inference",
            "int8 inference",
            "inference latency",
            "inference datasets",
            "fast inference",
            "optimize inference",
            "coco inference",
            "inference evaluation",
            "accuracy inference",
            "inference nvidia",
            "inference export",
            "inference documentation",
            "detection inference inference",
            "inference measure",
            "inference int8",
            "detection inference",
            "inference inference documentation"
        ],
        "bandwidth": [
            "cache",
            "jitter"
        ],
        "parallel processing systems": [
            "gpu",
            "cache",
            "gpus",
            "nvidia"
        ],
        "microprocessor chips": [
            "gpu",
            "cache",
            "gpus",
            "nvidia"
        ],
        "image coding": [
            "video",
            "video stream",
            "time video"
        ],
        "multimedia systems": [
            "video",
            "video stream",
            "time video"
        ],
        "pattern recognition": [
            "object detection"
        ],
        "image segmentation": [
            "object detection"
        ],
        "artificial intelligence": [
            "time inference",
            "schedule inference",
            "inference",
            "usage inference",
            "int8 inference",
            "inference latency",
            "optimize inference",
            "coco inference",
            "object detection",
            "inference evaluation",
            "accuracy inference",
            "inference export",
            "detection inference inference",
            "inference measure",
            "detection inference",
            "inference inference",
            "range inference",
            "performance inference",
            "size inference",
            "inference datasets",
            "fast inference",
            "inference nvidia",
            "inference documentation",
            "inference int8",
            "inference inference documentation"
        ],
        "computer science": [
            "video stream",
            "time inference",
            "schedule inference",
            "inference",
            "video",
            "usage inference",
            "gpu",
            "int8 inference",
            "inference latency",
            "optimize inference",
            "coco inference",
            "object detection",
            "inference evaluation",
            "accuracy inference",
            "cache",
            "inference export",
            "detection inference inference",
            "gpus",
            "inference measure",
            "detection inference",
            "inference inference",
            "time video",
            "range inference",
            "performance inference",
            "nvidia",
            "size inference",
            "inference datasets",
            "fast inference",
            "inference nvidia",
            "inference documentation",
            "inference int8",
            "inference inference documentation",
            "jitter"
        ],
        "telecommunication traffic": [
            "cache"
        ],
        "data communication systems": [
            "cache"
        ],
        "economics": [
            "trade"
        ],
        "parallel programming": [
            "gpu",
            "gpus"
        ],
        "graphics processing unit": [
            "gpu",
            "gpus"
        ],
        "probability": [
            "time inference",
            "schedule inference",
            "inference",
            "usage inference",
            "int8 inference",
            "inference latency",
            "optimize inference",
            "coco inference",
            "inference evaluation",
            "accuracy inference",
            "inference export",
            "detection inference inference",
            "inference measure",
            "detection inference",
            "inference inference",
            "range inference",
            "performance inference",
            "size inference",
            "inference datasets",
            "fast inference",
            "inference nvidia",
            "inference documentation",
            "inference int8",
            "inference inference documentation"
        ],
        "computer networks": [
            "cache",
            "jitter"
        ],
        "distributed systems": [
            "gpu",
            "cache",
            "gpus",
            "nvidia"
        ],
        "computer hardware": [
            "gpu",
            "cache",
            "gpus",
            "nvidia"
        ],
        "image processing": [
            "video",
            "video stream",
            "time video",
            "object detection"
        ],
        "internet": [
            "video",
            "video stream",
            "time video"
        ],
        "machine learning": [
            "object detection"
        ],
        "image analysis": [
            "object detection"
        ],
        "telecommunication networks": [
            "cache"
        ],
        "computer systems": [
            "gpu",
            "cache",
            "gpus",
            "nvidia"
        ],
        "parallel algorithms": [
            "gpu",
            "gpus"
        ],
        "parallel architectures": [
            "gpu",
            "gpus"
        ],
        "computer programming languages": [
            "gpu",
            "gpus"
        ],
        "distributed computer systems": [
            "gpu",
            "cache",
            "gpus",
            "nvidia"
        ],
        "computer imaging and vision": [
            "video",
            "video stream",
            "time video",
            "object detection"
        ],
        "telecommunication systems": [
            "cache"
        ],
        "computer programming": [
            "gpu",
            "gpus"
        ]
    }
}