Text;Label;Repo
"""""";General;https://github.com/JonasRSV/DQN
"""""";Computer Vision;https://github.com/PaiHsuehChung/DockerYOLOV3
"""""";Reinforcement Learning;https://github.com/tomaszsmaruj25/Twin-Delayed-DDPG-Implementation
"""""";Sequential;https://github.com/TartuNLP/deepvoice3_pytorch
"""""";Computer Vision;https://github.com/YanWei123/Pytorch-implementation-of-FoldingNet-encoder-and-decoder-with-graph-pooling-covariance-add-quanti
"""""";Audio;https://github.com/keonlee9420/STYLER
"""""";Computer Vision;https://github.com/wenxinxu/resnet-in-tensorflow
"""""";Computer Vision;https://github.com/facebookresearch/SparseConvNet
"""""";Computer Vision;https://github.com/AirBernard/DenseNet-by-Pytorch
"""""";General;https://github.com/slme1109/Lyrics_Generator_Using_LSTM
"""""";Computer Vision;https://github.com/zanmange/darknet
"""""";General;https://github.com/lavoiems/Cats-UDT
"""""";Computer Vision;https://github.com/Daividao/semantic-segmentation-deeplearning
"""""";Computer Vision;https://github.com/raguilar-f/Thermal-Face-Recognition-Using-Convolutional-Neural-Networks-and-Transfer-Learning
"""In this repo  I have written and refactored some code to mirror strings using the seq2seq model. For example  if the input is ABC  the ground-truth output is CBA.  """;Natural Language Processing;https://github.com/trqminh/seq2seq
"""""";General;https://github.com/SsnL/moco_align_uniform
"""""";General;https://github.com/tristandeleu/pytorch-structured-sparsity
"""""";Computer Vision;https://github.com/davidstap/AttnGAN
"""""";General;https://github.com/mkocabas/focal-loss-keras
"""""";Reinforcement Learning;https://github.com/anhtu293/NeurIPS-2019-Challenge
"""""";Computer Vision;https://github.com/ihdia/BoundaryNet
"""""";General;https://github.com/JaryHuang/awesome_SSD_FPN_GIoU
"""""";General;https://github.com/Sushma07/dancedarknet
"""""";General;https://github.com/mboudiaf/Mutual-Information-Variational-Bounds
"""""";Computer Vision;https://github.com/zzhuolun/IRL
"""""";General;https://github.com/alannguyencs/maskrcnn
"""""";General;https://github.com/Luvata/awesome-stars
"""""";Computer Vision;https://github.com/stefkim/stylegan-batik
"""""";General;https://github.com/jerry73204/mobilenet-v3-rs
"""""";General;https://github.com/kc-ml2/darts
"""""";Computer Vision;https://github.com/zhenxuan00/graphical-gan
"""""";Sequential;https://github.com/ShotDownDiane/tcn-master
"""""";Computer Vision;https://github.com/euske/derpyolo
"""""";Computer Vision;https://github.com/gnoses/ViT_examples
"""""";Natural Language Processing;https://github.com/feyzaakyurek/XLM-LwLL
"""""";Computer Vision;https://github.com/trichtu/ConvLSTM-RAU-net
"""""";Computer Vision;https://github.com/greatsharma/DeepLearning-Papers-Implementation
"""""";General;https://github.com/ShaojieJiang/tldr
"""""";General;https://github.com/majumderb/rezero
"""""";Natural Language Processing;https://github.com/thu-coai/ConvLab-2
"""""";General;https://github.com/myunghakLee/AdversarialTraining_manifoldMixup
"""""";Sequential;https://github.com/r9y9/wavenet
"""""";Computer Vision;https://github.com/uam-biometrics/Transformer-Networks
"""""";Computer Vision;https://github.com/richardrl/gan-pytorch
"""""";Computer Vision;https://github.com/TheTrveAnthony/no-Green
"""""";General;https://github.com/AD1024/torch-checkpointing
"""""";General;https://github.com/ArdalanM/nlp-benchmarks
"""""";General;https://github.com/xiaolai-sqlai/mobilenetv3
"""Automatic image colorization has been a popular image-to-image translation problem of significant interest for several practical application areas including restoration of aged or degraded images. This project attempts to utilize CycleGANs to colorize grayscale images back to their colorful RGB form.   """;General;https://github.com/ArkaJU/Image-Colorization-CycleGAN
"""""";Computer Vision;https://github.com/m-bain/video-transformers
"""""";General;https://github.com/OlivierAlgoet/Tensorflow2-GMM
"""""";Graphs;https://github.com/Diego999/pyGAT
"""""";General;https://github.com/semicontinuity/nlp
"""""";Computer Vision;https://github.com/vignesh-creator/VectorQuantized-VAE
"""""";Computer Vision;https://github.com/edchandler00/unet
"""Recent research has shown that numerous human-interpretable directions exist in the latent space of GANs. In this paper  we develop an automatic procedure for finding directions that lead to foreground-background image separation  and we use these directions to train an image segmentation model without human supervision. Our method is generator-agnostic  producing strong segmentation results with a wide range of different GAN architectures. Furthermore  by leveraging GANs pretrained on large datasets such as ImageNet  we are able to segment images from a range of domains without further training or finetuning. Evaluating our method on image segmentation benchmarks  we compare favorably to prior work while using neither human supervision nor access to the training data. Broadly  our results demonstrate that automatically extracting foreground-background structure from pretrained deep generative models can serve as a remarkably effective substitute for human supervision.    """;Computer Vision;https://github.com/lukemelas/unsupervised-image-segmentation
"""To lead the way with reproducibility  Reaver is bundled with pre-trained weights and full Tensorboard summary logs for all six minigames.  Simply download an experiment archive from the [releases](https://github.com/inoryy/reaver-pysc2/releases) tab and unzip onto the `results/` directory.  You can use pre-trained weights by appending `--experiment` flag to `reaver.run` command:      python reaver.run --map <map_name> --experiment <map_name>_reaver --test 2> stderr.log  Tensorboard logs are available if you launch `tensorboard --logidr=results/summaries`.   You can also view them [directly online](https://boards.aughie.org/board/HWi4xmuvuOSuw09QBfyDD-oNF1U) via [Aughie Boards](https://boards.aughie.org/).   Reaver is a modular deep reinforcement learning framework with a focus on various StarCraft II based tasks  following in DeepMind's footsteps  who are pushing state-of-the-art of the field through the lens of playing a modern video game with human-like interface and limitations.  This includes observing visual features similar (though not identical) to what a human player would perceive and choosing actions from similar pool of options a human player would have. See [StarCraft II: A New Challenge for Reinforcement Learning](https://arxiv.org/abs/1708.04782) article for more details.  Though development is research-driven  the philosophy behind Reaver API is akin to StarCraft II game itself -  it has something to offer both for novices and experts in the field. For hobbyist programmers Reaver offers all the tools necessary to train DRL agents by modifying only a small and isolated part of the agent (e.g. hyperparameters). For veteran researchers Reaver offers simple  but performance-optimized codebase with modular architecture:  agent  model  and environment are decoupled and can be swapped at will.  While the focus of Reaver is on StarCraft II  it also has full support for other popular environments  notably Atari and MuJoCo.  Reaver agent algorithms are validated against reference results  e.g. PPO agent is able to match [ Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347). Please see [below](#but-wait-theres-more) for more details.   """;Reinforcement Learning;https://github.com/inoryy/reaver
"""""";Reinforcement Learning;https://github.com/J93T/TP4-DDPG
"""""";General;https://github.com/anonNo2/MulTeacher-KD
"""The articulated 3D pose of the human body is high-dimensional and complex.  Many applications make use of a prior distribution over valid human poses  but modeling this distribution is difficult. Here we present VPoser  a learning based variational human pose prior trained from a large dataset of human poses represented as SMPL bodies. This body prior can be used as an Inverse Kinematics (IK) solver for many tasks such as fitting a body model to images  as the main contribution of this repository for [SMPLify-X](https://smpl-x.is.tue.mpg.de/).  VPoser has the following features:   - defines a prior of SMPL pose parameters  - is end-to-end differentiable  - provides a way to penalize impossible poses while admitting valid ones  - effectively models correlations among the joints of the body  - introduces an efficient  low-dimensional  representation for human pose  - can be used to generate valid 3D human poses for data-dependent tasks   """;Computer Vision;https://github.com/nghorbani/human_body_prior
"""This repository provides the official PyTorch implementation of UNAS that was presented at CVPR 2020.  The paper presents results in two search spaces including [DARTS](https://arxiv.org/abs/1806.09055)  and [ProxylessNAS](https://arxiv.org/abs/1812.00332) spaces. Our paper can be found [here](https://arxiv.org/abs/1912.07651).    """;Computer Vision;https://github.com/NVlabs/unas
"""""";General;https://github.com/gan3sh500/attention-augmented-conv
"""""";Computer Vision;https://github.com/lRomul/argus-freesound
"""""";Natural Language Processing;https://github.com/deepmind/xquad
"""""";General;https://github.com/Techyee/darknet_resource
"""""";Audio;https://github.com/Vikas-Sony/speech-to-text
"""""";General;https://github.com/DylanSpicker/STAT923-Final-Project
"""""";Computer Vision;https://github.com/se-p-93/Deep-Learning-Research-Papers
"""""";General;https://github.com/Bao-Jiarong/ResNet
"""""";Computer Vision;https://github.com/jmjeon94/MobileNet-Pytorch
"""""";Computer Vision;https://github.com/MEME-Phoenix/Autonomous-Driving-Cart-MEME
"""""";General;https://github.com/luzerochen/MobileNet_pytorch
"""""";Reinforcement Learning;https://github.com/rail-berkeley/softlearning
"""""";Computer Vision;https://github.com/raulmur/ORB_SLAM2
"""""";General;https://github.com/kongzhiyou/darknet-master
"""""";Computer Vision;https://github.com/snilloO/obj_det_loss
"""""";Computer Vision;https://github.com/mronta/CycleGAN-in-Keras
"""""";General;https://github.com/google-research/sam
"""""";General;https://github.com/jimzers/stylegan2-colabs
"""""";General;https://github.com/WGLab/SGAN
"""""";Computer Vision;https://github.com/ranjith0430/ML-tutorials
"""""";Computer Vision;https://github.com/sermanet/OverFeat
"""BART model [https://arxiv.org/pdf/1910.13461.pdf](https://arxiv.org/pdf/1910.13461.pdf)  Fairseq [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)  Fairseq tutorial on fine-tuning BART on Seq2Seq task [https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md](https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md)  Emerald dataset used for faceted summarization - https://github.com/hfthair/emerald_crawler    """;Sequential;https://github.com/khushsi/Finetuning_BART_for_FACET_Summarization
"""""";Computer Vision;https://github.com/wlstyql/keras_with_mat_for_unet
"""""";Reinforcement Learning;https://github.com/ianlimle/ItsMeMario
"""This is a MATLAB implementation of a 2 hidden-layers neural network that recognizes handwritten digit with 97% accuracy on MNIST database. The architecture and training parameters of the network is configurable (including number of layers  number of neurons in each layer  number of training rounds  learning rate  and mini-batch length). The network integrates input normalization  He weight initialization [1]  and Swish activation function [2].  """;General;https://github.com/phogbinh/handwritten-digit-recognition
"""""";Computer Vision;https://github.com/neherh/HyperStackNet
"""""";Computer Vision;https://github.com/samson6460/tf2_Segmentation
"""Mesh TensorFlow (`mtf`) is a language for distributed deep learning  capable of specifying a broad class of distributed tensor computations.  The purpose of Mesh TensorFlow is to formalize and implement distribution strategies for your computation graph over your hardware/processors. For example: ""Split the batch over rows of processors and split the units in the hidden layer across columns of processors."" Mesh TensorFlow is implemented as a layer over TensorFlow.  Watch our [YouTube video](https://www.youtube.com/watch?v=HgGyWS40g-g).    """;Natural Language Processing;https://github.com/tensorflow/mesh
"""""";Computer Vision;https://github.com/Yashgh7076/CU-Thesis
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/XMdidi/SMP_Bert
"""| Version | Validation Accuracy | Model Size | Training Time | | ------- | ------------------- | ---------- | ------------- | | 1       | 71.88%              | 219 MB     | 3 Hours       | | 2       | 39.53%              | 16 MB      | 7.6 Minutes   | | 3       | 48.59%              | 16 MB      | 14.9 Minutes  | | 4       | 67.19%              | 7 MB       | 10 Minutes    |   """;Computer Vision;https://github.com/2manslkh/korean-food-classification-flask
"""""";Natural Language Processing;https://github.com/wakafengfan/simcse-pytorch
"""Generative Adversarial Networks (GANs) are one of the most popular (and coolest) Machine Learning algorithms developed in recent times. They belong to a set of algorithms called generative models  which are widely used for unupervised learning tasks which aim to learn the uderlying structure of the given data. As the name suggests GANs allow you to generate new unseen data that mimic the actual given real data. However  GANs pose problems in training and require carefullly tuned hyperparameters.This paper aims to solve this problem.  DCGAN is one of the most popular and succesful network design for GAN. It mainly composes of convolution layers  without max pooling or fully connected layers. It uses strided convolutions and transposed convolutions  for the downsampling and the upsampling respectively.  **Generator architecture of DCGAN** <p align=""center""> <img src=""images/Generator.png"" title=""DCGAN Generator"" alt=""DCGAN Generator""> </p>  **Network Design of DCGAN:** * Replace all pooling layers with strided convolutions. * Remove all fully connected layers. * Use transposed convolutions for upsampling. * Use Batch Normalization after every layer except after the output layer of the generator and the input layer of the discriminator. * Use ReLU non-linearity for each layer in the generator except for output layer use tanh. * Use Leaky-ReLU non-linearity for each layer of the disciminator excpet for output layer use sigmoid.   """;Computer Vision;https://github.com/rajprakrit/DCGAN
"""""";Computer Vision;https://github.com/Element-Research/dpnn
"""![](./data/UncerGuidedI2I_Model.gif)  This repository provides the code for the **MICCAI-2021** paper titled ""[Uncertainty-guided Progressive GANs for Medical Image Translation](https://arxiv.org/abs/2106.15542)"".  We take inspiration from the progressive learning scheme demonstrated at [MedGAN](https://arxiv.org/abs/1806.06397) and [Progressive GANs](https://arxiv.org/abs/1710.10196)  and augment the learning with the estimation of intermediate uncertainty maps (as presented [here](http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-061.pdf) and [here](https://arxiv.org/pdf/2102.11747.pdf))  that are used as attention map to focus the image translation in poorly generated (highly uncertain) regions  progressively improving the images over multiple phases.  ![](./data/UncerGuidedI2I_res.gif)  The structure of the repository is as follows: ``` root  |-ckpt/ (will save all the checkpoints)  |-data/ (save your data and related script)  |-src/ (contains all the source code)     |-ds.py      |-networks.py     |-utils.py     |-losses.py ```   """;Computer Vision;https://github.com/ExplainableML/UncerGuidedI2I
""" This is the **ranked No.1** tensorflow based scene text spotting algorithm on [__ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text__](https://rrc.cvc.uab.es/?ch=14) (Latin Only  Latin and Chinese)  futhermore  the algorithm is also adopted in [__ICDAR2019 Robust Reading Challenge on Large-scale Street View Text with Partial Labeling__](https://rrc.cvc.uab.es/?ch=16) and [__ICDAR2019 Robust Reading Challenge on Reading Chinese Text on Signboard__](https://rrc.cvc.uab.es/?ch=12).   Scene text detection algorithm is modified from [__Tensorpack FasterRCNN__](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN)  and we only open source code in this repository for scene text recognition. I upload ICDAR2019 ArT competition model to docker hub  please refer to [Docker](#Docker). For more details  please refer to our [__arXiv technical report__](https://arxiv.org/abs/1912.04561).  Our text recognition algorithm not only recognizes Latin and Non-Latin characters  but also supports horizontal and vertical text recognition in one model. It is convenient for multi-lingual arbitrary-shaped text recognition.  **Note that the competition model in docker container as described in [__our technical report__](https://arxiv.org/abs/1912.04561) is slightly different from the recognition model trained from this updated repository.**   """;General;https://github.com/smisthzhu/attentionocr
"""""";Computer Vision;https://github.com/notmariekondo/notmariekondo
"""""";Computer Vision;https://github.com/deeplearningnapratica/pulse
"""""";Computer Vision;https://github.com/zhong110020/py-faster-rcnn
"""""";Computer Vision;https://github.com/khanh-moriaty/DarknetAlexey
"""""";General;https://github.com/32shivang/Blind-Eye
"""""";Audio;https://github.com/kaituoxu/Conv-TasNet
"""""";General;https://github.com/Xiaomao136/stylegan2-faceswap
"""""";Computer Vision;https://github.com/realmichaelye/Skin-Lesions-Classifier-ResNet50
"""• main.py script creates the whole model consiting of localisation network  Grid generator and sampler and Recognition network.<br /> • stn_network.py script crates spatial transformer network  Grid genrator and bilinearsampler.<br /> • resnet_stn.py script creates detection and recognition resnet network as proposed by the author.<br />   """;Computer Vision;https://github.com/vinod377/STN-OCR-Tensorflow
"""""";Computer Vision;https://github.com/vamsi3/simple-GAN
"""""";General;https://github.com/ariangc/breinchallenge
"""""";Computer Vision;https://github.com/demul/AlexNet
"""""";Reinforcement Learning;https://github.com/efg59/Rainbow
"""""";General;https://github.com/buddly27/stylish
"""""";Reinforcement Learning;https://github.com/harruff/Senior_Project_Repository
"""""";Computer Vision;https://github.com/RRDajay/DCGAN_MNIST
"""""";Reinforcement Learning;https://github.com/jedz5/hisFirstRepo
"""""";Computer Vision;https://github.com/paraficial/vae_pancreas_segmentation
"""R-CNN is a state-of-the-art visual object detection system that combines bottom-up region proposals with rich features computed by a convolutional neural network. At the time of its release  R-CNN improved the previous best detection performance on PASCAL VOC 2012 by 30% relative  going from 40.9% to 53.3% mean average precision. Unlike the previous best results  R-CNN achieves this performance without using contextual rescoring or an ensemble of feature types.  R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1311.2524) and will appear in a forthcoming CVPR 2014 paper.   """;Computer Vision;https://github.com/jiangbestone/detect_rcnn
"""""";General;https://github.com/lschirmer/Attention-Augmented-Convolutional-Keras-Networks
"""""";Computer Vision;https://github.com/fathimazarin/SegNet
"""""";Computer Vision;https://github.com/hamidriasat/Computer-Vision-and-Deep-Learning
"""""";Computer Vision;https://github.com/hkchengrex/Mask-Propagation
"""Fully Convolutional Networks (FCNs) are a natural extension of CNNs to tackle per pixel prediction problems such as semantic image segmentation. FCNs add upsampling layers to standard CNNs to recover the spatial resolution of the input at the output layer. In  order to compensate for the resolution loss induced by pooling layers  FCNs introduce skip connections between their downsampling  and upsampling paths. Skip connections help the upsampling path recover fine-grained information from the downsampling layers.  One evolution of CNNs are [Residual Networks](https://arxiv.org/abs/1512.03385) (ResNets). ResNets are designed to ease the training of  very deep networks by introducing a residual block that sums the non-linear transformation of the input and its identity mapping.  The identity mapping is implemented by means of a shortcut connection. ResNets can be extended to work as FCNs. ResNets incorporate  shortcut paths to FCNs and increase the number of connections within a network. This additional shortcut paths improve the segmentation  accuracy and also help the network to converge faster.  Recently another CNN architecture called [DenseNet](https://arxiv.org/abs/1608.06993) has been introduced. DenseNets are built from  *dense blocks* and pooling operations  where each dense block is an iterative concatenation of previous feature maps. This architecture  can be seen as an extension of ResNets  which performs iterative summation of previous feature maps. The result of this modification  is that DenseNets are more efficient in there parameter usage.  The [https://arxiv.org/abs/1611.09326](https://arxiv.org/abs/1611.09326) paper extends DenseNets to work as FCNs by adding an upsampling  path to recover the full input resolution.    """;General;https://github.com/asprenger/keras_fc_densenet
"""""";General;https://github.com/Edmonton-School-of-AI/ml5-Simple-Image-Classification
"""""";Computer Vision;https://github.com/NZ99/bottleneck-transformer-flax
"""""";General;https://github.com/r9y9/deepvoice3_pytorch
"""""";General;https://github.com/yiningzeng/darknet-fork-from-AlexeyAB
"""""";Computer Vision;https://github.com/JeongHyunJin/Jeong2020_SolarFarsideMagnetograms
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/liuky74/caffe_liuky74
"""""";General;https://github.com/deepmind/dm-haiku
"""""";Natural Language Processing;https://github.com/palmagro/gg2vec
"""""";Computer Vision;https://github.com/manicman1999/StyleGAN2-Tensorflow-2.0
"""""";Computer Vision;https://github.com/wen227/Pokemon_Classification
"""""";Computer Vision;https://github.com/silvandeleemput/memcnn
"""""";Computer Vision;https://github.com/hershd23/ObjectLocalizer
"""""";Natural Language Processing;https://github.com/askaydevs/distillbert-qa
"""""";Computer Vision;https://github.com/feizc/Object-Detection-Pytorch
"""""";Reinforcement Learning;https://github.com/JohannesAck/MATD3implementation
"""""";General;https://github.com/Stick-To/PyramidNet-tensorflow
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/1wy/bert
"""""";Computer Vision;https://github.com/johnsun03/myTest
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/yet124/yolov3-pytorch
"""""";Computer Vision;https://github.com/cant12/MammoGan
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/socc-io/piqaboo
"""""";Computer Vision;https://github.com/hjwdzh/ManifoldPlus
"""""";Computer Vision;https://github.com/fudan-zvg/SETR
"""""";Computer Vision;https://github.com/glouppe/info8010-deep-learning
"""""";Natural Language Processing;https://github.com/EssayKillerBrain/NLP-BERT-Chinese
"""""";Sequential;https://github.com/DevonFulcher/CryptoPricePredictor
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/Atom-min/MLBigWork
"""""";Computer Vision;https://github.com/princeton-vl/CornerNet-Lite
"""""";Computer Vision;https://github.com/janmejoykar1807/CoolHead
"""This project maps tree extent at the ten-meter scale using open source artificial intelligence and satellite imagery. The data enables accurate reporting of tree cover in urban areas  tree cover on agricultural lands  and tree cover in open canopy and dry forest ecosystems.   This repository contains the source code for the project. A full description of the methodology can be found [on arXiv](https://arxiv.org/abs/2005.08702). The data product specifications can be accessed on the wiki page. *  [Background](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#background) *  [Data Extent](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#data-extent) *  [Methodology](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#methodology) *  [Validation and Analysis](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#validation-and-analysis) | [Jupyter Notebook](https://github.com/wri/restoration-mapper/blob/master/notebooks/analysis/validation-analysis.ipynb) *  [Definitions](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#definitions) *  [Limitations](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#limitations)    """;General;https://github.com/wri/sentinel-tree-cover
"""""";Natural Language Processing;https://github.com/Skumarr53/Attention-is-All-you-Need-PyTorch
"""""";General;https://github.com/hz2538/iot_JennyGo
"""""";General;https://github.com/OpenBioLink/SAFRAN
"""""";Computer Vision;https://github.com/ithuanhuan/gpu-py3-faster-rcnn
"""""";Computer Vision;https://github.com/anxingle/UNet-pytorch
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/zqplgl/caffe
"""In this work  3D image features are learned in a self-supervised way by training pointNet network to recognize the 3d rotation that been applied to an input image. Those learned features are being used for classification task learned on top of this base network.  In this repository I uploaded code and data.   """;Computer Vision;https://github.com/aviros/pointnet_totations
"""""";General;https://github.com/SamSamhuns/ml_gan_human_face_synthesis
"""""";Computer Vision;https://github.com/tigerofmurder/tf-faster-rcnn
"""Question-answering problem is currenlty one of the most chellenging task in Natural Language Processing domain. In purpose to solve it transfer learning is state of the art method. Thanks to huggingface-transformers which made avaiable pretrained NLP most advanced models (like: BERT   GPT-2  XLNet  RoBERTa  DistilBERT) relatively easy to be used in different language tasks.   Original [akensert](https://www.kaggle.com/akensert/quest-bert-base-tf2-0) code was tested with different parameters and changed base models. From both implemented (XLNet  RoBERTa) the second one resulted in better score. Further improvement ccould be made by implementation of combined model version. For example it could consists of BERT  RoBERTa and XLNet.   Change of main algorithm from BERT to RoBERTa was justified by the fact that second one is an improved version of the first one. The expansion of the algorithm name is Robustly Optimized BERT Pretraining Approach  it modifications consists of [5]: - training the model longer  with bigger batches  over more data;  - removing the next sentence prediction objective;  - training on longer sequences;  - dynamically changing the masking pattern applied to the training data.  Use of RoBERTa consequently causes the need of configuration change and implementation of RoBERTa sepcific tokenizer. It constructs a RoBERTa BPE tokenizer  derived from the GPT-2 tokenizer  using byte-level Byte-Pair-Encoding. Which works in that order: 1. Prepare a large enough training data (i.e. corpus) 2. Define a desired subword vocabulary size 3. Split word to sequence of characters and appending suffix “</w>” to end of word with word frequency. So the basic unit is character in this stage. For example  the frequency of “low” is 5  then we rephrase it to “l o w </w>”: 5 4. Generating a new subword according to the high frequency occurrence. 5. Repeating step 4 until reaching subword vocabulary size which is defined in step 2 or the next highest frequency pair is 1.  Values of the tuning parameters (folds  epochs  batch_size) was mostly implicated by the kaggle GPU power and competition constrain of kernel computation limitation to 2 hours run-time.  Final step was calculation of predicitons taking into acount results averaged results for folds. Weights have been assigned by empricialy tring different values. The change of particular ones was based on the prediction score. Limitation was only the summing up of weights to one. Change of arithmetic mean of folds predictions to weighted average improved results in public leaderboard from 0.38459 to 0.38798. On the other hand as the final scores on the private leaderboard showed it was not good choice. Finally  soo strictly assignment of weights caused the decrease in final result from 0.36925 to 0.36724.  XLNet was also tested (to show it  the code with it was left commented). In theory XLNet should overcome BERT limitations. Relying on corrupting the input with masks  BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons XLNet  which is characterised by: - learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order; - overcomes the limitations of BERT thanks to its autoregressive formulation; - integrates ideas from Transformer-XL  the state-of-the-art autoregressive model  into pretraining. Empirically  under comparable experiment settings  XLNet outperforms BERT on 20 tasks  often by a large margin  including question answering  natural language inference  sentiment analysis  and document ranking [6].    After all public score for XLNet version was lower (0.36310) than score (0.37886) for base BERT model and was rejected.     ""In this competition  you’re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites  in a ""common-sense"" fashion. Our raters received minimal guidance and training  and relied largely on their subjective interpretation of the prompts. As such  each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines  we hope to increase the re-use value of this data set.""[1]     """;Natural Language Processing;https://github.com/bluejurand/Kaggle_QA_Google_Labeling
"""""";Computer Vision;https://github.com/vonclites/squeezenet
"""""";General;https://github.com/lucasgb98/StyleGAN2
"""""";Computer Vision;https://github.com/paperswithcode/model-index
"""**Swin Transformer** (the name `Swin` stands for **S**hifted **win**dow) is initially described in [arxiv](https://arxiv.org/abs/2103.14030)  which capably serves as a general-purpose backbone for computer vision. It is basically a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection.  Swin Transformer achieves strong performance on COCO object detection (`58.7 box AP` and `51.1 mask AP` on test-dev) and ADE20K semantic segmentation (`53.5 mIoU` on val)  surpassing previous models by a large margin.  ![teaser](figures/teaser.png)   """;Computer Vision;https://github.com/DominickZhang/Distillation-Swin-Transformer
"""""";Computer Vision;https://github.com/modelhub-ai/yolo-v3
"""""";Sequential;https://github.com/Tobias-K93/media-bias-prediction
"""""";Graphs;https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding
"""""";General;https://github.com/paddorch/SupContrast.paddle
"""CenterNet is a framework for object detection with deep convolutional neural networks. You can use the code to train and evaluate a network for object detection on the MS-COCO dataset.  * It achieves state-of-the-art performance (an AP of 47.0%) on one of the most challenging dataset: MS-COCO.  * Our code is written in Python  based on [CornerNet](https://github.com/princeton-vl/CornerNet).  *More detailed descriptions of our approach and code will be made available soon.*  **If you encounter any problems in using our code  please contact Kaiwen Duan: kaiwen.duan@vipl.ict.ac.cn.**   """;Computer Vision;https://github.com/guohaoyuan/CenterNet-annotation
"""""";General;https://github.com/seishinkikuchi/test
"""""";Computer Vision;https://github.com/leon-liangwu/MaskYolo_Caffe
"""""";General;https://github.com/fangyiyu/Modified_MNIST_Classification
"""""";General;https://github.com/paperswithcode/paperswithcode-client
"""""";Sequential;https://github.com/NTT123/pointer-networks
"""Chat-bots are becoming more and more useful in various simple professional tasks as they get more and more able to capture the essence of communicating with people. Still the development of good chat-bots that will answer more complicated questions  in general subjects  is a growing domain of research.   The goal of this project is to create a chat-bot able to answer python related questions.  Our project started with the main idea being that a programming assistant would be a much needed help by many people working or studying computer science. Although it sounds simple it soon proved to be a difficult task. The main challenge is that the model has to extract a technical correlation between Questions and Answers in order to be able to communicate effectively. The model that we used in order to achieve our goal is a recurrent sequence-to-sequence model. The main steps that we followed are described bellow.  - We found  downloaded  and processed data taken from stack overflow concerning questions that contained at least one python tag.[7] - Implement a sequence-to-sequence model. - Jointly train encoder and decoder models using mini-batches - Used greedy-search decoding - Interact with trained chatbot   """;Sequential;https://github.com/vGkatsis/Chat_Bot_DL
"""""";Computer Vision;https://github.com/aaron-wang-de/ab
"""""";Computer Vision;https://github.com/FenHua/Robust_Logo_Detection
"""""";Computer Vision;https://github.com/nataliele/waldo
"""""";General;https://github.com/jqhoogland/rgpy
"""Deep-trace is a graphsage-based machine-learning pipeline for contact tracing. Conventional methods can only exploit knowledge of an individual person's contacts. Taken over the set of all individuals  this contact set is essentially a graph with nodes representing people and edges connecting contact between people. The proposed method allows us to utilize information stored in the graph contacts as well as node features to develop a method to classify individuals in the contact set as either susceptible  exposed or infected. In this particular case we use the covid vulnerability index to assign a feature vector to each node. We are then able to learn the contact network based not only on the graph node and edgelist specification  but also the vulnerability feature mapping. Thus we create a three-dimensional node embedding for new contacts that shows an assessment of their likelihood of being in one of three exposure categories – Infected  Exposed or Susceptible. This low dimensional embedding allows contact tracing personnel to prioritize which individuals they should contact and test in situations where a pandemic is evolving too quickly under limited personnel and test resources to correspond with everyone in the contact set. Thu one can quickly identify and prioritize which persons to contact and isolate. <br /> #  Figure 1 below shows a TSNE projection of the data onto three dimensions for a simulated case study of 27 infected  519 susceptible  and 419 exposed individuals:  ![alt text][image_2]  This is a 2-D projection of the same TSNE embedding:  ![alt text][image_3]   """;Graphs;https://github.com/silent-code/deep-trace
"""""";General;https://github.com/titu1994/keras-attention-augmented-convs
"""""";Computer Vision;https://github.com/CheesyB/cpointnet
"""""";Computer Vision;https://github.com/ifzhang/FairMOT
"""""";General;https://github.com/titu1994/MobileNetworks
"""""";General;https://github.com/nameoverflow/neuro-cangjie
"""""";Computer Vision;https://github.com/solapark/darknet_interpark
"""""";Computer Vision;https://github.com/SpikeKing/mobilenet_v3
"""""";Computer Vision;https://github.com/twhui/SRGAN-PyTorch
"""""";General;https://github.com/bimal1988/detector
"""This repository contains an example program to demonstrate the functionality of Gaussian processes and Bayesian Neural Networks who approximate Gaussian Processes under certain conditions  as shown by (Gal  2016): https://arxiv.org/pdf/1506.02142.pdf. \ In this example  a Gaussian Process for a simple regression task is implemented to demonstrate its prior and posterior function distribution. Then  a Bayesian Neural Network is trained which approximates the Gaussian process by variational inference. Again  the posterior distribution is plotted.    """;General;https://github.com/arneschmidt/bayesian_deep_learning
"""""";Computer Vision;https://github.com/ZackPashkin/YOLOv3-EfficientNet-EffYolo
"""""";General;https://github.com/philippwirth/awd-lstm-test
"""""";Computer Vision;https://github.com/Qengineering/YoloV2-ncnn-Raspberry-Pi-4
"""""";General;https://github.com/Joyce511/OpenVino_Realtime_Multi-Person_Pose_Estimation
"""The objective of this project is to learn more about conditional generative models. Having worked with GANs  it seems beneficial to study more about adding additional descriptive information with the input image to produce models that are able to distinctly represent specific subjects in the generated data. It seems to be a part of how users can select specific features or labels for the model to generate. As an early step of looking at this and taking into account the limitations of resources and time  this project will be experimenting with the vanilla variational autoencoder and a conditional variational autoencoder.    """;General;https://github.com/jenyliu/DLA_interview
"""""";Computer Vision;https://github.com/sraashis/deepdyn
"""""";Computer Vision;https://github.com/TencentARC/GFPGAN
"""""";General;https://github.com/JobQiu/hackrice
"""""";General;https://github.com/d-li14/mobilenetv3.pytorch
"""""";Computer Vision;https://github.com/MIT-SPARK/Kimera
"""""";Natural Language Processing;https://github.com/MalteHB/-l-ctra
"""""";Computer Vision;https://github.com/jordan-bird/synthetic-fruit-image-generator
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.    """;Computer Vision;https://github.com/liuji93/yolo_v3
"""""";General;https://github.com/leehomyc/Photo-Realistic-Super-Resoluton
"""""";General;https://github.com/philipperemy/tensorflow-maxout
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/aviros/roatationPointnet
"""""";Computer Vision;https://github.com/STomoya/ResNeSt
"""""";Computer Vision;https://github.com/shib0li/VAE-torch
"""English | [简体中文](README_CN.md)  [![Documentation](https://readthedocs.org/projects/mmpose/badge/?version=latest)](https://mmpose.readthedocs.io/en/latest/?badge=latest) [![actions](https://github.com/open-mmlab/mmpose/workflows/build/badge.svg)](https://github.com/open-mmlab/mmpose/actions) [![codecov](https://codecov.io/gh/open-mmlab/mmpose/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmpose) [![PyPI](https://img.shields.io/pypi/v/mmpose)](https://pypi.org/project/mmpose/) [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/master/LICENSE) [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues) [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)  MMPose is an open-source toolbox for pose estimation based on PyTorch. It is a part of the [OpenMMLab project](https://github.com/open-mmlab).  The master branch works with **PyTorch 1.5+**.  https://user-images.githubusercontent.com/15977946/124654387-0fd3c500-ded1-11eb-84f6-24eeddbf4d91.mp4   """;Computer Vision;https://github.com/open-mmlab/mmpose
"""""";Computer Vision;https://github.com/saloni1998/Cifar-10
"""""";Sequential;https://github.com/google/lyra
"""  MegaFace dataset includes 1 027 060 faces  690 572 identities. [Link](http://megaface.cs.washington.edu/)   Challenge 1 is taken to test our model with 1 million distractors.   ![image](https://github.com/foamliu/InsightFace-v2/raw/master/images/megaface_stats.png)    Use Labeled Faces in the Wild (LFW) dataset for performance evaluation:  - 13233 faces - 5749 identities - 1680 identities with >=2 photo   MS-Celeb-1M dataset for training  3 804 846 faces over 85 164 identities.    """;General;https://github.com/foamliu/InsightFace-v2
"""""";Sequential;https://github.com/kaiidams/voice100
"""""";Computer Vision;https://github.com/benjamintli/training-results-azure
"""""";Computer Vision;https://github.com/pacharadanait/MobileNet_V2
"""""";General;https://github.com/ml-research/rational_sl
"""""";Natural Language Processing;https://github.com/shelleyHLX/bilm_EMLo
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/qilei123/DeformableConvV2_crop
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/paolanu/BERT_epitope
"""""";Computer Vision;https://github.com/daixiangzi/ImprovedGan-pytorch
"""""";Computer Vision;https://github.com/XJhaoren/Academic-Group
"""""";General;https://github.com/sufeidechabei/gluon-mobilenet-yolov3
"""""";General;https://github.com/tree-park/kor-to-eng-translation
"""""";Computer Vision;https://github.com/holdfire/CLS
"""""";General;https://github.com/thomasbrandon/swish-torch
"""""";Computer Vision;https://github.com/Qengineering/Rfcn_ncnn
"""""";Computer Vision;https://github.com/eriklindernoren/Keras-GAN
"""""";Audio;https://github.com/adityaagrawal7/speech-to-text-wavenet
"""""";Computer Vision;https://github.com/gpleiss/limits_of_large_width
"""""";Computer Vision;https://github.com/patelmiteshn/darknet
"""""";Sequential;https://github.com/hadi-abdine/FrenchWordEmbeddingsDemo
"""""";Natural Language Processing;https://github.com/akkarimi/aeda_nlp
"""""";Computer Vision;https://github.com/rishikksh20/CoaT-pytorch
"""""";General;https://github.com/raisinglc/object_detection_SSD
"""""";General;https://github.com/calebemonteiro/AIDL_Project
"""""";General;https://github.com/sambit9238/deep_text_corrector
"""""";Computer Vision;https://github.com/cocopambag/ResNet
"""""";General;https://github.com/la3ma/IWAE-torch
"""""";Computer Vision;https://github.com/Thehunk1206/Covid-19-covidcnn
"""""";Computer Vision;https://github.com/pransen/faster_rcnn
"""""";General;https://github.com/AhmadQasim/proxylessnas-dense
"""""";General;https://github.com/johntd54/stanford_car
"""""";Computer Vision;https://github.com/fzimmermann89/sr4rs
"""""";Computer Vision;https://github.com/eggplant60/dcgan_anime
"""""";General;https://github.com/monishramadoss/SRGAN
"""""";General;https://github.com/fstahlberg/tensor2tensor
"""""";Computer Vision;https://github.com/NEUdeep/Swin-Transformer-Object-Detection
"""""";Computer Vision;https://github.com/Montia/bw2color
"""[image1]: https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif ""Trained Agent""  ![Trained Agent][image1]  The RL agent is allowed to traverse across a two dimensional grid with blue and yellow bananas placed across it. The agent is expected to collect the yellow bananas while avoiding the blue ones. The agent receives a positive reward for every yellow banana it collects and a negative reward for every blue banana collected. The size of the state space is 37. The agent is able to move forwards and backwards as well as turn left and right  thus the size of the action space is 4. The minimal expected performance of the agent after training is a score of +13 over 100 consecutive episodes.   """;Reinforcement Learning;https://github.com/prajwalgatti/DRL-Navigation
"""""";Reinforcement Learning;https://github.com/wassname/rl-portfolio-management
"""""";Natural Language Processing;https://github.com/Nilanshrajput/Intent_classification
"""""";General;https://github.com/ericjang/gumbel-softmax
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Walter-B/bert-20-classes
"""""";Reinforcement Learning;https://github.com/cjohnchen/sai
"""""";General;https://github.com/yumatsuoka/check_cosine_annealing_lr
"""""";General;https://github.com/Izecson/saml-nmt
"""""";Computer Vision;https://github.com/arahusky/Tensorflow-Segmentation
"""""";Computer Vision;https://github.com/leongatys/DeepTextures
"""""";Computer Vision;https://github.com/sanjsvk/CycleGANs
"""""";General;https://github.com/megvii-model/DetNAS
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/algharak/BERTenhance
"""""";Natural Language Processing;https://github.com/ps2811/Bert-Multi-Label-Text-Classification-GitVersion
"""""";Computer Vision;https://github.com/WongKinYiu/CrossStagePartialNetworks
"""""";Computer Vision;https://github.com/Lxrd-AJ/Advanced_ML
"""""";General;https://github.com/skbadhsm/darknet2
"""""";Computer Vision;https://github.com/leemathew1998/RG
"""""";Natural Language Processing;https://github.com/kssteven418/I-BERT
"""""";Reinforcement Learning;https://github.com/natsumeS/analysis
"""""";Computer Vision;https://github.com/ivboh/3d_image_segmentation
"""""";Natural Language Processing;https://github.com/singhmanish979/Text-Anlaytics
"""The masks we generate are of two kinds and they both used OpenCV to draw their shapes. The first method contained in the script *utils/Mask_generator.py* creates masks composed of ellipses  lines and circle of random size and position. The second method contained in the script *utils/Mask_generator_circle.py* creates circular masks with center and radius that can vary according to one's preference. For simplicity  in the analysis done so far  the circular masks were always centered at the center of the images.   """;Computer Vision;https://github.com/GabrieleMonte/CoMBInE
"""""";General;https://github.com/Andy-zhujunwen/pytorch-CBAM-Segmentation-experiment-
"""""";General;https://github.com/Paperspace/hyperopt-keras-sample
"""""";General;https://github.com/cf020031308/mad-learning
"""This commited code changes the user's description to an icon representing the current moon phase. I'd like to expand on this to update the username to the name of the phase of the mooon. As with the last script  to use this  skip the ""Glitch your avatar"" section - and remind me to redo this README.md to account for all the different scripts soon!   """;General;https://github.com/philcryer/randota
"""""";General;https://github.com/minoring/mobilenet
"""""";General;https://github.com/Rishit-dagli/Greenathon-Plant-AI
"""""";Natural Language Processing;https://github.com/Kosuke-Szk/BERT-NER-ja
"""""";Sequential;https://github.com/suzhiba/SimpleRecurrentUnits-SRU-
"""""";Natural Language Processing;https://github.com/PaddlePaddle/Knover
"""""";Computer Vision;https://github.com/amritasaha1812/pytorch-faster-rcnn
"""| TaskID | Description                                                                          | Examples                                                                | | ------- | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------| | 1       | Detection of an aspect in a review.                                                  | Is sleep quality mentioned in this review?                             | | 2       | Prediction of the customer general satisfaction.                                     | Is the client satisfied by this hotel?                                   | | 3       | Prediction of the global trend of an aspect in a given review.                       | Is the client satisfied with the cleanliness of the hotel?               | | 4       | Prediction of whether the rating of a given aspect is above or under a given value.  | Is the rating of location under 4?                                     | | 5       | Prediction of the exact rating of an aspect in a review.                             | What is the rating of the aspect Value in this review?                 | | 6       | Prediction of the list of all the positive/negative aspects mentioned in the review. | Can you give me a list of all the positive aspects in this review?     | | 7.0     | Comparison between aspects.                                                          | Is the sleep quality better than the service in this hotel?            | | 7.1     | Comparison between aspects.                                                          | Which one of these two aspects  service  location has the best rating? | | 8       | Prediction of the strengths and weaknesses in a review.                              | What is the best aspect rated in this comment?                         |   """;General;https://github.com/qgrail/ReviewQA
"""The goal of Detectron is to provide a high-quality  high-performance codebase for object detection *research*. It is designed to be flexible in order to support rapid implementation and evaluation of novel research. Detectron includes implementations of the following object detection algorithms:  - [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017* - [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017* - [Faster R-CNN](https://arxiv.org/abs/1506.01497) - [RPN](https://arxiv.org/abs/1506.01497) - [Fast R-CNN](https://arxiv.org/abs/1504.08083) - [R-FCN](https://arxiv.org/abs/1605.06409)  using the following backbone network architectures:  - [ResNeXt{50 101 152}](https://arxiv.org/abs/1611.05431) - [ResNet{50 101 152}](https://arxiv.org/abs/1512.03385) - [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt) - [VGG16](https://arxiv.org/abs/1409.1556)  Additional backbone architectures may be easily implemented. For more details about these models  please see [References](#references) below.   """;General;https://github.com/jiajunhua/facebookresearch-Detectron
"""""";Computer Vision;https://github.com/MiuGod0126/Mlp-Mixer-Paddle
"""""";General;https://github.com/hisiter97/darknet
"""""";General;https://github.com/lucidrains/x-transformers
"""""";Reinforcement Learning;https://github.com/ninja18/AtariDQN
"""""";Natural Language Processing;https://github.com/ymcui/Chinese-ELECTRA
"""""";Computer Vision;https://github.com/wangyuan249/Mymmt767
"""""";Computer Vision;https://github.com/IBM/BigLittleNet
"""""";General;https://github.com/tonystevenj/vae-celeba-pytorch-lightning
"""""";Computer Vision;https://github.com/UnofficialJuliaMirror/DeepMark-deepmark
"""FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search(https://arxiv.org/abs/1812.03443)  The master branch works with (https://github.com/open-mmlab/mmdetection)edb03937964b583a59dd1bddf76eaba82df9e8c0  - **test_block_time**  python  tools/test_time.py configs/search_config/bottlenetck_kernel.py   - **fbnet_search**  ./search.sh configs/model_config/retinanet_fpn.py configs/search_config/bottlenetck_kernel.py speed/bottlenetck_kernel.py_speed_gpu.txt  - **fbnet_train** (according to search_result)  ./train.sh configs/model_config/retinanet_fpn.py configs/search_config/bottlenetck_kernel.py theta/bottlenetck_kernel_retinanet_fpn/base.txt --work_dir ./your_path_tosave  - **fbnet_test** (according to train_result)  ./test.sh configs/model_config/retinanet_fpn.py configs/search_config/bottlenetck_kernel.py theta/bottlenetck_kernel_retinanet_fpn/base.txt your_path_tosave/lastest.pth --show  """;Computer Vision;https://github.com/anorthman/custom
"""""";Computer Vision;https://github.com/ZFTurbo/segmentation_models_3D
"""""";General;https://github.com/taki0112/Group_Normalization-Tensorflow
"""""";Computer Vision;https://github.com/dorlivne/simple_net_pruning
"""""";Reinforcement Learning;https://github.com/uber-research/go-explore
"""""";Natural Language Processing;https://github.com/karenacorn99/explore-bert
"""""";Computer Vision;https://github.com/Jwrede/neural_style_transfer
"""""";General;https://github.com/scakc/QAwiki
"""""";Computer Vision;https://github.com/CosmiQ/yolt
"""""";General;https://github.com/saikrishnadas/ResNet-Pytorch
"""""";General;https://github.com/chameleonTK/continual-learning-for-HAR
"""""";General;https://github.com/OlafenwaMoses/Traffic-Net
"""""";Computer Vision;https://github.com/henriquemeloo/cat-dog-image-classifier
"""YOLOX is an anchor-free version of YOLO  with a simpler design but better performance! It aims to bridge the gap between research and industrial communities. For more details  please refer to our [report on Arxiv](https://arxiv.org/abs/2107.08430).  This repo is an implementation of PyTorch version YOLOX  there is also a [MegEngine implementation](https://github.com/MegEngine/YOLOX).  <img src=""assets/git_fig.png"" width=""1000"" >   """;Computer Vision;https://github.com/xiyie/yolox
"""""";General;https://github.com/lucidrains/DALLE-pytorch
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/zheng1weiyu/Insightface_for_FaceRecog
"""""";General;https://github.com/rishavbhurtel/sentiment-classifier
"""""";Natural Language Processing;https://github.com/weiylu/NLP
"""""";Natural Language Processing;https://github.com/fshdnc/enfi-XLM
"""This is an official pytorch implementation of [*Deep High-Resolution Representation Learning for Human Pose Estimation*](https://arxiv.org/abs/1902.09212).  In this work  we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods **recover high-resolution representations from low-resolution representations** produced by a high-to-low resolution network. Instead  our proposed network **maintains high-resolution representations** through the whole process. We start from a high-resolution subnetwork as the first stage  gradually add high-to-low resolution subnetworks one by one to form more stages  and connect the mutli-resolution subnetworks **in parallel**. We conduct **repeated multi-scale fusions** such that each of the high-to-low resolution representations receives information from other parallel representations over and over  leading to rich high-resolution representations. As a result  the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. </br>  ![Illustrating the architecture of the proposed HRNet](/figures/hrnet.png)  """;Computer Vision;https://github.com/anshky/HR-NET
"""""";General;https://github.com/scz233/darts
"""This repository is a PyTorch implementation for semantic segmentation / scene parsing. The code is easy to use for training and testing on various datasets. The codebase mainly uses ResNet50/101/152 as backbone and can be easily adapted to other basic classification structures. Implemented networks including [PSPNet](https://hszhao.github.io/projects/pspnet) and [PSANet](https://hszhao.github.io/projects/psanet)  which ranked 1st places in [ImageNet Scene Parsing Challenge 2016 @ECCV16](http://image-net.org/challenges/LSVRC/2016/results)  [LSUN Semantic Segmentation Challenge 2017 @CVPR17](https://blog.mapillary.com/product/2017/06/13/lsun-challenge.html) and [WAD Drivable Area Segmentation Challenge 2018 @CVPR18](https://bdd-data.berkeley.edu/wad-2018.html). Sample experimented datasets are [ADE20K](http://sceneparsing.csail.mit.edu)  [PASCAL VOC 2012](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6) and [Cityscapes](https://www.cityscapes-dataset.com).  <img src=""./figure/pspnet.png"" width=""900""/>   """;Computer Vision;https://github.com/hszhao/semseg
"""This model is a semantic image segmentation model  which assigns label to each pixel of an image to partition different objects into segments. The whole model is composed of two parts  namely backbone part and classifier part. The backbone part is resnet101 which has been pre-trained  and the classifier part (DeepLabV3+ head  implemented by https://github.com/jfzhang95/pytorch-deeplab-xception using PyTorch) is fine-tuned based on this specific task.    """;General;https://github.com/sdyy6211/plant-segmentation
"""""";General;https://github.com/uzielroy/CUT
"""""";Graphs;https://github.com/anish-lu-yihe/SVRT-by-GAT
"""""";Computer Vision;https://github.com/amaurylekens/SDC_Segnet
"""""";Computer Vision;https://github.com/KaiyangZhou/mixstyle-release
"""""";Computer Vision;https://github.com/wilson1yan/VideoGPT
"""""";Computer Vision;https://github.com/deepakHonakeri05/darknet
""" Synapses play an important role in biological neural networks.  They're joint points of neurons where learning and memory happened. The picture below demonstrates that two neurons (red) connected through a branch chain of synapses which may  link to other neurons.   <p align='center'> <img src=""./picture/synapse.jpg"" alt=""synapse"" width=""80%"" /> </p>  Inspired by the synapse research of neuroscience  we construct a simple model that can describe some key properties of a synapse.   <p align='center'> <img src=""./picture/synapse-unit.png"" alt=""synpase"" width=""70%"" />  </p>  A Synaptic Neural Network (SynaNN) contains non-linear synapse networks that connect to neurons. A synapse consists of an input from the excitatory-channel  an input from the inhibitory-channel  and an output channel which sends a value to other synapses or neurons. The synapse function is  <p align='center'> <img src=""https://latex.codecogs.com/svg.latex?S(x y;\alpha \beta)=\alpha%20x(1-\beta%20y)"" </p>  where x∈(0 1) is the open probability of all excitatory channels and α >0 is the parameter of the excitatory channels; y∈(0 1) is the open probability of all inhibitory channels and β∈(0 1) is the parameter of the inhibitory channels. The surface of the synapse function is    <p align='center'> <img src=""./picture/synpase.png"" alt=""synpase"" width=""50%"" /> </p>  By combining deep learning  we expect to build ultra large scale neural networks to solve real-world AI problems. At the same time  we want to create an explainable neural network model to better understand what an AI model doing instead of a black box solution.  <p align='center'> <img src=""./picture/E425.tmp.png"" alt=""synpase"" width=""60%"" /> </p>  A synapse graph is a connection of synapses. In particular  a synapse tensor is fully connected synapses from input neurons to output neurons with some hidden layers. Synapse learning can work with gradient descent and backpropagation algorithms. SynaNN can be applied to construct MLP  CNN  and RNN models.  Assume that the total number of input of the synapse graph equals the total number of outputs  the fully-connected synapse graph is defined as   <p align='center'> <img src=""https://latex.codecogs.com/svg.latex?y_{i}(\textbf{x};%20\pmb\beta_i)%20=%20\alpha_i%20x_{i}{\prod_{j=1}^{n}(1-\beta_{ij}x_{j})} \%20for\%20all\%20i%20\in%20[1 n]""/> </p>  where   <p align='center'> <img src=""https://latex.codecogs.com/svg.latex?\textbf{x}=(x_1 \cdots x_n) \textbf{y}=(y_1 \cdots y_n) x_i y_i\in(0 1) \alpha_i \geq 1 \beta_{ij}\in(0 1))""/> </p>  Transformed to tensor/matrix representation  we have the synapse log formula    <p align='center'> <img src=""https://latex.codecogs.com/svg.latex?log(\textbf{y})=log(\textbf{x})+{\textbf{1}_{|x|}}*log(\textbf{1}_{|\beta|}-diag(\textbf{x})*\pmb{\beta}^T)""/> </p>  We are going to implement this formula for fully-connected synapse network with Tensorflow and PyTorch in the examples.  Moreover  we can design synapse graph like circuit below for some special applications.   <p align='center'> <img src=""./picture/synapse-flip.png"" alt=""synapse-flip"" width=""50%"" /> </p>   """;General;https://github.com/Neatware/SynaNN
"""""";General;https://github.com/mapillary/inplace_abn
"""""";Computer Vision;https://github.com/artemmavrin/focal-loss
"""""";Computer Vision;https://github.com/ElectronicBabylonianLiterature/cuneiform-sign-detection
"""""";General;https://github.com/SHAHBAZSIDDIQI/DataScience
"""The current code trains an RNN ([Gated Recurrent Units](https://arxiv.org/abs/1406.1078)) to predict  at each timestep (i.e. visit)  the diagnosis codes occurring in the next visit. This is denoted as *Sequential Diagnoses Prediction* in the paper.  In the future  we will relases another version for making a single prediction for the entire visit sequence. (e.g. Predict the onset of heart failure given the visit record)  Note that the current code uses [Multi-level Clinical Classification Software for ICD-9-CM](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp) as the domain knowledge. We will release the one that uses ICD9 Diagnosis Hierarchy in the future. 	  """;General;https://github.com/mp2893/gram
"""""";Computer Vision;https://github.com/Jastot/Rodinka_Neural_Network
"""""";Computer Vision;https://github.com/JsFlo/Generative-Adversarial-Nets
"""""";General;https://github.com/Arcady1/Doodle-Recognition-Web
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/cospplay/bert-master
"""""";Computer Vision;https://github.com/purbayankar/Hyperspectral-Vision-Transformer
"""""";Computer Vision;https://github.com/tbullmann/imagetranslation-tensorflow
"""""";Computer Vision;https://github.com/amirfaraji/LowDoseCTPytorch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/ricardordb/bert
"""""";Reinforcement Learning;https://github.com/hemilpanchiwala/Dueling_Network_Architectures
"""""";Sequential;https://github.com/francescodisalvo05/66DaysOfData
"""""";General;https://github.com/jackyjsy/ACWGAN
"""[![Watch the video](figures/video_figure.png)](https://www.youtube.com/watch?v=a_OeT8MXzWI&feature=youtu.be)   """;General;https://github.com/seulkiyeom/once-for-all
"""""";Natural Language Processing;https://github.com/magic-lantern/nlp-transfer-learning
"""""";General;https://github.com/ayushmankumar7/SISR-Tensorflow-2.x
"""""";Natural Language Processing;https://github.com/pomonam/AttentionCluster
"""""";Computer Vision;https://github.com/chrischoy/open-ucn
"""""";Computer Vision;https://github.com/xiaogangLi/tensorflow-Darknet53-YOLOv3
"""""";Computer Vision;https://github.com/jfcrenshaw/pzflow
"""""";General;https://github.com/yashkant/PNAS-Binarized-Neural-Networks
"""""";Computer Vision;https://github.com/EstherBear/implementation-of-pruning-filters
"""""";Computer Vision;https://github.com/Ashish-Sankritya/darknet_CNN
"""""";General;https://github.com/threelittlemonkeys/transformer-pytorch
""" ![sn](./assests/sn.png)     """;General;https://github.com/taki0112/Spectral_Normalization-Tensorflow
"""""";Computer Vision;https://github.com/SahinTiryaki/Brain-tumor-segmentation-Vgg19UNet
"""""";Reinforcement Learning;https://github.com/ssainz/reinforcement_learning_algorithms
"""""";Computer Vision;https://github.com/DableUTeeF/keras-efficientnet
"""""";Computer Vision;https://github.com/nbl97/det
"""""";Computer Vision;https://github.com/malin9402/2020-0221
"""The pickled data is a dictionary with 4 key/value pairs:  - `'features'` is a 4D array containing raw pixel data of the traffic sign images  (num examples  width  height  channels). - `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id. - `'sizes'` is a list containing tuples  (width  height) representing the original width and height the image. - `'coords'` is a list containing tuples  (x1  y1  x2  y2) representing coordinates of a bounding box around the sign in the image.  **First  we will use `numpy` provide the number of images in each subset  in addition to the image size  and the number of unique classes.** Number of training examples:  34799 Number of testing examples:  12630 Number of validation examples:  4410 Image data shape = (32  32  3) Number of classes = 43  **Then  we used `matplotlib` plot sample images from each subset.**   <figure>  <img src=""./traffic-signs-data/Screenshots/Train.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>   <figure>  <img src=""./traffic-signs-data/Screenshots/Test.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  <figure>  <img src=""./traffic-signs-data/Screenshots/Valid.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>   **And finally  we will use `numpy` to plot a histogram of the count of images in each unique class.**   <figure>  <img src=""./traffic-signs-data/Screenshots/TrainHist.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  <figure>  <img src=""./traffic-signs-data/Screenshots/TestHist.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  <figure>  <img src=""./traffic-signs-data/Screenshots/ValidHist.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  ---   """;General;https://github.com/mohamedameen93/German-Traffic-Sign-Classification-Using-TensorFlow
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/rickyHong/Google-BERT-repl
"""""";Natural Language Processing;https://github.com/IrishCoffee/cudnnMultiHeadAttention
"""""";Computer Vision;https://github.com/softbankrobotics-research/darknet_ros
"""""";General;https://github.com/Sobeit-Tim/NuguEyeTest
"""This edition of yolov4 frame has been smoothly transferred from tf1+ and keras version. With tf2+ used extensively  many older functions have been optimized even won't be used any more. Keras has been embedded into tensorflow and won't be supported by official  except fixing bugs. For more readable  I've made a lot of changes while transferring  such as  layer aggregation  image augment and some other changes from tf1 to tf2.       【Now  IT ONLY SUPPORTS TF2+】     """;Computer Vision;https://github.com/robbebluecp/tf2-yolov4
"""""";Sequential;https://github.com/JonathanRaiman/tensorflow_qrnn
"""""";Computer Vision;https://github.com/GXU-GMU-MICCAI/PGAN-Paddle
"""Tensorflow's sequential model is a very intuitive way to start learning about Deep Neural Networks. However it is quite hard to dive into more complex networks without learning more about Keras.  Well it won't be hard anymore with Fast-layers! Define your Sequences and start building complex layers in a sequential fashion.  I created fast-layers for beginners who wants to build more advanced networks and for experimented users who needs to quickly build and test complex module architectures.   """;Computer Vision;https://github.com/AlexandreMahdhaoui/fast_layers
"""""";General;https://github.com/YeongHyeon/Shake-Shake
"""This repository contains a TensorFlow-based implementation of **[4PP-EUSR (""Deep learning-based image super-resolution considering quantitative and perceptual quality"")](http://arxiv.org/abs/1809.04789)**  which considers both the quantitative (e.g.  PSNR) and perceptual quality (e.g.  NIQE) of the upscaled images. Our method **won the 2nd place and got the highest human opinion score for Region 2** in the [2018 PIRM Challenge on Perceptual Image Super-resolution at ECCV 2018](https://arxiv.org/abs/1809.07517).  ![BSD100 - 37073](figures/bsd100_37073.png) ※ The perceptual index is calculated by ""0.5 * ((10 - [Ma](https://sites.google.com/site/chaoma99/sr-metric)) + [NIQE](https://doi.org/10.1109/LSP.2012.2227726))""  which is used in the [PIRM Challenge](https://www.pirm2018.org/PIRM-SR.html). Lower is better.  Followings are the performance comparison evaluated on the [BSD100](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/) dataset.  ![BSD100 PSNR vs. NIQE](figures/bsd100_psnr_niqe.png)  Method | PSNR (dB) (↓) | Perceptual Index ------------ | :---: | :---: [EDSR](https://github.com/thstkdgus35/EDSR-PyTorch) | 27.796 | 5.326 [MDSR](https://github.com/thstkdgus35/EDSR-PyTorch) | 27.771 | 5.424 [EUSR](https://github.com/ghgh3269/EUSR-Tensorflow) | 27.674 | 5.307 [SRResNet-MSE](https://arxiv.org/abs/1609.04802) | 27.601 | 5.217 **4PP-EUSR (PIRM Challenge)** | **26.569** | **2.683** [SRResNet-VGG22](https://arxiv.org/abs/1609.04802) | 26.322 | 5.183 [SRGAN-MSE](https://arxiv.org/abs/1609.04802) | 25.981 | 2.802 Bicubic interpolation | 25.957 | 6.995 [SRGAN-VGG22](https://arxiv.org/abs/1609.04802) | 25.697 | 2.631 [SRGAN-VGG54](https://arxiv.org/abs/1609.04802) | 25.176 | 2.351 [CX](https://arxiv.org/abs/1803.04626) | 24.581 | 2.250  Please cite following papers when you use the code  pre-trained models  or results: - J.-H. Choi  J.-H. Kim  M. Cheon  J.-S. Lee: **Deep learning-based image super-resolution considering quantitative and perceptual quality**. Neurocomputing (In Press) [[Paper]](https://doi.org/10.1016/j.neucom.2019.06.103) [[arXiv]](http://arxiv.org/abs/1809.04789) ``` @article{choi2018deep    title={Deep learning-based image super-resolution considering quantitative and perceptual quality}    author={Choi  Jun-Ho and Kim  Jun-Hyuk and Cheon  Manri and Lee  Jong-Seok}    journal={Neurocomputing}    year={2019}    publisher={Elsevier} } ``` - J.-H. Kim  J.-S. Lee: **Deep residual network with enhanced upscaling module for super-resolution**. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops  pp. 913-921 (2018) [[Paper]](http://openaccess.thecvf.com/content_cvpr_2018_workshops/w13/html/Kim_Deep_Residual_Network_CVPR_2018_paper.html) ``` @inproceedings{kim2018deep    title={Deep residual network with enhanced upscaling module for super-resolution}    author={Kim  Jun-Hyuk and Lee  Jong-Seok}    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops}    year={2018} } ```   """;General;https://github.com/idearibosome/tf-perceptual-eusr
"""""";General;https://github.com/e0015274/darknet
"""The goal of the project is to build a chatbot which can improve people's motivation and confidence to speak English.  An AI-based sound chatbot was built based on Pytorch framework and trained on the Cornell Movie-Dialogs Corpus.      """;General;https://github.com/T9-LIN/MSc-Project
"""""";Computer Vision;https://github.com/awesomephant/vf-installation
"""""";Computer Vision;https://github.com/sbetageri/MaskRCNN
"""""";Computer Vision;https://github.com/ryota2425/maskrcnn-benchmark
"""""";Computer Vision;https://github.com/KostasTok/keras_DCGAN_transfer_learning
"""""";Computer Vision;https://github.com/STVIR/Grid-R-CNN
"""Districts and cities have periods of high demand for electricity  which raise electricity prices and the overall cost of the power distribution networks. Flattening  smoothening  and reducing the overall curve of electrical demand helps reduce operational and capital costs of electricity generation  transmission  and distribution networks. Demand response is the coordination of electricity consuming agents (i.e. buildings) in order to reshape the overall curve of electrical demand. CityLearn allows the easy implementation of reinforcement learning agents in a multi-agent setting to reshape their aggregated curve of electrical demand by controlling the storage of energy by every agent. Currently  CityLearn allows controlling the storage of domestic hot water (DHW)  and chilled water (for sensible cooling and dehumidification). CityLearn also includes models of air-to-water heat pumps  electric heaters  solar photovoltaic arrays  and the pre-computed energy loads of the buildings  which include space cooling  dehumidification  appliances  DHW  and solar generation.  """;Reinforcement Learning;https://github.com/intelligent-environments-lab/CityLearn
"""""";Computer Vision;https://github.com/aurelien-peden/Deep-Learning-paper-implementations
"""""";General;https://github.com/Moon-sung-woo/ParallelWaveGan_korean
"""""";General;https://github.com/jday96314/Kuzushiji
"""""";Reinforcement Learning;https://github.com/google-research/seed_rl
"""""";Graphs;https://github.com/fau-is/grm
"""""";Computer Vision;https://github.com/chiqunz/Unsupervised_Models
"""""";General;https://github.com/xxlya/COS598D_Assignment1
"""""";General;https://github.com/Sharpiless/Versailles-text-generation-with-paddlepaddle
"""""";General;https://github.com/bplank/teaching-dl4nlp
"""""";Computer Vision;https://github.com/Byte7/GANs
"""""";General;https://github.com/thomaspool97/StyleGAN2
"""""";Natural Language Processing;https://github.com/nrc-cnrc/sockeye-multisource
"""""";Computer Vision;https://github.com/JasonLiTW/simple-railway-captcha-solver
"""""";Computer Vision;https://github.com/shirans/cv_course_project
"""""";Sequential;https://github.com/clemkoa/ntm
"""This repository holds the code framework used in the paper Reg R-CNN: Lesion Detection and Grading under Noisy Labels [1]. The framework is a fork of MIC's [medicaldetectiontoolkit](https://github.com/MIC-DKFZ/medicaldetectiontoolkit) with added regression capabilities.  As below figure shows  the regression capability allows for the preservation of ordinal relations in the training signal as opposed to a standard categorical classification loss like the cross entropy loss (see publication for details). <p align=""center""><img src=""assets/teaser.png""  width=50%></p><br> Network Reg R-CNN is a version of Mask R-CNN [2] but with a regressor in place of the object-class head (see figure below). In this scenario  the first stage makes foreground (fg) vs background (bg) detections  then the regression head determines the class on an ordinal scale. Consequently  prediction confidence scores are taken from the first stage as opposed to the head in the original Mask R-CNN. <p align=""center""><img src=""assets/regrcnn.png""  width=50%></p><br>  In the configs file of a data set in the framework  you may set attribute self.prediction_tasks = [""task""] to a value ""task"" from [""class""  ""regression_bin""  ""regression""]. ""class"" produces the same behavior as the original framework  i.e.  standard object-detection behavior. ""regression"" on the other hand  swaps the class head of network Mask R-CNN [2] for a regression head. Consequently  objects are identified as fg/bg and then the class is decided by the regressor. For the sake of comparability  ""regression_bin"" produces a similar behavior but with a classification head. Both methods should be evaluated with the (implemented) Average Viewpoint Precision instead of only Average Precision.  Below you will found a description of the general framework operations and handling. Basic framework functionality and description are for the most part identical to the original [medicaldetectiontoolkit](https://github.com/MIC-DKFZ/medicaldetectiontoolkit).  <br/> [1] Ramien  Gregor et al.  <a href=""https://arxiv.org/abs/1907.12915"">""Reg R-CNN: Lesion Detection and Grading under Noisy Labels""</a>. In: UNSURE Workshop at MICCAI  2019.<br> [2] He  Kaiming  et al.  <a href=""https://arxiv.org/abs/1703.06870"">""Mask R-CNN""</a> ICCV  2017<br> <br>   """;Computer Vision;https://github.com/MIC-DKFZ/RegRCNN
"""""";Computer Vision;https://github.com/wincelinux/ai-dn-FasterRCNN-caffe
"""""";Reinforcement Learning;https://github.com/GhadaSokar/Dynamic-Sparse-Training-for-Deep-Reinforcement-Learning
"""""";Reinforcement Learning;https://github.com/h-aboutalebi/SparseBaseline
"""""";Reinforcement Learning;https://github.com/shariqiqbal2810/maddpg-pytorch
"""""";General;https://github.com/DarshanDeshpande/tf-madgrad
"""""";General;https://github.com/tatp22/multidim-positional-encoding
"""<div align=""left"">   <img src=""https://insightface.ai/assets/img/custom/thumb_sdunet.png"" width=""600""/> </div>  In this module  we provide datasets and training/inference pipelines for face alignment.  Supported methods:  - [x] [SDUNets (BMVC'2018)](alignment/heatmap) - [x] [SimpleRegression](alignment/coordinate_reg)   [SDUNets](alignment/heatmap) is a heatmap based method which accepted on [BMVC](http://bmvc2018.org/contents/papers/0051.pdf).  [SimpleRegression](alignment/coordinate_reg) provides very lightweight facial landmark models with fast coordinate regression. The input of these models is loose cropped face image while the output is the direct landmark coordinates.    <div align=""left"">   <img src=""https://insightface.ai/assets/img/github/11513D05.jpg"" width=""640""/> </div>  In this module  we provide training data with annotation  network settings and loss designs for face detection training  evaluation and inference.  The supported methods are as follows:  - [x] [RetinaFace (CVPR'2020)](detection/retinaface) - [x] [SCRFD (Arxiv'2021)](detection/scrfd) - [x] [blazeface_paddle](detection/blazeface_paddle)  [RetinaFace](detection/retinaface) is a practical single-stage face detector which is accepted by [CVPR 2020](https://openaccess.thecvf.com/content_CVPR_2020/html/Deng_RetinaFace_Single-Shot_Multi-Level_Face_Localisation_in_the_Wild_CVPR_2020_paper.html). We provide training code  training dataset  pretrained models and evaluation scripts.   [SCRFD](detection/scrfd) is an efficient high accuracy face detection approach which is initialy described in [Arxiv](https://arxiv.org/abs/2105.04714). We provide an easy-to-use pipeline to train high efficiency face detectors with NAS supporting.    In this module  we provide training data  network settings and loss designs for deep face recognition.  The supported methods are as follows:  - [x] [ArcFace_mxnet (CVPR'2019)](recognition/arcface_mxnet) - [x] [ArcFace_torch (CVPR'2019)](recognition/arcface_torch) - [x] [SubCenter ArcFace (ECCV'2020)](recognition/subcenter_arcface) - [x] [PartialFC_mxnet (Arxiv'2020)](recognition/partial_fc) - [x] [PartialFC_torch (Arxiv'2020)](recognition/arcface_torch) - [x] [VPL (CVPR'2021)](recognition/vpl) - [x] [OneFlow_face](recognition/oneflow_face) - [x] [ArcFace_Paddle (CVPR'2019)](recognition/arcface_paddle)  Commonly used network backbones are included in most of the methods  such as IResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  etc..    [InsightFace](https://insightface.ai) is an open source 2D&3D deep face analysis toolbox  mainly based on PyTorch and MXNet.   Please check our [website](https://insightface.ai) for detail.  The master branch works with **PyTorch 1.6+** and/or **MXNet=1.6-1.8**  with **Python 3.x**.  InsightFace efficiently implements a rich variety of state of the art algorithms of face recognition  face detection and face alignment  which optimized for both training and deployment.   """;General;https://github.com/deepinsight/insightface
"""""";Sequential;https://github.com/oki5656/Estimating_Distance
"""**GCNet** is initially described in [arxiv](https://arxiv.org/abs/1904.11492). Via absorbing advantages of Non-Local Networks (NLNet) and Squeeze-Excitation Networks (SENet)   GCNet provides a simple  fast and effective approach for global context modeling  which generally outperforms both NLNet and SENet on major benchmarks for various recognition tasks.   """;Computer Vision;https://github.com/zhusiling/GCNet
"""""";Reinforcement Learning;https://github.com/yoavalon/QuadcopterReinforcementLearning
"""""";Sequential;https://github.com/andabi/voice-vector
"""""";Computer Vision;https://github.com/moskomule/senet.pytorch
"""""";Computer Vision;https://github.com/machinelearning-goettingen/EfficientNet-ModelScaling
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/CV-deeplearning/caffe
"""""";Natural Language Processing;https://github.com/XapaJIaMnu/transformative
"""""";General;https://github.com/jiajunhua/CycleGAN
"""Machine translation is a natural language processing task that aims to translate natural languages using computers automatically. Recent several years have witnessed the rapid development of end-to-end neural machine translation  which has become the new mainstream method in practical MT systems.  THUMT is an open-source toolkit for neural machine translation developed by [the Natural Language Processing Group at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/site2/index.php?lang=en). The website of THUMT is: [http://thumt.thunlp.org/](http://thumt.thunlp.org/).   """;General;https://github.com/wangmz15/Chinese-Error-Correction-with-THUMT
"""""";Computer Vision;https://github.com/MNaplesDevelopment/DCGAN-PyTorch
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/ModelBunker/PointNet-TensorFlow
"""""";General;https://github.com/yw0nam/DenseNet
"""""";General;https://github.com/junyanz/CycleGAN
"""""";General;https://github.com/Socialbird-AILab/BERT-Classification-Tutorial
"""""";Computer Vision;https://github.com/mrzhu-cool/pix2pix-pytorch
"""""";General;https://github.com/changhuixu/LSTM-sentiment-analysis
"""""";Computer Vision;https://github.com/mateuszbuda/brain-segmentation-pytorch
"""Here is a brief summary of the three MobileNet versions. Only MobileNetV3 is implemented.  """;Computer Vision;https://github.com/akrapukhin/MobileNetV3
"""""";Natural Language Processing;https://github.com/starsuzi/pretrained_calibration
"""BNM v1: we prove in the paper that Batch Nuclear-norm Maximization (BNM) can ensure the prediction discriminability and diversity  which is an effective method under label insufficient situations.  BNM v2: we further devise Batch Nuclear-norm Minimization (BNMin) and Fast BNM (FBNM) for multiple domain adaptation scenarios.   """;General;https://github.com/cuishuhao/BNM
"""""";Reinforcement Learning;https://github.com/GoingMyWay/dopamine_reward_decomposition
"""*Artistic Styling of images using CNNs  VGG*  *****************************************  """;Computer Vision;https://github.com/sarthakbaiswar/Artistic-Styling
"""""";Sequential;https://github.com/ahmetumutdurmus/awd-lstm
"""""";Computer Vision;https://github.com/woctezuma/steam-stylegan2
"""""";Reinforcement Learning;https://github.com/plkmo/AlphaZero_Connect4
"""To practice what you've learned  a good idea would be to spend an hour on 3 of the following (3-hours total  you could through them all if you want) and then write a blog post about what you've learned.  * For an overview of the different problems within NLP and how to solve them read through:    * [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)   * [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e) * Go through [MIT's Recurrent Neural Networks lecture](https://youtu.be/SEnXr6v2ifU). This will be one of the greatest additions to what's happening behind the RNN model's you've been building. * Read through the [word embeddings page on the TensorFlow website](https://www.tensorflow.org/tutorials/text/word_embeddings). Embeddings are such a large part of NLP. We've covered them throughout this notebook but extra practice would be well worth it. A good exercise would be to write out all the code in the guide in a new notebook.  * For more on RNN's in TensorFlow  read and reproduce [the TensorFlow RNN guide](https://www.tensorflow.org/guide/keras/rnn). We've covered many of the concepts in this guide  but it's worth writing the code again for yourself. * Text data doesn't always come in a nice package like the data we've downloaded. So if you're after more on preparing different text sources for being with your TensorFlow deep learning models  it's worth checking out the following:   * [TensorFlow text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text).   * [Reading text files with Python](https://realpython.com/read-write-files-python/) by Real Python. * This notebook has focused on writing NLP code. For a mathematically rich overview of how NLP with Deep Learning happens  read [Standford's Natural Language Processing with Deep Learning lecture notes Part 1](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).     * For an even deeper dive  you could even do the whole [CS224n](http://web.stanford.edu/class/cs224n/) (Natural Language Processing with Deep Learning) course.  * Great blog posts to read:   * Andrei Karpathy's [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) dives into generating Shakespeare text with RNNs.   * [Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) by Mauro Di Pietro. An overview of different techniques for turning text into numbers and then classifying it.   * [What are word embeddings?](https://machinelearningmastery.com/what-are-word-embeddings/) by Machine Learning Mastery. * Other topics worth looking into:   * [Attention mechanisms](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/). These are a foundational component of the transformer architecture and also often add improvments to deep NLP models.   * [Transformer architectures](http://jalammar.github.io/illustrated-transformer/). This model architecture has recently taken the NLP world by storm  achieving state of the art on many benchmarks. However  it does take a little more processing to get off the ground  the [HuggingFace Models (formerly HuggingFace Transformers) library](https://huggingface.co/models/) is probably your best quick start.  ---   1. Rebuild  compile and train `model_1`  `model_2` and `model_5` using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) instead of the Functional API. 2. Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data? 3. Try fine-tuning the TF Hub Universal Sentence Encoder model by setting `training=True` when instantiating it as a Keras layer.  ``` #: We can use this encoding layer in place of our text_vectorizer and embedding layer sentence_encoder_layer = hub.KerasLayer(""https://tfhub.dev/google/universal-sentence-encoder/4""                                          input_shape=[]                                          dtype=tf.string                                          trainable=True) #: turn training on to fine-tune the TensorFlow Hub model ``` 4. Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the `sample_submission.csv` file from Kaggle (see the Files tab in Colab for what the `sample_submission.csv` file looks like). Once you've done this  [make a submission to the Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/data)  how did your model perform? 5. Combine the ensemble predictions using the majority vote (mode)  how does this perform compare to averaging the prediction probabilities of each model? 6. Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels.   """;General;https://github.com/joelweber97/Python3_TF_Certificate
"""""";Computer Vision;https://github.com/levanpon98/inception_v3
"""""";Computer Vision;https://github.com/rahul1728jha/Object_Detection_Yolov2
"""""";Audio;https://github.com/DevonFulcher/CryptoPricePredictor
"""该系统实现了基于深度框架的语音识别中的声学模型和语言模型建模，其中声学模型包括CNN-CTC、GRU-CTC、CNN-RNN-CTC，语言模型包含[transformer](https://jalammar.github.io/illustrated-transformer/)、[CBHG](https://github.com/crownpku/Somiao-Pinyin)，数据集包含stc、primewords、Aishell、thchs30四个数据集。  本系统更整体介绍：https://blog.csdn.net/chinatelecom08/article/details/82557715  本项目现已训练一个迷你的语音识别系统，将项目下载到本地上，下载[thchs数据集](http://www.openslr.org/resources/18/data_thchs30.tgz)并解压至data，运行`test.py`，不出意外能够进行识别，结果如下：       the  0 th example.     文本结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文汉字： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然     识别结果： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然  若自己建立模型则需要删除现有模型，重新配置参数训练，具体实现流程参考本页最后。   """;Natural Language Processing;https://github.com/yumoh/speech-keras
"""""";General;https://github.com/christophmeyer/longboard-pothole-detection
"""""";Computer Vision;https://github.com/itijyou/ademxapp
"""""";General;https://github.com/marcotcr/lime
"""""";Computer Vision;https://github.com/JojiJoseph/GAN
"""""";Computer Vision;https://github.com/cjpurackal/m2det-tf
"""""";Computer Vision;https://github.com/jiama843/tf-mnist
"""""";Reinforcement Learning;https://github.com/tobiasemrich/SchafkopfRL
"""""";Computer Vision;https://github.com/gurkirt/realtime-action-detection
"""""";General;https://github.com/gerardoglz/GANs
"""""";Computer Vision;https://github.com/aksub99/U-Net-Pytorch
"""""";General;https://github.com/mhaseeb123/DeepMSim
"""""";General;https://github.com/jss367/thiskangaroodoesnotexist
"""This repository contains the original modified residual network which modified the classical resnet ""Deep Residual Learning for Image Recognition"" (http://arxiv.org/abs/1512.03385). Original residual block was modified to improve model performance.    """;Computer Vision;https://github.com/xinkuansong/modified-resnet-acc-0.9638-10.7M-parameters
"""""";Computer Vision;https://github.com/Daonancai/unet_muti_class
"""""";Computer Vision;https://github.com/ANIME305/Anime-GAN-tensorflow
"""""";Computer Vision;https://github.com/wondonghyeon/protest-detection-violence-estimation
"""""";General;https://github.com/minhto2802/keras-shufflenet
"""DeBERTa (Decoding-enhanced BERT with disentangled attention) improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism  where each word is represented using two vectors that encode its content and position  respectively  and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second  an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks.   """;Natural Language Processing;https://github.com/microsoft/DeBERTa
"""""";Natural Language Processing;https://github.com/imcaspar/gpt2-ml
"""""";Computer Vision;https://github.com/facebookarchive/fb.resnet.torch
"""""";Reinforcement Learning;https://github.com/jadag/DDQN_mario
"""""";Computer Vision;https://github.com/yoshitomo-matsubara/torchdistill
"""""";Audio;https://github.com/keonlee9420/Comprehensive-Tacotron2
"""""";Computer Vision;https://github.com/kishorkuttan/traffic_net
"""An one-stage object detection model is implemented on paddle 2.1.0 and paddledetection 2.1.0   """;Computer Vision;https://github.com/FL77N/RetinaNet-Based-on-PPdet
"""""";General;https://github.com/dungxibo123/vae
"""""";Computer Vision;https://github.com/brain-research/self-attention-gan
"""""";Computer Vision;https://github.com/sebsquire/FlowersRecognition_keras
"""""";Computer Vision;https://github.com/RocketFlash/CAP_augmentation
"""""";Computer Vision;https://github.com/idealo/imagededup
"""""";Computer Vision;https://github.com/JIA-HONG-CHU/Swin-Transformer-add-EncNet-DaNet-DraNet-for-semantic-segmentation-on-Statelite-Dataset
"""""";Computer Vision;https://github.com/PistonY/ResidualAttentionNetwork
"""""";Natural Language Processing;https://github.com/nishnik/poincare_embeddings
"""""";General;https://github.com/anonDEHB/DEHB
"""""";Natural Language Processing;https://github.com/arkel23/PyTorch-Pretrained-ViT
"""""";General;https://github.com/snf/keras-fractalnet
"""""";General;https://github.com/RyanWu2233/Style_GAN2_FFHQ
"""""";General;https://github.com/ays-dev/keras-transformer
"""[_CrazyAra_](https://crazyara.org/) is an open-source neural network chess variant engine  initially developed in pure python by [Johannes Czech](https://github.com/QueensGambit)  [Moritz Willig](https://github.com/MoritzWillig) and Alena Beyer in 2018. It started as a semester project at the [TU Darmstadt](https://www.tu-darmstadt.de/index.en.jsp) with the goal to train a neural network to play the chess variant [crazyhouse](https://en.wikipedia.org/wiki/Crazyhouse) via supervised learning on human data. The project was part of the course [_""Deep Learning: Architectures & Methods""_](https://piazza.com/tu-darmstadt.de/summer2019/20001034iv/home) held by [Kristian Kersting](https://ml-research.github.io/people/kkersting/index.html)  [Johannes Fürnkranz](http://www.ke.tu-darmstadt.de/staff/juffi) et al. in summer 2018.  The development was continued and the engine ported to C++ by [Johannes Czech](https://github.com/QueensGambit). In the course of a master thesis supervised by [Karl Stelzner](https://ml-research.github.io/people/kstelzner/) and [Kristian Kersting](https://ml-research.github.io/people/kkersting/index.html)  the engine learned crazyhouse in a reinforcement learning setting and was trained on other chess variants including chess960  King of the Hill and Three-Check.  The project is mainly inspired by the techniques described in the [Alpha-(Go)-Zero papers](https://arxiv.org/abs/1712.01815) by [David Silver](https://arxiv.org/search/cs?searchtype=author&query=Silver%2C+D)  [Thomas Hubert](https://arxiv.org/search/cs?searchtype=author&query=Hubert%2C+T)  [Julian Schrittwieser](https://arxiv.org/search/cs?searchtype=author&query=Schrittwieser%2C+J)  [Ioannis Antonoglou](https://arxiv.org/search/cs?searchtype=author&query=Antonoglou%2C+I)  [Matthew Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+M)  [Arthur Guez](https://arxiv.org/search/cs?searchtype=author&query=Guez%2C+A)  [Marc Lanctot](https://arxiv.org/search/cs?searchtype=author&query=Lanctot%2C+M)  [Laurent Sifre](https://arxiv.org/search/cs?searchtype=author&query=Sifre%2C+L)  [Dharshan Kumaran](https://arxiv.org/search/cs?searchtype=author&query=Kumaran%2C+D)  [Thore Graepel](https://arxiv.org/search/cs?searchtype=author&query=Graepel%2C+T)  [Timothy Lillicrap](https://arxiv.org/search/cs?searchtype=author&query=Lillicrap%2C+T)  [Karen Simonyan](https://arxiv.org/search/cs?searchtype=author&query=Simonyan%2C+K)  [Demis Hassabis](https://arxiv.org/search/cs?searchtype=author&query=Hassabis%2C+D).  The training scripts  preprocessing and neural network definition source files are written in python and located at [DeepCrazyhouse/src](https://github.com/QueensGambit/CrazyAra/tree/master/DeepCrazyhouse/src). There are two version of the search engine available: The initial version is written in python and located at [DeepCrazyhouse/src/domain/agent](https://github.com/QueensGambit/CrazyAra/tree/master/DeepCrazyhouse/src/domain/agent). The newer version is written in C++ and located at [engine/src](https://github.com/QueensGambit/CrazyAra/tree/master/engine/src).  _CrazyAra_ is an UCI chess engine and requires a GUI (e.g. [Cute Chess](https://github.com/cutechess/cutechess)  [XBoard](https://www.gnu.org/software/xboard/)  [WinBoard](http://hgm.nubati.net/)) for convinient usage.   """;Reinforcement Learning;https://github.com/QueensGambit/CrazyAra
"""Nowadays in computer vision  deep learning approach is performing superior result and even better than human. I decided to use deep learning approach to solve computer vision challenge in Grab AI For Sea. There is already some [published kernels in Kaggle](https://www.kaggle.com/jutrera/stanford-car-dataset-by-classes-folder/kernels) which I have referred to [their approach](https://www.kaggle.com/deepbear/pytorch-car-classifier-90-accuracy) as my starting point. I have made some changes on training scheme and network architecture. My approach of training scheme is better than baseline from the Kaggle kernel by 0.27% while performing another two tasks. Using state-of-the-art achitecture  performance is improved by 1.66%. I have also shown that not only we need to focus on performance  but also focus on size and computational power  I switched to lower resolution and state-of-the-art deep architecture for mobile  I managed to create a model that is efficient in terms of performance and computational power.    """;Computer Vision;https://github.com/kamwoh/Car-Model-Classification
"""""";Computer Vision;https://github.com/zjjszj/yolov3
"""""";Computer Vision;https://github.com/Mayurji/Image-Classification-PyTorch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/appcoreopc/berty
"""""";General;https://github.com/minhdua/PHONES
"""""";Reinforcement Learning;https://github.com/chainer/chainerrl
"""""";General;https://github.com/Adk2001tech/GAN-Image-Super-Resolution
"""""";General;https://github.com/viethungluu/OCRetina
"""""";Computer Vision;https://github.com/wangguanan/light-reid
"""""";General;https://github.com/sureshcrwr/Lower-Bound-on-Transmission-using-Non-Linear-Bounding-Function-in-Single-Image-Dehazing
"""""";General;https://github.com/thomasly/PaperTranslation
"""""";Computer Vision;https://github.com/AlphaJia/tf-faster-rcnn
"""""";General;https://github.com/leemathew1998/RG
"""""";Natural Language Processing;https://github.com/salesforce/awd-lstm-lm
"""""";General;https://github.com/Syarujianai/deeplab-commented
"""""";Computer Vision;https://github.com/alrojo/lasagne_residual_network
"""""";Computer Vision;https://github.com/zhuoyang125/CenterMask2
"""""";Natural Language Processing;https://github.com/francescodisalvo05/66DaysOfData
"""""";Natural Language Processing;https://github.com/lukemelas/PyTorch-Pretrained-ViT
"""""";Natural Language Processing;https://github.com/ptillet/torch-blocksparse
"""""";Natural Language Processing;https://github.com/sjyttkl/Transformer_learning
"""""";Graphs;https://github.com/marblet/gat-pytorch
"""""";Reinforcement Learning;https://github.com/deepmind/scalable_agent
"""""";General;https://github.com/saranda-2811/FixMatch
"""""";Computer Vision;https://github.com/hamidriasat/BASNet
"""""";General;https://github.com/gagan16/Cyclegan-tensorflow
"""""";Computer Vision;https://github.com/amdegroot/ssd.pytorch
"""""";General;https://github.com/youngwanLEE/vovnet-detectron2
"""自作関数’T’をtransformerで分類し   その根拠も示すプログラム   実装の大部分を http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks のプログラムを参考に実装した      Thanks to   Guillaume Klein and Yoon Kim and Yuntian Deng and Jean Senellart and Alexander M. Rush   and   https://arxiv.org/pdf/1706.03762.pdf    """;General;https://github.com/kotu931226/classifier_transformer_pytorch
"""""";Natural Language Processing;https://github.com/gsh199449/stickerchat
"""""";Audio;https://github.com/csiki/v2a
"""""";General;https://github.com/SeonghoBaek/FrameSequencePrediction
"""""";General;https://github.com/paniabhisek/maxout
"""""";Computer Vision;https://github.com/kentsommer/keras-inceptionV4
"""""";General;https://github.com/SongDark/generate_normal
"""""";General;https://github.com/LandyGuo/PaperGuide
"""""";General;https://github.com/jageshmaharjan/BERT_Service
"""""";Graphs;https://github.com/Monti03/VGAE
"""""";Computer Vision;https://github.com/woctezuma/steam-stylegan2-ada
"""""";General;https://github.com/chengyangfu/retinamask
"""""";Computer Vision;https://github.com/arminarj/FashionMNIST_VAE
"""""";General;https://github.com/santi-pdp/pase
"""""";General;https://github.com/SeonghoBaek/CycleGAN
"""""";Computer Vision;https://github.com/Hayashi-Yudai/SRGAN
"""""";Computer Vision;https://github.com/dstamoulis/single-path-nas
"""""";Computer Vision;https://github.com/cfdoge/w9ai
"""""";Graphs;https://github.com/Anieca/GCN
"""""";Computer Vision;https://github.com/meg965/pytorch-rl
"""""";Computer Vision;https://github.com/Sharpiless/pp-yolo-vehcile-detection-and-distance-estimation-for-self-driving
"""""";General;https://github.com/markson14/WheatDet
"""""";Natural Language Processing;https://github.com/griff4692/LMC
"""""";General;https://github.com/rgsachin/DMTN
"""Here is a brief summary of the three MobileNet versions. Only MobileNetV3 is implemented.  """;General;https://github.com/akrapukhin/MobileNetV3
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/coco60/bert-test
"""""";Computer Vision;https://github.com/lionelmessi6410/ntga
"""""";Reinforcement Learning;https://github.com/donamin/llc
"""""";Computer Vision;https://github.com/decentNick/Signals_big_lab_SPBSTU
"""""";Reinforcement Learning;https://github.com/rewindturtle/Pac-Man-Player
"""""";Computer Vision;https://github.com/km1414/GAN
"""""";Natural Language Processing;https://github.com/enod/arxiv-nlp-notes
"""""";General;https://github.com/adventure2165/Summarization_self-training_with_noisy_student_improves_imagenet_classification
"""""";Computer Vision;https://github.com/xunings/styleganime2
"""""";Computer Vision;https://github.com/kazu0914/ssd_keras_anotation
"""""";Computer Vision;https://github.com/computational-imaging/automatic-integration
"""""";Computer Vision;https://github.com/NaturalHistoryMuseum/semantic-segmentation
"""""";Computer Vision;https://github.com/Ceachi/Project-Self-Driving-Car-Advanced-Lane-Lines
"""This is the repo for the Tensorflow implementation of cellSTORM based on the conditional generative adversarial network (cGAN) implmented by pix2pix-tensoflow. In this case it learns a mapping from input images (i.e. degraded  noisy  compressed video-sequences of dSTORM blinking events) to output images (i.e. localization maps/center of possible blinking fluorophore)  The repository has also a scipt to export a given Graph trained on GPU to a cellphone. The cellSTORM localizer APP can be found in another repo.    """;Computer Vision;https://github.com/bionanoimaging/cellSTORM-Tensorflow
"""""";Computer Vision;https://github.com/xiaomi-automl/SCARLET-NAS
"""""";General;https://github.com/astorfi/sequence-to-sequence-from-scratch
"""""";Audio;https://github.com/juanalonso/DDSP-singing-experiments
"""""";Natural Language Processing;https://github.com/plkmo/Transformer-Eng2French
"""""";General;https://github.com/wondonghyeon/protest-detection-violence-estimation
"""""";Computer Vision;https://github.com/jrzech/reproduce-chexnet
"""""";Reinforcement Learning;https://github.com/flowersteam/curious
"""""";General;https://github.com/abods/generative_painting
"""""";General;https://github.com/b-etienne/Seq2seq-PyTorch
"""[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1hYHb0FTdKQCXZs3qCwVZnSuVGrZU2Z1w?usp=sharing)  It was in January of 2021 that **OpenAI** announced two new models: **DALL-E** and **CLIP**  both **multi-modality** models connecting **texts and images** in some way. In this article we are going to implement CLIP model from scratch in **PyTorch**. OpenAI has open-sourced some of the code relating to CLIP model but I found it intimidating and it was far from something short and simple. I also came across a good tutorial inspired by CLIP model on Keras code examples and I translated some parts of it into PyTorch to build this tutorial totally with our beloved PyTorch!  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1hYHb0FTdKQCXZs3qCwVZnSuVGrZU2Z1w?usp=sharing)   """;Computer Vision;https://github.com/moein-shariatnia/OpenAI-CLIP
"""The vanilla GAN (https://arxiv.org/abs/1406.2661) tries to find the Nash Equilibrium between Generator and Discriminator  and it minimizes the Jessen - Shannon Divergence at the optimal point. It is the generative model without the likelihood. However  there were some issues - GAN is very hard to train  and it is precarious. There were many proposed solutions to these problems  as mentioned earlier.  One of the breakthroughs was WGAN paper (https://arxiv.org/abs/1701.07875). Rather than finding the equilibrium between two neural networks  WGAN paper tries to minimize the 1-Wasserstein Distance(WD) between two networks. Intuitively  WD is the cost function of moving one distribution to the another. As the neural network is a powerful function approximator  WGAN finds the optimal transport from the sample to the real distribution.  However  the functions we derived from the WGAN need to meet the 1-Lipschitz condition. WGAN-GP came up with one solution to impose the gradient penalty(GP) as the gradient we obtained from the point between the real data and the samples deviates from 1. This approach works quite well.    """;General;https://github.com/wayne1123/mnist_wgan_gp
"""""";Sequential;https://github.com/2003pro/Graph2Tree
"""""";Computer Vision;https://github.com/syiin/ann_cancer_detection
"""SSD(Single Shot MultiBox Detector) is a state-of-art object detection algorithm  brought by Wei Liu and other wonderful guys  see [SSD: Single Shot MultiBox Detector @ arxiv](https://arxiv.org/abs/1512.02325)  recommended to read for better understanding.  Also  SSD currently performs good at PASCAL VOC Challenge  see [http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=3](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=3)   """;Computer Vision;https://github.com/nirajdpandey/Object-detection-and-localization-using-SSD
"""""";General;https://github.com/eric-erki/darknet
"""""";Computer Vision;https://github.com/Shellllllly/plateDetection
"""""";General;https://github.com/andrewbo29/mtm-meta-learning-sa
"""""";Computer Vision;https://github.com/Popoooo/darknet_test
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/theQuert/inlpfun
"""PyTorch Implementation of **Bottleneck Transformers for Visual Recognition**.   Link to paper: https://arxiv.org/abs/2101.11605  Structure of Self-Attention layer in paper:  ![self-attention layer](https://github.com/CandiceD17/Bottleneck-Transformers-for-Visual-Recognition/blob/master/asset/self-attention-layer.png)   """;Computer Vision;https://github.com/CandiceD17/Bottleneck-Transformers-for-Visual-Recognition
"""Deep Convolutional GAN is one of the most coolest and popular deep learning technique. It is a great improvement upon the [original GAN network](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) that was first introduced by Ian Goodfellow at NIPS 2014. (DCGANs are much more stable than Vanilla GANs) DCGAN uses the same framework of generator and discriminator. This is analogous to solving a two player minimax game: Ideally the goal of the discriminator is to be very sharp in distinguishing between the real and fake data  whereas  generator aims at faking data in such a way that it becomes nearly impossible for the discriminator to classify it as a fake. The below gif shows how quickly dcgan learns the distribution of celebrity images and generates real looking people. The gif is created for both  a fixed noise and variable noise:-  <p float=""left"">   <img src=""https://github.com/AKASHKADEL/dcgan-celeba/blob/master/results/variable_noise/animated.gif"" width=""400"" height=""400"" />   <img src=""https://github.com/AKASHKADEL/dcgan-celeba/blob/master/results/fixed_noise/animated.gif"" width=""400"" height=""400"" /> </p>   """;Computer Vision;https://github.com/AKASHKADEL/dcgan-celeba
"""""";General;https://github.com/VinayBN8997/CIFAR-10-object-detection
"""An unofficical low-resolution (32 x 32) implementation of BigBiGAN   Paper: https://arxiv.org/abs/1907.02544  """;Computer Vision;https://github.com/LEGO999/BigBiGAN-TensorFlow2.0
"""""";General;https://github.com/zuobinxiong/pix2pix-tensorflow
"""""";General;https://github.com/IvKosar/text2speech
"""""";Reinforcement Learning;https://github.com/Kaixhin/Rainbow
"""""";Computer Vision;https://github.com/TusharMhaske28/Thermal-Object
"""""";Reinforcement Learning;https://github.com/filippogiruzzi/semantic_segmentation
"""This project is based on our CVPR2019 paper. You can find the [arXiv](https://arxiv.org/abs/1811.07246) version here.  ``` @inproceedings{wu2019pointconv    title={Pointconv: Deep convolutional networks on 3d point clouds}    author={Wu  Wenxuan and Qi  Zhongang and Fuxin  Li}    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}    pages={9621--9630}    year={2019} } ```  Unlike images which are represented in regular dense grids  3D point clouds are irregular and unordered  hence applying convolution on them can be difficult. In this paper  we extend the dynamic filter to a new convolution operation  named PointConv. PointConv can be applied on point clouds to build deep convolutional networks. We treat convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight and density functions. With respect to a given point  the weight functions are learned with multi-layer perceptron networks and the density functions through kernel density estimation. A novel reformulation is proposed for efficiently computing the weight functions  which allowed us to dramatically scale up the network and significantly improve its performance. The learned convolution kernel can be used to compute translation-invariant and permutation-invariant convolution on any point set in the 3D space. Besides  PointConv can also be used as deconvolution operators to propagate features from a subsampled point cloud back to its original resolution. Experiments on ModelNet40  ShapeNet  and ScanNet show that deep convolutional neural networks built on PointConv are able to achieve state-of-the-art on challenging semantic segmentation benchmarks on 3D point clouds. Besides  our experiments converting CIFAR-10 into a point cloud showed that networks built on PointConv can match the performance of convolutional networks in 2D images of a similar structure.   """;Computer Vision;https://github.com/Young98CN/pointconv_pytorch
"""""";General;https://github.com/tensorflow/addons
"""""";General;https://github.com/shivam13juna/Sequence_Prediction_LSTM_CHAR
"""""";Computer Vision;https://github.com/zymk9/Yet-Another-Anime-Segmenter
"""""";Computer Vision;https://github.com/toandaominh1997/EfficientDet.Pytorch
"""""";Computer Vision;https://github.com/vayuvegula/kerasModels
"""""";General;https://github.com/idobronstein/vision_networks
"""""";Computer Vision;https://github.com/BoyuanJackChen/NeRF-Implementation
"""""";Computer Vision;https://github.com/maudzung/YOLO3D-YOLOv4-PyTorch
"""""";General;https://github.com/minoring/batch-norm-visualize
"""""";Computer Vision;https://github.com/yzgrfsy/tf-fastrcnn-crop
"""""";General;https://github.com/alekseynp/stylegan2-pytorch
"""""";General;https://github.com/JeonMinkyu/MAML_Pytorch
"""""";Computer Vision;https://github.com/reyvaz/pneumothorax_detection
"""""";General;https://github.com/sjyttkl/Transformer_learning
"""""";General;https://github.com/bethgelab/foolbox
"""""";Computer Vision;https://github.com/mathildor/DeepLab-v3
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/h4ste/oscar
"""""";Computer Vision;https://github.com/karlgodin/DP10-hockey-pose-estimation
"""""";Computer Vision;https://github.com/Pxtri2156/AdelaiDet_v2
"""""";General;https://github.com/thomasp85/lime
"""The same learning algorithm was used to train agents for each of the ten OpenAI Gym MuJoCo continuous control environments. The only difference between evaluations was the number of episodes used per training batch  otherwise all options were the same. The exact code used to generate the OpenAI Gym submissions is in the **`aigym_evaluation`** branch.  Here are the key points:  * Proximal Policy Optimization (similar to TRPO  but uses gradient descent with KL loss terms)  \[1\] \[2\] * Value function approximated with 3 hidden-layer NN (tanh activations):     * hid1 size = obs_dim x 10     * hid2 size = geometric mean of hid1 and hid3 sizes     * hid3 size = 5 * Policy is a multi-variate Gaussian parameterized by a 3 hidden-layer NN (tanh activations):     * hid1 size = obs_dim x 10     * hid2 size = geometric mean of hid1 and hid3 sizes     * hid3 size = action_dim x 10     * Diagonal covariance matrix variables are separately trained * Generalized Advantage Estimation (gamma = 0.995  lambda = 0.98) \[3\] \[4\] * ADAM optimizer used for both neural networks * The policy is evaluated for 20 episodes between updates  except:     * 50 episodes for Reacher     * 5 episodes for Swimmer     * 5 episodes for HalfCheetah     * 5 episodes for HumanoidStandup * Value function is trained on current batch + previous batch * KL loss factor and ADAM learning rate are dynamically adjusted during training * Policy and Value NNs built with TensorFlow   """;Reinforcement Learning;https://github.com/magnusja/ppo
"""""";General;https://github.com/mitscha/dplc
"""""";Computer Vision;https://github.com/justinjohn0306/iPERCore-Windows10
"""""";General;https://github.com/teakkkz/imageSR
"""""";Computer Vision;https://github.com/wsmlby/darknetnano
"""""";Computer Vision;https://github.com/Brunogomes97/Imdb
"""""";Computer Vision;https://github.com/gregbugaj/unet-reference-impl
"""""";General;https://github.com/MSeeker1340/Vision2018-Pose
"""""";General;https://github.com/waze96/SRGAN
"""""";Computer Vision;https://github.com/Muthu2093/U-Net-for-Brain-Tumor-Segmentation
"""""";Computer Vision;https://github.com/mgonzalezrivero/reef_learning
"""""";Graphs;https://github.com/benedekrozemberczki/karateclub
"""""";Computer Vision;https://github.com/bentrevett/pytorch-image-classification
"""""";General;https://github.com/Fuchai/language
"""""";Reinforcement Learning;https://github.com/170928/-Review-Proximal-Policy-Optimization-Algorithms
"""""";Computer Vision;https://github.com/danielenricocahall/Keras-UNet
"""""";General;https://github.com/yaringal/BayesianRNN
"""""";General;https://github.com/sharan-dce/A3C
"""""";General;https://github.com/parachutel/deeplabv3plus_on_Mapillary_Vistas
"""""";General;https://github.com/ollewelin/libtorch-GPU-CNN-test-MNIST-with-Batchnorm
"""""";General;https://github.com/ttruty/facial-feature-mouse-control
"""""";Computer Vision;https://github.com/martinarjovsky/WassersteinGAN
"""""";Graphs;https://github.com/zxhhh97/ABot
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Nstats/bert_senti_analysis_ch
"""""";General;https://github.com/facebookresearch/stochastic_gradient_push
"""English | [简体中文](/README_zh-CN.md)  [![Documentation](https://readthedocs.org/projects/mmaction2/badge/?version=latest)](https://mmaction2.readthedocs.io/en/latest/) [![actions](https://github.com/open-mmlab/mmaction2/workflows/build/badge.svg)](https://github.com/open-mmlab/mmaction2/actions) [![codecov](https://codecov.io/gh/open-mmlab/mmaction2/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmaction2) [![PyPI](https://img.shields.io/pypi/v/mmaction2)](https://pypi.org/project/mmaction2/) [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/blob/master/LICENSE) [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues) [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)  MMAction2 is an open-source toolbox for video understanding based on PyTorch. It is a part of the [OpenMMLab](http://openmmlab.org/) project.  The master branch works with **PyTorch 1.3+**.  <div align=""center"">   <div style=""float:left;margin-right:10px;"">   <img src=""https://github.com/open-mmlab/mmaction2/raw/master/resources/mmaction2_overview.gif"" width=""380px""><br>     <p style=""font-size:1.5vw;"">Action Recognition Results on Kinetics-400</p>   </div>   <div style=""float:right;margin-right:0px;"">   <img src=""https://user-images.githubusercontent.com/34324155/123989146-2ecae680-d9fb-11eb-916b-b9db5563a9e5.gif"" width=""380px""><br>     <p style=""font-size:1.5vw;"">Skeleton-base Action Recognition Results on NTU-RGB+D-120</p>   </div> </div> <div align=""center"">   <img src=""https://github.com/open-mmlab/mmaction2/raw/master/resources/spatio-temporal-det.gif"" width=""800px""/><br>     <p style=""font-size:1.5vw;"">Spatio-Temporal Action Detection Results on AVA-2.1</p> </div>   """;General;https://github.com/open-mmlab/mmaction2
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/SYangDong/bert-with-frozen-code
"""""";Computer Vision;https://github.com/YongWookHa/swin-transformer-ocr
"""This project was conducted as part of my engineering degree. The goal was to build a lip reading AI that could output words or sentences from a silent video input.   """;Computer Vision;https://github.com/khazit/Lip2Word
"""""";Graphs;https://github.com/shenweichen/GraphEmbedding
"""""";General;https://github.com/S-Abdelnabi/awt
"""<p float=""center"">     <img src=""figures/eca_module.jpg"" width=""1000"" alt=""Struct."">     <br>     <em>Structural comparison of SE and ECA attention mechanism.</em> </p>  Efficient Channel Attention (ECA) is a simple efficient extension of the popular Squeeze-and-Excitation Attention Mechanism  which is based on the foundation concept of Local Cross Channel Interaction (CCI). Instead of using fully-connected layers with reduction ratio bottleneck as in the case of SENets  ECANet uses an adaptive shared (across channels) 1D convolution kernel on the downsampled GAP *C* x 1 x 1 tensor. ECA is an equivalently plug and play module similar to SE attention mechanism and can be added anywhere in the blocks of a deep convolutional neural networks. Because of the shared 1D kernel  the parameter overhead and FLOPs cost added by ECA is significantly lower than that of SENets while achieving similar or superior performance owing to it's capabilities of constructing adaptive kernels. This work was accepted at the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)  2020.    """;Computer Vision;https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET
"""""";Audio;https://github.com/adamwhitakerwilson/speaker_separation
"""Image super-resolution (SR)  which refers to the process of recovering high- resolution (HR) images from low-resolution (LR) images  is an important class of image processing techniques in computer vision. In general  this problem is very challenging and inherently ill posed since there are always multiple HR images for a single LR image but with the rapid development of deep learning techniques  Super-resolution models based on Deep learning have been extensively explored and they often achieve state-of-the-art performance on different benchmarks of Super-Resolution.   """;General;https://github.com/aba450/Super-Resolution
"""""";General;https://github.com/mahdi-darvish/Cloud_Segmentation_using_Mask_R-CNN
"""""";General;https://github.com/PaperCodeReview/SupCL-TF
"""""";Computer Vision;https://github.com/LeeGitaek/Detection-Using-Convolutional-Networks
"""""";General;https://github.com/jiangyingjunn/i-darts
"""""";General;https://github.com/phamtrancsek12/offensive-identification
"""""";Computer Vision;https://github.com/Prachi-Agr/Image-Super-Resolution
"""""";Computer Vision;https://github.com/NeuralVFX/wasserstein-gan
"""""";Computer Vision;https://github.com/JensSettelmeier/EfficientDet-DeepSORT-Tracker
"""""";Computer Vision;https://github.com/hhk7734/tensorflow-yolov4
"""""";Computer Vision;https://github.com/zhucheng725/mobilenetv1_keras
"""""";General;https://github.com/jolibrain/caffe
"""""";Computer Vision;https://github.com/cokowpublic/Nuclei-Segmentation
"""""";General;https://github.com/zj463261929/darknet_mAP
"""""";General;https://github.com/sartorius-research/dime.pytorch
"""""";Computer Vision;https://github.com/cgebbe/kaggle_pku-autonomous-driving
"""""";Computer Vision;https://github.com/XuanjieXiao/vehicle-detection-by-using-camera
"""""";Computer Vision;https://github.com/facebookresearch/stochastic_gradient_push
"""""";Computer Vision;https://github.com/SalvadorAlbarran/TFG2020
"""""";Natural Language Processing;https://github.com/Hironsan/anago
"""""";Computer Vision;https://github.com/rahulguptagzb09/Video-Object-Detection-Using-SSD
"""""";General;https://github.com/kozistr/pytorch_optimizer
"""""";Computer Vision;https://github.com/damien911224/theWorldInSafety
"""""";Computer Vision;https://github.com/syahdeini/gan
"""""";Computer Vision;https://github.com/bethgelab/InDomainGeneralizationBenchmark
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/nmfisher/bert-modified
"""""";General;https://github.com/Ajasra/stylegan2
"""""";Computer Vision;https://github.com/junyongyou/triq
"""""";Natural Language Processing;https://github.com/Kinetikm/fastTextRelearning
"""""";Computer Vision;https://github.com/jakeret/unet
"""""";General;https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks
"""""";Reinforcement Learning;https://github.com/marwanihab/RL_Testing_Noise_ASRN
"""""";General;https://github.com/YouYueHuang/CycleGAN_Unsupervised_Domain_Adaptation
"""Here  we implement basic data-handling tools for [SARAS-ESAD](https://saras-esad.grand-challenge.org/Dataset/) dataset with FPN training process. We implement a pure pytorch code for train FPN with [Focal-Loss](https://arxiv.org/pdf/1708.02002.pdf) or [OHEM/multi-box-loss](https://arxiv.org/pdf/1512.02325.pdf) paper.  <!-- Aim of this repository try different loss functions and make a fair comparison in terms of performance on SARAR-ESAD dataset. -->  We hope this will help kick start more teams to get up to the speed and allow the time for more innovative solutions. We want to eliminate the pain of building data handling and training process from scratch. Our final aim is to get this repository the level of [realtime-action-detection](https://github.com/gurkirt/realtime-action-detection).  At the moment we support the latest pytorch and ubuntu with Anaconda distribution of python. Tested on a single machine with 2/4/8 GPUs.  You can found out about architecture and loss function on parent repository  i.e. [RetinaNet implementation in pytorch.1.x](https://github.com/gurkirt/RetinaNet.pytorch.1.x).  ResNet is used as a backbone network (a) to build the pyramid features (b).  Each classification (c) and regression (d) subnet is made of 4 convolutional layers and finally a convolutional layer to predict the class scores and bounding box coordinated respectively.  Similar to the original paper  we freeze the batch normalisation layers of ResNet based backbone networks. Also  few initial layers are also frozen  see `fbn` flag in training arguments.    """;Computer Vision;https://github.com/Viveksbawa/SARAS-ESAD-Baseline
"""""";Computer Vision;https://github.com/cvp19g2/cvp19g2
"""""";Computer Vision;https://github.com/yitu-opensource/T2T-ViT
"""""";Reinforcement Learning;https://github.com/hossein-haeri/MPE
"""""";Computer Vision;https://github.com/e-hulten/real-nvp-2d
"""""";Computer Vision;https://github.com/youngwanLEE/CenterMask
"""""";Computer Vision;https://github.com/carolgithubv1/convnets-keras
"""""";Reinforcement Learning;https://github.com/polixir/NeoRL
"""""";General;https://github.com/kklipski/ALHE-projekt
"""This project is about using the [Sign Language Digits Dataset](https://www.kaggle.com/ardamavi/sign-language-digits-dataset/data) to classify images of sign language digits. This is similar to the MNIST dataset that has been used throughout the years to classify a grayscale  handwritten digits between 0 to 9.   """;Computer Vision;https://github.com/francislata/Sign-Language-Digits-CNN
"""""";General;https://github.com/NVlabs/stylegan2
"""""";Computer Vision;https://github.com/JojiJoseph/DCGAN
"""""";General;https://github.com/zhuoyang125/CenterMask2
"""""";Computer Vision;https://github.com/xiaomi-automl/MoGA
"""""";Computer Vision;https://github.com/ilyakava/gan
"""""";General;https://github.com/arkel23/PyTorch-Pretrained-ViT
"""""";Computer Vision;https://github.com/jihoonerd/Unsupervised-Representation-Learning-with-Deep-Convolutional-Generative-Adversarial-Networks
"""""";General;https://github.com/ngiann/ApproximateVI.jl
"""""";Computer Vision;https://github.com/tuanzhangCS/res2net-on-mxnet
"""In this project  my aim was to develop a model that could regenerate patched/covered parts of human faces  and achieve believable results. I used the [Celeb-A](https://www.kaggle.com/jessicali9530/celeba-dataset) dataset  and created a Generative Adversarial Network with a Denoising Autoencoder as the Generator and a Deep Convolutional Network as the Discriminator. I chose this architecture based on *Avery Allen and Wenchen Li*'s [Generative Adversarial Denoising Autoencoder for Face Completion](https://www.cc.gatech.edu/~hays/7476/projects/Avery_Wenchen/).  The Denoising Autoencoder has 'relu' activations in the middle layers while the output layer had a 'tanh' activation. Each Convolution layer was followed by a BatchNormalization layer. The Discriminator has 'LeakyReLU' activations for the Convolution part  with a BatchNormalization layer following every Conv layer. At the end  the output from the CNN segment was flattened and connected to a Dense layer with 1 node  having 'sigmoid' as the activation function. This would enable the discrimator to predict the probability that the input image is real.  I added distortions to the images in two ways:- - Added random Gaussian noise. - Added random sized Black Patches.  The entire training was done on a GTX 1080 GPU  and took about 12days.  The latest checkpoints and the saved generator and discriminator can be found [here](https://drive.google.com/drive/folders/13wUgCcENajkPZ4MHz2bHrJtQepyVDvtb?usp=sharing).  A few sample generated images are present in `saved_imgs`.   """;Computer Vision;https://github.com/rdutta1999/Patched-Face-Regeneration-GAN
"""The articulated 3D pose of the human body is high-dimensional and complex.  Many applications make use of a prior distribution over valid human poses  but modeling this distribution is difficult. Here we present VPoser  a learning based variational human pose prior trained from a large dataset of human poses represented as SMPL bodies. This body prior can be used as an Inverse Kinematics (IK) solver for many tasks such as fitting a body model to images  as the main contribution of this repository for [SMPLify-X](https://smpl-x.is.tue.mpg.de/).  VPoser has the following features:   - defines a prior of SMPL pose parameters  - is end-to-end differentiable  - provides a way to penalize impossible poses while admitting valid ones  - effectively models correlations among the joints of the body  - introduces an efficient  low-dimensional  representation for human pose  - can be used to generate valid 3D human poses for data-dependent tasks   """;General;https://github.com/nghorbani/human_body_prior
"""""";Reinforcement Learning;https://github.com/learn-to-race/l2r
"""In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   """;Natural Language Processing;https://github.com/yyht/Conv_Bert
"""""";Computer Vision;https://github.com/Kunaldargan/Paper-summerization--DEEP-LEARNING-MADE-EASIER-BY-LINEAR-TRANSFORMATIONS-IN-PERCEPTRONS--
"""""";General;https://github.com/atpaino/deep-text-corrector
"""""";General;https://github.com/davda54/ada-hessian
"""""";General;https://github.com/Qengineering/NanoDet-ncnn-Jetson-Nano
"""""";General;https://github.com/andreidi/AC_DDPG_walker
"""""";Natural Language Processing;https://github.com/DataScienceNigeria/ALBERT-for-Natural-Language-Processing
"""""";General;https://github.com/mlz8/GAN-Reading-List
"""""";General;https://github.com/shmanubhav/LunarLander-ApproxQLearning
"""Given an image of a person’s face  the task of classifying the ID of the face is known as **face classification**. Whereas the problem of determining whether two face images are of the same person is known as **face verification** and this has several important applications. This mini-project uses convolutional neural networks (CNNs) to design an end-to-end system for face classification and face verification.   """;Computer Vision;https://github.com/anjandeepsahni/face_classification
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/kelly2016/multi-label-bert
"""""";Natural Language Processing;https://github.com/om00839/machine-suneung
"""""";Sequential;https://github.com/bentrevett/pytorch-language-modeling
"""""";Graphs;https://github.com/mitya8128/experiments_notes
"""""";Computer Vision;https://github.com/laurent-dinh/nice
"""Cutout is a simple regularization method for convolutional neural networks which consists of masking out random sections of input images during training. This technique simulates occluded examples and encourages the model to take more minor features into consideration when making decisions  rather than relying on the presence of a few major features.      ![Cutout applied to CIFAR-10](https://github.com/uoguelph-mlrg/Cutout/blob/master/images/cutout_on_cifar10.jpg ""Cutout applied to CIFAR-10"")  Bibtex:   ``` @article{devries2017cutout      title={Improved Regularization of Convolutional Neural Networks with Cutout}      author={DeVries  Terrance and Taylor  Graham W}      journal={arXiv preprint arXiv:1708.04552}      year={2017}   } ```   """;Computer Vision;https://github.com/uoguelph-mlrg/Cutout
"""""";General;https://github.com/AKASH2907/bird_species_classification
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/flyliu2017/bert_modularized
"""""";General;https://github.com/wujiyang/Face_Pytorch
"""""";General;https://github.com/sacmehta/delight
"""""";General;https://github.com/vg-sravan/Cycle-Consistent-Generative-Adversarial-Networks
"""""";Computer Vision;https://github.com/pterhoer/PrivacyPreservingFaceRecognition
"""""";Computer Vision;https://github.com/JinLi711/Convolution_Variants
"""""";Reinforcement Learning;https://github.com/wxj77/TransferReinforcementLearning
"""""";General;https://github.com/harpreetsodhi/ChangeMyPet_Deep_Learning_Model
"""""";Computer Vision;https://github.com/davidiommi/Pytorch--3D-Medical-Images-Segmentation--SALMON
"""""";General;https://github.com/hariv/pix2pix
"""""";General;https://github.com/jugatsingh/self_attention_video
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   """;Computer Vision;https://github.com/xjnpark/ds
"""""";General;https://github.com/JohanYe/CycleGAN-Pokemon
"""""";Sequential;https://github.com/coqui-ai/TTS
"""""";Computer Vision;https://github.com/pkuxmq/Invertible-Image-Rescaling
"""""";General;https://github.com/Bersaelor/AAPLMetalImageRecognition
"""""";General;https://github.com/SNUDerek/multiLSTM
"""""";General;https://github.com/meltnur/speed
"""""";Natural Language Processing;https://github.com/thecodemasterk/Text-to-Text-transfer-transformers
"""""";Computer Vision;https://github.com/adityabingi/DCGAN-TF2.0
"""The goal of the project is to identify flowers belonging to five classes: daisy  tulip  rose  sunflower and dandelion.  The dataset is available for download at https://www.kaggle.com/alxmamaev/flowers-recognition. I have built a simple convolutional neural network (CNN) using tensorflow and keras to classify the flowers. Furthermore  I have implemented tranfer learning where I have used the weights of the InceptionV3 model (https://arxiv.org/abs/1512.00567) trained on the ImageNet Dataset (http://www.image-net.org/).   """;Computer Vision;https://github.com/manashpratim/Flower_Recognition
"""""";General;https://github.com/idansc/fga
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/NovasMax/yolov3
"""""";General;https://github.com/libiseller/MobileNetV2-dynamicFPN
"""""";General;https://github.com/CWanli/myencoding
"""""";General;https://github.com/prajwaldp/gan-image-colorizer
"""""";General;https://github.com/awslabs/sockeye
"""""";General;https://github.com/facebookarchive/fb.resnet.torch
"""""";General;https://github.com/akashe/Python-Code-Generation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/cospplay/bert-master
"""""";General;https://github.com/wilson1yan/VideoGPT
"""""";Computer Vision;https://github.com/Proxim123/person-reID-No1-
"""""";Computer Vision;https://github.com/OlafenwaMoses/Traffic-Net
"""""";Computer Vision;https://github.com/patrick013/Image-Classification-CNN-and-VGG
"""""";Computer Vision;https://github.com/abhishekshakya/GAN-for-producing-MNIST-images
"""""";General;https://github.com/Robinatp/Deeplab_Tensorflow
"""""";Computer Vision;https://github.com/DineshAnalyticsandAI/Pneumonia-Detection-Chest-X-Rays
"""""";Computer Vision;https://github.com/lucasgb98/StyleGAN2
"""""";Computer Vision;https://github.com/chenjunweii/mxnet-ssd
"""""";General;https://github.com/lee-aaron/Image-Super-Resolution
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/zapplea/bert
"""**GCNet** is initially described in [arxiv](https://arxiv.org/abs/1904.11492). Via absorbing advantages of Non-Local Networks (NLNet) and Squeeze-Excitation Networks (SENet)   GCNet provides a simple  fast and effective approach for global context modeling  which generally outperforms both NLNet and SENet on major benchmarks for various recognition tasks.   """;Computer Vision;https://github.com/xvjiarui/GCNet
"""""";Computer Vision;https://github.com/sevakon/efficientdet
"""""";Computer Vision;https://github.com/schatty/dcgan-tf
"""""";Computer Vision;https://github.com/Rahmanzia3/darknet
"""""";Computer Vision;https://github.com/gtakos-ai/ENet
"""""";Computer Vision;https://github.com/chencq1234/maskrcnn_facebook
"""""";Computer Vision;https://github.com/JohanYe/CycleGAN-Pokemon
"""""";Computer Vision;https://github.com/tlikhomanenko/optimize-coordconv
"""""";General;https://github.com/cloudnine148/PHC_2nd_SPC
"""""";Computer Vision;https://github.com/witwickey/darknet
"""""";General;https://github.com/fer-moreira/ColorizerBot
"""""";Computer Vision;https://github.com/pcummer/cycle-gan-keras
"""""";General;https://github.com/sourabhagrawal23/human-action-classification
"""""";Computer Vision;https://github.com/rahulvigneswaran/longtail-buzz
"""""";General;https://github.com/cpm0722/transformer_pytorch
"""""";Computer Vision;https://github.com/mushfiqurrahman250/Face-Recognition-master
"""""";Reinforcement Learning;https://github.com/fantianwen/laalaz13E
"""""";Computer Vision;https://github.com/SimoneDutto/EDSR
"""""";Natural Language Processing;https://github.com/trthackthonFighters/ConvBert
"""""";Reinforcement Learning;https://github.com/brett-daley/fast-dqn
"""""";Reinforcement Learning;https://github.com/behzadanksu/rl-attack
"""""";Sequential;https://github.com/cchinchristopherj/Concert-of-Whales
"""""";General;https://github.com/demul/ResidualNet
"""""";Reinforcement Learning;https://github.com/V0LsTeR/DQN_heap
"""""";General;https://github.com/LEEPEIQIN/EDSR
"""""";General;https://github.com/bupt-ai-cz/LLVIP
"""""";Computer Vision;https://github.com/ankitAMD/Self-Attention-GAN-master_pytorch
"""""";Reinforcement Learning;https://github.com/SintolRTOS/multi-agent_Example
"""""";Computer Vision;https://github.com/Se-Hun/GAN_Study
"""""";Computer Vision;https://github.com/sandeeplbscek/awesome-deeplearning-resources
"""""";Computer Vision;https://github.com/buswinka/HcUnet
"""""";General;https://github.com/mpyrozhok/adamwr
"""""";General;https://github.com/oki5656/Estimating_Distance
"""""";Reinforcement Learning;https://github.com/wsjeon/multiagent-particle-envs-v2
"""""";Computer Vision;https://github.com/VinayBhupalam/predicting_Pneumonia
"""""";Computer Vision;https://github.com/Sezoir/DCGAN-Dog-Generator
"""""";Computer Vision;https://github.com/yashgarg98/VQ-VAE
"""""";General;https://github.com/lixihan/Resnet
"""""";Computer Vision;https://github.com/lvweiwolf/efficientdet
"""""";Computer Vision;https://github.com/SurajDonthi/Multi-Camera-Person-Re-Identification
"""""";Computer Vision;https://github.com/jaideepmurkute/Multi-purpose-Disentangling-Variational-Autoencoders-for-ECG-data
"""""";General;https://github.com/pranav-ust/nlm
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/frankcgq105/BERTCHEN
"""""";Graphs;https://github.com/tkipf/gcn
"""""";General;https://github.com/mikehuisman/revisiting-learned-optimizers
"""""";Sequential;https://github.com/carpedm20/NTM-tensorflow
"""""";Computer Vision;https://github.com/jerrywn121/TianChi_AIEarth
"""""";Natural Language Processing;https://github.com/semicontinuity/nlp
"""""";General;https://github.com/Nick-Zhao-Engr/Machine-Translation
"""""";Computer Vision;https://github.com/UPCLJ/py-faster-rcnn
"""""";General;https://github.com/ahundt/sharpDARTS
"""[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.   """;Natural Language Processing;https://github.com/amymariaparker2401/new
"""""";Computer Vision;https://github.com/universome/alis
"""""";Computer Vision;https://github.com/psmenon/DCGAN
"""""";General;https://github.com/jcjohnson/fast-neural-style
"""""";General;https://github.com/DaPenggg/AutoInt
"""""";Computer Vision;https://github.com/ericjang/odin
"""""";Computer Vision;https://github.com/xx88xx/stylegan2ky
"""``` The task is to generate real images from landscape paintings. ``` Folder structure --------------  ``` ├── datasets/Train/a       - this folder contains landscape images. │   ├── image1001.png │   └── image1002.png │   └── -------------------- │ │ ├── datasets/Train/b      - this folder contains Real-world images. │   ├── image1001.png │   └── image1002.png │   └── --------------------   │ ├── datasets/Test-set/a             - this folder contains Test images(landscapes). │   └── image1001.png │   └── --------------------  │ ├── save_model    -- this folder contains saved model │ │── Python-scripts      - this folder contains  python files(can be run driectly in Jupyter notebook/IDE) │ ├──  train-MANET.py        - this file is used for training image. │    ├──  testing.py         - this file is used for generating test images. │    ├──  result        - this folder contains generated test images. │  └──logs/tensorlogs       ```  """;Computer Vision;https://github.com/Nisnab/Pix2Pix
"""""";General;https://github.com/csinva/disentangled-attribution-curves
"""""";General;https://github.com/franknb/Self-attention-DCGAN
"""""";Reinforcement Learning;https://github.com/rikluost/RL_DQN_Pong
"""""";General;https://github.com/itsuki8914/wgan-gp-TensorFlow
"""""";Computer Vision;https://github.com/mr-bulb/DL_basic_github
"""""";Computer Vision;https://github.com/zju3dv/disprcnn
"""""";General;https://github.com/adammenges/DARTS-PyTorch
"""""";General;https://github.com/Lornatang/ESPCN-PyTorch
"""**R-FCN** is a region-based object detection framework leveraging deep fully-convolutional networks  which is accurate and efficient. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region sub-network hundreds of times  our region-based detector is fully convolutional with almost all computation shared on the entire image. R-FCN can natually adopt powerful fully convolutional image classifier backbones  such as [ResNets](https://github.com/KaimingHe/deep-residual-networks)  for object detection.  R-FCN was initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).  This code has been tested on Windows 7/8 64 bit  Windows Server 2012 R2  and Ubuntu 14.04  with Matlab 2014a.   """;Computer Vision;https://github.com/ishanashastri/RFCN-Breast-Cancer
"""The solution is a web-service.  Users interact with it via a standard web browser on a smartphone or a desktop computer. Results are displayed on the screen as images and text and can be sent to the user's E-mail.  This solution can also be installed as a standalone program on a personal computer and can be used through a command-line interface.  Video presentation: https://youtu.be/_vcvxPtAzOM     This service is available at the address: http://angelina-reader.ru       """;General;https://github.com/IlyaOvodov/AngelinaReader
"""""";General;https://github.com/datalogue/keras-attention
"""""";Graphs;https://github.com/bcsrn/gcn
"""""";Computer Vision;https://github.com/lunz-s/DeepAdverserialRegulariser
"""""";Computer Vision;https://github.com/DaVran369/face
"""""";Computer Vision;https://github.com/isl-org/DPT
"""""";Computer Vision;https://github.com/liangheming/sparse_rcnnv1
"""""";Reinforcement Learning;https://github.com/allenai/robothor-challenge
"""""";Natural Language Processing;https://github.com/aws-samples/amazon-sagemaker-bert-pytorch
"""""";Computer Vision;https://github.com/huyz1117/GAN-TensorFlow
"""""";Natural Language Processing;https://github.com/Zeeshan75/Bert_Telugu_Ner
"""""";General;https://github.com/amurthy1/dagan_torch
"""""";Audio;https://github.com/anandharaju/Basic_TCN
"""""";General;https://github.com/jdbugs/stylegan2
"""""";General;https://github.com/AnirudhMaiya/RAdam
"""""";Natural Language Processing;https://github.com/GauthierDmn/question_answering
"""""";General;https://github.com/YudeWang/SEAM
"""""";General;https://github.com/awslabs/dgl-ke
"""""";Computer Vision;https://github.com/onedayatatime0923/Cycle_Mcd_Gan
"""""";Reinforcement Learning;https://github.com/austinsilveria/Banana-Collection-DQN
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/fofore/yolov3-pyroch
"""""";Computer Vision;https://github.com/SHI-Labs/Convolutional-MLPs
"""Space exploration is now the fastest-growing subject. This was  however at the cost of billions worth of projects and twice the amount effort. Obi-Wan Kenobi is an integrated system of a space rover updated and modified to use minimum energy with the greatest efficiency possible. It aims to revolutionize the entire industry  rendering it a profitable organization. This is managed by a new arm designed to indulge the biggest and most accurate amount of information while also not sacrificing the energy and a brand new software that takes the camera’s 2-D image and process it with sensors on the rover to produce a high-quality 3-D environment. How We Addressed This Challenge Obi-Wan Kenobi is space rover designed to gather information about foreign plants and there composition in a new way. We have made it with many implementations improving on the previous rovers using new technics  mechanics  and software in astronomical ways. Obi-Wan Kenobi aims to take the prior models of space rovers and modify on them in ways that revolutionize the entire subject of space exploration by maximizing the amount of information it can indulge and allaying every drop of wasted energy putting it into more work.     """;Computer Vision;https://github.com/ahmedabdel-hady/Space-Xplores-Nasaspaceapp2020
""" Narcissus is an interactive art installation which takes an image of the viewer  compresses it into a set of descriptive features  and produces the ""reflection"" of those features as a neural network sees it. Particularly  this Generative Adversarial Network (GAN) draws the viewer's reflection from its artificial  distinctly neural ""latent"" space which it learns from its training dataset. I used a GAN trained on a well-known celebrity face dataset  CelebA-HQ  meaning that the neural network learns to reproduce faces of celebrities. Narcissus intends to raise more self-reflecting questions in the viewer than it answers  many of which are becoming increasingly relevant.  When the viewer looks into the mirror and sees a foreign face stare back  similarities and differences are visceral. What makes a celebrity’s face different from ours? How does the generated face staring back at us reflect what we deem as special  as 'celebrity'  or as beautiful? More broadly  how do the deep-seated biases inherent in our everyday society lie hidden in the datasets we collect?  These questions are all the more important in the new age of artificial intelligence; those biased datasets we use lead to inherently biased data-driven artificial intelligence models. Without sufficient checks and balances  data-driven systems can also create feedback loops resulting in an amplification of these biases  such as in [recidivism prediction](https://www.technologyreview.com/s/612775/algorithms-criminal-justice-ai/). As well as this  since data is necessarily a reduction of objective reality  models that learn only from training data will never have the full picture and there will always be cases where it lacks common sense. For example  if a model is trained on current recidivism data to predict recidivism rates  even if the data contains no information about race  the model might learn that certain crimes are committed by certain classes of people corresponding to race  and use that information to predict rates that might still be disproportionate across race. We must be careful not to blindly believe that these systems will not perpetuate the biases and mistakes of our current or previous societies.  As well as this  the model often makes mistakes that seem obvious to us  producing a celebrity that looks totally different from the viewer. Even though the celebrity is a product of the viewer's data  *is* it the viewer? This again brings to attention the point that data is simply a reduction of objective reality. How can we reduce the important features of a human to a representation in data? Is there a minimum amount of data that can represent a human? Corporations are collecting and consolidating data into unprecedented [data lakes]() with data sources from [amazon ring](https://www.usatoday.com/story/tech/2019/10/29/amazon-ring-doorbell-cams-police-home-security/2493974001/)  []()  and [](). While this data can be used to build powerful models  these models will never operate on objective reality  but just a reduction of it. As we progress into the age of big data and powerful models that are increasingly replacing humans in important decision-making including punishment sentencing  [facial recognition in HK](https://www.nytimes.com/2019/07/26/technology/hong-kong-protests-facial-recognition-surveillance.html)  and [healthcare](https://www.nytimes.com/2019/03/21/science/health-medicine-artificial-intelligence.html)  we need to be wary of the inadvertant effects caused by a general lack of human ""common sense"" in these models due to inherent properties of data.   """;Computer Vision;https://github.com/kyranstar/Narcissus
"""""";Audio;https://github.com/Salazar-99/Gravitational-WaveNet
"""""";Computer Vision;https://github.com/amerch/CIFAR100-Training
"""""";Natural Language Processing;https://github.com/Chinnu1103/Machine-Translation-using-Transformers
"""""";Sequential;https://github.com/ShigemichiMatsuzaki/MSPL
"""""";Natural Language Processing;https://github.com/chaneeh/Word2Vec_implementation
"""""";General;https://github.com/timsainb/ParametricUMAP_paper
"""""";Graphs;https://github.com/lukecavabarrett/pna
"""""";Graphs;https://github.com/ibalazevic/TuckER
"""""";General;https://github.com/solapark/da_yolo
"""""";General;https://github.com/sgamezrdo/anomaly-aoutoencoding
"""""";Natural Language Processing;https://github.com/lucidrains/performer-pytorch
"""The attention mechanism is a popular easy-to-implement model architecture designed to perform well on different NLP tasks  including machine translation.  Empowered with pretrained [SpaCy](#https://spacy.io/) word model this approach makes a strong baseline with comparable to state-of-the-art perfomance. In this repo the translator from German to English is trained and demonstrated. In case you need other languages support  thanks to SpaCy team there are  plenty of available language models to train the exact same model on.  Pretrained German-English model is already available after setup.     """;General;https://github.com/Andrey885/Machine_translation_PyTorch
"""<div align=""left"">   <img src=""https://insightface.ai/assets/img/custom/thumb_sdunet.png"" width=""600""/> </div>  In this module  we provide datasets and training/inference pipelines for face alignment.  Supported methods:  - [x] [SDUNets (BMVC'2018)](alignment/heatmap) - [x] [SimpleRegression](alignment/coordinate_reg)   [SDUNets](alignment/heatmap) is a heatmap based method which accepted on [BMVC](http://bmvc2018.org/contents/papers/0051.pdf).  [SimpleRegression](alignment/coordinate_reg) provides very lightweight facial landmark models with fast coordinate regression. The input of these models is loose cropped face image while the output is the direct landmark coordinates.    <div align=""left"">   <img src=""https://insightface.ai/assets/img/github/11513D05.jpg"" width=""640""/> </div>  In this module  we provide training data with annotation  network settings and loss designs for face detection training  evaluation and inference.  The supported methods are as follows:  - [x] [RetinaFace (CVPR'2020)](detection/retinaface) - [x] [SCRFD (Arxiv'2021)](detection/scrfd) - [x] [blazeface_paddle](detection/blazeface_paddle)  [RetinaFace](detection/retinaface) is a practical single-stage face detector which is accepted by [CVPR 2020](https://openaccess.thecvf.com/content_CVPR_2020/html/Deng_RetinaFace_Single-Shot_Multi-Level_Face_Localisation_in_the_Wild_CVPR_2020_paper.html). We provide training code  training dataset  pretrained models and evaluation scripts.   [SCRFD](detection/scrfd) is an efficient high accuracy face detection approach which is initialy described in [Arxiv](https://arxiv.org/abs/2105.04714). We provide an easy-to-use pipeline to train high efficiency face detectors with NAS supporting.    In this module  we provide training data  network settings and loss designs for deep face recognition.  The supported methods are as follows:  - [x] [ArcFace_mxnet (CVPR'2019)](recognition/arcface_mxnet) - [x] [ArcFace_torch (CVPR'2019)](recognition/arcface_torch) - [x] [SubCenter ArcFace (ECCV'2020)](recognition/subcenter_arcface) - [x] [PartialFC_mxnet (Arxiv'2020)](recognition/partial_fc) - [x] [PartialFC_torch (Arxiv'2020)](recognition/arcface_torch) - [x] [VPL (CVPR'2021)](recognition/vpl) - [x] [OneFlow_face](recognition/oneflow_face) - [x] [ArcFace_Paddle (CVPR'2019)](recognition/arcface_paddle)  Commonly used network backbones are included in most of the methods  such as IResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  etc..    [InsightFace](https://insightface.ai) is an open source 2D&3D deep face analysis toolbox  mainly based on PyTorch and MXNet.   Please check our [website](https://insightface.ai) for detail.  The master branch works with **PyTorch 1.6+** and/or **MXNet=1.6-1.8**  with **Python 3.x**.  InsightFace efficiently implements a rich variety of state of the art algorithms of face recognition  face detection and face alignment  which optimized for both training and deployment.   """;Computer Vision;https://github.com/deepinsight/insightface
"""""";Computer Vision;https://github.com/Panagiotou/Procedural3DTerrain
"""""";General;https://github.com/sithu31296/image-classification
"""""";Computer Vision;https://github.com/mnicnc404/CartoonGan-tensorflow
"""""";Sequential;https://github.com/xieyxclack/factual_coco
"""""";Computer Vision;https://github.com/MaybeShewill-CV/bisenetv2-tensorflow
"""""";Computer Vision;https://github.com/ZC119/Handwritten-CycleGAN
"""""";Computer Vision;https://github.com/whai362/PytorchInsight
"""""";General;https://github.com/ahmadelsallab/READ
"""""";General;https://github.com/rupakdas18/SuperGlue-tasks-using-BERT
"""""";Computer Vision;https://github.com/eddiebarry/SingleShotDetector
"""""";General;https://github.com/kiranu1024/deep-learning
"""""";Computer Vision;https://github.com/iejMac/ScriptWriter
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Misoknisky/Bert-MultiGpu
"""""";General;https://github.com/gcr/torch-residual-networks
"""""";Reinforcement Learning;https://github.com/PeterJochem/Deep_RL
"""""";Computer Vision;https://github.com/RedWang1989/YOLOv4
"""The paper Image-to-Image Translation with Conditional Adversarial Networks (https://arxiv.org/pdf/1611.07004.pdf) showed that a general purpose solution could be made for image-to-image translation. Since the time that this paper was published  multiple artists and researchers have made their own models and experiments  with stunning results. These models range from creating cats out of drawings to creating videos of the sea by using an input from general household appliances.<br> The objective of the pix2pix model is to find a model that can map one picture to a desired paired image  which is indistinguishable from the real thing. An example is shown in Figure 1  where 4 different models attempt the mapping from the pixelated image to the real image. Pix2pix uses Conditional Generative Adversarial Networks to achieve this objective. Conditional means that the loss here is structured  there exists a conditional dependency between the pixels  meaning that the loss of one pixel is influenced by the loss of another. The loss function that is used by the model is shown in Equation 1.  <p align=""center"">   <img src=""/ImagesInText/LossFunction.png"" width=""60%"" height=""60%""><br>   Equation 1 [P. Isola et al. (https://arxiv.org/pdf/1611.07004.pdf)] </p>  The model can then be trained and results evaluated  which has been done for a great variety of experiments. We wanted to see if another application could be made  that of image restoration of blurry pictures. We have most likely all seen a movie or tv-series in which a spy agency needed someone to ""enhance"" a photo in order to see smaller details  with the advent of deep learning these techniques are becoming more of a reality. We wanted to see if this general architecture of pix2pix for image translation could also be used for this application to see if you could enhance your images by using pix2pix.  This blog first starts with the method of our project  what exactly is the type of data that we investigate and in what type of datasets they are stored. This is followed up by an explanation about the hyperparameter tuning that was performed and why this was important. The last part of the method is how we would evaluate our results. The method is followed up by our experiments  which also gives a sample of the data that we used as well as the result of some of our experiments  these results are then discussed in our discussion as well with our conclusion about the experiment if pix2pix can be used for image restoration.   """;Computer Vision;https://github.com/PieterBijl/Group28
"""""";General;https://github.com/ShwetaBaranwal/Transformer
"""""";Computer Vision;https://github.com/cics-nd/predictive-cvs
"""""";Audio;https://github.com/PhilippeNguyen/keras_wavenet
"""""";Computer Vision;https://github.com/Visual-Behavior/detr-tensorflow
"""""";Computer Vision;https://github.com/toufiksk/darknet
"""Minecraft is a popular sandbox video game that contains a number of hostile non-player entities known as “mobs”; these entities are meant to attack and kill the player character. Our agent will have to learn strategies to deal with each type of hostile mob with the goal of defeating as many mobs and surviving as long as possible. Additionally  the environment in a Minecraft “world” can be randomly generated using an algorithm or built by the player. To create a closed environment for our agent to learn and fight against these mobs  we will be using Microsoft’s Project Malmo. Using machine learning in minecraft is the focus of a large competition called MineRL  which provides rigorous guidelines towards achieving an agent that can operate autonomously in minecraft. It is our hope that methods like the ones we are using to train an agent in a simulated environment can be extrapolated to real life applications like robotics in the physical world. Since minecraft as an environment is completely customizable  it makes it ideal for entry level testing of potential real world use cases.   """;Reinforcement Learning;https://github.com/rishavb123/MineRL
"""""";General;https://github.com/170928/-Review-Generative-Adversarial-Imitation-Learning
"""""";Natural Language Processing;https://github.com/amauriciorr/AubreyBot
"""""";Natural Language Processing;https://github.com/luomancs/retriever_reader_for_okvqa
"""""";General;https://github.com/CorentinJ/Real-Time-Voice-Cloning
"""""";Computer Vision;https://github.com/recluse27/Colorizator
"""This code contains two versions of the network architectures and hyper-parameters. The first one is based on the [TensorFlow implementation](https://github.com/hardikbansal/CycleGAN). The second one is based on the [official PyTorch implementation](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix). The differences are minor and we observed both versions produced good results. You may need to train several times as the quality of the results are sensitive to the initialization.    Below is a snapshot of our result at the 50th epoch on one training instance:  <img src='imgs/horse2zebra.png' width=""900px""/>   """;Computer Vision;https://github.com/leehomyc/cyclegan-1
"""""";Computer Vision;https://github.com/natanielruiz/android-yolo
"""""";Computer Vision;https://github.com/davidemartinelli/VAE
"""""";General;https://github.com/qdhill/openpose
"""""";Computer Vision;https://github.com/MartinGer/Stand-Alone-Self-Attention-in-Vision-Models
"""""";General;https://github.com/Dragonsson/SakeDrop-Pytorch
"""""";Computer Vision;https://github.com/karpathy/deep-vector-quantization
"""""";Computer Vision;https://github.com/srihari-humbarwadi/tensorflow_retinanet
"""""";General;https://github.com/manicman1999/StyleGAN-Keras
"""""";General;https://github.com/jpata/SparseDistance
"""""";Natural Language Processing;https://github.com/ypeleg/Fastformer-Keras
"""""";General;https://github.com/Matesxs/GAN-Playground
"""""";Natural Language Processing;https://github.com/wuch15/Fastformer
"""""";Computer Vision;https://github.com/dyhan0920/PyramidNet-PyTorch
"""""";General;https://github.com/chris-tng/semi-supervised-nlp
"""""";Natural Language Processing;https://github.com/utterworks/fast-bert
"""""";Computer Vision;https://github.com/xuanyuzhou98/SqueezeSeg
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/MonsterPeng/Deformable-ConvNets-master
"""- The wrong connection and lack joint error is 256px smaller than 321px. Sometimes there is missing part.  - In terms of time  256px is almost twice as fast as 321px.  - PifPaf is often wrong in the wrong connection  missing part error. In addition  MPII and COCO dataset 256px and 321px focus on other error.  - In sports environment  PifPaf focuses on wrong connection error (60-70% of total error).   The recent methods of estimating the human posture in two-dimensional space based on deep learning have shown better applicability and results than before. However  the problem also faced many different challenges such as in crowds  resolution  lighting  ... In this project  I analyze and evaluate pros and cons of the article “Pifpaf: Composite Fields For Human Pose Estimation ”  the author has focused on solving the challenge of occluded and low resolution. I analyze on different datasets: COCO dataset  in addition I analyze errors on 1000 MPII images  2000 images of sports dataset collected by us. Thereby having a more general view of the article  and thereby giving directions to develop research to improve the article.   This project based on paper : https://arxiv.org/abs/1903.06593 and code: https://github.com/vita-epfl/openpifpaf  I analyze and evaluate on datasets:4000 images COCO test-dev 2000 images sports 1000 images MPII datasets:  Because the author focused on solving the challenge on low resolution and obscuration. I resized the images to 3 different resolutions: 256px  321px  641px. The author have suggested that 321px is the best but I want to experiment with 256px more  then I can conclude that 321px is the best or not.  Dataset: https://drive.google.com/drive/folders/19xFqlgraUi7BZp9VgBUY_UfC6u4gYNyY?usp=sharing  Analysis: https://drive.google.com/drive/folders/1BUkAbabOqjFUhi2_RlYds0vFS7Wg4ryB?usp=sharing   """;Computer Vision;https://github.com/thanhtrung98/Pose_estimation
"""""";Reinforcement Learning;https://github.com/honghaow/FORK
"""""";Reinforcement Learning;https://github.com/philtabor/Deep-Q-Learning-Paper-To-Code
"""""";Computer Vision;https://github.com/shikhadahiya/Image-to-image-translation-using-C-GAN
"""""";General;https://github.com/Knight825/models-pytorch
"""**RNN based seq2seq model**:  A seq2seq model is mainly used in NLP tasks such as machine translation and often based on LSTM or GRU structure. It has encoder  decoder and intermediate step as its main components  mapping an arbitrarily long input sequence to an arbitrarily long output sequence with an intermediate encoded state.:  <p align=""center"">   <img src=""figures/seq2seq.png"">  </p>  In comparison to fully connected feed forward neural networks  recurrent neural networks has no longer the requirement a fixed-sized input and considers naturally the relation between previous and current time steps. In addition  LSTM or GRU are advanced RNN structures  which increase the ability of capturing long-term dependencies  by forcing a approximately constant back-propagation error flow during training.  However  due to the recurrent calculation for each time step  parrellelization is impossible for training theses networks. And it's a big disadvantage in the big data era. Even the input time range for a LSTM can not be arbitrary long in reality  and it is in fact severly limited by the training mechanism of RNN.  **Wavenet based approach**:  With Wavenet  the training procedure for all the time steps in the input can be parrellelized. We just let the output sequence be one time step ahead of the input sequence  and at every time step of the output  the value is only influenced by the previous steps in the input.  As for the inference stage  it yields every time only the prediction one step ahead as in the LSTM approach. But we don't need to define a distinct model for inferencing here. In each Iteration  the last point of the output sequence is selected as the prediction one step ahead of the previous iteration  and it is in turn concatenated to the input sequence  in order to predict one step further in the future.    The model architecture is similar to WaveNet  consisting of a stack of dilated causal convolutions  as demonstrated in the [diagram](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) below. For more details  see van den Oord's [paper](https://arxiv.org/abs/1609.03499).  <p align=""center"">   <img src=""figures/wavenet.gif"">  </p>  **Causal Convolution**:  The figure below shows a causal structure  which guarantees that the current time step is only influenced by the previous time steps. Then an expression of the conditional probability could be established. That is to say  we assume that the current value is conditioned on the previous values in a time sequence.    <p align=""center"">   <img src=""figures/WaveNet_causalconv.png"">  </p>  **Dilated Convolution**:  But as can be seen  the reception field is quite small with a limited number of stacks  and it results in poor performance handling long-term dependencies. So the idea of dilated convolution is employed. In a dilated convolution layer  filters are not applied to inputs in a simple sequential manner  but instead skip a constant dilation rate inputs in between each of the inputs they process  as in the WaveNet diagram below. By increasing the dilation rate multiplicatively at each layer (e.g. 1  2  4  8  …)  we can achieve the exponential relationship between layer depth and receptive field size that we desire. The figure below ilustrates the effect of dilation.  <p align=""center"">   <img src=""figures/WaveNet_dilatedconv.png"">  </p>   """;Audio;https://github.com/ZhouYuxuanYX/Wavenet-in-Keras-for-Kaggle-Competition-Web-Traffic-Time-Series-Forecasting
"""""";Computer Vision;https://github.com/vaibhavpec2012/ECE1512-Digital-Image-Processing-Homework-3
"""""";Audio;https://github.com/zassou65535/WaveGAN
"""""";Reinforcement Learning;https://github.com/qihongl/demo-advantage-actor-critic
"""""";Computer Vision;https://github.com/rahul-t-p/ASVspoof-2019
"""""";General;https://github.com/shunk031/LSUV.pytorch
"""""";Computer Vision;https://github.com/justld/EspnetV2_paddle
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Nstats/bert_MRC
"""""";Computer Vision;https://github.com/ZeeTsing/Carvana_challenge
"""WaveNet replication study. Before stepping up to WaveNet implementation it was decided to implement PixelCNN first as WaveNet based on its architecture.  This repository contains two modes: [Gated PixelCNN][pixelcnn-paper] and [WaveNet][wavenet-paper]  see class definitions in `wavenet/models.py`.  For detailed explanation of how these model work see [my blog post](http://sergeiturukin.com/2017/02/22/pixelcnn.html).   """;Computer Vision;https://github.com/rampage644/wavenet
"""""";Sequential;https://github.com/nkcr/overlap-ml
"""""";General;https://github.com/yramon/ShapCounterfactual
"""""";General;https://github.com/sheryl-ai/MetaPred
"""""";Natural Language Processing;https://github.com/lucidrains/electra-pytorch
"""""";Computer Vision;https://github.com/PaddlePaddle/PaddleClas
"""""";Natural Language Processing;https://github.com/fabiocorreacordeiro/Elsevier_abstracts-Classification
"""""";Computer Vision;https://github.com/GiantPandaCV/yolov3-point
"""""";Audio;https://github.com/zhangbo2008/fastSpeeck2_chinese_train
"""This repo contains the code to train and evaluate FC-DenseNets as described in [The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/abs/1611.09326). We investigate the use of [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993) for semantic segmentation  and report state of the art results on datasets such as CamVid.   """;Computer Vision;https://github.com/SimJeg/FC-DenseNet
"""""";Computer Vision;https://github.com/jaingaurav3/GAN-Hacks
"""""";Computer Vision;https://github.com/JWMON/yolo
"""""";Computer Vision;https://github.com/solaris33/YOLO-v1-tf2
"""""";General;https://github.com/ApexPredator1/DenseNet_tensorflow
"""""";General;https://github.com/neil-zeng/asr
"""""";Computer Vision;https://github.com/object-detection-algorithm/SSD
"""""";Computer Vision;https://github.com/2anchao/VovJpu
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/dzqjorking/transpose
"""""";Reinforcement Learning;https://github.com/puppetect/TradingBot-tensorflow
"""""";General;https://github.com/Avmb/lowrank-highwaynetwork
"""""";General;https://github.com/jacquelinelala/GFN
"""""";General;https://github.com/ahmetumutdurmus/awd-lstm
"""An easy-to-use and efficient system to support the Mixture of Experts (MoE)  model for PyTorch.    """;General;https://github.com/laekov/fastmoe
"""""";Graphs;https://github.com/ds4dm/sparse-gcn
"""""";General;https://github.com/AlexHeyman/PopulationBasedTraining
"""""";Graphs;https://github.com/Anou9531/GAT
"""""";Sequential;https://github.com/park-cheol/NLP-Seq2Seq
"""""";General;https://github.com/dl-nlp/dl-nlp.github.io
"""Just another implementation with Tensorflow for paper `Spatial Transformer Netrworks`.    """;Computer Vision;https://github.com/Legend94rz/spatial-transformer
"""""";Computer Vision;https://github.com/cmf3673/spotti
"""""";Natural Language Processing;https://github.com/keonlee9420/Comprehensive-Transformer-TTS
"""""";Computer Vision;https://github.com/fredotran/traffic-sign-detector-yolov4
"""""";General;https://github.com/google-research/crest
"""""";General;https://github.com/theamrzaki/text_summurization_abstractive_methods
"""""";Computer Vision;https://github.com/xmu-xiaoma666/RepMLP-pytorch
"""Py**T**orch **Im**age **M**odels (`timm`) is a collection of image models  layers  utilities  optimizers  schedulers  data-loaders / augmentations  and reference training / validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.  The work of many others is present here. I've tried to make sure all source material is acknowledged via links to github  arxiv papers  etc in the README  documentation  and code docstrings. Please let me know if I missed anything.   """;General;https://github.com/rwightman/pytorch-image-models
"""  [MegaFace](http://megaface.cs.washington.edu/) dataset includes 1 027 060 faces  690 572 identities.   Challenge 1 is taken to test our model with 1 million distractors.   ![image](https://github.com/foamliu/InsightFace-v2/raw/master/images/megaface_stats.png)    MS-Celeb-1M dataset for training  3 804 846 faces over 85 164 identities.    """;General;https://github.com/foamliu/InsightFace-PyTorch
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M and VGG2 datasets  which were already packed in the MxNet binary format. The network backbones include ResNet  InceptionResNet_v2  DenseNet  DPN and MobiletNet. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss. * loss-type=0:  Softmax * loss-type=1:  SphereFace * loss-type=2:  CosineFace * loss-type=4:  ArcFace * loss-type=5:  Combined Margin * loss-type=12: TripletLoss  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/sunil-rival/insightface
"""""";Computer Vision;https://github.com/rwightman/efficientdet-pytorch
"""""";Sequential;https://github.com/rand0musername/urnn
"""""";Reinforcement Learning;https://github.com/lukebhan/TwitterSentimentAnalysisTool
"""""";Computer Vision;https://github.com/noobpython/Notes
"""""";Computer Vision;https://github.com/pranjalrai-iitd/Fetal-head-segmentation-and-circumference-measurement-from-ultrasound-images
"""""";Computer Vision;https://github.com/andychinka/dcase-challenge
"""""";Computer Vision;https://github.com/ChristophReich1996/Involution
"""""";Audio;https://github.com/Shivendra-psc/speechbot
"""Distribuuuu is a Distributed Classification Training Framework powered by native PyTorch.  Please check [tutorial](./tutorial/) for detailed **Distributed Training** tutorials:  - Single Node Single GPU Card Training [[snsc.py](./tutorial/snsc.py)] - Single Node Multi-GPU Cards Training (with DataParallel) [[snmc_dp.py](./tutorial/snmc_dp.py)] - Multiple Nodes Multi-GPU Cards Training (with DistributedDataParallel)     - torch.distributed.launch [[mnmc_ddp_launch.py](./tutorial/mnmc_ddp_launch.py)]     - torch.multiprocessing [[mnmc_ddp_mp.py](./tutorial/mnmc_ddp_mp.py)]     - Slurm Workload Manager [[mnmc_ddp_slurm.py](./tutorial/mnmc_ddp_slurm.py)] - ImageNet training example [[imagenet.py](./tutorial/imagenet.py)]  For the complete training framework  please see [distribuuuu](./distribuuuu/).    """;Computer Vision;https://github.com/BIGBALLON/distribuuuu
"""""";Computer Vision;https://github.com/fredotran/yolov3-opencvdnn
"""""";Audio;https://github.com/dortenenboim314/cyclic-jukebox
"""""";Computer Vision;https://github.com/Stamatis-Ilias/PLAsTiCC-Astronomical-Classification
"""ROS에서 각 노드는 다음과 같이 연결되어 있습니다. Darknet_ros (YOLO) 노드는 ZED camera로 부터 left image를 subscribe하고  객체인식 결과로 Bounding Box들의 정보를 넘겨줍니다.  PSMnet 노드는 ZED camera로 부터 left and right (stereo) image 토픽을 subscribe하고  네트워크를 통해 disparity를  추정  깊이정보를 획득합니다. 최종적으로 객체인식 및 깊이추정 네트워크에서 획득한 정보를 결합하여 인식된 객체가 카매라로부터 몇 미터 떨어져 있는지 객체별로 정보를 표시합니다.  <img align=""center"" src=""https://user-images.githubusercontent.com/25498950/69921866-12429180-14da-11ea-8759-bb23bfb9151f.png"">        """;Computer Vision;https://github.com/ChelseaGH/sidewalk_prototype_AI_Hub
"""""";General;https://github.com/SeonbeomKim/TensorFlow-MemN2N
"""""";Sequential;https://github.com/thethirdwheel/malumagraph
"""""";Computer Vision;https://github.com/Jasonlee1995/DeepLab_v1
"""""";General;https://github.com/usnistgov/image-classification-resnet50
"""""";Computer Vision;https://github.com/seanmullery/iWGAN
"""""";Computer Vision;https://github.com/ardeal/EfficientNetV2
"""""";Reinforcement Learning;https://github.com/bacdavid/HER
"""PaddleOCR aims to create multilingual  awesome  leading  and practical OCR tools that help users train better models and apply them into practice.   """;Computer Vision;https://github.com/Mushroomcat9998/PaddleOCR
"""""";Computer Vision;https://github.com/RRoundTable/OceanLitter_dataset_generator
"""Pneumonia is a disease in which the air sacs in one or both lungs get infected and inflame. The air sacs may fill with fluid or pus (purulent material)  causing cough with phlegm or pus  fever  chills  and difficulty breathing. Diseases such as Pneumonia are responsible for over 1 million hospitalizations and 50 000 deaths a year in the US alone. Currently radiologists use Chest X-Rays to detect diseases such as Pneumonia. Other diseases detected in this manner include Atelectasis  Consolidation  Infiltration  Pneumothorax  Edema  Emphysema  Fibrosis  Effusion  Pleural Thickening  Cardiomegaly  Nodule  Hernia and Mass. Once detected  the patient can be treated. However if the disease is not detected at an early stage  the consequences can be severe.   Luckily algorithms can be trained to detect diseases and assist medical personel. In fact algorithms can be trained to detect diseases such as Pneumonia with greater accuracy than any human radiologist from chest X-Rays. Therfore  through decreasing human error in detection  countless lives can be saved!  Further an estimated two thirds of the global population lacks access to radiology diagnostics. These diagnostics include as mentioned above detection of diseases. With the automation of radiology experts  healthcare delivery can be improved and access to medical imaging expertise can be increased in many parts of the world. Therefore  through automating radiology experts  many parts of the world will gain radiology diagnostics and countless lives can be saved!  We set out to build an algorithm that could take as input a chest X-ray image and return probabilities for a collection of diseases detectable through chest X-rays (Atelectasis  Consolidation  Infiltration  Pneumothorax  Edema  Emphysema  Fibrosis  Effusion  Pleural Thickening  Cardiomegaly  Nodule  Hernia  Mass) and the probability of no disease being present.   ![Image of chest X-Ray and heatmap](https://github.com/thibaultwillmann/CheXNet-Pytorch/blob/master/chest_x_ray_example.png)  Image of a chest X-Ray left and heatmap highlighting areas with high probalility of a disease being present right   """;Computer Vision;https://github.com/thibaultwillmann/CheXNet-Pytorch
"""""";Computer Vision;https://github.com/NMADALI97/Learning-With-Wasserstein-Loss
"""""";General;https://github.com/tianhai123/yolov3
"""I used the pandas library to calculate summary statistics of the traffic signs data set:  * Number of training examples = 34799 * Number of validation examples = 4410 * Number of testing examples = 12630 * Image data shape = (32  32  3) * Number of classes = 43   """;General;https://github.com/waynecoffee9/Traffic-Sign-Classifier
"""""";General;https://github.com/imenurok/ShakeDrop
"""https://dreamtolearn.com/ryan/cognitivewingman/18/en  Character Cartridges - Embodied Identity  We are entering a magical convergence phase in media and technology. The next dozen years through 2030 are going to be very interesting as we begin to understand how to develop character and identity for sensemaking systems  leveraging AI.  Multiple technologies – including in mobile  AR  and deep learning - are rapidly converging to enable organizations to compose AI-powered systems only dreamt of in Sci-Fi novels and Hollywood movies.    These sensemaking (and empathetic) systems will quickly enable assistants who can play roles that include a “Cognitive Wingman” – similar to the automated intelligence seen in media: JARVIS (Iron Man); KITT (Knight Rider); HAL (Space Odyssey); Samantha (Her); TARS (Interstellar).      These systems will: · Talk and listen · Have identity · Have relationships · Are situationally aware · Reason  Understand and Learn · Understand context and remember things · Can hold state for multiple ‘conversation turns’ · Behave in a manner that simulates emotional intelligence     With readily available technology - can build alpha versions of these systems today.  Mind you many POC's quite crappy – but it’s a start - and demonstrates feasibility.  And with widely available ML tools and techniques  and our human tendency to improve on things – good stuff will happen soon.     A key component for the creation of Digital Humans (for applications extending well beyond gaming) is a sense of identity and character.   Empathetic systems that embody cognitive elements need personality.   The best rendered face and eyes  is still just a collection of high resolution pixels – until we add voice  emotion  identity and soul.    """;Natural Language Processing;https://github.com/rustyoldrake/Character-Cartridges-Embodied-Identity
"""""";Computer Vision;https://github.com/samsonadmin/modified-alexeyab-darknet
"""Chat-bots are becoming more and more useful in various simple professional tasks as they get more and more able to capture the essence of communicating with people. Still the development of good chat-bots that will answer more complicated questions  in general subjects  is a growing domain of research.   The goal of this project is to create a chat-bot able to answer python related questions.  Our project started with the main idea being that a programming assistant would be a much needed help by many people working or studying computer science. Although it sounds simple it soon proved to be a difficult task. The main challenge is that the model has to extract a technical correlation between Questions and Answers in order to be able to communicate effectively. The model that we used in order to achieve our goal is a recurrent sequence-to-sequence model. The main steps that we followed are described bellow.  - We found  downloaded  and processed data taken from stack overflow concerning questions that contained at least one python tag.[7] - Implement a sequence-to-sequence model. - Jointly train encoder and decoder models using mini-batches - Used greedy-search decoding - Interact with trained chatbot   """;Natural Language Processing;https://github.com/vGkatsis/Chat_Bot_DL
"""""";Computer Vision;https://github.com/krantirk/Self-Supervised-photo
"""""";General;https://github.com/TheConstant3/MobileNetV3-Keras
"""""";Computer Vision;https://github.com/liuzhuang13/DenseNetCaffe
"""""";Computer Vision;https://github.com/CPJKU/cca_layer
"""- *main.py*: main script to train or evaluate models - *train.py*: training and evaluation part of the code - *config*: storing configuration of datasets (and maybe other things in the future) - *utils.pypy*: useful functions - *getbest.py*: display the best validation error of each saving folder - *dataloader.py*: defines *getDataloaders* function which is used to load datasets - *models*: a folder storing all network models. Each script in it should contain a *createModel(\*\*kwargs)* function that takes the arguments and return a model (subclass of nn.Module) for training - *scripts*: a folder storing example training commands in UNIX shell scripts   """;Computer Vision;https://github.com/felixgwu/img_classification_pk_pytorch
"""""";Computer Vision;https://github.com/yell/kaggle-camera
"""""";Natural Language Processing;https://github.com/santient/sparse-transformer
"""""";Computer Vision;https://github.com/rajneeshaggarwal/google-efficientnet
"""""";Natural Language Processing;https://github.com/Maple728/transformer
"""""";General;https://github.com/theidentity/Improved-GAN-PyTorch
"""""";Computer Vision;https://github.com/gagan16/DcGan-Tensorflow
"""""";General;https://github.com/lkfo415579/MT-Readling-List
"""""";General;https://github.com/gustavla/fractalnet
"""""";Computer Vision;https://github.com/asyrovprog/cs230project
"""""";Computer Vision;https://github.com/yenchenlin/pix2pix-tensorflow
"""""";Computer Vision;https://github.com/WongKinYiu/yolor
"""""";Audio;https://github.com/WLM1ke/poptimizer
"""""";General;https://github.com/uzh-dqbm-cmi/PySeqLab
"""""";Graphs;https://github.com/williamleif/GraphSAGE
"""""";General;https://github.com/bdy9527/SDCN
"""""";General;https://github.com/rpinsler/active-bayesian-coresets
"""""";Computer Vision;https://github.com/MTCloudVision/mxnet-dssd
"""""";Computer Vision;https://github.com/iver56/audiomentations
"""""";Natural Language Processing;https://github.com/lonePatient/bert-sentence-similarity-pytorch
"""""";General;https://github.com/jramapuram/SimCLR
"""""";Computer Vision;https://github.com/FrancisCrickInstitute/Etch-a-Cell-Nuclear-Envelope
"""""";Computer Vision;https://github.com/haithink/myWindowsDarknet
"""""";Computer Vision;https://github.com/Lopezurrutia/DSB_2018
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/mike07026/insightface_20190620
"""""";General;https://github.com/GlassyWing/transformer-keras
"""""";Computer Vision;https://github.com/alexssanchez/unet-app-pucp
"""""";Computer Vision;https://github.com/google/hdrnet
"""""";Computer Vision;https://github.com/rubenrosales/inception_v4
"""""";General;https://github.com/CMU-Perceptual-Computing-Lab/openpose
"""""";Natural Language Processing;https://github.com/The-AI-Summer/self-attention-cv
"""""";General;https://github.com/aurelien-peden/Deep-Learning-paper-implementations
"""""";General;https://github.com/beresandras/contrastive-classification-keras
"""""";General;https://github.com/lucidrains/performer-pytorch
"""""";Reinforcement Learning;https://github.com/johan-gras/MuZero
"""""";Natural Language Processing;https://github.com/drewwilimitis/hyperbolic-learning
"""You can create WordNet noun hypernym pairs as follows  ```shell python ../scripts/create_wordnet_noun_hierarchy.py ./wordnet_noun_hypernyms.tsv ```  and mammal subtree is created by  ```shell python ../scripts/create_mammal_subtree.py ./mammal_subtree.tsv ```   """;Natural Language Processing;https://github.com/TatsuyaShirakawa/poincare-embedding
"""""";Computer Vision;https://github.com/YirunKCL/Tensorflow-Keras-Involution2D
"""""";Reinforcement Learning;https://github.com/andreidi/AC_DDPG_walker
"""""";General;https://github.com/cahya-wirawan/indonesian-language-models
"""""";Reinforcement Learning;https://github.com/JohannesAck/tf2multiagentrl
"""""";Computer Vision;https://github.com/ddehueck/pytorch-GAN
"""""";Computer Vision;https://github.com/Mariya1285/Codeathon
"""""";Sequential;https://github.com/scpark20/universal-music-translation
"""""";Sequential;https://github.com/peustr/wavenet
"""""";Reinforcement Learning;https://github.com/nandomp/AICollaboratory
"""""";General;https://github.com/robintyh1/icml2021-pengqlambda
"""""";Computer Vision;https://github.com/xternalz/WideResNet-pytorch
"""""";Computer Vision;https://github.com/branislavhesko/segmentation_framework
"""Recent advancement in deep learning has become one of the most powerful tools to solve the image classification and segmentation problem.Deep learning model learn the filter that helps in extraction and learning of the important feature form the images. These feature helps to find differences as well as similarities amongst the image. Deep learning models require large dataset to learn the complex data representation.In the paper[1] the authors have used DCNN model to find the green cover in the cities using Cityscapes dataset. Cityscapes dataset has 2975 images and mask of green cover of different cities around the world which was used as the training data and 500 image with masks were used as testing dataset. The images were google street view images. The DCNN model has an IOU of 61.2 percent. In this approach I used state of the art unet model and mobile net v2 model. Unet gave an IOU of 74.5 percent and mobile net v2 model gave an IOU of 64.3 percent which were better and lighter model than previously used DCNN model. The model were even tested on different machine type with different configuration to check their performance.   """;General;https://github.com/anant1203/Applying-Deep-Learning-for-Large-scale-Quantification-of-Urban-Tree-Cover
"""""";Natural Language Processing;https://github.com/openai/gpt-3
"""""";General;https://github.com/bratao/PySeqLab
"""""";Natural Language Processing;https://github.com/lucidrains/feedback-transformer-pytorch
"""""";Computer Vision;https://github.com/seyedsaleh/music-generator
"""""";General;https://github.com/kaituoxu/Speech-Transformer
"""![Trained Agent][image1]  The goal of the project is to create an agent that learns how to efficiently solve a Tennis environment made with Unity-ML agents. While active the agent is trying to approximate the policy that defines his behaviour and tries to maximize the performance in the context of the environment.  In this environment  two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net  it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds  it receives a reward of -0.01. Thus  the goal of each agent is to keep the ball in play.  The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own  local observation. Two continuous actions are available  corresponding to movement toward (or away from) the net  and jumping.  The environment is considered solved  when the average (over 100 episodes) of those **scores** is at least +0.5.   """;Reinforcement Learning;https://github.com/prajwalgatti/DRL-Collaboration-and-Competition
"""""";Computer Vision;https://github.com/kondukberna/Car_Detection
"""    This program performs segmentation of the left vertricle  myocardium  right ventricle     and backgound of Cardiovascular Magnetic Resonance Images  with use of a convolutional neural network based on the well-known U-Net     architecture  as described by [https://arxiv.org/pdf/1505.04597.pdf](Ronneberger et al.) For each patient  both a 3D end systolic       image and a 3D end diastolic image with its corresponding ground truth segmentation of the left ventricle  myocardium and right         ventricle is available.           The available code first divides the patients data into a training set and a test set. The training data is then loaded from the        stored location and subsequently preprocessed. Preprocessing steps include resampling the image to the same voxel spacing  removal of outliers  normalization  cropping and one-hot encoding of the labels. Before training  the trainingset is subdivided again for training and validation of the model.          For training  a network based on the U-Net architecture is used and implemented with keras. For training  many different variables       can be tweaked  which are described in some detail below. After training  the network is evaluated using the test dataset. This data is loaded and preprocessed in the same way as the training dataset and propagated through the network to obtain pixel-wise predictions for each class. These predictions are probabilities and are thresholded to obtain a binary segmentation.           The binary segmentations are then evaluated by computing the (multiclass) softdice coefficient and the Hausdorff distance between the obtained segmentations and the ground truth segmentations. The softdice coefficients and Hausdorff distances are computed for each image for each individual class and the multiclass softdice for all the classes together. These results are all automatically saved in a text file. Furthermore  the obtained segmentations as an overlay with the original images  the training log and corresponding plots and the model summary are also saved automatically.         Lastly  from the segmentations of the left ventricular cavity during the end systole and end diastole  the ejection fraction is calculated. This value is  alongside the ejection fraction computed from the ground truth segmentations  stored in the same text file with results.       """;Computer Vision;https://github.com/jellevankerk/Team-Challenge
"""""";Reinforcement Learning;https://github.com/abbadka/quadcopter
"""""";Natural Language Processing;https://github.com/lucidrains/local-attention
"""""";General;https://github.com/prasadji/Flower-Classifaction-with-Fine-Tuned-Mobilenet
"""""";Computer Vision;https://github.com/mv-lab/kuzushiji-recognition
"""""";General;https://github.com/sanghviyashiitb/GANS-VanillaAndMinibatchDiscrimination
"""""";General;https://github.com/Glp91/fire-detection
"""""";General;https://github.com/ksw0306/WaveVAE
"""""";Computer Vision;https://github.com/yoyotv/Image-derain-via-CGAN
"""""";General;https://github.com/yoshitomo-matsubara/torchdistill
"""> A team of radiologists from New Orleans studied the usefulness of Chest Radiographs for diagnosing COVID-19 compared to the reverse-transcription polymerase chain reaction (RT-PCR) and found out they could aid rapid diagnosis  especially in areas with limited testing facilities [[1]](https://pubs.rsna.org/doi/10.1148/ryct.2020200280 ""A Characteristic Chest Radiographic Pattern in the Setting of the COVID-19 Pandemic"").<br> > Another study found out that the radiographs of different viral cases of pneumonia are comparative  and they overlap with other infectious and inflammatory lung diseases  making it hard for radiologists to recognize COVID‐19 from other viral pneumonia cases [[2]](https://pubs.rsna.org/doi/10.1148/rg.2018170048 ""Radiographic and CT Features of Viral Pneumonia"").<br> > This project aims to make the former study a reality while dealing with the intricacies in the latter  with the help of Deep Learning.<br>   """;General;https://github.com/priyavrat-misra/xrays-and-gradcam
"""it has all kinds of baseline models for text classification.  it also support for multi-label classification where multi labels associate with an sentence or document.  although many of these models are simple  and may not get you to top level of the task. but some of these models are very   classic  so they may be good to serve as baseline models. each model has a test function under model class. you can run   it to performance toy task first. the model is independent from data set.  <a href='https://github.com/brightmart/text_classification/blob/master/multi-label-classification.pdf'>check here for formal report of large scale multi-label text classification with deep learning</a>  several models here can also be used for modelling question answering (with or without context)  or to do sequences generating.   we explore two seq2seq model(seq2seq with attention transformer-attention is all you need) to do text classification.   and these two models can also be used for sequences generating and other tasks. if your task is a multi-label classification    you can cast the problem to sequences generating.  we implement two memory network. one is dynamic memory network. previously it reached state of art in question   answering  sentiment analysis and sequence generating tasks. it is so called one model to do several different tasks    and reach high performance. it has four modules. the key component is episodic memory module. it use gate mechanism to   performance attention  and use gated-gru to update episode memory  then it has another gru( in a vertical direction) to   performance hidden state update. it has ability to do transitive inference.  the second memory network we implemented is recurrent entity network: tracking state of the world. it has blocks of   key-value pairs as memory  run in parallel  which achieve new state of art. it can be used for modelling question   answering with contexts(or history). for example  you can let the model to read some sentences(as context)  and ask a   question(as query)  then ask the model to predict an answer; if you feed story same as query  then it can do   classification task.   To discuss ML/DL/NLP problems and get tech support from each other  you can join QQ group: 836811304  Models: -------------------------------------------------------------------------  1) fastText 2) TextCNN  3) Bert:Pre-training of Deep Bidirectional Transformers for Language Understanding   4) TextRNN     5) RCNN      6) Hierarchical Attention Network     7) seq2seq with attention    8) Transformer(""Attend Is All You Need"") 9) Dynamic Memory Network 10) EntityNetwork:tracking state of the world 11) Ensemble models 12) Boosting:       for a single model  stack identical models together. each layer is a model. the result will be based on logits added together. the only connection between layers are label's weights. the front layer's prediction error rate of each label will become weight for the next layers. those labels with high error rate will have big weight. so later layer's will pay more attention to those mis-predicted labels  and try to fix previous mistake of former layer. as a result  we will get a much strong model.     check a00_boosting/boosting.py  and other models:  1) BiLstmTextRelation;  2) twoCNNTextRelation;  3) BiLstmTextRelationTwoRNN  Performance -------------------------------------------------------------------------  (mulit-label label prediction task ask to prediction top5  3 million training data full score:0.5)  Model   | fastText|TextCNN|TextRNN| RCNN | HierAtteNet|Seq2seqAttn|EntityNet|DynamicMemory|Transformer ---     | ---     | ---   | ---   |---   |---         |---        |---      |---          |---- Score   | 0.362   |  0.405| 0.358 | 0.395| 0.398      |0.322      |0.400    |0.392        |0.322 Training| 10m     |  2h   |10h    | 2h   | 2h         |3h         |3h       |5h           |7h --------------------------------------------------------------------------------------------------    Bert model achieves 0.368 after first 9 epoch from validation set.    Ensemble of TextCNN EntityNet DynamicMemory: 0.411    Ensemble EntityNet DynamicMemory: 0.403       --------------------------------------------------------------------------------------------------    Notice:     `m` stand for **minutes**; `h` stand for **hours**;   `HierAtteNet` means Hierarchical Attention Networkk;  `Seq2seqAttn` means Seq2seq with attention;  `DynamicMemory` means DynamicMemoryNetwork;  `Transformer` stand for model from 'Attention Is All You Need'.  Usage: ------------------------------------------------------------------------------------------------------- 1) model is in `xxx_model.py` 2) run python `xxx_train.py` to train the model 3) run python `xxx_predict.py` to do inference(test).  Each model has a test method under the model class. you can run the test method first to check whether the model can work properly.  -------------------------------------------------------------------------  Environment: ------------------------------------------------------------------------------------------------------- python 2.7+ tensorflow 1.8   (tensorflow 1.1 to 1.13 should also works; most of models should also work fine in other tensorflow version  since we   use very few features bond to certain version.  if you use python3  it will be fine as long as you change print/try catch function in case you meet any error.  TextCNN model is already transfomed to python 3.6   Sample data: <a href='https://pan.baidu.com/s/1yWZf2eAPxq15-r2hHk2M-Q'>cached file of baidu</a> or <a href=""https://drive.google.com/drive/folders/0AKEuT4gza2AlUk9PVA"">Google Drive:</a>send me an email ------------------------------------------------------------------------------------------------------- to help you run this repository  currently we re-generate training/validation/test data and vocabulary/labels  and saved   them as cache file using h5py. we suggest you to download it from above link.  it contain everything you need to run this repository: data is pre-processed  you can start to train the model in a minute.    it's a zip file about 1.8G  contains 3 million training data. although after unzip it's quite big  but with the help of   hdf5  it only need a normal size of memory of computer(e.g.8 G or less) during training.  we use jupyter notebook: <a href='https://github.com/brightmart/text_classification/blob/master/pre-processing.ipynb'>pre-processing.ipynb</a> to pre-process data. you can have a better understanding of this task and   data by taking a look of it. you can also generate data by yourself in the way your want  just change few lines of code   using this jupyter notebook.  If you want to try a model now  you can dowload cached file from above  then go to folder 'a02_TextCNN'  run                python  p7_TextCNN_train.py      it will use data from cached files to train the model  and print loss and F1 score periodically.  old sample data source: if you need some sample data and word embedding per-trained on word2vec  you can find it in closed issues  such as: <a href=""https://github.com/brightmart/text_classification/issues/3"">issue 3</a>.   you can also find some sample data at folder ""data"". it contains two files:'sample_single_label.txt'  contains 50k data   with single label; 'sample_multiple_label.txt'  contains 20k data with multiple labels. input and label of is separate by ""   __label__"".  if you want to know more detail about data set of text classification or task these models can be used  one of choose is below:  https://biendata.com/competition/zhihu/  Road Map ------------------------------------------------------------------------------------------------------- One way you can use this repository:   step 1: you can read through this article. you will get a general idea of various classic models used to do text classification.  step 2: pre-process data and/or download cached file.        a. take a look a look of jupyter notebook('pre-processing.ipynb')  where you can familiar with this text              classification task and data set. you will also know how we pre-process data and generate training/validation/test                         set. there are a list of things you can try at the end of this jupyter.         b. download zip file that contains cached files  so you will have all necessary data  and can start to train models.  step 3: run some of models list here  and change some codes and configurations as you want  to get a good performance.        record performances  and things you done that works  and things that are not.        for example  you can take this sequence to explore:               1) fasttext---> 2)TextCNN---> 3)Transformer---> 4)BERT  additionally  write your article about this topic  you can follow paper's style to write. you may need to read some papers                on the way  many of these papers list in the  """;Natural Language Processing;https://github.com/brightmart/text_classification
"""""";Computer Vision;https://github.com/eric-erki/yolov3
"""""";General;https://github.com/lukas-blecher/LaTeX-OCR
"""""";Computer Vision;https://github.com/ultralytics/yolov3
"""""";Computer Vision;https://github.com/josedolz/HyperDenseNet
"""""";Computer Vision;https://github.com/Arthur-Shi/py-faster-rcnn
"""""";General;https://github.com/Mariya1285/Codeathon
"""""";Computer Vision;https://github.com/binhdv92/darknet
"""""";Computer Vision;https://github.com/solapark/darknet_partdet_tmp
"""""";General;https://github.com/avillemin/Minecraft-AI
"""""";Computer Vision;https://github.com/jlazarow/learning_instance_occlusion
"""**RepPoints**  initially described in [arXiv](https://arxiv.org/abs/1904.11490)  is a new representation method for visual objects  on which visual understanding tasks are typically centered. Visual object representation  aiming at both geometric description and appearance feature extraction  is conventionally achieved by `bounding box + RoIPool (RoIAlign)`. The bounding box representation is convenient to use; however  it provides only a rectangular localization of objects that lacks geometric precision and may consequently degrade feature quality. Our new representation  RepPoints  models objects by a `point set` instead of a `bounding box`  which learns to adaptively position themselves over an object in a manner that circumscribes the object’s `spatial extent` and enables `semantically aligned feature extraction`. This richer and more flexible representation maintains the convenience of bounding boxes while facilitating various visual understanding applications. This repo demonstrated the effectiveness of RepPoints for COCO object detection.  Another feature of this repo is the demonstration of an `anchor-free detector`  which can be as effective as state-of-the-art anchor-based detection methods. The anchor-free detector can utilize either `bounding box` or `RepPoints` as the basic object representation.  <div align=""center"">   <img src=""demo/reppoints.png"" width=""400px"" />   <p>Learning RepPoints in Object Detection.</p> </div>   """;Computer Vision;https://github.com/muditchaudhary/RepPoints-x-Libra-R-CNN-x-Transformer-self-attention
"""""";General;https://github.com/artxtech/darknet-rnn
"""""";General;https://github.com/bamos/densenet.pytorch
"""""";General;https://github.com/asvcode/1_cycle
"""""";Computer Vision;https://github.com/aniketmaurya/ssd-tf2-tfds
"""""";Computer Vision;https://github.com/oxygen0605/ImageClassification
"""""";General;https://github.com/emilianavt/OpenSeeFace
"""""";Natural Language Processing;https://github.com/jageshmaharjan/BERT_Service
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/zzdang/match_fashion
"""""";Natural Language Processing;https://github.com/darr/nerbert
"""""";Computer Vision;https://github.com/andrecianflone/vector_quantization
"""""";Computer Vision;https://github.com/arnavdodiedo/DenseNet-Fashion-MNIST
"""""";General;https://github.com/ShreyasArthur/StyleGAN-2-with-Urban-Plans
"""""";General;https://github.com/aabbeell/reinforcementLearning.a2c.gym
"""""";General;https://github.com/Kate589/gan
"""The *Show and Tell* model is a deep neural network that learns how to describe the content of images. For example:  ![Example captions](g3doc/example_captions.jpg)   """;Computer Vision;https://github.com/brandontrabucco/im2txt_express
"""Glaucoma is a retinal disease caused due to increased intraocular pressure in the eyes. It is the second most dominant cause of irreversible blindness after cataract  and if this remains undiagnosed  it may become the first common cause. Ophthalmologists use different comprehensive retinal examinations such as ophthalmoscopy  tonometry  perimetry  gonioscopy and pachymetry to diagnose glaucoma. But all these approaches are manual and time-consuming. Thus  a computer-aided diagnosis system may aid as an assistive measure for the initial screening of glaucoma for diagnosis purposes  thereby reducing the computational complexity. This paper presents a deep learning-based disc cup segmentation glaucoma network (DC-Gnet) for the extraction of structural features namely cup-to-disc ratio  disc damage likelihood scale and inferior superior nasal temporal regions for diagnosis of glaucoma. The proposed approach of segmentation has been trained and tested on RIM-One and Drishti-GS dataset. Further  based on experimental analysis  the DC-Gnet is found to outperform U-net  Gnet and Deep-lab architectures.  ![**Glaucomatous Eye**](Glaucoma.jpg)   """;Computer Vision;https://github.com/archit31uniyal/DC-Gnet
"""""";General;https://github.com/snjstudent/CycleGAN.tensorflow.keras
"""""";Computer Vision;https://github.com/hero9968/pix2pix-tensorflow
"""""";Computer Vision;https://github.com/modelhub-ai/mobilenet
"""""";Computer Vision;https://github.com/Hirotane/SSD_keras2
"""""";Computer Vision;https://github.com/aboulch/sharesnet
"""""";Computer Vision;https://github.com/NZ99/transformer_in_transformer_flax
"""""";Computer Vision;https://github.com/zhijie-yang/pointnet.pytorch
"""""";General;https://github.com/TomokiKomiya/SRGAN-keras
"""""";Natural Language Processing;https://github.com/jialinwu17/tmpimgs
"""""";Computer Vision;https://github.com/WeiYangze/hibernate-demo
"""""";Computer Vision;https://github.com/nicolalandro/Padam
"""""";Computer Vision;https://github.com/kiruthick101/darknet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/autobotasia/vibert
"""""";General;https://github.com/Dominioncher/smart-sketch
"""""";Computer Vision;https://github.com/mtmd/Mobile_ConvNet
"""""";Computer Vision;https://github.com/NVlabs/stylegan
"""""";Computer Vision;https://github.com/MegviiDetection/video_analyst
"""""";Computer Vision;https://github.com/TheInfamousWayne/UNet
"""""";Reinforcement Learning;https://github.com/zhengant/dqn_reversi
"""""";General;https://github.com/uoguelph-mlrg/instance_selection_for_gans
"""""";Computer Vision;https://github.com/Stick-To/Inception-tensorflow
"""""";Natural Language Processing;https://github.com/madhawav/MML
"""""";General;https://github.com/yechanp/Membership-Inference-Attacks-Against-Object-Detection-Models
"""""";Natural Language Processing;https://github.com/VishalFun/Keras_practice
"""""";Computer Vision;https://github.com/espressif/esp-who
"""""";Computer Vision;https://github.com/ejlb/squeezenet-chainer
"""""";General;https://github.com/andi611/Mockingjay-Speech-Representation
"""""";Computer Vision;https://github.com/jo1jun/Vision_Transformer
"""""";Computer Vision;https://github.com/nicku-a/Weather_Classification
"""""";Computer Vision;https://github.com/Qengineering/YoloV4-ncnn-Jetson-Nano
"""An example diagram of our Cluster-NMS  where X denotes IoU matrix which is calculated by `X=jaccard(boxes boxes).triu_(diagonal=1) > nms_thresh` after sorted by score descending. (Here use 0 1 for visualization.)  <img src=""cluster-nms01.png"" width=""1150px""/> <img src=""cluster-nms02.png"" width=""1150px""/>  The inputs of NMS are `boxes` with size [n 4] and `scores` with size [80 n]. (take coco as example)  There are two ways for NMS. One is that all classes have the same number of boxes. First  we use top k=200 to select the top 200 detections for every class. Then `boxes` will be [80 m 4]  where m<=200. Do Cluster-NMS and keep the boxes with `scores>0.01`. Finally  return top 100 boxes across all classes.  The other approach is that different classes have different numbers of boxes. First  we use a score threshold (e.g. 0.01) to filter out most low score detection boxes. It results in the number of remaining boxes in different classes may be different. Then put all the boxes together and sorted by score descending. (Note that the same box may appear more than once  because its scores of multiple classes are greater than the threshold 0.01.) Adding offset for all the `boxes` according to their class labels. (use `torch.arange(0 80)`.) For example  since the coordinates (x1 y1 x2 y2) of all the boxes are on interval (0 1). By adding offset  if a box belongs to class 61  its coordinates will on interval (60 61). After that  the IoU of boxes belonging to different classes will be 0. (because they are treated as different clusters.) Do Cluster-NMS and return top 100 boxes across all classes. (For this method  please refer to another our repository https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/detection/detection.py)   """;Computer Vision;https://github.com/Zzh-tju/CIoU
"""""";Computer Vision;https://github.com/yewzijian/RPMNet
"""""";Computer Vision;https://github.com/vanoracai/Exploiting-Spatial-temporal-Relationships-for-3D-Pose-Estimation-via-Graph-Convolutional-Networks
"""""";Sequential;https://github.com/vasilikivmo/repo2_Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs_amalkraj
"""""";General;https://github.com/WANG-KX/SIREN-2D
"""""";Reinforcement Learning;https://github.com/JustinStitt/acrobotDDQN
"""""";Computer Vision;https://github.com/yuanyuanli85/tf-hrnet
"""""";General;https://github.com/CharlotteMorrison/Baxter-Research
"""Attention: I started this work as I thought this would be a great idea and a good application of deep learning in a health setting. However  it turned out that the availability of (labelled) data is a major issue  as it is for a lot of ML projects. I did not succeed in getting access to enough data so that I stopped working on it. Based on the results  I'm pretty sure this should work  but you can't really tell unless you experiement with enough real world data. In September 2020  there was a publication on general wound classification (8) which has a broader look into this problem. This work also refers to the Medetec Wound Database (9).  If you are interested in continuing this work and have access to data  I'd be happy to exchange ideas.  One further idea was to not only to classify the burn wounds  but also to determine the percentage of the body that is burnt with a specific grade. Based on recent work around dense pose estimation (7) and 3D body part reconstruction  this should also be valid based on existing work.   """;Computer Vision;https://github.com/CarstenIsert/DeepBurn
"""""";Computer Vision;https://github.com/alexklibisz/deep-calcium
"""""";General;https://github.com/Nirvan101/Image-Restoration-deep-learning
"""""";Computer Vision;https://github.com/Lotayou/Face-Renovation
"""""";Computer Vision;https://github.com/wangtuo0820/maskrcnn-benchmark-expansion
"""""";Reinforcement Learning;https://github.com/ailab-pku/rl-framework
"""""";General;https://github.com/vivek20dadhich/deep-learning-object-detection
"""""";Computer Vision;https://github.com/ajoshi944/Segmentation-severstal-steel
"""""";Natural Language Processing;https://github.com/jackbandy/deep_learning_ulmfit
"""""";Computer Vision;https://github.com/amogh7joshi/plant-health-detection
"""""";Computer Vision;https://github.com/surajkarki66/Generative-Model-Deep-Learning
"""""";Computer Vision;https://github.com/imistyrain/ssd
"""""";General;https://github.com/rohitkuk/Cartoonify
"""""";Computer Vision;https://github.com/Jeongseungwoo/U_net
"""""";Computer Vision;https://github.com/AlexanderBogatko/Keras-CCVAE
"""""";Computer Vision;https://github.com/tjwei/stylegan2_workshop
"""""";General;https://github.com/davidtellez/contrastive-predictive-coding
"""""";Computer Vision;https://github.com/vvvm23/vqvae-2
"""""";General;https://github.com/comicencyclo/TransferLearning_DiscriminativeFineTuning
"""""";General;https://github.com/ClaraBing/flow
"""""";General;https://github.com/pradyu1/Defect-Classification
"""""";Computer Vision;https://github.com/SiskonEmilia/StyleGAN-PyTorch
"""""";General;https://github.com/samisoto/keras_cosine_based_loss
"""""";Computer Vision;https://github.com/lavish619/MLP-Mixer-PyTorch
"""""";Natural Language Processing;https://github.com/chunghyunhee/twitter_disaster_NLP
"""""";General;https://github.com/recluse27/Colorizator
"""The *Show and Tell* model is a deep neural network that learns how to describe the content of images. For example:  ![Example captions](g3doc/example_captions.jpg)   """;Computer Vision;https://github.com/SophiaYuSophiaYu/ImageCaption
"""""";General;https://github.com/DaneyAlex5/Deep-Zooming-of-Images-using-SRGAN
"""""";Computer Vision;https://github.com/ahmedbhna/VGG_Paper
"""**Adaptive Notes Generator** *is a tool that helps us attend online classes effectively:star_struck:. Due to the Online class culture  taking notes in pen and paper is not a good idea  the only options left are to click screenshots or struggle to note down everything in your notebook:unamused:. Our application will make your life easier  once a meeting video:film_projector: is provided  we will create the notes that will save you time:stopwatch: of research and gathering resources. We will divide your meeting into useful segments and add additional data to make it easy to understand any concept.:bookmark_tabs::bookmark_tabs:*   """;Sequential;https://github.com/KushGrandhi/Polaroid
"""""";Computer Vision;https://github.com/Zachdr1/DCGAN
"""""";Computer Vision;https://github.com/mvenouziou/Project-Attention-Is-What-You-Get
"""""";General;https://github.com/bhumigodiwala/Face-Mask-Detection
"""""";General;https://github.com/stiasta/fraud_detection_notes
"""""";Computer Vision;https://github.com/uladzislau-varabei/progressively_growing_gan
"""""";General;https://github.com/KellyHwong/rethinking_generalization
"""""";Computer Vision;https://github.com/aim-uofa/AdelaiDet
"""CenterNet is a framework for object detection with deep convolutional neural networks. You can use the code to train and evaluate a network for object detection on the MS-COCO dataset.  * It achieves state-of-the-art performance (an AP of 47.0%) on one of the most challenging dataset: MS-COCO.  * Our code is written in Python  based on [CornerNet](https://github.com/princeton-vl/CornerNet).  *More detailed descriptions of our approach and code will be made available soon.*  **If you encounter any problems in using our code  please contact Kaiwen Duan: kaiwen.duan@vipl.ict.ac.cn.**   """;Computer Vision;https://github.com/DaiJianBo/CenterNet-duan-2080Ti
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/iRmantou/car_bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/vanpersie32/Multigpu-Bert
"""""";Computer Vision;https://github.com/daymos/simple_keras_GAN
"""""";Natural Language Processing;https://github.com/drumpt/Transformer
"""""";Natural Language Processing;https://github.com/bharat3012/Word-Representation-Vector-Space
"""""";Computer Vision;https://github.com/Uemuet/-gan
"""""";Computer Vision;https://github.com/FelixGruen/tensorflow-u-net
"""""";Reinforcement Learning;https://github.com/ugo-nama-kun/DQN-chainer
"""""";Computer Vision;https://github.com/hisiter97/darknet
"""""";Computer Vision;https://github.com/francis-kang/handson-unsupervised-learning
"""""";Computer Vision;https://github.com/xurui1217/pointnet.pytorch-master
"""""";Natural Language Processing;https://github.com/smallbenchnlp/ELECTRA-DeBERTa
"""""";General;https://github.com/Qengineering/NanoDet-ncnn-Raspberry-Pi-4
"""""";General;https://github.com/pipilurj/BONAS
"""""";Graphs;https://github.com/phanein/deepwalk
"""CascadePSP is a deep learning model for high-resolution segmentation refinement. This repository contains our PyTorch implementation with both training and testing functionalities. We also provide the annotated UHD dataset **BIG** and the pretrained model.  Here are some refinement results on high-resolution images. ![teaser](docs/images/teaser.jpg)   """;Computer Vision;https://github.com/hkchengrex/CascadePSP
"""""";Sequential;https://github.com/Kyubyong/quasi-rnn
"""""";General;https://github.com/PatriciaRodrigues1994/pytorch-facebook-udacity-challenge
"""""";Computer Vision;https://github.com/bshall/VectorQuantizedCPC
"""""";Computer Vision;https://github.com/Bingwen-Hu/DRIT
"""""";Computer Vision;https://github.com/twtygqyy/pytorch-SRResNet
"""""";General;https://github.com/jeromerony/adversarial-library
"""""";Computer Vision;https://github.com/pessimiss/ai100-w8-master
"""""";Natural Language Processing;https://github.com/UdbhavPrasad072300/Transformer-Implementations
"""""";Computer Vision;https://github.com/pokurin123/pix2pix_try
"""""";Computer Vision;https://github.com/KatherLab/HIA
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/qilei123/fpn_crop_v1_5d
"""""";General;https://github.com/llppff/ptb-lstmorqrnn-pytorch
"""""";General;https://github.com/CVxTz/face_age_gender
"""""";Reinforcement Learning;https://github.com/markub3327/rl-toolkit
"""![FeatureVis](assets/FeatureVis.png)  In this work  we design a new loss function which merges the merits of both [NormFace](https://github.com/happynear/NormFace) and [SphereFace](https://github.com/wy1iu/sphereface). It is much easier to understand and train  and outperforms the previous state-of-the-art loss function (SphereFace) by 2-5% on MegaFace.    """;General;https://github.com/happynear/AMSoftmax
"""""";General;https://github.com/GXYM/Focal-loss
"""""";General;https://github.com/Celiali/FixMatch
"""""";Computer Vision;https://github.com/aleksispi/drl-rpn-tf
"""""";Computer Vision;https://github.com/Hashir44/muraxray
"""""";Natural Language Processing;https://github.com/haianhle/ULMFiT-Sentiment
"""""";Computer Vision;https://github.com/CVxTz/face_age_gender
"""""";Audio;https://github.com/mostafaelaraby/wavegan-pytorch
"""""";General;https://github.com/gryan12/chat-bot
"""""";General;https://github.com/yaxingwang/Transferring-GANs
"""""";Computer Vision;https://github.com/addtt/ladder-vae-pytorch
"""""";Computer Vision;https://github.com/Sakib1263/Inception-InceptionResNet-1D-2D-Tensorflow-Keras
"""""";General;https://github.com/yeoedward/Neural-Turing-Machine
"""""";Graphs;https://github.com/gordicaleksa/pytorch-GAT
"""""";General;https://github.com/Shraddha2013/customyolo
"""""";General;https://github.com/xytpai/DetX-Retinanet
"""""";General;https://github.com/tcheung99/ResNet_MiniProject
"""""";General;https://github.com/pfnet-research/tgan2
"""""";Computer Vision;https://github.com/akanimax/pro_gan_pytorch
"""""";Computer Vision;https://github.com/misogil0116/Biscotti_misc
"""""";General;https://github.com/littell/NamedTransformer
"""""";Reinforcement Learning;https://github.com/jonahs99/baby-zero
"""BERT-base and BERT-large are respectively 110M and 340M parameters models and it can be difficult to fine-tune them on a single GPU with the recommended batch size for good performance (in most case a batch size of 32).  To help with fine-tuning these models  we have included several techniques that you can activate in the fine-tuning scripts [`run_classifier.py`](./examples/run_classifier.py) and [`run_squad.py`](./examples/run_squad.py): gradient-accumulation  multi-gpu training  distributed training and 16-bits training . For more details on how to use these techniques you can read [the tips on training large batches in PyTorch](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255) that I published earlier this month.  Here is how to use these techniques in our scripts:  - **Gradient Accumulation**: Gradient accumulation can be used by supplying a integer greater than 1 to the `--gradient_accumulation_steps` argument. The batch at each step will be divided by this integer and gradient will be accumulated over `gradient_accumulation_steps` steps. - **Multi-GPU**: Multi-GPU is automatically activated when several GPUs are detected and the batches are splitted over the GPUs. - **Distributed training**: Distributed training can be activated by supplying an integer greater or equal to 0 to the `--local_rank` argument (see below). - **16-bits training**: 16-bits training  also called mixed-precision training  can reduce the memory requirement of your model on the GPU by using half-precision training  basically allowing to double the batch size. If you have a recent GPU (starting from NVIDIA Volta architecture) you should see no decrease in speed. A good introduction to Mixed precision training can be found [here](https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/) and a full documentation is [here](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html). In our scripts  this option can be activated by setting the `--fp16` flag and you can play with loss scaling using the `--loss_scale` flag (see the previously linked documentation for details on loss scaling). The loss scale can be zero in which case the scale is dynamically adjusted or a positive power of two in which case the scaling is static.  To use 16-bits training and distributed training  you need to install NVIDIA's apex extension [as detailed here](https://github.com/nvidia/apex). You will find more information regarding the internals of `apex` and how to use `apex` in [the doc and the associated repository](https://github.com/nvidia/apex). The results of the tests performed on pytorch-BERT by the NVIDIA team (and my trials at reproducing them) can be consulted in [the relevant PR of the present repository](https://github.com/huggingface/pytorch-pretrained-BERT/pull/116).  Note: To use *Distributed Training*  you will need to run one training script on each of your machines. This can be done for example by running the following command on each server (see [the above mentioned blog post]((https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)) for more details): ```bash python -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=$THIS_MACHINE_INDEX --master_addr=""192.168.1.1"" --master_port=1234 run_classifier.py (--arg1 --arg2 --arg3 and all other arguments of the run_classifier script) ``` Where `$THIS_MACHINE_INDEX` is an sequential index assigned to each of your machine (0  1  2...) and the machine with rank 0 has an IP address `192.168.1.1` and an open port `1234`.   """;Natural Language Processing;https://github.com/cedrickchee/pytorch-pretrained-BERT
"""""";General;https://github.com/MADONOKOUKI/Block-wise-Scrambled-Image-Recognition
"""""";General;https://github.com/Tony-Y/pytorch_warmup
"""""";Natural Language Processing;https://github.com/G-4-R-Y/Tweet-Sentiment-Extraction-roBERTa-5fold
"""""";Computer Vision;https://github.com/tsunekoromochi/ssd
"""""";Computer Vision;https://github.com/rohitkuk/AnimeGAN
"""""";Audio;https://github.com/dipjyoti92/TTS-Style-Transfer
"""""";Computer Vision;https://github.com/JimmyDoan1309/GAN-DCGAN
"""""";Computer Vision;https://github.com/sprenkle/VectorCards
"""""";Reinforcement Learning;https://github.com/hepengli/multiagent-particle-envs
"""""";General;https://github.com/lonce/sonyGanFork
"""""";Computer Vision;https://github.com/albertomontesg/keras-model-zoo
"""""";Computer Vision;https://github.com/matteodalessio/CycleGAN-android
"""The objective of this project is to learn more about conditional generative models. Having worked with GANs  it seems beneficial to study more about adding additional descriptive information with the input image to produce models that are able to distinctly represent specific subjects in the generated data. It seems to be a part of how users can select specific features or labels for the model to generate. As an early step of looking at this and taking into account the limitations of resources and time  this project will be experimenting with the vanilla variational autoencoder and a conditional variational autoencoder.    """;Computer Vision;https://github.com/jenyliu/DLA_interview
"""English | [简体中文](README_zh-CN.md)  [![build](https://github.com/open-mmlab/mmocr/workflows/build/badge.svg)](https://github.com/open-mmlab/mmocr/actions) [![docs](https://readthedocs.org/projects/mmocr/badge/?version=latest)](https://mmocr.readthedocs.io/en/latest/?badge=latest) [![codecov](https://codecov.io/gh/open-mmlab/mmocr/branch/main/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmocr) [![license](https://img.shields.io/github/license/open-mmlab/mmocr.svg)](https://github.com/open-mmlab/mmocr/blob/main/LICENSE) [![PyPI](https://badge.fury.io/py/mmocr.svg)](https://pypi.org/project/mmocr/) [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmocr.svg)](https://github.com/open-mmlab/mmocr/issues) [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmocr.svg)](https://github.com/open-mmlab/mmocr/issues)  MMOCR is an open-source toolbox based on PyTorch and mmdetection for text detection  text recognition  and the corresponding downstream tasks including key information extraction. It is part of the [OpenMMLab](https://openmmlab.com/) project.  The main branch works with **PyTorch 1.6+**.  Documentation: https://mmocr.readthedocs.io/en/latest/.  <div align=""left"">   <img src=""resources/illustration.jpg""/> </div>   """;Computer Vision;https://github.com/open-mmlab/mmocr
"""The implementation is based on two papers:  - Simple Online and Realtime Tracking with a Deep Association Metric https://arxiv.org/abs/1703.07402 - YOLOv4: Optimal Speed and Accuracy of Object Detection https://arxiv.org/pdf/2004.10934.pdf   This repository contains a moded version of PyTorch YOLOv5 (https://github.com/ultralytics/yolov5). It filters out every detection that is not a person. The detections of persons are then passed to a Deep Sort algorithm (https://github.com/ZQPei/deep_sort_pytorch) which tracks the persons. The reason behind the fact that it just tracks persons is that the deep association metric is trained on a person ONLY datatset.   """;Computer Vision;https://github.com/ElipLam/IceBlock_Detection
"""""";General;https://github.com/idealwhite/tdanet
"""""";General;https://github.com/mminamina/Transfer_Learning_ResNet18---Keypoints_Detection
"""""";General;https://github.com/huyz1117/Non_Local_Net_TensorFlow
"""""";General;https://github.com/youngwanLEE/centermask2
"""""";General;https://github.com/Net-Mist/style-transfer-tf2
"""""";Computer Vision;https://github.com/fizyr/keras-retinanet
"""""";Computer Vision;https://github.com/Oichii/DeepPulse-pytorch
"""""";Natural Language Processing;https://github.com/linkfluence/fastText4j
"""""";Graphs;https://github.com/RausellLab/Recommended_learning_ressources
"""""";General;https://github.com/sfujim/TD3
"""""";Natural Language Processing;https://github.com/nyu-dl/dl4mt-cdec
"""""";Computer Vision;https://github.com/maggie0106/Graph-CNN-in-3D-Point-Cloud-Classification
"""""";Computer Vision;https://github.com/sayakpaul/Adaptive-Gradient-Clipping
"""""";General;https://github.com/dronefreak/human-action-classification
"""""";Computer Vision;https://github.com/Atmosphere-art/Self-Attention-GAN
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/ml-inory/Caffe
"""""";Computer Vision;https://github.com/witwickey/alexeyDarknet
"""""";General;https://github.com/BrookInternSOMA/pix2pix-barcode
"""""";Computer Vision;https://github.com/samuelmat19/GLOW-tf2
"""""";Computer Vision;https://github.com/trongnghia00/darknet
"""""";Computer Vision;https://github.com/PierrickPochelu/word_tree_label
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/xyfer17/ssd-caffe
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/longbowking/bert
"""""";Computer Vision;https://github.com/duxingren14/Hifill-tensorflow
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/chandu7077/mybert
"""The demos in this folder are designed to give straightforward samples of using TensorFlow in mobile applications.  Inference is done using the [TensorFlow Android Inference Interface](../../../tensorflow/contrib/android)  which may be built separately if you want a standalone library to drop into your existing application. Object tracking and YUV -> RGB conversion is handled by libtensorflow_demo.so.  A device running Android 5.0 (API 21) or higher is required to run the demo due to the use of the camera2 API  although the native libraries themselves can run on API >= 14 devices.   """;General;https://github.com/Adren98/EczemaApp
"""""";General;https://github.com/BenjaminGonzalez/BamCode
"""""";General;https://github.com/JianGoForIt/YellowFin
"""""";Computer Vision;https://github.com/spencerkraisler/Finger-Counter
"""The objective of the project is to train the agents to play Tennis using the [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment.   ![tennis](images/trained_agent.gif)  """;Reinforcement Learning;https://github.com/vgudapati/DRLND_Collaboration_Competetion
"""""";Computer Vision;https://github.com/jparcill/gansbestfriend
"""""";General;https://github.com/sroikl/MoCo
"""""";General;https://github.com/Yimin-Yuan/VeinProject
"""""";Computer Vision;https://github.com/LsAntonio/60daysofudacity
"""""";Graphs;https://github.com/HeapHop30/graph-attention-nets
"""""";Computer Vision;https://github.com/stephenharris/yolo-walkthrough
"""""";Computer Vision;https://github.com/JaryV/CycleGAN_OldYoung
"""""";Computer Vision;https://github.com/cyberjam/darknet_submit
"""""";Computer Vision;https://github.com/peteryuX/arcface-tf2
"""""";Computer Vision;https://github.com/paperswithcode/paperswithcode-client
"""""";General;https://github.com/bcmi220/esc4nmt
"""""";Reinforcement Learning;https://github.com/fshamshirdar/pytorch-rdpg
"""""";General;https://github.com/arnomoonens/yarll
"""""";Reinforcement Learning;https://github.com/IvanVigor/Deep-Deterministic-Policy-Gradient-Unity-Env
"""""";Graphs;https://github.com/zhangbo2008/GAT_network
"""""";Reinforcement Learning;https://github.com/Crevass/Hybrid-Agent
"""""";Reinforcement Learning;https://github.com/taku-y/20181125-pybullet
"""""";General;https://github.com/albertopolito/CarSNN
"""""";Audio;https://github.com/TanUkkii007/wavenet
"""""";General;https://github.com/IrishCoffee/cudnnMultiHeadAttention
"""""";General;https://github.com/riven314/PerceptualLoss-FastAI
"""""";Audio;https://github.com/Gal1eo/DT2119
"""""";General;https://github.com/cbritom/GAN
"""""";Natural Language Processing;https://github.com/dksifoua/Neural-Machine-Translation
"""""";General;https://github.com/blackPacha/VAE_ABIDE1
"""""";Computer Vision;https://github.com/lukemelas/EfficientNet-PyTorch
"""""";General;https://github.com/JCBrouwer/maua-stylegan2
"""""";Computer Vision;https://github.com/RuiminChen/GIouloss_CIouloss_caffe
"""""";General;https://github.com/yenchenlin/pix2pix-tensorflow
"""""";Reinforcement Learning;https://github.com/GauravPatel89/Car-Navigation-Simulation-using-TD3
"""Official Implementation of our pSp paper for both training and evaluation. The pSp method extends the StyleGAN model to  allow solving different image-to-image translation problems using its encoder.   """;Computer Vision;https://github.com/liuliuliu11/pixel2style2pixel-liu
"""""";General;https://github.com/phanav/simclr-presentation
"""""";General;https://github.com/nickfrosst/neural_additive_models
"""""";Reinforcement Learning;https://github.com/hermgerm29/learn-to-race
"""""";General;https://github.com/vietvulong/Python-People-Counting-in-Real-Time-master
"""""";Computer Vision;https://github.com/LignumResearch/stylewood-model-usage
"""We've desinged a novel neural architecture search framework for generative adversarial networks (GANs)  dubbed AutoGAN. Experiments validate the effectiveness of AutoGAN on the task of unconditional image generation. Specifically  our discovered architectures achieve highly competitive performance on unconditional image generation task of CIFAR-10  which obtains a record FID score of **12.42**  a competitive Inception score of **8.55**.   **RNN controller:** <p align=""center"">   <img src=""imgs/ctrl.png"" alt=""ctrl"" width=""90%""> </p>  **Search space:** <p align=""center"">   <img src=""imgs/ss.png"" alt=""ss"" width=""30%""> </p>  **Discovered network architecture:** <p align=""center"">   <img src=""imgs/cifar_arch1.png"" alt=""cifar_arch1"" width=""75%""> </p>   """;General;https://github.com/VITA-Group/AutoGAN
"""""";Computer Vision;https://github.com/cuonga1cvp/yolov3_cuong
"""This is the Tensorflow code corresponding to [A Two-Stage Method for Text Line Detection in Historical Documents ](#a-two-stage-method-for-text-line-detection-in-historical-documents). This repo contains the neural pixel labeling part described in the paper. It contains the so-called ARU-Net (among others) which is basically an extended version of the well known U-Net [[2]](#u-net-convolutional-networks-for-biomedical-image-segmentation).  Besides the model and the basic workflow to train and test models  different data augmentation strategies are implemented to reduce the amound of training data needed. The repo's features are summarized below: + Inference Demo     + Trained and freezed tensorflow graph included     + Easy to reuse for own inference tests + Workflow      + Full training workflow to parametrize and train your own models     + Contains different models  data augmentation strategies  loss functions      + Training on specific GPU  this enables the training of several models on a multi GPU system in parallel     + Easy validation for trained model either using classical or ema-shadow weights  Please cite [[1]](#a-two-stage-method-for-text-line-detection-in-historical-documents) if you find this repo useful and/or use this software for own work.    """;Computer Vision;https://github.com/TobiasGruening/ARU-Net
"""[ImageNet](http://www.image-net.org/) is a common academic data set in machine learning for training an image recognition system. Code in this directory demonstrates how to use TensorFlow to train and evaluate a type of convolutional neural network (CNN) on this data set.      http://arxiv.org/abs/1512.00567      This network achieves 21.2% top-1 and 5.6% top-5 error for single frame evaluation with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. Below is a visualization of the model architecture.   ![image](https://github.com/r-karthik/images/blob/master/detection_of_pests/layers.png)            """;Computer Vision;https://github.com/r-karthik/Detection-of-pests
"""""";Audio;https://github.com/nicobernasconi/specgan
"""""";Graphs;https://github.com/galkampel/HyperNetworks
"""""";General;https://github.com/ayushdabra/dubai-satellite-imagery-segmentation
"""""";Computer Vision;https://github.com/freddieee/pn_6d_single
"""""";General;https://github.com/slundberg/shap
"""""";General;https://github.com/TuringLang/Bijectors.jl
"""""";Computer Vision;https://github.com/thodorostsao/4-leg-robot-neural-network
"""""";Audio;https://github.com/as-ideas/ForwardTacotron
"""""";Computer Vision;https://github.com/Punakshi/Deep-Residual-Learning-for-Image-Recognition-Implementation
"""""";General;https://github.com/kuan-wang/pytorch-mobilenet-v3
"""""";Computer Vision;https://github.com/facebookresearch/convit
"""""";Computer Vision;https://github.com/alexey-pronkin/annealed
"""""";General;https://github.com/facebookresearch/dino
""" - Demo.ipynb - allows you to check enviroment and see working agent example  - Solver.ipynb - reproduces the training procedure  - agent.py - TD3 agent implementation  - networks.py - actor and critic Pytorch definitions  - replay_byffer.py - Replay Buffer implementation from OpenAI Baselines   - actor.pth - Saved weights for Actor network from TD3  - critic.pth - Saved weights from Critic networks from TD3    """;General;https://github.com/crazyleg/TD3-reacher
"""""";General;https://github.com/u7javed/Transformer-Multi-Language-Translator
"""""";Audio;https://github.com/PeihaoChen/regnet
"""""";Graphs;https://github.com/iwzy7071/graph_neural_network
"""""";Computer Vision;https://github.com/chriswxho/dynamic-inference
"""""";Computer Vision;https://github.com/eps696/aphantasia
"""Traditional [WGAN](https://arxiv.org/abs/1701.07875) uses an approximation of the Wasserstein metric to opimize the generator. This Wasserstein metric in turn depends upon an underlying metric on _images_ which is taken to be the <img src=""https://latex.codecogs.com/svg.latex?%5Cell%5E2""> norm  <img src=""https://latex.codecogs.com/svg.latex?%5C%7Cx%5C%7C_%7B2%7D%20%3D%20%5Cleft%28%20%5Csum_%7Bi%3D1%7D%5En%20x_i%5E2%20%5Cright%29%5E%7B1/2%7D"">  The article extends the theory of [WGAN-GP](https://arxiv.org/abs/1704.00028) to any [Banach space](https://en.wikipedia.org/wiki/Banach_space)  while this code can be used to train WGAN over any [Sobolev space](https://en.wikipedia.org/wiki/Sobolev_space) <img src=""https://latex.codecogs.com/svg.latex?W%5E%7Bs%2C%20p%7D""> with norm  <img src=""https://latex.codecogs.com/svg.latex?%5C%7Cf%5C%7C_%7BW%5E%7Bs%2C%20p%7D%7D%20%3D%20%5Cleft%28%20%5Cint_%7B%5COmega%7D%20%5Cleft%28%20%5Cmathcal%7BF%7D%5E%7B-1%7D%20%5Cleft%5B%20%281%20&plus;%20%7C%5Cxi%7C%5E2%29%5E%7Bs/2%7D%20%5Cmathcal%7BF%7D%20f%20%5Cright%5D%28x%29%20%5Cright%29%5Ep%20dx%20%5Cright%29%5E%7B1/p%7D"">  The parameters _p_ can be used to control the focus on outliers  with high _p_ indicating a strong focus on the worst offenders. _s_ can be used to control focus on small/large scale behaviour  where negative _s_ indicates focus on large scales  while positive _s_ indicates focus on small scales (e.g. edges).   """;Computer Vision;https://github.com/adler-j/bwgan
"""""";General;https://github.com/wtingda/DeepRLBreakout
"""""";General;https://github.com/SpikeKing/mobilenet_v3
"""""";General;https://github.com/fastforwardlabs/learning-to-learn
"""""";General;https://github.com/facebookresearch/qhoptim
"""""";General;https://github.com/floraxinru/NLP_HotelReviews
"""""";Computer Vision;https://github.com/vijayvee/behavior_recognition
"""""";Reinforcement Learning;https://github.com/JuanCCS/muzero-jc
"""The model is used to tackle the alpha matting problem. It is basically an encoder-decoder deep neural network. By feed the network an original image with tri-map  you can get the prediction alpha of the image.       Deep Image Matting     Ning Xu  Brian Price  Scott Cohen  and Thomas Huang.      CVPR  2017.  The input images should be mean pixel subtraction. And the channel should be BGR.   """;Computer Vision;https://github.com/ShawnNew/shape-alpha-matting
"""**Deep Vision Transformer** is initially described in [arxiv](https://arxiv.org/abs/2103.11886)  which observes the attention collapese phenomenon when training deep vision transformers: In this paper  we show that  unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers  the performance of ViTs saturate fast when scaled to be deeper. More specifically  we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper  the attention maps gradually become similar and even much the same after certain layers. In other words  the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs  the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation  we propose a simple yet effective method  named Re-attention  to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably  when training a deep ViT model with 32 transformer blocks  the Top-1 classification accuracy can be improved by 1.6% on ImageNet.  <p align=""center""> <img src=""https://github.com/zhoudaquan/DeepViT_ICCV21/blob/master/figures/performance_comparison.png"" | width=500> </p>   """;General;https://github.com/zhoudaquan/dvit_repo
"""""";Computer Vision;https://github.com/potterhsu/easy-faster-rcnn.pytorch
"""""";General;https://github.com/bojone/keras_lookahead
"""""";General;https://github.com/kuixu/Linear-Multihead-Attention
"""""";Computer Vision;https://github.com/tamasino52/UNETR
"""""";Computer Vision;https://github.com/sducournau/caffe-segnet
"""""";General;https://github.com/XuezheMax/fairseq-apollo
"""""";General;https://github.com/vinay-EIP/EIP-session-5-assignment
"""""";Computer Vision;https://github.com/sanghviyashiitb/GANS-VanillaAndMinibatchDiscrimination
"""""";General;https://github.com/snowkylin/ntm
"""""";Computer Vision;https://github.com/meryusha/seeds_faster
"""""";Computer Vision;https://github.com/Vishal-V/StackGAN
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Shinya-Kouda/kgc
"""""";General;https://github.com/vinthony/s2am
"""""";Computer Vision;https://github.com/EdenMelaku/Transfer-Learning-Pytorch-Implementation
"""""";General;https://github.com/ayoolaolafenwa/orangelib
"""""";General;https://github.com/1170500804/MoCo
"""""";General;https://github.com/NVlabs/SPADE
"""""";General;https://github.com/btekgit/FocusingNeuron-Keras
"""""";Computer Vision;https://github.com/NCNU-OpenSource/What-are-you-
"""""";General;https://github.com/ducha-aiki/LSUV-pytorch
"""""";General;https://github.com/ttt496/vit-pytorch
"""""";Computer Vision;https://github.com/JihaoLee/Randomly_Wired_reproducibility
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/AlvesRobot/Deep-Learning-on-Point-Cloud-for-3D-Classification-and-Segmentation
"""""";Computer Vision;https://github.com/soeaver/py-RFCN-priv
"""""";Computer Vision;https://github.com/s9mondal9upriti/U-Net
"""""";Computer Vision;https://github.com/T-C-J/GAN
"""""";General;https://github.com/Chinnu1103/Machine-Translation-using-Transformers
"""""";General;https://github.com/ronghuaiyang/arcface-pytorch
"""""";Sequential;https://github.com/SourangshuGhosh/NeuralTuringMachine
"""""";Reinforcement Learning;https://github.com/braemt/attentive-multi-task-deep-reinforcement-learning
"""""";Natural Language Processing;https://github.com/ypeleg/MinimalIsAllYouNeed
"""""";Computer Vision;https://github.com/RenYang-home/OpenDVC
"""""";General;https://github.com/Mayurji/SimCLR
"""""";General;https://github.com/heye0507/individualNLPClassifier
"""""";General;https://github.com/ewdowiak/Sicilian_Translator
"""""";General;https://github.com/DmitryUlyanov/texture_nets
"""This is the code repo of our TMM 2019 work titled  [""COMIC: Towards A Compact Image Captioning Model with Attention""](https://arxiv.org/abs/1903.01072).  In this paper  we tackle the problem of compactness of image captioning models which is hitherto unexplored.  We showed competitive results on both MS-COCO and InstaPIC-1.1M datasets despite having an embedding vocabularly size that is 39x-99x smaller.  <img src=""TMM.png"" height=""200"">  **Some pre-trained model checkpoints are available at  [this repo](https://github.com/jiahuei/COMIC-Pretrained-Captioning-Models).**    Updated on 25 Feb 2021: [Object Relation Transformer](https://papers.nips.cc/paper/9293-image-captioning-transforming-objects-into-words.pdf)  with Radix encoding that can achieve CIDEr score of 1.291 after SCST training. [Code at this repo](https://github.com/jiahuei/sparse-image-captioning).  Updated on 12 June 2019: Self-Critical Sequence Training (SCST)  Updated on 06 June 2019: Pre-trained model repo  Released on 03 June 2019.    """;General;https://github.com/jiahuei/COMIC-Compact-Image-Captioning-with-Attention
"""""";General;https://github.com/PaddlePaddle/Quantum
"""""";Computer Vision;https://github.com/mackitron/COVID19
"""""";Computer Vision;https://github.com/MINED30/Face_Mask_Detection_YOLO
"""""";Computer Vision;https://github.com/SwinTransformer/Transformer-SSL
"""""";Natural Language Processing;https://github.com/abhineet/sentence_classification_pubmed_scibert
"""""";General;https://github.com/cmhungsteve/SSTDA
"""Welcome to the code repository of [How to train your MAML](https://arxiv.org/abs/1810.09502). This repository includes code for training both MAML and MAML++ models  as well as data providers and the datasets for both. By using this codebase you agree to the terms  and conditions in the [LICENSE](https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/blob/master/LICENSE) file. If you choose to use the Mini-Imagenet dataset  you must abide by the terms and conditions in the [ImageNet LICENSE](https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/blob/master/imagenet_license.md)   """;General;https://github.com/dkalpakchi/ReproducingSCAPytorch
"""""";Reinforcement Learning;https://github.com/wangyy161/DDPG_CNN_Pendulum_practice
"""""";General;https://github.com/antoinecollas/transformer_neural_machine_translation
"""""";General;https://github.com/majing2019/transformer
"""""";General;https://github.com/SeoroMin/transformer_pytorch_ver2
"""In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   """;Natural Language Processing;https://github.com/yitu-opensource/ConvBert
"""The demand of applying semantic segmentation model on mobile devices has been increasing rapidly. Current state-of-the-art networks have enormous amount of parameters  hence unsuitable for mobile devices  while other small memory footprint models follow the spirit of classification network and ignore the inherent characteristic of semantic segmentation. To tackle this problem  we propose a novel Context Guided Network (CGNet)  which is a light-weight and efficient network for semantic segmentation. We first propose the Context Guided (CG) block  which learns the joint feature of both local feature and surrounding context  and further improves the joint feature with the global context. Based on the CG block  we develop CGNet which captures contextual information in all stages of the network and is specially tailored for increasing segmentation accuracy. CGNet is also elaborately designed to reduce the number of parameters and save memory footprint. Under an equivalent number of parameters  the proposed CGNet significantly outperforms existing segmentation networks. Extensive experiments on Cityscapes and CamVid datasets verify the effectiveness of the proposed approach. Specifically  without any post-processing and multi-scale testing  the proposed CGNet achieves 64.8% mean IoU on Cityscapes with less than 0.5 M parameters.    """;Computer Vision;https://github.com/wutianyiRosun/CGNet
"""""";Computer Vision;https://github.com/lucidrains/g-mlp-gpt
"""""";General;https://github.com/titu1994/neural-image-assessment
"""""";General;https://github.com/shashigharti/secure-privateai-scholarship-challenge
"""""";Computer Vision;https://github.com/qsyao/cuda_spatial_deform
"""""";Computer Vision;https://github.com/BenjiKCF/EfficientNet
"""""";General;https://github.com/QingruZhang/AdaShift_Release
"""""";General;https://github.com/snipsco/ntm-lasagne
"""""";Computer Vision;https://github.com/BingkAI-B21CAP0161/C-Mask-Machine-Learning
"""""";General;https://github.com/hussain18804/toonify-poc
"""""";Computer Vision;https://github.com/siyliepfl/deformation-aware-unpaired-image-translation
"""> A team of radiologists from New Orleans studied the usefulness of Chest Radiographs for diagnosing COVID-19 compared to the reverse-transcription polymerase chain reaction (RT-PCR) and found out they could aid rapid diagnosis  especially in areas with limited testing facilities [[1]](https://pubs.rsna.org/doi/10.1148/ryct.2020200280 ""A Characteristic Chest Radiographic Pattern in the Setting of the COVID-19 Pandemic"").<br> > Another study found out that the radiographs of different viral cases of pneumonia are comparative  and they overlap with other infectious and inflammatory lung diseases  making it hard for radiologists to recognize COVID‐19 from other viral pneumonia cases [[2]](https://pubs.rsna.org/doi/10.1148/rg.2018170048 ""Radiographic and CT Features of Viral Pneumonia"").<br> > This project aims to make the former study a reality while dealing with the intricacies in the latter  with the help of Deep Learning.<br>   """;Computer Vision;https://github.com/priyavrat-misra/xrays-and-gradcam
"""""";General;https://github.com/zjunlp/KnowPrompt
"""""";Natural Language Processing;https://github.com/jayleicn/ClipBERT
"""""";Computer Vision;https://github.com/nolanliou/PeopleSegmentationDemo
"""""";General;https://github.com/facebookresearch/mobile-vision
"""""";General;https://github.com/floydhub/word-language-model
"""Vega is an AutoML algorithm tool chain developed by Noah's Ark Laboratory  the main features are as follows:  1. Full pipeline capailities: The AutoML capabilities cover key functions such as Hyperparameter Optimization  Data Augmentation  Network Architecture Search (NAS)  Model Compression  and Fully Train. These functions are highly decoupled and can be configured as required  construct a complete pipeline. 2. Industry-leading AutoML algorithms: Provides Noah's Ark Laboratory's self-developed **[industry-leading algorithm (Benchmark)](./docs/benchmark.md)** and **[Model Zoo](./docs/model_zoo.md)** to download the state-of-the-art (SOTA) models. 3. Fine-grained network search space: The network search space can be freely defined  and rich network architecture parameters are provided for use in the search space. The network architecture parameters and model training hyperparameters can be searched at the same time  and the search space can be applied to Pytorch  TensorFlow and MindSpore. 4. High-concurrency neural network training capability: Provides high-performance trainers to accelerate model training and evaluation. 5. Multi-Backend: PyTorch (GPU and Ascend 910)  TensorFlow (GPU and Ascend 910)  MindSpore (Ascend 910). 6. Ascend platform: Search and training on the Ascend 910 and model evaluation on the Ascend 310.   """;General;https://github.com/huawei-noah/vega
"""""";Computer Vision;https://github.com/clovaai/cutblur
"""Spriteworld is a python-based RL environment that consists of a 2-dimensional arena with simple shapes that can be moved freely. This environment was developed for the COBRA agent introduced in the paper [""COBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration"" (Watters et al.  2019)](https://arxiv.org/abs/1905.09275). The motivation for the environment was to provide as much flexibility for procedurally generating multi-object scenes while retaining as simple an interface as possible.  Spriteworld sprites come in a variety of shapes and can vary continuously in position  size  color  angle  and velocity. The environment has occlusion but no physics  so by default sprites pass beneath each other but do not collide or interact in any way. Interactions may be introduced through the action space  which can update all sprites each timestep. For example  the DiscreteEmbodied action space (see `spriteworld/action_spaces.py`) implements a rudimentary form of physics in which an agent's body sprite can adhere to and carry sprites underneath it.  There are a variety of action spaces  some of which are continuous (like a touch-screen) and others of which are discrete (like an embodied agent that takes discrete steps).   """;Computer Vision;https://github.com/deepmind/spriteworld
"""""";General;https://github.com/eatamath/metallic
"""DeepLab is a state-of-art deep learning system for semantic image segmentation built on top of [Caffe](http://caffe.berkeleyvision.org).  It combines (1) *atrous convolution* to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks  (2) *atrous spatial pyramid pooling* to robustly segment objects at multiple scales with filters at multiple sampling rates and effective fields-of-views  and (3) densely connected conditional random fields (CRF) as post processing.  This distribution provides a publicly available implementation for the key model ingredients reported in our latest [arXiv paper](http://arxiv.org/abs/1606.00915). This version also supports the experiments (DeepLab v1) in our ICLR'15. You only need to modify the old prototxt files. For example  our proposed atrous convolution is called dilated convolution in CAFFE framework  and you need to change the convolution parameter ""hole"" to ""dilation"" (the usage is exactly the same). For the experiments in ICCV'15  there are some differences between our argmax and softmax_loss layers and Caffe's. Please refer to [DeepLabv1](https://bitbucket.org/deeplab/deeplab-public/) for details.  Please consult and consider citing the following papers:      @article{CP2016Deeplab        title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets  Atrous Convolution  and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        journal={arXiv:1606.00915}        year={2016}     }      @inproceedings{CY2016Attention        title={Attention to Scale: Scale-aware Semantic Image Segmentation}        author={Liang-Chieh Chen and Yi Yang and Jiang Wang and Wei Xu and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{CB2016Semantic        title={Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform}        author={Liang-Chieh Chen and Jonathan T Barron and George Papandreou and Kevin Murphy and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{PC2015Weak        title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation}        author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille}        booktitle={ICCV}        year={2015}     }      @inproceedings{CP2015Semantic        title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        booktitle={ICLR}        year={2015}     }   Note that if you use the densecrf implementation  please consult and cite the following paper:      @inproceedings{KrahenbuhlK11        title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}        author={Philipp Kr{\""{a}}henb{\""{u}}hl and Vladlen Koltun}        booktitle={NIPS}        year={2011}     }   """;Computer Vision;https://github.com/open-cv/deeplab-v2
"""""";Natural Language Processing;https://github.com/Omerktn/fastText-iterative
""" The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.    """;Computer Vision;https://github.com/GodofCCode/yolo3
"""""";Computer Vision;https://github.com/raphael-cohen/capsnet_for_Faster-RCNN
"""""";Computer Vision;https://github.com/ChienliMa/DeConvNet
"""""";General;https://github.com/marcotcr/lime-experiments
"""Inspired by the structure of Receptive Fields (RFs) in human visual systems  we propose a novel RF Block (RFB) module  which takes the relationship between the size and eccentricity of RFs into account  to enhance the discriminability and robustness of features. We further  assemble the RFB module to the top of SSD with a lightweight CNN model  constructing the RFB Net detector. You can use the code to train/evaluate the RFB Net for object detection. For more details  please refer to our [arXiv paper](https://arxiv.org/pdf/1711.07767.pdf).   <img align=""right"" src=""https://github.com/ruinmessi/RFBNet/blob/master/doc/rfb.png"">  &nbsp; &nbsp;   """;General;https://github.com/dishen12/py03
"""The objective of this post is to implement a music genre classification model by comparing two popular architectures for sequence modeling: Recurrent Neural networks and Transformers.  RNNs are popular for all sorts of 1D sequence processing tasks  they re-use the same weights at each time step and pass information from a time-step to the next by keeping an internal state and using a gating mechanism (LSTM  GRUs … ). Since they use recurrence  those models can suffer from vanishing/exploding gradients which can make training and learning long-range patterns harder.  ![](https://cdn-images-1.medium.com/max/800/1*3gB5yUL9lqQBuEY7qFIH2A.png)  <span class=""figcaption_hack"">[Source: https://en.wikipedia.org/wiki/Recurrent_neural_network](https://en.wikipedia.org/wiki/Recurrent_neural_network) by [fdeloche](https://commons.wikimedia.org/wiki/User:Ixnay) Under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0)</span>  Transformers are a relatively newer architecture that can process sequences without using any recurrence or convolution [[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)]. The transformer layer is mostly point-wise feed-forward operations and self-attention. These types of networks are having some great success in natural language processing  especially when pre-trained on a large amount of unlabeled data [[https://arxiv.org/pdf/1810.04805](https://arxiv.org/pdf/1810.04805)].  ![](https://cdn-images-1.medium.com/max/800/1*SW0xA1VEJZd3XSqc3NvxNw.png)  <span class=""figcaption_hack"">Transformer Layer — Image by author</span>   """;Natural Language Processing;https://github.com/CVxTz/music_genre_classification
"""""";Computer Vision;https://github.com/davide-coccomini/TimeSformer-Video-Classification
"""""";General;https://github.com/datalass1/fastai
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/wangshanmin/interface
"""""";Computer Vision;https://github.com/daveboat/pytorch_focal_loss
"""""";Natural Language Processing;https://github.com/horizonheart/ELMO
"""""";Computer Vision;https://github.com/zxqcreations/faster-rcnn-tf
"""""";Computer Vision;https://github.com/taeokimeng/object-detection-yolo
"""""";Computer Vision;https://github.com/nanwei1/MNIST_GAN
"""""";Computer Vision;https://github.com/RaiyaniNirav/Mask-R-CNN-for-water-detection
"""""";Computer Vision;https://github.com/ttruty/facial-feature-mouse-control
"""""";General;https://github.com/pytorch/botorch
"""""";General;https://github.com/KaiyangZhou/ssdg-benchmark
"""""";Natural Language Processing;https://github.com/facebookresearch/UnsupervisedMT
"""""";Natural Language Processing;https://github.com/PrashantRanjan09/Elmo-Tutorial
"""""";Computer Vision;https://github.com/richieyoum/Pneumonia-diagnosis
"""Object detection methods typically rely on only local evidence. For example  to detect the mouse in the image below  only the features extracted at/around the mouse are used. In contrast  HoughNet is able to utilize long-range (i.e. far away) evidence  too. Below  on the right  the votes that support the detection of the mouse are shown: in addition to the local evidence  far away but semantically relevant objects  the two keyboards  vote for the mouse.  <img src=""/readme/teaser.png"" width=""550"">  HoughNet is a one-stage  anchor-free  voting-based  bottom-up object detection method. Inspired by the Generalized Hough Transform  HoughNet determines the presence of an object at a certain location by the sum of the votes cast on that location. Votes are collected from both near and long-distance locations based on a log-polar vote field. Thanks to this voting mechanism  HoughNet is able to integrate both near and long-range  class-conditional evidence for visual recognition  thereby generalizing and enhancing current object detection methodology  which typically relies on only local evidence. On the COCO dataset  HoughNet achieves 46.4 AP (and 65.1 AP<sub>50</sub>)  performing on par with the state-of-the-art in bottom-up object detection and outperforming most  major one-stage and two-stage methods. We further validate the effectiveness of HoughNet in another task  namely  ""labels to photo"" image generation by integrating the voting module to two different GAN models and showing that the accuracy is significantly improved in both cases.   """;Computer Vision;https://github.com/nerminsamet/houghnet
"""""";Computer Vision;https://github.com/hanzhanggit/StackGAN-inception-model
"""""";General;https://github.com/MartinGer/Bottleneck-Transformers-for-Visual-Recognition
"""""";General;https://github.com/ikostrikov/pytorch-a3c
"""*Source Separation* is a repository to extract speeches from various recorded sounds. It focuses to adapt more real-like dataset for training models.   **This project aims at building a speech enhancement system to attenuate environmental noise.**  <img src=""denoise_10classes.gif"" alt=""Spectrogram denoising"" title=""Speech enhancement""/>    Audios have many different ways to be represented  going from raw time series to time-frequency decompositions. The choice of the representation is crucial for the performance of your system. Among time-frequency decompositions  Spectrograms have been proved to be a useful representation for audio processing. They consist in 2D images representing sequences of Short Time Fourier Transform (STFT) with time and frequency as axes  and brightness representing the strength of a frequency component at each time frame. In such they appear a natural domain to apply the CNNS architectures for images directly to sound. Between magnitude and phase spectrograms  magnitude spectrograms contain most the structure of the signal. Phase spectrograms appear to show only little temporal and spectral regularities.  In this project  I will use magnitude spectrograms as a representation of sound (cf image below) in order to predict the noise model to be subtracted to a noisy voice spectrogram.  <img src=""sound_to_spectrogram.png"" alt=""sound representation"" title=""sound representation"" />  The project is decomposed in three modes: `data creation`  `training` and `prediction`.   """;Computer Vision;https://github.com/tarit21/Speech-Enhancement-module
"""""";Computer Vision;https://github.com/shalabh147/DCGan
"""""";General;https://github.com/cryu854/FastStyle
"""""";Computer Vision;https://github.com/bangoc123/mlp-mixer
"""""";Reinforcement Learning;https://github.com/daviddcho/supermario
"""""";Computer Vision;https://github.com/liuzeyuMr/CycleGAN-TensorFlow-master
"""""";General;https://github.com/Rust401/pixel2piexl
"""""";General;https://github.com/cwvisuals/FleshDigressions
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/pengshuyuan/Bert
"""""";Computer Vision;https://github.com/ChristophReich1996/Semantic_Pyramid_for_Image_Generation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/halo090770/bert
"""A general YOLOv4/v3/v2 object detection pipeline inherited from [keras-yolo3-Mobilenet](https://github.com/Adamdad/keras-YOLOv3-mobilenet)/[keras-yolo3](https://github.com/qqwweee/keras-yolo3) and [YAD2K](https://github.com/allanzelener/YAD2K). Implement with tf.keras  including data collection/annotation  model training/tuning  model evaluation and on device deployment. Support different architecture and different technologies:   """;Computer Vision;https://github.com/grifon-239/diploma
"""""";Computer Vision;https://github.com/black0017/MedicalZooPytorch
"""""";General;https://github.com/karenacorn99/explore-bert
"""""";General;https://github.com/leftthomas/SimCLR
"""MMTracking is an open source video perception toolbox based on PyTorch. It is a part of the OpenMMLab project.  The master branch works with **PyTorch1.5+**.  <div align=""left"">   <img src=""https://user-images.githubusercontent.com/24663779/103343312-c724f480-4ac6-11eb-9c22-b56f1902584e.gif"" width=""800""/> </div>   """;Computer Vision;https://github.com/open-mmlab/mmtracking
"""""";Computer Vision;https://github.com/1512159/tf-faster-rcnn-medico
"""""";Computer Vision;https://github.com/163GitHub/AI
"""""";General;https://github.com/brycexu/MR-Residual-Net
"""""";Audio;https://github.com/Baichenjia/Tensorflow-TCN
"""""";General;https://github.com/Lornatang/SRGAN-PyTorch
"""""";Computer Vision;https://github.com/vaibhavmit074/remove-bg-2
"""""";Computer Vision;https://github.com/Soldie/DeOldify-colorir-imagens-antigas
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/jinzhenfan/BERT
"""""";Computer Vision;https://github.com/zuobinxiong/pix2pix-tensorflow
"""""";Computer Vision;https://github.com/feng-lab/nuclei
"""""";General;https://github.com/meryusha/seeds_faster
"""""";Computer Vision;https://github.com/kynk94/TF2-Image-Generation
"""""";General;https://github.com/rosinality/adaptive-softmax-pytorch
"""""";General;https://github.com/kingoflolz/mesh-transformer-jax
"""""";General;https://github.com/amazon-research/exponential-moving-average-normalization
"""The full Github link would be in here: https://github.com/StephenEkaputra/Mask_RCNN-TinyPascalVOC We used the repository from: https://github.com/matterport/Mask_RCNN   """;Computer Vision;https://github.com/StephenEkaputra/Mask_RCNN-TinyPascalVOC
"""""";General;https://github.com/potsawee/maml
"""""";Computer Vision;https://github.com/ak9250/stylegan-art
"""This project mainly introduces the learning rate schemes provided by tensorflow and observes their influences on convolutional neural networks. The problem about how they work is not included as it is difficult to explain. Maybe in the future  I will post it once I get them straight. So  there are 15 learning rate schemes we will talk about: - 1. exponential_decay - 2. piecewise_constant_decay - 3. polynominal_decay - 4. inverse_time_decay - 5. cosine_decay - 6. cosine_decay_restarts - 7. linear_cosine_decay - 8. noisy_linear_cosine_decay - 9. tf.train.GradientDescentOptimizer - 10. tf.train.MomentumOptimizer - 11. tf.train.AdamOptimizer // tf.train.AdagradOptimizer // tf.train.AdadeletaOptimizer // tf.train.AdagradDAOptimizer - 12. tf.train.RMSPropOptimizer - 13. tf.train.FtrlOptimizer We conduct experiments on Cifar10 with these shemes  and then make analyses on different combinations among them.   """;General;https://github.com/souxun2015/Survery-of-Learning-Rate-Shemes
"""""";Reinforcement Learning;https://github.com/jonathan-laurent/AlphaZero.jl
"""""";General;https://github.com/pessimiss/ai100-w8-master
"""Deep learning  also known as hierarchical learning or deep structured learning   is a type of machine learning that uses a layered algorithmic architecture to analyze data.  Unlike other types of machine learning  deep learning has an added advantage of being able to make decisions with  significantly less human intervention. While basic machine learning requires a programmer to identify whether a conclusion  is correct or not  deep learning can gauge the accuracy of its answers on its own due to the nature of its multi-layered structure.  The emergence of modern frameworks like PyTorch  has also made preprocessing of data more convenient.  Many of the filtering and normalization tasks that would involve a lot of manual tasks while using other machine learning techniques  are taken up automatically.  The essential characteristics of deep learning make it an ideal tool for giving the much needed impetus   to the field of automated medical diagnosis. With the right expertise  it can be leveraged to overcome several  limitations of conventional diagnosis done by medical practitioners  and take the dream of accurate and efficient  automated disease diagnosis to the realm of reality.  Given the team's vision to make healthcare better for everyone  everywhere  and having paid attention to the trends and  recent breakthroughs with deep learning  we decided to experiment with several variations of convolutional neural networks for this project.  Recent work has shown that convolutional networks can be substantially deeper  more accurate   and efficient to train if they contain shorter connections between layers close to the input and output.  We embraced this observation  and leveraged the power of the Dense Convolutional Network (DenseNet)   which connects each layer to every other layer in a feed-forward fashion.  Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer -  our network had L(L+1)/2 direct connections. We also experimented with other architectures  including residual networks  and used our  model variations as controls to validate each other.    """;Computer Vision;https://github.com/SGNovice/Disease-detection-using-chest-xrays
"""""";Computer Vision;https://github.com/wiegehtki/zoneminder
"""""";Computer Vision;https://github.com/lnstadrum/fastaugment
"""""";Computer Vision;https://github.com/andrewowens/multisensory
"""""";Natural Language Processing;https://github.com/lukexyz/Language-Models
"""""";General;https://github.com/ykrmm/ICLR_2020
"""""";Computer Vision;https://github.com/Contamination-Classification/DenseNet
"""""";Graphs;https://github.com/berlincho/RGCN-pytorch
"""""";Sequential;https://github.com/floleuerer/fastai_ulmfit
"""""";Reinforcement Learning;https://github.com/google-research/google-research
"""""";General;https://github.com/wuyangzhang/maskrcnn
"""""";Computer Vision;https://github.com/stigma0617/maskrcnn-benchmark-vovnet
"""""";Graphs;https://github.com/allenai/kb
"""""";General;https://github.com/iejMac/ScriptWriter
"""bert and multi_cased_L-12_H-768_A-12 should be extracted this directory.   ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ multi_cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip) |____ data		            #: train data (EnglishBERTdata3Labels  TurkishNERdata3Labels  TurkishNERdata7Labels) |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ run_ner.sh    		    #: run model and eval result  ```   """;Natural Language Processing;https://github.com/teghub/TurkishNER-BERT
"""""";General;https://github.com/quark0/darts
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/MaZhiyuanBUAA/bert-tf1.4.0
"""""";Reinforcement Learning;https://github.com/vermashresth/damage-aware-PPO
"""The model is a Convolutional Neural Network with shortcuts as showed in this paper: https://arxiv.org/pdf/1512.03385.pdf   """;General;https://github.com/LuigiRussoDev/ResNets
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/OpenCv30/Yolov3
"""""";Natural Language Processing;https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition
"""""";Computer Vision;https://github.com/oellop/GAN_MNIST
"""""";Graphs;https://github.com/atomistic-machine-learning/SchNet
"""""";General;https://github.com/greatsharma/DeepLearning-Papers-Implementation
"""""";Computer Vision;https://github.com/preritj/segmentation
"""""";Graphs;https://github.com/tlmakinen/GAT-noise
"""""";Computer Vision;https://github.com/JuliBaCSE/WeCare_makeathon
"""""";Computer Vision;https://github.com/nikhithakarennagari/1311
"""""";Natural Language Processing;https://github.com/lesterpjy/numeric-t5
"""""";Computer Vision;https://github.com/isaacschaal/SG_training
"""""";Computer Vision;https://github.com/jimzers/stylegan2-colabs
"""""";Computer Vision;https://github.com/guy-oren/DIRT-OST
"""""";Computer Vision;https://github.com/sar-gupta/convisualize_nb
"""""";Computer Vision;https://github.com/Sangkwun/Resnet
"""""";General;https://github.com/vincentbillaut/all-colors-matter
"""""";Natural Language Processing;https://github.com/manideep2510/siamese-BERT-fake-news-detection-LIAR
"""""";General;https://github.com/floraxhuang/Movie-Recommendation-System
"""""";General;https://github.com/ArvidWartenberg/retrosynthesis
"""""";Computer Vision;https://github.com/NVlabs/VAEBM
"""""";Computer Vision;https://github.com/cgsaxner/UB_Segmentation
"""""";Computer Vision;https://github.com/mnikitin/Shift-Invariant-CNNs
"""""";General;https://github.com/saghiralfasly/VFL-Vehicle-Re-Id
"""""";Computer Vision;https://github.com/xuebinqin/U-2-Net
"""""";Reinforcement Learning;https://github.com/ku2482/sac-discrete.pytorch
"""""";Audio;https://github.com/tdunity/fixedjukebox
"""""";General;https://github.com/LMescheder/GAN_stability
"""""";General;https://github.com/axelbrando/Mixture-Density-Networks-for-distribution-and-uncertainty-estimation
"""""";Computer Vision;https://github.com/modelhub-ai/vgg-19
"""""";Computer Vision;https://github.com/thekoshkina/blackai-challenge
"""""";Computer Vision;https://github.com/yan-roo/SpineNet-Pytorch
"""""";Computer Vision;https://github.com/kuan-wang/pytorch-mobilenet-v3
"""""";General;https://github.com/plkmo/Transformer-Eng2French
"""""";General;https://github.com/RangiLyu/nanodet
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/adairliulei/yolov3
"""""";Computer Vision;https://github.com/datalass1/fastai
"""""";Computer Vision;https://github.com/zhiggins11/Semantic-Segmentation
"""""";Computer Vision;https://github.com/Cjiangbpcs/cjiang.github.io
"""""";Computer Vision;https://github.com/airplane2230/keras_cutmix
"""""";Natural Language Processing;https://github.com/hendrycks/test
"""""";Computer Vision;https://github.com/CanshangD/ResNet_tensorflow2.0
"""""";General;https://github.com/hidekifujinami6/gan_evaluation
"""""";Computer Vision;https://github.com/x5675602/squeezeNet_keras
"""""";General;https://github.com/ChengBinJin/WGAN-GP-tensorflow
"""""";Computer Vision;https://github.com/yousuf907/DC-GAN-Demos
"""""";Computer Vision;https://github.com/Duplums/bhb10k-dl-benchmark
"""""";General;https://github.com/DaikiTanak/manifold_mixup
"""""";Computer Vision;https://github.com/XXXVincent/SOLO
"""""";Computer Vision;https://github.com/rohitgr7/tvmodels
"""""";General;https://github.com/developer0hye/SKNet-PyTorch
"""This is a Caffe implementation of Google's MobileNets (v1 and v2). For details  please read the following papers: - [v1] [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) - [v2] [Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification  Detection and Segmentation](https://arxiv.org/abs/1801.04381)    """;Computer Vision;https://github.com/shicai/MobileNet-Caffe
"""""";General;https://github.com/yinghao1019/NLP_and_DL_practice
"""""";Natural Language Processing;https://github.com/HuihuiChyan/BJTUNLP_Practice2021
"""""";Sequential;https://github.com/chris-tng/semi-supervised-nlp
"""""";Computer Vision;https://github.com/ikhlestov/vision_networks
"""""";Natural Language Processing;https://github.com/scrayish/ML_NLP
"""""";Computer Vision;https://github.com/saumya-jetley/pp_ICVSS15_TacklingBkgndDifferently
"""The hexadecimal 3 byte web color encoding can represent 16<sup>6</sup> ≈ 16.8 million different colors. With a combination of 3 colors there are (16.8x10<sup>6</sup>)<sup>3</sup> ≈ 4.7 billion possible palettes to choose from. This is definitely too much for a person to go through. One possible solution of the problem of finding good matches for a persons preferences of color combination is to let a recommendation system do the bidding.   """;General;https://github.com/Orchidaceae/AI_palette_recommendation
"""""";Computer Vision;https://github.com/BrixIA/Brixia-score-COVID-19
"""""";Natural Language Processing;https://github.com/jhave/RERITES-AvgWeightDescentLSTM-PoetryGeneration
"""- Mackay uses Gaussian approximation to get solutions to BNNs  approximates posterior centred around most probable parameter values which are found via optimisation. Error is estimated from Hessian. This approximation assumes posterior is unimodal  and breaks down when number of parameters approaches numbr of datapoints. Uses this analytic approximation to calculate the evidence. Picks value of hyperparameters which maximise evidence  which equivalently maximise the posterior of the hyperparameters given the data  for a uniform prior on the hyperparameters. Thus essentially assumes that marginalising over hyperparameters and taking their maximum are equal  i.e. the hyperparam posterior is also Gaussian. then looks at evidence values given these best hyperparameters and training set error to evaluate models. Uses some approximation of the evidence maximisation to update the hyperparameter.  - finds when a rubbish model used  evidence and test error not as correlated as when good model is used. Further  the evidence is low in some cases where test error is good. Uses this to deduce structure of model is wrong. Also sees Occam's hill.  - Likelihood variance is fixed. Initially one hyperparam used for all weights and biases. Found evidence and generalisation don’t correlate well. MacKay argues this is because the scales of the inputs  outputs and hidden layers are not the same  so one cannot expect scaling the weights by the same amount to work well. So then tried one hyperparam for hidden unit weights  one for hidden unit biases  then one for output weights and biases. This gave higher evidence  higher test set performance  and stronger correlation between the two  - Neal uses HMC to sample the BNN parameters  and Gibbs sampling to sample the hyperparameters. n.b. HMC requires gradient information  so can't be used to sample hyperparameters directly (to my knowledge). Also  HMC in essence has characteristics similar to common optimisation methods which use 'momentum' and 'velocity'.  - Neal also introduces concept of using Gaussian processes to introduce a prior over functions  which tells us what nn predicts mapping function to be without any data.  - From Neal it seems that sampling hyperparameters seems rather necessary to justify allowing NN to be arbitrarily big- if complex model is not needed  hyperparameters will 'quash' nodes which aren't important  according to the hyperparameter values assigned by the data during the training  and 'upweight' important nodes. Also avoids cross validation step.  - Uses stochastic/mini-batch methods.  - Neal's result (with same simple model as Mackay) on test data is similar to the Mackay best evidence model's results  but not as good as his best test error model results. Performance didn't necessarily get worse with larger networks for BNNs  but did for MAP estimates (though don't think this treated hyperparameters as stochastic).  - n.b. hyperparams in first layer indicate which inputs to network are important. using it generalises to test data better  as irrelevant attributes fit to noise in train. Furthermore  Neal scales hyperprior (gamma parameter w  which is mean precision) by number of units in previous layer i.e. for layer i w_i -> w_i * H_{i-1} for i >= 2 (note he doesn't scale first hidden layer). Note that he does not use this scaling on the biases  in particular  for hidden layers  biases are given standard hyperprior  and for output layer the biases aren't given a stochastic variance at all (instead they are usually fixed to a Gaussian with unit variance).  - larger network is  more uncertain it is to out of training distribution data.  - for bh  BNN does much better on test error than traditional (though I don't think this uses cross validation in traditional sense).  - Freitas uses reversible jump MCMC to sample neural network systems. reversible jump MCMC is necessary when number of parameters changes. This is the case here  as the number of radial basis functions (neurons) is allowed to vary in the analysis  resulting in a varying number of model parameters/hyperparameters throughout the sampling. Gives posteriors on number of functions  as well as the usual param/hyperparams ones.  - Also uses SMC to train NNs where data arrives one at a time. Idea is to model joint distribution of model parameters at each timestep  and appears to do a better job of predicting it with more time/data.  - Also does model selection  using posterior over number of basis functions. Can do this in sequential context as well.   - Finds reversible jump MCMC does as well as Mackay and Neal  and better than expectation maximisation algorithm (which is similar/equivalent to variational inference)  but is slower than EM algo.  - Gal provides the genius insight that stochastic draws from the distribution over neural networks can be done using traditional methods. Usually if using dropout regularisation  one disables the dropout once training is finished. Gal shows that using dropout during model deployment is equivalent to using variational inference to get a probabilistic model output. The parameters of the variational inference problem are determined by the dropout properties I believe. The higher the dropout probability  the stronger the prior on the inference problem.  - This essentially means a Bayesian approach can be used even for high dimensional problems  the training time is the same as that of maximisation methods  and during deployment  one is only limited by how many samples from the posterior one wants.  - Gal finds that this method exceeds traditional variational inference methods both in terms of speed and test set performance for most tasks  with the only doubts occurring in some CNNs. He also finds it outperforms traditional methods in terms of test set performance  with the added bonus that one gets an uncertainty estimate. The method however cannot give evidence estimates.   """;General;https://github.com/SuperKam91/bnn
"""""";General;https://github.com/ShenDezhou/aft-pytorch
"""""";Computer Vision;https://github.com/sathvikyesprabhu/brats-dl
"""""";Computer Vision;https://github.com/rishikksh20/convolution-vision-transformers
"""""";General;https://github.com/tmabraham/UPIT
"""""";Natural Language Processing;https://github.com/AnttiKarlsson/finnish_ulmfit
"""""";General;https://github.com/xiexiexiaoxiexie/Udacity-self-driving-car-engineer-P4-Behavioral-Cloning
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO** (https://pjreddie.com/darknet/yolo/) and to **Erik Lindernoren for the PyTorch implementation** this work is based on (https://github.com/eriklindernoren/PyTorch-YOLOv3).   This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information on Ultralytics projects please visit: https://www.ultralytics.com.   """;Computer Vision;https://github.com/thecryboy/yolov3_ultralytics_deconv
"""""";Reinforcement Learning;https://github.com/seungjaeryanlee/rldb
"""""";Computer Vision;https://github.com/AmirmohammadRostami/KeywordsSpotting-EfficientNet-A0
"""""";Sequential;https://github.com/yanggeng1995/GAN-TTS
"""""";Reinforcement Learning;https://github.com/traai/async-deep-rl
"""""";General;https://github.com/ChandhiniG/Generate-Stylized-Image-from-Edges
"""""";Natural Language Processing;https://github.com/abhilashreddys/Fake-News-Article
"""""";Reinforcement Learning;https://github.com/AndrewJWashington/protodriver
"""""";Computer Vision;https://github.com/patientzero/timage-icann2019
"""""";General;https://github.com/anzhao0503/group-normalization.pytorch
"""""";Computer Vision;https://github.com/native2019/hello
"""""";Computer Vision;https://github.com/sacmehta/EdgeNets
"""""";General;https://github.com/aniket03/pirl_pytorch
"""""";General;https://github.com/redevaaa/Transformer-for-EEG
"""""";General;https://github.com/Sangkwun/Resnet
"""""";General;https://github.com/omegafragger/DDU
"""""";Computer Vision;https://github.com/eshaanagarwal/hr-net-implementation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Chenrj233/bert_pratice
"""""";General;https://github.com/TieDanCuihua/transformer-Attention-is-All-You-Need
"""""";General;https://github.com/CellSMB/EM-net
"""SWA is a simple DNN training method that can be used as a drop-in replacement for SGD with improved generalization  faster convergence  and essentially no overhead. The key idea of SWA is to average multiple samples produced by SGD with a modified learning rate schedule. We use a constant or cyclical learning rate schedule that causes SGD to _explore_ the set of points in the weight space corresponding to high-performing networks. We observe that SWA converges more quickly than SGD  and to wider optima that provide higher test accuracy.   In this repo we implement the constant learning rate schedule that we found to be most practical on CIFAR datasets.  <p align=""center"">   <img src=""https://user-images.githubusercontent.com/14368801/37633888-89fdc05a-2bca-11e8-88aa-dd3661a44c3f.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633885-89d809a0-2bca-11e8-8d57-3bd78734cea3.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633887-89e93784-2bca-11e8-9d71-a385ea72ff7c.png"" width=250> </p>  Please cite our work if you find this approach useful in your research: ```bibtex @article{izmailov2018averaging    title={Averaging Weights Leads to Wider Optima and Better Generalization}    author={Izmailov  Pavel and Podoprikhin  Dmitrii and Garipov  Timur and Vetrov  Dmitry and Wilson  Andrew Gordon}    journal={arXiv preprint arXiv:1803.05407}    year={2018} } ```    """;General;https://github.com/timgaripov/swa
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/LONG-9621/PointNet
"""""";Natural Language Processing;https://github.com/kdrl/SCNE
"""""";Computer Vision;https://github.com/gshashank84/CheXNet
"""""";Computer Vision;https://github.com/leehomyc/mixup_pytorch
"""""";Computer Vision;https://github.com/williamwang96/Almond-Recognition
"""""";General;https://github.com/asharakeh/probdet
"""""";Natural Language Processing;https://github.com/Julian1070/Deep-Learning
"""""";Natural Language Processing;https://github.com/RonRaifer/BERT-Ghazali
"""""";Computer Vision;https://github.com/Ruturaj123/VGG16-Keras
"""""";Natural Language Processing;https://github.com/huggingface/swift-coreml-transformers
"""""";Natural Language Processing;https://github.com/u7javed/Transformer-Multi-Language-Translator
"""""";Computer Vision;https://github.com/sunqiangxtcsun/faster-rcnn
"""""";Computer Vision;https://github.com/qfgaohao/pytorch-ssd
"""""";General;https://github.com/samihadouaj/siyanWork
"""""";Computer Vision;https://github.com/vinits5/learning3d
"""""";General;https://github.com/bxck75/piss-ant-pix2pix
"""""";General;https://github.com/woutercools1998/SRResNet
"""Tensorflow object detection is very good  easy and free framework for object detection tasks.  It contains varous models trained on COCO  Kitty like large dataset and we use these pretrained models on our custom datset.    """;Computer Vision;https://github.com/vandangorade/BrandLOGO_detection
"""""";General;https://github.com/zoli333/Weight-Normalization
"""""";Computer Vision;https://github.com/ZackPashkin/text2cartoon-pytorch-CLIP
"""""";Sequential;https://github.com/seujung/SNAIL-gluon
"""""";Reinforcement Learning;https://github.com/rail-berkeley/rlkit
"""""";Reinforcement Learning;https://github.com/moduIo/Deep-Q-network
"""""";General;https://github.com/suryachintu/Quora-Insincere-Questions-Kaggle
"""This lab is based on the XDF 2018  Machine learning for Embedded Workshop. It has been modified to run on the Ultra96 board.  During this session you will gain hands-on experience with the Xlinx DNNDK  and learn how to quantize  compile and deploy pre-trained network models to Xilinx embedded SoC platforms.    """;Computer Vision;https://github.com/jimheaton/Ultra96_ML_Embedded_Workshop
"""""";Reinforcement Learning;https://github.com/kushagra06/DDPG
"""Google AI's BERT paper shows the amazing result on various NLP task (new 17 NLP tasks SOTA)   This paper proved that Transformer(self-attention) based encoder can be powerfully used as  alternative of previous language model with proper language model training method.  This repo is implementation of Mask LM in BERT. Code is very simple and easy to understand fastly. Some of these codes are based on [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)    """;Natural Language Processing;https://github.com/huanghonggit/Mask-Language-Model
"""""";General;https://github.com/tollymune/CycleGAN-PyTorch
"""""";Computer Vision;https://github.com/dwaithe/darknet3AB
"""""";General;https://github.com/2miatran/Natural-Language-Processing
"""""";Natural Language Processing;https://github.com/avi-jit/SWOW-eval
"""""";Graphs;https://github.com/FilippoMB/Spectral-Clustering-with-Graph-Neural-Networks-for-Graph-Pooling
"""""";Computer Vision;https://github.com/Ksuryateja/DCGAN-MNIST-pytorch
"""We included two Jupyter notebooks to demonstrate how the HDF5 datasets are created * For the medium scale datasets view `create_hdf_benchmarking_datasets.ipynb`. You will need `pytorch`  `ogb==1.1.1` and `dgl==0.4.2` libraries to run the notebook. The notebook is also runnable on Google Colaboratory. * For the large scale pcqm4m dataset view `create_hdf_pcqm4m.ipynb`. You will need `pytorch`  `ogb>=1.3.0` and `rdkit>=2019.03.1` to run the notebook.   This is the official implementation of the **Edge-augmented Graph Transformer (EGT)** as described in https://arxiv.org/abs/2108.03348  which augments the Transformer architecture with residual edge channels. The resultant architecture can directly process graph-structured data and acheives good results on supervised graph-learning tasks as presented by [Dwivedi et al.](https://arxiv.org/abs/2003.00982). It also achieves good performance on the large-scale [PCQM4M-LSC](https://arxiv.org/abs/2103.09430) (`0.1263 MAE` on val) dataset. EGT beats convolutional/message-passing graph neural networks on a wide range of supervised tasks and thus demonstrates that convolutional aggregation is not an essential inductive bias for graphs.   """;Graphs;https://github.com/shamim-hussain/egt
"""""";Computer Vision;https://github.com/alourositi/EAM_docker
"""""";General;https://github.com/jerrodparker20/adaptive-transformers-in-rl
"""This project provides an implementation for our CVPR2021 paper ""[OPANAS: One-Shot Path Aggregation Network Architecture Search for Object Detection](https://arxiv.org/abs/2103.04507)"" on PyTorch. The search code is coming soon.   """;Computer Vision;https://github.com/VDIGPKU/OPANAS
"""""";Sequential;https://github.com/basveeling/wavenet
"""""";General;https://github.com/dsinghnegi/atari_RL_agent
"""""";Computer Vision;https://github.com/VinayBN8997/ResNet-CIFAR10
"""""";Computer Vision;https://github.com/hariv/pix2pix
"""""";Natural Language Processing;https://github.com/yash-nishaant/Seq2Seq-Chatbot
"""""";Reinforcement Learning;https://github.com/prasoonkottarathil/Twin-Delayed-DDPG-TD3-
"""""";Computer Vision;https://github.com/Alibaba-MIIL/TResNet
"""""";Natural Language Processing;https://github.com/ElementAI/picard
"""""";Computer Vision;https://github.com/Zahra2351373/Image-Processing-Final-Project
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Yipeng91/text_classifier_pub
"""""";General;https://github.com/DevashishJoshi/Transferring-GANs-FYP
"""""";General;https://github.com/Praveen94/pytorch-encoding
"""VISSL is a computer **VI**sion library for state-of-the-art **S**elf-**S**upervised **L**earning research with [PyTorch](https://pytorch.org). VISSL aims to accelerate research cycle in self-supervised learning: from designing a new self-supervised task to evaluating the learned representations. Key features include:  - **Reproducible implementation of SOTA in Self-Supervision**: All existing SOTA in Self-Supervision are implemented - [SwAV](https://arxiv.org/abs/2006.09882)  [SimCLR](https://arxiv.org/abs/2002.05709)  [MoCo(v2)](https://arxiv.org/abs/1911.05722)  [PIRL](https://arxiv.org/abs/1912.01991)  [NPID](https://arxiv.org/pdf/1805.01978.pdf)  [NPID++](https://arxiv.org/abs/1912.01991)  [DeepClusterV2](https://arxiv.org/abs/2006.09882)  [ClusterFit](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_ClusterFit_Improving_Generalization_of_Visual_Representations_CVPR_2020_paper.pdf)  [RotNet](https://arxiv.org/abs/1803.07728)  [Jigsaw](https://arxiv.org/abs/1603.09246). Also supports supervised trainings.  - **Benchmark suite**: Variety of benchmarks tasks including [linear image classification (places205  imagenet1k  voc07  food  CLEVR  dsprites  UCF101  stanford cars and many more)](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/linear_image_classification)  [full finetuning](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/fulltune)  [semi-supervised benchmark](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/semi_supervised)  [nearest neighbor benchmark](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/nearest_neighbor)  [object detection (Pascal VOC and COCO)](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/object_detection).  - **Ease of Usability**: easy to use using yaml configuration system based on [Hydra](https://github.com/facebookresearch/hydra).  - **Modular**: Easy to design new tasks and reuse the existing components from other tasks (objective functions  model trunk and heads  data transforms  etc.). The modular components are simple *drop-in replacements* in yaml config files.  - **Scalability**: Easy to train model on 1-gpu  multi-gpu and multi-node. Several components for large scale trainings provided as simple config file plugs: [Activation checkpointing](https://pytorch.org/docs/stable/checkpoint.html)  [ZeRO](https://arxiv.org/abs/1910.02054)  [FP16](https://nvidia.github.io/apex/amp.html#o1-mixed-precision-recommended-for-typical-use)  [LARC](https://arxiv.org/abs/1708.03888)  Stateful data sampler  data class to handle invalid images  large model backbones like [RegNets](https://arxiv.org/abs/2003.13678)  etc.  - **Model Zoo**: Over *60 pre-trained self-supervised model* weights.   """;Computer Vision;https://github.com/facebookresearch/vissl
"""""";Natural Language Processing;https://github.com/DW-yejing/fasttext4j-jdk6
"""""";Natural Language Processing;https://github.com/mari756h/The_unemployed_cells
"""Use the fastai (https://www.fast.ai/)framework to implement transfer learning for text classification.  The language model was trained on a corpus named Wikitext-103.(https://openreview.net/pdf?id=Byj72udxe)  If you want to know some detailed information  please refer to Jeremy Howard’s paper.(https://arxiv.org/abs/1801.06146v5)  """;Natural Language Processing;https://github.com/SkullFang/ULMFIT_NLP_Classification
"""We propose a novel building block for CNNs  namely Res2Net  by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models  e.g.   ResNet  ResNeXt  BigLittleNet  and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models. <p align=""center""> 	<img src=""https://mftp.mmcheng.net/imgs800/19Res2Net.jpg"" alt=""Sample""  width=""500""> 	<p align=""center""> 		<em>Res2Net module</em> 	</p> </p>   """;Computer Vision;https://github.com/Res2Net/Res2Net-PretrainedModels
"""""";General;https://github.com/fbuchert/fixmatch-pytorch
"""""";General;https://github.com/ucsd-ml-arts/ml-art-final-justin-law
"""""";General;https://github.com/sonalisrijan/Deep-Learning-based-Defect-Prediction-Model-for-Source-Code
"""""";Computer Vision;https://github.com/nithinksath96/Face-Classification-and-Verification-Using-Convolutional-Neural-Networks
"""""";General;https://github.com/daixiangzi/ImprovedGan-pytorch
"""""";Computer Vision;https://github.com/squirrelim/sr-frcnn
"""""";Natural Language Processing;https://github.com/facebookresearch/poincare-embeddings
"""""";General;https://github.com/LeeHyeJin91/Wide_and_Deep
"""""";General;https://github.com/yueatsprograms/Stochastic_Depth
"""""";Natural Language Processing;https://github.com/GT-SALT/MixText
"""""";Computer Vision;https://github.com/delldu/MaskRCNN
"""""";Computer Vision;https://github.com/Ahsanr312/Object-Detection-and-Tracking-using-YOLOv3-and-DeepSort
"""""";Graphs;https://github.com/karthik63/attention
"""""";General;https://github.com/rajaswa/feedback-and-memory-in-transformers
"""""";Natural Language Processing;https://github.com/jind11/word2vec-on-wikipedia
"""""";Computer Vision;https://github.com/SiavashCS/sgan_simple
"""""";Audio;https://github.com/pbrandl/aNN_Audio
"""""";Computer Vision;https://github.com/DingXiaoH/RepMLP
"""""";Computer Vision;https://github.com/masataka46/demo_LSGAN_TF
"""""";Natural Language Processing;https://github.com/chz816/esacl
"""""";General;https://github.com/MarcBS/keras
"""""";General;https://github.com/Montia/bw2color
"""""";Computer Vision;https://github.com/alexismailov2/darknet
"""""";Computer Vision;https://github.com/fabiofumarola/ultrayolo
"""""";General;https://github.com/Bakikii/stylegan2-pytorch23
"""""";General;https://github.com/dhiraa/tener
"""""";Computer Vision;https://github.com/dronefreak/human-action-classification
"""&emsp;&emsp;This project is about movive reviews sentiment analysis based on Transformer and ULMFiT model.   **You can browse the full report from [here](https://github.com/PrideLee/sentiment-analysis/blob/master/Different%20Deep%20Learning%20Models%20Applied%20to%20Sentiment%20Analysis.pdf).**   """;General;https://github.com/PrideLee/sentiment-analysis
"""""";General;https://github.com/tristandeleu/pytorch-maml-rl
"""""";Computer Vision;https://github.com/Qengineering/YoloV2-ncnn-Jetson-Nano
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/jessychen1016/yolov3Onpytorch
"""""";Computer Vision;https://github.com/jameswang287/Car-Detection
"""""";General;https://github.com/philipperemy/keras-frn
"""""";General;https://github.com/justusschock/delira_cycle_gan_pytorch
"""""";Computer Vision;https://github.com/lucidrains/g-mlp-pytorch
"""""";Computer Vision;https://github.com/yash88600/MobileNet
"""The implementation is based on two papers:  - Simple Online and Realtime Tracking with a Deep Association Metric https://arxiv.org/abs/1703.07402 - YOLOv4: Optimal Speed and Accuracy of Object Detection https://arxiv.org/pdf/2004.10934.pdf   This repository contains a moded version of PyTorch YOLOv5 (https://github.com/ultralytics/yolov5). It filters out every detection that is not a person. The detections of persons are then passed to a Deep Sort algorithm (https://github.com/ZQPei/deep_sort_pytorch) which tracks the persons. The reason behind the fact that it just tracks persons is that the deep association metric is trained on a person ONLY datatset.   """;Computer Vision;https://github.com/ajcohen1/CoVision-Social-Distance-Violation-Detection
"""""";Natural Language Processing;https://github.com/mandubian/codenets
"""We provide a simple introduction in [Motivation](#motivation)  and more details can be found in our [paper](https://arxiv.org/abs/1908.03265). There are some unofficial introductions available (with better writings)  and they are listed here for reference only (contents/claims in our paper are more accurate):  [Medium Post](https://medium.com/@lessw/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b) > [related Twitter Post](https://twitter.com/jeremyphoward/status/1162118545095852032?ref_src=twsrc%5Etfw)  [CSDN Post (in Chinese)](https://blog.csdn.net/u014248127/article/details/99696029)   <h5 align=""center""><i>If warmup is the answer  what is the question?</i></h5>  The learning rate warmup for Adam is a must-have trick for stable training in certain situations (or eps tuning). But the underlying mechanism is largely unknown. In our study  we suggest one fundamental cause is __the large variance of the adaptive learning rates__  and provide both theoretical and empirical support evidence.  In addition to explaining __why we should use warmup__  we also propose __RAdam__  a theoretically sound variant of Adam.    """;General;https://github.com/LiyuanLucasLiu/RAdam
"""""";Computer Vision;https://github.com/PengchengAi/tf-faster-rcnn-pcai
"""To submit a multi-GPU job  use the `submit_pm.sh` with the `-n` option set to the desired number of GPUs. For example  to launch a training with multiple GPUs  you will use commands like: ``` sbatch -n NUM_GPU submit_pm.sh [OPTIONS] ``` This script automatically uses the slurm flags `--ntasks-per-node 4`  `--cpus-per-task 32`  `--gpus-per-task 1`  so slurm will allocate one process for each GPU we request  and give each process 1/4th of the CPU resources available on a Perlmutter GPU node. This way  multi-node trainings can easily be launched simply by setting `-n` greater than 4.  *Question: why do you think we run 1 task (cpu process) per GPU  instead of 1 task per node (each running 4 GPUs)?*  PyTorch `DistributedDataParallel`  or DDP for short  is flexible and can initialize process groups with a variety of methods. For this code  we will use the standard approach of initializing via environment variables  which can be easily read from the slurm environment. Take a look at the `export_DDP_vars.sh` helper script  which is used by our job script to expose for PyTorch DDP the global rank and node-local rank of each process  along with the total number of ranks and the address and port to use for network communication. In the [`train.py`](train.py) script  near the bottom in the main script execution  we set up the distributed backend using these environment variables via `torch.distributed.init_proces_group`.  When distributing a batch of samples in DDP training  we must make sure each rank gets a properly-sized subset of the full batch. See if you can find where we use the `DistributedSampler` from PyTorch to properly partition the data in [`utils/data_loader.py`](utils/data_loader.py). Note that in this particular example  we are already cropping samples randomly form a large simulation volume  so the partitioning does not ensure each rank gets unique data  but simply shortens the number of steps needed to complete an ""epoch"". For datasets with a fixed number of unique samples  `DistributedSampler` will also ensure each rank sees a unique minibatch.  In `train.py`  after our U-Net model is constructed  we convert it to a distributed data parallel model by wrapping it as: ``` model = DistributedDataParallel(model  device_ids=[local_rank]) ```  The DistributedDataParallel (DDP) model wrapper takes care of broadcasting initial model weights to all workers and performing all-reduce on the gradients in the training backward pass to properly synchronize and update the model weights in the distributed setting.  *Question: why does DDP broadcast the initial model weights to all workers? What would happen if it didn't?*   """;Computer Vision;https://github.com/NERSC/sc21-dl-tutorial
"""""";Computer Vision;https://github.com/JPlin/MSGAN
"""""";Natural Language Processing;https://github.com/sfu-natlang/SFUTranslate
"""""";Computer Vision;https://github.com/gfursin/browser-extension-for-reproducible-research
"""""";General;https://github.com/EugeneSel/EUMT
"""""";General;https://github.com/semskurto/APTOS
"""""";General;https://github.com/bobby20180331/darknet_pycharm
"""""";Graphs;https://github.com/Accenture/AmpliGraph
"""SWA is a simple DNN training method that can be used as a drop-in replacement for SGD with improved generalization  faster convergence  and essentially no overhead. The key idea of SWA is to average multiple samples produced by SGD with a modified learning rate schedule. We use a constant or cyclical learning rate schedule that causes SGD to _explore_ the set of points in the weight space corresponding to high-performing networks. We observe that SWA converges more quickly than SGD  and to wider optima that provide higher test accuracy.   In this repo we implement the constant learning rate schedule that we found to be most practical on CIFAR datasets.  <p align=""center"">   <img src=""https://user-images.githubusercontent.com/14368801/37633888-89fdc05a-2bca-11e8-88aa-dd3661a44c3f.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633885-89d809a0-2bca-11e8-8d57-3bd78734cea3.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633887-89e93784-2bca-11e8-9d71-a385ea72ff7c.png"" width=250> </p>  Please cite our work if you find this approach useful in your research: ```latex @article{izmailov2018averaging    title={Averaging Weights Leads to Wider Optima and Better Generalization}    author={Izmailov  Pavel and Podoprikhin  Dmitrii and Garipov  Timur and Vetrov  Dmitry and Wilson  Andrew Gordon}    journal={arXiv preprint arXiv:1803.05407}    year={2018} } ```    """;General;https://github.com/izmailovpavel/torch_swa_examples
"""""";Reinforcement Learning;https://github.com/CSautier/Breakout
"""""";General;https://github.com/Linear95/CLUB
"""""";Computer Vision;https://github.com/yuzq97/starter_project
"""""";Computer Vision;https://github.com/Epiphqny/VisTR
"""""";Sequential;https://github.com/fangyiyu/Modified_MNIST_Classification
"""""";Computer Vision;https://github.com/amittiwary42/target-detection-eeg-signals
"""""";Natural Language Processing;https://github.com/microsoft/vision-longformer
"""""";Computer Vision;https://github.com/WGLab/SGAN
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/guzhang480/Google_BERT
"""""";General;https://github.com/KeithGalli/pycon2020
"""""";Computer Vision;https://github.com/manicman1999/CycleGAN256-BR
"""""";General;https://github.com/J3698/AdaIN-reimplementation
"""""";Graphs;https://github.com/masakicktashiro/rgcn_pytorch_implementation
"""""";General;https://github.com/HazemLahoual/Project-Machine-Learning
"""""";Audio;https://github.com/MattSegal/speech-enhancement
"""""";Graphs;https://github.com/basiralab/RG-Select
"""""";Computer Vision;https://github.com/xmu-xiaoma666/External-Attention-pytorch
"""""";Computer Vision;https://github.com/musyoku/chainer-glow
"""""";Computer Vision;https://github.com/ghadahamed/darknet
"""""";Computer Vision;https://github.com/tkuanlun350/Kaggle_Ship_Detection_2018
"""""";General;https://github.com/AI-RG/rl-experiments
"""""";General;https://github.com/FrancescoSaverioZuppichini/ResNet
"""""";Audio;https://github.com/rishikksh20/LightSpeech
"""""";Computer Vision;https://github.com/wuhuikai/GP-GAN
"""""";Computer Vision;https://github.com/tyohei/chainerkfac
"""""";General;https://github.com/mcaseyrehm/pix2pixColab
"""""";General;https://github.com/tsc2017/Inception-Score
"""该系统实现了基于深度框架的语音识别中的声学模型和语言模型建模，其中声学模型包括CNN-CTC、GRU-CTC、CNN-RNN-CTC，语言模型包含[transformer](https://jalammar.github.io/illustrated-transformer/)、[CBHG](https://github.com/crownpku/Somiao-Pinyin)，数据集包含stc、primewords、Aishell、thchs30四个数据集。  本系统更整体介绍：https://blog.csdn.net/chinatelecom08/article/details/82557715  本项目现已训练一个迷你的语音识别系统，将项目下载到本地上，下载[thchs数据集](http://www.openslr.org/resources/18/data_thchs30.tgz)并解压至data，运行`test.py`，不出意外能够进行识别，结果如下：       the  0 th example.     文本结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文汉字： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然     识别结果： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然  若自己建立模型则需要删除现有模型，重新配置参数训练，具体实现流程参考本页最后。   """;General;https://github.com/StevenLai1994/AM_and_LM
"""""";Computer Vision;https://github.com/zhangbo2008/swin-transformer_noted_very_detail
"""""";Natural Language Processing;https://github.com/MinghaoYan/COMP480FinalProject
"""""";General;https://github.com/ChengBinJin/VAE-Tensorflow
"""""";Reinforcement Learning;https://github.com/saikrishna-1996/deep_pepper_chess
"""""";Computer Vision;https://github.com/shkim960520/YOLO-v1-for-studying
"""""";General;https://github.com/bioinf-jku/TTUR
"""""";General;https://github.com/thomlake/pytorch-attention
"""""";Computer Vision;https://github.com/ikostrikov/pytorch-flows
"""""";General;https://github.com/safi842/Microstructure-GAN
"""""";Computer Vision;https://github.com/RuoyuGuo/Visualising-Image-Classification-Models-and-Saliency-Maps
"""""";General;https://github.com/jingweiz/pytorch-dnc
"""""";General;https://github.com/AhmadQasim/FixMatch
"""In this module  we provide training data  network settings and loss designs for deep face recognition. The training data includes  but not limited to the cleaned MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  etc.. The loss functions include Softmax  SphereFace  CosineFace  ArcFace  Sub-Center ArcFace and Triplet (Euclidean/Angular) Loss.  You can check the detail page of our work [ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/ArcFace)(which accepted in CVPR-2019) and [SubCenter-ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/SubCenter-ArcFace)(which accepted in ECCV-2020).  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this module  you can simply achieve LFW 99.83%+ and Megaface 98%+ by a single model. This module can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   InsightFace is an open source 2D&3D deep face analysis toolbox  mainly based on MXNet.   The master branch works with **MXNet 1.2 to 1.6**  with **Python 3.x**.     """;General;https://github.com/vladimirwest/insightface_cinematic
"""""";Computer Vision;https://github.com/sanjaykrishnanrs/Project-7
"""""";Natural Language Processing;https://github.com/davidalbertonogueira/NLP-tutorials
"""""";Natural Language Processing;https://github.com/huggingface/tflite-android-transformers
"""""";General;https://github.com/mbinkowski/DeepSpeechDistances
"""""";Computer Vision;https://github.com/dungxibo123/vae
"""""";Natural Language Processing;https://github.com/ppke-nlpg/fastText_factored-cbow
"""""";Computer Vision;https://github.com/muditrastogi/chestai
"""**Refined Vision Transformer** is initially described in [arxiv](https://arxiv.org/abs/2106.03714)  which observes vision transformers require much more datafor model pre-training. Most of recent works thus are dedicated to designing morecomplex architectures or training methods to address the data-efficiency issue ofViTs. However  few of them explore improving the self-attention mechanism  akey factor distinguishing ViTs from CNNs.  Different from existing works  weintroduce a conceptually simple scheme  calledrefiner  to directly refine the self-attention maps of ViTs.  Specifically  refiner exploresattention expansionthatprojects the multi-head attention maps to a higher-dimensional space to promotetheir diversity.  Further  refiner applies convolutions to augment local patternsof the attention maps  which we show is equivalent to adistributed local atten-tion—features are aggregated locally with learnable kernels and then globallyaggregated with self-attention.  Extensive experiments demonstrate that refinerworks surprisingly well. Significantly  it enables ViTs to achieve 86% top-1 classifi-cation accuracy on ImageNet with only 81M parameters.  <p align=""center""> <img src=""https://github.com/zhoudaquan/Refiner_ViT/blob/master/figures/overall_flow.png"" | width=500> </p>  Please run git clone with --recursive to clone timm as submodule and install it with ` cd pytorch-image-models && pip install -e ./`    """;Computer Vision;https://github.com/zhoudaquan/Refiner_ViT
"""""";General;https://github.com/Aveek-Saha/Transformer
"""""";Natural Language Processing;https://github.com/SeoroMin/transformer_pytorch_ver2
"""""";Computer Vision;https://github.com/paulmthompson/StackedHourglass.jl
"""""";Natural Language Processing;https://github.com/neuralmind-ai/coliee
"""""";General;https://github.com/ruanvdmerwe/triplet-entropy-loss
"""""";Computer Vision;https://github.com/rdcarter1994/gitgan
"""""";General;https://github.com/mickan123/CycleGAN
"""""";General;https://github.com/infinitemugen/Attention-Conv-Pytorch
"""""";Computer Vision;https://github.com/YourGc/Unet_jinnan2
"""""";Computer Vision;https://github.com/Legoons/Melanoma_classification
"""""";Computer Vision;https://github.com/allanzelener/YAD2K
"""""";Graphs;https://github.com/shuix007/HMGNN
"""""";Computer Vision;https://github.com/fmu2/NICE
"""""";Computer Vision;https://github.com/AKASH2907/bird_species_classification
"""""";General;https://github.com/Michael-T-McCann/simple-WGAN
"""""";Natural Language Processing;https://github.com/Socialbird-AILab/BERT-Classification-Tutorial
"""""";General;https://github.com/0492wzl/tensorflow_slim_densenet
"""""";Computer Vision;https://github.com/karurb92/ldam_str_bn
"""Among all the skin cancer type  melanoma is the least common skin cancer  but it is responsible for **75%** of death [SIIM-ISIC Melanoma Classification  2020](https://www.kaggle.com/c/siim-isic-melanoma-classification). Being a less common skin cancer type but is spread very quickly to other body parts if not diagnosed early. The **International Skin Imaging Collaboration (ISIC)** is facilitating skin images to reduce melanoma mortality. Melanoma can be cured if diagnosed and treated in the early stages. Digital skin lesion images can be used to make a teledermatology automated diagnosis system that can support clinical decision.  Currently  deep learning has revolutionised the future as it can solve complex problems. The motivation is to develop a solution that can help dermatologists better support their diagnostic accuracy by ensembling contextual images and patient-level information  reducing the variance of predictions from the model.   """;Computer Vision;https://github.com/Tirth27/Skin-Cancer-Classification-using-Deep-Learning
"""""";Computer Vision;https://github.com/anantvir/Detecting_Localizing_Pneumonia_from_Chest_X-Ray_Scans
"""""";Computer Vision;https://github.com/cardwing/Codes-for-Lane-Detection
"""For this project  we will work with the [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment.  In this environment  a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus  the goal of your agent is to maintain its position at the target location for as many time steps as possible.  The observation space consists of 33 variables corresponding to position  rotation  velocity  and angular velocities of the arm. Each action is a vector with four numbers  corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.  The task is episodic  and in order to solve the environment   the agent must get an average score of +30 over 100 consecutive episodes.   """;Reinforcement Learning;https://github.com/biemann/Continuous-Control
"""""";Reinforcement Learning;https://github.com/hamishs/JAX-RL
"""""";Natural Language Processing;https://github.com/facebookresearch/code-prediction-transformer
"""""";Natural Language Processing;https://github.com/HazyResearch/hgcn
"""""";Reinforcement Learning;https://github.com/ShangtongZhang/DeepRL
"""""";Computer Vision;https://github.com/Qengineering/YoloV3-ncnn-Raspberry-Pi-4
"""""";Computer Vision;https://github.com/CodeChefVIT/VOID
"""""";General;https://github.com/ifrit98/lr_range_test
"""""";Computer Vision;https://github.com/NiucunCode/Deep-Learning-proj2
"""""";Computer Vision;https://github.com/MarkHershey/arxiv-dl
"""""";Computer Vision;https://github.com/klin059/lits
"""""";Reinforcement Learning;https://github.com/watchernyu/spinningup-drl-prototyping
"""""";General;https://github.com/deepmind/streetlearn
"""This is the repository for my trained Deep Deterministic Policy Gradient based agent on the Unity Reacher Enviroment from the Deep Reinforcement Learning nanodegree program. To 'solve' the environment the agent must navigate the Envirnoment with an average score of greater than 30 over the last 100 episodes. This repository provides the code to achieve this in 110 episodes.      """;Reinforcement Learning;https://github.com/Remtasya/DDPG-Actor-Critic-Reinforcement-Learning-Reacher-Environment
"""""";Computer Vision;https://github.com/solapark/da_yolo
"""""";General;https://github.com/github-jnauta/pytorch-pne
"""""";General;https://github.com/jsalbert/biotorch
"""""";General;https://github.com/vdumoulin/discgen
"""""";Computer Vision;https://github.com/PaddlePaddle/models
"""Inspired by the structure of Receptive Fields (RFs) in human visual systems  we propose a novel RF Block (RFB) module  which takes the relationship between the size and eccentricity of RFs into account  to enhance the discriminability and robustness of features. We further  assemble the RFB module to the top of SSD with a lightweight CNN model  constructing the RFB Net detector. You can use the code to train/evaluate the RFB Net for object detection. For more details  please refer to our [arXiv paper](https://arxiv.org/pdf/1711.07767.pdf).   <img align=""right"" src=""https://github.com/ruinmessi/RFBNet/blob/master/doc/rfb.png"">  &nbsp; &nbsp;   """;Computer Vision;https://github.com/dishen12/py03
"""The aim of this project is to dive into the field of action recognition and explore various techniques. Till now 2 models have been implemented <br/> 1 - The model.py  is a pytorch implementation of the paper - **A Closer Look at Spatiotemporal Convolutions for Action Recognition** Link to the paper is - **https://arxiv.org/abs/1711.11248v3**  2 - A pytorch implementation of MobileNets for less computational Models. Consult the paper - **MobileNetV2: Inverted Residuals and Linear Bottlenecks  - https://arxiv.org/abs/1801.04381v4**   """;Computer Vision;https://github.com/AD2605/Action-Recognition
"""""";General;https://github.com/kuangliu/pytorch-cifar
"""""";Computer Vision;https://github.com/eyalbetzalel/pytorch-generative
"""""";Computer Vision;https://github.com/Silver-L/OctConv
"""""";General;https://github.com/morganmcg1/ImageNette_ImageWoof_ImageWang
"""""";Natural Language Processing;https://github.com/prokopevaleksey/poincare-embeddings
"""""";Computer Vision;https://github.com/tianzhi0549/FCOS
"""""";Computer Vision;https://github.com/m1lhaus/SimpleSqueezeNet
"""""";Natural Language Processing;https://github.com/MESPA/npl
"""""";General;https://github.com/mbbrodie/stylegan2
"""""";General;https://github.com/ckyrkou/EmergencyNet
"""""";Computer Vision;https://github.com/sidneykingsley/pix2pix-tensorflow
"""""";Graphs;https://github.com/SabanciParallelComputing/GOSH
"""The goal of this project is to enable users to create cool web demos using the newly released OpenAI GPT-3 API **with just a few lines of Python.**   This project addresses the following issues:  1. Automatically formatting a user's inputs and outputs so that the model can effectively pattern-match 2. Creating a web app for a user to deploy locally and showcase their idea  Here's a quick example of priming GPT to convert English to LaTeX:  ``` #: Construct GPT object and show some examples gpt = GPT(engine=""davinci""            temperature=0.5            max_tokens=100) gpt.add_example(Example('Two plus two equals four'  '2 + 2 = 4')) gpt.add_example(Example('The integral from zero to infinity'  '\\int_0^{\\infty}')) gpt.add_example(Example('The gradient of x squared plus two times x with respect to x'  '\\nabla_x x^2 + 2x')) gpt.add_example(Example('The log of two times x'  '\\log{2x}')) gpt.add_example(Example('x squared plus y squared plus equals z squared'  'x^2 + y^2 = z^2'))  #: Define UI configuration config = UIConfig(description=""Text to equation""                    button_text=""Translate""                    placeholder=""x squared plus 2 times x"")  demo_web_app(gpt  config) ```  Running this code as a python script would automatically launch a web app for you to test new inputs and outputs with. There are already 3 example scripts in the `examples` directory.  You can also prime GPT from the UI. for that  pass `show_example_form=True` to `UIConfig` along with other parameters.  Technical details: the backend is in Flask  and the frontend is in React. Note that this repository is currently not intended for production use.   """;General;https://github.com/shreyashankar/gpt3-sandbox
"""""";Computer Vision;https://github.com/vita-epfl/rock-pytorch
"""In this two agent Unity environment the agents must learn to keep the ball in play and to hit the ball over the net enough times before it falls to the ground or outside the court.  An MADDPG agent is used to train the cooperating actor network policies.   """;Reinforcement Learning;https://github.com/petsol/MultiAgentCooperation_UnityAgent_MADDPG_Udacity
"""""";General;https://github.com/varshinireddyt/ULMFiT
"""""";General;https://github.com/NydiaAI/g-mlp-tensorflow
"""This is an implementation of our SV-X-Softmax loss by **Pytorch** library. The repository contains the fc_layers.py and loss.py The old version: ""Support Vector Guided Softmax Loss for Face Recognition"" [arxiv](https://arxiv.org/abs/1812.11317) \ is implemented by **Caffe** library and does not remove the overlaps between training set and test set. The performance comparsion  may not be fair in the old version.   """;General;https://github.com/xiaoboCASIA/SV-X-Softmax
"""""";Computer Vision;https://github.com/johnnylu305/deeplab-imagenet-pytorch
"""""";Computer Vision;https://github.com/noelcodes/cloth-recognition-version2
"""""";Natural Language Processing;https://github.com/Rick-McCoy/Reformer-pytorch
"""""";General;https://github.com/CodeChefVIT/VOID
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/kusti8/yolov3
"""""";Computer Vision;https://github.com/Mingtzge/2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement
"""""";Computer Vision;https://github.com/srihari-humbarwadi/retinanet-tensorflow2.x
"""""";Computer Vision;https://github.com/AzogDefiler/Sandbox
"""""";General;https://github.com/wangyuan249/Mymmt767
"""""";General;https://github.com/ahsanabbas123/Transformer
"""""";Computer Vision;https://github.com/leonardhan1979/fasterRCNN
"""""";Computer Vision;https://github.com/ayushdabra/dubai-satellite-imagery-segmentation
"""""";Computer Vision;https://github.com/AbhinavJhanwar/face_detection
"""""";Computer Vision;https://github.com/Demonhesusheng/darknet_v2
"""""";Computer Vision;https://github.com/zhuoyang125/simple_classifier
"""""";Computer Vision;https://github.com/Thinklab-SJTU/CSL_RetinaNet_Tensorflow
"""""";General;https://github.com/srihari-humbarwadi/adain-tensorflow2.x
"""""";Computer Vision;https://github.com/itsuki8914/wgan-gp-TensorFlow
"""""";General;https://github.com/wandb/awesome-dl-projects
"""""";Computer Vision;https://github.com/scrambleegg7/ssd_prescription
"""""";General;https://github.com/SevenZhan/Pytorch
"""""";Computer Vision;https://github.com/BUNN1E5/SLAM
"""<strong>Generative adversarial network</strong> contains the two components: generator and discriminator. The training process is just like zero-sum game  and it can be simply shown in Figure below.  <img src=""/asset/gan.png""/>  For generator  it should generate the image which is just like the real one. On the contrary  the discriminator should distinguish the image is fake or not. During the training  the generator should make itself have more capability to generate image which is more and more like the actual one  and the discriminator should make itself realize the difference with more and more accuracy.  The problem this paper is concerned with is that of unsupervised learning.Authors direct their attention towards<em>various ways to measure how close the model distribution and the real distribution are</em>  or equvalently on the various ways to define a distance or divergence  ρ(P<sub>θ</sub>  P<sub>r</sub>) where the real data distribution P<sub>r</sub> admits a density and P<sub>θ</sub> is the distribution of the parametrized density. The most fundamental difference between such distances is their impact on the convergence of sequences of probability distributions. In order to optimize the parameter θ  it is of course desirable to define our model distribution P<sub>θ</sub> in a manner that makes the mapping θ→P<sub>θ</sub> is continuous.   """;Computer Vision;https://github.com/akshay-gupta123/WGAN
"""""";General;https://github.com/NawafAlsuwailem/artathon_code
"""""";Computer Vision;https://github.com/keras-team/keras-applications
"""AMLA is a common framework to run different AutoML algorithms for neural networks without changing  the underlying systems needed to configure  train and evaluate the generated networks. This has two benefits: * It ensures that different AutoML algorithms can be easily compared using the same set of hyperparameters and infrastructure  allowing for  easy evaluation  comparison and ablation studies of AutoML algorithms. * It provides a easy way to deploy AutoML algorithms on multi-cloud infrastructure.  With a framework  we can manage the lifecycle of autoML easily. Without this  hyperparameters and architecture design are spread out  some embedded in the code  others in config files and other as command line parameters  making it hard to compare two algorithms or perform ablation studies.  Some design principles of AMLA: * The network generation process is decoupled from the training/evaluation process. * The network specification model is independent of the implementation of the training/evaluation/generation code and ML library (i.e. whether it uses TensorFlow/PyTorch etc.).  AMLA currently supports the [NAC using EnvelopeNets](http://arxiv.org/pdf/1803.06744) AutoML algorithm  and we are actively adding newer algorithms to the framework. More information on AutoML algorithms for Neural Networks can be found [here](https://github.com/hibayesian/awesome-automl-papers)   """;General;https://github.com/CiscoAI/amla
"""""";Computer Vision;https://github.com/oci-ai/oci-dreamer
"""""";Natural Language Processing;https://github.com/ProsusAI/finBERT
"""""";General;https://github.com/bentrevett/pytorch-language-modeling
"""""";Sequential;https://github.com/JunnYu/paddle_convbert
"""* **train.py** description for parameters(partly)    | Parameters           | Default                                            | Description                              |   | -------------------- | -------------------------------------------------- | ---------------------------------------- |   | **--image_dir**      | str: ‘/media/gallifrey/DJW/Dataset/Imagenet/train’ | Path for training data                   |   | **--continue_train** | bool: False                                        | wheather to continue training            |   | **--which_epoch**    | str: 'latest'                                      | start checkpoint                         |   | **--num_epoch**      | int: 20                                            | number of epoches                        |   | **--lr**             | float: 3.16e-5                                     | initial learning rate                    |   | **--rebalance**      | bool: True                                         | color rebalance or not                   |   | **--NN**             | int: 5                                             | number of neighor for KNN                |   | **--sigma**          | float: 5.0                                         | kernal size for gussian                  |   | **--gamma**          | float: 0.5                                         | coefficient for mixture of distributions |  * **test.py** description for parameters(partly)    | Parameters        | Default                                           | Description            |   | ----------------- | ------------------------------------------------- | ---------------------- |   | **--model_path**  | str： './model'                                   | path for models        |   | **--image_dir**   | str： ‘/media/gallifrey/DJW/Dataset/Imagenet/val' | path for testing data  |   | **--load_model**  | str: '19'                                         | checkpoint ID          |   | **--result_path** | str: './result'                                   | result path            |   | **--max_samples** | int: int(sys.maxsize)                             | max number to generate |     ![architecture](./imgs/architecture.png)  This project is based on the PaddlePaddle framework to reproduce the classical image colorization paper CIC (Colorful Image Colorization)  CIC is able to model the color channels for grayscale input and recover the color of the image. The innovation of this paper is to consider the prediction of color channels (ab) as a classification task  i.e.  the real ab channels are first encoded into 313 bins  and the forward process of the model is equivalent to performing 313 class classification. At the same time  in order to solve the problem that the image recovered color is affected by a large unsaturated area such as the background  the loss of each pixel is weighted according to the prior distribution of ab  which is essentially equivalent to doing color class balancing.  **Paper**  * [1] Zhang R    Isola P    Efros A A . Colorful Image Colorization[C]// European Conference on Computer Vision. Springer International Publishing  2016.  **Projects**  * [Official Caffe Implement](https://github.com/richzhang/colorization/tree/caffe) * [Unofficial Pytorch implement](https://github.com/Epiphqny/Colorization)  **Online Operation**  * Ai Studio job project：[https://aistudio.baidu.com/aistudio/clusterprojectdetail/2304371](https://aistudio.baidu.com/aistudio/clusterprojectdetail/2304371)     """;General;https://github.com/Callifrey/Paddle-CIC
"""""";Reinforcement Learning;https://github.com/wpiszlogin/driver_critic
"""""";Computer Vision;https://github.com/SeonghoBaek/CycleGAN
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/Soldie/insightface-Rec.Face
"""""";Computer Vision;https://github.com/u7javed/Conditional-WGAN-GP
"""""";General;https://github.com/ofirpress/attention_with_linear_biases
"""RetinaFace is a practical single-stage [SOTA](http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html) face detector which is initially described in [arXiv technical report](https://arxiv.org/abs/1905.00641)  ![demoimg1](https://github.com/deepinsight/insightface/blob/master/resources/11513D05.jpg)  ![demoimg2](https://github.com/deepinsight/insightface/blob/master/resources/widerfacevaltest.png)   """;General;https://github.com/1996scarlet/ArcFace-Multiplex-Recognition
"""""";Computer Vision;https://github.com/SfTI-Robotics/ROS-label-node
"""""";Computer Vision;https://github.com/guxiaowei1/yolov3--detection-of-rmb-code-region
"""""";General;https://github.com/xhlulu/arxiv-assistant
"""""";General;https://github.com/vene/sparse-structured-attention
"""""";General;https://github.com/qnduan/wgan-scrna
"""Salt formations are important in geology because they form one of the most important traps for hydrocarbons.  Interpretation on seismic images has long used texture attributes  to identify and highlight areas of interest. These can be seen like feature maps on the texture of the seismic. The texture of salt in a siesmic image is unique in the output images of collected siesmic data.   Geologists use 2D or 3D images of seismic images that have been heavily processed to study the subsurface for salt formations. Here I am going to use maching learning and computer vision to identify salt in a seismic image   """;Computer Vision;https://github.com/dbailey00/Semantic-Segmentation-With-Seismic-Images
"""""";General;https://github.com/nonslowrunner/GAN-Upscale-Image
"""""";Sequential;https://github.com/timrozday/spl-indications-bart
"""""";Reinforcement Learning;https://github.com/urw7rs/spiralpp
"""""";Reinforcement Learning;https://github.com/170928/-Review-Dueling-Deep-Q-Network
"""""";Natural Language Processing;https://github.com/fuqianya/ViLBERT-Paddle
"""""";Computer Vision;https://github.com/it6aidl/outdoorsegmentation
"""""";Computer Vision;https://github.com/DongyaoZhu/VQ-VAE-WaveNet
"""""";Computer Vision;https://github.com/facebookresearch/detectron2
"""""";Computer Vision;https://github.com/yueruchen/sppnet-pytorch
"""""";Computer Vision;https://github.com/HenningBuhl/VQ-VAE_Keras_Implementation
"""""";General;https://github.com/aelnouby/Text-to-Image-Synthesis
"""""";Reinforcement Learning;https://github.com/tcmxx/CNTKUnityTools
"""""";General;https://github.com/bzhangGo/zero
"""""";General;https://github.com/CanshangD/ResNet_tensorflow2.0
"""""";Natural Language Processing;https://github.com/keithRebello/ULMFiT_sentiment_analysis
"""""";Natural Language Processing;https://github.com/octanove/grammartagger
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/pesong/ssd-caffe
"""Distribuuuu is a Distributed Classification Training Framework powered by native PyTorch.  Please check [tutorial](./tutorial/) for detailed **Distributed Training** tutorials:  - Single Node Single GPU Card Training [[snsc.py](./tutorial/snsc.py)] - Single Node Multi-GPU Cards Training (with DataParallel) [[snmc_dp.py](./tutorial/snmc_dp.py)] - Multiple Nodes Multi-GPU Cards Training (with DistributedDataParallel)     - torch.distributed.launch [[mnmc_ddp_launch.py](./tutorial/mnmc_ddp_launch.py)]     - torch.multiprocessing [[mnmc_ddp_mp.py](./tutorial/mnmc_ddp_mp.py)]     - Slurm Workload Manager [[mnmc_ddp_slurm.py](./tutorial/mnmc_ddp_slurm.py)] - ImageNet training example [[imagenet.py](./tutorial/imagenet.py)]  For the complete training framework  please see [distribuuuu](./distribuuuu/).    """;General;https://github.com/BIGBALLON/distribuuuu
"""""";Natural Language Processing;https://github.com/pengbaolin/SC-GPT
"""""";Natural Language Processing;https://github.com/HarshKhandelwal1552/Language_classifier_with_Naive_Bayes
"""""";Natural Language Processing;https://github.com/ethancaballero/neural-engineers-first-attempt
"""""";General;https://github.com/maroxtn/tun-sentiment
"""""";Computer Vision;https://github.com/DaneyAlex5/Deep-Zooming-of-Images-using-SRGAN
"""""";General;https://github.com/ConcurrencyPractitioner/DeepLearning
"""""";Computer Vision;https://github.com/angelvillar96/FaceEmoji
"""""";General;https://github.com/sohelbhuiyan/pytorch-classification
"""""";Computer Vision;https://github.com/LebronGG/PointNet
"""""";General;https://github.com/tbullmann/imagetranslation-tensorflow
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/dbonner/tf1_bert
"""""";Computer Vision;https://github.com/serengil/deepface
"""""";Computer Vision;https://github.com/xiey1/Nucleus_detection
"""""";Computer Vision;https://github.com/liuruiyang98/Jittor-MLP
"""""";Computer Vision;https://github.com/soxHenry433/TF2_U2Model
"""""";Computer Vision;https://github.com/podgorskiy/StyleGan
"""""";Graphs;https://github.com/benedekrozemberczki/AttentionWalk
"""""";General;https://github.com/ls-da3m0ns/Short-ResNet
"""""";General;https://github.com/yangorwell/NGPlus
"""""";Sequential;https://github.com/uclanlp/NamedEntityLanguageModel
"""""";Graphs;https://github.com/ngminhhieu/DGI
"""**GCNet** is initially described in [arxiv](https://arxiv.org/abs/1904.11492). Via absorbing advantages of Non-Local Networks (NLNet) and Squeeze-Excitation Networks (SENet)   GCNet provides a simple  fast and effective approach for global context modeling  which generally outperforms both NLNet and SENet on major benchmarks for various recognition tasks.   """;General;https://github.com/zhusiling/GCNet
"""""";Computer Vision;https://github.com/fengye-lu/PyTorch-SRGAN
"""""";General;https://github.com/liuzeyuMr/CycleGAN-TensorFlow-master
"""""";Computer Vision;https://github.com/DakshIdnani/pytorch-nice
"""""";Computer Vision;https://github.com/opeco17/pointnet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/socc-io/piqaboo
"""""";General;https://github.com/Julian1070/Deep-Learning
"""""";General;https://github.com/jaykshirsagar05/CrowdCounting
"""""";General;https://github.com/bhargavajs07/Packed-Wasserstein-GAN-with-GradientPenalty-Example
"""""";General;https://github.com/raymon-tian/hourglass-facekeypoints-detection
"""""";General;https://github.com/makcedward/nlpaug
"""""";General;https://github.com/xslidi/EfficientNets_ddl_apex
"""""";Computer Vision;https://github.com/imatge-upc/3D-GAN-superresolution
"""""";Natural Language Processing;https://github.com/Yale-LILY/dart
"""""";Computer Vision;https://github.com/Meituan-AutoML/Twins
"""This code tries to implement the Neural Turing Machine  as found in  https://arxiv.org/abs/1410.5401  as a backend neutral recurrent keras layer.  A very default experiment  the copy task  is provided  too.  In the end there is a TODO-List. Help would be appreciated!  NOTE: * There is a nicely formatted paper describing the rough idea of the NTM  implementation difficulties and which discusses the   copy experiment. It is available here in the repository as The_NTM_-_Introduction_And_Implementation.pdf.  * You may want to change the LOGDIR_BASE in testing_utils.py to something that works for you or just set a symbolic   link.    """;General;https://github.com/flomlo/ntm_keras
"""""";Computer Vision;https://github.com/Jintao-Huang/Darknet53_PyTorch
"""""";Sequential;https://github.com/astanway/gated-conv-nets
"""""";General;https://github.com/apmoore1/language-model
"""""";Computer Vision;https://github.com/sokeeffe/caffe_yolo
"""""";Computer Vision;https://github.com/wutianze/darknet_rtmp_demo
"""""";Computer Vision;https://github.com/cds-mipt/cds-tracking
"""""";Computer Vision;https://github.com/xieenze/PolarMask
"""""";Computer Vision;https://github.com/krantirk/py-faster-rcnn
"""""";Computer Vision;https://github.com/cryu854/ArbitraryStyle-tfjs
"""""";Natural Language Processing;https://github.com/folivetti/HBLCoClust
"""""";Computer Vision;https://github.com/IEWbgfnYDwHRoRRSKtkdyMDUzgdwuBYgDKtDJWd/doldy
"""""";Computer Vision;https://github.com/namepen/AI_school_proj
"""""";Computer Vision;https://github.com/Baichenjia/Resnet
"""""";Computer Vision;https://github.com/Qengineering/ShuffleNetV2-ncnn
"""""";Computer Vision;https://github.com/Aarav-Patel/darknet
"""""";General;https://github.com/pcummer/cycle-gan-keras
"""Macaw (<b>M</b>ulti-<b>a</b>ngle <b>c</b>(q)uestion <b>a</b>ns<b>w</b>ering) is a ready-to-use model capable of general  question answering  showing robustness outside the domains it was  trained on. It has been trained in ""multi-angle"" fashion  which means it can handle a flexible set of input and output ""slots"" (like question  answer  explanation) .  Macaw was built on top of [T5](https://github.com/google-research/text-to-text-transfer-transformer) and  comes in different sizes:  [macaw-11b](https://huggingface.co/allenai/macaw-11b)  [macaw-3b](https://huggingface.co/allenai/macaw-3b)   and [macaw-large](https://huggingface.co/allenai/macaw-large)  as well as an answer-focused version featured on  various leaderboards: [macaw-answer-11b](https://huggingface.co/allenai/macaw-answer-11b) (see [below](#training-data)).   """;Natural Language Processing;https://github.com/allenai/macaw
"""""";Computer Vision;https://github.com/PhanTom2003/SSD
"""This is a half-semester project  an ongoing attempt to extend the original work by:   1. Using LIDAR point cloud data instead of RGB-D data   2. Using bird's eye view representation of LIDAR data to generate additional 3D bounding box proposals from the top view  The below figure shows the *proposed modification* to the original work. The components (at the top) connected by red arrows are what we are going to add.  ![proposed_architecture](https://github.com/witignite/Frustum-PointNets/blob/master/doc/Fig_3_ProposedArchitecture.PNG)  The purpose is to experiment on using the bird's eye view LIDAR data to help improving the performance in some extreme condition  such as low light or occlusion  where it may be difficult to detect the 2D bounding boxes in the image (and hence no 3D object will be detected). By using the bird's eye view LIDAR data  we expect to obtain additional 3D box proposals  which will be combined with the original proposals to improve the performance.   """;Computer Vision;https://github.com/witignite/Frustum-PointNet
"""""";Computer Vision;https://github.com/dmbernaal/Daedalus
"""""";Natural Language Processing;https://github.com/boom85423/Seq2seq-model-for-Meme-Generator
"""""";Natural Language Processing;https://github.com/johntiger1/multitask_multimodal
"""""";General;https://github.com/arnavdodiedo/DenseNet-MNIST
"""""";Computer Vision;https://github.com/skaldek/InceptionResNetV2
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/dzqjorking/transpose
"""""";Computer Vision;https://github.com/tarun005/FLAVR
"""""";Computer Vision;https://github.com/phungpx/efficient_det_pytorch
"""""";Computer Vision;https://github.com/suhuijia/pggan
"""""";Computer Vision;https://github.com/praveenk2k7/Image-CLassifier_MobileNet
"""""";Computer Vision;https://github.com/godspeedcurry/lung-nodule-detection
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/VaanHUANG/CSCI5210HW1
"""""";General;https://github.com/simonjisu/E2EMN
"""**ELECTRA** is a method for self-supervised language representation learning. It can be used to pre-train transformer networks using relatively little compute. ELECTRA models are trained to distinguish ""real"" input tokens vs ""fake"" input tokens generated by another neural network  similar to the discriminator of a [GAN](https://arxiv.org/pdf/1406.2661.pdf). At small scale  ELECTRA achieves strong results even when trained on a single GPU. At large scale  ELECTRA achieves state-of-the-art results on the [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset.  For a detailed description and experimental results  please refer to our ICLR 2020 paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/pdf?id=r1xMH1BtvB).  This repository contains code to pre-train ELECTRA  including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g . [GLUE](https://gluebenchmark.com/))  QA tasks (e.g.  [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/))  and sequence tagging tasks (e.g.  [text chunking](https://www.clips.uantwerpen.be/conll2000/chunking/)).  This repository also contains code for **Electric**  a version of ELECTRA inspired by [energy-based models](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf). Electric provides a more principled view of ELECTRA as a ""negative sampling"" [cloze model](https://en.wikipedia.org/wiki/Cloze_test). It can also efficiently produce [pseudo-likelihood scores](https://arxiv.org/pdf/1910.14659.pdf) for text  which can be used to re-rank the outputs of speech recognition or machine translation systems. For details on Electric  please refer to out EMNLP 2020 paper [Pre-Training Transformers as Energy-Based Cloze Models](https://www.aclweb.org/anthology/2020.emnlp-main.20.pdf).     """;Natural Language Processing;https://github.com/google-research/electra
"""""";General;https://github.com/AmrMKayid/nam
"""""";General;https://github.com/gs18113/ESPCN-TensorFlow2
"""""";Sequential;https://github.com/SaremS/MANN
"""""";General;https://github.com/flowersteam/explauto
"""""";Computer Vision;https://github.com/christiancosgrove/pytorch-spectral-normalization-gan
"""""";Reinforcement Learning;https://github.com/bonniesjli/PPO_Reacher
"""""";General;https://github.com/deciding/ParallelWaveGAN
"""""";Sequential;https://github.com/rizwan09/paper
"""""";Natural Language Processing;https://github.com/rajshrivastava/Skipgram
"""The description made above remains valid here ``` #:#: main parameters reload_model     #: model to reload for encoder decoder #:#: data location / training objective ae_steps          #: denoising auto-encoder training steps bt_steps          #: back-translation steps mt_steps          #: parallel training steps word_shuffle      #: noise for auto-encoding loss word_dropout      #: noise for auto-encoding loss word_blank        #: noise for auto-encoding loss lambda_ae         #: scheduling on the auto-encoding coefficient  #:#: transformer parameters encoder_only      #: use a decoder for MT  #:#: optimization tokens_per_batch  #: use batches with a fixed number of words eval_bleu         #: also evaluate the BLEU score ```  ``` #:#: main parameters exp_name                     #: experiment name exp_id                       #: Experiment ID dump_path                    #: where to store the experiment (the model will be stored in $dump_path/$exp_name/$exp_id)  #:#: data location / training objective data_path                    #: data location  lgs                          #: considered languages/meta-tasks clm_steps                    #: CLM objective mlm_steps                    #: MLM objective  #:#: transformer parameters emb_dim                      #: embeddings / model dimension n_layers                     #: number of layers n_heads                      #: number of heads dropout                      #: dropout attention_dropout            #: attention dropout gelu_activation              #: GELU instead of ReLU  #:#: optimization batch_size                   #: sequences per batch bptt                         #: sequences length optimizer                    #: optimizer epoch_size                   #: number of sentences per epoch max_epoch                    #: Maximum epoch size validation_metrics           #: validation metric (when to save the best model) stopping_criterion           #: end experiment if stopping criterion does not improve  #:#: dataset #:#:#:#: These three parameters will always be rounded to an integer number of batches  so don't be surprised if you see different values than the ones provided. train_n_samples              #: Just consider train_n_sample train data valid_n_samples              #: Just consider valid_n_sample validation data  test_n_samples               #: Just consider test_n_sample test data for #:#:#:#: If you don't have enough RAM/GPU or swap memory  leave these three parameters to True  otherwise you may get an error like this when evaluating : #:#:#:#:#:#: RuntimeError: copy_if failed to synchronize: cudaErrorAssert: device-side assert triggered remove_long_sentences_train #: remove long sentences in train dataset       remove_long_sentences_valid #: remove long sentences in valid dataset   remove_long_sentences_test  #: remove long sentences in test dataset   ```   """;General;https://github.com/Tikquuss/meta_XLM
"""""";Computer Vision;https://github.com/creotiv/hdrnet-pytorch
"""""";General;https://github.com/vlievin/Unet
"""""";Computer Vision;https://github.com/gorjida/ap-unet
"""""";General;https://github.com/erickgalinkin/dropout_privacy
"""""";Computer Vision;https://github.com/foamliu/StyleGAN-PyTorch
"""""";General;https://github.com/valeoai/SemanticPalette
"""""";Computer Vision;https://github.com/TimoKuenstle/timeseries
"""""";General;https://github.com/karan96/NewOne
"""""";Natural Language Processing;https://github.com/gooofy/zbrain
"""""";General;https://github.com/dmbernaal/Daedalus
"""""";Reinforcement Learning;https://github.com/Chan1998/MAAC
"""""";Natural Language Processing;https://github.com/yzh119/BPT
"""""";General;https://github.com/TinDang97/face_recognition
"""""";Graphs;https://github.com/BIG-S2/keras-gnm
"""""";General;https://github.com/FLHonker/Losses-in-image-classification-task
"""""";Graphs;https://github.com/weihua916/powerful-gnns
"""""";General;https://github.com/SwinTransformer/Transformer-SSL
"""""";General;https://github.com/UdbhavPrasad072300/Image-Super-Resolution
"""""";Reinforcement Learning;https://github.com/dchetelat/acer
"""""";Computer Vision;https://github.com/openai/improved-gan
"""""";Computer Vision;https://github.com/deolipankaj/Stone_Detection_MRCNN
"""""";General;https://github.com/cwvisuals/StyleGAN2
"""""";General;https://github.com/t-ae/singan-s4tf
"""""";General;https://github.com/mshunshin/SegNetCMR
"""""";Natural Language Processing;https://github.com/ky1994/SpeechRecognition
"""""";General;https://github.com/facebookresearch/adaptive-softmax
"""""";Computer Vision;https://github.com/wiegehtki/zoneminder-jetson
"""""";Natural Language Processing;https://github.com/thinkwee/DPP_CNN_Summarization
"""""";Computer Vision;https://github.com/vijishmadhavan/ArtLine
"""""";Reinforcement Learning;https://github.com/createamind/softlearning
"""""";Computer Vision;https://github.com/danieldritter/RL_Implementations
"""""";General;https://github.com/akhadangi/EM-net
"""""";Natural Language Processing;https://github.com/zaradana/Fast_BERT
"""""";General;https://github.com/sidward14/gan-lab
"""""";Computer Vision;https://github.com/facebookresearch/augmentation-corruption
"""""";Computer Vision;https://github.com/FlorentijnD/FairMOT
"""""";Computer Vision;https://github.com/preste-nakam/AI_whiteboard
"""""";Natural Language Processing;https://github.com/IBM/pytorch-seq2seq
"""""";General;https://github.com/GKalliatakis/Keras-VGG16-places365
"""""";Computer Vision;https://github.com/VIRUS-ATTACK/DCGAN-GPU-IMPLEMENTATION
"""""";General;https://github.com/ky1994/SpeechRecognition
"""""";Computer Vision;https://github.com/heuritech/convnets-keras
"""**RepPoints**  initially described in [arXiv](https://arxiv.org/abs/1904.11490)  is a new representation method for visual objects  on which visual understanding tasks are typically centered. Visual object representation  aiming at both geometric description and appearance feature extraction  is conventionally achieved by `bounding box + RoIPool (RoIAlign)`. The bounding box representation is convenient to use; however  it provides only a rectangular localization of objects that lacks geometric precision and may consequently degrade feature quality. Our new representation  RepPoints  models objects by a `point set` instead of a `bounding box`  which learns to adaptively position themselves over an object in a manner that circumscribes the object’s `spatial extent` and enables `semantically aligned feature extraction`. This richer and more flexible representation maintains the convenience of bounding boxes while facilitating various visual understanding applications. This repo demonstrated the effectiveness of RepPoints for COCO object detection.  Another feature of this repo is the demonstration of an `anchor-free detector`  which can be as effective as state-of-the-art anchor-based detection methods. The anchor-free detector can utilize either `bounding box` or `RepPoints` as the basic object representation.  <div align=""center"">   <img src=""demo/reppoints.png"" width=""400px"" />   <p>Learning RepPoints in Object Detection.</p> </div>   """;Computer Vision;https://github.com/microsoft/RepPoints
"""""";General;https://github.com/Heliang-Zheng/Style2
"""""";General;https://github.com/kiransom/mnist_vae
"""""";Computer Vision;https://github.com/nweakly/MSDSProject-II
"""""";General;https://github.com/vglsd/OpenSESAME
"""""";Reinforcement Learning;https://github.com/JulT1/RL_SS19
"""""";Reinforcement Learning;https://github.com/EconomistGrant/HTFE-tensortrade
"""""";Sequential;https://github.com/asappresearch/sru
"""""";Computer Vision;https://github.com/ayiyoh/style-gan-pytorch
"""**QA_PreProcess.py\QA_PreProcess.ipynb:** Converts the raw induction tasks data set to separate ndarrays containing questions  answers  and facts with all words being in the form of GloVe pre-trained vector representations.    **DMN+.py\DMN+.ipynb:** The DMN+ model  along with training  validation and testing.    """;General;https://github.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master
"""""";Computer Vision;https://github.com/freegyp/stylegan-keras-ece655
"""""";Reinforcement Learning;https://github.com/Codernauti/Exploration-of-DQN-in-CartPole-Environment
"""SqueezeNet is focused on size and performance over outright accuracy  however  it still achieved AlexNet-level accuracy in the paper by Iandola in 2016. The actual SqueezeNet architecture is different than what I will refer to as 'Squeeze Net' so I encourage you to read the paper (cited below) and visit the [Deepscale/SqueezeNet github page](https://github.com/deepscale/squeezenet). My model did not reach [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)-level accuracy (89%) but did reach approximately 80% with only 122k parameters (AlexNet is ~ 60million  VGG is 130million+). Additionally  my model is much smaller than even that referenced in the SqueezeNet paper.    """;Computer Vision;https://github.com/zshancock/SqueezeNet_vs_CIFAR10
"""""";Computer Vision;https://github.com/titu1994/DenseNet
"""""";Computer Vision;https://github.com/SharifElfouly/easy-model-zoo
"""""";Natural Language Processing;https://github.com/ufal/wembedding_service
"""""";General;https://github.com/pren1/Group-Normalization-Example
"""""";Computer Vision;https://github.com/nilesh0109/PedestrianTracking
"""""";Computer Vision;https://github.com/istvanmegyeri/audio_tagging
"""""";Computer Vision;https://github.com/shelhamer/fcn.berkeleyvision.org
"""""";Natural Language Processing;https://github.com/PSCSeifu/TheLibrary
"""""";Natural Language Processing;https://github.com/whr94621/NJUNMT-pytorch
"""""";Sequential;https://github.com/spratskevich/Lemmatizer
"""In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   """;General;https://github.com/yitu-opensource/ConvBert
"""""";General;https://github.com/DLHacks/pix2pix_PAN
"""""";General;https://github.com/hpnair/18663_Project_FBNet
"""""";Computer Vision;https://github.com/jeonsworld/ViT-pytorch
"""We propose AugMix  a data processing technique that mixes augmented images and enforces consistent embeddings of the augmented images  which results in increased robustness and improved uncertainty calibration. AugMix does not require tuning to work correctly  as with random cropping or CutOut  and thus enables plug-and-play data augmentation. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks  closing the gap between previous methods and the best possible performance by more than half in some cases. With AugMix  we obtain state-of-the-art on ImageNet-C  ImageNet-P and in uncertainty estimation when the train and test distribution do not match.  For more details please see our [ICLR 2020 paper](https://arxiv.org/pdf/1912.02781.pdf).   """;Computer Vision;https://github.com/google-research/augmix
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/darkAlert/yolov3-rt
"""""";Computer Vision;https://github.com/Atlas200dk/sample-imageinpainting-HiFill
"""""";Natural Language Processing;https://github.com/dewanderelex/LanguageTranslation
"""""";Computer Vision;https://github.com/justinessert/hierarchical-deep-cnn
"""""";General;https://github.com/ericjang/odin
"""""";General;https://github.com/lucidrains/compressive-transformer-pytorch
"""""";Computer Vision;https://github.com/Stomper10/CheXpert
"""""";General;https://github.com/sen-pai/audio-word2vec-pytorch
"""""";Computer Vision;https://github.com/XuyangSHEN/lane_detection_enet
"""""";Natural Language Processing;https://github.com/KnightZhang625/BERT_TF
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/deepset-ai/bert-tensorflow
"""""";Computer Vision;https://github.com/open-mmlab/OpenPCDet
"""WIDER Face Dataset WIDER FACE dataset is a Face Mask Segmentation benchmark dataset  of which images are selected from the publicly available WIDER dataset.  This data have 32 203 images and 393 703 faces are labeled with a high degree of variability in scale  pose and occlusion as depicted in the sample images. In this project  we are using 409 images and around 1000 faces for ease of computation.    We will be using transfer learning on an already trained model to build our segmenter. We will perform transfer learning on the MobileNet model which is already trained to perform image segmentation. We will need to train the last 6-7 layers and freeze the remaining layers to train the model for face mask segmentation. To be able to train the MobileNet model for face mask segmentation  we will be using the WIDER FACE dataset for various images with a single face and multiple faces. The output of the model is the face mask segmented data which masks the face in an image. We learn to build a face mask segmentation model using Keras supported by Tensorflow.    """;Computer Vision;https://github.com/amol-matkar/Face-Mask-Segmentation
"""""";Computer Vision;https://github.com/krrish94/nerf-pytorch
"""""";Computer Vision;https://github.com/ChielBruin/ros_faster_rcnn
"""""";Computer Vision;https://github.com/manishravula/yolov3_tiny_pruned
"""""";General;https://github.com/IBM/MAX-Image-Resolution-Enhancer
"""""";Computer Vision;https://github.com/daintlab/ct-denoising
"""""";Computer Vision;https://github.com/ShiyuLiang/odin-pytorch
"""""";Natural Language Processing;https://github.com/soyoung97/awd-lstm-gru
"""""";Sequential;https://github.com/mdiannna/Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs-adaptation
"""""";General;https://github.com/stefanthaler/dl-papers-imlemented
"""""";Computer Vision;https://github.com/divyachandana/solo-semantic-segmentation
"""""";Computer Vision;https://github.com/unsky/RetinaNet
"""""";Computer Vision;https://github.com/tarujg/domain-adapt
"""This is an implementation of submitted paper:   Desai  S. V.  Balasubramanian  V. N.  Fukatsu  T.  Ninomiya  S.  & Guo  W. (2019). Automatic estimation of heading date of paddy rice using deep learning. Plant Methods  15(1)  76. https://doi.org/10.1186/s13007-019-0457-1   To plan the perfect time for harvest of rice crops  we detect and quantify the flowering of paddy rice. The dataset of rice crop images used in this work is taken from [1].   """;Computer Vision;https://github.com/svdesai/heading-date-estimation
"""""";Graphs;https://github.com/tkipf/pygcn
"""""";Computer Vision;https://github.com/Jay0505/CycleGAN
"""""";Computer Vision;https://github.com/ruiliu-ai/DivCo
"""""";Computer Vision;https://github.com/azy64/Deep-Learning
"""""";Sequential;https://github.com/MiguelMonteiro/CRFasRNNLayer
"""""";Computer Vision;https://github.com/ajithvallabai/getsetgo_keras-beginner
"""""";Computer Vision;https://github.com/minhcong0891/wheat-detection
"""I used the pandas library to calculate summary statistics of the traffic signs data set:  * Number of training examples = 34799 * Number of validation examples = 4410 * Number of testing examples = 12630 * Image data shape = (32  32  3) * Number of classes = 43   """;Computer Vision;https://github.com/waynecoffee9/Traffic-Sign-Classifier
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/yuhangT/tf_bert
"""""";Computer Vision;https://github.com/IrvingMeng/MagFace
"""""";Computer Vision;https://github.com/oadonca/ANVAE
"""""";Computer Vision;https://github.com/bhfs9999/maskrcnn_isic
"""""";General;https://github.com/lucashueda/long_sentence_transformer
"""Sonnet has been designed and built by researchers at DeepMind. It can be used to construct neural networks for many different purposes (un/supervised learning  reinforcement learning  ...). We find it is a successful abstraction for our organization  you might too!  More specifically  Sonnet provides a simple but powerful programming model centered around a single concept: `snt.Module`. Modules can hold references to parameters  other modules and methods that apply some function on the user input. Sonnet ships with many predefined modules (e.g. `snt.Linear`  `snt.Conv2D`  `snt.BatchNorm`) and some predefined networks of modules (e.g. `snt.nets.MLP`) but users are also encouraged to build their own modules.  Unlike many frameworks Sonnet is extremely unopinionated about **how** you will use your modules. Modules are designed to be self contained and entirely decoupled from one another. Sonnet does not ship with a training framework and users are encouraged to build their own or adopt those built by others.  Sonnet is also designed to be simple to understand  our code is (hopefully!) clear and focussed. Where we have picked defaults (e.g. defaults for initial parameter values) we try to point out why.   """;General;https://github.com/deepmind/sonnet
"""This is an implementation of global context module in GCNet.(GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond)<https://arxiv.org/abs/1904.11492> I just modified it from the official code.I hope this can help some tensorflow users. **Please let me know if there is something wrong with my code.** Official code repository<https://github.com/xvjiarui/GCNet>  """;General;https://github.com/xggIoU/GCNet_global_context_module_tensorflow
"""""";Reinforcement Learning;https://github.com/Alina9/SAC
"""""";Graphs;https://github.com/priba/siamese_ged
"""""";Natural Language Processing;https://github.com/paulskeie/stadnamn
"""""";General;https://github.com/CellEight/Pytorch-Adaptive-Instance-Normalization
"""""";Computer Vision;https://github.com/raymon-tian/hourglass-facekeypoints-detection
"""""";Natural Language Processing;https://github.com/obj2vec/obj2vec
"""""";General;https://github.com/curto2/mckernel
"""Despite the plethora of different models for deep learning on graphs  few approaches have been proposed thus far for dealing with graphs that present some sort of dynamic nature (e.g. evolving features or connectivity over time).   In this paper  we present Temporal Graph Networks (TGNs)  a generic  efficient framework for deep learning on dynamic graphs represented as sequences of timed events. Thanks to a novel combination of memory modules and graph-based operators  TGNs are able to significantly outperform previous approaches being at the same time more computationally efficient.   We furthermore show that several previous models for learning on dynamic graphs can be cast as specific instances of our framework. We perform a detailed ablation study of different components of our framework and devise the best configuration that achieves state-of-the-art performance on several transductive and inductive prediction tasks for dynamic graphs.    """;Graphs;https://github.com/twitter-research/tgn
"""""";General;https://github.com/NUS-HPC-AI-Lab/LARS-ImageNet-PyTorch
"""""";General;https://github.com/tkuanlun350/Kaggle_Ship_Detection_2018
"""""";Computer Vision;https://github.com/yubaoliu/caffe-segnet
"""""";General;https://github.com/aredier/monte_carlo_dropout
"""""";General;https://github.com/AdeelH/pytorch-multi-class-focal-loss
"""""";Computer Vision;https://github.com/EmptySamurai/pytorch-reconet
"""""";Computer Vision;https://github.com/woctezuma/stylegan2-projecting-images
"""""";Computer Vision;https://github.com/Masquerade51256/DCGAN_SignPicture
"""""";General;https://github.com/fchollet/deep-learning-models
"""""";Reinforcement Learning;https://github.com/Gouet/DDPG_PendulumV1
"""""";Computer Vision;https://github.com/NadimKawwa/DCGAN_faces
"""""";General;https://github.com/mori97/CycleGAN-chainer
"""""";Computer Vision;https://github.com/zhangchi9/Airbus_Ship_Detection
"""""";General;https://github.com/keyuchen886/GoodReads
"""""";Natural Language Processing;https://github.com/kimiyoung/transformer-xl
"""Augmented Implementation of the [pSp implementation](https://github.com/eladrich/pixel2style2pixel) to train Images on white noise reduction   """;Computer Vision;https://github.com/rahuls02/Image-Noise-Reduction
"""""";General;https://github.com/leon-schi/ColorNet
"""""";General;https://github.com/racinmat/lecture-generator
"""Fire is very useful discovery of the humanity. Like any other human invention  mis-handeling of fire can cause huge damage to the humanity and nature. Every year  urban fire results huge life and property damage. Similary the tragic loss of natural resources in uncontroleld wildfire is well known. The control of wildfire has becoming a huge challenge by using the treditional technologies. On the other hand  recently  the advancement in technology and especially the machine learning have benifited the society a lot in diverse aspects  ranging from self driving car to cancer research. Hence  it can surely contribute to the technology to early detect the fire so that we can react it earlier before getting it worse and making a lots of damage or being it out of control. We can train a deep neural network to distinguish fire and non-fire situation with very good accuracy. This technology aided with suitable hardware design can be vary useful to minimize the fire hazard. In this work we train a convolutional neural network which can achieve out of sample accuracy of 97% classifying the fire and non-fire images.      """;Computer Vision;https://github.com/roshankoirala/Fire_Detection_Model
"""""";General;https://github.com/AnzorGozalishvili/autoencoders_playground
"""The economy of online shopping is bigger than it has ever been and continues to grow each year. It follows that the market for being able to deliver goods quickly and reliably is becoming more and more competitive. In addition to improving the overall flow of transactions  knowing when a package will be delivered is a major factor in customer satisfaction  making the ability to accurately predict delivery dates essential to companies such as eBay.  The process of achieving this  however  poses many challenges. In the case of eBay  the majority of transactions are carried out between individual sellers and buyers  often resulting in the data for goods and their delivery being inconsistently recorded. This  in addition to packages being processed by a variety of different delivery services  means that data labels are frequently missing or incorrect. Further  the shipment date is largely left to the sole decision of each individual seller  resulting in a high degree of variability.  ![image of shipping process](/images/diagram2.png)  We worked to provide a solution to these problems and provide a model to enable the accurate prediction of delivery dates using machine learning.  To implement our model  a number of decisions were made and tested  including deciding the optimal means by which to clean the data (for instance  whether to omit training data that has missing or incorrect features or to assign it average values based on correctly labeled training data)  deciding whether to compute the estimation holistically from shipment to delivery date or to compute multiple separate estimates on separate legs of the delivery process  and deciding which features to include and in which leg.  Should our model have some error  it is important that it produces random rather than systematic error. Specifically  we want to avoid creating a model which might consistently predict early delivery dates  which could lead to sellers and delivery services rushing packages and resulting in the employment of more non-sustainable methods  such as shipping half-full boxes  as well as increasing the pressure on employees to have to work faster and faster.  Using a loss function of the weighted average absolute error of the delivery predictions in days that was provided by eBay  our model achieved a loss of 0.411 where a loss of 0.759 was the baseline of random guessing.    """;Computer Vision;https://github.com/milliemince/eBay-shipping-predictions
"""""";General;https://github.com/HeshamElAbd/SelfAttentionLangModel
"""We propose a deep learning based model and well-organized dataset for context aware paper citation recommendation.  Our model comprises a document encoder and a context encoder  which uses Graph Convolutional Networks ([GCN](https://arxiv.org/abs/1611.07308))  layer and Bidirectional Encoder Representations from Transformers ([BERT](https://arxiv.org/abs/1810.04805))  which is a pretrained model of textual data.  By modifying the related PeerRead AAN dataset  we propose a new dataset called FullTextPeerRead  FullTextANN containing  context sentences to cited references and paper metadata.   ![](https://i.imgur.com/FzmbImx.png)  The code is based on that([BERT](https://github.com/google-research/bert)  [GCN](https://github.com/tkipf/gae/)).   """;Natural Language Processing;https://github.com/TeamLab/bert-gcn-for-paper-citation
"""""";Computer Vision;https://github.com/jojonki/arxiv-clip
"""""";Natural Language Processing;https://github.com/bzhangGo/zero
"""""";Reinforcement Learning;https://github.com/jsztompka/MultiAgent-PPO
"""- Fast AutoAugment (hereafter FAA) finds the optimal set of data augmentation operations via density matching using Bayesian optimization. - FAA delivers comparable performance to <a href=""https://arxiv.org/abs/1805.09501"">AutoAugment</a> but in a much shorter period of time. - Unlike AutoAugment that discretizes the search space  FAA can handle continuous search space directly.  <br>     """;Computer Vision;https://github.com/junkwhinger/fastautoaugment_jsh
"""""";Natural Language Processing;https://github.com/han-shi/SparseBERT
"""The implementation is based on two papers & Github Repository: - Object Tracking(https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch) - Simple Online and Realtime Tracking with a Deep Association Metric https://arxiv.org/abs/1703.07402 - YOLOv4: Optimal Speed and Accuracy of Object Detection https://arxiv.org/pdf/2004.10934.pdf   """;Computer Vision;https://github.com/JisuHann/Object-Tracking
"""""";General;https://github.com/TanyaChutani/VAE-TF2.0
"""""";Computer Vision;https://github.com/Ansheel9/GANs
"""High localization accuracy is crucial in many real-world applications. We propose a novel single stage end-to-end object detection network (RRC) to produce high accuracy detection results. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our paper (https://arxiv.org/abs/1704.05776).  | method | KITTI test *mAP* car (moderate)| | :-------: | :-----: | | [Mono3D](http://3dimage.ee.tsinghua.edu.cn/cxz/mono3d)| 88.66% | | [SDP+RPN](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Exploit_All_the_CVPR_2016_paper.pdf)| 88.85% | | [MS-CNN](https://github.com/zhaoweicai/mscnn) | 89.02% | | [Sub-CNN](https://arxiv.org/pdf/1604.04693.pdf) | 89.04% | | RRC (single model) | **89.85%** |    [KITTI ranking](http://www.jimmyren.com/papers/rrc_kitti.pdf)   """;Computer Vision;https://github.com/xiaohaoChen/rrc_detection
"""""";General;https://github.com/ivclab/BigGAN-Generator-Pretrained-Pytorch
"""""";Sequential;https://github.com/devsisters/pointer-network-tensorflow
"""""";General;https://github.com/stevensmiley1989/MrRobot
"""""";Computer Vision;https://github.com/jychoi118/ilvr_adm
"""""";Natural Language Processing;https://github.com/JHart96/keras_elmo_embedding_layer
"""This is a work by Charles R. Qi  Hao Su  Kaichun Mo  Leonidas J. Guibas from Stanford University.You can find the link to their paper here[https://arxiv.org/abs/1612.00593]. Their original work is done using tensorflow1.x and few other packages which are deprecated in newer version. So  I implemented their model using tensorflow 2.0 and python 3.7. Shown below is an example of 3D point cloud objects in ModelNet40 Dataset. You need to do few more steps to train /test the model.  Step_1 -> Install h5py<br>           sudo apt-get install libhdf5-dev<br>           sudo pip install h5py<br> Step_2 -> Download the ModelNet40 dataset and copy the files to data folder. Type the following command in the terminal and unzip the file.<br> <b>wget --no-check-certificate -P ""pointnet/data"" https://shapenet.cs.stanford.edu/ericyi/shapenetcore_partanno_v0.zip</b><br>  Step_3 -> In terminal use ""python train.py"". For evaluation run  ""python evaluate.py --visu""<br> <b> I have not included the weight files here cause of it's size. But if you need the weight files you can pull a request  I can share via my drive </b>   ![3D point cloud vase](https://github.com/SonuDileep/3-D-Object-Detection-using-PointNet/blob/master/vase.jpg)   """;Computer Vision;https://github.com/SonuDileep/3-D-Object-Detection-using-PointNet
"""""";Computer Vision;https://github.com/seunghwan1228/CNN_EfficientNet
""" This is a Mindspore implementation of our paper [SKFAC: Training Neural Networks With Faster Kronecker-Factored Approximate Curvature](https://openaccess.thecvf.com/content/CVPR2021/html/Tang_SKFAC_Training_Neural_Networks_With_Faster_Kronecker-Factored_Approximate_Curvature_CVPR_2021_paper.html) <!--  This is an example of training ResNet-50 V1.5 with ImageNet2012 dataset by second-order optimizer [SKFAC](https://openaccess.thecvf.com/content/CVPR2021/html/Tang_SKFAC_Training_Neural_Networks_With_Faster_Kronecker-Factored_Approximate_Curvature_CVPR_2021_paper.html). This example is based on modifications to the [THOR optimizer](https://gitee.com/mindspore/mindspore/tree/r1.1/model_zoo/official/cv/resnet_thor) on the [MindSpore framework](https://www.mindspore.cn/en). -->   <!-- TOC -->  - [ResNet-50-SKFAC Example](#resnet-50-skfac-example)  	- [Description](#description)  	- [Model Architecture](#model-architecture)  	- [Dataset](#dataset)  	- [Environment Requirements](#environment-requirements)  		- Hardware  		- Framework  	- [Quick Start](#quick-start)  	- [Script Parameters](#script-parameters) 	- [Optimize Performance](#optimize-performance) 	- [References](#references)  <!-- TOC -->     """;General;https://github.com/fL0n9/SKFAC-MindSpore
"""""";Reinforcement Learning;https://github.com/wangml999/chess_zero
"""""";Computer Vision;https://github.com/poloclub/jpeg-defense
"""""";Reinforcement Learning;https://github.com/molomono/CartPole_Optimized_DDQN
"""""";Computer Vision;https://github.com/lzrobots/dgmn
"""""";General;https://github.com/carnotaur/CycleGAN
"""""";Computer Vision;https://github.com/AcramBousa/darknet
"""""";Computer Vision;https://github.com/polmonroig/faceGAN
"""""";General;https://github.com/chizhu/BDC2019
"""""";Reinforcement Learning;https://github.com/andreadegiorgio/cylindrical-astar
"""""";Sequential;https://github.com/Helsinki-NLP/HBMP
"""""";Computer Vision;https://github.com/voxmenthe/ncsn_1
"""This repo is a [PyTorch](https://www.pytorch.org/) implementation of Vanilla DQN  Double DQN  and Dueling DQN based off these papers.  - [Human-level control through deep reinforcement learning](http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html) - [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) - [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)  Starter code is used from [Berkeley CS 294 Assignment 3](https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3) and modified for PyTorch with some guidance from [here](https://github.com/transedward/pytorch-dqn). Tensorboard logging has also been added (thanks [here](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/04-utils/tensorboard) for visualization during training in addition to what the Gym Monitor already does).   """;Reinforcement Learning;https://github.com/dxyang/DQN_pytorch
"""""";Computer Vision;https://github.com/mehdibnc/Cycle-GAN
"""""";General;https://github.com/tomdyer10/fake_news
"""""";Computer Vision;https://github.com/aqbewtra/Multi-Class-Aerial-Segmentation
"""""";Computer Vision;https://github.com/jaydeepthik/keras-GAN
"""""";Natural Language Processing;https://github.com/uchange/ulangel
"""""";Computer Vision;https://github.com/blueardour/AdelaiDet
"""""";Graphs;https://github.com/blueberryc/pyGAT
"""""";Computer Vision;https://github.com/awadalaa/DCGAN
"""""";Computer Vision;https://github.com/avanetten/yolt
"""""";General;https://github.com/squirrelim/sr-frcnn
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/binhetech/bert-application
"""""";General;https://github.com/LSSTDESC/DeblenderVAE
"""""";Computer Vision;https://github.com/aldo-aguilar/nu-style
"""""";General;https://github.com/sirius-image-inpainting/Free-Form-Image-Inpainting-With-Gated-Convolution
"""""";Computer Vision;https://github.com/xinntao/EDVR
"""""";Computer Vision;https://github.com/pytorch/vision
"""""";Natural Language Processing;https://github.com/mingdachen/bilm-tf
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  BERT is based on the Transformer architecture introduced in [Attention is all you need](https://arxiv.org/abs/1706.03762).  Googles academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  Googles Github repository where the original english models can be found here: [https://github.com/google-research/bert](https://github.com/google-research/bert).  Included in the downloads below are PyTorch versions of the models based on the work of  NLP researchers from HuggingFace. [PyTorch version of BERT available](https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/)   """;Natural Language Processing;https://github.com/af-ai-center/bert
"""""";General;https://github.com/marco-rudolph/differnet
"""Existing video polyp segmentation (VPS) models typically employ convolutional neural networks (CNNs) to extract features.  However  due to their limited receptive fields  CNNs can not fully exploit the global temporal and spatial information in successive video frames  resulting in false-positive segmentation results.  In this paper  we propose the novel PNS-Net (Progressively Normalized Self-attention Network)  which can efficiently learn representations from polyp videos with real-time speed (~140fps) on a single RTX 2080 GPU and no post-processing.   Our PNS-Net is based solely on a basic normalized self-attention block  dispensing with recurrence and CNNs entirely. Experiments on challenging VPS datasets demonstrate that the proposed PNS-Net achieves state-of-the-art performance.  We also conduct extensive experiments to study the effectiveness of the channel split  soft-attention  and progressive learning strategy.  We find that our PNS-Net works well under different settings  making it a promising solution to the VPS task.   """;Computer Vision;https://github.com/GewelsJI/PNS-Net
"""""";Computer Vision;https://github.com/CoinCheung/SphereReID
"""""";Computer Vision;https://github.com/itsuki8914/SRGAN-TensorFlow
"""""";Natural Language Processing;https://github.com/pauldevos/python-notes
"""""";Computer Vision;https://github.com/ithuanhuan/py-fatser-rcnn
"""""";Natural Language Processing;https://github.com/astanway/gated-conv-nets
"""""";Sequential;https://github.com/lukecq1231/nli
"""""";General;https://github.com/saisandeep97/Chat-botV2
"""""";General;https://github.com/liang-faan/openpose
"""""";Natural Language Processing;https://github.com/benywon/ComQA
"""""";General;https://github.com/metataro/DirectFeedbackAlignment
"""""";Natural Language Processing;https://github.com/awslabs/mlm-scoring
"""""";General;https://github.com/ZhuiyiTechnology/roformer
"""""";Computer Vision;https://github.com/RealHulubulu/openCVObjectDetectionRPi
"""In this work  we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN including only the most essential component in GCN—neighborhood aggregation—for collaborative filtering     """;Graphs;https://github.com/gusye1234/LightGCN-PyTorch
"""""";General;https://github.com/Rajas1211/SRGAN
"""""";General;https://github.com/vict0rsch/pytorch-fid-wrapper
"""""";Computer Vision;https://github.com/balancap/SSD-Tensorflow
"""""";General;https://github.com/baodi23/hourglass-facekeypoints-detection
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/h4ste/oscar
"""""";Computer Vision;https://github.com/DingXiaoH/DiverseBranchBlock
"""""";Computer Vision;https://github.com/busyboxs/faster_rcnn_voc
"""Deep reinforcement learning has made significant strides in recent years  with results achieved in board games such as Go. However  there are a number of obstacles preventing such methods from being applied to more real-world situations. For instance  more realistic strategic situations often involve much larger spaces of possible states and actions  an environment state which is only partially observed  multiple agents to control  and a necessity for long-term strategies involving not hundreds but thousands or tens of thousands of steps. It has thus been suggested that creating learning algorithms which outperform humans in playing real-time strategy (RTS) video games would signal a more generalizable result about the ability of a computer to make decisions in the real world.  Of the current RTS games on the market  StarCraft II is one of the most popular. The recent release by Google’s DeepMind of SC2LE (StarCraft II Learning Environment) presents an interface with which to train deep reinforcement learners to compete in the game  both in smaller “minigames” and on full matches. The SC2LE environment is described on [DeepMind's github repo.](https://github.com/deepmind/pysc2)   In this project  we focus on solving a variety of minigames  which capture various aspects of the full StarCraft II game. These minigames focus on tasks such as gathering resources  moving to waypoints  finding enemies  or skirmishing with units. In each case the player is given a homogeneous set of units (marines)  and a reward is based off the minigame (+5 for defeating each enemy roach in DefeatRoaches  for example).   """;Reinforcement Learning;https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames
"""""";General;https://github.com/snow-mn/GAN-INT-CLS
"""""";General;https://github.com/GPUPhobia/vocal-mask
"""""";Computer Vision;https://github.com/PDEUXA/AIF_CYCLEGAN
"""""";General;https://github.com/uw-mad-dash/Accordion
"""""";Sequential;https://github.com/ZubinGou/NER-BiLSTM-CRF-PyTorch
"""""";Sequential;https://github.com/mhagiwara/nanigonet
"""""";Natural Language Processing;https://github.com/SeonbeomKim/Python-Byte_Pair_Encoding
"""""";Computer Vision;https://github.com/fastai/fastai
"""""";General;https://github.com/mlvc-lab/CycleGan_pytorch
"""""";Computer Vision;https://github.com/t-ae/progan-s4tf
"""""";General;https://github.com/kekeller/semantic_soy_deeplabv3plus
"""""";Computer Vision;https://github.com/08173021/YOLOv4
"""""";General;https://github.com/YyzHarry/imbalanced-semi-self
"""""";General;https://github.com/xultaeculcis/coral-net
"""""";Computer Vision;https://github.com/GuoQuanhao/EfficientDet-Paddle
"""""";Natural Language Processing;https://github.com/czero69/acomoeye-NN
"""""";Computer Vision;https://github.com/alexgkendall/caffe-segnet
"""""";General;https://github.com/cchinchristopherj/Concert-of-Whales
"""Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent  we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. But if we instead take steps proportional to the positive of the gradient  we approach a local maximum of that function; the procedure is then known as gradient ascent. Gradient descent was originally proposed by Cauchy in 1847.    """;General;https://github.com/Arko98/Gradient-Descent-Algorithms
"""This repository contains the code for fine-tuning a CLIP model [[Arxiv paper](https://arxiv.org/abs/2103.00020)][[OpenAI Github Repo](https://github.com/openai/CLIP)] on the [ROCO dataset](https://github.com/razorx89/roco-dataset)  a dataset made of radiology images and a caption. This work is done as a part of the [**Flax/Jax community week**](https://github.com/huggingface/transformers/blob/master/examples/research_projects/jax-projects/README.md#quickstart-flax-and-jax-in-transformers) organized by Hugging Face and Google.  [[🤗 Model card]](https://huggingface.co/flax-community/medclip-roco) [[Streamlit demo]](https://huggingface.co/spaces/kaushalya/medclip-roco)   """;Computer Vision;https://github.com/Kaushalya/medclip
"""""";Natural Language Processing;https://github.com/aaronmueller/contextualized-topic-models
"""SSKD is implemented based on **FastReID v1.0.0**  it provides a semi-supervised feature learning framework to learn domain-general representations. The framework is shown in   <img src=""images/framework.png"" width=""850"" >   """;General;https://github.com/xiaomingzhid/sskd
"""""";Computer Vision;https://github.com/inacmor/mobiledets-yolov4-pytorch
"""""";General;https://github.com/tatp22/pytorch-fast-GAT
"""English | [简体中文](README_zh-CN.md)  MMGeneration is a powerful toolkit for generative models  especially for GANs now. It is based on PyTorch and [MMCV](https://github.com/open-mmlab/mmcv). The master branch works with **PyTorch 1.5+**.  <div align=""center"">     <img src=""https://user-images.githubusercontent.com/12726765/114534478-9a65a900-9c81-11eb-8087-de8b6816eed8.png"" width=""800""/> </div>    """;Computer Vision;https://github.com/open-mmlab/mmgeneration
"""""";General;https://github.com/dabsdamoon/Anime-Colorization-v0.2
"""[![Watch the video](figures/video_figure.png)](https://www.youtube.com/watch?v=a_OeT8MXzWI&feature=youtu.be)   """;Computer Vision;https://github.com/mit-han-lab/once-for-all
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/Kev1nZheng/yolov_mask
"""""";Computer Vision;https://github.com/nikhilroxtomar/Unet-for-Person-Segmentation
"""""";Computer Vision;https://github.com/gomboskriszta/yolo
"""""";Natural Language Processing;https://github.com/xingniu/sockeye
"""""";Computer Vision;https://github.com/PlumedSerpent/tmp_perspective_map
"""""";General;https://github.com/nikhilbarhate99/TD3-PyTorch-BipedalWalker-v2
"""""";Computer Vision;https://github.com/rijuldhir/TSM
"""""";Sequential;https://github.com/Smerity/sha-rnn
"""""";Natural Language Processing;https://github.com/tree-park/transformer_lm
"""""";Computer Vision;https://github.com/harsh2912/people-tracking
"""""";Computer Vision;https://github.com/maxfrei750/FibeR-CNN
"""""";Computer Vision;https://github.com/soribadiaby/Deep-Learning-liver-segmentation
"""""";Computer Vision;https://github.com/xuannianz/keras-fcos
"""""";General;https://github.com/ghaniskn/GCorp-Darknet
"""""";Sequential;https://github.com/r9y9/deepvoice3_pytorch
"""""";Sequential;https://github.com/anandaswarup/waveRNN
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Arthurizijar/Bert_Airport
"""该系统实现了基于深度框架的语音识别中的声学模型和语言模型建模，其中声学模型包括CNN-CTC、GRU-CTC、CNN-RNN-CTC，语言模型包含[transformer](https://jalammar.github.io/illustrated-transformer/)、[CBHG](https://github.com/crownpku/Somiao-Pinyin)，数据集包含stc、primewords、Aishell、thchs30四个数据集。  本系统更整体介绍：https://blog.csdn.net/chinatelecom08/article/details/82557715  本项目现已训练一个迷你的语音识别系统，将项目下载到本地上，下载[thchs数据集](http://www.openslr.org/resources/18/data_thchs30.tgz)并解压至data，运行`test.py`，不出意外能够进行识别，结果如下：       the  0 th example.     文本结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文汉字： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然     识别结果： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然  若自己建立模型则需要删除现有模型，重新配置参数训练，具体实现流程参考本页最后。   """;Natural Language Processing;https://github.com/StevenLai1994/AM_and_LM
"""""";General;https://github.com/Our4514/CAGFUZZ
"""""";Natural Language Processing;https://github.com/airsplay/vimpac
"""""";Computer Vision;https://github.com/yumaloop/mobilenetV2-cifar
"""""";Natural Language Processing;https://github.com/cemoody/lda2vec
"""""";Graphs;https://github.com/benedekrozemberczki/ClusterGCN
"""""";Computer Vision;https://github.com/taranjotsingh01/Car-detection-using-YOLO
"""""";Graphs;https://github.com/yangjun1994/CAGCN
"""""";Computer Vision;https://github.com/Martion-z/pytorch-Pulse
"""""";General;https://github.com/srinivas1857-ton/toonify-poc
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/why2000/DuReader-bert
"""""";Computer Vision;https://github.com/dayfixer/darknet-comment
"""""";General;https://github.com/natel9178/transformer-refork
"""""";Computer Vision;https://github.com/filipmu/Kaggle-APTOS-2019-Blindness
"""DeBERTa (Decoding-enhanced BERT with disentangled attention) improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism  where each word is represented using two vectors that encode its content and position  respectively  and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second  an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks.   """;Natural Language Processing;https://github.com/huberemanuel/DeBERTa
"""REPVGG is a simple but powerful architecture of CNN which has a VGG like inference time .It runs 101% faster then RESNET 101  There are many complicated architecture which has better accuracy then simple architectures  but the drawback of this kind of architecture is that they are difficult to customize . And  has very high inference time .REPVGG has various advantages like   Ithas plain topology   just like its earlier models such as vgg 19 etc . Its architecture highly depends upon 3x3 kernels and ReLU. It has novel structural reparamaterization which decouple a training time of multi branch topology with a inference time plain architecture .You can also se training of REPVGG in google colab on CIFAR10 [here](https://github.com/imad08/model-zoo-submissions/blob/main/REPVGG/REPVGG_with_complete_reparamaterization_.ipynb)   ![fusing batch normalization and convolutions for reparametrization](https://media.arxiv-vanity.com/render-output/4507333/x1.png)   """;Computer Vision;https://github.com/imad08/Repvgg_pytorch
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.      ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    """;Natural Language Processing;https://github.com/crx934080895/Bert-CRF_New2
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/chenglibo123/123123
"""""";Computer Vision;https://github.com/pearsonkyle/Neural-Nebula
"""""";Reinforcement Learning;https://github.com/RL-WFU/multi_agent_attack
"""""";Natural Language Processing;https://github.com/lucidrains/charformer-pytorch
"""""";General;https://github.com/chao-ji/tf-seq2seq
"""""";General;https://github.com/jefflai108/Contrastive-Predictive-Coding-PyTorch
"""""";Computer Vision;https://github.com/carpedm20/DCGAN-tensorflow
"""""";Computer Vision;https://github.com/rolux/stylegan2encoder
"""**TL; DR.** Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism.    ![deformable_detr](./figs/illustration.png)  ![deformable_detr](./figs/convergence.png)  **Abstract.** DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However  it suffers from slow convergence and limited feature spatial resolution  due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues  we proposed Deformable DETR  whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10× less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach.   """;Computer Vision;https://github.com/fundamentalvision/Deformable-DETR
"""""";General;https://github.com/Sakib1263/VGG-1D-2D-Tensorflow-Keras
"""""";General;https://github.com/idealo/image-super-resolution
"""""";General;https://github.com/HUJI-Deep/nice
"""""";Computer Vision;https://github.com/arnab39/cycleGAN-PyTorch
"""""";General;https://github.com/ivalab/FullResults_GoodFeature
"""""";Computer Vision;https://github.com/sagnik1511/U-Net-Reduced-with-TF-keras
"""""";Sequential;https://github.com/mkotha/WaveRNN
"""""";General;https://github.com/maituduy/Mxnet_MobileNet
"""""";Natural Language Processing;https://github.com/AtheMathmo/lookahead-lstm
"""""";Computer Vision;https://github.com/Suci2609/generate_train_list.py-
"""Generative Adversarial Networks (GANs) are one of the most interesting ideas in computer science today. Here two models named Generator and Discriminator are trained simultaneously. As the name says Generator generates the fake images or we can say it generates a random noise and the Discriminator job is to classify whether the image is fake or not. Here the only job of Generator is to fake the Discriminator. In this project we are using DCGAN(Deep Convolutional Generative Adversarial Network). A DCGAN is a direct extension of the GAN described above  except that it explicitly uses convolutional and convolutional-transpose layers in the discriminator and generator  respectively. DCGANs actually comes under Unsupervised Learning and was first described by Radford et. al. in the paper Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks.  ![](https://miro.medium.com/max/2850/1*Mw2c3eY5khtXafe5W-Ms_w.jpeg)   """;Computer Vision;https://github.com/hunnurjirao/DCGAN
"""""";Natural Language Processing;https://github.com/wangcongcong123/ttt
"""""";Computer Vision;https://github.com/clementchadebec/benchmark_VAE
"""""";Computer Vision;https://github.com/sciosci/graph_check
"""""";Computer Vision;https://github.com/NikhitaMethwani/Deep-Fakes-Cars-Generation
"""""";Computer Vision;https://github.com/alvarobartt/serving-pytorch-models
"""""";General;https://github.com/A-Telfer/AugKey
"""We build a plant disease diagnosis system on Android  by implementing a deep convolutional neural network with Tensorflow to detect disease from various plant leave images.   Generally  due to the size limitation of the dataset  we adopt the transefer learning in this system. Specifically  we retrain the MobileNets [[1]](https://arxiv.org/pdf/1704.04861.pdf)  which is first trained on ImageNet dataset  on the plant disease datasets. Finally  we port the trained model to Android.   """;General;https://github.com/sabrisangjaya/plantdoctor
"""""";Computer Vision;https://github.com/OkanKY/generative_adversarial_networks
"""Follow experimental summary [here](https://bit.ly/3arUw9q).   """;General;https://github.com/sayakpaul/EvoNorms-in-TensorFlow-2
"""""";Reinforcement Learning;https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail
"""""";Computer Vision;https://github.com/watsonyanghx/GAN_Lib_Tensorflow
"""""";General;https://github.com/grunnery/StyleGAN2
"""""";Reinforcement Learning;https://github.com/muupan/async-rl
"""SSD(Single Shot MultiBox Detector) is a state-of-art object detection algorithm  brought by Wei Liu and other wonderful guys  see [SSD: Single Shot MultiBox Detector @ arxiv](https://arxiv.org/abs/1512.02325)  recommended to read for better understanding.  Also  SSD currently performs good at PASCAL VOC Challenge  see [http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=3](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=3)     """;Computer Vision;https://github.com/rajs25/Object-Detection
"""""";Computer Vision;https://github.com/Camebush/real-time-yolov4-object-detection
"""""";Computer Vision;https://github.com/monk-ai/maskrcnn
"""""";Computer Vision;https://github.com/joycex99/conditional-presgan
"""""";Natural Language Processing;https://github.com/macco3k/deepstories
"""""";Computer Vision;https://github.com/DevashishJoshi/Transferring-GANs-FYP
"""""";Reinforcement Learning;https://github.com/wtingda/DeepRLBreakout
"""**Faster** R-CNN is an object detection framework based on deep convolutional networks  which includes a Region Proposal Network (RPN) and an Object Detection Network. Both networks are trained for sharing convolutional layers for fast testing.  **This repo contains a Python implementation of Faster-RCNN originally developed in Matlab. This code works with models trained using Matlab version of Faster-RCNN which is main difference between this and py-faster-rcnn.**  This code was developed for internal use in one of my projects at the end of 2015. I decided to publish it as is.   """;Computer Vision;https://github.com/smichalowski/faster_rcnn
"""""";Computer Vision;https://github.com/transcendentsky/mixup
"""""";Computer Vision;https://github.com/akjgskwi/mixup_chainer
"""""";Reinforcement Learning;https://github.com/LucasAlegre/sac-plus
"""""";Computer Vision;https://github.com/worldlife123/maskrcnn-benchmark
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.   ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    """;Natural Language Processing;https://github.com/habibullah-araphat/BERT-NER-TPU
"""""";Computer Vision;https://github.com/youchangxin/YOLOv4_tensorflow2
"""Get a conceptual overview of image classification  object localization  object detection  and image segmentation. Also be able to describe multi-label classification  and distinguish between semantic segmentation and instance segmentation. In the rest of this course  you will apply TensorFlow to build object detection and image segmentation models.  Learning Objectives:  - Distinguish between object localization and object detection - Distinguish between object detection and image segmentation - Distinguish between semantic segmentation and instance segmentation - Explain what is transfer learning and why it's used - Describe design options when using transfer learning - Implement object localization with a CNN - Implement an image classifier with transfer learning  Papers about Image segmentation models:  Fully Convolutional Networks for Semantic Segmentation https://people.eecs.berkeley.edu/~shelhamer/data/fcn.pdf  U-Net: Convolutional Networks for Biomedical Image Segmentation https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/  DeepLab: Semantic Image Segmentation with Deep Convolutional Nets  Atrous Convolution  and Fully Connected CRFs http://liangchiehchen.com/projects/DeepLab.html  Mask R-CNN: https://arxiv.org/abs/1703.06870    """;Computer Vision;https://github.com/polospeter/TensorFlow-Advanced-Techniques-Specialization
"""""";General;https://github.com/GISH123/Cathay-Holdings-CIP-Projects-for-Interpretable-Machine-Learning
"""The model is a Convolutional Neural Network with shortcuts as showed in this paper: https://arxiv.org/pdf/1512.03385.pdf   """;Computer Vision;https://github.com/LuigiRussoDev/ResNets
"""""";Natural Language Processing;https://github.com/twobooks/intro-aws-training
"""+ Implementation  Implemented based on [THUMT-TensorFlow](https://github.com/THUNLP-MT/THUMT)  an open-source toolkit for neural machine translation developed by the Natural Language Processing Group at Tsinghua University which was implemented strictly referring to [Vaswani et al. (2017)](https://arxiv.org/pdf/1706.03762.pdf).  + Data  [WMT14 English-German](https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-wmt14en2de.sh)  [WMT19 Chinese-English](https://drive.google.com/file/d/1LvUPsIZ_xRwuB1vHlvi1COeZEOxfbYy0/view?usp=sharing)    """;Natural Language Processing;https://github.com/xydaytoy/BMI-NMT
"""""";Computer Vision;https://github.com/MioChiu/MobileNet_V2_TensorFlow
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/kangyiS/caffe_ky
"""""";Computer Vision;https://github.com/owena11/papers
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/TSLNIHAOGIT/bert
"""""";Computer Vision;https://github.com/sebsquire/Automated-Video-Filtering-YOLOv2
"""""";Computer Vision;https://github.com/qubvel/efficientnet
"""""";Reinforcement Learning;https://github.com/anita-hu/TF2-RL
"""""";Computer Vision;https://github.com/conradry/copy-paste-aug
"""""";General;https://github.com/fra-mari/The-Karl-Marx-Press-Review
"""The goal of Detectron is to provide a high-quality  high-performance codebase for object detection *research*. It is designed to be flexible in order to support rapid implementation and evaluation of novel research. Detectron includes implementations of the following object detection algorithms:  - [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017* - [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017* - [Faster R-CNN](https://arxiv.org/abs/1506.01497) - [RPN](https://arxiv.org/abs/1506.01497) - [Fast R-CNN](https://arxiv.org/abs/1504.08083) - [R-FCN](https://arxiv.org/abs/1605.06409)  using the following backbone network architectures:  - [ResNeXt{50 101 152}](https://arxiv.org/abs/1611.05431) - [ResNet{50 101 152}](https://arxiv.org/abs/1512.03385) - [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt) - [VGG16](https://arxiv.org/abs/1409.1556)  Additional backbone architectures may be easily implemented. For more details about these models  please see [References](#references) below.   """;Computer Vision;https://github.com/jiajunhua/facebookresearch-Detectron
"""""";Computer Vision;https://github.com/jiye-ML/Classify_FRACTALNET
"""""";Computer Vision;https://github.com/hycis/TensorGraph
"""""";Computer Vision;https://github.com/Yashgh7076/CPSC-8810-Project
"""""";Natural Language Processing;https://github.com/Joshuaoneheart/Fintech-Final-Report
"""""";General;https://github.com/pralab/Fast-Minimum-Norm-FMN-Attack
"""""";General;https://github.com/anguyen8/sam
"""We build a plant disease diagnosis system on Android  by implementing a deep convolutional neural network with Tensorflow to detect disease from various plant leave images.   Generally  due to the size limitation of the dataset  we adopt the transefer learning in this system. Specifically  we retrain the MobileNets [[1]](https://arxiv.org/pdf/1704.04861.pdf)  which is first trained on ImageNet dataset  on the plant disease datasets. Finally  we port the trained model to Android.   """;General;https://github.com/nyak10/mwc
"""""";Computer Vision;https://github.com/TillBeemelmanns/auto-augment-tf2-operations
"""""";Computer Vision;https://github.com/Yodai1996/CV
"""""";Computer Vision;https://github.com/tdrussell/IllustrationGAN
"""""";General;https://github.com/TottiPuc/Hyperspectral_ImageReconstruction_GAN
"""""";General;https://github.com/ryujaehun/maml
"""""";General;https://github.com/lucidrains/En-transformer
"""<img src=""./Report/25_inf.png"">     (first row ~ last row)<br>     0633: this flower has petals that are yellow with red blotches <br>     0194: the flower has white stringy petals with yellow and purple pollen tubes<br>     2014: this flower is pink in color with only one large petal<br>     4683: this flower is yellow in color with petals that are rounded<br>     3327: the flower has a several pieces of yellow colored petals that looks similar to its leaves  """;General;https://github.com/BeyondCloud/Comp04_ReverseImageCaption
"""""";Computer Vision;https://github.com/adrianjav/heterogeneous_vaes
"""""";Computer Vision;https://github.com/northeastsquare/effficientnet
"""""";Natural Language Processing;https://github.com/michael-wzhu/mpnet_zh
"""""";Natural Language Processing;https://github.com/kpot/keras-transformer
"""This package provides a simple way to boost the performance of image captioning task by reranking the hypotheses sentences according to the captions of nearest neighbor images in the training set. We denote this method as consensus reranking for the rest of this document.  It also provides images features (both refined by the [m-RNN model](www.stat.ucla.edu/~junhua.mao/m-RNN.html) and the original [VGG feature](http://arxiv.org/abs/1409.1556) ) on MS COCO Train2014  Val2014  and Test2014 dataset. We take the [m-RNN model](www.stat.ucla.edu/~junhua.mao/m-RNN.html) as an example to generate the hypotheses descriptions of an image.  The details is described in Section 8 of the latest version of the m-RNN paper: [Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)](http://arxiv.org/abs/1412.6632). The work is inspired by the [nearest neighbor captions retrieval method](http://arxiv.org/abs/1505.04467).   """;Computer Vision;https://github.com/mjhucla/mRNN-CR
"""""";General;https://github.com/raylyh/misgan-reimplementation
"""""";Computer Vision;https://github.com/GrayXu/SpectralNormalization-TF-Keras
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/goldenbili/bert_lamb_pretrain
"""""";Computer Vision;https://github.com/Shawn-Shan/fawkes
"""""";General;https://github.com/GuHongyang/VAEs
"""""";Audio;https://github.com/as-ideas/TransformerTTS
"""""";General;https://github.com/minoring/nin-tf2
"""""";General;https://github.com/nanwei1/MNIST_GAN
"""""";Natural Language Processing;https://github.com/kamalkraj/BERT-SQuAD
"""""";Computer Vision;https://github.com/tuanzhangCS/octconv_resnet
"""The Neural Network used for this work is a Odenet neural network: https://arxiv.org/abs/1806.07366 In particular I used a Res-Ode that's mean a miniblock of resnet before the ODE block.    """;Computer Vision;https://github.com/LuigiRussoDev/Covid19Detection
"""""";General;https://github.com/pszemraj/BoulderAreaDetector
"""""";Computer Vision;https://github.com/alexwdong/IncubatorCVProject
"""""";General;https://github.com/seraphlabs-ca/MIM
"""""";Computer Vision;https://github.com/adityabingi/Beta-VAE
"""""";General;https://github.com/midusi/cacic2019-handshapes
"""""";General;https://github.com/keenborder786/Image-Annotation-Research
"""""";Computer Vision;https://github.com/DeNA/Chainer_Mask_R-CNN
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/vijay120/bert
"""""";Computer Vision;https://github.com/DeepVoltaire/AutoAugment
"""""";Computer Vision;https://github.com/Eljefemasao/MarkNet-ObjectDetector
"""""";General;https://github.com/deepmind/rlax
"""As many others  this paper buids on recent work on linear attention that is calculated as <img src=""https://render.githubusercontent.com/render/math?math=\phi(Q) \left(\phi(K)^T V\right)""> instead of <img src=""https://render.githubusercontent.com/render/math?math=\text{softmax}\left(Q K^T\right)V"">  where <img src=""https://render.githubusercontent.com/render/math?math=\phi""> is a kernel function. This reduces the complexity from <img src=""https://render.githubusercontent.com/render/math?math=\mathcal{O}(N^2 D)""> to <img src=""https://render.githubusercontent.com/render/math?math=\mathcal{O}(ND^2)"">. The authors propose to extend this mechanism by including relative distance information in the Q  K product as <img src=""https://render.githubusercontent.com/render/math?math=\phi(Q_i)\phi(K_j)^T\cos\left(\frac{\pi}{2}\times\frac{i-j}{M}\right)"">. After expanding the trigonometric identity  the full equation becomes:  <p align=""center"">   <img src=""https://render.githubusercontent.com/render/math?math=\text{Attention}(Q  K  V)  =  Q^{\cos} \left(K^{\cos} V\right) %2B Q^{\sin} \left(K^{\sin} V\right)""> </p>  where <img src=""https://render.githubusercontent.com/render/math?math=Q_i^{\cos} = \phi(Q_i)\cos\left(\frac{\pi i}{2M}\right)  Q_i^{\sin} = \phi(Q_i)\sin\left(\frac{\pi i}{2M}\right)""> etc.  As the author of this repo possesses neither the time nor the ability  only the non-causal version of this approach is implemented.       """;General;https://github.com/davidsvy/cosformer-pytorch
"""""";Computer Vision;https://github.com/Guanghan/ROLO
"""""";Natural Language Processing;https://github.com/llppff/ptb-lstmorqrnn-pytorch
"""""";Natural Language Processing;https://github.com/lucidrains/sinkhorn-transformer
"""""";Computer Vision;https://github.com/kdethoor/panoptictorch
"""""";General;https://github.com/wodyjowski/colab-training
"""""";Computer Vision;https://github.com/xu3kev/octconv-chainer
"""""";Computer Vision;https://github.com/LeChangAlex/GAN-Metric
"""""";Natural Language Processing;https://github.com/FrancescoSaverioZuppichini/How-To-Embed-in-TensorFlow
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/wchh127/yykf
"""""";Computer Vision;https://github.com/qilei123/maskrcnn_benchmark_ROP
"""""";General;https://github.com/Neoanarika/Searching-for-activation-functions
"""""";General;https://github.com/mehdibnc/Cycle-GAN
"""""";Sequential;https://github.com/albarji/neurowriter
"""""";Computer Vision;https://github.com/noelcodes/Mask_RCNN
"""""";Computer Vision;https://github.com/sarrrrry/maskrcnn-benchmark
"""""";Computer Vision;https://github.com/NVlabs/SegFormer
"""""";Sequential;https://github.com/salesforce/awd-lstm-lm
"""This post describes how I used the eo-learn and fastai libraries to create a machine learning data pipeline that can classify crop types from satellite imagery. I used this pipeline to enter Zindi’s [Farm Pin Crop Detection Challenge](https://zindi.africa/competitions/farm-pin-crop-detection-challenge). I may not have won the contest but I learnt some great techniques for working with remote-sensing data which I detail in this post.  Here are the preprocessing steps I followed:  1. divided an area of interest into a grid of ‘patches’   1. loaded imagery from disk   1. masked out cloud cover   1. added NDVI and euclidean norm features   1. resampled the imagery to regular time intervals   1. added raster layers with the targets and identifiers.  I reframed the problem of crop type classification as a semantic segmentation task and trained a U-Net with a ResNet50 encoder on multi-temporal multi-spectral data using image augmentation and mixup to prevent over-fitting.  My solution borrows heavily from the approach outlined by [Matic Lubej](undefined) in his [three](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-1-2471e8098195) [excellent](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-2-bd9aa86f8500) [posts](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-3-c62ed9ecd405) on land cover classification with [eo-learn](https://github.com/sentinel-hub/eo-learn).  The python notebooks I created can be found in this github repository: [https://github.com/simongrest/farm-pin-crop-detection-challenge](https://github.com/simongrest/farm-pin-crop-detection-challenge)   """;Computer Vision;https://github.com/simongrest/farm-pin-crop-detection-challenge
"""""";Computer Vision;https://github.com/Maskify/darknet
"""""";General;https://github.com/RoboticsDesignLab/jitterbug
"""""";Computer Vision;https://github.com/Sinestro38/qosf-qgan
"""""";General;https://github.com/tomguluson92/StyleGAN2_PyTorch
"""""";General;https://github.com/Uemuet/stylegan22
"""""";General;https://github.com/wkhademi/ImageEnhancement
"""""";Computer Vision;https://github.com/ZhouKai90/face-detection-ssd-mxnet
"""Inspired by the structure of Receptive Fields (RFs) in human visual systems  we propose a novel RF Block (RFB) module  which takes the relationship between the size and eccentricity of RFs into account  to enhance the discriminability and robustness of features. We further  assemble the RFB module to the top of SSD with a lightweight CNN model  constructing the RFB Net detector. You can use the code to train/evaluate the RFB Net for object detection. For more details  please refer to our [arXiv paper](https://arxiv.org/pdf/1711.07767.pdf).   <img align=""right"" src=""https://github.com/ruinmessi/RFBNet/blob/master/doc/rfb.png"">  &nbsp; &nbsp;   """;General;https://github.com/ZTao-z/multiflow-resnet-ssd
"""""";Computer Vision;https://github.com/minlee077/BARS
"""""";Computer Vision;https://github.com/virafpatrawala/DCGAN
"""**nmtpy** is a suite of Python tools  primarily based on the starter code provided in [dl4mt-tutorial](https://github.com/nyu-dl/dl4mt-tutorial) for training neural machine translation networks using Theano. The basic motivation behind forking **dl4mt-tutorial** was to create a framework where it would be easy to implement a new model by just copying and modifying an existing model class (or even inheriting from it and overriding some of its methods).  To achieve this purpose  **nmtpy** tries to completely isolate training loop  beam search  iteration and model definition:   - `nmt-train` script to start a training experiment   - `nmt-translate` to produce model-agnostic translations. You just pass a trained model's   checkpoint file and it does its job.   - `nmt-rescore` to rescore translation hypotheses using an nmtpy model.   - An abstract `BaseModel` class to derive from to define your NMT architecture.   - An abstract `Iterator` to derive from for your custom iterators.  A non-exhaustive list of differences between **nmtpy** and **dl4mt-tutorial** is as follows:    - No shell script  everything is in Python   - Overhaul object-oriented refactoring of the code: clear separation of API and scripts that interface with the API   - INI style configuration files to define everything regarding a training experiment   - Transparent cleanup mechanism to kill stale processes  remove temporary files   - Simultaneous logging of training details to stdout and log file   - Supports out-of-the-box BLEU  METEOR and COCO eval metrics   - Includes [subword-nmt](https://github.com/rsennrich/subword-nmt) utilities for training and applying BPE model (NOTE: This may change as the upstream subword-nmt moves forward as well.)   - Plugin-like text filters for hypothesis post-processing (Example: BPE  Compound  Char2Words for Char-NMT)   - Early-stopping and checkpointing based on perplexity  BLEU or METEOR (Ability to add new metrics easily)   - Single `.npz` file to store everything about a training experiment   - Automatic free GPU selection and reservation using `nvidia-smi`   - Shuffling support between epochs:     - Simple shuffle     - [Homogeneous batches of same-length samples](https://github.com/kelvinxu/arctic-captions) to improve training speed   - Improved parallel translation decoding on CPU   - Forced decoding i.e. rescoring using NMT   - Export decoding informations into `json` for further visualization of attention coefficients   - Improved numerical stability and reproducibility   - Glorot/Xavier  He  Orthogonal weight initializations   - Efficient SGD  Adadelta  RMSProp and ADAM: Single forward/backward theano function without intermediate variables   - Ability to stop updating a set of weights by recompiling optimizer   - Several recurrent blocks:     - GRU  Conditional GRU (CGRU) and LSTM     - Multimodal attentive CGRU variants   - [Layer Normalization](https://github.com/ryankiros/layer-norm) support for GRU   - 2-way or 3-way [tied target embeddings](https://arxiv.org/abs/1608.05859)   - Simple/Non-recurrent Dropout  L2 weight decay   - Training and validation loss normalization for comparable perplexities   - Initialization of a model with a pretrained NMT for further finetuning   """;General;https://github.com/lium-lst/nmtpy
"""""";Reinforcement Learning;https://github.com/gingkg/multiagent-particle-envs
"""""";Sequential;https://github.com/PaddlePaddle/models
"""""";General;https://github.com/yeyinthtoon/tf2-resmlp
"""""";General;https://github.com/Vishal-V/StackGAN
"""""";Natural Language Processing;https://github.com/dongdong199408/teachchatrobot
"""""";General;https://github.com/nlinc1905/dsilt-tsa
"""""";Natural Language Processing;https://github.com/JohnGiorgi/DeCLUTR
"""""";General;https://github.com/applied-ai-lab/genesis
"""""";General;https://github.com/CKRC24/Listen-and-Translate
"""""";General;https://github.com/xiaochus/MobileNetV2
"""""";General;https://github.com/theburgman21/Pix2Pix-Deblurrer
"""""";Computer Vision;https://github.com/Raghuvar/pneumonia_detection_using_X-rays
"""""";General;https://github.com/qq345736500/sarcasm
"""""";Sequential;https://github.com/kaishengtai/torch-ntm
"""""";Computer Vision;https://github.com/vuongtrannguyenkhoi/darknet
"""""";Computer Vision;https://github.com/akanimax/dcgan_pytorch
"""""";Reinforcement Learning;https://github.com/soumik12345/DDPG
"""""";Natural Language Processing;https://github.com/simonjisu/NMT
"""""";Computer Vision;https://github.com/albarji/neurowriter
"""""";Reinforcement Learning;https://github.com/claudeHifly/BipedalWalker-v3
"""""";General;https://github.com/uchange/ulangel
"""""";Computer Vision;https://github.com/leongkhing/stnbhwd
"""The goal of **semantic segmentation** is to identify objects  like cars and dogs  in an image by labelling the corresponding groups of pixels according to their classes. For an introduction  see <a href=""https://nanonets.com/blog/semantic-image-segmentation-2020/"">this article</a>. As an example  below is an image and its labelled pixels.  | <img src=""assets/rider.jpg"" alt=""biker"" width=400> | <img src=""assets/rider_label.png"" alt=""true label"" width=400> | |:---:|:---:| | Image | True label |  A **fully convolutional network (FCN)** is an artificial neural network that performs semantic segmentation.  The bottom layers of a FCN are those of a convolutional neural network (CNN)  usually taken from a pre-trained network like VGGNet or GoogLeNet. The purpose of these layers is to perform classification on subregions of the image. The top layers of a FCN are **transposed convolution/deconvolution** layers  which upsample the results of the classification to the resolution of the original image. This gives us a label for each pixel. When upsampling  we can also utilize the intermediate layers of the CNN to improve the accuracy of the segmentation. For an introduction  see <a href=""https://nanonets.com/blog/how-to-do-semantic-segmentation-using-deep-learning/"">this article</a>.  The <a href=""http://host.robots.ox.ac.uk/pascal/VOC/"">Pascal VOC project</a> is a dataset containing images whose pixels have been labeled according to 20 classes (excluding the background)  which include aeroplanes  cars  and people. We will be performing semantic segmentation according to this dataset.   """;Computer Vision;https://github.com/kevinddchen/Keras-FCN
