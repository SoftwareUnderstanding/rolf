Text;Label;Repo
"""InfoGAN is an information-theoretic extension to the simple Generative Adversarial Networks that is able to learn disentangled representations in a completely unsupervised manner. What this means is that InfoGAN successfully disentangle wrirting styles from digit shapes on th MNIST dataset and discover visual concepts such as hair styles and gender on the CelebA dataset. To achieve this an information-theoretic regularization is added to the loss function that enforces the maximization of mutual information between latent codes  c  and the generator distribution G(z  c).   """;Computer Vision;https://github.com/vinoth654321/Casia-Webface
"""The project implement the clipped version of Proximal Policy Optimization Algorithms described here https://arxiv.org/pdf/1707.06347.pdf  Into the config.yaml file are defined some of the hyper parameter used into the various implementation. Those parameters are initilized with the values proposed in the article.   Here the key point: * Loss function parameters:   * epsilon = 0.2   * gamma = 0.99   * entropy loss = 1e-3    * Network size:   * state_size = 27 (25 laser scan + target heading + target distance)   * action_size (angular velocity) = 5   * action_size2 (linear velocity) = 3   * batch_size = 64   * output layer = 8 into 2 streams (5 nodes for angular and 3 for linear velocity)   * lossWeights for the output layer:      * 0 5     * 0.5      The values for the loss weights are the result of some test. With an equal weight the success rate is lower.    """;Reinforcement Learning;https://github.com/MatteoBrentegani/PPO
"""We build a plant disease diagnosis system on Android  by implementing a deep convolutional neural network with Tensorflow to detect disease from various plant leave images.   Generally  due to the size limitation of the dataset  we adopt the transefer learning in this system. Specifically  we retrain the MobileNets [[1]](https://arxiv.org/pdf/1704.04861.pdf)  which is first trained on ImageNet dataset  on the plant disease datasets. Finally  we port the trained model to Android.   """;General;https://github.com/sabrisangjaya/plantdoctor
"""---  Here I have taken a flower classification dataset from http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html. After some preprocessing of images I took InceptionNet as my model and fine tuned it's last 50 layers and freezed all the first 50 but batch normalization layers.   """;General;https://github.com/bhuyanamit986/FlowerClassification
"""The goal of this project is to enable users to create cool web demos using the newly released OpenAI GPT-3 API **with just a few lines of Python.**   This project addresses the following issues:  1. Automatically formatting a user's inputs and outputs so that the model can effectively pattern-match 2. Creating a web app for a user to deploy locally and showcase their idea  Here's a quick example of priming GPT to convert English to LaTeX:  ``` #: Construct GPT object and show some examples gpt = GPT(engine=""davinci""            temperature=0.5            max_tokens=100) gpt.add_example(Example('Two plus two equals four'  '2 + 2 = 4')) gpt.add_example(Example('The integral from zero to infinity'  '\\int_0^{\\infty}')) gpt.add_example(Example('The gradient of x squared plus two times x with respect to x'  '\\nabla_x x^2 + 2x')) gpt.add_example(Example('The log of two times x'  '\\log{2x}')) gpt.add_example(Example('x squared plus y squared plus equals z squared'  'x^2 + y^2 = z^2'))  #: Define UI configuration config = UIConfig(description=""Text to equation""                    button_text=""Translate""                    placeholder=""x squared plus 2 times x"")  demo_web_app(gpt  config) ```  Running this code as a python script would automatically launch a web app for you to test new inputs and outputs with. There are already 3 example scripts in the `examples` directory.  You can also prime GPT from the UI. for that  pass `show_example_form=True` to `UIConfig` along with other parameters.  Technical details: the backend is in Flask  and the frontend is in React. Note that this repository is currently not intended for production use.   """;Natural Language Processing;https://github.com/shreyashankar/gpt3-sandbox
"""Nowadays  we have so many high resolution camera  from camera to smartphone  every devices can take photos or videos at high resolution. But sometime we miss a great moments because of blurring or we want to get a photo from video but all the frames is blury.  This project is to build an web-application use Scale-Recurrent Network for deblur that blurred images to restore the value that we want to get from that images.   """;Computer Vision;https://github.com/natuan310/scale-recurrent-network-images-deblurring
"""We trained the model on the dataset with NSFW images as positive and SFW(suitable for work) images as negative. These images were editorially labelled. We cannot release the dataset or other details due to the nature of the data.   We use [CaffeOnSpark](https://github.com/yahoo/CaffeOnSpark) which is a wonderful framework for distributed learning that brings deep learning to Hadoop and Spark clusters for training models for our experiments. Big thanks to the CaffeOnSpark team!  The deep model was first pretrained on ImageNet 1000 class dataset. Then we finetuned the weights on the NSFW dataset. We used the thin resnet 50 1by2 architecture as the pretrained network. The model was generated using [pynetbuilder](https://github.com/jay-mahadeokar/pynetbuilder) tool and replicates the [residual network](https://arxiv.org/pdf/1512.03385v1.pdf) paper's 50 layer network (with half number of filters in each layer).  You can find more details on how the model was generated and trained [here](https://github.com/jay-mahadeokar/pynetbuilder/tree/master/models/imagenet)  Please note that deeper networks  or networks with more filters can improve accuracy. We train the model using a thin residual network architecture  since it provides good tradeoff in terms of accuracy  and the model is light-weight in terms of runtime (or flops) and memory (or number of parameters).   """;Computer Vision;https://github.com/yahoo/open_nsfw
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/appcoreopc/berty
"""The data set could found [Allen Institute for AI ARC](https://leaderboard.allenai.org/arc/submissions/public). The dataset contains 7 787 natural grade-school level multiple-choice SCIENCE questions. This dataset's level of difficulty requires far more powerful knowledge and reasoning capability than ever before datasets such SQuAD or SNLI. The data set has two partitions: EASY Set and CHALLENGE Set. And inside each set  it is also devided into train  test and development sets. A corpus is also given in the dataset which could be used as background inforamtion source. But the ARC challenge is not limited to this corpus knowledge and it could also be open book.  <b> Easy: </b>   Easy-Train Set: 2251 questions   Easy-Test Set: 2376 questions   Easy-Development Set: 570 questions    <b> Challenge: </b>   The Challenge Set contains only questions answer incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm.    Challenge-Train Set: 1119 questions   Challenge-Test Set: 1172 questions   Challenge-Development Set: 299 questions    <b> Reference: </b>   P. Clark  I. Cowhey  O. Etzioni  T. Khot  A. Sabharwal  C. Schoenick  and O. Tafjord. 2018. Think you have solved question answering? Try ARC  the AI2 reasoning challenge. CoRR  abs/1803.05457.  <b> Example: </b>   EASY: Which technology was developed most recently?   &nbsp; &nbsp; A. cellular telephone(correct)   &nbsp; &nbsp; B. television   &nbsp; &nbsp; C. refrigerator   &nbsp; &nbsp; D. airplane    CHALLENGE: Which technology was developed most recently?   &nbsp; &nbsp; A. cellular telephone   &nbsp; &nbsp; B. television   &nbsp; &nbsp; C. refrigerator   &nbsp; &nbsp; D. airplane (correct)   1. T5 model: t5_test.ipynb and t5_ARC.ipynb  2. BERT baseline model: arc_easy_BERT_base_model.ipynb and arc_challenge_BERT_base_model.ipynb  3. RoBERTa-base without/without knowl-edge: LSH_attention.ipynb  4. Report for the project: CSE_576_2020Spring_Project_ARC.pdf   """;Natural Language Processing;https://github.com/duanchi1230/NLP_Project_AI2_Reasoning_Challenge
"""![alt text](https://github.com/saeedabi1/deep_learning_project/blob/master/pictures/Screen%20Shot%202020-05-23%20at%205.08.25%20PM.png?raw=true)    ![alt text](https://github.com/saeedabi1/deep_learning_project/blob/master/pictures/Screen%20Shot%202020-05-23%20at%205.08.39%20PM.png?raw=true)    Attention mechanisms are broadly used in present image captioning encoder / decoder frameworks  where at each step a weighted average is generated on encoded vectors to direct the process of caption decoding.  However  the decoder has no knowledge of whether or how well the vector being attended and the attention question being given are related  which may result in the decoder providing erroneous results.  Image captioning  that is to say generating natural automatic descriptions of language images are useful for visually impaired images and for the quest of natural language related pictures.  It is significantly more demanding than traditional vision tasks recognition of objects and classification of images for two guidelines.  First  well formed structured output space natural language sentences are considerably more challenging than just a set of class labels to predict.  Secondly  this dynamic output space enables a more thin understanding of the visual scenario  and therefore also a more informative one visual scene analysis to do well on this task.   """;General;https://github.com/varun-bhaseen/Image-caption-generation-using-attention-model
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/bhavitvyamalik/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/algharak/BERTenhance
"""Here we introduce RainNet -- a convolutional neural network for radar-based precipitation nowcasting. RainNet was trained to predict continuous precipitation intensities at a lead time of five minutes  using several years of quality-controlled weather radar composites provided by the German Weather Service (DWD).   The source code of the RainNet model written using [_Keras_](https://keras.io) functional API is in the file `rainnet.py`.  The pretrained instance of `keras` `Model` for RainNet  as well as RainNet's pretrained weights are available on Zenodo:   [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3630429.svg)](https://doi.org/10.5281/zenodo.3630429)   """;Computer Vision;https://github.com/hydrogo/rainnet
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/jack16888/caffessd
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/guzhang480/Google_BERT
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/bt77/pointnet
"""1. [requirements.txt](https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/blob/main/requirements.txt) - Includes all the required libraries for the Conda Environment. 2. [model.py](https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/blob/main/model.py) - Defines the actor and critic networks. 3. [ddpg_agent.py](https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/blob/main/ddpg_agent.py) -  Defines the Agent that uses DDPG to determine the best action to take and maximizes the overall or total reward. 4. [Continuous_Control.ipynb](https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/blob/main/Continuous_Control.ipynb) - The main file that trains the actor and critic networks. This file can be run in the Conda environment.   """;Reinforcement Learning;https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/icewing1996/bert_dep
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/duyunshu/bert-sentiment-analysis
"""Accurately ranking the vast number of candidate detections is crucial for dense object detectors to achieve high performance. In this work  we propose to learn IoU-aware classification scores (**IACS**) that simultaneously represent the object presence confidence and localization accuracy  to produce a more accurate ranking of detections in dense object detectors. In particular  we design a new loss function  named **Varifocal Loss (VFL)**  for training a dense object detector to predict the IACS  and a new efficient star-shaped bounding box feature representation (the features at nine yellow sampling points) for estimating the IACS and refining coarse bounding boxes. Combining these two new components and a bounding box refinement branch  we build a new IoU-aware dense object detector based on the FCOS+ATSS architecture  what we call **VarifocalNet** or **VFNet** for short. Extensive experiments on MS COCO benchmark show that our VFNet consistently surpasses the strong baseline by ~2.0 AP with different backbones. Our best model VFNet-X-1200 with Res2Net-101-DCN reaches a single-model single-scale AP of **55.1** on COCO `test-dev`  achieving the state-of-the-art performance among various object detectors.  <div align=""center"">   <img src=""VFNet.png"" width=""600px"" />   <p>Learning to Predict the IoU-aware Classification Score.</p> </div>   """;General;https://github.com/hyz-xmaster/VarifocalNet
"""The goal of this project is to enable users to create cool web demos using the newly released OpenAI GPT-3 API **with just a few lines of Python.**   This project addresses the following issues:  1. Automatically formatting a user's inputs and outputs so that the model can effectively pattern-match 2. Creating a web app for a user to deploy locally and showcase their idea  Here's a quick example of priming GPT to convert English to LaTeX:  ``` #: Construct GPT object and show some examples gpt = GPT(engine=""davinci""            temperature=0.5            max_tokens=100) gpt.add_example(Example('Two plus two equals four'  '2 + 2 = 4')) gpt.add_example(Example('The integral from zero to infinity'  '\\int_0^{\\infty}')) gpt.add_example(Example('The gradient of x squared plus two times x with respect to x'  '\\nabla_x x^2 + 2x')) gpt.add_example(Example('The log of two times x'  '\\log{2x}')) gpt.add_example(Example('x squared plus y squared plus equals z squared'  'x^2 + y^2 = z^2'))  #: Define UI configuration config = UIConfig(description=""Text to equation""                    button_text=""Translate""                    placeholder=""x squared plus 2 times x"")  demo_web_app(gpt  config) ```  Running this code as a python script would automatically launch a web app for you to test new inputs and outputs with. There are already 3 example scripts in the `examples` directory.  You can also prime GPT from the UI. for that  pass `show_example_form=True` to `UIConfig` along with other parameters.  Technical details: the backend is in Flask  and the frontend is in React. Note that this repository is currently not intended for production use.   """;General;https://github.com/shreyashankar/gpt3-sandbox
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/zzdxfei/defor_conv_mxnet_code
"""1. [requirements.txt](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/requirements.txt) - Includes all the required libraries for the Conda Environment. 2. [model.py](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/model.py) - Defines the QNetwork which is the nonlinear function approximator to calculate the value actions based directly on observation from the environment. 3. [dqn_agent.py](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/dqn_agent.py) -  Defines the Agent that uses Deep Learning to find the optimal parameters for the function approximators  determines the best action to take and maximizes the overall or total reward. 4. [Navigation.ipynb](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/Navigation.ipynb) - The main file that trains the Deep Q-Network and shows the trained agent in action. This file can be run in the Conda environment.   """;Reinforcement Learning;https://github.com/iDataist/Navigation-with-Deep-Q-Network
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/y2kmz/pointnetv2
"""notebooks/ contains an example of data augmentation (DataAugmentation)  the training of the model (ModelTraining) and the testing of the trained model (ModelTesting)  app/ contains all the files to run the application: the trained model with the weights  the Flask application  the Dockerfile and the requirements file  test_images/ contains few images from the test subset that can be used to test the app   Computer vision is used for surface defects inspection in multiple fields  like manufacturing and civil engineering. In this project  the problem of detecting cracks in a concrete surface has been tackled using a deep learning model.   """;General;https://github.com/simo-bat/Crack_detection
"""This is the official code of [high-resolution representations for Semantic Segmentation](https://arxiv.org/abs/1904.04514).  We augment the HRNet with a very simple segmentation head shown in the figure below. We aggregate the output representations at four different resolutions  and then use a 1x1 convolutions to fuse these representations. The output representations is fed into the classifier. We evaluate our methods on three datasets  Cityscapes  PASCAL-Context and LIP.  <!-- ![](figures/seg-hrnet.png) --> <figure>   <text-align: center;>   <img src=""./figures/seg-hrnet.png"" alt=""hrnet"" title="""" width=""900"" height=""150"" /> </figcaption> </figure>  Besides  we further combine HRNet with [Object Contextual Representation](https://arxiv.org/pdf/1909.11065.pdf) and achieve higher performance on the three datasets. The code of HRNet+OCR is contained in this branch. We illustrate the overall framework of OCR in the Figure and the equivalent Transformer pipelines:  <figure>   <text-align: center;>   <img src=""./figures/OCR.PNG"" alt=""OCR"" title="""" width=""900"" height=""200"" /> </figure>     <figure>   <text-align: center;>   <img src=""./figures/SegmentationTransformerOCR.png"" alt=""Segmentation Transformer"" title="""" width=""600"" /> </figure>   """;Computer Vision;https://github.com/HRNet/HRNet-Semantic-Segmentation
"""This post describes how I used the eo-learn and fastai libraries to create a machine learning data pipeline that can classify crop types from satellite imagery. I used this pipeline to enter Zindi’s [Farm Pin Crop Detection Challenge](https://zindi.africa/competitions/farm-pin-crop-detection-challenge). I may not have won the contest but I learnt some great techniques for working with remote-sensing data which I detail in this post.  Here are the preprocessing steps I followed:  1. divided an area of interest into a grid of ‘patches’   1. loaded imagery from disk   1. masked out cloud cover   1. added NDVI and euclidean norm features   1. resampled the imagery to regular time intervals   1. added raster layers with the targets and identifiers.  I reframed the problem of crop type classification as a semantic segmentation task and trained a U-Net with a ResNet50 encoder on multi-temporal multi-spectral data using image augmentation and mixup to prevent over-fitting.  My solution borrows heavily from the approach outlined by [Matic Lubej](undefined) in his [three](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-1-2471e8098195) [excellent](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-2-bd9aa86f8500) [posts](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-3-c62ed9ecd405) on land cover classification with [eo-learn](https://github.com/sentinel-hub/eo-learn).  The python notebooks I created can be found in this github repository: [https://github.com/simongrest/farm-pin-crop-detection-challenge](https://github.com/simongrest/farm-pin-crop-detection-challenge)   """;Computer Vision;https://github.com/simongrest/farm-pin-crop-detection-challenge
"""This Python project is a  from scratch  implementation of a Yolo object detection neural network model in Tensorflow. This implementation consists out of a functioning Yolo model  trainable using the Tensorflow ADAM optimizer on data like the Microsoft COCO dataset.    """;Computer Vision;https://github.com/RobbertBrand/Yolo-Tensorflow-Implementation
"""This package provides a simple way to boost the performance of image captioning task by reranking the hypotheses sentences according to the captions of nearest neighbor images in the training set. We denote this method as consensus reranking for the rest of this document.  It also provides images features (both refined by the [m-RNN model](www.stat.ucla.edu/~junhua.mao/m-RNN.html) and the original [VGG feature](http://arxiv.org/abs/1409.1556) ) on MS COCO Train2014  Val2014  and Test2014 dataset. We take the [m-RNN model](www.stat.ucla.edu/~junhua.mao/m-RNN.html) as an example to generate the hypotheses descriptions of an image.  The details is described in Section 8 of the latest version of the m-RNN paper: [Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)](http://arxiv.org/abs/1412.6632). The work is inspired by the [nearest neighbor captions retrieval method](http://arxiv.org/abs/1505.04467).   """;Computer Vision;https://github.com/mjhucla/mRNN-CR
"""This code tries to implement the Neural Turing Machine  as found in  https://arxiv.org/abs/1410.5401  as a backend neutral recurrent keras layer.  A very default experiment  the copy task  is provided  too.  In the end there is a TODO-List. Help would be appreciated!  NOTE: * There is a nicely formatted paper describing the rough idea of the NTM  implementation difficulties and which discusses the   copy experiment. It is available here in the repository as The_NTM_-_Introduction_And_Implementation.pdf.  * You may want to change the LOGDIR_BASE in testing_utils.py to something that works for you or just set a symbolic   link.    """;General;https://github.com/fuchason/NTM-keras
"""Convolutional neural network is a key architure to recognize and classify the image to classes. However  variation of convolutional neural networks are very many and very vary their space/time usage for learning.   In this project  I compared the state-of-art convolutional neural networks and compared them based on **Performance** and **Resources**(space/time). Through this project  it helps to decide the model to solve problem based on problem size and limitation of resources.    --------   """;General;https://github.com/wonjaek36/sodeep_final
"""DeepLab is a series of image semantic segmentation models  whose latest version  i.e. v3+  proves to be the state-of-art. Its major contribution is the use of atrous spatial pyramid pooling (ASPP) operation at the end of the encoder. While the model works extremely well  its open source code is hard to read (at least from my personal perspective). Here we re-implemented DeepLab V3  the earlier version of v3+ (which only additionally employs the decoder architecture)  in a much simpler and more understandable way.   """;Computer Vision;https://github.com/leimao/DeepLab-V3
"""Our project will process videos that contain human faces and return video with all facial features removed  either by performing a blur with randomized parameters  or by omitting facial pixels all together. We will likely do this by training a convolution neural network with a dataset used for video facial recognition  such as ‘Youtube Faces with Facial Keypoints’ found here.   Existing video editing software has blurring functionality  but the user often has to select the features  and it’s unclear whether deblurring could reveal the identity after-the-fact. There are a few papers and similar projects available online that have demonstrated such work  such as this research paper  the following two articles  and the work of Terrance Boult and Walter Schierer.  If time and project complexity allow  an additional portion of the project could be examining feasibility of an on-device-algorithm that could be used on a camera so there was no back-door to deanonymize the data.    """;Computer Vision;https://github.com/johngear/eecs504
"""This is the repo for the Tensorflow implementation of cellSTORM based on the conditional generative adversarial network (cGAN) implmented by pix2pix-tensoflow. In this case it learns a mapping from input images (i.e. degraded  noisy  compressed video-sequences of dSTORM blinking events) to output images (i.e. localization maps/center of possible blinking fluorophore)  The repository has also a scipt to export a given Graph trained on GPU to a cellphone. The cellSTORM localizer APP can be found in another repo.    """;General;https://github.com/bionanoimaging/cellSTORM-Tensorflow
"""Py**T**orch **Im**age **M**odels (`timm`) is a collection of image models  layers  utilities  optimizers  schedulers  data-loaders / augmentations  and reference training / validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.  The work of many others is present here. I've tried to make sure all source material is acknowledged via links to github  arxiv papers  etc in the README  documentation  and code docstrings. Please let me know if I missed anything.   """;General;https://github.com/rwightman/pytorch-image-models
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/autobotasia/vibert
"""This repository is build for the proposed self-attention network (SAN)  which contains full training and testing code. The implementation of SA module with optimized CUDA kernels are also included.  <p align=""center""><img src=""./figure/sa.jpg"" width=""400""/></p>   """;Computer Vision;https://github.com/hszhao/SAN
"""HAR-Web is a web application that can be utilized to carry out the task of Human Activity Recognition in real time on the web using GPU-enabled devices. The web application is based on a micro service architecture where the project has been divided into 4 basic services  each running on a different port and deployed using Docker containers. I would like to add support for Kubernetes but as of now it doesn't support native hardware access like Docker-Swarm. One way of solving this would be to write a host device plugin like https://github.com/honkiko/k8s-hostdev-plugin but specifically for webcam access. If anyone has an idea on how to enable local webcam access on K8S  I would love to hear about it.     """;General;https://github.com/ChetanTayal138/HAR-Web
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Shinya-Kouda/kgc
"""![](https://user-images.githubusercontent.com/10624937/43851024-320ba930-9aff-11e8-8493-ee547c6af349.gif ""Trained Agent"")  In this environment  a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus  the goal of your agent is to maintain its position at the target location for as many time steps as possible.  The observation space consists of 33 variables corresponding to position  rotation  velocity  and angular velocities of the arm. Each action is a vector with four numbers  corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.  This projects implements DDPG for continous control for the [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment.   """;Reinforcement Learning;https://github.com/prajwalgatti/DRL-Continuous-Control
"""**RepPoints**  initially described in [arXiv](https://arxiv.org/abs/1904.11490)  is a new representation method for visual objects  on which visual understanding tasks are typically centered. Visual object representation  aiming at both geometric description and appearance feature extraction  is conventionally achieved by `bounding box + RoIPool (RoIAlign)`. The bounding box representation is convenient to use; however  it provides only a rectangular localization of objects that lacks geometric precision and may consequently degrade feature quality. Our new representation  RepPoints  models objects by a `point set` instead of a `bounding box`  which learns to adaptively position themselves over an object in a manner that circumscribes the object’s `spatial extent` and enables `semantically aligned feature extraction`. This richer and more flexible representation maintains the convenience of bounding boxes while facilitating various visual understanding applications. This repo demonstrated the effectiveness of RepPoints for COCO object detection.  Another feature of this repo is the demonstration of an `anchor-free detector`  which can be as effective as state-of-the-art anchor-based detection methods. The anchor-free detector can utilize either `bounding box` or `RepPoints` as the basic object representation.  <div align=""center"">   <img src=""demo/reppoints.png"" width=""400px"" />   <p>Learning RepPoints in Object Detection.</p> </div>   """;Computer Vision;https://github.com/muditchaudhary/RepPoints-x-Libra-R-CNN-x-Transformer-self-attention
"""Chess has been studied as a subject of artificial intelligence for many decades  which is why it has been described as  “the most widely-studied domain in the history of artificial intelligence” (Silver et al. 1). Ranging back all the way to the 1950s  various aspects of the game have been studied; one of the most notable instances of this is IBM's Deep Blue computer. In 1997  Deep Blue beat chess champion Garry Kasparov (""Deep Blue"")  proving AI's ability to compete against (and beat) the best human chess players.  Given that IBM was able to achieve this over 20 years ago and technology has only continued to evolve  our team chose to focus on creating something that has the ability to derive conclusions from prior player performance and the opening moves used in a chess match. Chess always begins with the same initial board layout; as a result  different combinations of opening moves occur frequently in play and have the potential to be used to predict the outcome of the match at a very early stage. Point systems in chess are often used as the game progresses as a measure of which player holds an advantage; however  our work studies the earliest moves in the game (before players generally begin trading pieces).  Our goal is to predict the likelihood that the result of a chess game can be predicted using only the knowledge of the first x moves and the ratings of the two players involved in the match. Initially  we believed that this may be beneficial for helping players to choose certain openings; though our work may be useful in this respect  the choice of moves should be viewed as a causality of the player's experience. As a result  an inexperienced player making the moves in a bid to play better may yield moderate performance gains  but ultimately the opening moves lead to victory not because they are superior  but because the player making them is superior and recognizes the advantages of using certain openings.   """;Reinforcement Learning;https://github.com/samiamkhan/4641Project
"""A Keras implementation of YOLOv3 (Tensorflow backend) inspired by [allanzelener/YAD2K](https://github.com/allanzelener/YAD2K).   ---   """;Computer Vision;https://github.com/qqwweee/keras-yolo3
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   """;Computer Vision;https://github.com/sunhui1234/haha
""" This report has two results from the lab. The first result is about sentiment analysis with the BERT model on Drug review datasets.1 We built a model to predict the sentiment of the drug review. The second result is about survival analysis with RSF and ANN on the breast cancer datasets from the University of Wisconsin.2 We built a model to predict time till the events  breast cancer occurs in our case  with other features.   """;General;https://github.com/JeheonPark596/sentimentsurvival
"""------------------ *BeautyGAN*: Instance-level Facial Makeup Transfer with Deep Generative Adversarial Network  - Website: [http://liusi-group.com/projects/BeautyGAN](http://liusi-group.com/projects/BeautyGAN)  *MobileNetV2*: Computer Vision and PatternRecognition  - Website: [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)  Usage  - Python 3.6+ - Tensorflow 1.x  Download pretrained models  - [https://pan.baidu.com/s/1wngvgT0qzcKJ5LfLMO7m8A](https://pan.baidu.com/s/1wngvgT0qzcKJ5LfLMO7m8A) - [https://drive.google.com/drive/folders/1pgVqnF2-rnOxcUQ3SO4JwHUFTdiSe5t9](https://drive.google.com/drive/folders/1pgVqnF2-rnOxcUQ3SO4JwHUFTdiSe5t9)  Save pretrained model  index  checkpoint to `models/model_p2`  ``` . +--docs +--mlib +--models |   +-- model_dlib |   +-- model_p2 -> make folder  here save models +--static ... ```  """;General;https://github.com/eunki7/flask-deeplearning-service-demo
"""COVID-19가 전 세계적인 영향을 미치며  자유로운 외부 활동이 거의 불가능해지는 현상이 지구 전체에 발생하였습니다. 이로 인해 집 안에서 활동해야 하는 시간이 늘어나면서 외부 활동과 관련된 소비액이 자연스레 ‘집안 활동’과 관련된 항목으로 옮겨가는 현상이 발생하였습니다.   국내외 가구 브랜드에서는 AR  VR 등 다양한 기술을 접목한 홈 스타일링 서비스를 선보이고 있지만  여전히 공간 전반을 아우르는 홈 스타일링 서비스를 제공받기 위해서는 매장을 방문해야 한다는 치명적인 단점이 존재하는 상황입니다.  이에 저희 팀은 모바일 홈 스타일링 솔루션 ‘Fitting Room’을 제공하고자 합니다.   서비스에 관한 자세한 내용은 ""APP Repositories""의 Readme에 정리하였으며  다음의 URL을 통해 확인해주시면 감사하겠습니다.   """;Computer Vision;https://github.com/KPMG-wiseuniv/AI
"""This is an unofficial submission to ICLR 2019 Reproducibility Challenge. The central theme of the work by the authors is to reduce the computations while improving the accuracy in the case of Object Recognition and Speech Recognition by using multiple branches with different scales in the CNN architecture. This helps in feature detection at different scales. The authors claim that in the case of Object Recognition they can improve the accuracy by 1% while reducing the computations by 1/3rd of the original.  **Update**: For the official code for Big Little Net  checkout [IBM's](https://github.com/IBM/BigLittleNet) Official repository. This repository is being archived.    """;Computer Vision;https://github.com/apoorvagnihotri/big-little-net
"""We included two Jupyter notebooks to demonstrate how the HDF5 datasets are created * For the medium scale datasets view `create_hdf_benchmarking_datasets.ipynb`. You will need `pytorch`  `ogb==1.1.1` and `dgl==0.4.2` libraries to run the notebook. The notebook is also runnable on Google Colaboratory. * For the large scale pcqm4m dataset view `create_hdf_pcqm4m.ipynb`. You will need `pytorch`  `ogb>=1.3.0` and `rdkit>=2019.03.1` to run the notebook.   This is the official implementation of the **Edge-augmented Graph Transformer (EGT)** as described in https://arxiv.org/abs/2108.03348  which augments the Transformer architecture with residual edge channels. The resultant architecture can directly process graph-structured data and acheives good results on supervised graph-learning tasks as presented by [Dwivedi et al.](https://arxiv.org/abs/2003.00982). It also achieves good performance on the large-scale [PCQM4M-LSC](https://arxiv.org/abs/2103.09430) (`0.1263 MAE` on val) dataset. EGT beats convolutional/message-passing graph neural networks on a wide range of supervised tasks and thus demonstrates that convolutional aggregation is not an essential inductive bias for graphs.   """;Graphs;https://github.com/shamim-hussain/egt
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   """;Computer Vision;https://github.com/xjnpark/ds
"""With the rapid growth of Artificial Intelligence (AI)  multimodal emotion recognition has become a major research topic  primarily due to its potential applications in many challenging tasks  such as dialogue generation  user behavior understanding  multimodal interaction  and others. A conversational emotion recognition system can be used to generate appropriate responses by analyzing user emotions (Zhou et al.  2017; Rashkin et al.  2018). In Dialogue Conversations  the utterance by a speaker y (say) depends on the utterance/statement given previously by a speaker x. That there occurs a correlation between the sequence of utterances. This is depicted in Figure-1  where 2 speakers are conversing and there is an emotional shift visible as the conversation proceeds. <br>  ![Figure 1: MELD_about_data](MELD.PNG) <br> **Figure 1: MELD_about_data** <br>   Conversation in its natural form is multimodal. In dialogues  we rely on others’ facial expressions  vocal tonality  language  and gestures to anticipate their stance. For emotion recognition  multimodality is particularly important. For the utterances with language that is difficult to understand  we often resort to other modalities  such as prosodic and visual cues  to identify their emotions. Figure-2 presents examples from the dataset where the presence of multimodal signals in addition to the text itself is necessary in order to make correct predictions of their emotions and sentiments. Multimodal emotion recognition of sequential turns encounters several other challenges. One such example is the classification of short utterances. Utterances like “yeah”  “okay”  “no” can express varied emotions depending on the context and discourse of the dialogue. However  due to the difficulty of perceiving emotions from text alone  most models resort to assigning the majority class. <br> ![Figure 2: Importance of Multimodal Cues](MELD_fig2.png) <br> **Figure 2: Importance of Multimodal Cues** <br> We use MELD [Multimodal Multi-party Dataset for Emotion Recognitions in Conversations Dataset](https://arxiv.org/pdf/1810.02508.pdf). The [dataset](https://github.com/SenticNet/MELD) contains the same dialogue instances available in EmotionLines  but it also encompasses audio and visual modality along with the text. MELD has more than 1400 dialogues and 13000 utterances from Friends TV series. Multiple speakers participated in the dialogues. Each utterance in dialogue has been labeled by any of these seven emotions -- Anger  Disgust  Sadness  Joy  Neutral  Surprise and Fear. MELD also has sentiment (positive  negative and neutral) annotation for each utterance. The other publicly available multimodal emotion and sentiment recognition datasets are MOSEI  MOSI  MOUD. However  none of those datasets is conversational. <br>   """;General;https://github.com/ankurbhatia24/MULTIMODAL-EMOTION-RECOGNITION
"""In this project  my aim was to develop a model that could regenerate patched/covered parts of human faces  and achieve believable results. I used the [Celeb-A](https://www.kaggle.com/jessicali9530/celeba-dataset) dataset  and created a Generative Adversarial Network with a Denoising Autoencoder as the Generator and a Deep Convolutional Network as the Discriminator. I chose this architecture based on *Avery Allen and Wenchen Li*'s [Generative Adversarial Denoising Autoencoder for Face Completion](https://www.cc.gatech.edu/~hays/7476/projects/Avery_Wenchen/).  The Denoising Autoencoder has 'relu' activations in the middle layers while the output layer had a 'tanh' activation. Each Convolution layer was followed by a BatchNormalization layer. The Discriminator has 'LeakyReLU' activations for the Convolution part  with a BatchNormalization layer following every Conv layer. At the end  the output from the CNN segment was flattened and connected to a Dense layer with 1 node  having 'sigmoid' as the activation function. This would enable the discrimator to predict the probability that the input image is real.  I added distortions to the images in two ways:- - Added random Gaussian noise. - Added random sized Black Patches.  The entire training was done on a GTX 1080 GPU  and took about 12days.  The latest checkpoints and the saved generator and discriminator can be found [here](https://drive.google.com/drive/folders/13wUgCcENajkPZ4MHz2bHrJtQepyVDvtb?usp=sharing).  A few sample generated images are present in `saved_imgs`.   """;Computer Vision;https://github.com/rdutta1999/Patched-Face-Regeneration-GAN
"""This model consists of 2 generators and 2 discriminators. The two generators as U-net like CNNs. During the evaluation of the model  I directly used the pretrained salient objective detection model from Joker  https://github.com/Joker316701882/Salient-Object-Detection.  This is a project about image style transfer developed by Tao Liang  Tianrui Yu  Ke Han and Yifan Ruan. Our project contains three different models  one is in ""cycle_gan_unet"" directory which uses the u-net like cnn as generators  one is in ""Ukiyoe_codes"" directory which uses Resnet blocks as generators  which uses the model proposed in this paper https://arxiv.org/pdf/1703.10593.pdf  the other is in neural_style_transfer that implement sytle transfer using convolution neural network proposed in this paper https://arxiv.org/pdf/1508.06576.pdf.   """;General;https://github.com/CarpdiemLiang/style_transfer
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M and VGG2 datasets  which were already packed in the MxNet binary format. The network backbones include ResNet  InceptionResNet_v2  DenseNet  DPN and MobiletNet. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss. * loss-type=0:  Softmax * loss-type=1:  SphereFace * loss-type=2:  CosineFace * loss-type=4:  ArcFace * loss-type=5:  Combined Margin * loss-type=12: TripletLoss  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/zrui94/insight_mx
"""Code repo for reproducing [2017 CVPR](https://arxiv.org/abs/1611.08050) paper using keras.     """;General;https://github.com/anatolix/keras_Realtime_Multi-Person_Pose_Estimation
"""This repo will release the two proposed benchmarks GFR-R and GFR-V in our paper. I hope the proposed benchmarks will attract researchers on _Generalized Face Recognition_ problem. More details can be referred to our paper [Learning Meta Face Recognition in Unseen Domains](https://me.guojianzhu.com/assets/pdfs/05997.pdf)  accepted to CVPR2020.    """;Computer Vision;https://github.com/cleardusk/MFR
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   """;Computer Vision;https://github.com/BlackAngel1111/Fast-RCNN
"""This folder contains the deploy files(include generator scripts) and pre-train models of resnet-v1  resnet-v2  inception-v3  inception-resnet-v2 and densenet(coming soon).  We didn't train any model from scratch  some of them are converted from other deep learning framworks (inception-v3 from [mxnet](https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-inception-v3.md)  inception-resnet-v2 from [tensorflow](https://github.com/tensorflow/models/blob/master/slim/nets/inception_resnet_v2.py))  some of them are converted from other modified caffe ([resnet-v2](https://github.com/yjxiong/caffe/tree/mem)). But to achieve the original performance  finetuning is performed on imagenet for several epochs.   The main contribution belongs to the authors and model trainers.   """;Computer Vision;https://github.com/GeekLiB/caffe-model
""" **BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/MOHAMEDELDAKDOUKY/bert_adjusted
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/JoyHsiao/Haitec
"""Population Based Augmentation (PBA) is a algorithm that quickly and efficiently learns data augmentation functions for neural network training. PBA matches state-of-the-art results on CIFAR with one thousand times less compute  enabling researchers and practitioners to effectively learn new augmentation policies using a single workstation GPU.  This repository contains code for the work ""Population Based Augmentation: Efficient Learning of Augmentation Schedules"" (http://arxiv.org/abs/1905.05393) in TensorFlow and Python. It includes training of models with the reported augmentation schedules and discovery of new augmentation policy schedules.  See below for a visualization of our augmentation strategy.  <p align=""center""> <img src=""figs/augs_v2_crop.png"" width=""40%""> </p>   """;Computer Vision;https://github.com/Zhiwei-Z/pba_experiment
"""* **What:**    * The authors propose an alternative generator architecture for generative adversarial    networks  borrowing from style transfer literature    * The new generator improves the state-of-the-art in terms    of traditional distribution quality metrics   * The authors propose two new  automated methods to quantify interpolation quality    and disentanglement  that are applicable to any generator architecture   * The authors introduce a new  highly varied and high-quality dataset of    human faces The (FFHQ  Flickr-Faces-HQ).    * The authors proposed the way to control different features of the generated image and made   the model more explainable. See theirs official video (https://www.youtube.com/watch?v=kSLJriaOumA&feature=youtu.be) * **Method:**  The idea of creating Mapping Network with the transformed/inner latent space is used  instead of using input  latent code. This mapping part consists of 8 MLP blocks(8 gave the best performance)  which gives __new__ `W-space`  latent code  that is passed to the __Generator__ as a style which consists of 18 conv layers(two for each resolution 4 ** 2 - 1024 ** 2).  See [#Architecture] figure.    Through the experimental process it was found out that starting image at the top of `g-Generator` can be learn   constant tensor instead of a real image. Following the architecture this image then in each block   follows the next process. First conv layer. Then summed with _per channel scaled  noise_ (scaling is learned as B-parameter). Then again conv. Then AdaIN block.    AdaIN Block can be described the next way:  ![AdaIN](assets/adain.png)  where `x_i` is input per channel feature passed vertically from top and `(y_s_i  y_b_i)` are _style_ parameters  got from transformation of `w-vector` from input `W` latent space. To get these style parameters this vector is   first passed through learned affine transformation.       **To sum it all up**  the main interesting points are: the input `4x4x512` tensor is always the same and we   only get styles from our latent variable. Moreover they say that at each scale changing our _style_ inputs   we can change different **very** localised parts of generated image (See video).      > Copying the styles corresponding to coarse spatial resolutions (4 ** 2–8 ** 2) brings high-level aspects such as pose  general hair style  face shape  and eyeglasses from source B  while all colors(eyes  hair  lighting) and finer facial features resemble A. If we instead copy the styles of middle resolutions (16 ** 2–32 ** 2) from B  we inheritsmaller scale facial features  hair style  eyes open/closed from B  while the pose  general face shape  and eyeglasses from A are preserved.Finally  copying the fine styles (64 ** 2–1024 ** 2)   By the way most of the architecture/training process is based on https://arxiv.org/pdf/1710.10196.pdf (Baseline configuration is the Progressive GAN).  The changes made to architecture: - The baseline using bilinear up/downsampling operations  longer training  and tuned hyperparameters - Adding the mapping network and AdaIN operations - Simplify the architecture by removing the traditional input layer and starting the image synthesis from a learned 4 × 4 × 512 constant tensor - Adding the noise inputs - Adding the mixing regularization(I didn't mention it. The idea is the following: in the learning process we don't  take one latent variable  but take two. Then for one part of the network we feed as style first variable and then the other. That way the model separate the influence and doesn't entangle it. At test time see [Mixing] figure below)  ![Architecture](assets/photo5197353128075307948.jpg)  Figure _Architecture_ * **W** - an intermediate latent space * **AdaIN** - adaptive *style* instance normalization * **A** - learned affine transform * **B** - learned per-channel scaling fac-tors to the noise input  ![Mixture](assets/mixture.png)  _Figure Mixing_. Two sets of images were generated from their respective latent codes (sources A and B); the rest of the images were generated bycopying a specified subset of styles from source B and taking the rest from source A.   * **Proposed metrics:**        - Perceptual path length:     ![Path_length](assets/path_length.png)     where `z1  z2` are latent variables; `t` is random value in `(0 1)` interval;     `eps` = `1e-4`; `G` stands for generator; `slerp` - spherical      interpolation; `d(x1  x2)` - `l2(vgg(x1)  vgg(x2))`;     The idea is that for less entangled and informative latent code interpolation of latent-space      vectors should not yield surprisingly non-linear changes in the image. Quantative results in the Table 3.     - Linear separability. Tries to find out the separability in the input     latent code. Given the features of generated images as Male/Female  find out     how informative for this info is input latent code.     The idea is the next: given good dataset as CELEBA-HQ train good classifier of      a given feature; then generate images using our **Generator** and label using trained     classifier so we have <latent variable  class of interest> mapping; then train SVM on this     data and exponent of cross entropy of this model will be the score of Linear Separability.     See table:     ![Metrics](assets/metrics.png)  * **Results:**    * **Frechet inception distance (FID)**  ![FID](assets/photo5195447889172737473.jpg)       """;Computer Vision;https://github.com/irynakostyshyn/A-Style-Based-Generator-Architecture-for-GAN
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/binhetech/bert-application
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Chonwai/Learning_BERT
"""**You can see trained agent in action [here](https://www.youtube.com/watch?v=kldATbEf1zE)**  A reward of +0.1 is provided for holding the ball at each step. The maximum reward is 40 points.    The state space has 33 dimensions and contains the position  rotation  velocity  and angular velocities of the arm.  The action has 4 dimension in range -1 to 1. And describe the tourge to each part of arm.  The task is episodic  and in order to solve the environment  your agent must get an average *score of +30 over 100* consecutive episodes.   This project  describes the reinforcement learning to resolve the continues control problem.  The problem is describe with continues state space and continues action space. The goal is to hold the ball with moving arm :)  I used TD3 algorithm which is extension of Deep Deterministic Gradient Policy method. I include my private extension of local exploration. more details in Report.pdf and https://arxiv.org/pdf/1802.09477.pdf  The enviroment comes from Unity  please read the Unity Environment  before making a copy and trying yourself!   """;Reinforcement Learning;https://github.com/pkasala/ContinuesControl
"""1. [Installation](#installation) 2. [Data preprocessing](#data-preprocessing) 3. [3 models](#create-separate-models-for-age-mask-and-gender) 4. [1 model](#create-a-single-model-for-all-age-mask-and-gender) 5. [Voting](#voting) 6. [Grayscale images training](#grayscale-images-training) 7. [Cropped images training](#cropped-images-training) 8. [References](#references)   """;Computer Vision;https://github.com/JIHOO97/Image-Classification-Competition
"""For ArXiv PDF / abstract tabs:  - Renames the title to paper's title automatically in the background. (Originally is meaningless paper id  or start with paper id) - Add a browser button to open its corresponding abstract / PDF page. (Originally is hard to get back to abstract page from PDF page) - Add a direct download link on abstract page  click it to download the PDF with the title as filename. (Originally is paper id as filename) - Better title even for bookmarks and the [OneTab](https://www.one-tab.com/) plugin! - Firefox has [strict restrictions on PDF.js](https://bugzilla.mozilla.org/show_bug.cgi?id=1454760). So it doesn't work well with OneTab  the PDF renaming is achieved by intercepting requests and show the PDF in a container. The bookmark works well though. - Works well with native tab search (credits: [@The Rooler](https://addons.mozilla.org/en-US/firefox/addon/arxiv-utils/reviews/1674567/))   - [Tab search on Firefox](https://support.mozilla.org/en-US/kb/search-open-tabs-firefox)   - [Enable Tab search on Chrome](https://www.howtogeek.com/722640/how-to-enable-or-disable-the-tab-search-icon-in-chrome/)  [Tab search on Chrome](https://www.howtogeek.com/704212/how-to-search-open-tabs-on-google-chrome/)   - [Enable Tab search on Edge](https://www.makeuseof.com/microsoft-edge-chrome-tab-search/)   ArXiv is a really nice website for researchers  but I think it has 3 main shortages:  1. Unable to link to abstract page from PDF page if the PDF page is accessed directly. 2. No meaningful title for the PDF page  the abstract page have a redundant paper id as the prefix of the title. Bookmarking the PDF page is useless for later bookmark searches. 3. Downloading PDF requires a manual renaming afterwards.  This extension provides a solution to all of them!   """;Computer Vision;https://github.com/j3soon/arxiv-utils
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/mengfu188/insightface.bak
"""This repo contains the code to train and evaluate FCN8 network as described in [A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images](https://arxiv.org/submit/1741331). We investigate the use of [Fully Convolutional Neural Networks](https://arxiv.org/abs/1608.06993) for Endoluminal Scene Segmentation  and report state of the art results on EndoScene dataset.   """;General;https://github.com/jbernoz/deeppolyp
"""This project makes use of the freely-available [Million Song Dataset](http://millionsongdataset.com/)  and its integration with the [Last.fm Dataset](http://millionsongdataset.com/lastfm/). The former provides a link between all the useful information about the tracks (such as title  artist or year) and the audio track themselves  whereas the latter contains tags information on some of the tracks. A preview of the audio tracks can be fetched from services such as 7Digital  but this is allegedly not an easy task.   If you are only interested in our final results  click [here](https://github.com/pukkapies/urop2019#results).  If you want to use some of our code  or try to re-train our model on your own  read on. We will assume you have access to the actual songs in the dataset. Here is the outline of the approach we followed:  1. Extracte all the useful information from the Million Song Dataset and clean both the audio tracks and the Last.fm tags database to produce our final 'clean' data;  2. Prepare a flexible data input pipeline and transform the data in a format which is easy to consume by the training algorithm;  3. Prepare a flexible training script which would allow for multiple experiments (such as slightly different architectures  slightly different versions of the tags database  or different training parameters);  4. Train our model and use it to make sensible tag predictions from a given input audio.  In the following sections  we will provide a brief tutorial of how you may use this repository to make genre predictions of your own  or carry out some further experiments.   """;General;https://github.com/pukkapies/urop2019
"""Tensorflow object detection is very good  easy and free framework for object detection tasks.  It contains varous models trained on COCO  Kitty like large dataset and we use these pretrained models on our custom datset.    """;Computer Vision;https://github.com/vandangorade/BrandLOGO_detection
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/saurabhkulkarni77/BERT_multilabel
"""This is the official PyTorch implementation of the **Attribute2Font: Creating Fonts You Want From Attributes**.  ![Teaser](img/teaser.png)  Paper: [arXiv](https://arxiv.org/abs/2005.07865) | [Research Gate](https://www.researchgate.net/publication/341423467_Attribute2Font_Creating_Fonts_You_Want_From_Attributes/comments)    Supplementary Material: [link](paper/Siggraph2020_Attr2Font_Supplemental_Material.pdf)    Video: [link](img/att2font_demo.mov)    Code: [GitHub](https://github.com/hologerry/Attr2Font)        """;Computer Vision;https://github.com/hologerry/Attr2Font
"""SWA is a simple DNN training method that can be used as a drop-in replacement for SGD with improved generalization  faster convergence  and essentially no overhead. The key idea of SWA is to average multiple samples produced by SGD with a modified learning rate schedule. We use a constant or cyclical learning rate schedule that causes SGD to _explore_ the set of points in the weight space corresponding to high-performing networks. We observe that SWA converges more quickly than SGD  and to wider optima that provide higher test accuracy.   In this repo we implement the constant learning rate schedule that we found to be most practical on CIFAR datasets.  <p align=""center"">   <img src=""https://user-images.githubusercontent.com/14368801/37633888-89fdc05a-2bca-11e8-88aa-dd3661a44c3f.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633885-89d809a0-2bca-11e8-8d57-3bd78734cea3.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633887-89e93784-2bca-11e8-9d71-a385ea72ff7c.png"" width=250> </p>  Please cite our work if you find this approach useful in your research: ```latex @article{izmailov2018averaging    title={Averaging Weights Leads to Wider Optima and Better Generalization}    author={Izmailov  Pavel and Podoprikhin  Dmitrii and Garipov  Timur and Vetrov  Dmitry and Wilson  Andrew Gordon}    journal={arXiv preprint arXiv:1803.05407}    year={2018} } ```    """;General;https://github.com/izmailovpavel/contrib_swa_examples
"""Puzzle RLE (Reinforcement Learning Environment) is an environment for learning the puzzle gameplay from the mobile game [Puzzle and Dragons](https://youtu.be/tLku-s20EBE) (Gungho Online Entertainment  Tokyo  Japan). The environment is a re-implemntation based on orb-matching and clearing mechanics encountered during normal gameplay.  The environment supports the following: * pygame environment visualization engine * 5 Actions: select-orb  move left  up  right  down * Baseline random agent * OpenAI Baselines agents   """;Reinforcement Learning;https://github.com/nathanin/pad
"""BERT-base and BERT-large are respectively 110M and 340M parameters models and it can be difficult to fine-tune them on a single GPU with the recommended batch size for good performance (in most case a batch size of 32).  To help with fine-tuning these models  we have included several techniques that you can activate in the fine-tuning scripts [`run_classifier.py`](./examples/run_classifier.py) and [`run_squad.py`](./examples/run_squad.py): gradient-accumulation  multi-gpu training  distributed training and 16-bits training . For more details on how to use these techniques you can read [the tips on training large batches in PyTorch](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255) that I published earlier this month.  Here is how to use these techniques in our scripts:  - **Gradient Accumulation**: Gradient accumulation can be used by supplying a integer greater than 1 to the `--gradient_accumulation_steps` argument. The batch at each step will be divided by this integer and gradient will be accumulated over `gradient_accumulation_steps` steps. - **Multi-GPU**: Multi-GPU is automatically activated when several GPUs are detected and the batches are splitted over the GPUs. - **Distributed training**: Distributed training can be activated by supplying an integer greater or equal to 0 to the `--local_rank` argument (see below). - **16-bits training**: 16-bits training  also called mixed-precision training  can reduce the memory requirement of your model on the GPU by using half-precision training  basically allowing to double the batch size. If you have a recent GPU (starting from NVIDIA Volta architecture) you should see no decrease in speed. A good introduction to Mixed precision training can be found [here](https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/) and a full documentation is [here](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html). In our scripts  this option can be activated by setting the `--fp16` flag and you can play with loss scaling using the `--loss_scale` flag (see the previously linked documentation for details on loss scaling). The loss scale can be zero in which case the scale is dynamically adjusted or a positive power of two in which case the scaling is static.  To use 16-bits training and distributed training  you need to install NVIDIA's apex extension [as detailed here](https://github.com/nvidia/apex). You will find more information regarding the internals of `apex` and how to use `apex` in [the doc and the associated repository](https://github.com/nvidia/apex). The results of the tests performed on pytorch-BERT by the NVIDIA team (and my trials at reproducing them) can be consulted in [the relevant PR of the present repository](https://github.com/huggingface/pytorch-pretrained-BERT/pull/116).  Note: To use *Distributed Training*  you will need to run one training script on each of your machines. This can be done for example by running the following command on each server (see [the above mentioned blog post]((https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)) for more details): ```bash python -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=$THIS_MACHINE_INDEX --master_addr=""192.168.1.1"" --master_port=1234 run_classifier.py (--arg1 --arg2 --arg3 and all other arguments of the run_classifier script) ``` Where `$THIS_MACHINE_INDEX` is an sequential index assigned to each of your machine (0  1  2...) and the machine with rank 0 has an IP address `192.168.1.1` and an open port `1234`.   """;Natural Language Processing;https://github.com/dnanhkhoa/pytorch-pretrained-BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/FengJiaChunFromSYSU/Bert
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/KaidongLi/tf-3d-alpha
"""------------ A weighted version of the [Soft Actor-Critic Algorithms](https://arxiv.org/pdf/1801.01290.pdf). The code is largely derived from a [Pytorch SAC Implementation](https://github.com/pranz24/pytorch-soft-actor-critic)  We support five sampling strategies:  - Uniform Sampling (as used in the original version of SAC) - [Emphasizing Recent Experience (ERE)](https://arxiv.org/abs/1906.04009) - Approximated version of ERE (ERE\_apx  Proposition 1 in our paper) - 1/age weighed sampling - [Prioritized Experience Replay (PER)](https://arxiv.org/abs/1511.05952)   This implementation demonstrates that ERE  ERE\_apx and 1/age share very similar performances and are better than uniform sampling and PER.   """;Reinforcement Learning;https://github.com/sunfex/weighted-sac
"""  ![Summary](/images/summary.PNG)    """;Natural Language Processing;https://github.com/Raman-Raje/Machine-Reading-Comprehension-Neural-Question-Answer-
"""**AMDIM** (Augmented Multiscale Deep InfoMax) is an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple *views* of a shared *context*.   Our paper describing AMDIM is available at: https://arxiv.org/abs/1906.00910.   """;Computer Vision;https://github.com/cfld/amdim
"""This repository holds the code framework used in the paper Reg R-CNN: Lesion Detection and Grading under Noisy Labels [1]. The framework is a fork of MIC's [medicaldetectiontoolkit](https://github.com/MIC-DKFZ/medicaldetectiontoolkit) with added regression capabilities.  As below figure shows  the regression capability allows for the preservation of ordinal relations in the training signal as opposed to a standard categorical classification loss like the cross entropy loss (see publication for details). <p align=""center""><img src=""assets/teaser.png""  width=50%></p><br> Network Reg R-CNN is a version of Mask R-CNN [2] but with a regressor in place of the object-class head (see figure below). In this scenario  the first stage makes foreground (fg) vs background (bg) detections  then the regression head determines the class on an ordinal scale. Consequently  prediction confidence scores are taken from the first stage as opposed to the head in the original Mask R-CNN. <p align=""center""><img src=""assets/regrcnn.png""  width=50%></p><br>  In the configs file of a data set in the framework  you may set attribute self.prediction_tasks = [""task""] to a value ""task"" from [""class""  ""regression_bin""  ""regression""]. ""class"" produces the same behavior as the original framework  i.e.  standard object-detection behavior. ""regression"" on the other hand  swaps the class head of network Mask R-CNN [2] for a regression head. Consequently  objects are identified as fg/bg and then the class is decided by the regressor. For the sake of comparability  ""regression_bin"" produces a similar behavior but with a classification head. Both methods should be evaluated with the (implemented) Average Viewpoint Precision instead of only Average Precision.  Below you will found a description of the general framework operations and handling. Basic framework functionality and description are for the most part identical to the original [medicaldetectiontoolkit](https://github.com/MIC-DKFZ/medicaldetectiontoolkit).  <br/> [1] Ramien  Gregor et al.  <a href=""https://arxiv.org/abs/1907.12915"">""Reg R-CNN: Lesion Detection and Grading under Noisy Labels""</a>. In: UNSURE Workshop at MICCAI  2019.<br> [2] He  Kaiming  et al.  <a href=""https://arxiv.org/abs/1703.06870"">""Mask R-CNN""</a> ICCV  2017<br> <br>   """;Computer Vision;https://github.com/MIC-DKFZ/RegRCNN
"""Our Bregman learning framework aims at training sparse neural networks in an inverse scale space manner  starting with very few parameters and gradually adding only relevant parameters during training. We train a neural network <img src=""https://latex.codecogs.com/svg.latex?f_\theta:\mathcal{X}\rightarrow\mathcal{Y}"" title=""net""/> parametrized by weights <img src=""https://latex.codecogs.com/svg.latex?\theta"" title=""weights""/> using the simple baseline algorithm <p align=""center"">       <img src=""https://latex.codecogs.com/svg.latex?\begin{cases}v\gets\ v-\tau\hat{\nabla}\mathcal{L}(\theta) \\\theta\gets\mathrm{prox}_{\delta\ J}(\delta\ v) \end{cases}"" title=""Update"" /> </p>  where  * <img src=""https://latex.codecogs.com/svg.latex?\mathcal{L}"" title=""loss""/> denotes a loss function with stochastic gradient <img src=""https://latex.codecogs.com/svg.latex?\hat{\nabla}\mathcal{L}"" title=""stochgrad""/>  * <img src=""https://latex.codecogs.com/svg.latex?J"" title=""J""/> is a sparsity-enforcing functional  e.g.  the <img src=""https://latex.codecogs.com/svg.latex?\ell_1"" title=""ell1""/>-norm  * <img src=""https://latex.codecogs.com/svg.latex?\mathrm{prox}_{\delta\ J}"" title=""prox""/> is the proximal operator of <img src=""https://latex.codecogs.com/svg.latex?J"" title=""J""/>.  Our algorithm is based on linearized Bregman iterations [[2]](#2) and is a simple extension of stochastic gradient descent which is recovered choosing <img src=""https://latex.codecogs.com/svg.latex?J=0"" title=""Jzero""/>. We also provide accelerations of our baseline algorithm using momentum and Adam [[3]](#3).   The variable <img src=""https://latex.codecogs.com/svg.latex?v"" title=""v""/> is a subgradient of <img src=""https://latex.codecogs.com/svg.latex?\theta"" title=""weights""/> with respect to the *elastic net* functional   <p align=""center"">       <img src=""https://latex.codecogs.com/svg.latex?J_\delta(\theta)=J(\theta)+\frac1\delta\|\theta\|^2"" title=""el-net""/> </p>  and stores the information which parameters are non-zero.   """;General;https://github.com/TimRoith/BregmanLearning
"""lr - learning rate can be scalar or function  in second case relative step size is using.  beta1  beta2 - is also can be scalar or functions  in first case algorithm works as AMSGrad. Setting beta1 to zero is turning off moments updates.  non_constant_decay - boolean  has effect if betas are scalars. If True using functions for betas (from section 7.1)  enable_factorization - boolean. Factorization works on 2D weights.  clipping_threshold - scalar. Threshold value for update clipping (from section 6)  """;General;https://github.com/DeadAt0m/adafactor-pytorch
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/KhusDM/PointNetTree
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.   ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    """;Natural Language Processing;https://github.com/habibullah-araphat/BERT-GPU
"""The embeddings generated by Deep Speaker can be used for many tasks  including speaker identification  verification  and clustering. We experiment with ResCNN architectures to extract the acoustic features  then mean pool to produce utterance-level speaker embeddings  and train using triplet loss based on cosine similarity.   """;Computer Vision;https://github.com/dodoproptit99/deep-speaker
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/guzhang480/Google_BERT
"""| Name           | Type  | Default                       | Description                         | | -------------- | ----- | ----------------------------- | ----------------------------------- | | data_dir       | str   | ""modelnet40_normal_resampled"" | train & test data dir               | | num_point      | int   | 1024                          | sample number of points             | | batch_size     | int   | 32                            | batch size in training              | | num_category   | int   | 40                            | ModelNet10/40                       | | learning_rate  | float | 1e-3                          | learning rate in training           | | max_epochs     | int   | 200                           | max epochs in training              | | num_workers    | int   | 32                            | number of workers in dataloader     | | log_batch_num  | int   | 50                            | log info per log_batch_num          | | model_path     | str   | ""pointnet.pdparams""           | save/load model in training/testing | | lr_decay_step  | int   | 20                            | step_size in StepDecay              | | lr_decay_gamma | float | 0.7                           | gamma in StepDecay                  |   This project reproduces PointNet based on paddlepaddle framework.  PointNet provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  **Paper:** [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/pdf/1612.00593.pdf)  **Competition Page:** [PaddlePaddle AI Studio](https://aistudio.baidu.com/aistudio/competition/detail/106)  **PointNet Architecture:** ![arch](arch.png)  **Other Version Implementation:**  - [TensorFlow (Official)](https://github.com/charlesq34/pointnet) - [PyTorch](https://github.com/yanx27/Pointnet_Pointnet2_pytorch)  **Acceptance condition**  - Classification Accuracy 89.2 on ModelNet40 Dataset   """;Computer Vision;https://github.com/Phimos/Paddle-PointNet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/saurabhnlp/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/rpuiggari/bert2
"""**GCNet** is initially described in [arxiv](https://arxiv.org/abs/1904.11492). Via absorbing advantages of Non-Local Networks (NLNet) and Squeeze-Excitation Networks (SENet)   GCNet provides a simple  fast and effective approach for global context modeling  which generally outperforms both NLNet and SENet on major benchmarks for various recognition tasks.   """;Computer Vision;https://github.com/xvjiarui/GCNet
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/Eltomad/insightface
"""Siamese networks have drawn great attention in visual tracking because of their balanced accuracy and speed.  However  the backbone network utilized in these trackers is still the classical AlexNet  which does not fully take advantage of the capability of modern deep neural networks.     Our proposals improve the performances of fully convolutional siamese trackers by  1) introducing CIR and CIR-D units to unveil the power of deeper and wider networks like [ResNet](https://arxiv.org/abs/1512.03385) and [Inceptipon](https://arxiv.org/abs/1409.4842);  2) designing backbone networks according to the analysis on internal network factors (e.g. receptive field  stride  output feature size)  which affect tracking performances.  <div align=""center"">   <img src=""demo/vis.gif"" width=""800px"" />   <!-- <p>Example SiamFC  SiamRPN and SiamMask outputs.</p> --> </div>  <!-- :tada::tada: **Highlight !!** Siamese tracker is severely sensitive to hyper-parameter  which is a common sense in tracking field. Although significant progresses have been made in some works  the result is hard to reproduce. In this case  we provide a [parameter tuning toolkit]() to make our model being reproduced easily. We hope our efforts and supplies will be helpful to your work. -->   """;General;https://github.com/researchmm/SiamDW
"""R-CNN is a state-of-the-art visual object detection system that combines bottom-up region proposals with rich features computed by a convolutional neural network. At the time of its release  R-CNN improved the previous best detection performance on PASCAL VOC 2012 by 30% relative  going from 40.9% to 53.3% mean average precision. Unlike the previous best results  R-CNN achieves this performance without using contextual rescoring or an ensemble of feature types.  R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1311.2524) and will appear in a forthcoming CVPR 2014 paper.   """;Computer Vision;https://github.com/jiangbestone/detect_rcnn
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/paolanu/BERT_epitope
"""**Quasi-Recurrent Neural Networks (QRNNs)** are used for the RNA sub-cellular localization (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5210605/).   QRNNs embrace the benefits of both convolutional and recurrent neural networks alike. QRNNs beat other networks such as LSTM in both accuracy and speed.   The followind figure depicts the difference between an LSTM layer and a QRNN layer. The first is totally sequential; the second can be parallelized with convolutions and the sequential part in much lighter. ![layer](https://github.com/AllenMont/qrnn-rna-localization/blob/master/img/layer.PNG)  A description of QRNNs can be found in https://arxiv.org/abs/1611.01576. The PyTorch implementation follows https://github.com/salesforce/pytorch-qrnn.   """;Sequential;https://github.com/montallen/qrnn-rna-localization
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/JimmyLauren/caffe_ssd_mofified
"""Super-resolution (SR) of images refers to the process of generating or reconstructing the high- resolution (HR) images from low-resolution images (LR). This project mainly focuses on dealing with this problem of super-resolution using the generative adversarial network  named SRGAN  a deep learning framework. In this project  SRGAN was trained and evaluated using 'DIV2K'  ‘MS-COCO’ and ‘VID4’ [6] which are the popular datasets for image resolution tasks.  In total  datasets were merged to form:   1. 5800 training images  2. 100 validation images  3. 4 videos for testing  Apart from the datasets mentioned above  ‘LFW’  ‘Set5’ and ‘Set14’ datasets [6] were used to get inferences and compare the performance of models implemented in this project with the models from Ledig et al. [2].  Most of this project is built upon the ideas of Ledig et al [2]. Apart from that  I did some research on comparing the results obtained using different objective functions available in TensorFlow’s “TFGAN” library for loss optimizations of SRGAN. Different model implementations were evaluated for pixel quality through the peak signal-to-noise ratio (PSNR) scores as a metric. Intuitively  this metric does not capture the essence of the perceptual quality of an image. However  it is comparatively easy to use PSNR when evaluating the performance while training the model compared to mean-opinion-score (MOS) that has been used by Ledig et al [2]. To evaluate the perceptual quality of images  I have compared the generated images from both the models. This paper also proposes a method of super-resolution using SRGAN with “Per-Pix loss” which I defined in the losses section of this paper. Based on results from [2] and [5]  I have combined both MSE and VGG losses  named it “Per-Pix loss” that stands for ‘Perceptual and Pixel’ qualities of the image  which resulted in preserving the pixel quality besides improving the perceptual quality of images. Finally  I have compared the models built in this project with the models from Ledig et al. [2] to know the performance and effectiveness of models implemented in this project.   """;Computer Vision;https://github.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/molyswu/knowledge-graph-embeding
"""""Several areas of Earth with large accumulations of oil and gas also have huge deposits of salt below the surface. But unfortunately  knowing where large salt deposits are precisely is very difficult. Professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective  highly variable renderings. More alarmingly  it leads to potentially dangerous situations for oil and gas company drillers."" - from competition description.  In this competition TGS provided images collected using seismic reflection of the ground in various locations. Thus in the data we are given training data as the images and their appropriate masks highlighting the salt deposit within that image as labels. The goal of the competition is to build a model that best performs this image segmentation task.   """;Computer Vision;https://github.com/JHLee0513/Salt_detection_challenge
"""  [MegaFace](http://megaface.cs.washington.edu/) dataset includes 1 027 060 faces  690 572 identities.   Challenge 1 is taken to test our model with 1 million distractors.   ![image](https://github.com/foamliu/InsightFace-v2/raw/master/images/megaface_stats.png)    MS-Celeb-1M dataset for training  3 804 846 faces over 85 164 identities.    """;General;https://github.com/foamliu/InsightFace-PyTorch
"""SWA-Gaussian (SWAG) is a convenient method for uncertainty representation and calibration in Bayesian deep learning. The key idea of SWAG is that the SGD iterates  with a modified learning rate schedule  act like samples from a Gaussian distribution; SWAG fits this Gaussian distribution by capturing the [SWA](https://arxiv.org/abs/1803.05407) mean and a covariance matrix  representing the first two moments of SGD iterates. We use this Gaussian distribution as a posterior over neural network weights  and then perform a Bayesian model average  for uncertainty representation and calibration.  <p align=""center"">   <img src=""https://user-images.githubusercontent.com/14368801/52224039-09ab0b80-2875-11e9-9c12-c72b88abf4a9.png"" width=350>   <img src=""https://user-images.githubusercontent.com/14368801/52224049-0dd72900-2875-11e9-9de8-540ceaae60b3.png"" width=350> </p>   In this repo  we implement SWAG for image classification with several different architectures on both CIFAR datasets and ImageNet. We also implement SWAG for semantic segmentation on CamVid using our implementation of a FCDenseNet67. We additionally include several other experiments on exploring the covariance of the gradients of the SGD iterates  the eigenvalues of the Hessian  and width/PCA decompositions of the SWAG approximate posterior.  CIFAR10 -> STL10             |  CIFAR100 :-------------------------:|:-------------------------: ![](plots/stl_wrn.jpg)  |  ![](plots/c100_resnet110.jpg)  Please cite our work if you find it useful: ```bibtex @inproceedings{maddox_2019_simple    title={A simple baseline for bayesian uncertainty in deep learning}    author={Maddox  Wesley J and Izmailov  Pavel and Garipov  Timur and Vetrov  Dmitry P and Wilson  Andrew Gordon}    booktitle={Advances in Neural Information Processing Systems}    pages={13153--13164}    year={2019} } ```   """;General;https://github.com/wjmaddox/swa_gaussian
"""Object Cut is an online image background removal service that uses [BASNet](https://github.com/NathanUA/BASNet). Removing the background from an image is a common operation in the daily work of professional photographers and image editors. This process is usually a repeatable and manual task that requires a lot of effort and human time. However  thanks to [BASNet](https://github.com/NathanUA/BASNet)  one of the most robust and fastest performance deep learning models in image segmentation  Object Cut was able to turn it into an easy and automatic process.   It was built as an API to make it as easy as possible to integrate. APIs  also known as Application Programming Interfaces  are already a common way to integrate different types of solutions to improve systems without actually knowing what is happening inside. Specifically  RESTful APIs are a standard in the Software Engineering field for designing and specifying APIs. Making it substantially easier to adapt your desired APIs to your workflows.  <br> <p align=""center"">   <img alt=""Pipeline"" src=""docs/images/pipeline.png"" width=""100%""/> </p> <br>  Object Cut was born to power up the designing and image editing process from the people who work with images daily. Integrating the Object Cut API removes the necessity of understanding the complex inner workings behind it and automates the process of removing the background from images in a matter of seconds.   """;Computer Vision;https://github.com/AlbertSuarez/object-cut
"""This toolkit is for developing the Natural Language Processing (NLP) pipelines  including model design and development  model deployment and distributed computing.   """;Natural Language Processing;https://github.com/Impavidity/relogic
"""This project is to study the use of Convolutional Neural Network and in particular the ResNet architecture. The show case is segmentation of Magnetic Resonance Images (MRI) of human brain into anatomical regions[2]. We will extend the ResNet topology into the processing of 3-dimensional voxels.    """;General;https://github.com/bclwan/MRI_Brain_Segmentation
"""Cutout is a simple regularization method for convolutional neural networks which consists of masking out random sections of input images during training. This technique simulates occluded examples and encourages the model to take more minor features into consideration when making decisions  rather than relying on the presence of a few major features.      ![Cutout applied to CIFAR-10](https://github.com/uoguelph-mlrg/Cutout/blob/master/images/cutout_on_cifar10.jpg ""Cutout applied to CIFAR-10"")  Bibtex:   ``` @article{devries2017cutout      title={Improved Regularization of Convolutional Neural Networks with Cutout}      author={DeVries  Terrance and Taylor  Graham W}      journal={arXiv preprint arXiv:1708.04552}      year={2017}   } ```   """;Computer Vision;https://github.com/uoguelph-mlrg/Cutout
"""Recent techniques built on Generative Adversarial Networks (GANs) like [CycleGAN](https://arxiv.org/abs/1703.10593) are able to learn mappings between domains from unpaired datasets through min-max optimization games between generators and discriminators. However  it remains challenging to stabilize the training process and diversify generated results. To address these problems  we present the non-trivial Bayesian extension of cyclic model and an integrated cyclic framework for inter-domain mappings.  The proposed method stimulated by [Bayesian GAN](https://arxiv.org/abs/1705.09558) explores the full posteriors of Bayesian cyclic model (with latent sampling) and optimizes the model with maximum a posteriori (MAP) estimation. By exploring the full posteriors over model parameters  the Bayesian marginalization could alleviate the risk of model collapse and boost multimodal distribution learning. Besides  we deploy a combination of L1 loss and GANLoss between reconstructed images and source images to enhance the reconstructed learning  we also prove that this variation has a global optimality theoretically and show its effectiveness in experiments.   """;Computer Vision;https://github.com/ranery/Bayesian-CycleGAN
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/WLK12580/12
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/shanry/bert
"""The implementation is based on two papers:  - Simple Online and Realtime Tracking with a Deep Association Metric https://arxiv.org/abs/1703.07402 - YOLOv4: Optimal Speed and Accuracy of Object Detection https://arxiv.org/pdf/2004.10934.pdf   This repository contains a moded version of PyTorch YOLOv5 (https://github.com/ultralytics/yolov5). It filters out every detection that is not a person. The detections of persons are then passed to a Deep Sort algorithm (https://github.com/ZQPei/deep_sort_pytorch) which tracks the persons. The reason behind the fact that it just tracks persons is that the deep association metric is trained on a person ONLY datatset.   """;Computer Vision;https://github.com/ElipLam/IceBlock_Detection
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/944284742/1.FaceRecognition
"""For this project  we train an agent to navigate (and collect bananas!) in a large  square world.    ![Trained Agent][image1]  A reward of +1 is provided for collecting a yellow banana  and a reward of -1 is provided for collecting a blue banana.  Thus  the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.    The state space has 37 dimensions and contains the agent's velocity  along with ray-based perception of objects around agent's forward direction.  Given this information  the agent has to learn how to best select actions.  Four discrete actions are available  corresponding to: - **`0`** - move forward. - **`1`** - move backward. - **`2`** - turn left. - **`3`** - turn right.  The task is episodic  and in order to solve the environment  the agent must get an average score of +13 over 100 consecutive episodes.   """;Reinforcement Learning;https://github.com/shehrum/RL_Navigation
"""This is an official pytorch implementation of [*Deep High-Resolution Representation Learning for Human Pose Estimation*](https://arxiv.org/abs/1902.09212).  In this work  we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods **recover high-resolution representations from low-resolution representations** produced by a high-to-low resolution network. Instead  our proposed network **maintains high-resolution representations** through the whole process. We start from a high-resolution subnetwork as the first stage  gradually add high-to-low resolution subnetworks one by one to form more stages  and connect the mutli-resolution subnetworks **in parallel**. We conduct **repeated multi-scale fusions** such that each of the high-to-low resolution representations receives information from other parallel representations over and over  leading to rich high-resolution representations. As a result  the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. </br>  ![Illustrating the architecture of the proposed HRNet](/figures/hrnet.png)  """;Computer Vision;https://github.com/leoxiaobin/deep-high-resolution-net.pytorch
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/madamak/ultrapy-yolov3-chess
"""This paper presents a normalization mechanism called Instance-Level Meta Normalization (ILM-Norm) to address a learning-to-normalize problem. ILM-Norm learns to predict the normalization parameters via both the feature feed-forward and the gradient back-propagation paths. ILM-Norm provides a meta normalization mechanism and has several good properties. It can be easily plugged into existing instance-level normalization schemes such as Instance Normalization  Layer Normalization  or Group Normalization. ILM-Norm normalizes each instance individually and therefore maintains high performance even when small mini-batch is used. The experimental results show that ILM-Norm well adapts to different network architectures and tasks  and it consistently improves the performance of the original models.    """;General;https://github.com/Gasoonjia/ILM-Norm
"""[![video summary](youtube.png)](https://www.youtube.com/watch?v=vUm6vurIwyM)   """;Computer Vision;https://github.com/boschresearch/OASIS
"""U-Net is a deep  fully convolutional neural network architecture proposed for biomedical image segmentation. A visual representation of the network  as shown in the original publication [1]  can be found below.  ![Image of U-Net Architecture](images/U-Net.png)   """;Computer Vision;https://github.com/pricebenjamin/unet-estimator
"""English | [简体中文](README_zh-CN.md)  [![build](https://github.com/open-mmlab/mmocr/workflows/build/badge.svg)](https://github.com/open-mmlab/mmocr/actions) [![docs](https://readthedocs.org/projects/mmocr/badge/?version=latest)](https://mmocr.readthedocs.io/en/latest/?badge=latest) [![codecov](https://codecov.io/gh/open-mmlab/mmocr/branch/main/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmocr) [![license](https://img.shields.io/github/license/open-mmlab/mmocr.svg)](https://github.com/open-mmlab/mmocr/blob/main/LICENSE) [![PyPI](https://badge.fury.io/py/mmocr.svg)](https://pypi.org/project/mmocr/) [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmocr.svg)](https://github.com/open-mmlab/mmocr/issues) [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmocr.svg)](https://github.com/open-mmlab/mmocr/issues)  MMOCR is an open-source toolbox based on PyTorch and mmdetection for text detection  text recognition  and the corresponding downstream tasks including key information extraction. It is part of the [OpenMMLab](https://openmmlab.com/) project.  The main branch works with **PyTorch 1.6+**.  Documentation: https://mmocr.readthedocs.io/en/latest/.  <div align=""left"">   <img src=""resources/illustration.jpg""/> </div>   """;Computer Vision;https://github.com/open-mmlab/mmocr
"""**QA_PreProcess.py\QA_PreProcess.ipynb:** Converts the raw induction tasks data set to separate ndarrays containing questions  answers  and facts with all words being in the form of GloVe pre-trained vector representations.    **DMN+.py\DMN+.ipynb:** The DMN+ model  along with training  validation and testing.    """;General;https://github.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master
"""- We have 3 different networks: a) Discriminator  b) Encoder  and c) Generator - A cGAN-VAE (Conditional Generative Adversarial Network- Variational Autoencoder) is used to encode the ground truth output image B to latent vector z which is then used to reconstruct the output image B' i.e.  B -> z -> B' - For inverse mapping (z->B'->z')  we use LR-GAN (Latent Regressor Generative Adversarial Networks) in which a Generator is used to generate B' from input image A and z. - Combining both these models  we get BicycleGAN. - The architecture of Generator is same as U-net in which there are encoder and decoder nets with symmetric skip connections. - For Encoder  we use several residual blocks for an efficient encoding of the input image. - The model is trained using Adam optimizer using BatchNormalization with batch size 1. - LReLU activation function is used for all types of networks.   """;Computer Vision;https://github.com/prakashpandey9/BicycleGAN
"""1. MNIST  2. cifar10  3. ImageNet   """;Computer Vision;https://github.com/horse007666/ResNet
"""The description made above remains valid here ``` #:#: main parameters reload_model     #: model to reload for encoder decoder #:#: data location / training objective ae_steps          #: denoising auto-encoder training steps bt_steps          #: back-translation steps mt_steps          #: parallel training steps word_shuffle      #: noise for auto-encoding loss word_dropout      #: noise for auto-encoding loss word_blank        #: noise for auto-encoding loss lambda_ae         #: scheduling on the auto-encoding coefficient  #:#: transformer parameters encoder_only      #: use a decoder for MT  #:#: optimization tokens_per_batch  #: use batches with a fixed number of words eval_bleu         #: also evaluate the BLEU score ```  ``` #:#: main parameters exp_name                     #: experiment name exp_id                       #: Experiment ID dump_path                    #: where to store the experiment (the model will be stored in $dump_path/$exp_name/$exp_id)  #:#: data location / training objective data_path                    #: data location  lgs                          #: considered languages/meta-tasks clm_steps                    #: CLM objective mlm_steps                    #: MLM objective  #:#: transformer parameters emb_dim                      #: embeddings / model dimension n_layers                     #: number of layers n_heads                      #: number of heads dropout                      #: dropout attention_dropout            #: attention dropout gelu_activation              #: GELU instead of ReLU  #:#: optimization batch_size                   #: sequences per batch bptt                         #: sequences length optimizer                    #: optimizer epoch_size                   #: number of sentences per epoch max_epoch                    #: Maximum epoch size validation_metrics           #: validation metric (when to save the best model) stopping_criterion           #: end experiment if stopping criterion does not improve  #:#: dataset #:#:#:#: These three parameters will always be rounded to an integer number of batches  so don't be surprised if you see different values than the ones provided. train_n_samples              #: Just consider train_n_sample train data valid_n_samples              #: Just consider valid_n_sample validation data  test_n_samples               #: Just consider test_n_sample test data for #:#:#:#: If you don't have enough RAM/GPU or swap memory  leave these three parameters to True  otherwise you may get an error like this when evaluating : #:#:#:#:#:#: RuntimeError: copy_if failed to synchronize: cudaErrorAssert: device-side assert triggered remove_long_sentences_train #: remove long sentences in train dataset       remove_long_sentences_valid #: remove long sentences in valid dataset   remove_long_sentences_test  #: remove long sentences in test dataset   ```   """;Natural Language Processing;https://github.com/Tikquuss/meta_XLM
"""The dataset creation process has several stages outlined below. We describe the process here at a high level. If you have questions about any individual steps  please contact Rebecca Roelofs (roelofs@cs.berkeley.edu) and Ludwig Schmidt (ludwig@berkeley.edu).    """;Computer Vision;https://github.com/modestyachts/ImageNetV2
"""Fully Convolutional Networks (FCNs) are a natural extension of CNNs to tackle per pixel prediction problems such as semantic image segmentation. FCNs add upsampling layers to standard CNNs to recover the spatial resolution of the input at the output layer. In  order to compensate for the resolution loss induced by pooling layers  FCNs introduce skip connections between their downsampling  and upsampling paths. Skip connections help the upsampling path recover fine-grained information from the downsampling layers.  One evolution of CNNs are [Residual Networks](https://arxiv.org/abs/1512.03385) (ResNets). ResNets are designed to ease the training of  very deep networks by introducing a residual block that sums the non-linear transformation of the input and its identity mapping.  The identity mapping is implemented by means of a shortcut connection. ResNets can be extended to work as FCNs. ResNets incorporate  shortcut paths to FCNs and increase the number of connections within a network. This additional shortcut paths improve the segmentation  accuracy and also help the network to converge faster.  Recently another CNN architecture called [DenseNet](https://arxiv.org/abs/1608.06993) has been introduced. DenseNets are built from  *dense blocks* and pooling operations  where each dense block is an iterative concatenation of previous feature maps. This architecture  can be seen as an extension of ResNets  which performs iterative summation of previous feature maps. The result of this modification  is that DenseNets are more efficient in there parameter usage.  The [https://arxiv.org/abs/1611.09326](https://arxiv.org/abs/1611.09326) paper extends DenseNets to work as FCNs by adding an upsampling  path to recover the full input resolution.    """;General;https://github.com/asprenger/keras_fc_densenet
""" The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.    This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/phnk/yolov3
"""Sonnet has been designed and built by researchers at DeepMind. It can be used to construct neural networks for many different purposes (un/supervised learning  reinforcement learning  ...). We find it is a successful abstraction for our organization  you might too!  More specifically  Sonnet provides a simple but powerful programming model centered around a single concept: `snt.Module`. Modules can hold references to parameters  other modules and methods that apply some function on the user input. Sonnet ships with many predefined modules (e.g. `snt.Linear`  `snt.Conv2D`  `snt.BatchNorm`) and some predefined networks of modules (e.g. `snt.nets.MLP`) but users are also encouraged to build their own modules.  Unlike many frameworks Sonnet is extremely unopinionated about **how** you will use your modules. Modules are designed to be self contained and entirely decoupled from one another. Sonnet does not ship with a training framework and users are encouraged to build their own or adopt those built by others.  Sonnet is also designed to be simple to understand  our code is (hopefully!) clear and focussed. Where we have picked defaults (e.g. defaults for initial parameter values) we try to point out why.   """;General;https://github.com/deepmind/sonnet
"""This open source book is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See [LICENSE](LICENSE) file.  The sample and reference code within this open source book is made available under a modified MIT license. See the [LICENSE-SAMPLECODE](LICENSE-SAMPLECODE) file.  [Chinese version](https://github.com/d2l-ai/d2l-zh) | [Discuss and report issues](https://discuss.d2l.ai/) | [Code of conduct](CODE_OF_CONDUCT.md) | [Other Information](INFO.md)   """;Computer Vision;https://github.com/d2l-ai/d2l-en
"""This repository walks through an example of LIME(Local Interpretable Model-Agnostic Explanations). Orginial LIME paper - https://arxiv.org/abs/1602.04938  LIME provides a means to explain any black-box classifier or regressor.   Models can be difficult to analyze on a global level but may be possible to analyze for a specific instance.  Desirable characterstics of an explainable model: - Interpretable - Local Fidelity - Model-Agnostic - Global Perspective   """;General;https://github.com/blazecolby/PyTorch-LIME
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/ravenagcaoili/yolov3
"""This is the code base for our work of Generative Map at <https://arxiv.org/abs/1902.11124>.   It is an effort to combine generative model (in particular  [Variational Auto-Encoders](https://arxiv.org/abs/1312.6114)) and the classic Kalman filter for generation with localization.   For more details  please refer to [our paper on arXiv](https://arxiv.org/abs/1902.11124).   """;Computer Vision;https://github.com/Mingpan/generative_map
"""![EPS](figure/figure_EPS.png) Existing studies in weakly-supervised semantic segmentation (WSSS) using image-level weak supervision have several limitations:  sparse object coverage  inaccurate object boundaries   and co-occurring pixels from non-target objects.  To overcome these challenges  we propose a novel framework   namely Explicit Pseudo-pixel Supervision (EPS)   which learns from pixel-level feedback by combining two weak supervisions;  the image-level label provides the object identity via the localization map  and the saliency map from the off-the-shelf saliency detection model  offers rich boundaries. We devise a joint training strategy to fully  utilize the complementary relationship between both information.  Our method can obtain accurate object boundaries and discard co-occurring pixels   thereby significantly improving the quality of pseudo-masks.    """;Computer Vision;https://github.com/halbielee/EPS
""" This report has two results from the lab. The first result is about sentiment analysis with the BERT model on Drug review datasets.1 We built a model to predict the sentiment of the drug review. The second result is about survival analysis with RSF and ANN on the breast cancer datasets from the University of Wisconsin.2 We built a model to predict time till the events  breast cancer occurs in our case  with other features.   """;Natural Language Processing;https://github.com/JeheonPark596/sentimentsurvival
"""PaddleOCR aims to create multilingual  awesome  leading  and practical OCR tools that help users train better models and apply them into practice.   """;Computer Vision;https://github.com/raychiu0202/paddleOCR_flask
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/longbowking/bert
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/zengzhaoyang/Weak_Detection
"""VISSL is a computer **VI**sion library for state-of-the-art **S**elf-**S**upervised **L**earning research with [PyTorch](https://pytorch.org). VISSL aims to accelerate research cycle in self-supervised learning: from designing a new self-supervised task to evaluating the learned representations. Key features include:  - **Reproducible implementation of SOTA in Self-Supervision**: All existing SOTA in Self-Supervision are implemented - [SwAV](https://arxiv.org/abs/2006.09882)  [SimCLR](https://arxiv.org/abs/2002.05709)  [MoCo(v2)](https://arxiv.org/abs/1911.05722)  [PIRL](https://arxiv.org/abs/1912.01991)  [NPID](https://arxiv.org/pdf/1805.01978.pdf)  [NPID++](https://arxiv.org/abs/1912.01991)  [DeepClusterV2](https://arxiv.org/abs/2006.09882)  [ClusterFit](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_ClusterFit_Improving_Generalization_of_Visual_Representations_CVPR_2020_paper.pdf)  [RotNet](https://arxiv.org/abs/1803.07728)  [Jigsaw](https://arxiv.org/abs/1603.09246). Also supports supervised trainings.  - **Benchmark suite**: Variety of benchmarks tasks including [linear image classification (places205  imagenet1k  voc07  food  CLEVR  dsprites  UCF101  stanford cars and many more)](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/linear_image_classification)  [full finetuning](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/fulltune)  [semi-supervised benchmark](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/semi_supervised)  [nearest neighbor benchmark](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/nearest_neighbor)  [object detection (Pascal VOC and COCO)](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/object_detection).  - **Ease of Usability**: easy to use using yaml configuration system based on [Hydra](https://github.com/facebookresearch/hydra).  - **Modular**: Easy to design new tasks and reuse the existing components from other tasks (objective functions  model trunk and heads  data transforms  etc.). The modular components are simple *drop-in replacements* in yaml config files.  - **Scalability**: Easy to train model on 1-gpu  multi-gpu and multi-node. Several components for large scale trainings provided as simple config file plugs: [Activation checkpointing](https://pytorch.org/docs/stable/checkpoint.html)  [ZeRO](https://arxiv.org/abs/1910.02054)  [FP16](https://nvidia.github.io/apex/amp.html#o1-mixed-precision-recommended-for-typical-use)  [LARC](https://arxiv.org/abs/1708.03888)  Stateful data sampler  data class to handle invalid images  large model backbones like [RegNets](https://arxiv.org/abs/2003.13678)  etc.  - **Model Zoo**: Over *60 pre-trained self-supervised model* weights.   """;Computer Vision;https://github.com/facebookresearch/vissl
"""The Adversarial Autoencoder behaves similarly to [Variational Autoencoders](https://arxiv.org/abs/1312.6114)  forcing the latent space of an autoencoder to follow a predefined prior. In the case of the Adversarial Autoencoder  this latent space can be defined arbitrarily and easily sampled and fed into the Discriminator in the network.  <div> <img src=""https://raw.githubusercontent.com/greentfrapp/keras-aae/master/images/aae_latent.png"" alt=""Latent space from Adversarial Autoencoder"" width=""whatever"" height=""300px"" style=""display: inline-block;""> <img src=""https://raw.githubusercontent.com/greentfrapp/keras-aae/master/images/regular_latent.png"" alt=""Latent space from regular Autoencoder"" width=""whatever"" height=""300px"" style=""display: inline-block;""> </div>  *The left image shows the latent space of an unseen MNIST test set after training with an Adversarial Autoencoder for 50 epochs  which follows a 2D Gaussian prior. Contrast this with the latent space of the regular Autoencoder trained under the same conditions  with a far more irregular latent distribution.*   """;Computer Vision;https://github.com/greentfrapp/keras-aae
"""This project is a tensorflow implementation of the impressive work  [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/pdf/1609.04802.pdf). <br /> The result is obtained following to same setting from the v5 edition of the [paper on arxiv](https://arxiv.org/pdf/1609.04802.pdf). However  due to limited resources  I train my network on the [RAISE dataset](http://mmlab.science.unitn.it/RAISE/) which contains 8156 high resoution images captured by good cameras. As the results showed below  the performance is close to the result presented in the paper without using the imagenet training set. <br /> The result on BSD100  Set14  Set5 will be reported later. The code is highly inspired by the [pix2pix-tensorflow](https://github.com/affinelayer/pix2pix-tensorflow).   """;General;https://github.com/brade31919/SRGAN-tensorflow
"""AutoInt：An effective and efficient algorithm to automatically learn high-order feature interactions for (sparse) categorical and numerical features.  <div align=center>   <img src=""https://github.com/shichence/AutoInt/blob/master/figures/model.png"" width = 50% height = 50% /> </div> The illustration of AutoInt. We first project all sparse features (both categorical and numerical features) into the low-dimensional space. Next  we feed embeddings of all fields into stacked multiple interacting layers implemented by self-attentive neural network. The output of the final interacting layer is the low-dimensional representation of learnt combinatorial features  which is further used for estimating the CTR via sigmoid function.   """;General;https://github.com/shichence/AutoInt
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/FaskyCC/TextClassification
"""**XLNet** is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally  XLNet employs [Transformer-XL](https://arxiv.org/abs/1901.02860) as the backbone model  exhibiting excellent performance for language tasks involving long context. Overall  XLNet achieves state-of-the-art (SOTA) results on various downstream language tasks including question answering  natural language inference  sentiment analysis  and document ranking.  For a detailed description of technical details and experimental results  please refer to our paper:  ​        [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)  ​        Zhilin Yang\*  Zihang Dai\*  Yiming Yang  Jaime Carbonell  Ruslan Salakhutdinov  Quoc V. Le   ​        (*: equal contribution)   ​        Preprint 2019      """;Natural Language Processing;https://github.com/huggingface/xlnet
"""A brief content description is provided here  for detailed descriptions check the notebook comments     """;Computer Vision;https://github.com/5m0k3/gwd-yolov5-pytorch
"""The description made above remains valid here ``` #:#: main parameters reload_model     #: model to reload for encoder decoder #:#: data location / training objective ae_steps          #: denoising auto-encoder training steps bt_steps          #: back-translation steps mt_steps          #: parallel training steps word_shuffle      #: noise for auto-encoding loss word_dropout      #: noise for auto-encoding loss word_blank        #: noise for auto-encoding loss lambda_ae         #: scheduling on the auto-encoding coefficient  #:#: transformer parameters encoder_only      #: use a decoder for MT  #:#: optimization tokens_per_batch  #: use batches with a fixed number of words eval_bleu         #: also evaluate the BLEU score ```  ``` #:#: main parameters exp_name                     #: experiment name exp_id                       #: Experiment ID dump_path                    #: where to store the experiment (the model will be stored in $dump_path/$exp_name/$exp_id)  #:#: data location / training objective data_path                    #: data location  lgs                          #: considered languages/meta-tasks clm_steps                    #: CLM objective mlm_steps                    #: MLM objective  #:#: transformer parameters emb_dim                      #: embeddings / model dimension n_layers                     #: number of layers n_heads                      #: number of heads dropout                      #: dropout attention_dropout            #: attention dropout gelu_activation              #: GELU instead of ReLU  #:#: optimization batch_size                   #: sequences per batch bptt                         #: sequences length optimizer                    #: optimizer epoch_size                   #: number of sentences per epoch max_epoch                    #: Maximum epoch size validation_metrics           #: validation metric (when to save the best model) stopping_criterion           #: end experiment if stopping criterion does not improve  #:#: dataset #:#:#:#: These three parameters will always be rounded to an integer number of batches  so don't be surprised if you see different values than the ones provided. train_n_samples              #: Just consider train_n_sample train data valid_n_samples              #: Just consider valid_n_sample validation data  test_n_samples               #: Just consider test_n_sample test data for #:#:#:#: If you don't have enough RAM/GPU or swap memory  leave these three parameters to True  otherwise you may get an error like this when evaluating : #:#:#:#:#:#: RuntimeError: copy_if failed to synchronize: cudaErrorAssert: device-side assert triggered remove_long_sentences_train #: remove long sentences in train dataset       remove_long_sentences_valid #: remove long sentences in valid dataset   remove_long_sentences_test  #: remove long sentences in test dataset   ```   """;General;https://github.com/Tikquuss/meta_XLM
"""MMSegmentation is an open source semantic segmentation toolbox based on PyTorch. It is a part of the OpenMMLab project.  The master branch works with **PyTorch 1.5+**.  ![demo image](resources/seg_demo.gif)   """;General;https://github.com/open-mmlab/mmsegmentation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/nmfisher/bert-modified
"""This Repository includes YOLOv3 with some lightweight backbones (***ShuffleNetV2  GhostNet  VoVNet***)  some computer vision attention mechanism (***SE Block  CBAM Block  ECA Block***)  pruning quantization and distillation for GhostNet.  """;Computer Vision;https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention
"""```python #:#:#: To start off let's do a basic data summary.  #: Number of training examples n_train = augmented_X_train.shape[0]  #: Number of validation examples n_validation = X_valid.shape[0]  #: Number of testing examples n_test = X_test.shape[0]  #: What's the shape of an image? image_shape = augmented_X_train[0].shape  #: How many classes are in the dataset n_classes = np.unique(augmented_y_train).shape[0]  print(""Number of training examples =""  n_train) print(""Number of validation examples =""  n_validation) print(""Number of testing examples =""  n_test) print(""Image data shape =""  image_shape) print(""Number of classes =""  n_classes) ```      Number of training examples = 155275     Number of validation examples = 4410     Number of testing examples = 12630     Image data shape = (32  32  3)     Number of classes = 43    ```python X_train = augmented_X_train y_train = augmented_y_train  ```   ```python #:#:#: To start off let's do a basic data summary.  #: Number of training examples n_train = X_train.shape[0]  #: Number of validation examples n_validation = X_valid.shape[0]  #: Number of testing examples n_test = X_test.shape[0]  #: What's the shape of an image? image_shape = X_train[0].shape  #: How many classes are in the dataset n_classes = np.unique(y_train).shape[0]  print(""Number of training examples =""  n_train) print(""Number of validation examples =""  n_validation) print(""Number of testing examples =""  n_test) print(""Image data shape =""  image_shape) print(""Number of classes =""  n_classes) ```      Number of training examples = 34799     Number of validation examples = 4410     Number of testing examples = 12630     Image data shape = (32  32  3)     Number of classes = 43    """;General;https://github.com/ibabbar/Traffic-Sign-Classifier
"""Walking pass an interesting restaurant/coffee shop  thinking about going in but not sure about what they offered and too lazy to search for it in Google?  With this app  all you have to do is to taking one picture  and all these information are there for you. You could also make a purchase for the drink you like. It's time to make your evening adventure much more easier and enjoyable.    """;Computer Vision;https://github.com/minhhuu291/Logo-Recognition
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/thecodemasterk/BERT
"""Online Hard Example Mining (OHEM) is an online bootstrapping algorithm for training region-based ConvNet object detectors like [Fast R-CNN](https://github.com/rbgirshick/fast-rcnn). OHEM  - works nicely in the Stochastic Gradient Descent (SGD) paradigm  - simplifies training by removing some heuristics and hyperparameters  - leads to better convergence (lower training set loss)  - consistently gives significanlty higher mAP on PASCAL VOC and MS COCO.  OHEM was initially presented at CVPR 2016 as an Oral Presentation. For more details  see the [arXiv tech report](http://arxiv.org/abs/1604.03540).   """;General;https://github.com/abhi2610/ohem
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/AmitNativ1984/yolov3
"""SWA is a simple DNN training method that can be used as a drop-in replacement for SGD with improved generalization  faster convergence  and essentially no overhead. The key idea of SWA is to average multiple samples produced by SGD with a modified learning rate schedule. We use a constant or cyclical learning rate schedule that causes SGD to _explore_ the set of points in the weight space corresponding to high-performing networks. We observe that SWA converges more quickly than SGD  and to wider optima that provide higher test accuracy.   In this repo we implement the constant learning rate schedule that we found to be most practical on CIFAR datasets.  <p align=""center"">   <img src=""https://user-images.githubusercontent.com/14368801/37633888-89fdc05a-2bca-11e8-88aa-dd3661a44c3f.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633885-89d809a0-2bca-11e8-8d57-3bd78734cea3.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633887-89e93784-2bca-11e8-9d71-a385ea72ff7c.png"" width=250> </p>  Please cite our work if you find this approach useful in your research: ```latex @article{izmailov2018averaging    title={Averaging Weights Leads to Wider Optima and Better Generalization}    author={Izmailov  Pavel and Podoprikhin  Dmitrii and Garipov  Timur and Vetrov  Dmitry and Wilson  Andrew Gordon}    journal={arXiv preprint arXiv:1803.05407}    year={2018} } ```    """;General;https://github.com/izmailovpavel/torch_swa_examples
"""Pose estimation find the keypoints belong to the people in the image. There are two methods exist for pose estimation.  * **Bottom-Up** first finds the keypoints and associates them into different people in the image. (Generally faster and lower accuracy) * **Top-Down** first detect people in the image and estimate the keypoints. (Generally computationally intensive but better accuracy)  This repo will only include top-down pose estimation models.   """;Computer Vision;https://github.com/sithu31296/pose-estimation
"""This repository contains an implementation of a Resnet-32 model as described in the paper ""Deep Residual Learning for Image Recognition"" (http://arxiv.org/abs/1512.03385) to solve Intel's Image Scene Classification.     """;Computer Vision;https://github.com/Olayemiy/Image-Classification-With-Resnet
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/Hiwyl/yolov3-attentation
"""This notebook contains the deep convolutiona neural network trained to distinguish the numbers shown by fingers on images. It develops on existing VGG neural network and uses the altered loss function to generate the novel images. The original network could be found: https://arxiv.org/abs/1512.03385 The original resnet weights could be found: https://www.kaggle.com/keras/resnet50  It has been developed as a part of the course deeplearning.ai The notebook contains the code and explanations.   Copyright 2020 Deeplearning.ai  """;Computer Vision;https://github.com/abuchin/resnet_sign
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/joytianya/google_bert
"""[fastDNA](#continuous-embedding-of-dna-reads-and-application-to-metagenomics) is a library for classification of short DNA sequences. It is adapted from the [fastText](https://fasttext.cc/) library.    """;Natural Language Processing;https://github.com/rmenegaux/fastDNA
"""Accurately ranking the vast number of candidate detections is crucial for dense object detectors to achieve high performance. In this work  we propose to learn IoU-aware classification scores (**IACS**) that simultaneously represent the object presence confidence and localization accuracy  to produce a more accurate ranking of detections in dense object detectors. In particular  we design a new loss function  named **Varifocal Loss (VFL)**  for training a dense object detector to predict the IACS  and a new efficient star-shaped bounding box feature representation (the features at nine yellow sampling points) for estimating the IACS and refining coarse bounding boxes. Combining these two new components and a bounding box refinement branch  we build a new IoU-aware dense object detector based on the FCOS+ATSS architecture  what we call **VarifocalNet** or **VFNet** for short. Extensive experiments on MS COCO benchmark show that our VFNet consistently surpasses the strong baseline by ~2.0 AP with different backbones. Our best model VFNet-X-1200 with Res2Net-101-DCN reaches a single-model single-scale AP of **55.1** on COCO `test-dev`  achieving the state-of-the-art performance among various object detectors.  <div align=""center"">   <img src=""VFNet.png"" width=""600px"" />   <p>Learning to Predict the IoU-aware Classification Score.</p> </div>   """;Computer Vision;https://github.com/hyz-xmaster/VarifocalNet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/goldenbili/Bert_Test2
"""We present Sandwich Batch Normalization (SaBN)  an extremely easy improvement of Batch Normalization (BN) with only a few lines of code changes.  ![method](imgs/architect.png)  We demonstrate the prevailing effectiveness of SaBN as a drop-in replacement in four tasks: 1. **conditional image generation**  2. **neural architecture search**  3. **adversarial training**  4. **arbitrary neural style transfer**.   """;General;https://github.com/VITA-Group/Sandwich-Batch-Normalization
"""MutualGuide is a compact object detector specially designed for embedded devices. Comparing to existing detectors  this repo contains two key features.   Firstly  the Mutual Guidance mecanism assigns labels to the classification task based on the prediction on the localization task  and vice versa  alleviating the misalignment problem between both tasks; Secondly  the teacher-student prediction disagreements guides the knowledge transfer in a feature-based detection distillation framework  thereby reducing the performance gap between both models.  For more details  please refer to our [ACCV paper](https://openaccess.thecvf.com/content/ACCV2020/html/Zhang_Localize_to_Classify_and_Classify_to_Localize_Mutual_Guidance_in_ACCV_2020_paper.html) and [BMVC paper](https://www.bmvc2021.com/).   """;Computer Vision;https://github.com/ZHANGHeng19931123/MutualGuide
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/tvinith/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/junhahyung/bert_transfer
"""This repository contains my own implementation of the MobileNet Convolutional Neural Network (CNN) developed in Python programming language with Keras and TensorFlow enabled for Graphic Processing Unit (GPU). The provided source-code contains two functions representing the implementations of the MobileNetV1 and MobileNetV2 architectures. A case study in a image set of two airplane models is also provided to evaluate the performance of the network.   """;Computer Vision;https://github.com/danilojodas/MobileNet
"""The purpose of the project is to generate human faces that do not exist in the training set. Four models are compared: An autoencoder (AE)  a variational autoencoder (VAE)  a Deep Convolutional Generative Adversarial Network (DC-GAN)  and a Variational Autoencoder Generative Adversarial Network (VAE-GAN). All models are trained on a processed version of the publicly available LFW face dataset.  Our motivation is to learn about different face generation architectures since there are many applications related to face generation  including police sketching and data augmentation. We are also interested in assessing the effects of GANs on face generation. We would like to determine if GANs create sharper reconstructions and whether a VAE-GAN has more control over generations due to the restrictions on its latent space [1].  Considering the complexity of the task  machine learning is an appropriate tool for it; face generation is a task that does not lend itself to rules-based systems. Unlike traditional algorithms  machine learning algorithms can learn from large amounts of input data and create new data based on the structure of existing data [2].   """;Computer Vision;https://github.com/OsvaldN/APS360_Project
"""- Positional encoding is important (it's quite obvious) to make it work - It seems to be more robust than other approaches I tried before - Surprisingly it works with one head and with only one layer  also without having the residual connection and normalization - Vanilla SGD did not work that well   """;Natural Language Processing;https://github.com/krocki/np-transformer
""" BART model [https://arxiv.org/pdf/1910.13461.pdf](https://arxiv.org/pdf/1910.13461.pdf)  Fairseq [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)  Fairseq tutorial on fine-tuning BART on Seq2Seq task [https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md](https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md)  COVID Dialogue Dataset [https://github.com/UCSD-AI4H/COVID-Dialogue](https://github.com/UCSD-AI4H/COVID-Dialogue)   """;Sequential;https://github.com/huangxt39/BART_on_COVID_dialogue
"""Glaucoma is a retinal disease caused due to increased intraocular pressure in the eyes. It is the second most dominant cause of irreversible blindness after cataract  and if this remains undiagnosed  it may become the first common cause. Ophthalmologists use different comprehensive retinal examinations such as ophthalmoscopy  tonometry  perimetry  gonioscopy and pachymetry to diagnose glaucoma. But all these approaches are manual and time-consuming. Thus  a computer-aided diagnosis system may aid as an assistive measure for the initial screening of glaucoma for diagnosis purposes  thereby reducing the computational complexity. This paper presents a deep learning-based disc cup segmentation glaucoma network (DC-Gnet) for the extraction of structural features namely cup-to-disc ratio  disc damage likelihood scale and inferior superior nasal temporal regions for diagnosis of glaucoma. The proposed approach of segmentation has been trained and tested on RIM-One and Drishti-GS dataset. Further  based on experimental analysis  the DC-Gnet is found to outperform U-net  Gnet and Deep-lab architectures.  ![**Glaucomatous Eye**](Glaucoma.jpg)   """;Computer Vision;https://github.com/archit31uniyal/DC-Gnet
"""https://biomedical-engineering-online.biomedcentral.com<br> https://www.kceyeclinic.com<br> https://uvahealth.com<br> https://www.asrs.org<br> https://entokey.com/retina-4/<br> https://www.atlasophthalmology.net/<br>    In this project  deep learning methods were used to predict abnormalities in fundus images from Kaggle. The images in the dataset were augmented and different colorschemes were tried.   Aproach - Collection of information on the field of Funduns Images Illnesses and Epidemiology  for Benchmarking and Orientation - Decision on tasks  metrics  tools and methods - Exploration of dataset - Preprocessing of data - Augmentation of data - Training of Model on various variants of architecture  hyperparameters and data - Summarize results <br> <br>   """;Computer Vision;https://github.com/daimonae/Fundus-Image-Classification
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Chonwai/Learning_BERT
"""This repository shows how the Object detection and Recognition task can be performed for a personal assistance robot based on **ROS (Robot Operating System)**.  The whole project can be found in this youtube [video](https://youtu.be/AEgZd6wD7dk)  The following are the detailed task steps: 1) Extracting the real time image from the Kinect Camera 2) Detect and Recognize different objects in each image frame from the camera 3) Check distance between the robot camera and each of the detected objects 4) Each of the detected objects within a certain distance from the robot camera is stored with its equivelant position on the envirnoment map extracted from the Rtabmap current odometry published topic 5) While performing the above 4 steps  the robot keep waiting for any voice command sent from the user using the mobile app including the name of an object  in order to start navigating towards the required it if exists in the stored dictionary of previously detected objects and their location  as shown in the figure below.  <p align=""center""> <img src=""https://github.com/youssef-kishk/ROS-Based-Robot-Object-Detection-Recognition-Module/blob/master/images/image.png"" width=""600"" height=""300"" />  </p>    In the Object detection and recognition task  we are dealing with a **Microsoft xbox 360 Kinect camera RGB D** to have a real time image of the environment of the robot  which is then used to detect different objects using **You Only Look Once (YOLO) version 2** model  pretrained on a dataset of 80 different object categories [Common Objects in Context (COCO) dataset](https://cocodataset.org/).   """;Computer Vision;https://github.com/youssef-kishk/ROS-Based-Robot-Object-Detection-Recognition
"""The demos in this folder are designed to give straightforward samples of using TensorFlow in mobile applications.  Inference is done using the [TensorFlow Android Inference Interface](../../../tensorflow/contrib/android)  which may be built separately if you want a standalone library to drop into your existing application. Object tracking and YUV -> RGB conversion is handled by libtensorflow_demo.so.  A device running Android 5.0 (API 21) or higher is required to run the demo due to the use of the camera2 API  although the native libraries themselves can run on API >= 14 devices.   """;Computer Vision;https://github.com/leoliao2008/TensorflowMobileOfficialExample
"""For this project  we will work with the [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment.  In this environment  two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net  it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds  it receives a reward of -0.01.  Thus  the goal of each agent is to keep the ball in play.  The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own  local observation.  Two continuous actions are available  corresponding to movement toward (or away from) the net  and jumping.   The task is episodic  and in order to solve the environment  your agents must get an average score of +0.5 (over 100 consecutive episodes  after taking the maximum over both agents). Specifically   - After each episode  we add up the rewards that each agent received (without discounting)  to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores. - This yields a single **score** for each episode.  The environment is considered solved  when the average (over 100 episodes) of those **scores** is at least +0.5.   """;Reinforcement Learning;https://github.com/biemann/Collaboration-and-Competition
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/faizansuhail89/bert
"""* **Day_01 : 資料介紹與評估指標**     * 探索流程 : 找到問題 -> 初探 -> 改進 -> 分享 -> 練習 -> 實戰     * 思考關鍵點 :         * 為什麼這個問題重要？         * 資料從何而來？         * 資料型態是什麼？         * 回答問題的關鍵指標是什麼？ * **Day_02 : 機器學習概論**     * 機器學習範疇 : **深度學習 (Deep Learning)** ⊂ **機器學習 (Machine Learning)** ⊂ **人工智慧 (Artificial Intelligence)**     * 機器學習是什麼 :         * 讓機器從資料找尋規律與趨勢，不需要給定特殊規則         * 給定目標函數與訓練資料，學習出能讓目標函數最佳的模型參數     * 機器學習總類 :         * **監督是學習 (Supervised Learning)** : 圖像分類 (Classification)、詐騙偵測 (Fraud detection)，需成對資料 (x y)         * **非監督是學習 (Unsupervised Learning)** : 降維 (Dimension Reduction)、分群 (Clustering)、壓縮，只需資料 (x)         * **強化學習 (Reinforcement Learning)** : 下圍棋、打電玩，透過代理機器人 (Agent) 與環境 (Environment) 互動，學習如何獲取最高獎勵 (Reward)，例如 Alpha GO * **Day_03 : 機器學習流程與步驟**     * **資料蒐集、前處理**         * 政府公開資料、Kaggle 資料             * 結構化資料 : Excel 檔、CSV 檔             * 非結構化資料 : 圖片、影音、文字         * 使用 Python 套件             * 開啟圖片 : `PIL`、`skimage`、`open-cv`             * 開啟文件 : `pandas`         * 資料前處理 :             * 缺失值填補             * 離群值處理             * 標準化     * **定義目標與評估準則**         * 回歸問題？分類問題？         * 預測目標是什麼？(target or y)         * 用什麼資料進行預測？(predictor or x)         * 將資料分為 :             * 訓練集，training set             * 驗證集，validation set             * 測試集，test set         * 評估指標             * 回歸問題 (預測值為實數)                 * RMSE : Root Mean Squeare Error                 * MAE : Mean Absolute Error                 * R-Square             * 分類問題 (預測值為類別)                 * Accuracy                 * [F1-score](https://en.wikipedia.org/wiki/F1_score)                 * [AUC](https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF)，Area Under Curve     * **建立模型與調整參數**         * Regression，回歸模型         * Tree-base model，樹模型         * Neural network，神經網路         * Hyperparameter，根據對模型了解和訓練情形進行調整     * **導入**         * 建立資料蒐集、前處理(Preprocessing)等流程         * 送進模型進行預測         * 輸出預測結果         * 視專案需求調整前後端 * **Day_04 : 讀取資料與分析流程 (EDA，Exploratory Data Analysis)**        * 透過視覺化和統計工具進行分析         * 了解資料 : 獲取資料包含的資訊、結構、特點         * 發現 outlier 或異常數值 : 檢查資料是否有誤         * 分析各變數間的關聯性 : 找出重要的變數     * 收集資料 -> 數據清理 -> 特徵萃取 -> 資料視覺化 -> 建立模型 -> 驗證模型 -> 決策應用  """;General;https://github.com/Halesu/4th-ML100Days
"""The goal of **semantic segmentation** is to identify objects  like cars and dogs  in an image by labelling the corresponding groups of pixels according to their classes. For an introduction  see <a href=""https://nanonets.com/blog/semantic-image-segmentation-2020/"">this article</a>. As an example  below is an image and its labelled pixels.  | <img src=""assets/rider.jpg"" alt=""biker"" width=400> | <img src=""assets/rider_label.png"" alt=""true label"" width=400> | |:---:|:---:| | Image | True label |  A **fully convolutional network (FCN)** is an artificial neural network that performs semantic segmentation.  The bottom layers of a FCN are those of a convolutional neural network (CNN)  usually taken from a pre-trained network like VGGNet or GoogLeNet. The purpose of these layers is to perform classification on subregions of the image. The top layers of a FCN are **transposed convolution/deconvolution** layers  which upsample the results of the classification to the resolution of the original image. This gives us a label for each pixel. When upsampling  we can also utilize the intermediate layers of the CNN to improve the accuracy of the segmentation. For an introduction  see <a href=""https://nanonets.com/blog/how-to-do-semantic-segmentation-using-deep-learning/"">this article</a>.  The <a href=""http://host.robots.ox.ac.uk/pascal/VOC/"">Pascal VOC project</a> is a dataset containing images whose pixels have been labeled according to 20 classes (excluding the background)  which include aeroplanes  cars  and people. We will be performing semantic segmentation according to this dataset.   """;Computer Vision;https://github.com/kevinddchen/Keras-FCN
"""This project is about using the [Sign Language Digits Dataset](https://www.kaggle.com/ardamavi/sign-language-digits-dataset/data) to classify images of sign language digits. This is similar to the MNIST dataset that has been used throughout the years to classify a grayscale  handwritten digits between 0 to 9.   """;Computer Vision;https://github.com/francislata/Sign-Language-Digits-CNN
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Charliesgithub20221030/BERT
"""Spatial transformer networks are a generalization of differentiable attention to any spatial transformation. Spatial transformer networks (STN for short) allow a neural network to learn how to perform spatial transformations on the input image in order to enhance the geometric invariance of the model. For example  it can crop a region of interest  scale and correct the orientation of an image. It can be a useful mechanism because CNNs are not invariant to rotation and scale and more general affine transformations.   Goals of the project:  1. Investigate if using CoordConv layers instead of standard Conv will help to improve the performance. 2. Compare the performance of the new model in evaluation metric and motivate the choice of metrics. 3. Explore new ideas that might achieve better performance than conventional STNs.   """;Computer Vision;https://github.com/vicsesi/PyTorch-STN
"""This project maps tree extent at the ten-meter scale using open source artificial intelligence and satellite imagery. The data enables accurate reporting of tree cover in urban areas  tree cover on agricultural lands  and tree cover in open canopy and dry forest ecosystems.   This repository contains the source code for the project. A full description of the methodology can be found [on arXiv](https://arxiv.org/abs/2005.08702). The data product specifications can be accessed on the wiki page. *  [Background](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#background) *  [Data Extent](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#data-extent) *  [Methodology](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#methodology) *  [Validation and Analysis](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#validation-and-analysis) | [Jupyter Notebook](https://github.com/wri/restoration-mapper/blob/master/notebooks/analysis/validation-analysis.ipynb) *  [Definitions](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#definitions) *  [Limitations](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#limitations)    """;General;https://github.com/wri/sentinel-tree-cover
"""This project is dedicated to the investigation of methods for predicting meaningful events in footage of car racing. This repository is focused on the exploration of **collision detection** but contains a tool for the classification of  as well. During the work on this project we've also developed a **monadic pipeline** library [mPyPl](https://github.com/shwars/mPyPl) to simplify tasks of data processing and creating complex data pipelines.    Due to the small amount of data in this problem; we could not rely on neural networks to learn representations as part of the training process. Instead; we needed to design bespoke features  crafted with domain knowledge. After series of experiments  we've created a model based on features obtained using three different approaches:   * Dense Optical Flow * VGG16 embeddings  * A special kind of Optical Flow - [Focused Optical Flow](#focused-optical-flow).     ℹ️ *After the release of mPyPl as a independent [pip-installable framework](https://pypi.org/project/mPyPl/)  some experimental notebooks in the ```notebooks``` folder have not been updated  but may contain interesting things to explore.*   """;General;https://github.com/sulasen/race-events-recognition-1
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/pengshuyuan/Bert
"""English | [简体中文](/README_zh-CN.md)  [![Documentation](https://readthedocs.org/projects/mmaction2/badge/?version=latest)](https://mmaction2.readthedocs.io/en/latest/) [![actions](https://github.com/open-mmlab/mmaction2/workflows/build/badge.svg)](https://github.com/open-mmlab/mmaction2/actions) [![codecov](https://codecov.io/gh/open-mmlab/mmaction2/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmaction2) [![PyPI](https://img.shields.io/pypi/v/mmaction2)](https://pypi.org/project/mmaction2/) [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/blob/master/LICENSE) [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues) [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)  MMAction2 is an open-source toolbox for video understanding based on PyTorch. It is a part of the [OpenMMLab](http://openmmlab.org/) project.  The master branch works with **PyTorch 1.3+**.  <div align=""center"">   <div style=""float:left;margin-right:10px;"">   <img src=""https://github.com/open-mmlab/mmaction2/raw/master/resources/mmaction2_overview.gif"" width=""380px""><br>     <p style=""font-size:1.5vw;"">Action Recognition Results on Kinetics-400</p>   </div>   <div style=""float:right;margin-right:0px;"">   <img src=""https://user-images.githubusercontent.com/34324155/123989146-2ecae680-d9fb-11eb-916b-b9db5563a9e5.gif"" width=""380px""><br>     <p style=""font-size:1.5vw;"">Skeleton-base Action Recognition Results on NTU-RGB+D-120</p>   </div> </div> <div align=""center"">   <img src=""https://github.com/open-mmlab/mmaction2/raw/master/resources/spatio-temporal-det.gif"" width=""800px""/><br>     <p style=""font-size:1.5vw;"">Spatio-Temporal Action Detection Results on AVA-2.1</p> </div>   """;General;https://github.com/open-mmlab/mmaction2
"""![FeatureVis](assets/FeatureVis.png)  In this work  we design a new loss function which merges the merits of both [NormFace](https://github.com/happynear/NormFace) and [SphereFace](https://github.com/wy1iu/sphereface). It is much easier to understand and train  and outperforms the previous state-of-the-art loss function (SphereFace) by 2-5% on MegaFace.    """;General;https://github.com/happynear/AMSoftmax
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Gaozhen0816/BERT_QA_For_AILaw
"""Two experiments are included in this repository  where benchmarks are from the paper [Generalized Sliced Wasserstein Distances](http://papers.nips.cc/paper/8319-generalized-sliced-wasserstein-distances) and the paper [Distributional Sliced-Wasserstein and Applications to Generative Modeling](https://arxiv.org/pdf/2002.07367.pdf)  respectively. The first one is on the task of sliced Wasserstein flow  and the second one is on generative modellings with GANs. For more details and setups  please refer to the original paper **Augmented Sliced Wasserstein Distances**.  """;General;https://github.com/ShwanMario/ASWD
""" - Demo.ipynb - allows you to check enviroment and see working agent example  - Solver.ipynb - reproduces the training procedure  - agent.py - TD3 agent implementation  - networks.py - actor and critic Pytorch definitions  - replay_byffer.py - Replay Buffer implementation from OpenAI Baselines   - actor.pth - Saved weights for Actor network from TD3  - critic.pth - Saved weights from Critic networks from TD3    """;General;https://github.com/crazyleg/TD3-reacher
"""To lead the way with reproducibility  Reaver is bundled with pre-trained weights and full Tensorboard summary logs for all six minigames.  Simply download an experiment archive from the [releases](https://github.com/inoryy/reaver-pysc2/releases) tab and unzip onto the `results/` directory.  You can use pre-trained weights by appending `--experiment` flag to `reaver.run` command:      python reaver.run --map <map_name> --experiment <map_name>_reaver --test 2> stderr.log  Tensorboard logs are available if you launch `tensorboard --logidr=results/summaries`.   You can also view them [directly online](https://boards.aughie.org/board/HWi4xmuvuOSuw09QBfyDD-oNF1U) via [Aughie Boards](https://boards.aughie.org/).   Reaver is a modular deep reinforcement learning framework with a focus on various StarCraft II based tasks  following in DeepMind's footsteps  who are pushing state-of-the-art of the field through the lens of playing a modern video game with human-like interface and limitations.  This includes observing visual features similar (though not identical) to what a human player would perceive and choosing actions from similar pool of options a human player would have. See [StarCraft II: A New Challenge for Reinforcement Learning](https://arxiv.org/abs/1708.04782) article for more details.  Though development is research-driven  the philosophy behind Reaver API is akin to StarCraft II game itself -  it has something to offer both for novices and experts in the field. For hobbyist programmers Reaver offers all the tools necessary to train DRL agents by modifying only a small and isolated part of the agent (e.g. hyperparameters). For veteran researchers Reaver offers simple  but performance-optimized codebase with modular architecture:  agent  model  and environment are decoupled and can be swapped at will.  While the focus of Reaver is on StarCraft II  it also has full support for other popular environments  notably Atari and MuJoCo.  Reaver agent algorithms are validated against reference results  e.g. PPO agent is able to match [ Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347). Please see [below](#but-wait-theres-more) for more details.   """;Reinforcement Learning;https://github.com/inoryy/reaver
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.    """;Computer Vision;https://github.com/jellywangjie/yolov3
"""SSD(Single Shot MultiBox Detector) is a state-of-art object detection algorithm  brought by Wei Liu and other wonderful guys  see [SSD: Single Shot MultiBox Detector @ arxiv](https://arxiv.org/abs/1512.02325)  recommended to read for better understanding.  Also  SSD currently performs good at PASCAL VOC Challenge  see [http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=3](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=3)     """;Computer Vision;https://github.com/rajs25/Object-Detection
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/MonsterPeng/Deformable-ConvNets-master
"""In this research project  to solve real world problems with machine learning  I noted that there is a limit to the traditional Deep Learning application  which is highly dependent on existing datasets because it is still difficult to obtain enough labled data.  The basis for judgment must be clear in the biomedical field  so I decided to use image data among various types for the reason of being visualized intuitively.  Using just one labeled image data for training  I wanted to categorize a lot of unseen data based on it by the basic concept of one shot learning through reinforcement learning.  In this project  I redefined the one shot image segmentation problem as a reinforcement learning and solved it using PPO. I found that there was actually a dramatic performance.  <p align=""center""> <img src=""oneshotgo/data/res/un.png"" width=70%/> </p>   """;Reinforcement Learning;https://github.com/decoderkurt/research_project_school_of_ai_2019
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/duyunshu/bert-sentiment-analysis
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/saurabhnlp/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/TYTYTYTYTYTYTYTYTY/558-project
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.   ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    """;Natural Language Processing;https://github.com/habibullah-araphat/BERT-NER-TPU
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/rpuiggari/bert2
"""* What   * GANs are based on adversarial training.   * Adversarial training is a basic technique to train generative models (so here primarily models that create new images).   * In an adversarial training one model (G  Generator) generates things (e.g. images). Another model (D  discriminator) sees real things (e.g. real images) as well as fake things (e.g. images from G) and has to learn how to differentiate the two.   * Neural Networks are models that can be trained in an adversarial way (and are the only models discussed here).  * How   * G is a simple neural net (e.g. just one fully connected hidden layer). It takes a vector as input (e.g. 100 dimensions) and produces an image as output.   * D is a simple neural net (e.g. just one fully connected hidden layer). It takes an image as input and produces a quality rating as output (0-1  so sigmoid).   * You need a training set of things to be generated  e.g. images of human faces.   * Let the batch size be B.   * G is trained the following way:     * Create B vectors of 100 random values each  e.g. sampled uniformly from [-1  +1]. (Number of values per components depends on the chosen input size of G.)     * Feed forward the vectors through G to create new images.     * Feed forward the images through D to create ratings.     * Use a cross entropy loss on these ratings. All of these (fake) images should be viewed as label=0 by D. If D gives them label=1  the error will be low (G did a good job).     * Perform a backward pass of the errors through D (without training D). That generates gradients/errors per image and pixel.     * Perform a backward pass of these errors through G to train G.   * D is trained the following way:     * Create B/2 images using G (again  B/2 random vectors  feed forward through G).     * Chose B/2 images from the training set. Real images get label=1.     * Merge the fake and real images to one batch. Fake images get label=0.     * Feed forward the batch through D.     * Measure the error using cross entropy.     * Perform a backward pass with the error through D.   * Train G for one batch  then D for one (or more) batches. Sometimes D can be too slow to catch up with D  then you need more iterations of D per batch of G.  * Results   * Good looking images MNIST-numbers and human faces. (Grayscale  rather homogeneous datasets.)   * Not so good looking images of CIFAR-10. (Color  rather heterogeneous datasets.)   ![Generated Faces](images/Generative_Adversarial_Networks__faces.jpg?raw=true ""Generated Faces"")  *Faces generated by MLP GANs. (Rightmost column shows examples from the training set.)*  -------------------------   """;Computer Vision;https://github.com/ashishmurali/Generative-Adversarial-Network
"""[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.   """;Natural Language Processing;https://github.com/romik9999/fasttext-1925f09ed3
"""DeBERTa (Decoding-enhanced BERT with disentangled attention) improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism  where each word is represented using two vectors that encode its content and position  respectively  and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second  an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks.   """;Natural Language Processing;https://github.com/huberemanuel/DeBERTa
"""-------------------------------------------------------------- In order to design a fully autonomous Vehicle the following techniques have been used:          1. Waypoint Following techniques     2. Control     3. Traffic Light Detection and Classification      The Waypoint Following technique would take information from the traffic light detection and classification with the current waypoints in order to update the target velocities for each waypoint based on this information.  For Control part  I have designed a drive-by-wire (dbw) node that could take the target linear and angular velocities and publish commands for the throttle  brake  and steering of the car.   For Traffic Light Detection and classification  I have designed a classification node that would take the current waypoints of the car and an image taken from the car and determine if the closest traffic light was red or green.   ![alt text][image1]   -------------------------------------------------------------- This is the project repo for the final project of the Udacity Self-Driving Car Nanodegree-Capstone Project: Programming a Real Self-Driving Car.    """;General;https://github.com/KarimDahawy/CapStone-Project
"""This model consists of 2 generators and 2 discriminators. The two generators as U-net like CNNs. During the evaluation of the model  I directly used the pretrained salient objective detection model from Joker  https://github.com/Joker316701882/Salient-Object-Detection.  This is a project about image style transfer developed by Tao Liang  Tianrui Yu  Ke Han and Yifan Ruan. Our project contains three different models  one is in ""cycle_gan_unet"" directory which uses the u-net like cnn as generators  one is in ""Ukiyoe_codes"" directory which uses Resnet blocks as generators  which uses the model proposed in this paper https://arxiv.org/pdf/1703.10593.pdf  the other is in neural_style_transfer that implement sytle transfer using convolution neural network proposed in this paper https://arxiv.org/pdf/1508.06576.pdf.   """;Computer Vision;https://github.com/CarpdiemLiang/style_transfer
"""There are numerous other things that I have learned and failed through so far. I went into this semester with barely any knowledge of anything in this realm  and I really feel like I have learned a lot up to now  and am excited to continue to learn more and more.  The first few weeks of my semester were spent trying to learn and understand the mathematics and qualities that modern day style transfer is based off of. I read a few of the dominant papers in the field that discussed the methods.  To summarize  when creating a stylized image  there are three main players. The content photo (c)  the style photo (s)  and the new pistache (p).  At a high level  we want to make (p) as similar in content to (c)  and as similar in style to (s). So how can we define  what ‘content’ and ‘style’ are? [4] found that Convolutional Neural Networks (CNN’s)  when trained for object recognition  extract different types of information at different layers. If you then try to minimize the feature reconstruction for particular layers  you can extract different information. Importantly  minimizing early layers in the CNN seems to capture the texture and color images  whereas  minimizing higher layers in the CNN seem to capture image content and overall spatial structure [5].  There is a lot of math that is involved in reconstructing those layers  and minimizing the differences between images  that I won’t get into here. After the initial paper was published  an important addition was made that enabled the creation of the Android application that I ended up editing. [5] demonstrated a way of not only creating stylized images  but also creating them in real time. To create the pistache in [4]  there was both a forward and backward pass through a pretrained network. To fix the problem of speed  [5] trained a different neural network to quickly approximate solutions to their problem.  Finally  [6] resolved an issue that allowed you to use the same network for N distinct styles instead of 1  thus saving a ton of space. This allowed the Google Codelab to contain so many distinct styles  as well as allow some of the more well known style transfer apps  like Prisma to work.    A pistache is an artistic work that imitates the style of another one.  This git repo is the culumination of half a semester of work for my independent study. To date  I have edited android app  by implemented image-segmented background blurring  and increased luminance matching to a Google Codelab [1]  that focuses on creating pistaches.  After getting to this point  and remembering that this independent study was supposed to be about machine learning and art  and I hadn’t made any art projects  I began to think about what I wanted to make with this unique application.  I quickly realized that my moms birthday was coming up  and that I have historically never gotten her anything. For those who don’t know my mom  I personally believe that she has a manic addiction to her children. That belief is entirely centered around the life she wishes to portray through her Facebook.  Facebook photos tell your story  either intentionally or unintentionally  to everyone that kinda knows you.  Either way  in deciding how to celebrate the birth of my mom  I came to the conclusion that I wanted to tell her story. The one she tells through Facebook that is. It isn’t necessarily the story I would have chosen to portray  but it’s the one she has.  So  I had the pleasure of scouring Janice’s photos to build my collage  and I grew increasingly fearful for my privacy in the process. Regardless  I grabbed about 60 photos from her Facebook (and some others I wanted to throw in)  and created 26 different pistaches with styles ranging from Picasso to Van Gogh  resulting in over 1500 total photos.  Then  I went through the 26 pistaches for each of the 60 images  and chose the one that I thought most truly represented that memory to me. The photo that I think best captures that frozen moment. Thus  trying to portray how I view the world that she is portraying. Seeing her world through my eyes.  Also  for fun  I took all 1500 photos and put it in a super-collage  I call the meg. That photo is 170MB. That image is fucking awesome! But I had to scale down the image so I can have it on github. So the resolution is wack. But I have the real version and its literally the best thing ever.  <p align=""center"">  <img src=""scaled_down_mega_collage.jpg?""> </p>   """;General;https://github.com/chrismgeorge/Artistic_Additions_To_Style_Transfer
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/SpikeKing/My-Bert
"""This repo contains imlementation of the the `Pointer Sentinel Mixture Model`  as described in the [paper](https://arxiv.org/abs/1609.07843) by Stephen Merity et al.  See my [blog post](https://elanmart.github.io/2018-02-10-psmm) for details.  See [model.py](https://github.com/elanmart/psmm/blob/master/psmm/model.py) for the core of this architecture.  """;Sequential;https://github.com/elanmart/psmm
"""SSKD is implemented based on **FastReID v1.0.0**  it provides a semi-supervised feature learning framework to learn domain-general representations. The framework is shown in   <img src=""images/framework.png"" width=""850"" >   """;General;https://github.com/xiaomingzhid/sskd
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/qilei123/DEEPLAB_4_RETINA
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/wayalhruhi/gogle_bert
""" Synapses play an important role in biological neural networks.  They're joint points of neurons where learning and memory happened. The picture below demonstrates that two neurons (red) connected through a branch chain of synapses which may  link to other neurons.   <p align='center'> <img src=""./picture/synapse.jpg"" alt=""synapse"" width=""80%"" /> </p>  Inspired by the synapse research of neuroscience  we construct a simple model that can describe some key properties of a synapse.   <p align='center'> <img src=""./picture/synapse-unit.png"" alt=""synpase"" width=""70%"" />  </p>  A Synaptic Neural Network (SynaNN) contains non-linear synapse networks that connect to neurons. A synapse consists of an input from the excitatory-channel  an input from the inhibitory-channel  and an output channel which sends a value to other synapses or neurons. The synapse function is  <p align='center'> <img src=""https://latex.codecogs.com/svg.latex?S(x y;\alpha \beta)=\alpha%20x(1-\beta%20y)"" </p>  where x∈(0 1) is the open probability of all excitatory channels and α >0 is the parameter of the excitatory channels; y∈(0 1) is the open probability of all inhibitory channels and β∈(0 1) is the parameter of the inhibitory channels. The surface of the synapse function is    <p align='center'> <img src=""./picture/synpase.png"" alt=""synpase"" width=""50%"" /> </p>  By combining deep learning  we expect to build ultra large scale neural networks to solve real-world AI problems. At the same time  we want to create an explainable neural network model to better understand what an AI model doing instead of a black box solution.  <p align='center'> <img src=""./picture/E425.tmp.png"" alt=""synpase"" width=""60%"" /> </p>  A synapse graph is a connection of synapses. In particular  a synapse tensor is fully connected synapses from input neurons to output neurons with some hidden layers. Synapse learning can work with gradient descent and backpropagation algorithms. SynaNN can be applied to construct MLP  CNN  and RNN models.  Assume that the total number of input of the synapse graph equals the total number of outputs  the fully-connected synapse graph is defined as   <p align='center'> <img src=""https://latex.codecogs.com/svg.latex?y_{i}(\textbf{x};%20\pmb\beta_i)%20=%20\alpha_i%20x_{i}{\prod_{j=1}^{n}(1-\beta_{ij}x_{j})} \%20for\%20all\%20i%20\in%20[1 n]""/> </p>  where   <p align='center'> <img src=""https://latex.codecogs.com/svg.latex?\textbf{x}=(x_1 \cdots x_n) \textbf{y}=(y_1 \cdots y_n) x_i y_i\in(0 1) \alpha_i \geq 1 \beta_{ij}\in(0 1))""/> </p>  Transformed to tensor/matrix representation  we have the synapse log formula    <p align='center'> <img src=""https://latex.codecogs.com/svg.latex?log(\textbf{y})=log(\textbf{x})+{\textbf{1}_{|x|}}*log(\textbf{1}_{|\beta|}-diag(\textbf{x})*\pmb{\beta}^T)""/> </p>  We are going to implement this formula for fully-connected synapse network with Tensorflow and PyTorch in the examples.  Moreover  we can design synapse graph like circuit below for some special applications.   <p align='center'> <img src=""./picture/synapse-flip.png"" alt=""synapse-flip"" width=""50%"" /> </p>   """;General;https://github.com/Neatware/SynaNN
"""RetinaFace is a practical single-stage [SOTA](http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html) face detector which is initially described in [arXiv technical report](https://arxiv.org/abs/1905.00641)  ![demoimg1](https://github.com/deepinsight/insightface/blob/master/resources/11513D05.jpg)  ![demoimg2](https://github.com/deepinsight/insightface/blob/master/resources/widerfacevaltest.png)   """;General;https://github.com/1996scarlet/ArcFace-Multiplex-Recognition
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/lehoanganh298/BERT-Question-Answering
"""``` Layer (type)                 Output Shape              Param #:    ================================================================= input_word_ids (InputLayer)  [(None  1500)]            0          _________________________________________________________________ tf_bert_model_1 (TFBertModel ((None  1500  768)  (None 109482240))  _________________________________________________________________ tf_op_layer_strided_slice_1  [(None  768)]             0          _________________________________________________________________ dense_1 (Dense)              (None  16)                12304      ================================================================= Total params: 109 494 544 Trainable params: 109 494 544 Non-trainable params: 0 ```   """;Natural Language Processing;https://github.com/MLH-Fellowship/Social-BERTerfly
"""The master branch works with **PyTorch 1.1** or higher.  mmdetection is an open source object detection toolbox based on PyTorch. It is a part of the open-mmlab project developed by [Multimedia Laboratory  CUHK](http://mmlab.ie.cuhk.edu.hk/).  ![demo image](demo/coco_test_12510.jpg)   """;Computer Vision;https://github.com/OceanPang/Libra_R-CNN
"""R-CNN is a state-of-the-art visual object detection system that combines bottom-up region proposals with rich features computed by a convolutional neural network. At the time of its release  R-CNN improved the previous best detection performance on PASCAL VOC 2012 by 30% relative  going from 40.9% to 53.3% mean average precision. Unlike the previous best results  R-CNN achieves this performance without using contextual rescoring or an ensemble of feature types.  R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1311.2524) and will appear in a forthcoming CVPR 2014 paper.   """;Computer Vision;https://github.com/rbgirshick/rcnn
"""Question-answering problem is currenlty one of the most chellenging task in Natural Language Processing domain. In purpose to solve it transfer learning is state of the art method. Thanks to huggingface-transformers which made avaiable pretrained NLP most advanced models (like: BERT   GPT-2  XLNet  RoBERTa  DistilBERT) relatively easy to be used in different language tasks.   Original [akensert](https://www.kaggle.com/akensert/quest-bert-base-tf2-0) code was tested with different parameters and changed base models. From both implemented (XLNet  RoBERTa) the second one resulted in better score. Further improvement ccould be made by implementation of combined model version. For example it could consists of BERT  RoBERTa and XLNet.   Change of main algorithm from BERT to RoBERTa was justified by the fact that second one is an improved version of the first one. The expansion of the algorithm name is Robustly Optimized BERT Pretraining Approach  it modifications consists of [5]: - training the model longer  with bigger batches  over more data;  - removing the next sentence prediction objective;  - training on longer sequences;  - dynamically changing the masking pattern applied to the training data.  Use of RoBERTa consequently causes the need of configuration change and implementation of RoBERTa sepcific tokenizer. It constructs a RoBERTa BPE tokenizer  derived from the GPT-2 tokenizer  using byte-level Byte-Pair-Encoding. Which works in that order: 1. Prepare a large enough training data (i.e. corpus) 2. Define a desired subword vocabulary size 3. Split word to sequence of characters and appending suffix “</w>” to end of word with word frequency. So the basic unit is character in this stage. For example  the frequency of “low” is 5  then we rephrase it to “l o w </w>”: 5 4. Generating a new subword according to the high frequency occurrence. 5. Repeating step 4 until reaching subword vocabulary size which is defined in step 2 or the next highest frequency pair is 1.  Values of the tuning parameters (folds  epochs  batch_size) was mostly implicated by the kaggle GPU power and competition constrain of kernel computation limitation to 2 hours run-time.  Final step was calculation of predicitons taking into acount results averaged results for folds. Weights have been assigned by empricialy tring different values. The change of particular ones was based on the prediction score. Limitation was only the summing up of weights to one. Change of arithmetic mean of folds predictions to weighted average improved results in public leaderboard from 0.38459 to 0.38798. On the other hand as the final scores on the private leaderboard showed it was not good choice. Finally  soo strictly assignment of weights caused the decrease in final result from 0.36925 to 0.36724.  XLNet was also tested (to show it  the code with it was left commented). In theory XLNet should overcome BERT limitations. Relying on corrupting the input with masks  BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons XLNet  which is characterised by: - learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order; - overcomes the limitations of BERT thanks to its autoregressive formulation; - integrates ideas from Transformer-XL  the state-of-the-art autoregressive model  into pretraining. Empirically  under comparable experiment settings  XLNet outperforms BERT on 20 tasks  often by a large margin  including question answering  natural language inference  sentiment analysis  and document ranking [6].    After all public score for XLNet version was lower (0.36310) than score (0.37886) for base BERT model and was rejected.     ""In this competition  you’re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites  in a ""common-sense"" fashion. Our raters received minimal guidance and training  and relied largely on their subjective interpretation of the prompts. As such  each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines  we hope to increase the re-use value of this data set.""[1]     """;General;https://github.com/bluejurand/Kaggle_QA_Google_Labeling
"""WaveNet replication study. Before stepping up to WaveNet implementation it was decided to implement PixelCNN first as WaveNet based on its architecture.  This repository contains two modes: [Gated PixelCNN][pixelcnn-paper] and [WaveNet][wavenet-paper]  see class definitions in `wavenet/models.py`.  For detailed explanation of how these model work see [my blog post](http://sergeiturukin.com/2017/02/22/pixelcnn.html).   """;Audio;https://github.com/rampage644/wavenet
"""The goal of PySlowFast is to provide a high-performance  light-weight pytorch codebase provides state-of-the-art video backbones for video understanding research on different tasks (classification  detection  and etc). It is designed in order to support rapid implementation and evaluation of novel video research ideas. PySlowFast includes implementations of the following backbone network architectures:  - SlowFast - Slow - C2D - I3D - Non-local Network - X3D   """;Computer Vision;https://github.com/facebookresearch/SlowFast
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/zengzhaoyang/trident
"""***Note***: there is now a PyTorch version of this toolkit ([fairseq-py](https://github.com/pytorch/fairseq)) and new development efforts will focus on it. The Lua version is preserved here  but is provided without any support.  This is fairseq  a sequence-to-sequence learning toolkit for [Torch](http://torch.ch/) from Facebook AI Research tailored to Neural Machine Translation (NMT). It implements the convolutional NMT models proposed in [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) and [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344) as well as a standard LSTM-based model. It features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU. We provide pre-trained models for English to French  English to German and English to Romanian translation.  ![Model](fairseq.gif)   """;Natural Language Processing;https://github.com/facebookresearch/fairseq
"""**RepPoints**  initially described in [arXiv](https://arxiv.org/abs/1904.11490)  is a new representation method for visual objects  on which visual understanding tasks are typically centered. Visual object representation  aiming at both geometric description and appearance feature extraction  is conventionally achieved by `bounding box + RoIPool (RoIAlign)`. The bounding box representation is convenient to use; however  it provides only a rectangular localization of objects that lacks geometric precision and may consequently degrade feature quality. Our new representation  RepPoints  models objects by a `point set` instead of a `bounding box`  which learns to adaptively position themselves over an object in a manner that circumscribes the object’s `spatial extent` and enables `semantically aligned feature extraction`. This richer and more flexible representation maintains the convenience of bounding boxes while facilitating various visual understanding applications. This repo demonstrated the effectiveness of RepPoints for COCO object detection.  Another feature of this repo is the demonstration of an `anchor-free detector`  which can be as effective as state-of-the-art anchor-based detection methods. The anchor-free detector can utilize either `bounding box` or `RepPoints` as the basic object representation.  <div align=""center"">   <img src=""demo/reppoints.png"" width=""400px"" />   <p>Learning RepPoints in Object Detection.</p> </div>   """;Computer Vision;https://github.com/microsoft/RepPoints
"""The goal of Detectron is to provide a high-quality  high-performance codebase for object detection *research*. It is designed to be flexible in order to support rapid implementation and evaluation of novel research. Detectron includes implementations of the following object detection algorithms:  - [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017* - [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017* - [Faster R-CNN](https://arxiv.org/abs/1506.01497) - [RPN](https://arxiv.org/abs/1506.01497) - [Fast R-CNN](https://arxiv.org/abs/1504.08083) - [R-FCN](https://arxiv.org/abs/1605.06409)  using the following backbone network architectures:  - [ResNeXt{50 101 152}](https://arxiv.org/abs/1611.05431) - [ResNet{50 101 152}](https://arxiv.org/abs/1512.03385) - [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt) - [VGG16](https://arxiv.org/abs/1409.1556)  Additional backbone architectures may be easily implemented. For more details about these models  please see [References](#references) below.   """;Computer Vision;https://github.com/jiajunhua/facebookresearch-Detectron
"""Intention Classifier is a module that analyzes the intention of the user’s utterance. This module modifies and combines “bi-RNN” and “Attention mechanism” to implement an Intention classification model.   - 2.1 Maintainer status: maintained - 2.2 Maintainer: Yuri Kim  [yurikim@hanyang.ac.kr]() - 2.3 Author: Yuri Kim  [yurikim@hanyang.ac.kr]() - 2.4 License (optional):  - 2.5 Source git: https://github.com/DeepTaskHY/DM_Intent   """;Natural Language Processing;https://github.com/DeepTaskHY/DM_Intent
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/anhnktp/yolov3
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/flyliu2017/bert
"""Requires a training set of summaries that are not a headline. There is a wikipedia dataset that may be useful  https://github.com/tscheepers/Wikipedia-Summary-Dataset  part of this https://github.com/tscheepers/CompVec   """;Natural Language Processing;https://github.com/mlennox/summarisers
"""This method aims at helping computer vision practitioners faced with an overfit problem. The idea is to replace  in a 3-branch ResNet  the standard summation of residual branches by a stochastic affine combination. The largest tested model improves on the best single shot published result on CIFAR-10 by reaching 2.72% test error.  ![shake-shake](https://s3.eu-central-1.amazonaws.com/github-xg/architecture3.png)  Figure 1: **Left:** Forward training pass. **Center:** Backward training pass. **Right:** At test time.   """;General;https://github.com/loshchil/AdamW-and-SGDW
"""------------ Reimplementation of [Soft Actor-Critic Algorithms and Applications](https://arxiv.org/pdf/1812.05905.pdf) and a deterministic variant of SAC from [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/pdf/1801.01290.pdf).  Added another branch for [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/pdf/1801.01290.pdf) -> [SAC_V](https://github.com/pranz24/pytorch-soft-actor-critic/tree/SAC_V).   """;Reinforcement Learning;https://github.com/pranz24/pytorch-soft-actor-critic
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/chalothon/BERT_Practice
"""This project extends the original [Google DeViSE](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf) paper to create a functioning image search engine with a focus on interpreting search results. We have extended the original paper in the following ways. First  we added an RNN to process variable length queries as opposed to single words. Next  to understand how the network responds to different parts of the query(like noun phrases) and the image  we leverage [Ribeiro et.al's LIME](https://arxiv.org/pdf/1602.04938v1.pdf) for model-agnostic interpretability. It has been tested on subsets of the [UIUC-PASCAL dataset](http://vision.cs.uiuc.edu/pascal-sentences/) and the final network has been trained on the [MSCOCO 2014 dataset](http://cocodataset.org/#home).   """;General;https://github.com/priyamtejaswin/devise-keras
"""Existing video polyp segmentation (VPS) models typically employ convolutional neural networks (CNNs) to extract features.  However  due to their limited receptive fields  CNNs can not fully exploit the global temporal and spatial information in successive video frames  resulting in false-positive segmentation results.  In this paper  we propose the novel PNS-Net (Progressively Normalized Self-attention Network)  which can efficiently learn representations from polyp videos with real-time speed (~140fps) on a single RTX 2080 GPU and no post-processing.   Our PNS-Net is based solely on a basic normalized self-attention block  dispensing with recurrence and CNNs entirely. Experiments on challenging VPS datasets demonstrate that the proposed PNS-Net achieves state-of-the-art performance.  We also conduct extensive experiments to study the effectiveness of the channel split  soft-attention  and progressive learning strategy.  We find that our PNS-Net works well under different settings  making it a promising solution to the VPS task.   """;Computer Vision;https://github.com/GewelsJI/PNS-Net
"""Recent research has shown that numerous human-interpretable directions exist in the latent space of GANs. In this paper  we develop an automatic procedure for finding directions that lead to foreground-background image separation  and we use these directions to train an image segmentation model without human supervision. Our method is generator-agnostic  producing strong segmentation results with a wide range of different GAN architectures. Furthermore  by leveraging GANs pretrained on large datasets such as ImageNet  we are able to segment images from a range of domains without further training or finetuning. Evaluating our method on image segmentation benchmarks  we compare favorably to prior work while using neither human supervision nor access to the training data. Broadly  our results demonstrate that automatically extracting foreground-background structure from pretrained deep generative models can serve as a remarkably effective substitute for human supervision.    """;Computer Vision;https://github.com/lukemelas/unsupervised-image-segmentation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/cuber2460/bert
"""Traditional [WGAN](https://arxiv.org/abs/1701.07875) uses an approximation of the Wasserstein metric to opimize the generator. This Wasserstein metric in turn depends upon an underlying metric on _images_ which is taken to be the <img src=""https://latex.codecogs.com/svg.latex?%5Cell%5E2""> norm  <img src=""https://latex.codecogs.com/svg.latex?%5C%7Cx%5C%7C_%7B2%7D%20%3D%20%5Cleft%28%20%5Csum_%7Bi%3D1%7D%5En%20x_i%5E2%20%5Cright%29%5E%7B1/2%7D"">  The article extends the theory of [WGAN-GP](https://arxiv.org/abs/1704.00028) to any [Banach space](https://en.wikipedia.org/wiki/Banach_space)  while this code can be used to train WGAN over any [Sobolev space](https://en.wikipedia.org/wiki/Sobolev_space) <img src=""https://latex.codecogs.com/svg.latex?W%5E%7Bs%2C%20p%7D""> with norm  <img src=""https://latex.codecogs.com/svg.latex?%5C%7Cf%5C%7C_%7BW%5E%7Bs%2C%20p%7D%7D%20%3D%20%5Cleft%28%20%5Cint_%7B%5COmega%7D%20%5Cleft%28%20%5Cmathcal%7BF%7D%5E%7B-1%7D%20%5Cleft%5B%20%281%20&plus;%20%7C%5Cxi%7C%5E2%29%5E%7Bs/2%7D%20%5Cmathcal%7BF%7D%20f%20%5Cright%5D%28x%29%20%5Cright%29%5Ep%20dx%20%5Cright%29%5E%7B1/p%7D"">  The parameters _p_ can be used to control the focus on outliers  with high _p_ indicating a strong focus on the worst offenders. _s_ can be used to control focus on small/large scale behaviour  where negative _s_ indicates focus on large scales  while positive _s_ indicates focus on small scales (e.g. edges).   """;General;https://github.com/adler-j/bwgan
"""We provided following boundaries in folder `boundaries/`. The boundaries can be more accurate if stronger attribute predictor is used.  - ProgressiveGAN model trained on CelebA-HQ dataset:   - Single boundary:     - `pggan_celebahq_pose_boundary.npy`: Pose.     - `pggan_celebahq_smile_boundary.npy`: Smile (expression).     - `pggan_celebahq_age_boundary.npy`: Age.     - `pggan_celebahq_gender_boundary.npy`: Gender.     - `pggan_celebahq_eyeglasses_boundary.npy`: Eyeglasses.     - `pggan_celebahq_quality_boundary.npy`: Image quality.   - Conditional boundary:     - `pggan_celebahq_age_c_gender_boundary.npy`: Age (conditioned on gender).     - `pggan_celebahq_age_c_eyeglasses_boundary.npy`: Age (conditioned on eyeglasses).     - `pggan_celebahq_age_c_gender_eyeglasses_boundary.npy`: Age (conditioned on gender and eyeglasses).     - `pggan_celebahq_gender_c_age_boundary.npy`: Gender (conditioned on age).     - `pggan_celebahq_gender_c_eyeglasses_boundary.npy`: Gender (conditioned on eyeglasses).     - `pggan_celebahq_gender_c_age_eyeglasses_boundary.npy`: Gender (conditioned on age and eyeglasses).     - `pggan_celebahq_eyeglasses_c_age_boundary.npy`: Eyeglasses (conditioned on age).     - `pggan_celebahq_eyeglasses_c_gender_boundary.npy`: Eyeglasses (conditioned on gender).     - `pggan_celebahq_eyeglasses_c_age_gender_boundary.npy`: Eyeglasses (conditioned on age and gender). - StyleGAN model trained on CelebA-HQ dataset:   - Single boundary in $\mathcal{Z}$ space:     - `stylegan_celebahq_pose_boundary.npy`: Pose.     - `stylegan_celebahq_smile_boundary.npy`: Smile (expression).     - `stylegan_celebahq_age_boundary.npy`: Age.     - `stylegan_celebahq_gender_boundary.npy`: Gender.     - `stylegan_celebahq_eyeglasses_boundary.npy`: Eyeglasses.   - Single boundary in $\mathcal{W}$ space:     - `stylegan_celebahq_pose_w_boundary.npy`: Pose.     - `stylegan_celebahq_smile_w_boundary.npy`: Smile (expression).     - `stylegan_celebahq_age_w_boundary.npy`: Age.     - `stylegan_celebahq_gender_w_boundary.npy`: Gender.     - `stylegan_celebahq_eyeglasses_w_boundary.npy`: Eyeglasses.  - StyleGAN model trained on FF-HQ dataset:   - Single boundary in $\mathcal{Z}$ space:     - `stylegan_ffhq_pose_boundary.npy`: Pose.     - `stylegan_ffhq_smile_boundary.npy`: Smile (expression).     - `stylegan_ffhq_age_boundary.npy`: Age.     - `stylegan_ffhq_gender_boundary.npy`: Gender.     - `stylegan_ffhq_eyeglasses_boundary.npy`: Eyeglasses.   - Conditional boundary in $\mathcal{Z}$ space:     - `stylegan_ffhq_age_c_gender_boundary.npy`: Age (conditioned on gender).     - `stylegan_ffhq_age_c_eyeglasses_boundary.npy`: Age (conditioned on eyeglasses).     - `stylegan_ffhq_eyeglasses_c_age_boundary.npy`: Eyeglasses (conditioned on age).     - `stylegan_ffhq_eyeglasses_c_gender_boundary.npy`: Eyeglasses (conditioned on gender).   - Single boundary in $\mathcal{W}$ space:     - `stylegan_ffhq_pose_w_boundary.npy`: Pose.     - `stylegan_ffhq_smile_w_boundary.npy`: Smile (expression).     - `stylegan_ffhq_age_w_boundary.npy`: Age.     - `stylegan_ffhq_gender_w_boundary.npy`: Gender.     - `stylegan_ffhq_eyeglasses_w_boundary.npy`: Eyeglasses.   """;Computer Vision;https://github.com/genforce/interfacegan
"""[_CrazyAra_](https://crazyara.org/) is an open-source neural network chess variant engine  initially developed in pure python by [Johannes Czech](https://github.com/QueensGambit)  [Moritz Willig](https://github.com/MoritzWillig) and Alena Beyer in 2018. It started as a semester project at the [TU Darmstadt](https://www.tu-darmstadt.de/index.en.jsp) with the goal to train a neural network to play the chess variant [crazyhouse](https://en.wikipedia.org/wiki/Crazyhouse) via supervised learning on human data. The project was part of the course [_""Deep Learning: Architectures & Methods""_](https://piazza.com/tu-darmstadt.de/summer2019/20001034iv/home) held by [Kristian Kersting](https://ml-research.github.io/people/kkersting/index.html)  [Johannes Fürnkranz](http://www.ke.tu-darmstadt.de/staff/juffi) et al. in summer 2018.  The development was continued and the engine ported to C++ by [Johannes Czech](https://github.com/QueensGambit). In the course of a master thesis supervised by [Karl Stelzner](https://ml-research.github.io/people/kstelzner/) and [Kristian Kersting](https://ml-research.github.io/people/kkersting/index.html)  the engine learned crazyhouse in a reinforcement learning setting and was trained on other chess variants including chess960  King of the Hill and Three-Check.  The project is mainly inspired by the techniques described in the [Alpha-(Go)-Zero papers](https://arxiv.org/abs/1712.01815) by [David Silver](https://arxiv.org/search/cs?searchtype=author&query=Silver%2C+D)  [Thomas Hubert](https://arxiv.org/search/cs?searchtype=author&query=Hubert%2C+T)  [Julian Schrittwieser](https://arxiv.org/search/cs?searchtype=author&query=Schrittwieser%2C+J)  [Ioannis Antonoglou](https://arxiv.org/search/cs?searchtype=author&query=Antonoglou%2C+I)  [Matthew Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+M)  [Arthur Guez](https://arxiv.org/search/cs?searchtype=author&query=Guez%2C+A)  [Marc Lanctot](https://arxiv.org/search/cs?searchtype=author&query=Lanctot%2C+M)  [Laurent Sifre](https://arxiv.org/search/cs?searchtype=author&query=Sifre%2C+L)  [Dharshan Kumaran](https://arxiv.org/search/cs?searchtype=author&query=Kumaran%2C+D)  [Thore Graepel](https://arxiv.org/search/cs?searchtype=author&query=Graepel%2C+T)  [Timothy Lillicrap](https://arxiv.org/search/cs?searchtype=author&query=Lillicrap%2C+T)  [Karen Simonyan](https://arxiv.org/search/cs?searchtype=author&query=Simonyan%2C+K)  [Demis Hassabis](https://arxiv.org/search/cs?searchtype=author&query=Hassabis%2C+D).  The training scripts  preprocessing and neural network definition source files are written in python and located at [DeepCrazyhouse/src](https://github.com/QueensGambit/CrazyAra/tree/master/DeepCrazyhouse/src). There are two version of the search engine available: The initial version is written in python and located at [DeepCrazyhouse/src/domain/agent](https://github.com/QueensGambit/CrazyAra/tree/master/DeepCrazyhouse/src/domain/agent). The newer version is written in C++ and located at [engine/src](https://github.com/QueensGambit/CrazyAra/tree/master/engine/src).  _CrazyAra_ is an UCI chess engine and requires a GUI (e.g. [Cute Chess](https://github.com/cutechess/cutechess)  [XBoard](https://www.gnu.org/software/xboard/)  [WinBoard](http://hgm.nubati.net/)) for convinient usage.   """;Reinforcement Learning;https://github.com/QueensGambit/CrazyAra
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Gaozhen0816/BERT_QA_for_Chinese
"""Remote sensing offers the unique opportunity to monitor the evolution of human settlements from space.   Using multi-spectral images from Landsat 5  Landsat 7  Landsat 8 along with nighttime images from DMSP-OLS and NPP-VIIRS  this project aims to:  - Build a model quantifying the footprint of cities and monitor their evolution over time. - Use the model to provide an empirical validation of the scaling law in the city size distribution  at difference scales (regional level  country level  worldwide).  Yearly images between 1992 to 2020 of Las Vegas  Nevada - USA   |RGB composites             |  Nighttime Lights        | |:-------------------------:|:-------------------------:| ![Alt Text](./data/demo/las_vegas_area_rgb.gif) |  ![Alt Text](./data/demo/las_vegas_area_nl.gif) |Image segmentation             |  Segmentation legend    | |![Alt Text](./data/demo/las_vegas_area_preds.gif) | ![Alt Text](./data/demo/legend.png) |   """;Computer Vision;https://github.com/badrbmb/cities-watch
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/qilei123/DeformableConvV2
"""Deep Convolutional GAN is one of the most coolest and popular deep learning technique. It is a great improvement upon the [original GAN network](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) that was first introduced by Ian Goodfellow at NIPS 2014. (DCGANs are much more stable than Vanilla GANs) DCGAN uses the same framework of generator and discriminator. This is analogous to solving a two player minimax game: Ideally the goal of the discriminator is to be very sharp in distinguishing between the real and fake data  whereas  generator aims at faking data in such a way that it becomes nearly impossible for the discriminator to classify it as a fake. The below gif shows how quickly dcgan learns the distribution of celebrity images and generates real looking people. The gif is created for both  a fixed noise and variable noise:-  <p float=""left"">   <img src=""https://github.com/AKASHKADEL/dcgan-celeba/blob/master/results/variable_noise/animated.gif"" width=""400"" height=""400"" />   <img src=""https://github.com/AKASHKADEL/dcgan-celeba/blob/master/results/fixed_noise/animated.gif"" width=""400"" height=""400"" /> </p>   """;Computer Vision;https://github.com/AKASHKADEL/dcgan-celeba
"""<div align=""left"">   <img src=""https://insightface.ai/assets/img/custom/thumb_sdunet.png"" width=""600""/> </div>  In this module  we provide datasets and training/inference pipelines for face alignment.  Supported methods:  - [x] [SDUNets (BMVC'2018)](alignment/heatmap) - [x] [SimpleRegression](alignment/coordinate_reg)   [SDUNets](alignment/heatmap) is a heatmap based method which accepted on [BMVC](http://bmvc2018.org/contents/papers/0051.pdf).  [SimpleRegression](alignment/coordinate_reg) provides very lightweight facial landmark models with fast coordinate regression. The input of these models is loose cropped face image while the output is the direct landmark coordinates.    <div align=""left"">   <img src=""https://insightface.ai/assets/img/github/11513D05.jpg"" width=""640""/> </div>  In this module  we provide training data with annotation  network settings and loss designs for face detection training  evaluation and inference.  The supported methods are as follows:  - [x] [RetinaFace (CVPR'2020)](detection/retinaface) - [x] [SCRFD (Arxiv'2021)](detection/scrfd) - [x] [blazeface_paddle](detection/blazeface_paddle)  [RetinaFace](detection/retinaface) is a practical single-stage face detector which is accepted by [CVPR 2020](https://openaccess.thecvf.com/content_CVPR_2020/html/Deng_RetinaFace_Single-Shot_Multi-Level_Face_Localisation_in_the_Wild_CVPR_2020_paper.html). We provide training code  training dataset  pretrained models and evaluation scripts.   [SCRFD](detection/scrfd) is an efficient high accuracy face detection approach which is initialy described in [Arxiv](https://arxiv.org/abs/2105.04714). We provide an easy-to-use pipeline to train high efficiency face detectors with NAS supporting.    In this module  we provide training data  network settings and loss designs for deep face recognition.  The supported methods are as follows:  - [x] [ArcFace_mxnet (CVPR'2019)](recognition/arcface_mxnet) - [x] [ArcFace_torch (CVPR'2019)](recognition/arcface_torch) - [x] [SubCenter ArcFace (ECCV'2020)](recognition/subcenter_arcface) - [x] [PartialFC_mxnet (Arxiv'2020)](recognition/partial_fc) - [x] [PartialFC_torch (Arxiv'2020)](recognition/arcface_torch) - [x] [VPL (CVPR'2021)](recognition/vpl) - [x] [OneFlow_face](recognition/oneflow_face) - [x] [ArcFace_Paddle (CVPR'2019)](recognition/arcface_paddle)  Commonly used network backbones are included in most of the methods  such as IResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  etc..    [InsightFace](https://insightface.ai) is an open source 2D&3D deep face analysis toolbox  mainly based on PyTorch and MXNet.   Please check our [website](https://insightface.ai) for detail.  The master branch works with **PyTorch 1.6+** and/or **MXNet=1.6-1.8**  with **Python 3.x**.  InsightFace efficiently implements a rich variety of state of the art algorithms of face recognition  face detection and face alignment  which optimized for both training and deployment.   """;General;https://github.com/deepinsight/insightface
"""    This program performs segmentation of the left vertricle  myocardium  right ventricle     and backgound of Cardiovascular Magnetic Resonance Images  with use of a convolutional neural network based on the well-known U-Net     architecture  as described by [https://arxiv.org/pdf/1505.04597.pdf](Ronneberger et al.) For each patient  both a 3D end systolic       image and a 3D end diastolic image with its corresponding ground truth segmentation of the left ventricle  myocardium and right         ventricle is available.           The available code first divides the patients data into a training set and a test set. The training data is then loaded from the        stored location and subsequently preprocessed. Preprocessing steps include resampling the image to the same voxel spacing  removal of outliers  normalization  cropping and one-hot encoding of the labels. Before training  the trainingset is subdivided again for training and validation of the model.          For training  a network based on the U-Net architecture is used and implemented with keras. For training  many different variables       can be tweaked  which are described in some detail below. After training  the network is evaluated using the test dataset. This data is loaded and preprocessed in the same way as the training dataset and propagated through the network to obtain pixel-wise predictions for each class. These predictions are probabilities and are thresholded to obtain a binary segmentation.           The binary segmentations are then evaluated by computing the (multiclass) softdice coefficient and the Hausdorff distance between the obtained segmentations and the ground truth segmentations. The softdice coefficients and Hausdorff distances are computed for each image for each individual class and the multiclass softdice for all the classes together. These results are all automatically saved in a text file. Furthermore  the obtained segmentations as an overlay with the original images  the training log and corresponding plots and the model summary are also saved automatically.         Lastly  from the segmentations of the left ventricular cavity during the end systole and end diastole  the ejection fraction is calculated. This value is  alongside the ejection fraction computed from the ground truth segmentations  stored in the same text file with results.       """;Computer Vision;https://github.com/jellevankerk/Team-Challenge
"""We present 1-dimensional (1D) convolutional neural networks (CNN) for the classification of soil texture based on hyperspectral data. The following CNN models are included:  * `LucasCNN` * `LucasResNet` * `LucasCoordConv` * `HuEtAl`: 1D CNN by Hu et al. (2015)  DOI: [10.1155/2015/258619](http://dx.doi.org/10.1155/2015/258619) * `LiuEtAl`: 1D CNN by Liu et al. (2018)  DOI: [10.3390/s18093169](https://dx.doi.org/10.3390%2Fs18093169)  These 1D CNNs are optimized for the soil texture classification based on the hyperspectral data of the *Land Use/Cover Area Frame Survey* (LUCAS) topsoil dataset. It is available [here](https://esdac.jrc.ec.europa.eu/projects/lucas). For more information have a look in our publication (see below).  **Introducing paper:** [arXiv:1901.04846](https://arxiv.org/abs/1901.04846)  **Licence:** [MIT](LICENSE)  **Authors:**  * [Felix M. Riese](mailto:felix.riese@kit.edu) * [Sina Keller](mailto:sina.keller@kit.edu)  **Citation of the code and the paper:** see [below](#citation) and in the [bibtex](bibliography.bib) file   """;Computer Vision;https://github.com/felixriese/CNN-SoilTextureClassification
"""The paper Image-to-Image Translation with Conditional Adversarial Networks (https://arxiv.org/pdf/1611.07004.pdf) showed that a general purpose solution could be made for image-to-image translation. Since the time that this paper was published  multiple artists and researchers have made their own models and experiments  with stunning results. These models range from creating cats out of drawings to creating videos of the sea by using an input from general household appliances.<br> The objective of the pix2pix model is to find a model that can map one picture to a desired paired image  which is indistinguishable from the real thing. An example is shown in Figure 1  where 4 different models attempt the mapping from the pixelated image to the real image. Pix2pix uses Conditional Generative Adversarial Networks to achieve this objective. Conditional means that the loss here is structured  there exists a conditional dependency between the pixels  meaning that the loss of one pixel is influenced by the loss of another. The loss function that is used by the model is shown in Equation 1.  <p align=""center"">   <img src=""/ImagesInText/LossFunction.png"" width=""60%"" height=""60%""><br>   Equation 1 [P. Isola et al. (https://arxiv.org/pdf/1611.07004.pdf)] </p>  The model can then be trained and results evaluated  which has been done for a great variety of experiments. We wanted to see if another application could be made  that of image restoration of blurry pictures. We have most likely all seen a movie or tv-series in which a spy agency needed someone to ""enhance"" a photo in order to see smaller details  with the advent of deep learning these techniques are becoming more of a reality. We wanted to see if this general architecture of pix2pix for image translation could also be used for this application to see if you could enhance your images by using pix2pix.  This blog first starts with the method of our project  what exactly is the type of data that we investigate and in what type of datasets they are stored. This is followed up by an explanation about the hyperparameter tuning that was performed and why this was important. The last part of the method is how we would evaluate our results. The method is followed up by our experiments  which also gives a sample of the data that we used as well as the result of some of our experiments  these results are then discussed in our discussion as well with our conclusion about the experiment if pix2pix can be used for image restoration.   """;Computer Vision;https://github.com/PieterBijl/Group28
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/Lw510107/pointnet-2018.6.27-
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/nmfisher/bert-modified
"""SWA is a simple DNN training method that can be used as a drop-in replacement for SGD with improved generalization  faster convergence  and essentially no overhead. The key idea of SWA is to average multiple samples produced by SGD with a modified learning rate schedule. We use a constant or cyclical learning rate schedule that causes SGD to _explore_ the set of points in the weight space corresponding to high-performing networks. We observe that SWA converges more quickly than SGD  and to wider optima that provide higher test accuracy.   In this repo we implement the constant learning rate schedule that we found to be most practical on CIFAR datasets.  <p align=""center"">   <img src=""https://user-images.githubusercontent.com/14368801/37633888-89fdc05a-2bca-11e8-88aa-dd3661a44c3f.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633885-89d809a0-2bca-11e8-8d57-3bd78734cea3.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633887-89e93784-2bca-11e8-9d71-a385ea72ff7c.png"" width=250> </p>  Please cite our work if you find this approach useful in your research: ```bibtex @article{izmailov2018averaging    title={Averaging Weights Leads to Wider Optima and Better Generalization}    author={Izmailov  Pavel and Podoprikhin  Dmitrii and Garipov  Timur and Vetrov  Dmitry and Wilson  Andrew Gordon}    journal={arXiv preprint arXiv:1803.05407}    year={2018} } ```    """;General;https://github.com/timgaripov/swa
"""Work in progress       """;Graphs;https://github.com/WiktorJ/msnode2vec
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Gaozhen0816/BERT_QA_for_Chinese
"""This repository contains the original modified residual network which modified the classical resnet ""Deep Residual Learning for Image Recognition"" (http://arxiv.org/abs/1512.03385). Original residual block was modified to improve model performance.    """;Computer Vision;https://github.com/xinkuansong/modified-resnet-acc-0.9638-10.7M-parameters
"""We have two networks  G (Generator) and D (Discriminator).The Generator is a network for generating images.  It receives a random noise z and generates images from this noise  which is called G(z).Discriminator is  a discriminant network that discriminates whether an image is real. The input is x  x is a picture   and the output is D of x is the probability that x is a real picture  and if it's 1  it's 100% real   and if it's 0  it's not real.   """;Computer Vision;https://github.com/HyeongJu916/Boaz-SR-ESRGAN-PyTorch
"""Official Implementation of our pSp paper for both training and evaluation. The pSp method extends the StyleGAN model to  allow solving different image-to-image translation problems using its encoder.   """;Computer Vision;https://github.com/liuliuliu11/pixel2style2pixel-liu
"""A general YOLOv4/v3/v2 object detection pipeline inherited from [keras-yolo3-Mobilenet](https://github.com/Adamdad/keras-YOLOv3-mobilenet)/[keras-yolo3](https://github.com/qqwweee/keras-yolo3) and [YAD2K](https://github.com/allanzelener/YAD2K). Implement with tf.keras  including data collection/annotation  model training/tuning  model evaluation and on device deployment. Support different architecture and different technologies:   """;Computer Vision;https://github.com/symoon94/YOLO-keras
"""Augmented Implementation of the [pSp implementation](https://github.com/eladrich/pixel2style2pixel) to train Images on white noise reduction   """;Computer Vision;https://github.com/rahuls02/Image-Noise-Reduction
"""The project is an implementation of Wasserstein GAN in Tensorflow 2.0.  Paper link: https://arxiv.org/abs/1701.07875   """;Computer Vision;https://github.com/WangZesen/WGAN-GP-Tensorflow-v2
"""[rife-ncnn-vulkan](https://github.com/nihui/rife-ncnn-vulkan) is nihui's ncnn implementation of Real-World Super-Resolution via Kernel Estimation and Noise Injection super resolution.  rife-ncnn-vulkan-python wraps [rife-ncnn-vulkan project](https://github.com/nihui/rife-ncnn-vulkan) by SWIG to make it easier to integrate rife-ncnn-vulkan with existing python projects.   """;Computer Vision;https://github.com/media2x/rife-ncnn-vulkan-python
"""In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   """;Natural Language Processing;https://github.com/yitu-opensource/ConvBert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/junhahyung/bert_finetune
"""Unofficial!  A replication of [Octave Convoltuion](https://arxiv.org/abs/1904.05049).  Require TensorFlow 2.0 to run the code.  Model and layers are from [SUL](https://github.com/ddddwee1/sul).   """;Computer Vision;https://github.com/ddddwee1/Octave-Convolution
"""We trained the model on the dataset with NSFW images as positive and SFW(suitable for work) images as negative. These images were editorially labelled. We cannot release the dataset or other details due to the nature of the data.   We use [CaffeOnSpark](https://github.com/yahoo/CaffeOnSpark) which is a wonderful framework for distributed learning that brings deep learning to Hadoop and Spark clusters for training models for our experiments. Big thanks to the CaffeOnSpark team!  The deep model was first pretrained on ImageNet 1000 class dataset. Then we finetuned the weights on the NSFW dataset. We used the thin resnet 50 1by2 architecture as the pretrained network. The model was generated using [pynetbuilder](https://github.com/jay-mahadeokar/pynetbuilder) tool and replicates the [residual network](https://arxiv.org/pdf/1512.03385v1.pdf) paper's 50 layer network (with half number of filters in each layer).  You can find more details on how the model was generated and trained [here](https://github.com/jay-mahadeokar/pynetbuilder/tree/master/models/imagenet)  Please note that deeper networks  or networks with more filters can improve accuracy. We train the model using a thin residual network architecture  since it provides good tradeoff in terms of accuracy  and the model is light-weight in terms of runtime (or flops) and memory (or number of parameters).   """;General;https://github.com/yahoo/open_nsfw
"""**[Update:]** I've further simplified the code to pytorch 1.5  torchvision 0.6  and replace the customized ops roipool and nms with the one from torchvision.  if you want the old version code  please checkout branch [v1.0](https://github.com/chenyuntc/simple-faster-rcnn-pytorch/tree/v1.0)    This project is a **Simplified** Faster R-CNN implementation based on [chainercv](https://github.com/chainer/chainercv) and other [projects](#acknowledgement) . I hope it can serve as an start code for those who want to know the detail of Faster R-CNN.  It aims to:  - Simplify the code (*Simple is better than complex*) - Make the code more straightforward (*Flat is better than nested*) - Match the performance reported in [origin paper](https://arxiv.org/abs/1506.01497) (*Speed Counts and mAP Matters*)  And it has the following features: - It can be run as pure Python code  no more build affair.  - It's a minimal implemention in around 2000 lines valid code with a lot of comment and instruction.(thanks to chainercv's excellent documentation) - It achieves higher mAP than the origin implementation (0.712 VS 0.699) - It achieve speed compariable with other implementation (6fps and 14fps for train and test in TITAN XP) - It's memory-efficient (about 3GB for vgg16)   ![img](imgs/faster-speed.jpg)     """;Computer Vision;https://github.com/chenyuntc/simple-faster-rcnn-pytorch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/felix-do-wizardry/bert_felix
"""**GCNet** is initially described in [arxiv](https://arxiv.org/abs/1904.11492). Via absorbing advantages of Non-Local Networks (NLNet) and Squeeze-Excitation Networks (SENet)   GCNet provides a simple  fast and effective approach for global context modeling  which generally outperforms both NLNet and SENet on major benchmarks for various recognition tasks.   """;General;https://github.com/zhusiling/GCNet
"""* this project using gym-style interface of ai2thor environment * objective is simply picking an apple in kitchen environment - FloorPlan28 * observation space is first-view RGB 128x128 image from agent's camera * maximum step in this project is 500 * reward fuction:      * -0.01 each time step     * 1 if agent can pick an apple  the env than terminate     * 0.01 if agent saw an apple (has been removed in latest code)  * a pre-train mobilenet-v2 model on image-net is used an feature extractor for later dense layer both actor and critic model * actor optimizer using Advantages + Entropy term to encourage exploration (https://arxiv.org/abs/1602.01783)   """;Reinforcement Learning;https://github.com/gungui98/deeprl-a3c-ai2thor
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/kiko441500/google_bert
"""To date  most open access public smart meter datasets are still at 30-minute or hourly temporal resolution. While this level of granularity could be sufficient for billing or deriving aggregated generation or consumption patterns  it may not fully capture the weather transients or consumption spikes. One potential solution is to synthetically interpolate high resolution data from commonly accessible lower resolution data  for this work  the SRGAN model is used for this purpose.  """;Computer Vision;https://github.com/tomtrac/SRGAN_power_data_generation
"""The RadioTalk corpus is in JSONL format  with one json document per line. Each line represents one ""snippet"" of audio  may contain multiple sentences  and is represented as a dictionary object with the following keys: * `content`: The transcribed speech from the snippet. * `callsign`: The call letters of the station the snippet aired on. * `city`: The city the station is based in  as in FCCC filings. * `state`: The state the station is based in  as in FCCC filings. * `show_name`: The name of the show containing this snippet. * `signature`: The initial 8 bytes of an MD5 hash of the `content` field  after lowercasing and removing English stopwords (specifically the NLTK stopword list)  intended to help with deduplication. * `studio_or_telephone`: A flag for whether the underlying audio came from a telephone or studio audio equipment. (The most useful feature in distinguishing these is the [narrow frequency range](https://en.wikipedia.org/wiki/Plain_old_telephone_service#Characteristics) of telephone audio.) * `guessed_gender`: The imputed speaker gender. * `segment_start_time`: The Unix timestamp of the beginning of the underlying audio. * `segment_end_time`: The Unix timestamp of the end of the underlying audio. * `speaker_id`: A diarization ID for the person speaking in the audio snippet. * `audio_chunk_id`: An ID for the audio chunk this snippet came from (each chunk may be split into multiple snippets).  An example snippet from the corpus (originally on one line but pretty-printed here for readability): ``` {     ""content"": ""This would be used for housing programs and you talked a little bit about how the attorney""      ""callsign"": ""KABC""      ""city"": ""Los Angeles""      ""state"": ""CA""      ""show_name"": ""The Drive Home With Jillian Barberie & John Phillips""      ""signature"": ""afd7d2ee""      ""studio_or_telephone"": ""T""      ""guessed_gender"": ""F""      ""segment_start_time"": 1540945402.6      ""segment_end_time"": 1540945408.6      ""speaker_id"": ""S0""      ""audio_chunk_id"": ""2018-10-31/KABC/00_20_28/16"" } ```   """;Natural Language Processing;https://github.com/social-machines/RadioTalk
"""The masks we generate are of two kinds and they both used OpenCV to draw their shapes. The first method contained in the script *utils/Mask_generator.py* creates masks composed of ellipses  lines and circle of random size and position. The second method contained in the script *utils/Mask_generator_circle.py* creates circular masks with center and radius that can vary according to one's preference. For simplicity  in the analysis done so far  the circular masks were always centered at the center of the images.   """;Computer Vision;https://github.com/GabrieleMonte/CoMBInE
""" ![sn](./assests/sn.png)     """;General;https://github.com/taki0112/Spectral_Normalization-Tensorflow
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.      ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    """;Natural Language Processing;https://github.com/crx934080895/Bert-CRF_New2
"""**Model Type:** Artificial Neural Network ( 37 717 Trainable params )  <img src='./Images/model.png'>  **Features used (Variable X):**  The features (total 45) are of three categories:  * Market Data Features * Engineered Features - Rolling averages and Exponential Weighted Moving averages of Open and Close Prices * News Data Features  <img src='./Images/X.png'> <img src='./Images/X2.png'>  Some of these features were further normalized with Scalers from SKLearn.  **Features Predicted (Variable Y):**   	returnsOpenNextMktres10(float64) - 10 day  market-residualized return.      You must predict a signed confidence value  (-1  1)  which is multiplied by the market-adjusted return of a given assetCode over a ten day window.    """;Natural Language Processing;https://github.com/diabhaque/Sixth-Sense
"""High localization accuracy is crucial in many real-world applications. We propose a novel single stage end-to-end object detection network (RRC) to produce high accuracy detection results. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our paper (https://arxiv.org/abs/1704.05776).  | method | KITTI test *mAP* car (moderate)| | :-------: | :-----: | | [Mono3D](http://3dimage.ee.tsinghua.edu.cn/cxz/mono3d)| 88.66% | | [SDP+RPN](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Exploit_All_the_CVPR_2016_paper.pdf)| 88.85% | | [MS-CNN](https://github.com/zhaoweicai/mscnn) | 89.02% | | [Sub-CNN](https://arxiv.org/pdf/1604.04693.pdf) | 89.04% | | RRC (single model) | **89.85%** |    [KITTI ranking](http://www.jimmyren.com/papers/rrc_kitti.pdf)   """;Computer Vision;https://github.com/xiaohaoChen/rrc_detection
"""we propose what point affect to accuracy from between Python Package and C raw programming without Package.   """;General;https://github.com/jooyounghun/Separable-CNN
"""This notebook focuses on the so-called Universal Language Model Fine-tuning (ULMFiT) introduced by  Jeremy Howard and Sebastian Ruder [[1](https://arxiv.org/abs/1801.06146)].  The ULMFiT model consists of three main stages in the building a language model (LM):  1. **General-domain LM  pre-training**: Similar to the ImageNet database used in computer vision   the idea is to pre-train a large corpus of text. The ULMFiT has a pre-trained model called the  `Wikitext-103` where more than 20 000 Wikipedia articles was trained on.  This is alreadly included in the `fastai.text` API  thus  it is not necessary to carry out this step.  2. **LM fine-tuning**: Because the target data (typically) comes from a different  distribution from the general-domain  it is necessary to fine-tune the LM to adapt to  the idosyncrasies of the target data. Howard and Ruder suggested *discriminative fine-tuning* and *slanted triangular learning rates* for fine-tuning the LM. These techniques are available  in `fastai` and are described in [[1](https://arxiv.org/abs/1801.06146)].  3. **Classifier fine-tuning**: Using the updated weights from the previous step  a classifier can be fine-tuned. Howard and Ruder suggested a few techniques which include *concat pooling*  and *gradual unfreezing*. The latter in particular is used in this demonstration.  Again  the `fastai` framework allows one to perform this technique.  **Reference**  [1] J. Howard and S. Ruder. 2018.*Universal Language Model Fine-tuning for Text Classification*. [arXiv:1801.06146](https://arxiv.org/abs/1801.06146).  """;General;https://github.com/anthonyckleung/Transfer-Learning-in-Sentiment-Tweets
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/TYTYTYTYTYTYTYTYTY/558-project
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Arthurizijar/Bert_Airport
"""we propose what point affect to accuracy from between Python Package and C raw programming without Package.   """;Computer Vision;https://github.com/jooyounghun/Separable-CNN
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/qilei123/DEEPLAB_4_RETINAIMG
"""This is a MATLAB implementation of a 2 hidden-layers neural network that recognizes handwritten digit with 97% accuracy on MNIST database. The architecture and training parameters of the network is configurable (including number of layers  number of neurons in each layer  number of training rounds  learning rate  and mini-batch length). The network integrates input normalization  He weight initialization [1]  and Swish activation function [2].  """;General;https://github.com/phogbinh/handwritten-digit-recognition
"""- [X] InsightFace inference example (production ready architecture)  - [X] Face recognition demo with insightface (visualization missing  add later)  - [ ] InsightFace training pipeline   """;General;https://github.com/bingxinhu/arcface
"""This is the pytorch implementation of Paper: Image Inpainting With Learnable Bidirectional Attention Maps (ICCV 2019) [paper](http://openaccess.thecvf.com/content_ICCV_2019/papers/Xie_Image_Inpainting_With_Learnable_Bidirectional_Attention_Maps_ICCV_2019_paper.pdf) [suppl](http://openaccess.thecvf.com/content_ICCV_2019/supplemental/Xie_Image_Inpainting_With_ICCV_2019_supplemental.pdf)   """;General;https://github.com/Vious/LBAM_Pytorch
"""Manufacturing is becoming automated on a broad scale. The technology enables manufacturers to affordably boost their throughput  improve quality and become nimbler as they respond to customer demands. Automation is a revolution in manufacturing quality control. It allows the companies to set certain bars or criteria for the products being manufactured. Then it also aids in real-time tracking of the manufacturing process through machine vision cameras and/or recordings.   The core deliverable for this project is building deep learning image classification models which can automate the process of inspection for casting defects. I have produced a rest endpoint which can accept a cast image and subsequently run a tuned model to classify if the cast is acceptable or not.    As part of this project  I have built the computer vision models in 3 different ways addressing different personas  because not all companies will have a resolute data science team.   1.	Using Keras Tensorflow model (convolution2d) what a trained team of data scientists would do. 2.	Using Azure Machine Learning Designer (designer) which enables AI engineers and Data scientists use a drag and drop prebuilt model DenseNet (densenet).  3.	Using Azure custom vision (custom-vision) which democratizes the process of building the computer vision model with little to no training.   """;Computer Vision;https://github.com/RajdeepBiswas/Manufacturing-Quality-Inspection
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/DeokO/bert-excercise-ongoing
"""- preprocess.py: a Python script to index the data. - train.lua: a Lua script to train and load the QA model. - interact.lua: a Lua script to load the saved model and querying user requirements. - model.dot: a grahical representation of the model configuration. - output.png: a sample output screenshot.  """;General;https://github.com/rakeshbm/QA-using-Torch
"""InfoGAN is an information-theoretic extension to the simple Generative Adversarial Networks that is able to learn disentangled representations in a completely unsupervised manner. What this means is that InfoGAN successfully disentangle wrirting styles from digit shapes on th MNIST dataset and discover visual concepts such as hair styles and gender on the CelebA dataset. To achieve this an information-theoretic regularization is added to the loss function that enforces the maximization of mutual information between latent codes  c  and the generator distribution G(z  c).   """;Computer Vision;https://github.com/inkplatform/InfoGAN-PyTorch
"""vedaseg is an open source semantic segmentation toolbox based on PyTorch.   """;Computer Vision;https://github.com/Media-Smart/vedaseg
"""English | [简体中文](README_zh-CN.md)  MMGeneration is a powerful toolkit for generative models  especially for GANs now. It is based on PyTorch and [MMCV](https://github.com/open-mmlab/mmcv). The master branch works with **PyTorch 1.5+**.  <div align=""center"">     <img src=""https://user-images.githubusercontent.com/12726765/114534478-9a65a900-9c81-11eb-8087-de8b6816eed8.png"" width=""800""/> </div>    """;General;https://github.com/open-mmlab/mmgeneration
"""This package contains the source code implementation of the paper ""Discovering Autoregressive Orderings with Variational Inference"".  It implements the Variational Order Inference (VOI) algorithm mentioned in the paper  where the encoder generates nonsequential autoregressive orders as the latent variable  and the decoder maximizes the joint probability of generating the target sequence under these nonsequential orders. In conditional text generation tasks  the encoder is implemented as non-causal Transformer  and the decoder is implemented as Transformer-InDIGO (Gu et al.  2019) which generates target sequences through insertion.  Taking away the encoder Transformer (we also call it as the Permutation Transformer (PT)  which outputs latent orderings) and the VOI algorithm  this repo is also a standalone implementation of Transformer-INDIGO. Training Transformer-INDIGO with left-to-right ordering is equivalent to training a Transformer with relative position representations ([Link](https://arxiv.org/abs/1803.02155)) (Shaw et al.  2018).   """;General;https://github.com/anonymouscode115/autoregressive_inference
"""This code applies [dynamic evaluation](https://arxiv.org/abs/1709.07432) to pretrained Transformer-XL models from this [paper](https://arxiv.org/abs/1901.02860). Our codebase is a modified version of [their codebase](https://github.com/kimiyoung/transformer-xl). We used this code to obtain state of the art results on WikiText-103 (perplexity: 16.4)  enwik8 (bits/char: 0.94)  and text8 (bits/char: 1.04).   """;Natural Language Processing;https://github.com/benkrause/dynamiceval-transformer
"""PaddleOCR aims to create multilingual  awesome  leading  and practical OCR tools that help users train better models and apply them into practice.   **Recent updates** - 2021.12.21 OCR open source online course starts. The lesson starts at 8:30 every night and lasts for ten days. Free registration: https://aistudio.baidu.com/aistudio/course/introduce/25207 - 2021.12.21 release PaddleOCR v2.4  release 1 text detection algorithm (PSENet)  3 text recognition algorithms (NRTR、SEED、SAR)  1 key information extraction algorithm (SDMGR  [tutorial](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.4/ppstructure/docs/kie.md)) and 3 DocVQA algorithms (LayoutLM  LayoutLMv2  LayoutXLM  [tutorial](https://github.com/PaddlePaddle/PaddleOCR/tree/release/2.4/ppstructure/vqa)). - PaddleOCR R&D team would like to share the key points of PP-OCRv2  at 20:15 pm on September 8th  [Course Address](https://aistudio.baidu.com/aistudio/education/group/info/6758). - 2021.9.7 release PaddleOCR v2.3  [PP-OCRv2](#PP-OCRv2) is proposed. The inference speed of PP-OCRv2 is 220% higher than that of PP-OCR server in CPU device. The F-score of PP-OCRv2 is 7% higher than that of PP-OCR mobile. - 2021.8.3 released PaddleOCR v2.2  add a new structured documents analysis toolkit  i.e.  [PP-Structure](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/ppstructure/README.md)  support layout analysis and table recognition (One-key to export chart images to Excel files). - 2021.4.8 release end-to-end text recognition algorithm [PGNet](https://www.aaai.org/AAAI21Papers/AAAI-2885.WangP.pdf) which is published in AAAI 2021. Find tutorial [here](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.1/doc/doc_en/pgnet_en.md)；release multi language recognition [models](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.1/doc/doc_en/multi_languages_en.md)  support more than 80 languages recognition; especically  the performance of [English recognition model](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.1/doc/doc_en/models_list_en.md#English) is Optimized.  - [more](./doc/doc_en/update_en.md)   """;Computer Vision;https://github.com/PaddlePaddle/PaddleOCR
"""Data scientists often choose the uniform distribution or the normal distribution as the latent variable distribution when they build representative models of datasets. For example  the studies of the GANs [1] and the VAEs[2] used the uniform random distribution and the normal one  respectively.  As the approximate function implemented by neural networks is usually continuous  the topological structure of the latent variable distribution is preserved after the transformation from the latent variables space to the observable variables space. Given that the observed variables are distributed on a torus and that networks  for example the GANs  are trained with the latent variables sampled from the normal distribution  the structure of the projected distribution by the trained networks does not meet with the torus  even if residual error is small enough. Imagine another example where the observable variables follow a mixture distribution  of which clusters separate each other  trained variational autoencoder can encode the feature on the latent variable space with high precision  however  the decoded distribution consists of connected clusters since the latent variable is topologically equal with the ball. This means that the topology of the given dataset is not represented by the projection of the trained networks.  In this short text  we study the consequence of autoencoders' training due to  the topological mismatch. We use the SAE[4] as autoencoders  which is enhanced based on the WAE[3] owing to the sinkhorn algorithm.   """;Computer Vision;https://github.com/allnightlight/ConditionalWassersteinAutoencoderPoweredBySinkhornDistance
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/hoangtrungchinh/Bert-SQuAD-v2
"""- `main.lua` (~30 lines) - loads all other files  starts training. - `opts.lua` (~50 lines) - all the command-line options and description - `data.lua` (~60 lines) - contains the logic to create K threads for parallel data-loading. - `donkey.lua` (~200 lines) - contains the data-loading logic and details. It is run by each data-loader thread. random image cropping  generating 10-crops etc. are in here. - `model.lua` (~80 lines) - creates AlexNet model and criterion - `train.lua` (~190 lines) - logic for training the network. we hard-code a learning rate + weight decay schedule that produces good results. - `test.lua` (~120 lines) - logic for testing the network on validation set (including calculating top-1 and top-5 errors) - `dataset.lua` (~430 lines) - a general purpose data loader  mostly derived from [here: imagenetloader.torch](https://github.com/soumith/imagenetloader.torch). That repo has docs and more examples of using this loader.  """;Computer Vision;https://github.com/soumith/imagenet-multiGPU.torch
"""**Faster** R-CNN is an object detection framework based on deep convolutional networks  which includes a Region Proposal Network (RPN) and an Object Detection Network. Both networks are trained for sharing convolutional layers for fast testing.  **This repo contains a Python implementation of Faster-RCNN originally developed in Matlab. This code works with models trained using Matlab version of Faster-RCNN which is main difference between this and py-faster-rcnn.**  This code was developed for internal use in one of my projects at the end of 2015. I decided to publish it as is.   """;Computer Vision;https://github.com/smichalowski/faster_rcnn
"""English | [简体中文](README_zh-CN.md)  The master branch works with **PyTorch 1.3+**.  MMDetection3D is an open source object detection toolbox based on PyTorch  towards the next-generation platform for general 3D detection. It is a part of the OpenMMLab project developed by [MMLab](http://mmlab.ie.cuhk.edu.hk/).  ![demo image](resources/mmdet3d_outdoor_demo.gif)   """;Computer Vision;https://github.com/open-mmlab/mmdetection3d
"""The purpose of this project is to develop deep learning approaches for the segmentation of brain tissues. These segmentations are useful for measuring and visualizing anatomical structures  but also to analyze brain changes in case of diseases like Alzheimer. Today different automatic segmentations are available thanks to FAST (FSL)  Freesurfer and ANTS. But these approaches are often inaccurate and require additional manual segmentations which are both time consuming and challenging.    """;Computer Vision;https://github.com/sophieloiz/brain-tissues-segmentation
"""            * No summary available !!!!!!         ''' style={'background-color':'#87ceeb'                                                                  'foreground-color':'red'                                                                  'color':'black'                                                                  'fontSize':12                                                                  'textAlign': 'left'                                                                  'verticalAlign':'top'                                                                  'position':'fixed'                                                                  'width':'50%'                                                                  'height':'18%'                                                                  'top':'80%'                                                                  'left':'27%'                                                                  'border-radius':10})   #######################   #if __name__ == '__main__':                 * Tracking is done quarterly at 3 different levels to give us a long term as well as short term indication of model performance         ''' style={'background-color':'#87ceeb'                                                                  'foreground-color':'red'                                                                  'color':'black'                                                                  'fontSize':12                                                                  'textAlign': 'left'                                                                  'verticalAlign':'top'                                                                  'position':'fixed'                                                                  'width':'50%'                                                                  'height':'18%'                                                                  'top':'80%'                                                                  'left':'27%'                                                                  'border-radius':10})     elif model_no == '111111':         return dcc.Markdown('''                   * Tracking is done quarterly at 3 different levels to give us a long term as well as short term indication of model performance         *Results: Q2 UTM Tracking is rates GREEN at overall level         * Segment level some deterioration is observed.         * Segment 1’s KS/PSI shifts are due to a known issue where the benchmark dataset with “very long ARF (Automated Response Format)” records (rich bureau history and FICO scores) were treated incorrectly by FICO during development         time. The validation dataset does not have this issue.         * Booked PSI shifts for segments 1  2  and 3 are caused by tightening risk tolerance since the 2010 benchmark time period.         * Segment 5's deterioration led to a deep dive in 2018 that found that 3 input attribute's had an amber rating for PSI (>10%)         ''' style={'background-color':'#87ceeb'                                                                  'foreground-color':'red'                                                                  'color':'black'                                                                  'fontSize':12                                                                  'textAlign': 'left'                                                                  'verticalAlign':'top'                                                                  'position':'fixed'                                                                  'width':'50%'                                                                  'height':'18%'                                                                  'top':'80%'                                                                  'left':'27%'                                                                  'border-radius':10})     elif model_no == '803456':         return dcc.Markdown('''           """;Computer Vision;https://github.com/vaibhavtmnit/Theano-Projects
"""The traditional methods of estimating uncertainties were mainly Bayesian methods. Bayesian methods should introduce the preor to estimate the distribution of the posteror. But DNN has too many parameters  so it's hard to calculate. So there are also non-bayesian methods that are most famous for modelling methods.  This method learns several DNNs to obtain an uncertainty with different degrees of prediction.  This method has a large computational cost because it requires the learning of several models.  Other methods have problems that cannot be measured by distinguishing between an entity's data from an entity's internal uncertainty.  Solve these problems using SDE-Net. SDE-Net alternates between drift net and diffuse net.  The drift net increases the accuracy of the prediction and allows the measurement of the analistic entity  and the epistemical entity is measured with the diffusion net.   """;General;https://github.com/Junghwan-brian/SDE-Net
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/NovasMax/yolov3
"""This project is to study the use of Convolutional Neural Network and in particular the ResNet architecture. The show case is segmentation of Magnetic Resonance Images (MRI) of human brain into anatomical regions[2]. We will extend the ResNet topology into the processing of 3-dimensional voxels.    """;Computer Vision;https://github.com/bclwan/MRI_Brain_Segmentation
"""Generative Adversarial Networks (GANs) are one of the most interesting ideas in computer science today. Here two models named Generator and Discriminator are trained simultaneously. As the name says Generator generates the fake images or we can say it generates a random noise and the Discriminator job is to classify whether the image is fake or not. Here the only job of Generator is to fake the Discriminator. In this project we are using DCGAN(Deep Convolutional Generative Adversarial Network). A DCGAN is a direct extension of the GAN described above  except that it explicitly uses convolutional and convolutional-transpose layers in the discriminator and generator  respectively. DCGANs actually comes under Unsupervised Learning and was first described by Radford et. al. in the paper Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks.  ![](https://miro.medium.com/max/2850/1*Mw2c3eY5khtXafe5W-Ms_w.jpeg)   """;Computer Vision;https://github.com/hunnurjirao/DCGAN
"""GANs have proven to be very powerful generative models. So  here's a well-structured **Tensorflow** project containing implementations of some GANs architectures.   """;General;https://github.com/DarkGeekMS/Tensorflow-GANs-Architectures-Implementation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/xitianxiaofeixue/BERT
"""[![Watch the video](figures/video_figure.png)](https://www.youtube.com/watch?v=a_OeT8MXzWI&feature=youtu.be)   """;General;https://github.com/mit-han-lab/once-for-all
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/nachiketaa/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/jinzhenfan/BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/SYangDong/bert-with-frozen-code
"""<div align=center><img width=""100%"" src=""figs/FFB6D_overview.png""/></div>  [FFB6D](https://arxiv.org/abs/2103.02242v1) is a general framework for representation learning from a single RGBD image  and we applied it to the 6D pose estimation task by cascading downstream prediction headers for instance semantic segmentation and 3D keypoint voting prediction from PVN3D([Arxiv](https://arxiv.org/abs/1911.04231)  [Code](https://github.com/ethnhe/PVN3D)  [Video](https://www.bilibili.com/video/av89408773/)).  At the representation learning stage of FFB6D  we build **bidirectional** fusion modules in the **full flow** of the two networks  where fusion is applied to each encoding and decoding layer. In this way  the two networks can leverage local and global complementary information from the other one to obtain better representations. Moreover  at the output representation stage  we designed a simple but effective 3D keypoints selection algorithm considering the texture and geometry information of objects  which simplifies keypoint localization for precise pose estimation.  Please cite [FFB6D](https://arxiv.org/abs/2103.02242v1) & [PVN3D](https://arxiv.org/abs/1911.04231) if you use this repository in your publications:  ``` @InProceedings{He_2021_CVPR  author = {He  Yisheng and Huang  Haibin and Fan  Haoqiang and Chen  Qifeng and Sun  Jian}  title = {FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation}  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}  month = {June}  year = {2021} }  @InProceedings{He_2020_CVPR  author = {He  Yisheng and Sun  Wei and Huang  Haibin and Liu  Jianran and Fan  Haoqiang and Sun  Jian}  title = {PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation}  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}  month = {June}  year = {2020} } ```   """;Computer Vision;https://github.com/hz-ants/FFB6D
"""FastSeq provides efficient implementation of popular sequence models (e.g. [Bart](https://arxiv.org/pdf/1910.13461.pdf)  [ProphetNet](https://github.com/microsoft/ProphetNet)) for text generation  summarization  translation tasks etc. It automatically optimizes inference speed based on popular NLP toolkits (e.g. [FairSeq](https://github.com/pytorch/fairseq) and [HuggingFace-Transformers](https://github.com/huggingface/transformers)) without accuracy loss. All these can be easily done (no need to change any code/model/data if using our command line tool  or simply add one-line code `import fastseq` if using source code).   """;Sequential;https://github.com/microsoft/fastseq
"""In this study  we try to understand the limits of our system when running a Deep Learning training. The step to train a model is the most time consuming step of the model building process. With contraints put on the hardware  what can we do on the programming side to help us train models better? What if you had a limited amount of time? To try out our hand at augmentation  we will be using the Flickr27 dataset.     """;Computer Vision;https://github.com/premthomas/keras-image-classification
"""![motivation of CCNet](https://user-images.githubusercontent.com/4509744/50546460-7df6ed00-0bed-11e9-9340-d026373b2cbe.png) Long-range dependencies can capture useful contextual information to benefit visual understanding problems. In this work  we propose a Criss-Cross Network (CCNet) for obtaining such important information through a more effective and efficient way. Concretely  for each pixel  our CCNet can harvest the contextual information of its surrounding pixels on the criss-cross path through a novel criss-cross attention module. By taking a further recurrent operation  each pixel can finally capture the long-range dependencies from all pixels. Overall  our CCNet is with the following merits:  - **GPU memory friendly**   - **High computational efficiency**  - **The state-of-the-art performance**    """;Computer Vision;https://github.com/speedinghzl/CCNet
"""A tensorflow re-implementation of Self-Paced Network Embedding use random walk to get positive pair  """;Graphs;https://github.com/liuxinkai94/Graph-embedding
"""A summary of the performance can be produced by invoking the following command from inside the ```my_project``` folder or ```predictions``` sub-folder:  ``` mp summary  >> [***] SUMMARY REPORT FOR FOLDER [***] >> ./my_project/predictions/csv/ >>  >>  >> Per class: >> -------------------------------- >>    Mean dice by class  +/- STD    min    max   N >> 1               0.856    0.060  0.672  0.912  34 >> 2               0.891    0.029  0.827  0.934  34 >> 3               0.888    0.027  0.829  0.930  34 >> 4               0.802    0.164  0.261  0.943  34 >> 5               0.819    0.075  0.552  0.926  34 >> 6               0.863    0.047  0.663  0.917  34 >>  >> Overall mean: 0.853 +- 0.088 >> -------------------------------- >>  >> By views: >> -------------------------------- >> [0.8477811  0.50449719 0.16355361]          0.825 >> [ 0.70659414 -0.35532932  0.6119361 ]       0.819 >> [ 0.11799461 -0.07137918  0.9904455 ]       0.772 >> [ 0.95572575 -0.28795306  0.06059151]       0.827 >> [-0.16704373 -0.96459936  0.20406974]       0.810 >> [-0.72188903  0.68418977  0.10373322]       0.819 >> -------------------------------- ```   """;Computer Vision;https://github.com/perslev/MultiPlanarUNet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/dzqjorking/transpose
"""The main purpose is to study how well the word order information learned by different neural networks. Specifically  we randomly move one word to another position  and examine whether a trained model can detect both the original and inserted positions. Our codes were built upon [THUMT-MT](https://github.com/THUNLP-MT/THUMT). We compare self-attention networks (SAN  [Vaswani et al.  2017](https://arxiv.org/pdf/1706.03762.pdf)) with re-implemented RNN ([Chen et al.  2018](https://www.aclweb.org/anthology/P18-1008))  as well as directional SAN (DiSAN [Shen et al.  2018](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16126/16099)) that augments SAN with recurrence modeling.   """;General;https://github.com/baosongyang/WRD
"""---  Here I have taken a flower classification dataset from http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html. After some preprocessing of images I took InceptionNet as my model and fine tuned it's last 50 layers and freezed all the first 50 but batch normalization layers.   """;Computer Vision;https://github.com/bhuyanamit986/FlowerClassification
"""The economy of online shopping is bigger than it has ever been and continues to grow each year. It follows that the market for being able to deliver goods quickly and reliably is becoming more and more competitive. In addition to improving the overall flow of transactions  knowing when a package will be delivered is a major factor in customer satisfaction  making the ability to accurately predict delivery dates essential to companies such as eBay.  The process of achieving this  however  poses many challenges. In the case of eBay  the majority of transactions are carried out between individual sellers and buyers  often resulting in the data for goods and their delivery being inconsistently recorded. This  in addition to packages being processed by a variety of different delivery services  means that data labels are frequently missing or incorrect. Further  the shipment date is largely left to the sole decision of each individual seller  resulting in a high degree of variability.  ![image of shipping process](/images/diagram2.png)  We worked to provide a solution to these problems and provide a model to enable the accurate prediction of delivery dates using machine learning.  To implement our model  a number of decisions were made and tested  including deciding the optimal means by which to clean the data (for instance  whether to omit training data that has missing or incorrect features or to assign it average values based on correctly labeled training data)  deciding whether to compute the estimation holistically from shipment to delivery date or to compute multiple separate estimates on separate legs of the delivery process  and deciding which features to include and in which leg.  Should our model have some error  it is important that it produces random rather than systematic error. Specifically  we want to avoid creating a model which might consistently predict early delivery dates  which could lead to sellers and delivery services rushing packages and resulting in the employment of more non-sustainable methods  such as shipping half-full boxes  as well as increasing the pressure on employees to have to work faster and faster.  Using a loss function of the weighted average absolute error of the delivery predictions in days that was provided by eBay  our model achieved a loss of 0.411 where a loss of 0.759 was the baseline of random guessing.    """;Computer Vision;https://github.com/milliemince/eBay-shipping-predictions
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/Talgin/facerec
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/longchr123/insightface-analysis
"""This framework was created in order to help compare learned and variational approaches to CT reconstruction in a systematic way. The implementation is based on python libraries **odl** and **pyTorch**. The list of implemented algorithms include:  * FBP (Filtered back-projection)  * TV (Total Variation) * ADR (Adversarial Regularizer): https://arxiv.org/abs/1805.11572 * LG (Learned gradient descent): https://arxiv.org/abs/1704.04058 * LPD (Learned primal dual): https://arxiv.org/abs/1707.06474 * FL (Fully learned): https://nature.com/articles/nature25988.pdf * FBP+U (FBP with a U-Net denoiser): https://arxiv.org/abs/1505.04597  In order to add your own algorithms to the list  create a new file in the **Algorithms** folder in the form *name*.py and use BaseAlg.py as the template.  """;Computer Vision;https://github.com/Zakobian/CT_framework_
"""The model is used to tackle the alpha matting problem. It is basically an encoder-decoder deep neural network. By feed the network an original image with tri-map  you can get the prediction alpha of the image.       Deep Image Matting     Ning Xu  Brian Price  Scott Cohen  and Thomas Huang.      CVPR  2017.  The input images should be mean pixel subtraction. And the channel should be BGR.   """;Computer Vision;https://github.com/ShawnNew/shape-alpha-matting
"""The foundational model that we use is Inception-V3 from Keras' pretrained models. However  we cut it off until 'mixed7' layer  and then add our own layers.  Read more about this model at: - https://keras.io/api/applications/inceptionv3/ - https://arxiv.org/abs/1512.00567     There are only 3 important files in this repository. - `modeling.ipynb` is a jupyter notebook which can be run on Google Colab (with GPU for faster training). It contains step-by-step on how to create the image classifier and export the model.  - `model_inception_weights.h5` is the trained weights of our deep learning model's layers. This is used to load the model in our web app. - `apps.py` is the python file to deploy our web app in Streamlit.     """;Computer Vision;https://github.com/myarist/Rock-Paper-Scissors
""" The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.    """;Computer Vision;https://github.com/GodofCCode/yolo3
"""**TL; DR.** Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism.    ![deformable_detr](./figs/illustration.png)  ![deformable_detr](./figs/convergence.png)  **Abstract.** DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However  it suffers from slow convergence and limited feature spatial resolution  due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues  we proposed Deformable DETR  whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10× less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach.   """;Computer Vision;https://github.com/fundamentalvision/Deformable-DETR
"""This repository is a PyTorch implementation for semantic segmentation / scene parsing. The code is easy to use for training and testing on various datasets. The codebase mainly uses ResNet50/101/152 as backbone and can be easily adapted to other basic classification structures. Implemented networks including [PSPNet](https://hszhao.github.io/projects/pspnet) and [PSANet](https://hszhao.github.io/projects/psanet)  which ranked 1st places in [ImageNet Scene Parsing Challenge 2016 @ECCV16](http://image-net.org/challenges/LSVRC/2016/results)  [LSUN Semantic Segmentation Challenge 2017 @CVPR17](https://blog.mapillary.com/product/2017/06/13/lsun-challenge.html) and [WAD Drivable Area Segmentation Challenge 2018 @CVPR18](https://bdd-data.berkeley.edu/wad-2018.html). Sample experimented datasets are [ADE20K](http://sceneparsing.csail.mit.edu)  [PASCAL VOC 2012](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6) and [Cityscapes](https://www.cityscapes-dataset.com).  <img src=""./figure/pspnet.png"" width=""900""/>   """;Computer Vision;https://github.com/hszhao/semseg
"""This work is accepted at the PRIME workshop in MICCAI 2021.  > **Investigating and Quantifying the Reproducibility of Graph Neural Networks in Predictive Medicine** > > Mohammed Amine Gharsallaoui  Furkan Tornaci and Islem Rekik > > BASIRA Lab  Faculty of Computer and Informatics  Istanbul Technical University  Istanbul  Turkey > > **Abstract:** *Graph neural networks (GNNs) have gained an unprecedented attention in many domains including dysconnectivity disorder diagnosis thanks to their high performance in tackling graph classification tasks. Despite the large stream of GNNs developed recently  prior efforts invariably focus on boosting the classification accuracy while ignoring the model reproducibility and interpretability  which are vital in pinning down disorder-specific biomarkers. Although less investigated  the discriminativeness of the original input features -biomarkers  which is reflected by their learnt weights using a GNN gives informative insights about their reliability. Intuitively  the reliability of a given biomarker is emphasized if it belongs to the sets of top discriminative regions of interest (ROIs) using different models. Therefore  we define the first axis in our work as \emph{reproducibility across models}  which evaluates the commonalities between sets of top discriminative biomarkers for a pool of GNNs. This task mainly answers this question: \emph{How likely can two models be congruent in terms of their respective sets of top discriminative biomarkers?} The second axis of research in our work is to investigate \emph{reproducibility in generated connectomic datasets}. This is addressed by answering this question: \emph{how likely would the set of top discriminative biomarkers by a trained model for a ground-truth dataset be consistent with a predicted dataset by generative learning?} In this paper  we propose a reproducibility assessment framework  a method for quantifying the commonalities in the GNN-specific learnt feature maps across models  which can complement explanatory approaches of GNNs and provide new ways to assess predictive medicine via biomarkers reliability. We evaluated our framework using four multiview connectomic datasets of healthy neurologically disordered subjects with five GNN architectures and two different learning mindsets: (a) conventional training on all samples (resourceful) and (b) a few-shot training on random samples (frugal).*    """;Graphs;https://github.com/basiralab/Reproducible-Generative-Learning
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/AndrewZhou924/YOLOv3_pytorch
"""This is a project based on [InsightFace: 2D and 3D Face Analysis Project](https://github.com/deepinsight/insightface).   """;General;https://github.com/nv-quan/insightface-attendance
"""This is the official code of [High-Resolution Representations for Object Detection](https://arxiv.org/pdf/1904.04514.pdf). We extend the high-resolution representation (HRNet) [1] by augmenting the high-resolution representation by aggregating the (upsampled) representations from all the parallel convolutions  leading to stronger representations. We build a multi-level representation from the high resolution and apply it to the Faster R-CNN  Mask R-CNN and Cascade R-CNN framework. This proposed approach achieves superior results to existing single-model networks on COCO object detection. The code is based on [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark)  <div align=center>  ![](images/hrnetv2p.png)  </div>    """;Computer Vision;https://github.com/HRNet/HRNet-MaskRCNN-Benchmark
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/MichaelZhouwang/LMlexsub
"""This is the implementation of our model called Style-Restricted GAN (SRGAN)  which is designed for the unpaired image translation with multiple styles. The main features of this models are 1) the enhancement of diversification and 2) the restriction of diversification. As for the former one  while the base model ([SingleGAN](https://github.com/Xiaoming-Yu/SingleGAN)) employed KL divergence loss to restrict the distribution of encoded features like [VAE](https://arxiv.org/abs/1312.6114)  SRGAN exploits 3 new losses instead: batch KL divergence loss  correlation loss  and histogram imitation loss. When it comes to the restriction  in the previous  it wasn't explicitly designed to control how the generator diversifies the results  which can have an adverse effect on some applications. Therefore  in this paper  the encoder is pre-trained with the classification task before being used as an encoder.  We'll proceed this implementation in a notebook form. And we also share our docker environment in order for everybody to run the code as well as observing the implementation.  ---  """;General;https://github.com/shinshoji01/Style-Restricted_GAN
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/chrisleunglokhin/Capstone-BERT
"""* this project using gym-style interface of ai2thor environment * objective is simply picking an apple in kitchen environment - FloorPlan28 * observation space is first-view RGB 128x128 image from agent's camera * maximum step in this project is 500 * reward fuction:      * -0.01 each time step     * 1 if agent can pick an apple  the env than terminate     * 0.01 if agent saw an apple (has been removed in latest code)  * a pre-train mobilenet-v2 model on image-net is used an feature extractor for later dense layer both actor and critic model * actor optimizer using Advantages + Entropy term to encourage exploration (https://arxiv.org/abs/1602.01783)   """;General;https://github.com/gungui98/deeprl-a3c-ai2thor
"""Properties of dilated convolution are discussed in our [ICLR 2016 conference paper](http://arxiv.org/abs/1511.07122). This repository contains the network definitions and the trained models. You can use this code together with vanilla Caffe to segment images using the pre-trained models. If you want to train the models yourself  please check out the [document for training](https://github.com/fyu/dilation/blob/master/docs/training.md).  **If you are looking for dilation models with state-of-the-art performance and Python implementation  please check out [Dilated Residual Networks](https://github.com/fyu/drn).**   """;Computer Vision;https://github.com/fyu/dilation
"""Architecture of our complementary segmentation network  the optimal CompNet. The dense blocks (DB)  corresponding to the gray bars  are used in each encoder and decoder. The triple (x y z) in each dense block indicates that it has x convolutional layers with a kernel size 3×3; each layer has y filters  except for the last one that has z filters. SO: segmentation output for the brain mask; CO: complementary segmentation output for the non-brain mask; RO: reconstruction output for the input image. These three outputs produced by the Sigmoid function are the final predictions; while all other Sigmoids produce intermediate outputs  except for the green one that is the concatenation of the summation from each intermediate layers. Best viewed in color.  *ROI and CO branches -  We take the downsampling branch of a U-Net as it is  however we split the upsampling branch into two halves  one to obtain the Region of Interest and the other for Complementary aka non region of interest. Losses here are negative dice for ROI and positive dice for Non-ROI region.*  *Reconstruction Branch -  Next we merge these two ROI and non ROI outputs using ""Summation"" operation and then pass it into another U-Net  This U-Net is the reconstruction branch. The input is the summed image from previous step and the output is the ""original"" image that we start with. The loss of reconstruction branch is MSE.*  ``` The code in this repository provides only the stand alone code for this architecture. You may implement it as is  or convert it into modular structure if you so wish. The dataset of OASIS can obtained from the link above and the preprocessiong steps involved are mentioned in the paper.  You have to provide the inputs. ```   email me - rd31879@uga.edu for any questions !! Am happy to discuss    """;Computer Vision;https://github.com/raun1/MICCAI2018---Complementary_Segmentation_Network-Raw-Code
"""This repo contains **experimental and unofficial** implementation of image captioning frameworks including:  - Self-Critical Sequence Training (SCST) [[arxiv]](https://arxiv.org/abs/1612.00563)     - Sampling is done via beam search [[arxiv]](https://arxiv.org/abs/1707.07998) - Multi-Head Visual Attention     - Additive (with optional Layer Norm) [[arxiv]](https://arxiv.org/abs/1903.01072)     - Scaled Dot-Product [[arxiv]](https://arxiv.org/abs/1706.03762) - Graph-based Beam Search  Greedy Search and Sampling  The features might not be completely tested. For a more stable implementation  please refer to [this repo](https://github.com/jiahuei/COMIC-Compact-Image-Captioning-with-Attention).    """;Natural Language Processing;https://github.com/jiahuei/Self-Critical-SCST-TensorFlow
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/brightmart/bert_customized
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.      ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    """;Natural Language Processing;https://github.com/kyzhouhzau/BERT-NER
"""utils.py: Read data and data processing.<br> layer.py: Attention layer.<br> model.py: Graph attention model network.<br> main.py: Training  validation and testing.<br> You can run it through： ```python python main.py ```   """;Graphs;https://github.com/taishan1994/pytorch_gat
