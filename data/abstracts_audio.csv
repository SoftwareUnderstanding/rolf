Text;Label
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/xlsr XLSR XLSR **XLSR** is a multilingual speech recognition model built on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. A shared quantization module over feature encoder representations produces multilingual quantized speech units whose embeddings are then used as targets for a [Transformer](https://paperswithcode.com/method/transformer) trained by contrastive learning. The model learns to share discrete tokens across languages  creating bridges across languages. unsupervised-cross-lingual-representation-3 2000 None None None """;Audio
"""https://paperswithcode.com/method/jukebox Jukebox Jukebox **Jukebox** is a model that generates music with singing in the raw audio domain. It tackles the long context of raw audio using a multi-scale [VQ-VAE](https://paperswithcode.com/method/vq-vae) to compress it to discrete codes  and modeling those using [autoregressive Transformers](https://paperswithcode.com/methods/category/autoregressive-transformers). It can condition on artist and genre to steer the musical and vocal style  and on unaligned lyrics to make the singing more controllable.  Three separate VQ-VAE models are trained with different temporal resolutions. At each level  the input audio is segmented and encoded into latent vectors $\mathbf{h}\_{t}$  which are then quantized to the closest codebook vectors $\mathbf{e}\_{z\_{t}}$. The code $z\_{t}$ is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction  since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels  where the least abstract bottom-level codes result in the highest-quality audio. jukebox-a-generative-model-for-music-1 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad WaveGrad WaveGrad **WaveGrad** is a conditional model for waveform generation through estimating gradients of the data density. This model is built on the prior work on score matching and diffusion probabilistic models. It starts from Gaussian white noise and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad is non-autoregressive  and requires only a constant number of generation steps during inference. It can use as few as 6 iterations to generate high fidelity audio samples. wavegrad-estimating-gradients-for-waveform 2000 None None https://github.com/lmnt-com/wavegrad/blob/master/src/wavegrad/model.py """;Audio
"""https://paperswithcode.com/method/film-module FiLM Module FiLM Module The **Feature-wise linear modulation** (**FiLM**) module combines information from both noisy waveform and input mel-spectrogram. It is used in the [WaveGrad](https://paperswithcode.com/method/wavegrad) model. The authors also added iteration index $n$ which indicates the noise level of the input waveform by using the [Transformer](https://paperswithcode.com/method/transformer) sinusoidal positional embedding. To condition on the noise level directly  $n$ is replaced by $\sqrt{\bar{\alpha}}$ and a linear scale $C = 5000$ is applied. The FiLM module produces both scale and bias vectors given inputs  which are used in a UBlock for feature-wise affine transformation as:  $$ \gamma\left(D  \sqrt{\bar{\alpha}}\right) \odot U + \zeta\left(D  \sqrt{\bar{\alpha}}\right) $$  where $\gamma$ and $\zeta$ correspond to the scaling and shift vectors from the FiLM module  $D$ is the output from corresponding [DBlock](https://paperswithcode.com/method/dblock)  $U$ is an intermediate output in the UBlock. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad-dblock WaveGrad DBlock WaveGrad DBlock **WaveGrad DBlocks** are used to downsample the temporal dimension of noisy waveform in [WaveGrad](https://paperswithcode.com/method/wavegrad). They are similar to UBlocks except that only one [residual block](https://paperswithcode.com/method/residual-block) is included. The dilation factors are 1  2  4 in the main branch. Orthogonal initialization is used. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad-ublock WaveGrad UBlock WaveGrad UBlock The **WaveGrad UBlock** is used for upsampling in [WaveGrad](https://paperswithcode.com/method/wavegrad). Neural audio generation models often use large receptive field. Dilation factors of four convolutional layers are 1  2  1  2 for the first two UBlocks and 1  2  4  8 for the rest. Orthogonal initialization is used. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wav2vec-u wav2vec-U wav2vec Unsupervised **wav2vec-U** is an unsupervised method to train speech recognition models without any labeled data. It leverages self-supervised speech representations to segment unlabeled language and learn a mapping from these representations to phonemes via adversarial training.   Specifically  we learn self-supervised representations with wav2vec 2.0 on unlabeled speech audio  then identify clusters in the representations with k-means to segment the audio data. Next  we build segment representations by mean pooling the wav2vec 2.0 representations  performing [PCA](https://paperswithcode.com/method/pca) and a second mean pooling step between adjacent segments. This is input to the generator which outputs a phoneme sequence that is fed to the discriminator  similar to phonemized unlabeled text to perform adversarial training. unsupervised-speech-recognition 2000 None None None """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/bridge-net Bridge-net Bridge-net **Bridge-net** is an audio model block used in the [ClariNet](https://paperswithcode.com/method/clarinet) text-to-speech architecture. Bridge-net maps frame-level hidden representation to sample-level through several [convolution](https://paperswithcode.com/method/convolution) blocks and [transposed convolution](https://paperswithcode.com/method/transposed-convolution) layers interleaved with softsign non-linearities. clarinet-parallel-wave-generation-in-end-to 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/jukebox Jukebox Jukebox **Jukebox** is a model that generates music with singing in the raw audio domain. It tackles the long context of raw audio using a multi-scale [VQ-VAE](https://paperswithcode.com/method/vq-vae) to compress it to discrete codes  and modeling those using [autoregressive Transformers](https://paperswithcode.com/methods/category/autoregressive-transformers). It can condition on artist and genre to steer the musical and vocal style  and on unaligned lyrics to make the singing more controllable.  Three separate VQ-VAE models are trained with different temporal resolutions. At each level  the input audio is segmented and encoded into latent vectors $\mathbf{h}\_{t}$  which are then quantized to the closest codebook vectors $\mathbf{e}\_{z\_{t}}$. The code $z\_{t}$ is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction  since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels  where the least abstract bottom-level codes result in the highest-quality audio. jukebox-a-generative-model-for-music-1 2000 None None  """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/xlsr XLSR XLSR **XLSR** is a multilingual speech recognition model built on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. A shared quantization module over feature encoder representations produces multilingual quantized speech units whose embeddings are then used as targets for a [Transformer](https://paperswithcode.com/method/transformer) trained by contrastive learning. The model learns to share discrete tokens across languages  creating bridges across languages. unsupervised-cross-lingual-representation-3 2000 None None None """;Audio
"""https://paperswithcode.com/method/glow-tts Glow-TTS Glow-TTS **Glow-TTS** is a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming  the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech.  The model is directly trained to maximize the log-likelihood of speech with the alignment. Enforcing hard monotonic alignments helps enable robust TTS  which generalizes to long utterances  and employing flows enables fast  diverse  and controllable speech synthesis. glow-tts-a-generative-flow-for-text-to-speech 2000 None None None """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/convtasnet ConvTasNet Convolutional time-domain audio separation network Combines learned time-frequency representation with a masker architecture based on 1D [dilated convolution](https://paperswithcode.com/method/dilated-convolution). tasnet-surpassing-ideal-time-frequency 2000 None None https://github.com/asteroid-team/asteroid/blob/master/asteroid/models/conv_tasnet.py """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/glow-tts Glow-TTS Glow-TTS **Glow-TTS** is a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming  the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech.  The model is directly trained to maximize the log-likelihood of speech with the alignment. Enforcing hard monotonic alignments helps enable robust TTS  which generalizes to long utterances  and employing flows enables fast  diverse  and controllable speech synthesis. glow-tts-a-generative-flow-for-text-to-speech 2000 None None None """;Audio
"""https://paperswithcode.com/method/bridge-net Bridge-net Bridge-net **Bridge-net** is an audio model block used in the [ClariNet](https://paperswithcode.com/method/clarinet) text-to-speech architecture. Bridge-net maps frame-level hidden representation to sample-level through several [convolution](https://paperswithcode.com/method/convolution) blocks and [transposed convolution](https://paperswithcode.com/method/transposed-convolution) layers interleaved with softsign non-linearities. clarinet-parallel-wave-generation-in-end-to 2000 None None None """;Audio
"""https://paperswithcode.com/method/waveglow WaveGlow WaveGlow **WaveGlow** is a flow-based generative model that generates audio by sampling from a distribution. Specifically samples are taken from a zero mean spherical Gaussian with the same number of dimensions as our desired output  and those samples are put through a series of layers that transforms the simple distribution to one which has the desired distribution. waveglow-a-flow-based-generative-network-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/convtasnet ConvTasNet Convolutional time-domain audio separation network Combines learned time-frequency representation with a masker architecture based on 1D [dilated convolution](https://paperswithcode.com/method/dilated-convolution). tasnet-surpassing-ideal-time-frequency 2000 None None https://github.com/asteroid-team/asteroid/blob/master/asteroid/models/conv_tasnet.py """;Audio
"""https://paperswithcode.com/method/wavevae WaveVAE WaveVAE **WaveVAE** is a generative audio model that can be used as a vocoder in text-to-speech systems. It is a [VAE](https://paperswithcode.com/method/vae) based model that can be trained from scratch by jointly optimizing the encoder $q\_{\phi}\left(\mathbf{z}|\mathbf{x}  \mathbf{c}\right)$ and decoder $p\_{\theta}\left(\mathbf{x}|\mathbf{z}  \mathbf{c}\right)$  where $\mathbf{z}$ is latent variables and $\mathbf{c}$ is the mel spectrogram conditioner.   The encoder of WaveVAE $q\_{\phi}\left(\mathbf{z}|\mathbf{x}\right)$ is parameterized by a Gaussian autoregressive [WaveNet](https://paperswithcode.com/method/wavenet) that maps the ground truth audio x into the same length latent representation $\mathbf{z}$. The decoder $p\_{\theta}\left(\mathbf{x}|\mathbf{z}\right)$ is parameterized by the one-step ahead predictions from an inverse autoregressive flow.  The training objective is the ELBO for the observed $\mathbf{x}$ in the VAE. parallel-neural-text-to-speech 2000 None None None """;Audio
"""https://paperswithcode.com/method/convtasnet ConvTasNet Convolutional time-domain audio separation network Combines learned time-frequency representation with a masker architecture based on 1D [dilated convolution](https://paperswithcode.com/method/dilated-convolution). tasnet-surpassing-ideal-time-frequency 2000 None None https://github.com/asteroid-team/asteroid/blob/master/asteroid/models/conv_tasnet.py """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/convtasnet ConvTasNet Convolutional time-domain audio separation network Combines learned time-frequency representation with a masker architecture based on 1D [dilated convolution](https://paperswithcode.com/method/dilated-convolution). tasnet-surpassing-ideal-time-frequency 2000 None None https://github.com/asteroid-team/asteroid/blob/master/asteroid/models/conv_tasnet.py """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/convtasnet ConvTasNet Convolutional time-domain audio separation network Combines learned time-frequency representation with a masker architecture based on 1D [dilated convolution](https://paperswithcode.com/method/dilated-convolution). tasnet-surpassing-ideal-time-frequency 2000 None None https://github.com/asteroid-team/asteroid/blob/master/asteroid/models/conv_tasnet.py """;Audio
"""https://paperswithcode.com/method/ddsp DDSP Differentiable Digital Signal Processing  ddsp-differentiable-digital-signal-processing-1 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/multi-band-melgan Multi-band MelGAN Multi-band MelGAN **Multi-band MelGAN**  or **MB-MelGAN**  is a waveform generation model focusing on high-quality text-to-speech. It improves the original [MelGAN](https://paperswithcode.com/method/melgan) in several ways. First  it increases the receptive field of the generator  which is proven to be beneficial to speech generation. Second  it substitutes the feature matching loss with the multi-resolution STFT loss to better measure the difference between fake and real speech. Lastly  [MelGAN](https://paperswithcode.com/method/melgan) is extended with multi-band processing: the generator takes mel-spectrograms as input and produces sub-band signals which are subsequently summed back to full-band signals as discriminator input. multi-band-melgan-faster-waveform-generation 2000 None None None """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/accomontage AccoMontage AccoMontage **AccoMontage** is a model for accompaniment arrangement  a type of music generation task involving intertwined constraints of melody  harmony  texture  and music structure. AccoMontage generates piano accompaniments for folk/pop songs based on a lead sheet (i.e. a melody with chord progression). It first retrieves phrase montages from a database while recombining them structurally using dynamic programming. Second  chords of the retrieved phrases are manipulated to match the lead sheet via style transfer. Lastly  the system offers controls over the generation process. In contrast to pure deep learning approaches  AccoMontage uses a hybrid pathway  in which rule-based optimization and deep learning are both leveraged. accomontage-accompaniment-arrangement-via 2000 None None  """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/convtasnet ConvTasNet Convolutional time-domain audio separation network Combines learned time-frequency representation with a masker architecture based on 1D [dilated convolution](https://paperswithcode.com/method/dilated-convolution). tasnet-surpassing-ideal-time-frequency 2000 None None https://github.com/asteroid-team/asteroid/blob/master/asteroid/models/conv_tasnet.py """;Audio
"""https://paperswithcode.com/method/wavegrad WaveGrad WaveGrad **WaveGrad** is a conditional model for waveform generation through estimating gradients of the data density. This model is built on the prior work on score matching and diffusion probabilistic models. It starts from Gaussian white noise and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad is non-autoregressive  and requires only a constant number of generation steps during inference. It can use as few as 6 iterations to generate high fidelity audio samples. wavegrad-estimating-gradients-for-waveform 2000 None None https://github.com/lmnt-com/wavegrad/blob/master/src/wavegrad/model.py """;Audio
"""https://paperswithcode.com/method/film-module FiLM Module FiLM Module The **Feature-wise linear modulation** (**FiLM**) module combines information from both noisy waveform and input mel-spectrogram. It is used in the [WaveGrad](https://paperswithcode.com/method/wavegrad) model. The authors also added iteration index $n$ which indicates the noise level of the input waveform by using the [Transformer](https://paperswithcode.com/method/transformer) sinusoidal positional embedding. To condition on the noise level directly  $n$ is replaced by $\sqrt{\bar{\alpha}}$ and a linear scale $C = 5000$ is applied. The FiLM module produces both scale and bias vectors given inputs  which are used in a UBlock for feature-wise affine transformation as:  $$ \gamma\left(D  \sqrt{\bar{\alpha}}\right) \odot U + \zeta\left(D  \sqrt{\bar{\alpha}}\right) $$  where $\gamma$ and $\zeta$ correspond to the scaling and shift vectors from the FiLM module  $D$ is the output from corresponding [DBlock](https://paperswithcode.com/method/dblock)  $U$ is an intermediate output in the UBlock. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad-dblock WaveGrad DBlock WaveGrad DBlock **WaveGrad DBlocks** are used to downsample the temporal dimension of noisy waveform in [WaveGrad](https://paperswithcode.com/method/wavegrad). They are similar to UBlocks except that only one [residual block](https://paperswithcode.com/method/residual-block) is included. The dilation factors are 1  2  4 in the main branch. Orthogonal initialization is used. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad-ublock WaveGrad UBlock WaveGrad UBlock The **WaveGrad UBlock** is used for upsampling in [WaveGrad](https://paperswithcode.com/method/wavegrad). Neural audio generation models often use large receptive field. Dilation factors of four convolutional layers are 1  2  1  2 for the first two UBlocks and 1  2  4  8 for the rest. Orthogonal initialization is used. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/jukebox Jukebox Jukebox **Jukebox** is a model that generates music with singing in the raw audio domain. It tackles the long context of raw audio using a multi-scale [VQ-VAE](https://paperswithcode.com/method/vq-vae) to compress it to discrete codes  and modeling those using [autoregressive Transformers](https://paperswithcode.com/methods/category/autoregressive-transformers). It can condition on artist and genre to steer the musical and vocal style  and on unaligned lyrics to make the singing more controllable.  Three separate VQ-VAE models are trained with different temporal resolutions. At each level  the input audio is segmented and encoded into latent vectors $\mathbf{h}\_{t}$  which are then quantized to the closest codebook vectors $\mathbf{e}\_{z\_{t}}$. The code $z\_{t}$ is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction  since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels  where the least abstract bottom-level codes result in the highest-quality audio. jukebox-a-generative-model-for-music-1 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/sepformer SepFormer SepFormer **SepFormer** is [Transformer](https://paperswithcode.com/methods/category/transformers)-based neural network for speech separation. The SepFormer learns short and long-term dependencies with a multi-scale approach that employs transformers. It is mainly composed of multi-head attention and feed-forward layers. A dual-path framework (introduced by DPRNN) is adopted and [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) are replaced with a multiscale pipeline composed of transformers that learn both short and long-term dependencies. The dual-path framework enables the mitigation of the quadratic complexity of transformers  as transformers in the dual-path framework process smaller chunks.  The model is based on the learned-domain masking approach and employs an encoder  a decoder  and a masking network  as shown in the figure. The encoder is fully convolutional  while the decoder employs two Transformers embedded inside the dual-path processing block. The decoder finally reconstructs the separated signals in the time domain by using the masks predicted by the masking network. attention-is-all-you-need-in-speech 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/multi-band-melgan Multi-band MelGAN Multi-band MelGAN **Multi-band MelGAN**  or **MB-MelGAN**  is a waveform generation model focusing on high-quality text-to-speech. It improves the original [MelGAN](https://paperswithcode.com/method/melgan) in several ways. First  it increases the receptive field of the generator  which is proven to be beneficial to speech generation. Second  it substitutes the feature matching loss with the multi-resolution STFT loss to better measure the difference between fake and real speech. Lastly  [MelGAN](https://paperswithcode.com/method/melgan) is extended with multi-band processing: the generator takes mel-spectrograms as input and produces sub-band signals which are subsequently summed back to full-band signals as discriminator input. multi-band-melgan-faster-waveform-generation 2000 None None None """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/glow-tts Glow-TTS Glow-TTS **Glow-TTS** is a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming  the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech.  The model is directly trained to maximize the log-likelihood of speech with the alignment. Enforcing hard monotonic alignments helps enable robust TTS  which generalizes to long utterances  and employing flows enables fast  diverse  and controllable speech synthesis. glow-tts-a-generative-flow-for-text-to-speech 2000 None None None """;Audio
"""https://paperswithcode.com/method/fastpitch FastPitch FastPitch **FastPitch** is a fully-parallel text-to-speech model based on FastSpeech  conditioned on fundamental frequency contours. The architecture of FastPitch is shown in the Figure. It is based on FastSpeech and composed mainly of two feed-forward [Transformer](https://paperswithcode.com/method/transformer) (FFTr) stacks. The first one operates in the resolution of input tokens  the second one in the resolution of the output frames. Let $x=\left(x\_{1}  \ldots  x\_{n}\right)$ be the sequence of input lexical units  and $\mathbf{y}=\left(y\_{1}  \ldots  y\_{t}\right)$ be the sequence of target mel-scale spectrogram frames. The first FFTr stack produces the hidden representation $\mathbf{h}=\operatorname{FFTr}(\mathbf{x})$. The hidden representation $h$ is used to make predictions about the duration and average pitch of every character with a 1-D CNN   $$ \hat{\mathbf{d}}=\text { DurationPredictor }(\mathbf{h})  \quad \hat{\mathbf{p}}=\operatorname{PitchPredictor}(\mathbf{h}) $$  where $\hat{\mathbf{d}} \in \mathbb{N}^{n}$ and $\hat{\mathbf{p}} \in \mathbb{R}^{n}$. Next  the pitch is projected to match the dimensionality of the hidden representation $h \in$ $\mathbb{R}^{n \times d}$ and added to $\mathbf{h}$. The resulting sum $\mathbf{g}$ is discretely upsampled and passed to the output FFTr  which produces the output mel-spectrogram sequence  $$ \mathbf{g}=\mathbf{h}+\operatorname{PitchEmbedding}(\mathbf{p}) $$  $$ \hat{\mathbf{y}}=\operatorname{FFTr}\left([\underbrace{g\_{1}  \ldots  g\_{1}}\_{d\_{1}}  \ldots \underbrace{g\_{n}  \ldots  g\_{n}}_{d\_{n}}]\right) $$   Ground truth $\mathbf{p}$ and $\mathbf{d}$ are used during training  and predicted $\hat{\mathbf{p}}$ and $\hat{\mathbf{d}}$ are used during inference. The model optimizes mean-squared error (MSE) between the predicted and ground-truth modalities  $$ \mathcal{L}=\|\hat{\mathbf{y}}-\mathbf{y}\|\_{2}^{2}+\alpha\|\hat{\mathbf{p}}-\mathbf{p}\|\_{2}^{2}+\gamma\|\hat{\mathbf{d}}-\mathbf{d}\|\_{2}^{2} $$ fastpitch-parallel-text-to-speech-with-pitch 2000 None None  """;Audio
"""https://paperswithcode.com/method/jukebox Jukebox Jukebox **Jukebox** is a model that generates music with singing in the raw audio domain. It tackles the long context of raw audio using a multi-scale [VQ-VAE](https://paperswithcode.com/method/vq-vae) to compress it to discrete codes  and modeling those using [autoregressive Transformers](https://paperswithcode.com/methods/category/autoregressive-transformers). It can condition on artist and genre to steer the musical and vocal style  and on unaligned lyrics to make the singing more controllable.  Three separate VQ-VAE models are trained with different temporal resolutions. At each level  the input audio is segmented and encoded into latent vectors $\mathbf{h}\_{t}$  which are then quantized to the closest codebook vectors $\mathbf{e}\_{z\_{t}}$. The code $z\_{t}$ is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction  since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels  where the least abstract bottom-level codes result in the highest-quality audio. jukebox-a-generative-model-for-music-1 2000 None None  """;Audio
"""https://paperswithcode.com/method/jukebox Jukebox Jukebox **Jukebox** is a model that generates music with singing in the raw audio domain. It tackles the long context of raw audio using a multi-scale [VQ-VAE](https://paperswithcode.com/method/vq-vae) to compress it to discrete codes  and modeling those using [autoregressive Transformers](https://paperswithcode.com/methods/category/autoregressive-transformers). It can condition on artist and genre to steer the musical and vocal style  and on unaligned lyrics to make the singing more controllable.  Three separate VQ-VAE models are trained with different temporal resolutions. At each level  the input audio is segmented and encoded into latent vectors $\mathbf{h}\_{t}$  which are then quantized to the closest codebook vectors $\mathbf{e}\_{z\_{t}}$. The code $z\_{t}$ is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction  since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels  where the least abstract bottom-level codes result in the highest-quality audio. jukebox-a-generative-model-for-music-1 2000 None None  """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/wavegrad WaveGrad WaveGrad **WaveGrad** is a conditional model for waveform generation through estimating gradients of the data density. This model is built on the prior work on score matching and diffusion probabilistic models. It starts from Gaussian white noise and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad is non-autoregressive  and requires only a constant number of generation steps during inference. It can use as few as 6 iterations to generate high fidelity audio samples. wavegrad-estimating-gradients-for-waveform 2000 None None https://github.com/lmnt-com/wavegrad/blob/master/src/wavegrad/model.py """;Audio
"""https://paperswithcode.com/method/film-module FiLM Module FiLM Module The **Feature-wise linear modulation** (**FiLM**) module combines information from both noisy waveform and input mel-spectrogram. It is used in the [WaveGrad](https://paperswithcode.com/method/wavegrad) model. The authors also added iteration index $n$ which indicates the noise level of the input waveform by using the [Transformer](https://paperswithcode.com/method/transformer) sinusoidal positional embedding. To condition on the noise level directly  $n$ is replaced by $\sqrt{\bar{\alpha}}$ and a linear scale $C = 5000$ is applied. The FiLM module produces both scale and bias vectors given inputs  which are used in a UBlock for feature-wise affine transformation as:  $$ \gamma\left(D  \sqrt{\bar{\alpha}}\right) \odot U + \zeta\left(D  \sqrt{\bar{\alpha}}\right) $$  where $\gamma$ and $\zeta$ correspond to the scaling and shift vectors from the FiLM module  $D$ is the output from corresponding [DBlock](https://paperswithcode.com/method/dblock)  $U$ is an intermediate output in the UBlock. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad-dblock WaveGrad DBlock WaveGrad DBlock **WaveGrad DBlocks** are used to downsample the temporal dimension of noisy waveform in [WaveGrad](https://paperswithcode.com/method/wavegrad). They are similar to UBlocks except that only one [residual block](https://paperswithcode.com/method/residual-block) is included. The dilation factors are 1  2  4 in the main branch. Orthogonal initialization is used. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad-ublock WaveGrad UBlock WaveGrad UBlock The **WaveGrad UBlock** is used for upsampling in [WaveGrad](https://paperswithcode.com/method/wavegrad). Neural audio generation models often use large receptive field. Dilation factors of four convolutional layers are 1  2  1  2 for the first two UBlocks and 1  2  4  8 for the rest. Orthogonal initialization is used. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/fastpitch FastPitch FastPitch **FastPitch** is a fully-parallel text-to-speech model based on FastSpeech  conditioned on fundamental frequency contours. The architecture of FastPitch is shown in the Figure. It is based on FastSpeech and composed mainly of two feed-forward [Transformer](https://paperswithcode.com/method/transformer) (FFTr) stacks. The first one operates in the resolution of input tokens  the second one in the resolution of the output frames. Let $x=\left(x\_{1}  \ldots  x\_{n}\right)$ be the sequence of input lexical units  and $\mathbf{y}=\left(y\_{1}  \ldots  y\_{t}\right)$ be the sequence of target mel-scale spectrogram frames. The first FFTr stack produces the hidden representation $\mathbf{h}=\operatorname{FFTr}(\mathbf{x})$. The hidden representation $h$ is used to make predictions about the duration and average pitch of every character with a 1-D CNN   $$ \hat{\mathbf{d}}=\text { DurationPredictor }(\mathbf{h})  \quad \hat{\mathbf{p}}=\operatorname{PitchPredictor}(\mathbf{h}) $$  where $\hat{\mathbf{d}} \in \mathbb{N}^{n}$ and $\hat{\mathbf{p}} \in \mathbb{R}^{n}$. Next  the pitch is projected to match the dimensionality of the hidden representation $h \in$ $\mathbb{R}^{n \times d}$ and added to $\mathbf{h}$. The resulting sum $\mathbf{g}$ is discretely upsampled and passed to the output FFTr  which produces the output mel-spectrogram sequence  $$ \mathbf{g}=\mathbf{h}+\operatorname{PitchEmbedding}(\mathbf{p}) $$  $$ \hat{\mathbf{y}}=\operatorname{FFTr}\left([\underbrace{g\_{1}  \ldots  g\_{1}}\_{d\_{1}}  \ldots \underbrace{g\_{n}  \ldots  g\_{n}}_{d\_{n}}]\right) $$   Ground truth $\mathbf{p}$ and $\mathbf{d}$ are used during training  and predicted $\hat{\mathbf{p}}$ and $\hat{\mathbf{d}}$ are used during inference. The model optimizes mean-squared error (MSE) between the predicted and ground-truth modalities  $$ \mathcal{L}=\|\hat{\mathbf{y}}-\mathbf{y}\|\_{2}^{2}+\alpha\|\hat{\mathbf{p}}-\mathbf{p}\|\_{2}^{2}+\gamma\|\hat{\mathbf{d}}-\mathbf{d}\|\_{2}^{2} $$ fastpitch-parallel-text-to-speech-with-pitch 2000 None None  """;Audio
"""https://paperswithcode.com/method/multi-band-melgan Multi-band MelGAN Multi-band MelGAN **Multi-band MelGAN**  or **MB-MelGAN**  is a waveform generation model focusing on high-quality text-to-speech. It improves the original [MelGAN](https://paperswithcode.com/method/melgan) in several ways. First  it increases the receptive field of the generator  which is proven to be beneficial to speech generation. Second  it substitutes the feature matching loss with the multi-resolution STFT loss to better measure the difference between fake and real speech. Lastly  [MelGAN](https://paperswithcode.com/method/melgan) is extended with multi-band processing: the generator takes mel-spectrograms as input and produces sub-band signals which are subsequently summed back to full-band signals as discriminator input. multi-band-melgan-faster-waveform-generation 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/jukebox Jukebox Jukebox **Jukebox** is a model that generates music with singing in the raw audio domain. It tackles the long context of raw audio using a multi-scale [VQ-VAE](https://paperswithcode.com/method/vq-vae) to compress it to discrete codes  and modeling those using [autoregressive Transformers](https://paperswithcode.com/methods/category/autoregressive-transformers). It can condition on artist and genre to steer the musical and vocal style  and on unaligned lyrics to make the singing more controllable.  Three separate VQ-VAE models are trained with different temporal resolutions. At each level  the input audio is segmented and encoded into latent vectors $\mathbf{h}\_{t}$  which are then quantized to the closest codebook vectors $\mathbf{e}\_{z\_{t}}$. The code $z\_{t}$ is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction  since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels  where the least abstract bottom-level codes result in the highest-quality audio. jukebox-a-generative-model-for-music-1 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/multi-band-melgan Multi-band MelGAN Multi-band MelGAN **Multi-band MelGAN**  or **MB-MelGAN**  is a waveform generation model focusing on high-quality text-to-speech. It improves the original [MelGAN](https://paperswithcode.com/method/melgan) in several ways. First  it increases the receptive field of the generator  which is proven to be beneficial to speech generation. Second  it substitutes the feature matching loss with the multi-resolution STFT loss to better measure the difference between fake and real speech. Lastly  [MelGAN](https://paperswithcode.com/method/melgan) is extended with multi-band processing: the generator takes mel-spectrograms as input and produces sub-band signals which are subsequently summed back to full-band signals as discriminator input. multi-band-melgan-faster-waveform-generation 2000 None None None """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/multi-band-melgan Multi-band MelGAN Multi-band MelGAN **Multi-band MelGAN**  or **MB-MelGAN**  is a waveform generation model focusing on high-quality text-to-speech. It improves the original [MelGAN](https://paperswithcode.com/method/melgan) in several ways. First  it increases the receptive field of the generator  which is proven to be beneficial to speech generation. Second  it substitutes the feature matching loss with the multi-resolution STFT loss to better measure the difference between fake and real speech. Lastly  [MelGAN](https://paperswithcode.com/method/melgan) is extended with multi-band processing: the generator takes mel-spectrograms as input and produces sub-band signals which are subsequently summed back to full-band signals as discriminator input. multi-band-melgan-faster-waveform-generation 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/convtasnet ConvTasNet Convolutional time-domain audio separation network Combines learned time-frequency representation with a masker architecture based on 1D [dilated convolution](https://paperswithcode.com/method/dilated-convolution). tasnet-surpassing-ideal-time-frequency 2000 None None https://github.com/asteroid-team/asteroid/blob/master/asteroid/models/conv_tasnet.py """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/convtasnet ConvTasNet Convolutional time-domain audio separation network Combines learned time-frequency representation with a masker architecture based on 1D [dilated convolution](https://paperswithcode.com/method/dilated-convolution). tasnet-surpassing-ideal-time-frequency 2000 None None https://github.com/asteroid-team/asteroid/blob/master/asteroid/models/conv_tasnet.py """;Audio
"""https://paperswithcode.com/method/convtasnet ConvTasNet Convolutional time-domain audio separation network Combines learned time-frequency representation with a masker architecture based on 1D [dilated convolution](https://paperswithcode.com/method/dilated-convolution). tasnet-surpassing-ideal-time-frequency 2000 None None https://github.com/asteroid-team/asteroid/blob/master/asteroid/models/conv_tasnet.py """;Audio
"""https://paperswithcode.com/method/glow-tts Glow-TTS Glow-TTS **Glow-TTS** is a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming  the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech.  The model is directly trained to maximize the log-likelihood of speech with the alignment. Enforcing hard monotonic alignments helps enable robust TTS  which generalizes to long utterances  and employing flows enables fast  diverse  and controllable speech synthesis. glow-tts-a-generative-flow-for-text-to-speech 2000 None None None """;Audio
"""https://paperswithcode.com/method/jukebox Jukebox Jukebox **Jukebox** is a model that generates music with singing in the raw audio domain. It tackles the long context of raw audio using a multi-scale [VQ-VAE](https://paperswithcode.com/method/vq-vae) to compress it to discrete codes  and modeling those using [autoregressive Transformers](https://paperswithcode.com/methods/category/autoregressive-transformers). It can condition on artist and genre to steer the musical and vocal style  and on unaligned lyrics to make the singing more controllable.  Three separate VQ-VAE models are trained with different temporal resolutions. At each level  the input audio is segmented and encoded into latent vectors $\mathbf{h}\_{t}$  which are then quantized to the closest codebook vectors $\mathbf{e}\_{z\_{t}}$. The code $z\_{t}$ is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction  since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels  where the least abstract bottom-level codes result in the highest-quality audio. jukebox-a-generative-model-for-music-1 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/directional-sparse-filtering Directional Sparse Filtering Directional Sparse FIltering  learning-complex-valued-latent-filters-with 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegrad WaveGrad WaveGrad **WaveGrad** is a conditional model for waveform generation through estimating gradients of the data density. This model is built on the prior work on score matching and diffusion probabilistic models. It starts from Gaussian white noise and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad is non-autoregressive  and requires only a constant number of generation steps during inference. It can use as few as 6 iterations to generate high fidelity audio samples. wavegrad-estimating-gradients-for-waveform 2000 None None https://github.com/lmnt-com/wavegrad/blob/master/src/wavegrad/model.py """;Audio
"""https://paperswithcode.com/method/film-module FiLM Module FiLM Module The **Feature-wise linear modulation** (**FiLM**) module combines information from both noisy waveform and input mel-spectrogram. It is used in the [WaveGrad](https://paperswithcode.com/method/wavegrad) model. The authors also added iteration index $n$ which indicates the noise level of the input waveform by using the [Transformer](https://paperswithcode.com/method/transformer) sinusoidal positional embedding. To condition on the noise level directly  $n$ is replaced by $\sqrt{\bar{\alpha}}$ and a linear scale $C = 5000$ is applied. The FiLM module produces both scale and bias vectors given inputs  which are used in a UBlock for feature-wise affine transformation as:  $$ \gamma\left(D  \sqrt{\bar{\alpha}}\right) \odot U + \zeta\left(D  \sqrt{\bar{\alpha}}\right) $$  where $\gamma$ and $\zeta$ correspond to the scaling and shift vectors from the FiLM module  $D$ is the output from corresponding [DBlock](https://paperswithcode.com/method/dblock)  $U$ is an intermediate output in the UBlock. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad-dblock WaveGrad DBlock WaveGrad DBlock **WaveGrad DBlocks** are used to downsample the temporal dimension of noisy waveform in [WaveGrad](https://paperswithcode.com/method/wavegrad). They are similar to UBlocks except that only one [residual block](https://paperswithcode.com/method/residual-block) is included. The dilation factors are 1  2  4 in the main branch. Orthogonal initialization is used. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad-ublock WaveGrad UBlock WaveGrad UBlock The **WaveGrad UBlock** is used for upsampling in [WaveGrad](https://paperswithcode.com/method/wavegrad). Neural audio generation models often use large receptive field. Dilation factors of four convolutional layers are 1  2  1  2 for the first two UBlocks and 1  2  4  8 for the rest. Orthogonal initialization is used. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/waveglow WaveGlow WaveGlow **WaveGlow** is a flow-based generative model that generates audio by sampling from a distribution. Specifically samples are taken from a zero mean spherical Gaussian with the same number of dimensions as our desired output  and those samples are put through a series of layers that transforms the simple distribution to one which has the desired distribution. waveglow-a-flow-based-generative-network-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/voicefilter-lite VoiceFilter-Lite VoiceFilter-Lite **VoiceFilter-Lite** is a single-channel source separation model that runs on the device to preserve only the speech signals from a target user  as part of a streaming speech recognition system. In this architecture  the voice filtering model operates as a frame-by-frame frontend signal processor to enhance the features consumed by the speech recognizer  without reconstructing audio signals from the features. The key contributions are (1) A system to perform speech separation directly on ASR input features; (2) An asymmetric loss function to penalize oversuppression during training  to make the model harmless under various acoustic environments  (3) An adaptive suppression strength mechanism to adapt to different noise conditions. voicefilter-lite-streaming-targeted-voice 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegrad WaveGrad WaveGrad **WaveGrad** is a conditional model for waveform generation through estimating gradients of the data density. This model is built on the prior work on score matching and diffusion probabilistic models. It starts from Gaussian white noise and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad is non-autoregressive  and requires only a constant number of generation steps during inference. It can use as few as 6 iterations to generate high fidelity audio samples. wavegrad-estimating-gradients-for-waveform 2000 None None https://github.com/lmnt-com/wavegrad/blob/master/src/wavegrad/model.py """;Audio
"""https://paperswithcode.com/method/film-module FiLM Module FiLM Module The **Feature-wise linear modulation** (**FiLM**) module combines information from both noisy waveform and input mel-spectrogram. It is used in the [WaveGrad](https://paperswithcode.com/method/wavegrad) model. The authors also added iteration index $n$ which indicates the noise level of the input waveform by using the [Transformer](https://paperswithcode.com/method/transformer) sinusoidal positional embedding. To condition on the noise level directly  $n$ is replaced by $\sqrt{\bar{\alpha}}$ and a linear scale $C = 5000$ is applied. The FiLM module produces both scale and bias vectors given inputs  which are used in a UBlock for feature-wise affine transformation as:  $$ \gamma\left(D  \sqrt{\bar{\alpha}}\right) \odot U + \zeta\left(D  \sqrt{\bar{\alpha}}\right) $$  where $\gamma$ and $\zeta$ correspond to the scaling and shift vectors from the FiLM module  $D$ is the output from corresponding [DBlock](https://paperswithcode.com/method/dblock)  $U$ is an intermediate output in the UBlock. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad-dblock WaveGrad DBlock WaveGrad DBlock **WaveGrad DBlocks** are used to downsample the temporal dimension of noisy waveform in [WaveGrad](https://paperswithcode.com/method/wavegrad). They are similar to UBlocks except that only one [residual block](https://paperswithcode.com/method/residual-block) is included. The dilation factors are 1  2  4 in the main branch. Orthogonal initialization is used. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad-ublock WaveGrad UBlock WaveGrad UBlock The **WaveGrad UBlock** is used for upsampling in [WaveGrad](https://paperswithcode.com/method/wavegrad). Neural audio generation models often use large receptive field. Dilation factors of four convolutional layers are 1  2  1  2 for the first two UBlocks and 1  2  4  8 for the rest. Orthogonal initialization is used. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/vocgan VocGAN VocGAN Please enter a description about the method here vocgan-a-high-fidelity-real-time-vocoder-with 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/multi-band-melgan Multi-band MelGAN Multi-band MelGAN **Multi-band MelGAN**  or **MB-MelGAN**  is a waveform generation model focusing on high-quality text-to-speech. It improves the original [MelGAN](https://paperswithcode.com/method/melgan) in several ways. First  it increases the receptive field of the generator  which is proven to be beneficial to speech generation. Second  it substitutes the feature matching loss with the multi-resolution STFT loss to better measure the difference between fake and real speech. Lastly  [MelGAN](https://paperswithcode.com/method/melgan) is extended with multi-band processing: the generator takes mel-spectrograms as input and produces sub-band signals which are subsequently summed back to full-band signals as discriminator input. multi-band-melgan-faster-waveform-generation 2000 None None None """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/frill FRILL FRILL **FRILL** is a non-semantic speech embedding model trained via knowledge distillation that is fast enough to be run in real-time on a mobile device. The fastest model runs at 0.9 ms  which is 300x faster than TRILL and 25x faster than TRILL-distilled. frill-a-non-semantic-speech-embedding-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegrad WaveGrad WaveGrad **WaveGrad** is a conditional model for waveform generation through estimating gradients of the data density. This model is built on the prior work on score matching and diffusion probabilistic models. It starts from Gaussian white noise and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad is non-autoregressive  and requires only a constant number of generation steps during inference. It can use as few as 6 iterations to generate high fidelity audio samples. wavegrad-estimating-gradients-for-waveform 2000 None None https://github.com/lmnt-com/wavegrad/blob/master/src/wavegrad/model.py """;Audio
"""https://paperswithcode.com/method/film-module FiLM Module FiLM Module The **Feature-wise linear modulation** (**FiLM**) module combines information from both noisy waveform and input mel-spectrogram. It is used in the [WaveGrad](https://paperswithcode.com/method/wavegrad) model. The authors also added iteration index $n$ which indicates the noise level of the input waveform by using the [Transformer](https://paperswithcode.com/method/transformer) sinusoidal positional embedding. To condition on the noise level directly  $n$ is replaced by $\sqrt{\bar{\alpha}}$ and a linear scale $C = 5000$ is applied. The FiLM module produces both scale and bias vectors given inputs  which are used in a UBlock for feature-wise affine transformation as:  $$ \gamma\left(D  \sqrt{\bar{\alpha}}\right) \odot U + \zeta\left(D  \sqrt{\bar{\alpha}}\right) $$  where $\gamma$ and $\zeta$ correspond to the scaling and shift vectors from the FiLM module  $D$ is the output from corresponding [DBlock](https://paperswithcode.com/method/dblock)  $U$ is an intermediate output in the UBlock. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad-dblock WaveGrad DBlock WaveGrad DBlock **WaveGrad DBlocks** are used to downsample the temporal dimension of noisy waveform in [WaveGrad](https://paperswithcode.com/method/wavegrad). They are similar to UBlocks except that only one [residual block](https://paperswithcode.com/method/residual-block) is included. The dilation factors are 1  2  4 in the main branch. Orthogonal initialization is used. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad-ublock WaveGrad UBlock WaveGrad UBlock The **WaveGrad UBlock** is used for upsampling in [WaveGrad](https://paperswithcode.com/method/wavegrad). Neural audio generation models often use large receptive field. Dilation factors of four convolutional layers are 1  2  1  2 for the first two UBlocks and 1  2  4  8 for the rest. Orthogonal initialization is used. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/xlsr XLSR XLSR **XLSR** is a multilingual speech recognition model built on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. A shared quantization module over feature encoder representations produces multilingual quantized speech units whose embeddings are then used as targets for a [Transformer](https://paperswithcode.com/method/transformer) trained by contrastive learning. The model learns to share discrete tokens across languages  creating bridges across languages. unsupervised-cross-lingual-representation-3 2000 None None None """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/bridge-net Bridge-net Bridge-net **Bridge-net** is an audio model block used in the [ClariNet](https://paperswithcode.com/method/clarinet) text-to-speech architecture. Bridge-net maps frame-level hidden representation to sample-level through several [convolution](https://paperswithcode.com/method/convolution) blocks and [transposed convolution](https://paperswithcode.com/method/transposed-convolution) layers interleaved with softsign non-linearities. clarinet-parallel-wave-generation-in-end-to 2000 None None None """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/ipl IPL Iterative Pseudo-Labeling **Iterative Pseudo-Labeling** (IPL) is a semi-supervised algorithm for speech recognition which efficiently performs multiple iterations of pseudo-labeling on unlabeled data as the acoustic model evolves. In particular  IPL fine tunes an existing model at each iteration using both labeled data and a subset of unlabeled data. iterative-pseudo-labeling-for-speech 2000 None None None """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/multi-band-melgan Multi-band MelGAN Multi-band MelGAN **Multi-band MelGAN**  or **MB-MelGAN**  is a waveform generation model focusing on high-quality text-to-speech. It improves the original [MelGAN](https://paperswithcode.com/method/melgan) in several ways. First  it increases the receptive field of the generator  which is proven to be beneficial to speech generation. Second  it substitutes the feature matching loss with the multi-resolution STFT loss to better measure the difference between fake and real speech. Lastly  [MelGAN](https://paperswithcode.com/method/melgan) is extended with multi-band processing: the generator takes mel-spectrograms as input and produces sub-band signals which are subsequently summed back to full-band signals as discriminator input. multi-band-melgan-faster-waveform-generation 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/fastpitch FastPitch FastPitch **FastPitch** is a fully-parallel text-to-speech model based on FastSpeech  conditioned on fundamental frequency contours. The architecture of FastPitch is shown in the Figure. It is based on FastSpeech and composed mainly of two feed-forward [Transformer](https://paperswithcode.com/method/transformer) (FFTr) stacks. The first one operates in the resolution of input tokens  the second one in the resolution of the output frames. Let $x=\left(x\_{1}  \ldots  x\_{n}\right)$ be the sequence of input lexical units  and $\mathbf{y}=\left(y\_{1}  \ldots  y\_{t}\right)$ be the sequence of target mel-scale spectrogram frames. The first FFTr stack produces the hidden representation $\mathbf{h}=\operatorname{FFTr}(\mathbf{x})$. The hidden representation $h$ is used to make predictions about the duration and average pitch of every character with a 1-D CNN   $$ \hat{\mathbf{d}}=\text { DurationPredictor }(\mathbf{h})  \quad \hat{\mathbf{p}}=\operatorname{PitchPredictor}(\mathbf{h}) $$  where $\hat{\mathbf{d}} \in \mathbb{N}^{n}$ and $\hat{\mathbf{p}} \in \mathbb{R}^{n}$. Next  the pitch is projected to match the dimensionality of the hidden representation $h \in$ $\mathbb{R}^{n \times d}$ and added to $\mathbf{h}$. The resulting sum $\mathbf{g}$ is discretely upsampled and passed to the output FFTr  which produces the output mel-spectrogram sequence  $$ \mathbf{g}=\mathbf{h}+\operatorname{PitchEmbedding}(\mathbf{p}) $$  $$ \hat{\mathbf{y}}=\operatorname{FFTr}\left([\underbrace{g\_{1}  \ldots  g\_{1}}\_{d\_{1}}  \ldots \underbrace{g\_{n}  \ldots  g\_{n}}_{d\_{n}}]\right) $$   Ground truth $\mathbf{p}$ and $\mathbf{d}$ are used during training  and predicted $\hat{\mathbf{p}}$ and $\hat{\mathbf{d}}$ are used during inference. The model optimizes mean-squared error (MSE) between the predicted and ground-truth modalities  $$ \mathcal{L}=\|\hat{\mathbf{y}}-\mathbf{y}\|\_{2}^{2}+\alpha\|\hat{\mathbf{p}}-\mathbf{p}\|\_{2}^{2}+\gamma\|\hat{\mathbf{d}}-\mathbf{d}\|\_{2}^{2} $$ fastpitch-parallel-text-to-speech-with-pitch 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/convtasnet ConvTasNet Convolutional time-domain audio separation network Combines learned time-frequency representation with a masker architecture based on 1D [dilated convolution](https://paperswithcode.com/method/dilated-convolution). tasnet-surpassing-ideal-time-frequency 2000 None None https://github.com/asteroid-team/asteroid/blob/master/asteroid/models/conv_tasnet.py """;Audio
"""https://paperswithcode.com/method/convtasnet ConvTasNet Convolutional time-domain audio separation network Combines learned time-frequency representation with a masker architecture based on 1D [dilated convolution](https://paperswithcode.com/method/dilated-convolution). tasnet-surpassing-ideal-time-frequency 2000 None None https://github.com/asteroid-team/asteroid/blob/master/asteroid/models/conv_tasnet.py """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/jukebox Jukebox Jukebox **Jukebox** is a model that generates music with singing in the raw audio domain. It tackles the long context of raw audio using a multi-scale [VQ-VAE](https://paperswithcode.com/method/vq-vae) to compress it to discrete codes  and modeling those using [autoregressive Transformers](https://paperswithcode.com/methods/category/autoregressive-transformers). It can condition on artist and genre to steer the musical and vocal style  and on unaligned lyrics to make the singing more controllable.  Three separate VQ-VAE models are trained with different temporal resolutions. At each level  the input audio is segmented and encoded into latent vectors $\mathbf{h}\_{t}$  which are then quantized to the closest codebook vectors $\mathbf{e}\_{z\_{t}}$. The code $z\_{t}$ is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction  since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels  where the least abstract bottom-level codes result in the highest-quality audio. jukebox-a-generative-model-for-music-1 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/fastpitch FastPitch FastPitch **FastPitch** is a fully-parallel text-to-speech model based on FastSpeech  conditioned on fundamental frequency contours. The architecture of FastPitch is shown in the Figure. It is based on FastSpeech and composed mainly of two feed-forward [Transformer](https://paperswithcode.com/method/transformer) (FFTr) stacks. The first one operates in the resolution of input tokens  the second one in the resolution of the output frames. Let $x=\left(x\_{1}  \ldots  x\_{n}\right)$ be the sequence of input lexical units  and $\mathbf{y}=\left(y\_{1}  \ldots  y\_{t}\right)$ be the sequence of target mel-scale spectrogram frames. The first FFTr stack produces the hidden representation $\mathbf{h}=\operatorname{FFTr}(\mathbf{x})$. The hidden representation $h$ is used to make predictions about the duration and average pitch of every character with a 1-D CNN   $$ \hat{\mathbf{d}}=\text { DurationPredictor }(\mathbf{h})  \quad \hat{\mathbf{p}}=\operatorname{PitchPredictor}(\mathbf{h}) $$  where $\hat{\mathbf{d}} \in \mathbb{N}^{n}$ and $\hat{\mathbf{p}} \in \mathbb{R}^{n}$. Next  the pitch is projected to match the dimensionality of the hidden representation $h \in$ $\mathbb{R}^{n \times d}$ and added to $\mathbf{h}$. The resulting sum $\mathbf{g}$ is discretely upsampled and passed to the output FFTr  which produces the output mel-spectrogram sequence  $$ \mathbf{g}=\mathbf{h}+\operatorname{PitchEmbedding}(\mathbf{p}) $$  $$ \hat{\mathbf{y}}=\operatorname{FFTr}\left([\underbrace{g\_{1}  \ldots  g\_{1}}\_{d\_{1}}  \ldots \underbrace{g\_{n}  \ldots  g\_{n}}_{d\_{n}}]\right) $$   Ground truth $\mathbf{p}$ and $\mathbf{d}$ are used during training  and predicted $\hat{\mathbf{p}}$ and $\hat{\mathbf{d}}$ are used during inference. The model optimizes mean-squared error (MSE) between the predicted and ground-truth modalities  $$ \mathcal{L}=\|\hat{\mathbf{y}}-\mathbf{y}\|\_{2}^{2}+\alpha\|\hat{\mathbf{p}}-\mathbf{p}\|\_{2}^{2}+\gamma\|\hat{\mathbf{d}}-\mathbf{d}\|\_{2}^{2} $$ fastpitch-parallel-text-to-speech-with-pitch 2000 None None  """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/eend EEND End-to-End Neural Diarization **End-to-End Neural Diarization** is a neural network for speaker diarization in which a neural network directly outputs speaker diarization results given a multi-speaker recording. To realize such an end-to-end model  the speaker diarization problem is formulated as a multi-label classification problem and a permutation-free objective function is introduced to directly minimize diarization errors. The EEND method can explicitly handle speaker overlaps during training and inference. Just by feeding multi-speaker recordings with corresponding speaker segment labels  the model can be adapted to real conversations. end-to-end-neural-diarization-reformulating 2000 None None None """;Audio
"""https://paperswithcode.com/method/directional-sparse-filtering Directional Sparse Filtering Directional Sparse FIltering  learning-complex-valued-latent-filters-with 2000 None None None """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/jukebox Jukebox Jukebox **Jukebox** is a model that generates music with singing in the raw audio domain. It tackles the long context of raw audio using a multi-scale [VQ-VAE](https://paperswithcode.com/method/vq-vae) to compress it to discrete codes  and modeling those using [autoregressive Transformers](https://paperswithcode.com/methods/category/autoregressive-transformers). It can condition on artist and genre to steer the musical and vocal style  and on unaligned lyrics to make the singing more controllable.  Three separate VQ-VAE models are trained with different temporal resolutions. At each level  the input audio is segmented and encoded into latent vectors $\mathbf{h}\_{t}$  which are then quantized to the closest codebook vectors $\mathbf{e}\_{z\_{t}}$. The code $z\_{t}$ is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction  since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels  where the least abstract bottom-level codes result in the highest-quality audio. jukebox-a-generative-model-for-music-1 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/fastpitch FastPitch FastPitch **FastPitch** is a fully-parallel text-to-speech model based on FastSpeech  conditioned on fundamental frequency contours. The architecture of FastPitch is shown in the Figure. It is based on FastSpeech and composed mainly of two feed-forward [Transformer](https://paperswithcode.com/method/transformer) (FFTr) stacks. The first one operates in the resolution of input tokens  the second one in the resolution of the output frames. Let $x=\left(x\_{1}  \ldots  x\_{n}\right)$ be the sequence of input lexical units  and $\mathbf{y}=\left(y\_{1}  \ldots  y\_{t}\right)$ be the sequence of target mel-scale spectrogram frames. The first FFTr stack produces the hidden representation $\mathbf{h}=\operatorname{FFTr}(\mathbf{x})$. The hidden representation $h$ is used to make predictions about the duration and average pitch of every character with a 1-D CNN   $$ \hat{\mathbf{d}}=\text { DurationPredictor }(\mathbf{h})  \quad \hat{\mathbf{p}}=\operatorname{PitchPredictor}(\mathbf{h}) $$  where $\hat{\mathbf{d}} \in \mathbb{N}^{n}$ and $\hat{\mathbf{p}} \in \mathbb{R}^{n}$. Next  the pitch is projected to match the dimensionality of the hidden representation $h \in$ $\mathbb{R}^{n \times d}$ and added to $\mathbf{h}$. The resulting sum $\mathbf{g}$ is discretely upsampled and passed to the output FFTr  which produces the output mel-spectrogram sequence  $$ \mathbf{g}=\mathbf{h}+\operatorname{PitchEmbedding}(\mathbf{p}) $$  $$ \hat{\mathbf{y}}=\operatorname{FFTr}\left([\underbrace{g\_{1}  \ldots  g\_{1}}\_{d\_{1}}  \ldots \underbrace{g\_{n}  \ldots  g\_{n}}_{d\_{n}}]\right) $$   Ground truth $\mathbf{p}$ and $\mathbf{d}$ are used during training  and predicted $\hat{\mathbf{p}}$ and $\hat{\mathbf{d}}$ are used during inference. The model optimizes mean-squared error (MSE) between the predicted and ground-truth modalities  $$ \mathcal{L}=\|\hat{\mathbf{y}}-\mathbf{y}\|\_{2}^{2}+\alpha\|\hat{\mathbf{p}}-\mathbf{p}\|\_{2}^{2}+\gamma\|\hat{\mathbf{d}}-\mathbf{d}\|\_{2}^{2} $$ fastpitch-parallel-text-to-speech-with-pitch 2000 None None  """;Audio
"""https://paperswithcode.com/method/convtasnet ConvTasNet Convolutional time-domain audio separation network Combines learned time-frequency representation with a masker architecture based on 1D [dilated convolution](https://paperswithcode.com/method/dilated-convolution). tasnet-surpassing-ideal-time-frequency 2000 None None https://github.com/asteroid-team/asteroid/blob/master/asteroid/models/conv_tasnet.py """;Audio
"""https://paperswithcode.com/method/waveglow WaveGlow WaveGlow **WaveGlow** is a flow-based generative model that generates audio by sampling from a distribution. Specifically samples are taken from a zero mean spherical Gaussian with the same number of dimensions as our desired output  and those samples are put through a series of layers that transforms the simple distribution to one which has the desired distribution. waveglow-a-flow-based-generative-network-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad WaveGrad WaveGrad **WaveGrad** is a conditional model for waveform generation through estimating gradients of the data density. This model is built on the prior work on score matching and diffusion probabilistic models. It starts from Gaussian white noise and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad is non-autoregressive  and requires only a constant number of generation steps during inference. It can use as few as 6 iterations to generate high fidelity audio samples. wavegrad-estimating-gradients-for-waveform 2000 None None https://github.com/lmnt-com/wavegrad/blob/master/src/wavegrad/model.py """;Audio
"""https://paperswithcode.com/method/film-module FiLM Module FiLM Module The **Feature-wise linear modulation** (**FiLM**) module combines information from both noisy waveform and input mel-spectrogram. It is used in the [WaveGrad](https://paperswithcode.com/method/wavegrad) model. The authors also added iteration index $n$ which indicates the noise level of the input waveform by using the [Transformer](https://paperswithcode.com/method/transformer) sinusoidal positional embedding. To condition on the noise level directly  $n$ is replaced by $\sqrt{\bar{\alpha}}$ and a linear scale $C = 5000$ is applied. The FiLM module produces both scale and bias vectors given inputs  which are used in a UBlock for feature-wise affine transformation as:  $$ \gamma\left(D  \sqrt{\bar{\alpha}}\right) \odot U + \zeta\left(D  \sqrt{\bar{\alpha}}\right) $$  where $\gamma$ and $\zeta$ correspond to the scaling and shift vectors from the FiLM module  $D$ is the output from corresponding [DBlock](https://paperswithcode.com/method/dblock)  $U$ is an intermediate output in the UBlock. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad-dblock WaveGrad DBlock WaveGrad DBlock **WaveGrad DBlocks** are used to downsample the temporal dimension of noisy waveform in [WaveGrad](https://paperswithcode.com/method/wavegrad). They are similar to UBlocks except that only one [residual block](https://paperswithcode.com/method/residual-block) is included. The dilation factors are 1  2  4 in the main branch. Orthogonal initialization is used. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavegrad-ublock WaveGrad UBlock WaveGrad UBlock The **WaveGrad UBlock** is used for upsampling in [WaveGrad](https://paperswithcode.com/method/wavegrad). Neural audio generation models often use large receptive field. Dilation factors of four convolutional layers are 1  2  1  2 for the first two UBlocks and 1  2  4  8 for the rest. Orthogonal initialization is used. wavegrad-estimating-gradients-for-waveform 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2s FastSpeech 2s FastSpeech 2s **FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden  which makes it more compact in inference by discarding the mel-spectrogram decoder.  Two main design changes are made to the waveform decoder.   First  considering that the phase information is difficult to predict using a variance predictor  [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself.   Secondly  the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged  which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure  the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN  which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN.   In inference  the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/fastspeech-2 FastSpeech 2 FastSpeech 2 **FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS  i.e.  multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher  and 2) introducing more variation information of speech (e.g.  pitch  energy and more accurate duration) as conditional inputs. Specifically  in FastSpeech 2  we extract duration  pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.  The encoder converts the phoneme embedding sequence into the phoneme hidden sequence  and then the variance adaptor adds different variance information such as duration  pitch and energy into the hidden sequence  finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block  which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech  as the basic structure for the encoder and mel-spectrogram decoder. fastspeech-2-fast-and-high-quality-end-to-end 2000 None None  """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/jukebox Jukebox Jukebox **Jukebox** is a model that generates music with singing in the raw audio domain. It tackles the long context of raw audio using a multi-scale [VQ-VAE](https://paperswithcode.com/method/vq-vae) to compress it to discrete codes  and modeling those using [autoregressive Transformers](https://paperswithcode.com/methods/category/autoregressive-transformers). It can condition on artist and genre to steer the musical and vocal style  and on unaligned lyrics to make the singing more controllable.  Three separate VQ-VAE models are trained with different temporal resolutions. At each level  the input audio is segmented and encoded into latent vectors $\mathbf{h}\_{t}$  which are then quantized to the closest codebook vectors $\mathbf{e}\_{z\_{t}}$. The code $z\_{t}$ is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction  since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels  where the least abstract bottom-level codes result in the highest-quality audio. jukebox-a-generative-model-for-music-1 2000 None None  """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/tacotron-2 Tacotron 2 Tacotron2 **Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:  - a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence - a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames  In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron)  Tacotron 2 uses simpler building blocks  using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”  i.e.  each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention). natural-tts-synthesis-by-conditioning-wavenet 2000 None None https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457 """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/phase-shuffle Phase Shuffle Phase Shuffle **Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.  In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan)  the authors only apply phase shuffle to the discriminator  as the latent vector already provides the generator a mechanism to manipulate the phase of a resultant waveform. Intuitively speaking  phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavegan WaveGAN WaveGAN **WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms).   The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field  using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5  and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way  using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters  numerical operations  and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:  1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D). 2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4). 3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator. 4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/specgan SpecGAN SpecGAN **SpecGAN** is a generative adversarial network method for spectrogram-based  frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted.   To process audio into suitable spectrograms  the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride  resulting in 128 frequency bins  linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\left[−1  1\right]$.  They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra. adversarial-audio-synthesis 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/bridge-net Bridge-net Bridge-net **Bridge-net** is an audio model block used in the [ClariNet](https://paperswithcode.com/method/clarinet) text-to-speech architecture. Bridge-net maps frame-level hidden representation to sample-level through several [convolution](https://paperswithcode.com/method/convolution) blocks and [transposed convolution](https://paperswithcode.com/method/transposed-convolution) layers interleaved with softsign non-linearities. clarinet-parallel-wave-generation-in-end-to 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavevae WaveVAE WaveVAE **WaveVAE** is a generative audio model that can be used as a vocoder in text-to-speech systems. It is a [VAE](https://paperswithcode.com/method/vae) based model that can be trained from scratch by jointly optimizing the encoder $q\_{\phi}\left(\mathbf{z}|\mathbf{x}  \mathbf{c}\right)$ and decoder $p\_{\theta}\left(\mathbf{x}|\mathbf{z}  \mathbf{c}\right)$  where $\mathbf{z}$ is latent variables and $\mathbf{c}$ is the mel spectrogram conditioner.   The encoder of WaveVAE $q\_{\phi}\left(\mathbf{z}|\mathbf{x}\right)$ is parameterized by a Gaussian autoregressive [WaveNet](https://paperswithcode.com/method/wavenet) that maps the ground truth audio x into the same length latent representation $\mathbf{z}$. The decoder $p\_{\theta}\left(\mathbf{x}|\mathbf{z}\right)$ is parameterized by the one-step ahead predictions from an inverse autoregressive flow.  The training objective is the ELBO for the observed $\mathbf{x}$ in the VAE. parallel-neural-text-to-speech 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan MelGAN MelGAN **MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at a 256× lower temporal resolution  the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs  the MelGAN generator does not use a global noise vector as input.  To deal with 'checkerboard artifacts' in audio  instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle)  MelGAN uses kernel-size as a multiple of stride.  [Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator)  similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator. melgan-generative-adversarial-networks-for 2000 None None None """;Audio
"""https://paperswithcode.com/method/melgan-residual-block MelGAN Residual Block MelGAN Residual Block The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps  leading to better long range correlation. melgan-generative-adversarial-networks-for 2000 None None https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72 """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/ddsp DDSP Differentiable Digital Signal Processing  ddsp-differentiable-digital-signal-processing-1 2000 None None None """;Audio
"""https://paperswithcode.com/method/wavenet WaveNet WaveNet **WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation  architectures are developed based on dilated causal convolutions  which exhibit very large receptive fields.  The joint probability of a waveform $\vec{x} = \{ x_1  \dots  x_T \}$ is factorised as a product of conditional probabilities as follows:  $$p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1  \dots  x_{t-1}\right)$$  Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps. wavenet-a-generative-model-for-raw-audio 2000 None None None """;Audio
"""https://paperswithcode.com/method/sepformer SepFormer SepFormer **SepFormer** is [Transformer](https://paperswithcode.com/methods/category/transformers)-based neural network for speech separation. The SepFormer learns short and long-term dependencies with a multi-scale approach that employs transformers. It is mainly composed of multi-head attention and feed-forward layers. A dual-path framework (introduced by DPRNN) is adopted and [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) are replaced with a multiscale pipeline composed of transformers that learn both short and long-term dependencies. The dual-path framework enables the mitigation of the quadratic complexity of transformers  as transformers in the dual-path framework process smaller chunks.  The model is based on the learned-domain masking approach and employs an encoder  a decoder  and a masking network  as shown in the figure. The encoder is fully convolutional  while the decoder employs two Transformers embedded inside the dual-path processing block. The decoder finally reconstructs the separated signals in the time domain by using the masks predicted by the masking network. attention-is-all-you-need-in-speech 2000 None None  """;Audio
"""https://paperswithcode.com/method/convtasnet ConvTasNet Convolutional time-domain audio separation network Combines learned time-frequency representation with a masker architecture based on 1D [dilated convolution](https://paperswithcode.com/method/dilated-convolution). tasnet-surpassing-ideal-time-frequency 2000 None None https://github.com/asteroid-team/asteroid/blob/master/asteroid/models/conv_tasnet.py """;Audio
