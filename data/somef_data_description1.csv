Text;Label;Repo
"""We provide **RecAdam** (Recall Adam) optimizer to facilitate fine-tuning deep pretrained language models (e.g.  BERT  ALBERT) with less forgetting.  For a detailed description and experimental results  please refer to our paper: [Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting](https://www.aclweb.org/anthology/2020.emnlp-main.634/) (Accepted by EMNLP 2020).   """;Natural Language Processing;https://github.com/Sanyuan-Chen/RecAdam
"""In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   """;General;https://github.com/yyht/Conv_Bert
"""In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   """;Natural Language Processing;https://github.com/yyht/Conv_Bert
"""In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   """;Sequential;https://github.com/yyht/Conv_Bert
"""Following project was a part of my master thesis. Current version is a little bit modified and  much improved. The project is using modified algorithm Deterministic Policy Gradient  (Lillicrap et al.[arXiv:1509.02971](https://arxiv.org/pdf/1509.02971.pdf))  (written in Tensorflow) to control mobile robot. The main idea was to learn mobile robot navigate  to goal and also avoid obstacles. For obstacles avoidance  robot is using 5 ultrasonic sensors.   For navigation task  required information (like absolute pose) are taken from simulation   engine. That's a little hack. However  more realistic (like odometry) pose estimation can be    found in [gym-vrep](https://github.com/Souphis/gym-vrep).   """;Reinforcement Learning;https://github.com/Souphis/mobile_robot_rl
"""![](https://user-images.githubusercontent.com/10624937/43851024-320ba930-9aff-11e8-8493-ee547c6af349.gif ""Trained Agent"")  In this environment  a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus  the goal of your agent is to maintain its position at the target location for as many time steps as possible.  The observation space consists of 33 variables corresponding to position  rotation  velocity  and angular velocities of the arm. Each action is a vector with four numbers  corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.  This projects implements DDPG for continous control for the [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment.   """;Reinforcement Learning;https://github.com/prajwalgatti/DRL-Continuous-Control
"""You can create WordNet noun hypernym pairs as follows  ```shell python ../scripts/create_wordnet_noun_hierarchy.py ./wordnet_noun_hypernyms.tsv ```  and mammal subtree is created by  ```shell python ../scripts/create_mammal_subtree.py ./mammal_subtree.tsv ```   """;Natural Language Processing;https://github.com/TatsuyaShirakawa/poincare-embedding
"""This repository contains the code for the blog post: [Using Microsoft AI to Build a Lung-Disease Prediction Model using Chest X-Ray Images](https://blogs.technet.microsoft.com/machinelearning/2018/03/07/using-microsoft-ai-to-build-a-lung-disease-prediction-model-using-chest-x-ray-images/)  by Xiaoyong Zhu  George Iordanescu  Ilia Karmanov  data scientists from Microsoft  and Mazen Zawaideh  radiologist resident from University of Washington Medical Center.  In this repostory  we provide you the Keras code (`001-003 Jupyter Notebooks under AzureChestXRay_AMLWB\Code\02_Model`) and PyTorch code (`AzureChestXRay_AMLWB\Code\02_Model060_Train_pyTorch`). You should be able to run the code from scratch and get the below result using Azure Machine Learning platform or run it using your own GPU machine.   """;Computer Vision;https://github.com/Azure/AzureChestXRay
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/qilei123/DeformableConvV2
"""This is implementation of real-valued non-volume preserving(real NVP) for Density Estimation(real NVP). I used Two Moons dataset of sklearn and tried to reproduce fig.1 in the paper.    """;Computer Vision;https://github.com/ANLGBOY/RealNVP-with-PyTorch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/chen-xiong-yi/OwnBERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/chen-xiong-yi/OwnBERT
"""Fully Convolutional Networks (FCNs) are a natural extension of CNNs to tackle per pixel prediction problems such as semantic image segmentation. FCNs add upsampling layers to standard CNNs to recover the spatial resolution of the input at the output layer. In  order to compensate for the resolution loss induced by pooling layers  FCNs introduce skip connections between their downsampling  and upsampling paths. Skip connections help the upsampling path recover fine-grained information from the downsampling layers.  One evolution of CNNs are [Residual Networks](https://arxiv.org/abs/1512.03385) (ResNets). ResNets are designed to ease the training of  very deep networks by introducing a residual block that sums the non-linear transformation of the input and its identity mapping.  The identity mapping is implemented by means of a shortcut connection. ResNets can be extended to work as FCNs. ResNets incorporate  shortcut paths to FCNs and increase the number of connections within a network. This additional shortcut paths improve the segmentation  accuracy and also help the network to converge faster.  Recently another CNN architecture called [DenseNet](https://arxiv.org/abs/1608.06993) has been introduced. DenseNets are built from  *dense blocks* and pooling operations  where each dense block is an iterative concatenation of previous feature maps. This architecture  can be seen as an extension of ResNets  which performs iterative summation of previous feature maps. The result of this modification  is that DenseNets are more efficient in there parameter usage.  The [https://arxiv.org/abs/1611.09326](https://arxiv.org/abs/1611.09326) paper extends DenseNets to work as FCNs by adding an upsampling  path to recover the full input resolution.    """;General;https://github.com/asprenger/keras_fc_densenet
"""Fully Convolutional Networks (FCNs) are a natural extension of CNNs to tackle per pixel prediction problems such as semantic image segmentation. FCNs add upsampling layers to standard CNNs to recover the spatial resolution of the input at the output layer. In  order to compensate for the resolution loss induced by pooling layers  FCNs introduce skip connections between their downsampling  and upsampling paths. Skip connections help the upsampling path recover fine-grained information from the downsampling layers.  One evolution of CNNs are [Residual Networks](https://arxiv.org/abs/1512.03385) (ResNets). ResNets are designed to ease the training of  very deep networks by introducing a residual block that sums the non-linear transformation of the input and its identity mapping.  The identity mapping is implemented by means of a shortcut connection. ResNets can be extended to work as FCNs. ResNets incorporate  shortcut paths to FCNs and increase the number of connections within a network. This additional shortcut paths improve the segmentation  accuracy and also help the network to converge faster.  Recently another CNN architecture called [DenseNet](https://arxiv.org/abs/1608.06993) has been introduced. DenseNets are built from  *dense blocks* and pooling operations  where each dense block is an iterative concatenation of previous feature maps. This architecture  can be seen as an extension of ResNets  which performs iterative summation of previous feature maps. The result of this modification  is that DenseNets are more efficient in there parameter usage.  The [https://arxiv.org/abs/1611.09326](https://arxiv.org/abs/1611.09326) paper extends DenseNets to work as FCNs by adding an upsampling  path to recover the full input resolution.    """;Computer Vision;https://github.com/asprenger/keras_fc_densenet
"""A summary of the performance can be produced by invoking the following command from inside the ```my_project``` folder or ```predictions``` sub-folder:  ``` mp summary  >> [***] SUMMARY REPORT FOR FOLDER [***] >> ./my_project/predictions/csv/ >>  >>  >> Per class: >> -------------------------------- >>    Mean dice by class  +/- STD    min    max   N >> 1               0.856    0.060  0.672  0.912  34 >> 2               0.891    0.029  0.827  0.934  34 >> 3               0.888    0.027  0.829  0.930  34 >> 4               0.802    0.164  0.261  0.943  34 >> 5               0.819    0.075  0.552  0.926  34 >> 6               0.863    0.047  0.663  0.917  34 >>  >> Overall mean: 0.853 +- 0.088 >> -------------------------------- >>  >> By views: >> -------------------------------- >> [0.8477811  0.50449719 0.16355361]          0.825 >> [ 0.70659414 -0.35532932  0.6119361 ]       0.819 >> [ 0.11799461 -0.07137918  0.9904455 ]       0.772 >> [ 0.95572575 -0.28795306  0.06059151]       0.827 >> [-0.16704373 -0.96459936  0.20406974]       0.810 >> [-0.72188903  0.68418977  0.10373322]       0.819 >> -------------------------------- ```   """;Computer Vision;https://github.com/perslev/MultiPlanarUNet
"""Large-Scale Learnable Graph Convolutional Networks provide an efficient way (LGCL and LGCN) for learnable graph convolution.  Detailed information about LGCL and LGCN is provided in (https://dl.acm.org/citation.cfm?id=3219947).   """;Graphs;https://github.com/divelab/lgcn
"""Code adapted from https://github.com/chrischute/glow Adding conditioning layer to affine coupling layer. Tried conditioning for many domain. Applied style transfer using the property of conditional flow model. (reconstruct image giving different condition in forward and reverse procedure of Glow)    """;Computer Vision;https://github.com/5yearsKim/Conditional-Normalizing-Flow
"""Code adapted from https://github.com/chrischute/glow Adding conditioning layer to affine coupling layer. Tried conditioning for many domain. Applied style transfer using the property of conditional flow model. (reconstruct image giving different condition in forward and reverse procedure of Glow)    """;General;https://github.com/5yearsKim/Conditional-Normalizing-Flow
"""![FeatureVis](assets/FeatureVis.png)  In this work  we design a new loss function which merges the merits of both [NormFace](https://github.com/happynear/NormFace) and [SphereFace](https://github.com/wy1iu/sphereface). It is much easier to understand and train  and outperforms the previous state-of-the-art loss function (SphereFace) by 2-5% on MegaFace.    """;General;https://github.com/happynear/AMSoftmax
"""Word2Vec is a popular word embedding model which is used in major NLP models. Instead of representing every word in the document as one-hot vector  we learn the representation of word in form of an embedding based on the context and semantics around it  For more information  please follow https://arxiv.org/pdf/1301.3781.pdf    """;Natural Language Processing;https://github.com/rohith2506/word_embeddings
"""Due to the speed limitation of 20 FPS  we started with [YOLOv2-Tiny detector](https://pjreddie.com/darknet/yolov2/)  which consists of a backbone network for feature extraction and a detection network for candidate bounding box generation. Considering that there is no need to classify in our task  we reduced the detection network to a location network  in which a candidate bounding box is only represented by a confidence socre and a position.  However  with such a simple model  we were soon faced with the challenges of tiny objects  occlusions and distractions from the provided data set. In order to tackle to the aforementioned challenges  we investigated various network architectures for both training and inference.   <p align=""center""> <img src=""https://raw.githubusercontent.com/jndeng/DACSDC-DeepZ/master/Train/cfg/architecture.png"" alt=""network architecture"" width=""380px"" height=""400px""> </p>  We later combined [Feature Pyramid Network](https://arxiv.org/abs/1612.03144v2) to fuse fine-grained features with strong semantic features to enhance the ability in detecting small objects. Meanwhile  we utilized [Focal Loss](https://arxiv.org/abs/1708.02002) function to mitigate the imbalance between the single ground truth box and the candidate boxes at training phase  thereby partially resolving occlusions and distractions. With the combined techniques  we achieved the inference network as shown in the figure with an accuracy improvement of ~ 0.042.   Moreover  we used multithreading to accelerate the process of prediction by loading images and infering in parallel  which improved about 7 FPS on NVIDIA Jetson TX2.   The performance of our model is as follow:  | Self-Test Accuracy (mean IoU) | Organizer-Test Accuracy (mean IoU) | Speed (FPS on Jetson TX2) |:-----:|:-----:|:-----:| | 0.866 | 0.691 | ~25 |  **Note:**    We develop two projects for different purposes in this repository. Project `Train` is mainly used for model training and accuracy evaluation on powerful GPU(NVIDIA Titan X Pascal in our experiments). While project `Inference` is dedicated to inference on embedded GPU(NVIDIA Jetson TX2) with better optimization in speed and energy consumption.    """;Computer Vision;https://github.com/jndeng/DACSDC-DeepZ
"""Due to the speed limitation of 20 FPS  we started with [YOLOv2-Tiny detector](https://pjreddie.com/darknet/yolov2/)  which consists of a backbone network for feature extraction and a detection network for candidate bounding box generation. Considering that there is no need to classify in our task  we reduced the detection network to a location network  in which a candidate bounding box is only represented by a confidence socre and a position.  However  with such a simple model  we were soon faced with the challenges of tiny objects  occlusions and distractions from the provided data set. In order to tackle to the aforementioned challenges  we investigated various network architectures for both training and inference.   <p align=""center""> <img src=""https://raw.githubusercontent.com/jndeng/DACSDC-DeepZ/master/Train/cfg/architecture.png"" alt=""network architecture"" width=""380px"" height=""400px""> </p>  We later combined [Feature Pyramid Network](https://arxiv.org/abs/1612.03144v2) to fuse fine-grained features with strong semantic features to enhance the ability in detecting small objects. Meanwhile  we utilized [Focal Loss](https://arxiv.org/abs/1708.02002) function to mitigate the imbalance between the single ground truth box and the candidate boxes at training phase  thereby partially resolving occlusions and distractions. With the combined techniques  we achieved the inference network as shown in the figure with an accuracy improvement of ~ 0.042.   Moreover  we used multithreading to accelerate the process of prediction by loading images and infering in parallel  which improved about 7 FPS on NVIDIA Jetson TX2.   The performance of our model is as follow:  | Self-Test Accuracy (mean IoU) | Organizer-Test Accuracy (mean IoU) | Speed (FPS on Jetson TX2) |:-----:|:-----:|:-----:| | 0.866 | 0.691 | ~25 |  **Note:**    We develop two projects for different purposes in this repository. Project `Train` is mainly used for model training and accuracy evaluation on powerful GPU(NVIDIA Titan X Pascal in our experiments). While project `Inference` is dedicated to inference on embedded GPU(NVIDIA Jetson TX2) with better optimization in speed and energy consumption.    """;General;https://github.com/jndeng/DACSDC-DeepZ
"""[Remote sensing](https://www.usgs.gov/faqs/what-remote-sensing-and-what-it-used) is the science of obtaining information about objects or areas from a distance  typically from aircraft or satellites.  We realized the problem of satellite image classification as a [semantic segmentation](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) problem and built semantic segmentation algorithms in deep learning to tackle this.   """;Computer Vision;https://github.com/manideep2510/eye-in-the-sky
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/nachiketaa/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/nachiketaa/bert
"""This is the pytorch implementation of Paper: Image Inpainting With Learnable Bidirectional Attention Maps (ICCV 2019) [paper](http://openaccess.thecvf.com/content_ICCV_2019/papers/Xie_Image_Inpainting_With_Learnable_Bidirectional_Attention_Maps_ICCV_2019_paper.pdf) [suppl](http://openaccess.thecvf.com/content_ICCV_2019/supplemental/Xie_Image_Inpainting_With_ICCV_2019_supplemental.pdf)   """;Computer Vision;https://github.com/Vious/LBAM_Pytorch
"""This is the pytorch implementation of Paper: Image Inpainting With Learnable Bidirectional Attention Maps (ICCV 2019) [paper](http://openaccess.thecvf.com/content_ICCV_2019/papers/Xie_Image_Inpainting_With_Learnable_Bidirectional_Attention_Maps_ICCV_2019_paper.pdf) [suppl](http://openaccess.thecvf.com/content_ICCV_2019/supplemental/Xie_Image_Inpainting_With_ICCV_2019_supplemental.pdf)   """;General;https://github.com/Vious/LBAM_Pytorch
"""Please follow [faster-rcnn](https://github.com/jwyang/faster-rcnn.pytorch/tree/pytorch-1.0) respository to setup the environment. In this project  we use Pytorch 1.0.1 and CUDA version is 10.0.130.    """;Computer Vision;https://github.com/chaoqichen/HTCN
"""Deep learning  also known as hierarchical learning or deep structured learning   is a type of machine learning that uses a layered algorithmic architecture to analyze data.  Unlike other types of machine learning  deep learning has an added advantage of being able to make decisions with  significantly less human intervention. While basic machine learning requires a programmer to identify whether a conclusion  is correct or not  deep learning can gauge the accuracy of its answers on its own due to the nature of its multi-layered structure.  The emergence of modern frameworks like PyTorch  has also made preprocessing of data more convenient.  Many of the filtering and normalization tasks that would involve a lot of manual tasks while using other machine learning techniques  are taken up automatically.  The essential characteristics of deep learning make it an ideal tool for giving the much needed impetus   to the field of automated medical diagnosis. With the right expertise  it can be leveraged to overcome several  limitations of conventional diagnosis done by medical practitioners  and take the dream of accurate and efficient  automated disease diagnosis to the realm of reality.  Given the team's vision to make healthcare better for everyone  everywhere  and having paid attention to the trends and  recent breakthroughs with deep learning  we decided to experiment with several variations of convolutional neural networks for this project.  Recent work has shown that convolutional networks can be substantially deeper  more accurate   and efficient to train if they contain shorter connections between layers close to the input and output.  We embraced this observation  and leveraged the power of the Dense Convolutional Network (DenseNet)   which connects each layer to every other layer in a feed-forward fashion.  Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer -  our network had L(L+1)/2 direct connections. We also experimented with other architectures  including residual networks  and used our  model variations as controls to validate each other.    """;Computer Vision;https://github.com/SGNovice/Disease-detection-using-chest-xrays
"""My name is Sidney Kingsley and I am a third year student studying Digital Media Design (BSc) at the University of Plymouth. This is the git repository for my final year project. ======= My name is Sidney Kingsley and I am a final year student studying Digital Media Design (BSc) at the University of Plymouth. This is the git repository for my final year project. >>>>>>> e51bd872f2250558bfe7086cc4e07157e3c87de2  My final year project _(FYP)_ focuses on the **web** application of **machine learning** _(ML)_ to create  understand and interact with art.  """;Computer Vision;https://github.com/sidneykingsley/fyp
"""My name is Sidney Kingsley and I am a third year student studying Digital Media Design (BSc) at the University of Plymouth. This is the git repository for my final year project. ======= My name is Sidney Kingsley and I am a final year student studying Digital Media Design (BSc) at the University of Plymouth. This is the git repository for my final year project. >>>>>>> e51bd872f2250558bfe7086cc4e07157e3c87de2  My final year project _(FYP)_ focuses on the **web** application of **machine learning** _(ML)_ to create  understand and interact with art.  """;General;https://github.com/sidneykingsley/fyp
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/yrouphail/yolo-independent
"""Machine translation is a natural language processing task that aims to translate natural languages using computers automatically. Recent several years have witnessed the rapid development of end-to-end neural machine translation  which has become the new mainstream method in practical MT systems.  THUMT is an open-source toolkit for neural machine translation developed by [the Natural Language Processing Group at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/site2/index.php?lang=en).    """;General;https://github.com/insigh/THUMT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  BERT is based on the Transformer architecture introduced in [Attention is all you need](https://arxiv.org/abs/1706.03762).  Googles academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  Googles Github repository where the original english models can be found here: [https://github.com/google-research/bert](https://github.com/google-research/bert).  Included in the downloads below are PyTorch versions of the models based on the work of  NLP researchers from HuggingFace. [PyTorch version of BERT available](https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/)   """;General;https://github.com/af-ai-center/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  BERT is based on the Transformer architecture introduced in [Attention is all you need](https://arxiv.org/abs/1706.03762).  Googles academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  Googles Github repository where the original english models can be found here: [https://github.com/google-research/bert](https://github.com/google-research/bert).  Included in the downloads below are PyTorch versions of the models based on the work of  NLP researchers from HuggingFace. [PyTorch version of BERT available](https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/)   """;Natural Language Processing;https://github.com/af-ai-center/bert
"""This project makes use of the freely-available [Million Song Dataset](http://millionsongdataset.com/)  and its integration with the [Last.fm Dataset](http://millionsongdataset.com/lastfm/). The former provides a link between all the useful information about the tracks (such as title  artist or year) and the audio track themselves  whereas the latter contains tags information on some of the tracks. A preview of the audio tracks can be fetched from services such as 7Digital  but this is allegedly not an easy task.   If you are only interested in our final results  click [here](https://github.com/pukkapies/urop2019#results).  If you want to use some of our code  or try to re-train our model on your own  read on. We will assume you have access to the actual songs in the dataset. Here is the outline of the approach we followed:  1. Extracte all the useful information from the Million Song Dataset and clean both the audio tracks and the Last.fm tags database to produce our final 'clean' data;  2. Prepare a flexible data input pipeline and transform the data in a format which is easy to consume by the training algorithm;  3. Prepare a flexible training script which would allow for multiple experiments (such as slightly different architectures  slightly different versions of the tags database  or different training parameters);  4. Train our model and use it to make sensible tag predictions from a given input audio.  In the following sections  we will provide a brief tutorial of how you may use this repository to make genre predictions of your own  or carry out some further experiments.   """;General;https://github.com/pukkapies/urop2019
"""This project provides a PyTorch implementation about [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) based on [fairseq-py](https://github.com/facebookresearch/fairseq-py) (An official toolkit of facebook research). You can also use official code about *Attention is all you need* from [tensor2tensor](https://github.com/tensorflow/tensor2tensor).  If you use this code about cnn  please cite: ``` @inproceedings{gehring2017convs2s    author    = {Gehring  Jonas  and Auli  Michael and Grangier  David and Yarats  Denis and Dauphin  Yann N}    title     = ""{Convolutional Sequence to Sequence Learning}""    booktitle = {Proc. of ICML}    year      = 2017  } ``` And if you use this code about transformer  please cite: ``` @inproceedings{46201    title = {Attention is All You Need}    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin}    year  = {2017}    booktitle = {Proc. of NIPS}  } ``` Feel grateful for the contribution of the facebook research and google research. **Besides  if you get benefits from this repository  please give me a star.**   """;General;https://github.com/StillKeepTry/Transformer-PyTorch
"""This project provides a PyTorch implementation about [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) based on [fairseq-py](https://github.com/facebookresearch/fairseq-py) (An official toolkit of facebook research). You can also use official code about *Attention is all you need* from [tensor2tensor](https://github.com/tensorflow/tensor2tensor).  If you use this code about cnn  please cite: ``` @inproceedings{gehring2017convs2s    author    = {Gehring  Jonas  and Auli  Michael and Grangier  David and Yarats  Denis and Dauphin  Yann N}    title     = ""{Convolutional Sequence to Sequence Learning}""    booktitle = {Proc. of ICML}    year      = 2017  } ``` And if you use this code about transformer  please cite: ``` @inproceedings{46201    title = {Attention is All You Need}    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin}    year  = {2017}    booktitle = {Proc. of NIPS}  } ``` Feel grateful for the contribution of the facebook research and google research. **Besides  if you get benefits from this repository  please give me a star.**   """;Natural Language Processing;https://github.com/StillKeepTry/Transformer-PyTorch
"""This model consists of 2 generators and 2 discriminators. The two generators as U-net like CNNs. During the evaluation of the model  I directly used the pretrained salient objective detection model from Joker  https://github.com/Joker316701882/Salient-Object-Detection.  This is a project about image style transfer developed by Tao Liang  Tianrui Yu  Ke Han and Yifan Ruan. Our project contains three different models  one is in ""cycle_gan_unet"" directory which uses the u-net like cnn as generators  one is in ""Ukiyoe_codes"" directory which uses Resnet blocks as generators  which uses the model proposed in this paper https://arxiv.org/pdf/1703.10593.pdf  the other is in neural_style_transfer that implement sytle transfer using convolution neural network proposed in this paper https://arxiv.org/pdf/1508.06576.pdf.   """;Computer Vision;https://github.com/CarpdiemLiang/style_transfer
"""This model consists of 2 generators and 2 discriminators. The two generators as U-net like CNNs. During the evaluation of the model  I directly used the pretrained salient objective detection model from Joker  https://github.com/Joker316701882/Salient-Object-Detection.  This is a project about image style transfer developed by Tao Liang  Tianrui Yu  Ke Han and Yifan Ruan. Our project contains three different models  one is in ""cycle_gan_unet"" directory which uses the u-net like cnn as generators  one is in ""Ukiyoe_codes"" directory which uses Resnet blocks as generators  which uses the model proposed in this paper https://arxiv.org/pdf/1703.10593.pdf  the other is in neural_style_transfer that implement sytle transfer using convolution neural network proposed in this paper https://arxiv.org/pdf/1508.06576.pdf.   """;General;https://github.com/CarpdiemLiang/style_transfer
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/brightmart/bert_customized
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/brightmart/bert_customized
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Zehui127/SQUAD_BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Zehui127/SQUAD_BERT
"""The data set could found [Allen Institute for AI ARC](https://leaderboard.allenai.org/arc/submissions/public). The dataset contains 7 787 natural grade-school level multiple-choice SCIENCE questions. This dataset's level of difficulty requires far more powerful knowledge and reasoning capability than ever before datasets such SQuAD or SNLI. The data set has two partitions: EASY Set and CHALLENGE Set. And inside each set  it is also devided into train  test and development sets. A corpus is also given in the dataset which could be used as background inforamtion source. But the ARC challenge is not limited to this corpus knowledge and it could also be open book.  <b> Easy: </b>   Easy-Train Set: 2251 questions   Easy-Test Set: 2376 questions   Easy-Development Set: 570 questions    <b> Challenge: </b>   The Challenge Set contains only questions answer incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm.    Challenge-Train Set: 1119 questions   Challenge-Test Set: 1172 questions   Challenge-Development Set: 299 questions    <b> Reference: </b>   P. Clark  I. Cowhey  O. Etzioni  T. Khot  A. Sabharwal  C. Schoenick  and O. Tafjord. 2018. Think you have solved question answering? Try ARC  the AI2 reasoning challenge. CoRR  abs/1803.05457.  <b> Example: </b>   EASY: Which technology was developed most recently?   &nbsp; &nbsp; A. cellular telephone(correct)   &nbsp; &nbsp; B. television   &nbsp; &nbsp; C. refrigerator   &nbsp; &nbsp; D. airplane    CHALLENGE: Which technology was developed most recently?   &nbsp; &nbsp; A. cellular telephone   &nbsp; &nbsp; B. television   &nbsp; &nbsp; C. refrigerator   &nbsp; &nbsp; D. airplane (correct)   1. T5 model: t5_test.ipynb and t5_ARC.ipynb  2. BERT baseline model: arc_easy_BERT_base_model.ipynb and arc_challenge_BERT_base_model.ipynb  3. RoBERTa-base without/without knowl-edge: LSH_attention.ipynb  4. Report for the project: CSE_576_2020Spring_Project_ARC.pdf   """;Natural Language Processing;https://github.com/duanchi1230/NLP_Project_AI2_Reasoning_Challenge
"""Generative Adversarial Networks (GANs) are one of the most interesting ideas in computer science today. Here two models named Generator and Discriminator are trained simultaneously. As the name says Generator generates the fake images or we can say it generates a random noise and the Discriminator job is to classify whether the image is fake or not. Here the only job of Generator is to fake the Discriminator. In this project we are using DCGAN(Deep Convolutional Generative Adversarial Network). A DCGAN is a direct extension of the GAN described above  except that it explicitly uses convolutional and convolutional-transpose layers in the discriminator and generator  respectively. DCGANs actually comes under Unsupervised Learning and was first described by Radford et. al. in the paper Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks.  ![](https://miro.medium.com/max/2850/1*Mw2c3eY5khtXafe5W-Ms_w.jpeg)   """;Computer Vision;https://github.com/hunnurjirao/DCGAN
"""Deep reinforcement learning has made significant strides in recent years  with results achieved in board games such as Go. However  there are a number of obstacles preventing such methods from being applied to more real-world situations. For instance  more realistic strategic situations often involve much larger spaces of possible states and actions  an environment state which is only partially observed  multiple agents to control  and a necessity for long-term strategies involving not hundreds but thousands or tens of thousands of steps. It has thus been suggested that creating learning algorithms which outperform humans in playing real-time strategy (RTS) video games would signal a more generalizable result about the ability of a computer to make decisions in the real world.  Of the current RTS games on the market  StarCraft II is one of the most popular. The recent release by Google’s DeepMind of SC2LE (StarCraft II Learning Environment) presents an interface with which to train deep reinforcement learners to compete in the game  both in smaller “minigames” and on full matches. The SC2LE environment is described on [DeepMind's github repo.](https://github.com/deepmind/pysc2)   In this project  we focus on solving a variety of minigames  which capture various aspects of the full StarCraft II game. These minigames focus on tasks such as gathering resources  moving to waypoints  finding enemies  or skirmishing with units. In each case the player is given a homogeneous set of units (marines)  and a reward is based off the minigame (+5 for defeating each enemy roach in DefeatRoaches  for example).   """;Reinforcement Learning;https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames
"""Deep reinforcement learning has made significant strides in recent years  with results achieved in board games such as Go. However  there are a number of obstacles preventing such methods from being applied to more real-world situations. For instance  more realistic strategic situations often involve much larger spaces of possible states and actions  an environment state which is only partially observed  multiple agents to control  and a necessity for long-term strategies involving not hundreds but thousands or tens of thousands of steps. It has thus been suggested that creating learning algorithms which outperform humans in playing real-time strategy (RTS) video games would signal a more generalizable result about the ability of a computer to make decisions in the real world.  Of the current RTS games on the market  StarCraft II is one of the most popular. The recent release by Google’s DeepMind of SC2LE (StarCraft II Learning Environment) presents an interface with which to train deep reinforcement learners to compete in the game  both in smaller “minigames” and on full matches. The SC2LE environment is described on [DeepMind's github repo.](https://github.com/deepmind/pysc2)   In this project  we focus on solving a variety of minigames  which capture various aspects of the full StarCraft II game. These minigames focus on tasks such as gathering resources  moving to waypoints  finding enemies  or skirmishing with units. In each case the player is given a homogeneous set of units (marines)  and a reward is based off the minigame (+5 for defeating each enemy roach in DefeatRoaches  for example).   """;General;https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames
"""![Block](imgs/diagonal_illustration.png)  AdaHessian is a second order based optimizer for the neural network training based on PyTorch. The library supports the training of convolutional neural networks ([image_classification](https://github.com/amirgholami/adahessian/tree/master/image_classification)) and transformer-based models ([transformer](https://github.com/amirgholami/adahessian/tree/master/transformer)). Our TensorFlow implementation is [adahessian_tf](https://github.com/amirgholami/adahessian/tree/master/adahessian_tf).  Please see [this paper](https://arxiv.org/pdf/2006.00719.pdf) for more details on the AdaHessian algorithm.  For more details please see:  - [Video explanation of AdaHessian](https://www.youtube.com/watch?v=S87ancnZ0MM) - [AdaHessian paper](https://arxiv.org/pdf/2006.00719.pdf).   """;General;https://github.com/amirgholami/adahessian
"""Mixup is a generic and straightforward data augmentation principle. In essence  mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so  mixup regularizes the neural network to favor simple linear behavior in-between training examples.  This repository contains the implementation used for the results in our paper (https://arxiv.org/abs/1710.09412).   """;Computer Vision;https://github.com/CaoShuning/MIXUP
"""Input image (608  608  3)  The input image goes through a CNN  resulting in a (19  19  5  85) dimensional output.  After flattening the last two dimensions  the output is a volume of shape (19  19  425). Each cell in a 19 x 19 grid over the input image gives 425 numbers: 425 = 5 x 85 because each cell contains predictions for 5 boxes  corresponding to 5 anchor boxes; 85 = 5 + 80 where 5 is because (pc  bx  by  bh  bw) has 5 numbers  and 80 is the number of classes we'd like to detect.  We then select only a few boxes based on score-thresholding--discarding boxes that have detected a class with a score less than the threshold  and non-max suppression - computing the Intersection over Union (IOU) and avoiding selecting overlapping boxes.  This gives the YOLO's final output.   """;Computer Vision;https://github.com/TheClub4/car-detection-yolov2
"""The aim of this project is to dive into the field of action recognition and explore various techniques. Till now 2 models have been implemented <br/> 1 - The model.py  is a pytorch implementation of the paper - **A Closer Look at Spatiotemporal Convolutions for Action Recognition** Link to the paper is - **https://arxiv.org/abs/1711.11248v3**  2 - A pytorch implementation of MobileNets for less computational Models. Consult the paper - **MobileNetV2: Inverted Residuals and Linear Bottlenecks  - https://arxiv.org/abs/1801.04381v4**   """;Computer Vision;https://github.com/AD2605/Action-Recognition
"""The aim of this project is to dive into the field of action recognition and explore various techniques. Till now 2 models have been implemented <br/> 1 - The model.py  is a pytorch implementation of the paper - **A Closer Look at Spatiotemporal Convolutions for Action Recognition** Link to the paper is - **https://arxiv.org/abs/1711.11248v3**  2 - A pytorch implementation of MobileNets for less computational Models. Consult the paper - **MobileNetV2: Inverted Residuals and Linear Bottlenecks  - https://arxiv.org/abs/1801.04381v4**   """;General;https://github.com/AD2605/Action-Recognition
"""We propose AugMix  a data processing technique that mixes augmented images and enforces consistent embeddings of the augmented images  which results in increased robustness and improved uncertainty calibration. AugMix does not require tuning to work correctly  as with random cropping or CutOut  and thus enables plug-and-play data augmentation. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks  closing the gap between previous methods and the best possible performance by more than half in some cases. With AugMix  we obtain state-of-the-art on ImageNet-C  ImageNet-P and in uncertainty estimation when the train and test distribution do not match.  For more details please see our [ICLR 2020 paper](https://arxiv.org/pdf/1912.02781.pdf).   """;Computer Vision;https://github.com/google-research/augmix
"""This repository contains a [Torch](http://torch.ch) implementation for the [ResNeXt](https://arxiv.org/abs/1611.05431) algorithm for image classification. The code is based on [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch).  [ResNeXt](https://arxiv.org/abs/1611.05431) is a simple  highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous  multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension  which we call “cardinality” (the size of the set of transformations)  as an essential factor in addition to the dimensions of depth and width.   ![teaser](http://vcl.ucsd.edu/resnext/teaser.png)  """;Computer Vision;https://github.com/facebookresearch/ResNeXt
"""Cutout is a simple regularization method for convolutional neural networks which consists of masking out random sections of input images during training. This technique simulates occluded examples and encourages the model to take more minor features into consideration when making decisions  rather than relying on the presence of a few major features.      ![Cutout applied to CIFAR-10](https://github.com/uoguelph-mlrg/Cutout/blob/master/images/cutout_on_cifar10.jpg ""Cutout applied to CIFAR-10"")  Bibtex:   ``` @article{devries2017cutout      title={Improved Regularization of Convolutional Neural Networks with Cutout}      author={DeVries  Terrance and Taylor  Graham W}      journal={arXiv preprint arXiv:1708.04552}      year={2017}   } ```   """;Computer Vision;https://github.com/uoguelph-mlrg/Cutout
"""This framework was created in order to help compare learned and variational approaches to CT reconstruction in a systematic way. The implementation is based on python libraries **odl** and **pyTorch**. The list of implemented algorithms include:  * FBP (Filtered back-projection)  * TV (Total Variation) * ADR (Adversarial Regularizer): https://arxiv.org/abs/1805.11572 * LG (Learned gradient descent): https://arxiv.org/abs/1704.04058 * LPD (Learned primal dual): https://arxiv.org/abs/1707.06474 * FL (Fully learned): https://nature.com/articles/nature25988.pdf * FBP+U (FBP with a U-Net denoiser): https://arxiv.org/abs/1505.04597  In order to add your own algorithms to the list  create a new file in the **Algorithms** folder in the form *name*.py and use BaseAlg.py as the template.  """;Computer Vision;https://github.com/Zakobian/CT_framework_
"""In this hands-on project  the goal is to build a face detection model which includes building a face detector to locate the position of a face in an image.    """;General;https://github.com/sivaole/Face_Detection
"""In this hands-on project  the goal is to build a face detection model which includes building a face detector to locate the position of a face in an image.    """;Computer Vision;https://github.com/sivaole/Face_Detection
"""utils.py: Read data and data processing.<br> layer.py: Attention layer.<br> model.py: Graph attention model network.<br> main.py: Training  validation and testing.<br> You can run it through： ```python python main.py ```   """;Graphs;https://github.com/taishan1994/pytorch_gat
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/tkhhhh/yolov3-master
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/dzqjorking/transpose
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/dzqjorking/transpose
"""REPVGG is a simple but powerful architecture of CNN which has a VGG like inference time .It runs 101% faster then RESNET 101  There are many complicated architecture which has better accuracy then simple architectures  but the drawback of this kind of architecture is that they are difficult to customize . And  has very high inference time .REPVGG has various advantages like   Ithas plain topology   just like its earlier models such as vgg 19 etc . Its architecture highly depends upon 3x3 kernels and ReLU. It has novel structural reparamaterization which decouple a training time of multi branch topology with a inference time plain architecture .You can also se training of REPVGG in google colab on CIFAR10 [here](https://github.com/imad08/model-zoo-submissions/blob/main/REPVGG/REPVGG_with_complete_reparamaterization_.ipynb)   ![fusing batch normalization and convolutions for reparametrization](https://media.arxiv-vanity.com/render-output/4507333/x1.png)   """;Computer Vision;https://github.com/imad08/Repvgg_pytorch
"""Simple image classification project using Tensorflow. It allows to train DNN model and classify images. Contains pretrained models for cat/dog/human recognition.    """;Computer Vision;https://github.com/nidolow/image-classification
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/DeligientSloth/bert-tensorflow
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/DeligientSloth/bert-tensorflow
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/harsh2011/Yolov3-Detector
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/chandu7077/mybert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/chandu7077/mybert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/halo090770/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/halo090770/bert
"""            * No summary available !!!!!!         ''' style={'background-color':'#87ceeb'                                                                  'foreground-color':'red'                                                                  'color':'black'                                                                  'fontSize':12                                                                  'textAlign': 'left'                                                                  'verticalAlign':'top'                                                                  'position':'fixed'                                                                  'width':'50%'                                                                  'height':'18%'                                                                  'top':'80%'                                                                  'left':'27%'                                                                  'border-radius':10})   #######################   #if __name__ == '__main__':                 * Tracking is done quarterly at 3 different levels to give us a long term as well as short term indication of model performance         ''' style={'background-color':'#87ceeb'                                                                  'foreground-color':'red'                                                                  'color':'black'                                                                  'fontSize':12                                                                  'textAlign': 'left'                                                                  'verticalAlign':'top'                                                                  'position':'fixed'                                                                  'width':'50%'                                                                  'height':'18%'                                                                  'top':'80%'                                                                  'left':'27%'                                                                  'border-radius':10})     elif model_no == '111111':         return dcc.Markdown('''                   * Tracking is done quarterly at 3 different levels to give us a long term as well as short term indication of model performance         *Results: Q2 UTM Tracking is rates GREEN at overall level         * Segment level some deterioration is observed.         * Segment 1’s KS/PSI shifts are due to a known issue where the benchmark dataset with “very long ARF (Automated Response Format)” records (rich bureau history and FICO scores) were treated incorrectly by FICO during development         time. The validation dataset does not have this issue.         * Booked PSI shifts for segments 1  2  and 3 are caused by tightening risk tolerance since the 2010 benchmark time period.         * Segment 5's deterioration led to a deep dive in 2018 that found that 3 input attribute's had an amber rating for PSI (>10%)         ''' style={'background-color':'#87ceeb'                                                                  'foreground-color':'red'                                                                  'color':'black'                                                                  'fontSize':12                                                                  'textAlign': 'left'                                                                  'verticalAlign':'top'                                                                  'position':'fixed'                                                                  'width':'50%'                                                                  'height':'18%'                                                                  'top':'80%'                                                                  'left':'27%'                                                                  'border-radius':10})     elif model_no == '803456':         return dcc.Markdown('''           """;Computer Vision;https://github.com/vaibhavtmnit/Theano-Projects
"""I used the pandas library to calculate summary statistics of the traffic signs data set:  * Number of training examples = 34799 * Number of validation examples = 4410 * Number of testing examples = 12630 * Image data shape = (32  32  3) * Number of classes = 43   """;Computer Vision;https://github.com/waynecoffee9/Traffic-Sign-Classifier
"""I used the pandas library to calculate summary statistics of the traffic signs data set:  * Number of training examples = 34799 * Number of validation examples = 4410 * Number of testing examples = 12630 * Image data shape = (32  32  3) * Number of classes = 43   """;General;https://github.com/waynecoffee9/Traffic-Sign-Classifier
"""Recent Ubuntu releases come with python3 installed. I use pip3 for installing dependencies so install that with `sudo apt install python3-pip`. Install git if you don't already have it with `sudo apt install git`.  Then clone this repo with `git clone https://github.com/harvitronix/reinforcement-learning-car.git`. It has some pretty big weights files saved in past commits  so to just get the latest the fastest  do `git clone https://github.com/harvitronix/reinforcement-learning-car.git --depth 1`.   """;Reinforcement Learning;https://github.com/vsquareg/RL_ERA
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/jianmingwuhasco/yolov3
"""-------------------------------------------------------------- In order to design a fully autonomous Vehicle the following techniques have been used:          1. Waypoint Following techniques     2. Control     3. Traffic Light Detection and Classification      The Waypoint Following technique would take information from the traffic light detection and classification with the current waypoints in order to update the target velocities for each waypoint based on this information.  For Control part  I have designed a drive-by-wire (dbw) node that could take the target linear and angular velocities and publish commands for the throttle  brake  and steering of the car.   For Traffic Light Detection and classification  I have designed a classification node that would take the current waypoints of the car and an image taken from the car and determine if the closest traffic light was red or green.   ![alt text][image1]   -------------------------------------------------------------- This is the project repo for the final project of the Udacity Self-Driving Car Nanodegree-Capstone Project: Programming a Real Self-Driving Car.    """;General;https://github.com/KarimDahawy/CapStone-Project
"""-------------------------------------------------------------- In order to design a fully autonomous Vehicle the following techniques have been used:          1. Waypoint Following techniques     2. Control     3. Traffic Light Detection and Classification      The Waypoint Following technique would take information from the traffic light detection and classification with the current waypoints in order to update the target velocities for each waypoint based on this information.  For Control part  I have designed a drive-by-wire (dbw) node that could take the target linear and angular velocities and publish commands for the throttle  brake  and steering of the car.   For Traffic Light Detection and classification  I have designed a classification node that would take the current waypoints of the car and an image taken from the car and determine if the closest traffic light was red or green.   ![alt text][image1]   -------------------------------------------------------------- This is the project repo for the final project of the Udacity Self-Driving Car Nanodegree-Capstone Project: Programming a Real Self-Driving Car.    """;Computer Vision;https://github.com/KarimDahawy/CapStone-Project
"""Few months back when trying to search for Subtitles for a French Movie  I got an idea to build a mini-version of Neural Machine Translation system for French - English and see how it feels to build one. Courses CS 224n : Natural Language Processing with Deep learning and Sequence models from Coursera helped a lot in understanding Sequence models  although there is a long way to go!   """;General;https://github.com/HemaDevaSagar35/NeuralMachineTranslation-French2English
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/Talgin/facerec
"""In this work  we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN including only the most essential component in GCN—neighborhood aggregation—for collaborative filtering     """;Graphs;https://github.com/tanya525625/LightGCN-PyTorch
"""``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result |____BERT_NER_pb.py   		    #: run model and eval result and transfer ckpt to saved model (pb) |____ner_local_pb.py         #:load pb and predict  ```    """;Natural Language Processing;https://github.com/broccolik/BERT-NER
"""- Mackay uses Gaussian approximation to get solutions to BNNs  approximates posterior centred around most probable parameter values which are found via optimisation. Error is estimated from Hessian. This approximation assumes posterior is unimodal  and breaks down when number of parameters approaches numbr of datapoints. Uses this analytic approximation to calculate the evidence. Picks value of hyperparameters which maximise evidence  which equivalently maximise the posterior of the hyperparameters given the data  for a uniform prior on the hyperparameters. Thus essentially assumes that marginalising over hyperparameters and taking their maximum are equal  i.e. the hyperparam posterior is also Gaussian. then looks at evidence values given these best hyperparameters and training set error to evaluate models. Uses some approximation of the evidence maximisation to update the hyperparameter.  - finds when a rubbish model used  evidence and test error not as correlated as when good model is used. Further  the evidence is low in some cases where test error is good. Uses this to deduce structure of model is wrong. Also sees Occam's hill.  - Likelihood variance is fixed. Initially one hyperparam used for all weights and biases. Found evidence and generalisation don’t correlate well. MacKay argues this is because the scales of the inputs  outputs and hidden layers are not the same  so one cannot expect scaling the weights by the same amount to work well. So then tried one hyperparam for hidden unit weights  one for hidden unit biases  then one for output weights and biases. This gave higher evidence  higher test set performance  and stronger correlation between the two  - Neal uses HMC to sample the BNN parameters  and Gibbs sampling to sample the hyperparameters. n.b. HMC requires gradient information  so can't be used to sample hyperparameters directly (to my knowledge). Also  HMC in essence has characteristics similar to common optimisation methods which use 'momentum' and 'velocity'.  - Neal also introduces concept of using Gaussian processes to introduce a prior over functions  which tells us what nn predicts mapping function to be without any data.  - From Neal it seems that sampling hyperparameters seems rather necessary to justify allowing NN to be arbitrarily big- if complex model is not needed  hyperparameters will 'quash' nodes which aren't important  according to the hyperparameter values assigned by the data during the training  and 'upweight' important nodes. Also avoids cross validation step.  - Uses stochastic/mini-batch methods.  - Neal's result (with same simple model as Mackay) on test data is similar to the Mackay best evidence model's results  but not as good as his best test error model results. Performance didn't necessarily get worse with larger networks for BNNs  but did for MAP estimates (though don't think this treated hyperparameters as stochastic).  - n.b. hyperparams in first layer indicate which inputs to network are important. using it generalises to test data better  as irrelevant attributes fit to noise in train. Furthermore  Neal scales hyperprior (gamma parameter w  which is mean precision) by number of units in previous layer i.e. for layer i w_i -> w_i * H_{i-1} for i >= 2 (note he doesn't scale first hidden layer). Note that he does not use this scaling on the biases  in particular  for hidden layers  biases are given standard hyperprior  and for output layer the biases aren't given a stochastic variance at all (instead they are usually fixed to a Gaussian with unit variance).  - larger network is  more uncertain it is to out of training distribution data.  - for bh  BNN does much better on test error than traditional (though I don't think this uses cross validation in traditional sense).  - Freitas uses reversible jump MCMC to sample neural network systems. reversible jump MCMC is necessary when number of parameters changes. This is the case here  as the number of radial basis functions (neurons) is allowed to vary in the analysis  resulting in a varying number of model parameters/hyperparameters throughout the sampling. Gives posteriors on number of functions  as well as the usual param/hyperparams ones.  - Also uses SMC to train NNs where data arrives one at a time. Idea is to model joint distribution of model parameters at each timestep  and appears to do a better job of predicting it with more time/data.  - Also does model selection  using posterior over number of basis functions. Can do this in sequential context as well.   - Finds reversible jump MCMC does as well as Mackay and Neal  and better than expectation maximisation algorithm (which is similar/equivalent to variational inference)  but is slower than EM algo.  - Gal provides the genius insight that stochastic draws from the distribution over neural networks can be done using traditional methods. Usually if using dropout regularisation  one disables the dropout once training is finished. Gal shows that using dropout during model deployment is equivalent to using variational inference to get a probabilistic model output. The parameters of the variational inference problem are determined by the dropout properties I believe. The higher the dropout probability  the stronger the prior on the inference problem.  - This essentially means a Bayesian approach can be used even for high dimensional problems  the training time is the same as that of maximisation methods  and during deployment  one is only limited by how many samples from the posterior one wants.  - Gal finds that this method exceeds traditional variational inference methods both in terms of speed and test set performance for most tasks  with the only doubts occurring in some CNNs. He also finds it outperforms traditional methods in terms of test set performance  with the added bonus that one gets an uncertainty estimate. The method however cannot give evidence estimates.   """;General;https://github.com/SuperKam91/bnn
"""This class wraps around the general RL environment class to launch the CoppeliaSim with our custom scene. Additionally  in the beginning of every episode  it initialises the properties of the mating part: 2D position in the workspace (`setup_goal()` method)  as well as its colour.  The environment wrapper contains following methods:  * `get_observation()`  capture a grayscale image as an observation.  *  `distance_to_goal()`  compute the distance between the target and current position. The distance is used in reward design.     *  `success_check()`  check whether the goal state is reached. If yes  significantly boost agent's reward.     * `collision_check()`  check whether an agent collided with any object.     Episode termination occurs when the robot gets too far from the target  collides with any object in the environment or exceeds the maximum number of time steps. Those conditions are specified at the end of `step()` method and are checked at each step taken in the environment by the agent. Once the episode terminates  the whole cycle is repeated for the next episode.     One of the most exciting advancements  that has pushed the frontier of the Artificial Intelligence (AI) in recent years  is Deep Reinforcement Learning (DRL). DRL belongs to the family of machine learning algorithms. It assumes that intelligent machines can learn from their actions similar to the way humans learn from experience. Over the recent years we could witness some impressive [real-world applications of DRL](https://neptune.ai/blog/reinforcement-learning-applications). The algorithms allowed for major progress especially in the field of robotics. If you are interested in learning more about DRL  we encourage you to get familiar with the exceptional [**Introduction to RL**](https://spinningup.openai.com/en/latest) by OpenAI. We believe this is the best place to start your adventure with DRL.  The **goal of this tutorial is to show how you can apply DRL to solve your own robotic challenge**. For the sake of this tutorial we have chosen one of the classic assembly tasks: peg-in-hole insertion. By the time you finish the tutorial  you will understand how to create a complete  end-to-end pipeline for training the robot in the simulation using DRL.  The accompanying code together with all the details of the implementation can be found in our [GitHub repository](https://github.com/arrival-ltd/catalyst-rl-tutorial).   """;General;https://github.com/arrival-ltd/catalyst-rl-tutorial
"""This class wraps around the general RL environment class to launch the CoppeliaSim with our custom scene. Additionally  in the beginning of every episode  it initialises the properties of the mating part: 2D position in the workspace (`setup_goal()` method)  as well as its colour.  The environment wrapper contains following methods:  * `get_observation()`  capture a grayscale image as an observation.  *  `distance_to_goal()`  compute the distance between the target and current position. The distance is used in reward design.     *  `success_check()`  check whether the goal state is reached. If yes  significantly boost agent's reward.     * `collision_check()`  check whether an agent collided with any object.     Episode termination occurs when the robot gets too far from the target  collides with any object in the environment or exceeds the maximum number of time steps. Those conditions are specified at the end of `step()` method and are checked at each step taken in the environment by the agent. Once the episode terminates  the whole cycle is repeated for the next episode.     One of the most exciting advancements  that has pushed the frontier of the Artificial Intelligence (AI) in recent years  is Deep Reinforcement Learning (DRL). DRL belongs to the family of machine learning algorithms. It assumes that intelligent machines can learn from their actions similar to the way humans learn from experience. Over the recent years we could witness some impressive [real-world applications of DRL](https://neptune.ai/blog/reinforcement-learning-applications). The algorithms allowed for major progress especially in the field of robotics. If you are interested in learning more about DRL  we encourage you to get familiar with the exceptional [**Introduction to RL**](https://spinningup.openai.com/en/latest) by OpenAI. We believe this is the best place to start your adventure with DRL.  The **goal of this tutorial is to show how you can apply DRL to solve your own robotic challenge**. For the sake of this tutorial we have chosen one of the classic assembly tasks: peg-in-hole insertion. By the time you finish the tutorial  you will understand how to create a complete  end-to-end pipeline for training the robot in the simulation using DRL.  The accompanying code together with all the details of the implementation can be found in our [GitHub repository](https://github.com/arrival-ltd/catalyst-rl-tutorial).   """;Reinforcement Learning;https://github.com/arrival-ltd/catalyst-rl-tutorial
"""``` Layer (type)                 Output Shape              Param #:    ================================================================= input_word_ids (InputLayer)  [(None  1500)]            0          _________________________________________________________________ tf_bert_model_1 (TFBertModel ((None  1500  768)  (None 109482240))  _________________________________________________________________ tf_op_layer_strided_slice_1  [(None  768)]             0          _________________________________________________________________ dense_1 (Dense)              (None  16)                12304      ================================================================= Total params: 109 494 544 Trainable params: 109 494 544 Non-trainable params: 0 ```   """;Natural Language Processing;https://github.com/MLH-Fellowship/Social-BERTerfly
"""We build a plant disease diagnosis system on Android  by implementing a deep convolutional neural network with Tensorflow to detect disease from various plant leave images.   Generally  due to the size limitation of the dataset  we adopt the transefer learning in this system. Specifically  we retrain the MobileNets [[1]](https://arxiv.org/pdf/1704.04861.pdf)  which is first trained on ImageNet dataset  on the plant disease datasets. Finally  we port the trained model to Android.   """;General;https://github.com/abhimangalms/PlantDoctor
"""We build a plant disease diagnosis system on Android  by implementing a deep convolutional neural network with Tensorflow to detect disease from various plant leave images.   Generally  due to the size limitation of the dataset  we adopt the transefer learning in this system. Specifically  we retrain the MobileNets [[1]](https://arxiv.org/pdf/1704.04861.pdf)  which is first trained on ImageNet dataset  on the plant disease datasets. Finally  we port the trained model to Android.   """;Computer Vision;https://github.com/abhimangalms/PlantDoctor
"""Our project will process videos that contain human faces and return video with all facial features removed  either by performing a blur with randomized parameters  or by omitting facial pixels all together. We will likely do this by training a convolution neural network with a dataset used for video facial recognition  such as ‘Youtube Faces with Facial Keypoints’ found here.   Existing video editing software has blurring functionality  but the user often has to select the features  and it’s unclear whether deblurring could reveal the identity after-the-fact. There are a few papers and similar projects available online that have demonstrated such work  such as this research paper  the following two articles  and the work of Terrance Boult and Walter Schierer.  If time and project complexity allow  an additional portion of the project could be examining feasibility of an on-device-algorithm that could be used on a camera so there was no back-door to deanonymize the data.    """;General;https://github.com/johngear/eecs504
"""Our project will process videos that contain human faces and return video with all facial features removed  either by performing a blur with randomized parameters  or by omitting facial pixels all together. We will likely do this by training a convolution neural network with a dataset used for video facial recognition  such as ‘Youtube Faces with Facial Keypoints’ found here.   Existing video editing software has blurring functionality  but the user often has to select the features  and it’s unclear whether deblurring could reveal the identity after-the-fact. There are a few papers and similar projects available online that have demonstrated such work  such as this research paper  the following two articles  and the work of Terrance Boult and Walter Schierer.  If time and project complexity allow  an additional portion of the project could be examining feasibility of an on-device-algorithm that could be used on a camera so there was no back-door to deanonymize the data.    """;Computer Vision;https://github.com/johngear/eecs504
"""This is a classifier to predict label of hand drawn doodle images in real time. The idea is based on [QuickDraw](https://quickdraw.withgoogle.com/#) by Google. The [dataset](https://github.com/googlecreativelab/quickdraw-dataset) they provide contains 50 million images across 345 categories! I am using a subset of 50 categories for my model because of limited resources but one can use the same code with small tweaks to train on all 345 categories.  I built and trained the model in Pytorch and converted it to onnx format to use it in the browser. Initially my plan was to perform the classification on the backend. After drawing  the user would press a button and the request would be sent to the server for classification. That is how I built the app. However  it was very expensive on the server because for every image there would a request. Hence I decided to move the classification on the frontend. Also it is a lot more fun to see the model try to classify the image in real time :grin:.  It took me a very long time to train and tweak the model to obtain a good accuracy particularly because the dataset was huge even though I was using only a subset of it and training the model on the GPU. How someone draws a certain object varies a lot. It is all based on imagination and perception of that person about that object. Hence it was necessary to use lots of images per category to capture maximum variations.   For the record  88% was the average test accuracy(averaged out across the classes). Had it been allowed to train for longer  I believe it could have crossed 90% mark but I had already spent days on training it so I let it go. Yes days! Colab has a limit on the usage of GPUs after which the runtime gets disconnected. So I had to train the model for some hours every day for about 2-3 days to get good accuracy. Then I would make tweaks to the model and restart the process.     """;Computer Vision;https://github.com/Prateek93a/DoodleAI
"""- `main.lua` (~30 lines) - loads all other files  starts training. - `opts.lua` (~50 lines) - all the command-line options and description - `data.lua` (~60 lines) - contains the logic to create K threads for parallel data-loading. - `donkey.lua` (~200 lines) - contains the data-loading logic and details. It is run by each data-loader thread. random image cropping  generating 10-crops etc. are in here. - `model.lua` (~80 lines) - creates AlexNet model and criterion - `train.lua` (~190 lines) - logic for training the network. we hard-code a learning rate + weight decay schedule that produces good results. - `test.lua` (~120 lines) - logic for testing the network on validation set (including calculating top-1 and top-5 errors) - `dataset.lua` (~430 lines) - a general purpose data loader  mostly derived from [here: imagenetloader.torch](https://github.com/soumith/imagenetloader.torch). That repo has docs and more examples of using this loader.  """;Computer Vision;https://github.com/soumith/imagenet-multiGPU.torch
"""We provided following boundaries in folder `boundaries/`. The boundaries can be more accurate if stronger attribute predictor is used.  - ProgressiveGAN model trained on CelebA-HQ dataset:   - Single boundary:     - `pggan_celebahq_pose_boundary.npy`: Pose.     - `pggan_celebahq_smile_boundary.npy`: Smile (expression).     - `pggan_celebahq_age_boundary.npy`: Age.     - `pggan_celebahq_gender_boundary.npy`: Gender.     - `pggan_celebahq_eyeglasses_boundary.npy`: Eyeglasses.     - `pggan_celebahq_quality_boundary.npy`: Image quality.   - Conditional boundary:     - `pggan_celebahq_age_c_gender_boundary.npy`: Age (conditioned on gender).     - `pggan_celebahq_age_c_eyeglasses_boundary.npy`: Age (conditioned on eyeglasses).     - `pggan_celebahq_age_c_gender_eyeglasses_boundary.npy`: Age (conditioned on gender and eyeglasses).     - `pggan_celebahq_gender_c_age_boundary.npy`: Gender (conditioned on age).     - `pggan_celebahq_gender_c_eyeglasses_boundary.npy`: Gender (conditioned on eyeglasses).     - `pggan_celebahq_gender_c_age_eyeglasses_boundary.npy`: Gender (conditioned on age and eyeglasses).     - `pggan_celebahq_eyeglasses_c_age_boundary.npy`: Eyeglasses (conditioned on age).     - `pggan_celebahq_eyeglasses_c_gender_boundary.npy`: Eyeglasses (conditioned on gender).     - `pggan_celebahq_eyeglasses_c_age_gender_boundary.npy`: Eyeglasses (conditioned on age and gender). - StyleGAN model trained on CelebA-HQ dataset:   - Single boundary in $\mathcal{Z}$ space:     - `stylegan_celebahq_pose_boundary.npy`: Pose.     - `stylegan_celebahq_smile_boundary.npy`: Smile (expression).     - `stylegan_celebahq_age_boundary.npy`: Age.     - `stylegan_celebahq_gender_boundary.npy`: Gender.     - `stylegan_celebahq_eyeglasses_boundary.npy`: Eyeglasses.   - Single boundary in $\mathcal{W}$ space:     - `stylegan_celebahq_pose_w_boundary.npy`: Pose.     - `stylegan_celebahq_smile_w_boundary.npy`: Smile (expression).     - `stylegan_celebahq_age_w_boundary.npy`: Age.     - `stylegan_celebahq_gender_w_boundary.npy`: Gender.     - `stylegan_celebahq_eyeglasses_w_boundary.npy`: Eyeglasses.  - StyleGAN model trained on FF-HQ dataset:   - Single boundary in $\mathcal{Z}$ space:     - `stylegan_ffhq_pose_boundary.npy`: Pose.     - `stylegan_ffhq_smile_boundary.npy`: Smile (expression).     - `stylegan_ffhq_age_boundary.npy`: Age.     - `stylegan_ffhq_gender_boundary.npy`: Gender.     - `stylegan_ffhq_eyeglasses_boundary.npy`: Eyeglasses.   - Conditional boundary in $\mathcal{Z}$ space:     - `stylegan_ffhq_age_c_gender_boundary.npy`: Age (conditioned on gender).     - `stylegan_ffhq_age_c_eyeglasses_boundary.npy`: Age (conditioned on eyeglasses).     - `stylegan_ffhq_eyeglasses_c_age_boundary.npy`: Eyeglasses (conditioned on age).     - `stylegan_ffhq_eyeglasses_c_gender_boundary.npy`: Eyeglasses (conditioned on gender).   - Single boundary in $\mathcal{W}$ space:     - `stylegan_ffhq_pose_w_boundary.npy`: Pose.     - `stylegan_ffhq_smile_w_boundary.npy`: Smile (expression).     - `stylegan_ffhq_age_w_boundary.npy`: Age.     - `stylegan_ffhq_gender_w_boundary.npy`: Gender.     - `stylegan_ffhq_eyeglasses_w_boundary.npy`: Eyeglasses.   """;Computer Vision;https://github.com/genforce/interfacegan
"""This repository is an academical work on a new subject introduced by google researchers called:  Transformer for Image classification at scale. We worked at Georgia Tech Lorraine with the DREAM research team  a robotic laboratory  in in order to test this new image classification technique on a diatom dataset.  This technique called Vision Transformer was published in the folowing paper:  [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929).  Overview of the model given by Google: we split an image into fixed-size patches  linearly embed each of them  add position embeddings  and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification  we use the standard approach of adding an extra learnable ""classification token"" to the sequence.   """;Computer Vision;https://github.com/Thanusan19/Vision_Transformer
"""To practice what you've learned  a good idea would be to spend an hour on 3 of the following (3-hours total  you could through them all if you want) and then write a blog post about what you've learned.  * For an overview of the different problems within NLP and how to solve them read through:    * [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)   * [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e) * Go through [MIT's Recurrent Neural Networks lecture](https://youtu.be/SEnXr6v2ifU). This will be one of the greatest additions to what's happening behind the RNN model's you've been building. * Read through the [word embeddings page on the TensorFlow website](https://www.tensorflow.org/tutorials/text/word_embeddings). Embeddings are such a large part of NLP. We've covered them throughout this notebook but extra practice would be well worth it. A good exercise would be to write out all the code in the guide in a new notebook.  * For more on RNN's in TensorFlow  read and reproduce [the TensorFlow RNN guide](https://www.tensorflow.org/guide/keras/rnn). We've covered many of the concepts in this guide  but it's worth writing the code again for yourself. * Text data doesn't always come in a nice package like the data we've downloaded. So if you're after more on preparing different text sources for being with your TensorFlow deep learning models  it's worth checking out the following:   * [TensorFlow text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text).   * [Reading text files with Python](https://realpython.com/read-write-files-python/) by Real Python. * This notebook has focused on writing NLP code. For a mathematically rich overview of how NLP with Deep Learning happens  read [Standford's Natural Language Processing with Deep Learning lecture notes Part 1](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).     * For an even deeper dive  you could even do the whole [CS224n](http://web.stanford.edu/class/cs224n/) (Natural Language Processing with Deep Learning) course.  * Great blog posts to read:   * Andrei Karpathy's [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) dives into generating Shakespeare text with RNNs.   * [Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) by Mauro Di Pietro. An overview of different techniques for turning text into numbers and then classifying it.   * [What are word embeddings?](https://machinelearningmastery.com/what-are-word-embeddings/) by Machine Learning Mastery. * Other topics worth looking into:   * [Attention mechanisms](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/). These are a foundational component of the transformer architecture and also often add improvments to deep NLP models.   * [Transformer architectures](http://jalammar.github.io/illustrated-transformer/). This model architecture has recently taken the NLP world by storm  achieving state of the art on many benchmarks. However  it does take a little more processing to get off the ground  the [HuggingFace Models (formerly HuggingFace Transformers) library](https://huggingface.co/models/) is probably your best quick start.  ---   1. Rebuild  compile and train `model_1`  `model_2` and `model_5` using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) instead of the Functional API. 2. Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data? 3. Try fine-tuning the TF Hub Universal Sentence Encoder model by setting `training=True` when instantiating it as a Keras layer.  ``` #: We can use this encoding layer in place of our text_vectorizer and embedding layer sentence_encoder_layer = hub.KerasLayer(""https://tfhub.dev/google/universal-sentence-encoder/4""                                          input_shape=[]                                          dtype=tf.string                                          trainable=True) #: turn training on to fine-tune the TensorFlow Hub model ``` 4. Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the `sample_submission.csv` file from Kaggle (see the Files tab in Colab for what the `sample_submission.csv` file looks like). Once you've done this  [make a submission to the Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/data)  how did your model perform? 5. Combine the ensemble predictions using the majority vote (mode)  how does this perform compare to averaging the prediction probabilities of each model? 6. Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels.   """;General;https://github.com/mrdbourke/tensorflow-deep-learning
"""To practice what you've learned  a good idea would be to spend an hour on 3 of the following (3-hours total  you could through them all if you want) and then write a blog post about what you've learned.  * For an overview of the different problems within NLP and how to solve them read through:    * [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)   * [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e) * Go through [MIT's Recurrent Neural Networks lecture](https://youtu.be/SEnXr6v2ifU). This will be one of the greatest additions to what's happening behind the RNN model's you've been building. * Read through the [word embeddings page on the TensorFlow website](https://www.tensorflow.org/tutorials/text/word_embeddings). Embeddings are such a large part of NLP. We've covered them throughout this notebook but extra practice would be well worth it. A good exercise would be to write out all the code in the guide in a new notebook.  * For more on RNN's in TensorFlow  read and reproduce [the TensorFlow RNN guide](https://www.tensorflow.org/guide/keras/rnn). We've covered many of the concepts in this guide  but it's worth writing the code again for yourself. * Text data doesn't always come in a nice package like the data we've downloaded. So if you're after more on preparing different text sources for being with your TensorFlow deep learning models  it's worth checking out the following:   * [TensorFlow text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text).   * [Reading text files with Python](https://realpython.com/read-write-files-python/) by Real Python. * This notebook has focused on writing NLP code. For a mathematically rich overview of how NLP with Deep Learning happens  read [Standford's Natural Language Processing with Deep Learning lecture notes Part 1](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).     * For an even deeper dive  you could even do the whole [CS224n](http://web.stanford.edu/class/cs224n/) (Natural Language Processing with Deep Learning) course.  * Great blog posts to read:   * Andrei Karpathy's [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) dives into generating Shakespeare text with RNNs.   * [Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) by Mauro Di Pietro. An overview of different techniques for turning text into numbers and then classifying it.   * [What are word embeddings?](https://machinelearningmastery.com/what-are-word-embeddings/) by Machine Learning Mastery. * Other topics worth looking into:   * [Attention mechanisms](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/). These are a foundational component of the transformer architecture and also often add improvments to deep NLP models.   * [Transformer architectures](http://jalammar.github.io/illustrated-transformer/). This model architecture has recently taken the NLP world by storm  achieving state of the art on many benchmarks. However  it does take a little more processing to get off the ground  the [HuggingFace Models (formerly HuggingFace Transformers) library](https://huggingface.co/models/) is probably your best quick start.  ---   1. Rebuild  compile and train `model_1`  `model_2` and `model_5` using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) instead of the Functional API. 2. Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data? 3. Try fine-tuning the TF Hub Universal Sentence Encoder model by setting `training=True` when instantiating it as a Keras layer.  ``` #: We can use this encoding layer in place of our text_vectorizer and embedding layer sentence_encoder_layer = hub.KerasLayer(""https://tfhub.dev/google/universal-sentence-encoder/4""                                          input_shape=[]                                          dtype=tf.string                                          trainable=True) #: turn training on to fine-tune the TensorFlow Hub model ``` 4. Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the `sample_submission.csv` file from Kaggle (see the Files tab in Colab for what the `sample_submission.csv` file looks like). Once you've done this  [make a submission to the Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/data)  how did your model perform? 5. Combine the ensemble predictions using the majority vote (mode)  how does this perform compare to averaging the prediction probabilities of each model? 6. Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels.   """;Natural Language Processing;https://github.com/mrdbourke/tensorflow-deep-learning
"""**Refined Vision Transformer** is initially described in [arxiv](https://arxiv.org/abs/2106.03714)  which observes vision transformers require much more datafor model pre-training. Most of recent works thus are dedicated to designing morecomplex architectures or training methods to address the data-efficiency issue ofViTs. However  few of them explore improving the self-attention mechanism  akey factor distinguishing ViTs from CNNs.  Different from existing works  weintroduce a conceptually simple scheme  calledrefiner  to directly refine the self-attention maps of ViTs.  Specifically  refiner exploresattention expansionthatprojects the multi-head attention maps to a higher-dimensional space to promotetheir diversity.  Further  refiner applies convolutions to augment local patternsof the attention maps  which we show is equivalent to adistributed local atten-tion—features are aggregated locally with learnable kernels and then globallyaggregated with self-attention.  Extensive experiments demonstrate that refinerworks surprisingly well. Significantly  it enables ViTs to achieve 86% top-1 classifi-cation accuracy on ImageNet with only 81M parameters.  <p align=""center""> <img src=""https://github.com/zhoudaquan/Refiner_ViT/blob/master/figures/overall_flow.png"" | width=500> </p>  Please run git clone with --recursive to clone timm as submodule and install it with ` cd pytorch-image-models && pip install -e ./`    """;Computer Vision;https://github.com/zhoudaquan/Refiner_ViT
"""This project is dedicated to the investigation of methods for predicting meaningful events in footage of car racing. This repository is focused on the exploration of **collision detection** but contains a tool for the classification of  as well. During the work on this project we've also developed a **monadic pipeline** library [mPyPl](https://github.com/shwars/mPyPl) to simplify tasks of data processing and creating complex data pipelines.    Due to the small amount of data in this problem; we could not rely on neural networks to learn representations as part of the training process. Instead; we needed to design bespoke features  crafted with domain knowledge. After series of experiments  we've created a model based on features obtained using three different approaches:   * Dense Optical Flow * VGG16 embeddings  * A special kind of Optical Flow - [Focused Optical Flow](#focused-optical-flow).     ℹ️ *After the release of mPyPl as a independent [pip-installable framework](https://pypi.org/project/mPyPl/)  some experimental notebooks in the ```notebooks``` folder have not been updated  but may contain interesting things to explore.*   """;Computer Vision;https://github.com/sulasen/race-events-recognition-1
"""This project is dedicated to the investigation of methods for predicting meaningful events in footage of car racing. This repository is focused on the exploration of **collision detection** but contains a tool for the classification of  as well. During the work on this project we've also developed a **monadic pipeline** library [mPyPl](https://github.com/shwars/mPyPl) to simplify tasks of data processing and creating complex data pipelines.    Due to the small amount of data in this problem; we could not rely on neural networks to learn representations as part of the training process. Instead; we needed to design bespoke features  crafted with domain knowledge. After series of experiments  we've created a model based on features obtained using three different approaches:   * Dense Optical Flow * VGG16 embeddings  * A special kind of Optical Flow - [Focused Optical Flow](#focused-optical-flow).     ℹ️ *After the release of mPyPl as a independent [pip-installable framework](https://pypi.org/project/mPyPl/)  some experimental notebooks in the ```notebooks``` folder have not been updated  but may contain interesting things to explore.*   """;General;https://github.com/sulasen/race-events-recognition-1
"""Machine translation is a natural language processing task that aims to translate natural languages using computers automatically. Recent several years have witnessed the rapid development of end-to-end neural machine translation  which has become the new mainstream method in practical MT systems.  THUMT is an open-source toolkit for neural machine translation developed by [the Natural Language Processing Group at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/site2/index.php?lang=en).    """;Natural Language Processing;https://github.com/insigh/THUMT
"""A general YOLOv4/v3/v2 object detection pipeline inherited from [keras-yolo3-Mobilenet](https://github.com/Adamdad/keras-YOLOv3-mobilenet)/[keras-yolo3](https://github.com/qqwweee/keras-yolo3) and [YAD2K](https://github.com/allanzelener/YAD2K). Implement with tf.keras  including data collection/annotation  model training/tuning  model evaluation and on device deployment. Support different architecture and different technologies:   """;Computer Vision;https://github.com/david8862/keras-YOLOv3-model-set
"""Manufacturing is becoming automated on a broad scale. The technology enables manufacturers to affordably boost their throughput  improve quality and become nimbler as they respond to customer demands. Automation is a revolution in manufacturing quality control. It allows the companies to set certain bars or criteria for the products being manufactured. Then it also aids in real-time tracking of the manufacturing process through machine vision cameras and/or recordings.   The core deliverable for this project is building deep learning image classification models which can automate the process of inspection for casting defects. I have produced a rest endpoint which can accept a cast image and subsequently run a tuned model to classify if the cast is acceptable or not.    As part of this project  I have built the computer vision models in 3 different ways addressing different personas  because not all companies will have a resolute data science team.   1.	Using Keras Tensorflow model (convolution2d) what a trained team of data scientists would do. 2.	Using Azure Machine Learning Designer (designer) which enables AI engineers and Data scientists use a drag and drop prebuilt model DenseNet (densenet).  3.	Using Azure custom vision (custom-vision) which democratizes the process of building the computer vision model with little to no training.   """;General;https://github.com/RajdeepBiswas/Manufacturing-Quality-Inspection
"""Manufacturing is becoming automated on a broad scale. The technology enables manufacturers to affordably boost their throughput  improve quality and become nimbler as they respond to customer demands. Automation is a revolution in manufacturing quality control. It allows the companies to set certain bars or criteria for the products being manufactured. Then it also aids in real-time tracking of the manufacturing process through machine vision cameras and/or recordings.   The core deliverable for this project is building deep learning image classification models which can automate the process of inspection for casting defects. I have produced a rest endpoint which can accept a cast image and subsequently run a tuned model to classify if the cast is acceptable or not.    As part of this project  I have built the computer vision models in 3 different ways addressing different personas  because not all companies will have a resolute data science team.   1.	Using Keras Tensorflow model (convolution2d) what a trained team of data scientists would do. 2.	Using Azure Machine Learning Designer (designer) which enables AI engineers and Data scientists use a drag and drop prebuilt model DenseNet (densenet).  3.	Using Azure custom vision (custom-vision) which democratizes the process of building the computer vision model with little to no training.   """;Computer Vision;https://github.com/RajdeepBiswas/Manufacturing-Quality-Inspection
"""JDet is an object detection benchmark based on [Jittor](https://github.com/Jittor/jittor)  and mainly focus on aerial image object detection (oriented object detection).   <!-- **Features** - Automatic compilation. Our framwork is based on Jittor  which means we don't need to Manual compilation for these code with CUDA and C++. -  -->  <!-- Framework details are avaliable in the [framework.md](docs/framework.md) -->  """;Computer Vision;https://github.com/Jittor/JDet
"""JDet is an object detection benchmark based on [Jittor](https://github.com/Jittor/jittor)  and mainly focus on aerial image object detection (oriented object detection).   <!-- **Features** - Automatic compilation. Our framwork is based on Jittor  which means we don't need to Manual compilation for these code with CUDA and C++. -  -->  <!-- Framework details are avaliable in the [framework.md](docs/framework.md) -->  """;General;https://github.com/Jittor/JDet
"""We provide code and training configurations of VoTr-SSD/TSD on the KITTI and Waymo Open dataset. Checkpoints will not be released.    **Important Notes**: VoTr generally requires quite a long time (more than 60 epochs on Waymo) to converge  and a large GPU memory (32Gb) is needed for reproduction. Please strictly follow the instructions and train with sufficient number of epochs. If you don't have a 32G GPU  you can decrease the attention SIZE parameters in yaml files  but this may possibly harm the performance.    """;Computer Vision;https://github.com/PointsCoder/VOTR
"""We provide code and training configurations of VoTr-SSD/TSD on the KITTI and Waymo Open dataset. Checkpoints will not be released.    **Important Notes**: VoTr generally requires quite a long time (more than 60 epochs on Waymo) to converge  and a large GPU memory (32Gb) is needed for reproduction. Please strictly follow the instructions and train with sufficient number of epochs. If you don't have a 32G GPU  you can decrease the attention SIZE parameters in yaml files  but this may possibly harm the performance.    """;General;https://github.com/PointsCoder/VOTR
"""该系统实现了基于深度框架的语音识别中的声学模型和语言模型建模，其中声学模型包括CNN-CTC、GRU-CTC、CNN-RNN-CTC，语言模型包含[transformer](https://jalammar.github.io/illustrated-transformer/)、[CBHG](https://github.com/crownpku/Somiao-Pinyin)，数据集包含stc、primewords、Aishell、thchs30四个数据集。  本系统更整体介绍：https://blog.csdn.net/chinatelecom08/article/details/82557715  本项目现已训练一个迷你的语音识别系统，将项目下载到本地上，下载[thchs数据集](http://www.openslr.org/resources/18/data_thchs30.tgz)并解压至data，运行`test.py`，不出意外能够进行识别，结果如下：       the  0 th example.     文本结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文汉字： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然     识别结果： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然  若自己建立模型则需要删除现有模型，重新配置参数训练，具体实现流程参考本页最后。   """;General;https://github.com/yumoh/speech-keras
"""该系统实现了基于深度框架的语音识别中的声学模型和语言模型建模，其中声学模型包括CNN-CTC、GRU-CTC、CNN-RNN-CTC，语言模型包含[transformer](https://jalammar.github.io/illustrated-transformer/)、[CBHG](https://github.com/crownpku/Somiao-Pinyin)，数据集包含stc、primewords、Aishell、thchs30四个数据集。  本系统更整体介绍：https://blog.csdn.net/chinatelecom08/article/details/82557715  本项目现已训练一个迷你的语音识别系统，将项目下载到本地上，下载[thchs数据集](http://www.openslr.org/resources/18/data_thchs30.tgz)并解压至data，运行`test.py`，不出意外能够进行识别，结果如下：       the  0 th example.     文本结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文汉字： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然     识别结果： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然  若自己建立模型则需要删除现有模型，重新配置参数训练，具体实现流程参考本页最后。   """;Natural Language Processing;https://github.com/yumoh/speech-keras
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/pengshuyuan/Bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/pengshuyuan/Bert
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.   ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER_ORIG.py		    #: original code without doc_stride |____ BERT_NER_STRIDE.py		    #: main code with doc_stride |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    """;Natural Language Processing;https://github.com/anupamsingh610/bert_ner_stride
"""Just another implementation with Tensorflow for paper `Spatial Transformer Netrworks`.    """;Computer Vision;https://github.com/Legend94rz/spatial-transformer
"""This repo uses [*PoolNet* (cvpr19)](https://arxiv.org/abs/1904.09569) as the baseline method for Salient Object Detection .   [Res2Net](https://github.com/gasvn/Res2Net) is a powerful backbone architecture that can be easily implemented into state-of-the-art models by replacing the bottleneck with Res2Net module. More detail can be found on [ ""Res2Net: A New Multi-scale Backbone Architecture""](https://arxiv.org/pdf/1904.01169.pdf)   """;Computer Vision;https://github.com/Res2Net/Res2Net-PoolNet
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to [arXiv paper](http://arxiv.org/abs/1512.02325).  This repository contains a TensorFlow re-implementation of the original [Caffe code](https://github.com/weiliu89/caffe/tree/ssd). At present  it only implements VGG-based SSD networks (with 300 and 512 inputs)  but the architecture of the project is modular  and should make easy the implementation and training of other SSD variants (ResNet or Inception based for instance). Present TF checkpoints have been directly converted from SSD Caffe models.  The organisation is inspired by the [TF-Slim models](https://github.com/tensorflow/models/blob/master/research/inception/inception/slim/README.md) repository containing the implementation of popular architectures (ResNet  Inception and VGG).   """;Computer Vision;https://github.com/amirmohammadii/SSD-object-detection
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/google-research/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/RenXiangyuan/tf_bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/RenXiangyuan/tf_bert
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.      ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    """;Natural Language Processing;https://github.com/crx934080895/Bert-CRF_New2
"""What is a model? What is their role? I think a good model needs to have a good [Inductive Bias](https://en.wikipedia.org/wiki/Inductive_bias)  which means it has good generalization capability to unseen example during training.  The difference between the Neural Network method of learning and other learning paradigm is that the Neural Network method learns from data by making a good representation of that data. On the contrary  many other methods learn by features that are manually selected by humans.  The Transformer model is one of the most popular representation generators of Neural Network methods of learning. Because of its general representation processing mechanism such as Attention-based learning  many recent advancements of deep learning rely on it.  So what actually Transformers do? What modules comprise Transformers? What are their implications? This is a natural question of mine as a beginner.   """;General;https://github.com/hiun/learning-transformers
"""What is a model? What is their role? I think a good model needs to have a good [Inductive Bias](https://en.wikipedia.org/wiki/Inductive_bias)  which means it has good generalization capability to unseen example during training.  The difference between the Neural Network method of learning and other learning paradigm is that the Neural Network method learns from data by making a good representation of that data. On the contrary  many other methods learn by features that are manually selected by humans.  The Transformer model is one of the most popular representation generators of Neural Network methods of learning. Because of its general representation processing mechanism such as Attention-based learning  many recent advancements of deep learning rely on it.  So what actually Transformers do? What modules comprise Transformers? What are their implications? This is a natural question of mine as a beginner.   """;Natural Language Processing;https://github.com/hiun/learning-transformers
"""We propose ResRep  a novel method for lossless channel pruning (a.k.a. filter pruning)  which aims to slim down a convolutional neural network (CNN) by reducing the width (number of output channels) of convolutional layers. Inspired by the neurobiology research about the independence of remembering and forgetting  we propose to re-parameterize a CNN into the remembering parts and forgetting parts  where the former learn to maintain the performance and the latter learn for efficiency. By training the re-parameterized model using regular SGD on the former but a novel update rule with penalty gradients on the latter  we realize structured sparsity  enabling us to equivalently convert the re-parameterized model into the original architecture with narrower layers. Such a methodology distinguishes ResRep from the traditional learning-based pruning paradigm that applies a penalty on parameters to produce structured sparsity  which may suppress the parameters essential for the remembering. Our method slims down a standard ResNet-50 with 76.15% accuracy on ImageNet to a narrower one with only 45% FLOPs and no accuracy drop  which is the first to achieve lossless pruning with such a high compression ratio  to the best of our knowledge.   """;Computer Vision;https://github.com/DingXiaoH/ResRep
"""[rife-ncnn-vulkan](https://github.com/nihui/rife-ncnn-vulkan) is nihui's ncnn implementation of Real-World Super-Resolution via Kernel Estimation and Noise Injection super resolution.  rife-ncnn-vulkan-python wraps [rife-ncnn-vulkan project](https://github.com/nihui/rife-ncnn-vulkan) by SWIG to make it easier to integrate rife-ncnn-vulkan with existing python projects.   """;Computer Vision;https://github.com/media2x/rife-ncnn-vulkan-python
"""Paddle version of Paper“PointRend: Image Segmentation as Rendering(CVPR2020)”.  This project uses Baidu's paddlepaddle framework to reproduce the CVPR2020 paper's model PointRend. **Note: only the semantic segmentation experiment of Semantic FPN + PointRend on the cityscapes dataset is done here  excluding the instance segmentation experiment of Maskrcnn + Pointrend. The correctness of PointRend based on paste reproduction is verified.**  The project relies on the paddleseg tool.  **PointRend With Seg Architecture:**  ![PointRend](./images/pointrend.png)  ![PointRend Result](./images/pointrendresult.png)   **Paper:** [PointRend: Image Segmentation as Rendering](https://arxiv.org/abs/1912.08193)   """;Computer Vision;https://github.com/CuberrChen/PointRend-Paddle
"""Chat-bots are becoming more and more useful in various simple professional tasks as they get more and more able to capture the essence of communicating with people. Still the development of good chat-bots that will answer more complicated questions  in general subjects  is a growing domain of research.   The goal of this project is to create a chat-bot able to answer python related questions.  Our project started with the main idea being that a programming assistant would be a much needed help by many people working or studying computer science. Although it sounds simple it soon proved to be a difficult task. The main challenge is that the model has to extract a technical correlation between Questions and Answers in order to be able to communicate effectively. The model that we used in order to achieve our goal is a recurrent sequence-to-sequence model. The main steps that we followed are described bellow.  - We found  downloaded  and processed data taken from stack overflow concerning questions that contained at least one python tag.[7] - Implement a sequence-to-sequence model. - Jointly train encoder and decoder models using mini-batches - Used greedy-search decoding - Interact with trained chatbot   """;General;https://github.com/vGkatsis/Chat_Bot_DL
"""Chat-bots are becoming more and more useful in various simple professional tasks as they get more and more able to capture the essence of communicating with people. Still the development of good chat-bots that will answer more complicated questions  in general subjects  is a growing domain of research.   The goal of this project is to create a chat-bot able to answer python related questions.  Our project started with the main idea being that a programming assistant would be a much needed help by many people working or studying computer science. Although it sounds simple it soon proved to be a difficult task. The main challenge is that the model has to extract a technical correlation between Questions and Answers in order to be able to communicate effectively. The model that we used in order to achieve our goal is a recurrent sequence-to-sequence model. The main steps that we followed are described bellow.  - We found  downloaded  and processed data taken from stack overflow concerning questions that contained at least one python tag.[7] - Implement a sequence-to-sequence model. - Jointly train encoder and decoder models using mini-batches - Used greedy-search decoding - Interact with trained chatbot   """;Sequential;https://github.com/vGkatsis/Chat_Bot_DL
"""Chat-bots are becoming more and more useful in various simple professional tasks as they get more and more able to capture the essence of communicating with people. Still the development of good chat-bots that will answer more complicated questions  in general subjects  is a growing domain of research.   The goal of this project is to create a chat-bot able to answer python related questions.  Our project started with the main idea being that a programming assistant would be a much needed help by many people working or studying computer science. Although it sounds simple it soon proved to be a difficult task. The main challenge is that the model has to extract a technical correlation between Questions and Answers in order to be able to communicate effectively. The model that we used in order to achieve our goal is a recurrent sequence-to-sequence model. The main steps that we followed are described bellow.  - We found  downloaded  and processed data taken from stack overflow concerning questions that contained at least one python tag.[7] - Implement a sequence-to-sequence model. - Jointly train encoder and decoder models using mini-batches - Used greedy-search decoding - Interact with trained chatbot   """;Natural Language Processing;https://github.com/vGkatsis/Chat_Bot_DL
"""Parameters related to training and evaluation can be set in `train.py`  as follows:  |  Parameters   | default  | description | other | |  ----  |  ----  |  ----  |  ----  | | config| None  Mandatory| Configuration file path || | --eval| None  Optional| Evaluate after an epoch |If you don't select this  you might have trouble finding the best_model| | --fp16| None  Optional| Semi-precision training |If this option is not selected  32GB of video memory may not be sufficient| | --resume| None  Optional | Recovery training |For example: --resume output/yolov2_voc/66|   """;Computer Vision;https://github.com/nuaaceieyty/Paddle-YOLOv4
"""Traditional [WGAN](https://arxiv.org/abs/1701.07875) uses an approximation of the Wasserstein metric to opimize the generator. This Wasserstein metric in turn depends upon an underlying metric on _images_ which is taken to be the <img src=""https://latex.codecogs.com/svg.latex?%5Cell%5E2""> norm  <img src=""https://latex.codecogs.com/svg.latex?%5C%7Cx%5C%7C_%7B2%7D%20%3D%20%5Cleft%28%20%5Csum_%7Bi%3D1%7D%5En%20x_i%5E2%20%5Cright%29%5E%7B1/2%7D"">  The article extends the theory of [WGAN-GP](https://arxiv.org/abs/1704.00028) to any [Banach space](https://en.wikipedia.org/wiki/Banach_space)  while this code can be used to train WGAN over any [Sobolev space](https://en.wikipedia.org/wiki/Sobolev_space) <img src=""https://latex.codecogs.com/svg.latex?W%5E%7Bs%2C%20p%7D""> with norm  <img src=""https://latex.codecogs.com/svg.latex?%5C%7Cf%5C%7C_%7BW%5E%7Bs%2C%20p%7D%7D%20%3D%20%5Cleft%28%20%5Cint_%7B%5COmega%7D%20%5Cleft%28%20%5Cmathcal%7BF%7D%5E%7B-1%7D%20%5Cleft%5B%20%281%20&plus;%20%7C%5Cxi%7C%5E2%29%5E%7Bs/2%7D%20%5Cmathcal%7BF%7D%20f%20%5Cright%5D%28x%29%20%5Cright%29%5Ep%20dx%20%5Cright%29%5E%7B1/p%7D"">  The parameters _p_ can be used to control the focus on outliers  with high _p_ indicating a strong focus on the worst offenders. _s_ can be used to control focus on small/large scale behaviour  where negative _s_ indicates focus on large scales  while positive _s_ indicates focus on small scales (e.g. edges).   """;General;https://github.com/adler-j/bwgan
"""Traditional [WGAN](https://arxiv.org/abs/1701.07875) uses an approximation of the Wasserstein metric to opimize the generator. This Wasserstein metric in turn depends upon an underlying metric on _images_ which is taken to be the <img src=""https://latex.codecogs.com/svg.latex?%5Cell%5E2""> norm  <img src=""https://latex.codecogs.com/svg.latex?%5C%7Cx%5C%7C_%7B2%7D%20%3D%20%5Cleft%28%20%5Csum_%7Bi%3D1%7D%5En%20x_i%5E2%20%5Cright%29%5E%7B1/2%7D"">  The article extends the theory of [WGAN-GP](https://arxiv.org/abs/1704.00028) to any [Banach space](https://en.wikipedia.org/wiki/Banach_space)  while this code can be used to train WGAN over any [Sobolev space](https://en.wikipedia.org/wiki/Sobolev_space) <img src=""https://latex.codecogs.com/svg.latex?W%5E%7Bs%2C%20p%7D""> with norm  <img src=""https://latex.codecogs.com/svg.latex?%5C%7Cf%5C%7C_%7BW%5E%7Bs%2C%20p%7D%7D%20%3D%20%5Cleft%28%20%5Cint_%7B%5COmega%7D%20%5Cleft%28%20%5Cmathcal%7BF%7D%5E%7B-1%7D%20%5Cleft%5B%20%281%20&plus;%20%7C%5Cxi%7C%5E2%29%5E%7Bs/2%7D%20%5Cmathcal%7BF%7D%20f%20%5Cright%5D%28x%29%20%5Cright%29%5Ep%20dx%20%5Cright%29%5E%7B1/p%7D"">  The parameters _p_ can be used to control the focus on outliers  with high _p_ indicating a strong focus on the worst offenders. _s_ can be used to control focus on small/large scale behaviour  where negative _s_ indicates focus on large scales  while positive _s_ indicates focus on small scales (e.g. edges).   """;Computer Vision;https://github.com/adler-j/bwgan
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/xiaoyongzhu/Deformable-ConvNets
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/qilei123/DEEPLAB_4_RETINAIMG
"""Use the fastai (https://www.fast.ai/)framework to implement transfer learning for text classification.  The language model was trained on a corpus named Wikitext-103.(https://openreview.net/pdf?id=Byj72udxe)  If you want to know some detailed information  please refer to Jeremy Howard’s paper.(https://arxiv.org/abs/1801.06146v5)  """;General;https://github.com/SkullFang/ULMFIT_NLP_Classification
"""Use the fastai (https://www.fast.ai/)framework to implement transfer learning for text classification.  The language model was trained on a corpus named Wikitext-103.(https://openreview.net/pdf?id=Byj72udxe)  If you want to know some detailed information  please refer to Jeremy Howard’s paper.(https://arxiv.org/abs/1801.06146v5)  """;Natural Language Processing;https://github.com/SkullFang/ULMFIT_NLP_Classification
"""  pytorch implement for PointNet(point cloud segmentation)   """;Computer Vision;https://github.com/PaParaZz1/PointNet
"""The *Show and Tell* model is a deep neural network that learns how to describe the content of images. For example:  ![Example captions](g3doc/example_captions.jpg)   """;Computer Vision;https://github.com/brandontrabucco/im2txt_match
"""We are given satellite images (more accurately sections of satellite images)  which might contain ships or other waterborne vehicles. The goal is to segment the images to the ""ship""/""no-ship"" classes (label each pixel using these classes). The images might contain multiple ships  they can be placed close to each other (yet should be detected as separate ships)  they can be located in ports  can be moving or stationary  etc. The pictures might show inland areas the sea without ships  can be cloudy or foggy  lighting conditions can vary.  The training data is given as images and masks for the ships (in a run length encoded format). If an image contains multiple ships  each ship has a separate record  mask.    """;Computer Vision;https://github.com/kaland313/vitmav45-ShipSeakers
"""Generative Adversarial Networks (GANs) are one of the most popular (and coolest) Machine Learning algorithms developed in recent times. They belong to a set of algorithms called generative models  which are widely used for unupervised learning tasks which aim to learn the uderlying structure of the given data. As the name suggests GANs allow you to generate new unseen data that mimic the actual given real data. However  GANs pose problems in training and require carefullly tuned hyperparameters.This paper aims to solve this problem.  DCGAN is one of the most popular and succesful network design for GAN. It mainly composes of convolution layers  without max pooling or fully connected layers. It uses strided convolutions and transposed convolutions  for the downsampling and the upsampling respectively.  **Generator architecture of DCGAN** <p align=""center""> <img src=""images/Generator.png"" title=""DCGAN Generator"" alt=""DCGAN Generator""> </p>  **Network Design of DCGAN:** * Replace all pooling layers with strided convolutions. * Remove all fully connected layers. * Use transposed convolutions for upsampling. * Use Batch Normalization after every layer except after the output layer of the generator and the input layer of the discriminator. * Use ReLU non-linearity for each layer in the generator except for output layer use tanh. * Use Leaky-ReLU non-linearity for each layer of the disciminator excpet for output layer use sigmoid.   """;Computer Vision;https://github.com/Natsu6767/DCGAN-PyTorch
"""This repo trains a Reinforcement Learning Neural Network so that it's able to play Pong from raw pixel input.  I've written up a [blog post](https://medium.com/@omkarv/intro-to-reinforcement-learning-pong-92a94aa0f84d) which walks through the code here and the basic principles of Reinforcement Learning  with Pong as the guiding example.  It is largely based on [a Gist by Andrej Karpathy](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)  which in turn is based on the [Playing Atari with Deep Reinforcement Learning paper by Mnih et al.](https://arxiv.org/abs/1312.5602)  This script uses the [Open AI Gym environments](https://github.com/openai/gym) in order to run the Atari emulator and environments  and currently uses no external ML framework & only numpy.   """;Reinforcement Learning;https://github.com/omkarv/pong-from-pixels
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M and VGG2 datasets  which were already packed in the MxNet binary format. The network backbones include ResNet  InceptionResNet_v2  DenseNet  DPN and MobileNet. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss. * loss-type=0:  Softmax * loss-type=1:  SphereFace * loss-type=2:  CosineFace * loss-type=4:  ArcFace * loss-type=5:  Combined Margin * loss-type=12: TripletLoss  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/chenggongliang/arcface
""" ![sn](./assests/sn.png)     """;Computer Vision;https://github.com/taki0112/Spectral_Normalization-Tensorflow
""" ![sn](./assests/sn.png)     """;General;https://github.com/taki0112/Spectral_Normalization-Tensorflow
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   """;Computer Vision;https://github.com/sunhui1234/haha
"""***Note***: there is now a PyTorch version of this toolkit ([fairseq-py](https://github.com/pytorch/fairseq)) and new development efforts will focus on it. The Lua version is preserved here  but is provided without any support.  This is fairseq  a sequence-to-sequence learning toolkit for [Torch](http://torch.ch/) from Facebook AI Research tailored to Neural Machine Translation (NMT). It implements the convolutional NMT models proposed in [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) and [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344) as well as a standard LSTM-based model. It features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU. We provide pre-trained models for English to French  English to German and English to Romanian translation.  ![Model](fairseq.gif)   """;Natural Language Processing;https://github.com/facebookresearch/fairseq
"""This is the repo for the Tensorflow implementation of cellSTORM based on the conditional generative adversarial network (cGAN) implmented by pix2pix-tensoflow. In this case it learns a mapping from input images (i.e. degraded  noisy  compressed video-sequences of dSTORM blinking events) to output images (i.e. localization maps/center of possible blinking fluorophore)  The repository has also a scipt to export a given Graph trained on GPU to a cellphone. The cellSTORM localizer APP can be found in another repo.    """;Computer Vision;https://github.com/bionanoimaging/cellSTORM-Tensorflow
"""This is the repo for the Tensorflow implementation of cellSTORM based on the conditional generative adversarial network (cGAN) implmented by pix2pix-tensoflow. In this case it learns a mapping from input images (i.e. degraded  noisy  compressed video-sequences of dSTORM blinking events) to output images (i.e. localization maps/center of possible blinking fluorophore)  The repository has also a scipt to export a given Graph trained on GPU to a cellphone. The cellSTORM localizer APP can be found in another repo.    """;General;https://github.com/bionanoimaging/cellSTORM-Tensorflow
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO** (https://pjreddie.com/darknet/yolo/) and to **Erik Lindernoren for the PyTorch implementation** this work is based on (https://github.com/eriklindernoren/PyTorch-YOLOv3).   This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information on Ultralytics projects please visit: https://www.ultralytics.com.   """;Computer Vision;https://github.com/guagen/yolov3-ultralytics-source
"""Generative Adversarial Networks (GANs) are one of the most popular (and coolest) Machine Learning algorithms developed in recent times. They belong to a set of algorithms called generative models  which are widely used for unupervised learning tasks which aim to learn the uderlying structure of the given data. As the name suggests GANs allow you to generate new unseen data that mimic the actual given real data. However  GANs pose problems in training and require carefullly tuned hyperparameters.This paper aims to solve this problem.  DCGAN is one of the most popular and succesful network design for GAN. It mainly composes of convolution layers  without max pooling or fully connected layers. It uses strided convolutions and transposed convolutions  for the downsampling and the upsampling respectively.  **Generator architecture of DCGAN** <p align=""center""> <img src=""images/Generator.png"" title=""DCGAN Generator"" alt=""DCGAN Generator""> </p>  **Network Design of DCGAN:** * Replace all pooling layers with strided convolutions. * Remove all fully connected layers. * Use transposed convolutions for upsampling. * Use Batch Normalization after every layer except after the output layer of the generator and the input layer of the discriminator. * Use ReLU non-linearity for each layer in the generator except for output layer use tanh. * Use Leaky-ReLU non-linearity for each layer of the disciminator excpet for output layer use sigmoid.   """;Computer Vision;https://github.com/davidhalladay/CE-compressive-sensing-on-generative-model-pytorch
"""RetinaFace is a practical single-stage [SOTA](http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html) face detector which is initially described in [arXiv technical report](https://arxiv.org/abs/1905.00641)  ![demoimg1](https://github.com/deepinsight/insightface/blob/master/resources/11513D05.jpg)  ![demoimg2](https://github.com/deepinsight/insightface/blob/master/resources/widerfacevaltest.png)   """;General;https://github.com/1996scarlet/ArcFace-Multiplex-Recognition
"""This folder contains the deploy files(include generator scripts) and pre-train models of resnet-v1  resnet-v2  inception-v3  inception-resnet-v2 and densenet(coming soon).  We didn't train any model from scratch  some of them are converted from other deep learning framworks (inception-v3 from [mxnet](https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-inception-v3.md)  inception-resnet-v2 from [tensorflow](https://github.com/tensorflow/models/blob/master/slim/nets/inception_resnet_v2.py))  some of them are converted from other modified caffe ([resnet-v2](https://github.com/yjxiong/caffe/tree/mem)). But to achieve the original performance  finetuning is performed on imagenet for several epochs.   The main contribution belongs to the authors and model trainers.   """;Computer Vision;https://github.com/GeekLiB/caffe-model
"""This folder contains the deploy files(include generator scripts) and pre-train models of resnet-v1  resnet-v2  inception-v3  inception-resnet-v2 and densenet(coming soon).  We didn't train any model from scratch  some of them are converted from other deep learning framworks (inception-v3 from [mxnet](https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-inception-v3.md)  inception-resnet-v2 from [tensorflow](https://github.com/tensorflow/models/blob/master/slim/nets/inception_resnet_v2.py))  some of them are converted from other modified caffe ([resnet-v2](https://github.com/yjxiong/caffe/tree/mem)). But to achieve the original performance  finetuning is performed on imagenet for several epochs.   The main contribution belongs to the authors and model trainers.   """;General;https://github.com/GeekLiB/caffe-model
"""CenterNet is a framework for object detection with deep convolutional neural networks. You can use the code to train and evaluate a network for object detection on the MS-COCO dataset.  * It achieves state-of-the-art performance (an AP of 47.0%) on one of the most challenging dataset: MS-COCO.  * Our code is written in Python  based on [CornerNet](https://github.com/princeton-vl/CornerNet).  *More detailed descriptions of our approach and code will be made available soon.*  **If you encounter any problems in using our code  please contact Kaiwen Duan: kaiwen.duan@vipl.ict.ac.cn.**   """;Computer Vision;https://github.com/takooctopus/CenterNet-Tako
"""In Neural Network Language Models(NNLM) with huge number of words in vocabulary  exhaustive activation functions such as Softmax are very slow.  This paper addresses shortcomings of Softmax. It consists of mainly two ideas 1. Representing words as low-dimensional feature vectors - to learn relation between words and contexts. 2. Clustering similar words in similar components(subtree) using the feature vectors.  Following is the summary of the Hierarchical Log-Bilinear Model. (If this explanation doesn't summarise the content please go to Section 4 in the Paper) * Initially start with a random binary tree. With words as leaf. * Use log-bilinear model to fit training data.    * Input will be context: w<sub>1</sub> w<sub>2</sub> ... w<sub>n-1</sub>.      * Each word w is represented by a feature vector r<sub>w<sub>1</sub></sub>. Say shape (1 100) each.     * So input at each forward pass will be (n-1  1  100)   * Hidden Layers apply matrix transformations  with weights C <p align='center'> <img src='https://github.com/AshwinDeshpande96/Hierarchical-Softmax/blob/master/Screenshot%202019-06-05%20at%208.38.05%20PM.png' width=150> </p>      * Output will be w<sub>n</sub>     * Will be a predicted feature vector r_hat     * So output shape at each forward pass will be (1 100)       * If there are 8 words in vocabulary (output classes)(Fig-1)         * Each of q<sub>i</sub> are multiplied with output r_hat and activated using sigmoid. Gives the probability of decision going to left subtree.          <p align='center'> P(d<sub>i</sub> = 1): sigmoid( r_hat * q<sub>i</sub> + b<sub>i</sub>) </p>                  * Each leaf is scored according to it's decision code. For example:            * leaf_5: P(d<sub>1</sub>=0  d<sub>3</sub>=1  d<sub>6</sub>=1)           * leaf_3: P(d<sub>1</sub>=1  d<sub>2</sub>=0  d<sub>5</sub>=1) * Fit the model with given data   * This is a teacher-forcing type of model  output at time step t is used at the next step t+1.   * This creates feature vectors r_hat depending on the context as we train. <p align='center'> <img src='https://github.com/AshwinDeshpande96/Hierarchical-Softmax/blob/master/tree.png'> </p>  Task specific feature vector perform well in every NLP task  because the tailored feature vector represent the training data well: [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)     """;Natural Language Processing;https://github.com/AshwinDeshpande96/Hierarchical-Softmax
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/icewing1996/bert_dep
"""![motivation of CCNet](https://user-images.githubusercontent.com/4509744/50546460-7df6ed00-0bed-11e9-9340-d026373b2cbe.png) Long-range dependencies can capture useful contextual information to benefit visual understanding problems. In this work  we propose a Criss-Cross Network (CCNet) for obtaining such important information through a more effective and efficient way. Concretely  for each pixel  our CCNet can harvest the contextual information of its surrounding pixels on the criss-cross path through a novel criss-cross attention module. By taking a further recurrent operation  each pixel can finally capture the long-range dependencies from all pixels. Overall  our CCNet is with the following merits:  - **GPU memory friendly**   - **High computational efficiency**  - **The state-of-the-art performance**    """;Computer Vision;https://github.com/speedinghzl/CCNet
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/wangshanmin/interface
"""In this project we are trying to build a system which can parse clinical discharge notes and generate information about clinical events along with temporal information about their occurrence. In  phase 1 of the project we establish a foundation for that by treating clinical events and temporal information as individual clinical entities and extracting those by performing Named Entity Recognition (NER) using fine-tuned NCBI BERT model. In phase 2 of the project  we take inspiration from the paper KG-BERT to train a model using the word embedding representations generated from phase-1 fine-tuned model. We observe that the model do not perform well on test data. Thus we analyze the nature of the relationships and the model behavior to find out possible reasons of the poor performance.  Then we decide to fine-tune NCBI BERT for relation extraction task. For that we format the input in a specific way so that the model can perform sequence classification successfully using Transformer's attention patterns and BERT special tokens. We evaluate the model on test data and see significant improvement. We dockerise the solution along with the Flask application developed in phase-1 used to visualise the tagged data.   Though the fine-tuned NCBI-BERT model perform well on test data  there exists a major challenge related to usability of the model. So far the model is tested on relevant entity pairs for which TLINKs are available. The way of finding such relevant entity pairs from the available set of entities is yet to be found.  Another area with scope of improvement is to augment data using the nature of relationships to improve the model behavior. For example  if event A is known to happen before patient admission  it means it also occurs before all other events which occurred after admission. Also the input to BERT while fine-tuning can be improved. Our experiment can be treated as a baseline for future research and development in this area.   """;Natural Language Processing;https://github.com/ManasRMohanty/DS5500-capstone
"""DeepLab is a state-of-art deep learning system for semantic image segmentation built on top of [Caffe](http://caffe.berkeleyvision.org).  It combines (1) *atrous convolution* to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks  (2) *atrous spatial pyramid pooling* to robustly segment objects at multiple scales with filters at multiple sampling rates and effective fields-of-views  and (3) densely connected conditional random fields (CRF) as post processing.  This distribution provides a publicly available implementation for the key model ingredients reported in our latest [arXiv paper](http://arxiv.org/abs/1606.00915). This version also supports the experiments (DeepLab v1) in our ICLR'15. You only need to modify the old prototxt files. For example  our proposed atrous convolution is called dilated convolution in CAFFE framework  and you need to change the convolution parameter ""hole"" to ""dilation"" (the usage is exactly the same). For the experiments in ICCV'15  there are some differences between our argmax and softmax_loss layers and Caffe's. Please refer to [DeepLabv1](https://bitbucket.org/deeplab/deeplab-public/) for details.  Please consult and consider citing the following papers:      @article{CP2016Deeplab        title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets  Atrous Convolution  and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        journal={arXiv:1606.00915}        year={2016}     }      @inproceedings{CY2016Attention        title={Attention to Scale: Scale-aware Semantic Image Segmentation}        author={Liang-Chieh Chen and Yi Yang and Jiang Wang and Wei Xu and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{CB2016Semantic        title={Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform}        author={Liang-Chieh Chen and Jonathan T Barron and George Papandreou and Kevin Murphy and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{PC2015Weak        title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation}        author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille}        booktitle={ICCV}        year={2015}     }      @inproceedings{CP2015Semantic        title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        booktitle={ICLR}        year={2015}     }   Note that if you use the densecrf implementation  please consult and cite the following paper:      @inproceedings{KrahenbuhlK11        title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}        author={Philipp Kr{\""{a}}henb{\""{u}}hl and Vladlen Koltun}        booktitle={NIPS}        year={2011}     }   """;Computer Vision;https://github.com/open-cv/deeplab-v2
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/gmt710/caffe_gpu
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/SYangDong/bert-with-frozen-code
"""Convolutional neural network is a key architure to recognize and classify the image to classes. However  variation of convolutional neural networks are very many and very vary their space/time usage for learning.   In this project  I compared the state-of-art convolutional neural networks and compared them based on **Performance** and **Resources**(space/time). Through this project  it helps to decide the model to solve problem based on problem size and limitation of resources.    --------   """;Computer Vision;https://github.com/wonjaek36/sodeep_final
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/junhahyung/bert_transfer
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/junhahyung/bert_transfer
"""[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.   """;Natural Language Processing;https://github.com/romik9999/fasttext-1925f09ed3
"""Today's deep learning models need to be trained on large-scale surveillance data sets. This means that for each data  there will be a corresponding label. In the popular ImageNet dataset  there are one million images with manual annotations  that is  1000 of each of the 1000 categories. Creating such a data set requires a lot of effort  and it may take a lot of people to spend months working on it. Assuming that you now have to create a data set of one million classes  you must label each frame in a total of 100 million frames of video data  which is basically impossible.  Ideally  we want to have a model that runs more like our brain. It requires only a few tags to understand many things in the real world. In the real world  I refer to classes that are object categories  action categories  environment categories  categories of objects  and many more.   """;Computer Vision;https://github.com/Yiming-Miao/Miniproject2
"""This is the official code of [high-resolution representations for ImageNet classification](https://arxiv.org/abs/1904.04514).  We augment the HRNet with a classification head shown in the figure below. First  the four-resolution feature maps are fed into a bottleneck and the number of output channels are increased to 128  256  512  and 1024  respectively. Then  we downsample the high-resolution representations by a 2-strided 3x3 convolution outputting 256 channels and add them to the representations of the second-high-resolution representations. This process is repeated two times to get 1024 channels over the small resolution. Last  we transform 1024 channels to 2048 channels through a 1x1 convolution  followed by a global average pooling operation. The output 2048-dimensional representation is fed into the classifier.  ![](figures/cls-hrnet.png)   """;Computer Vision;https://github.com/HRNet/HRNet-Image-Classification
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/zqplgl/caffe
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M and VGG2 datasets  which were already packed in the MxNet binary format. The network backbones include ResNet  InceptionResNet_v2  DenseNet  DPN and MobiletNet. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss. * loss-type=0:  Softmax * loss-type=1:  SphereFace * loss-type=2:  CosineFace * loss-type=4:  ArcFace * loss-type=5:  Combined Margin * loss-type=12: TripletLoss  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/sunil-rival/insightface
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/icewing1996/bert_dep
"""- A disciplined approach to Neural Network Hyper-parameters:  - IIIT and Oxford Pet research paper:    """;General;https://github.com/AbhimanyuAryan/ImageClassification
"""This is the official PyTorch implementation of the **Attribute2Font: Creating Fonts You Want From Attributes**.  ![Teaser](img/teaser.png)  Paper: [arXiv](https://arxiv.org/abs/2005.07865) | [Research Gate](https://www.researchgate.net/publication/341423467_Attribute2Font_Creating_Fonts_You_Want_From_Attributes/comments)    Supplementary Material: [link](paper/Siggraph2020_Attr2Font_Supplemental_Material.pdf)    Video: [link](img/att2font_demo.mov)    Code: [GitHub](https://github.com/hologerry/Attr2Font)        """;Computer Vision;https://github.com/hologerry/Attr2Font
"""This is a PyTorch implementation of the __L__ ow Rank F __a__ ctorization for Compact __M__ ulti-Head __A__ ttention (LAMA) mechanism and the corresponding pooler introduced in the paper: ""[Low Rank Factorization for Compact Multi-Head Self-Attention](https://arxiv.org/abs/1912.00835)"".  ![](img/figure_1.jpg)  > Figure 1 from [Low Rank Factorization for Compact Multi-Head Self-Attention](https://arxiv.org/abs/1912.00835).  Note: I am _not_ one of the authors on the paper.   """;General;https://github.com/JohnGiorgi/compact-multi-head-self-attention-pytorch
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/sunqiangxtcsun/SSD
"""Convolutional neural network is a key architure to recognize and classify the image to classes. However  variation of convolutional neural networks are very many and very vary their space/time usage for learning.   In this project  I compared the state-of-art convolutional neural networks and compared them based on **Performance** and **Resources**(space/time). Through this project  it helps to decide the model to solve problem based on problem size and limitation of resources.    --------   """;General;https://github.com/wonjaek36/sodeep_final
"""Deep Q-Learning Network (DQN) [2] is one of the most popular deep reinforcement learning algorithms. It is an off-policy learning algorithm that is highly sample efficient. Over the years  many improvements were proposed to improve the performance of DQN. Of the many extensions available for the DQN algorithm  some popular enhancements were combined by the DeepMind team and presented as the Rainbow DQN algorithm. These imporvements were found to be mostly orthogonal  with each component contributing to various degrees.  The six add-ons to the base DQN algorithm in the Rainbow version are      1. Double Q-Learning      2. Prioritized Experience Replay      3. Dueling Networks      4. Multi-step (n-step) Learning      5. Distributional Value Learning      6. Noisy Networks   This repository contains an implementation of the rainbow DQN algorithm put forward by the DeepMind team in the paper '[Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)'. [1]   <p align=""center"" >   <img width=""160"" height=""210"" src=""media/pong.gif"">   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   <img width=""315"" height=""210"" src=""media/cartpole.gif""> </p>    """;Reinforcement Learning;https://github.com/mohith-sakthivel/rainbow_dqn
"""This repository contains code to train and evaluate 3D Convolutional Neural Networks for semantic segmentation on medical images. The architectures developed in this framework are a combination of auto-encoder [UNet](https://arxiv.org/abs/1505.04597) with shortcut connections as in [ResNet](https://arxiv.org/abs/1512.03385)  densely connections for deep supervision as in [DensetNet](https://arxiv.org/abs/1608.06993) and Merge-And-Run mapping for attention focusing as in [MRGE](https://arxiv.org/abs/1611.07718).   """;General;https://github.com/lab-midas/med_segmentation
"""This repository contains code to train and evaluate 3D Convolutional Neural Networks for semantic segmentation on medical images. The architectures developed in this framework are a combination of auto-encoder [UNet](https://arxiv.org/abs/1505.04597) with shortcut connections as in [ResNet](https://arxiv.org/abs/1512.03385)  densely connections for deep supervision as in [DensetNet](https://arxiv.org/abs/1608.06993) and Merge-And-Run mapping for attention focusing as in [MRGE](https://arxiv.org/abs/1611.07718).   """;Computer Vision;https://github.com/lab-midas/med_segmentation
"""- [Installation](https://github.com/Cadene/pretrained-models.pytorch#installation) - [Quick examples](https://github.com/Cadene/pretrained-models.pytorch#quick-examples) - [Few use cases](https://github.com/Cadene/pretrained-models.pytorch#few-use-cases)     - [Compute imagenet logits](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-logits)     - [Compute imagenet validation metrics](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-validation-metrics) - [Evaluation on ImageNet](https://github.com/Cadene/pretrained-models.pytorch#evaluation-on-imagenet)     - [Accuracy on valset](https://github.com/Cadene/pretrained-models.pytorch#accuracy-on-validation-set)     - [Reproducing results](https://github.com/Cadene/pretrained-models.pytorch#reproducing-results) - [Documentation](https://github.com/Cadene/pretrained-models.pytorch#documentation)     - [Available models](https://github.com/Cadene/pretrained-models.pytorch#available-models)         - [AlexNet](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [BNInception](https://github.com/Cadene/pretrained-models.pytorch#bninception)         - [CaffeResNet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet)         - [DenseNet121](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet161](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet169](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DualPathNet68](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet92](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet98](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet107](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet113](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [FBResNet152](https://github.com/Cadene/pretrained-models.pytorch#facebook-resnet)         - [InceptionResNetV2](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [InceptionV3](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [InceptionV4](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [NASNet-A-Large](https://github.com/Cadene/pretrained-models.pytorch#nasnet)         - [NASNet-A-Mobile](https://github.com/Cadene/pretrained-models.pytorch#nasnet)         - [PNASNet-5-Large](https://github.com/Cadene/pretrained-models.pytorch#pnasnet)         - [PolyNet](https://github.com/Cadene/pretrained-models.pytorch#polynet)         - [ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)         - [ResNeXt101_64x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)         - [ResNet101](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet152](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet18](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet34](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet50](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [SENet154](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet50](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet101](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet152](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNeXt50_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SqueezeNet1_0](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [SqueezeNet1_1](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG11](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG13](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG16](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG19](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG11_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG13_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG16_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG19_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [Xception](https://github.com/Cadene/pretrained-models.pytorch#xception)     - [Model API](https://github.com/Cadene/pretrained-models.pytorch#model-api)         - [model.input_size](https://github.com/Cadene/pretrained-models.pytorch#modelinput_size)         - [model.input_space](https://github.com/Cadene/pretrained-models.pytorch#modelinput_space)         - [model.input_range](https://github.com/Cadene/pretrained-models.pytorch#modelinput_range)         - [model.mean](https://github.com/Cadene/pretrained-models.pytorch#modelmean)         - [model.std](https://github.com/Cadene/pretrained-models.pytorch#modelstd)         - [model.features](https://github.com/Cadene/pretrained-models.pytorch#modelfeatures)         - [model.logits](https://github.com/Cadene/pretrained-models.pytorch#modellogits)         - [model.forward](https://github.com/Cadene/pretrained-models.pytorch#modelforward) - [Reproducing porting](https://github.com/Cadene/pretrained-models.pytorch#reproducing)     - [ResNet*](https://github.com/Cadene/pretrained-models.pytorch#hand-porting-of-resnet152)     - [ResNeXt*](https://github.com/Cadene/pretrained-models.pytorch#automatic-porting-of-resnext)     - [Inception*](https://github.com/Cadene/pretrained-models.pytorch#hand-porting-of-inceptionv4-and-inceptionresnetv2)   """;Computer Vision;https://github.com/Cadene/pretrained-models.pytorch
"""- [Installation](https://github.com/Cadene/pretrained-models.pytorch#installation) - [Quick examples](https://github.com/Cadene/pretrained-models.pytorch#quick-examples) - [Few use cases](https://github.com/Cadene/pretrained-models.pytorch#few-use-cases)     - [Compute imagenet logits](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-logits)     - [Compute imagenet validation metrics](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-validation-metrics) - [Evaluation on ImageNet](https://github.com/Cadene/pretrained-models.pytorch#evaluation-on-imagenet)     - [Accuracy on valset](https://github.com/Cadene/pretrained-models.pytorch#accuracy-on-validation-set)     - [Reproducing results](https://github.com/Cadene/pretrained-models.pytorch#reproducing-results) - [Documentation](https://github.com/Cadene/pretrained-models.pytorch#documentation)     - [Available models](https://github.com/Cadene/pretrained-models.pytorch#available-models)         - [AlexNet](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [BNInception](https://github.com/Cadene/pretrained-models.pytorch#bninception)         - [CaffeResNet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet)         - [DenseNet121](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet161](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet169](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DualPathNet68](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet92](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet98](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet107](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet113](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [FBResNet152](https://github.com/Cadene/pretrained-models.pytorch#facebook-resnet)         - [InceptionResNetV2](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [InceptionV3](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [InceptionV4](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [NASNet-A-Large](https://github.com/Cadene/pretrained-models.pytorch#nasnet)         - [NASNet-A-Mobile](https://github.com/Cadene/pretrained-models.pytorch#nasnet)         - [PNASNet-5-Large](https://github.com/Cadene/pretrained-models.pytorch#pnasnet)         - [PolyNet](https://github.com/Cadene/pretrained-models.pytorch#polynet)         - [ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)         - [ResNeXt101_64x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)         - [ResNet101](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet152](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet18](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet34](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet50](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [SENet154](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet50](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet101](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet152](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNeXt50_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SqueezeNet1_0](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [SqueezeNet1_1](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG11](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG13](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG16](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG19](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG11_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG13_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG16_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG19_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [Xception](https://github.com/Cadene/pretrained-models.pytorch#xception)     - [Model API](https://github.com/Cadene/pretrained-models.pytorch#model-api)         - [model.input_size](https://github.com/Cadene/pretrained-models.pytorch#modelinput_size)         - [model.input_space](https://github.com/Cadene/pretrained-models.pytorch#modelinput_space)         - [model.input_range](https://github.com/Cadene/pretrained-models.pytorch#modelinput_range)         - [model.mean](https://github.com/Cadene/pretrained-models.pytorch#modelmean)         - [model.std](https://github.com/Cadene/pretrained-models.pytorch#modelstd)         - [model.features](https://github.com/Cadene/pretrained-models.pytorch#modelfeatures)         - [model.logits](https://github.com/Cadene/pretrained-models.pytorch#modellogits)         - [model.forward](https://github.com/Cadene/pretrained-models.pytorch#modelforward) - [Reproducing porting](https://github.com/Cadene/pretrained-models.pytorch#reproducing)     - [ResNet*](https://github.com/Cadene/pretrained-models.pytorch#hand-porting-of-resnet152)     - [ResNeXt*](https://github.com/Cadene/pretrained-models.pytorch#automatic-porting-of-resnext)     - [Inception*](https://github.com/Cadene/pretrained-models.pytorch#hand-porting-of-inceptionv4-and-inceptionresnetv2)   """;General;https://github.com/Cadene/pretrained-models.pytorch
"""We implement both the Nueral ODE and ResNet architectures for classification of electrocardiogram signals. The data is taken from the frequently used MIT-BIH ECG database  which contains over 100 000 labeled samples of ECG signals from a single heartbeat. The data and a description of the sampling procedure used can be found at https://www.physionet.org/content/mitdb/1.0.0/. We briefly visualize the data  which enables us an intuitive look at the various features and differences between each class that our neural networks will learn and distinguish.   We construct both network architectures with the help of PyTorch and the torchdiffeq library found in the original Neural ODE paper: https://arxiv.org/abs/1806.07366. The ResNet and ODENet  the implementation of the Neural ODE  are designed to be as similar as possible  so we can compare the two fairly. Both models contain identical downampling layers  1D convolutions  normalizations (Group)  activations (ReLU)  and output layers. We train both models and evaluate them on the testing set while also noting differences in speed  memory  and accuracy. Overall  both models perform comparably on the testing data. However  there is a tradeoff between speed and memory. The ResNet is faster to train while the ODENet contains fewer tunable parameters (less memory).   """;General;https://github.com/abaietto/neural_ode_classification
"""We implement both the Nueral ODE and ResNet architectures for classification of electrocardiogram signals. The data is taken from the frequently used MIT-BIH ECG database  which contains over 100 000 labeled samples of ECG signals from a single heartbeat. The data and a description of the sampling procedure used can be found at https://www.physionet.org/content/mitdb/1.0.0/. We briefly visualize the data  which enables us an intuitive look at the various features and differences between each class that our neural networks will learn and distinguish.   We construct both network architectures with the help of PyTorch and the torchdiffeq library found in the original Neural ODE paper: https://arxiv.org/abs/1806.07366. The ResNet and ODENet  the implementation of the Neural ODE  are designed to be as similar as possible  so we can compare the two fairly. Both models contain identical downampling layers  1D convolutions  normalizations (Group)  activations (ReLU)  and output layers. We train both models and evaluate them on the testing set while also noting differences in speed  memory  and accuracy. Overall  both models perform comparably on the testing data. However  there is a tradeoff between speed and memory. The ResNet is faster to train while the ODENet contains fewer tunable parameters (less memory).   """;Computer Vision;https://github.com/abaietto/neural_ode_classification
"""Population Based Augmentation (PBA) is a algorithm that quickly and efficiently learns data augmentation functions for neural network training. PBA matches state-of-the-art results on CIFAR with one thousand times less compute  enabling researchers and practitioners to effectively learn new augmentation policies using a single workstation GPU.  This repository contains code for the work ""Population Based Augmentation: Efficient Learning of Augmentation Schedules"" (http://arxiv.org/abs/1905.05393) in TensorFlow and Python. It includes training of models with the reported augmentation schedules and discovery of new augmentation policy schedules.  See below for a visualization of our augmentation strategy.  <p align=""center""> <img src=""figs/augs_v2_crop.png"" width=""40%""> </p>   """;Computer Vision;https://github.com/arcelien/pba
"""Mesh TensorFlow (`mtf`) is a language for distributed deep learning  capable of specifying a broad class of distributed tensor computations.  The purpose of Mesh TensorFlow is to formalize and implement distribution strategies for your computation graph over your hardware/processors. For example: ""Split the batch over rows of processors and split the units in the hidden layer across columns of processors."" Mesh TensorFlow is implemented as a layer over TensorFlow.  Watch our [YouTube video](https://www.youtube.com/watch?v=HgGyWS40g-g).    """;General;https://github.com/tensorflow/mesh
"""* **Day_01 : 資料介紹與評估指標**     * 探索流程 : 找到問題 -> 初探 -> 改進 -> 分享 -> 練習 -> 實戰     * 思考關鍵點 :         * 為什麼這個問題重要？         * 資料從何而來？         * 資料型態是什麼？         * 回答問題的關鍵指標是什麼？ * **Day_02 : 機器學習概論**     * 機器學習範疇 : **深度學習 (Deep Learning)** ⊂ **機器學習 (Machine Learning)** ⊂ **人工智慧 (Artificial Intelligence)**     * 機器學習是什麼 :         * 讓機器從資料找尋規律與趨勢，不需要給定特殊規則         * 給定目標函數與訓練資料，學習出能讓目標函數最佳的模型參數     * 機器學習總類 :         * **監督是學習 (Supervised Learning)** : 圖像分類 (Classification)、詐騙偵測 (Fraud detection)，需成對資料 (x y)         * **非監督是學習 (Unsupervised Learning)** : 降維 (Dimension Reduction)、分群 (Clustering)、壓縮，只需資料 (x)         * **強化學習 (Reinforcement Learning)** : 下圍棋、打電玩，透過代理機器人 (Agent) 與環境 (Environment) 互動，學習如何獲取最高獎勵 (Reward)，例如 Alpha GO * **Day_03 : 機器學習流程與步驟**     * **資料蒐集、前處理**         * 政府公開資料、Kaggle 資料             * 結構化資料 : Excel 檔、CSV 檔             * 非結構化資料 : 圖片、影音、文字         * 使用 Python 套件             * 開啟圖片 : `PIL`、`skimage`、`open-cv`             * 開啟文件 : `pandas`         * 資料前處理 :             * 缺失值填補             * 離群值處理             * 標準化     * **定義目標與評估準則**         * 回歸問題？分類問題？         * 預測目標是什麼？(target or y)         * 用什麼資料進行預測？(predictor or x)         * 將資料分為 :             * 訓練集，training set             * 驗證集，validation set             * 測試集，test set         * 評估指標             * 回歸問題 (預測值為實數)                 * RMSE : Root Mean Squeare Error                 * MAE : Mean Absolute Error                 * R-Square             * 分類問題 (預測值為類別)                 * Accuracy                 * [F1-score](https://en.wikipedia.org/wiki/F1_score)                 * [AUC](https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF)，Area Under Curve     * **建立模型與調整參數**         * Regression，回歸模型         * Tree-base model，樹模型         * Neural network，神經網路         * Hyperparameter，根據對模型了解和訓練情形進行調整     * **導入**         * 建立資料蒐集、前處理(Preprocessing)等流程         * 送進模型進行預測         * 輸出預測結果         * 視專案需求調整前後端 * **Day_04 : 讀取資料與分析流程 (EDA，Exploratory Data Analysis)**        * 透過視覺化和統計工具進行分析         * 了解資料 : 獲取資料包含的資訊、結構、特點         * 發現 outlier 或異常數值 : 檢查資料是否有誤         * 分析各變數間的關聯性 : 找出重要的變數     * 收集資料 -> 數據清理 -> 特徵萃取 -> 資料視覺化 -> 建立模型 -> 驗證模型 -> 決策應用  """;General;https://github.com/Halesu/4th-ML100Days
"""* **Day_01 : 資料介紹與評估指標**     * 探索流程 : 找到問題 -> 初探 -> 改進 -> 分享 -> 練習 -> 實戰     * 思考關鍵點 :         * 為什麼這個問題重要？         * 資料從何而來？         * 資料型態是什麼？         * 回答問題的關鍵指標是什麼？ * **Day_02 : 機器學習概論**     * 機器學習範疇 : **深度學習 (Deep Learning)** ⊂ **機器學習 (Machine Learning)** ⊂ **人工智慧 (Artificial Intelligence)**     * 機器學習是什麼 :         * 讓機器從資料找尋規律與趨勢，不需要給定特殊規則         * 給定目標函數與訓練資料，學習出能讓目標函數最佳的模型參數     * 機器學習總類 :         * **監督是學習 (Supervised Learning)** : 圖像分類 (Classification)、詐騙偵測 (Fraud detection)，需成對資料 (x y)         * **非監督是學習 (Unsupervised Learning)** : 降維 (Dimension Reduction)、分群 (Clustering)、壓縮，只需資料 (x)         * **強化學習 (Reinforcement Learning)** : 下圍棋、打電玩，透過代理機器人 (Agent) 與環境 (Environment) 互動，學習如何獲取最高獎勵 (Reward)，例如 Alpha GO * **Day_03 : 機器學習流程與步驟**     * **資料蒐集、前處理**         * 政府公開資料、Kaggle 資料             * 結構化資料 : Excel 檔、CSV 檔             * 非結構化資料 : 圖片、影音、文字         * 使用 Python 套件             * 開啟圖片 : `PIL`、`skimage`、`open-cv`             * 開啟文件 : `pandas`         * 資料前處理 :             * 缺失值填補             * 離群值處理             * 標準化     * **定義目標與評估準則**         * 回歸問題？分類問題？         * 預測目標是什麼？(target or y)         * 用什麼資料進行預測？(predictor or x)         * 將資料分為 :             * 訓練集，training set             * 驗證集，validation set             * 測試集，test set         * 評估指標             * 回歸問題 (預測值為實數)                 * RMSE : Root Mean Squeare Error                 * MAE : Mean Absolute Error                 * R-Square             * 分類問題 (預測值為類別)                 * Accuracy                 * [F1-score](https://en.wikipedia.org/wiki/F1_score)                 * [AUC](https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF)，Area Under Curve     * **建立模型與調整參數**         * Regression，回歸模型         * Tree-base model，樹模型         * Neural network，神經網路         * Hyperparameter，根據對模型了解和訓練情形進行調整     * **導入**         * 建立資料蒐集、前處理(Preprocessing)等流程         * 送進模型進行預測         * 輸出預測結果         * 視專案需求調整前後端 * **Day_04 : 讀取資料與分析流程 (EDA，Exploratory Data Analysis)**        * 透過視覺化和統計工具進行分析         * 了解資料 : 獲取資料包含的資訊、結構、特點         * 發現 outlier 或異常數值 : 檢查資料是否有誤         * 分析各變數間的關聯性 : 找出重要的變數     * 收集資料 -> 數據清理 -> 特徵萃取 -> 資料視覺化 -> 建立模型 -> 驗證模型 -> 決策應用  """;Computer Vision;https://github.com/Halesu/4th-ML100Days
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/thecodemasterk/BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/thecodemasterk/BERT
"""The increasing number of cars in cities can cause high volume of traffic  and implies that traffic violations become more critical nowadays in Bangladesh and also around the world. This causes severe destruction of property and more accidents that may endanger the lives of the people. To solve the alarming problem and prevent such unfathomable consequences  traffic violation detection systems are needed. For which the system enforces proper traffic regulations at all times  and apprehend those who does not comply. A traffic violation detection system must be realized in real-time as the authorities track the roads all the time. Hence  traffic enforcers will not only be at ease in implementing safe roads accurately  but also efficiently; as the traffic detection system detects violations faster than humans. This system can detect most common three types of traffic violation in real-time which are signal violation  parking violation and wrong direction violation. A user friendly graphical interface is associated with the system to make it simple for the user to operate the system  monitor traffic and take action against the violations of traffic rules.   """;General;https://github.com/rahulchaurasiya1/cautious-waddle
"""The increasing number of cars in cities can cause high volume of traffic  and implies that traffic violations become more critical nowadays in Bangladesh and also around the world. This causes severe destruction of property and more accidents that may endanger the lives of the people. To solve the alarming problem and prevent such unfathomable consequences  traffic violation detection systems are needed. For which the system enforces proper traffic regulations at all times  and apprehend those who does not comply. A traffic violation detection system must be realized in real-time as the authorities track the roads all the time. Hence  traffic enforcers will not only be at ease in implementing safe roads accurately  but also efficiently; as the traffic detection system detects violations faster than humans. This system can detect most common three types of traffic violation in real-time which are signal violation  parking violation and wrong direction violation. A user friendly graphical interface is associated with the system to make it simple for the user to operate the system  monitor traffic and take action against the violations of traffic rules.   """;Computer Vision;https://github.com/rahulchaurasiya1/cautious-waddle
"""One way to build good natural images is by training Generative Adversarial Networks (GANS).  and we can later even reuse parts of the generator and discriminator networks as feature extractors for supervised tasks .DCGANs is actually one of the class of GANs using CNNs architecture for both of its components i.e.  a generator and a discriminator.   •	DCGAN  """;Computer Vision;https://github.com/shvmshri/DCGAN_Tensorflow
"""* What   * GANs are based on adversarial training.   * Adversarial training is a basic technique to train generative models (so here primarily models that create new images).   * In an adversarial training one model (G  Generator) generates things (e.g. images). Another model (D  discriminator) sees real things (e.g. real images) as well as fake things (e.g. images from G) and has to learn how to differentiate the two.   * Neural Networks are models that can be trained in an adversarial way (and are the only models discussed here).  * How   * G is a simple neural net (e.g. just one fully connected hidden layer). It takes a vector as input (e.g. 100 dimensions) and produces an image as output.   * D is a simple neural net (e.g. just one fully connected hidden layer). It takes an image as input and produces a quality rating as output (0-1  so sigmoid).   * You need a training set of things to be generated  e.g. images of human faces.   * Let the batch size be B.   * G is trained the following way:     * Create B vectors of 100 random values each  e.g. sampled uniformly from [-1  +1]. (Number of values per components depends on the chosen input size of G.)     * Feed forward the vectors through G to create new images.     * Feed forward the images through D to create ratings.     * Use a cross entropy loss on these ratings. All of these (fake) images should be viewed as label=0 by D. If D gives them label=1  the error will be low (G did a good job).     * Perform a backward pass of the errors through D (without training D). That generates gradients/errors per image and pixel.     * Perform a backward pass of these errors through G to train G.   * D is trained the following way:     * Create B/2 images using G (again  B/2 random vectors  feed forward through G).     * Chose B/2 images from the training set. Real images get label=1.     * Merge the fake and real images to one batch. Fake images get label=0.     * Feed forward the batch through D.     * Measure the error using cross entropy.     * Perform a backward pass with the error through D.   * Train G for one batch  then D for one (or more) batches. Sometimes D can be too slow to catch up with D  then you need more iterations of D per batch of G.  * Results   * Good looking images MNIST-numbers and human faces. (Grayscale  rather homogeneous datasets.)   * Not so good looking images of CIFAR-10. (Color  rather heterogeneous datasets.)   ![Generated Faces](images/Generative_Adversarial_Networks__faces.jpg?raw=true ""Generated Faces"")  *Faces generated by MLP GANs. (Rightmost column shows examples from the training set.)*  -------------------------   """;Computer Vision;https://github.com/ashishmurali/Generative-Adversarial-Network
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Walter-B/bert-20-classes
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Walter-B/bert-20-classes
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/llx666/git_demo
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/llx666/git_demo
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/mamromer/yolov3
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/Gryffindor112358/Arcface
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/malin9402/retiface
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/jinzhenfan/BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/jinzhenfan/BERT
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/aviros/roatationPointnet
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/jih189/yolo3
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/shenyunhang/caffe-fwsl
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Gaozhen0816/BERT_QA_for_Chinese
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Gaozhen0816/BERT_QA_for_Chinese
"""COVID-19가 전 세계적인 영향을 미치며  자유로운 외부 활동이 거의 불가능해지는 현상이 지구 전체에 발생하였습니다. 이로 인해 집 안에서 활동해야 하는 시간이 늘어나면서 외부 활동과 관련된 소비액이 자연스레 ‘집안 활동’과 관련된 항목으로 옮겨가는 현상이 발생하였습니다.   국내외 가구 브랜드에서는 AR  VR 등 다양한 기술을 접목한 홈 스타일링 서비스를 선보이고 있지만  여전히 공간 전반을 아우르는 홈 스타일링 서비스를 제공받기 위해서는 매장을 방문해야 한다는 치명적인 단점이 존재하는 상황입니다.  이에 저희 팀은 모바일 홈 스타일링 솔루션 ‘Fitting Room’을 제공하고자 합니다.   서비스에 관한 자세한 내용은 ""APP Repositories""의 Readme에 정리하였으며  다음의 URL을 통해 확인해주시면 감사하겠습니다.   """;Computer Vision;https://github.com/KPMG-wiseuniv/AI
"""U-Net is a fully convolutional neural network with an encoder-decoder structure designed for sementic image segmantation on biomedical images. [[1]](#1) It is a very effective meta-network architecture that has been adapted to incorporate other convolutional neural network architecture designs.   """;Computer Vision;https://github.com/hayashimasa/UNet-PyTorch
"""Mesh TensorFlow (`mtf`) is a language for distributed deep learning  capable of specifying a broad class of distributed tensor computations.  The purpose of Mesh TensorFlow is to formalize and implement distribution strategies for your computation graph over your hardware/processors. For example: ""Split the batch over rows of processors and split the units in the hidden layer across columns of processors."" Mesh TensorFlow is implemented as a layer over TensorFlow.  Watch our [YouTube video](https://www.youtube.com/watch?v=HgGyWS40g-g).    """;Natural Language Processing;https://github.com/tensorflow/mesh
"""The goal of **semantic segmentation** is to identify objects  like cars and dogs  in an image by labelling the corresponding groups of pixels according to their classes. For an introduction  see <a href=""https://nanonets.com/blog/semantic-image-segmentation-2020/"">this article</a>. As an example  below is an image and its labelled pixels.  | <img src=""assets/rider.jpg"" alt=""biker"" width=400> | <img src=""assets/rider_label.png"" alt=""true label"" width=400> | |:---:|:---:| | Image | True label |  A **fully convolutional network (FCN)** is an artificial neural network that performs semantic segmentation.  The bottom layers of a FCN are those of a convolutional neural network (CNN)  usually taken from a pre-trained network like VGGNet or GoogLeNet. The purpose of these layers is to perform classification on subregions of the image. The top layers of a FCN are **transposed convolution/deconvolution** layers  which upsample the results of the classification to the resolution of the original image. This gives us a label for each pixel. When upsampling  we can also utilize the intermediate layers of the CNN to improve the accuracy of the segmentation. For an introduction  see <a href=""https://nanonets.com/blog/how-to-do-semantic-segmentation-using-deep-learning/"">this article</a>.  The <a href=""http://host.robots.ox.ac.uk/pascal/VOC/"">Pascal VOC project</a> is a dataset containing images whose pixels have been labeled according to 20 classes (excluding the background)  which include aeroplanes  cars  and people. We will be performing semantic segmentation according to this dataset.   """;Computer Vision;https://github.com/kevinddchen/Keras-FCN
"""├── kws    │   ├── metrics     │   │   ├── fnr_fpr.py   │   │   ├── __init__.py   │   ├── models    │   │   ├── attention.py   │   │   ├── crnn.py   │   │   ├── densenet.py   │   │   ├── dpn.py   │   │   ├── __init__.py   │   │   ├── resnet.py    │   │   ├── resnext.py   │   │   ├── treasure_net.py   │   │   ├── vgg.py   │   │   └── wideresnet.py   │   ├── transforms   │   ├── utils.py   ├── config.py    * *./kws/metrics* : Evaluation matrics  defining the False Rejection Rate (FRR) and False Alarm Rate (FAR) for keyword spotting * *./kws/models* : Diffferent network architecture  * *.config.py* : Configuration about parameters and hyperparameters  """;General;https://github.com/bozliu/E2E-Keyword-Spotting
"""├── kws    │   ├── metrics     │   │   ├── fnr_fpr.py   │   │   ├── __init__.py   │   ├── models    │   │   ├── attention.py   │   │   ├── crnn.py   │   │   ├── densenet.py   │   │   ├── dpn.py   │   │   ├── __init__.py   │   │   ├── resnet.py    │   │   ├── resnext.py   │   │   ├── treasure_net.py   │   │   ├── vgg.py   │   │   └── wideresnet.py   │   ├── transforms   │   ├── utils.py   ├── config.py    * *./kws/metrics* : Evaluation matrics  defining the False Rejection Rate (FRR) and False Alarm Rate (FAR) for keyword spotting * *./kws/models* : Diffferent network architecture  * *.config.py* : Configuration about parameters and hyperparameters  """;Computer Vision;https://github.com/bozliu/E2E-Keyword-Spotting
"""Automatic image colorization has been a popular image-to-image translation problem of significant interest for several practical application areas including restoration of aged or degraded images. This project attempts to utilize CycleGANs to colorize grayscale images back to their colorful RGB form.   """;Computer Vision;https://github.com/ArkaJU/Image-Colorization-CycleGAN
"""Automatic image colorization has been a popular image-to-image translation problem of significant interest for several practical application areas including restoration of aged or degraded images. This project attempts to utilize CycleGANs to colorize grayscale images back to their colorful RGB form.   """;General;https://github.com/ArkaJU/Image-Colorization-CycleGAN
"""![Swin Transformer Architecture Diagram](./images/swin-transformer.png)  **Swin Transformer** (the name `Swin` stands for **S**hifted **win**dow) is initially described in [arxiv](https://arxiv.org/abs/2103.14030)  which capably serves as a general-purpose backbone for computer vision. It is basically a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection.  Swin Transformer achieves strong performance on COCO object detection (`58.7 box AP` and `51.1 mask AP` on test-dev) and ADE20K semantic segmentation (`53.5 mIoU` on val)  surpassing previous models by a large margin.    """;Computer Vision;https://github.com/VcampSoldiers/Swin-Transformer-Tensorflow
"""***Note***: there is now a PyTorch version of this toolkit ([fairseq-py](https://github.com/pytorch/fairseq)) and new development efforts will focus on it. The Lua version is preserved here  but is provided without any support.  This is fairseq  a sequence-to-sequence learning toolkit for [Torch](http://torch.ch/) from Facebook AI Research tailored to Neural Machine Translation (NMT). It implements the convolutional NMT models proposed in [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) and [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344) as well as a standard LSTM-based model. It features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU. We provide pre-trained models for English to French  English to German and English to Romanian translation.  ![Model](fairseq.gif)   """;Audio;https://github.com/facebookresearch/fairseq
"""![EPS](figure/figure_EPS.png) Existing studies in weakly-supervised semantic segmentation (WSSS) using image-level weak supervision have several limitations:  sparse object coverage  inaccurate object boundaries   and co-occurring pixels from non-target objects.  To overcome these challenges  we propose a novel framework   namely Explicit Pseudo-pixel Supervision (EPS)   which learns from pixel-level feedback by combining two weak supervisions;  the image-level label provides the object identity via the localization map  and the saliency map from the off-the-shelf saliency detection model  offers rich boundaries. We devise a joint training strategy to fully  utilize the complementary relationship between both information.  Our method can obtain accurate object boundaries and discard co-occurring pixels   thereby significantly improving the quality of pseudo-masks.    """;Computer Vision;https://github.com/halbielee/EPS
"""YOLOX is an anchor-free version of YOLO  with a simpler design but better performance! It aims to bridge the gap between research and industrial communities. For more details  please refer to our [report on Arxiv](https://arxiv.org/abs/2107.08430).  This repo is an implementation of [MegEngine](https://github.com/MegEngine/MegEngine) version YOLOX  there is also a [PyTorch implementation](https://github.com/Megvii-BaseDetection/YOLOX).  <img src=""assets/git_fig.png"" width=""1000"" >   """;Computer Vision;https://github.com/MegEngine/YOLOX
"""***Note***: there is now a PyTorch version of this toolkit ([fairseq-py](https://github.com/pytorch/fairseq)) and new development efforts will focus on it. The Lua version is preserved here  but is provided without any support.  This is fairseq  a sequence-to-sequence learning toolkit for [Torch](http://torch.ch/) from Facebook AI Research tailored to Neural Machine Translation (NMT). It implements the convolutional NMT models proposed in [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) and [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344) as well as a standard LSTM-based model. It features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU. We provide pre-trained models for English to French  English to German and English to Romanian translation.  ![Model](fairseq.gif)   """;General;https://github.com/facebookresearch/fairseq
"""***Note***: there is now a PyTorch version of this toolkit ([fairseq-py](https://github.com/pytorch/fairseq)) and new development efforts will focus on it. The Lua version is preserved here  but is provided without any support.  This is fairseq  a sequence-to-sequence learning toolkit for [Torch](http://torch.ch/) from Facebook AI Research tailored to Neural Machine Translation (NMT). It implements the convolutional NMT models proposed in [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) and [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344) as well as a standard LSTM-based model. It features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU. We provide pre-trained models for English to French  English to German and English to Romanian translation.  ![Model](fairseq.gif)   """;Sequential;https://github.com/facebookresearch/fairseq
"""SSKD is implemented based on **FastReID v1.0.0**  it provides a semi-supervised feature learning framework to learn domain-general representations. The framework is shown in   <img src=""images/framework.png"" width=""850"" >   """;General;https://github.com/xiaomingzhid/sskd
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/SYangDong/bert-with-frozen-code
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.   ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    """;Natural Language Processing;https://github.com/habibullah-araphat/BERT-NER-TPU
"""SCRFD is an efficient high accuracy face detection approach which initially described in [Arxiv](https://arxiv.org/abs/2105.04714).  <img src=""https://github.com/nttstar/insightface-resources/blob/master/images/scrfd_evelope.jpg"" width=""400"" alt=""prcurve""/>   """;Computer Vision;https://github.com/SajjadAemmi/SCR-Face-Detection
"""SCRFD is an efficient high accuracy face detection approach which initially described in [Arxiv](https://arxiv.org/abs/2105.04714).  <img src=""https://github.com/nttstar/insightface-resources/blob/master/images/scrfd_evelope.jpg"" width=""400"" alt=""prcurve""/>   """;General;https://github.com/SajjadAemmi/SCR-Face-Detection
"""This repo provides the official implementation of Hamburger for further research. We sincerely hope that this paper can bring you inspiration about the Attention Mechanism  especially how **the low-rankness and the optimization-driven method** can help model the so-called *Global Information* in deep learning. We also highlight **Hamburger as a semi-implicit model and one-step gradient as an alternative for training both implicit and semi-implicit models**.  We model the global context issue as a low-rank completion problem and show that its optimization algorithms can help design global information blocks. This paper then proposes a series of Hamburgers  in which we employ the optimization algorithms for solving MDs to factorize the input representations into sub-matrices and reconstruct a low-rank embedding. Hamburgers with different MDs can perform favorably against the popular global context module self-attention when carefully coping with gradients back-propagated through MDs.  ![contents](assets/Hamburger.jpg)  We are working on some exciting topics. Please wait for our new papers. :)  Enjoy Hamburger  please!   """;General;https://github.com/Gsunshine/Enjoy-Hamburger
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/h4ste/oscar
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/h4ste/oscar
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/qilei123/fpn_crop
"""A brief content description is provided here  for detailed descriptions check the notebook comments     """;Computer Vision;https://github.com/5m0k3/gwd-yolov5-pytorch
"""SWA-Gaussian (SWAG) is a convenient method for uncertainty representation and calibration in Bayesian deep learning. The key idea of SWAG is that the SGD iterates  with a modified learning rate schedule  act like samples from a Gaussian distribution; SWAG fits this Gaussian distribution by capturing the [SWA](https://arxiv.org/abs/1803.05407) mean and a covariance matrix  representing the first two moments of SGD iterates. We use this Gaussian distribution as a posterior over neural network weights  and then perform a Bayesian model average  for uncertainty representation and calibration.  <p align=""center"">   <img src=""https://user-images.githubusercontent.com/14368801/52224039-09ab0b80-2875-11e9-9c12-c72b88abf4a9.png"" width=350>   <img src=""https://user-images.githubusercontent.com/14368801/52224049-0dd72900-2875-11e9-9de8-540ceaae60b3.png"" width=350> </p>   In this repo  we implement SWAG for image classification with several different architectures on both CIFAR datasets and ImageNet. We also implement SWAG for semantic segmentation on CamVid using our implementation of a FCDenseNet67. We additionally include several other experiments on exploring the covariance of the gradients of the SGD iterates  the eigenvalues of the Hessian  and width/PCA decompositions of the SWAG approximate posterior.  CIFAR10 -> STL10             |  CIFAR100 :-------------------------:|:-------------------------: ![](plots/stl_wrn.jpg)  |  ![](plots/c100_resnet110.jpg)  Please cite our work if you find it useful: ```bibtex @inproceedings{maddox_2019_simple    title={A simple baseline for bayesian uncertainty in deep learning}    author={Maddox  Wesley J and Izmailov  Pavel and Garipov  Timur and Vetrov  Dmitry P and Wilson  Andrew Gordon}    booktitle={Advances in Neural Information Processing Systems}    pages={13153--13164}    year={2019} } ```   """;General;https://github.com/wjmaddox/swa_gaussian
"""This project aims to implement biomedical image segmentation with the use of U-Net model. The below image briefly explains the output we want:  <p align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/segmentation_image.jpg"">   The dataset we used is Transmission Electron Microscopy (ssTEM) data set of the Drosophila first instar larva ventral nerve cord (VNC)  which is dowloaded from [ISBI Challenge: Segmentation of of neural structures in EM stacks](http://brainiac2.mit.edu/isbi_challenge/home)  The dataset contains 30 images (.png) of size 512x512 for each train  train-labels and test.    """;Computer Vision;https://github.com/ugent-korea/pytorch-unet-segmentation
""" **Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/fourmi1995/IronExperiment-DCN
"""We have two networks  G (Generator) and D (Discriminator).The Generator is a network for generating images.  It receives a random noise z and generates images from this noise  which is called G(z).Discriminator is  a discriminant network that discriminates whether an image is real. The input is x  x is a picture   and the output is D of x is the probability that x is a real picture  and if it's 1  it's 100% real   and if it's 0  it's not real.   """;General;https://github.com/HyeongJu916/Boaz-SR-ESRGAN-PyTorch
"""We have two networks  G (Generator) and D (Discriminator).The Generator is a network for generating images.  It receives a random noise z and generates images from this noise  which is called G(z).Discriminator is  a discriminant network that discriminates whether an image is real. The input is x  x is a picture   and the output is D of x is the probability that x is a real picture  and if it's 1  it's 100% real   and if it's 0  it's not real.   """;Computer Vision;https://github.com/HyeongJu916/Boaz-SR-ESRGAN-PyTorch
"""Recent Ubuntu releases come with python3 installed. I use pip3 for installing dependencies so install that with `sudo apt install python3-pip`. Install git if you don't already have it with `sudo apt install git`.  Then clone this repo with `git clone https://github.com/harvitronix/reinforcement-learning-car.git`. It has some pretty big weights files saved in past commits  so to just get the latest the fastest  do `git clone https://github.com/harvitronix/reinforcement-learning-car.git --depth 1`.   """;Reinforcement Learning;https://github.com/harvitronix/reinforcement-learning-car
"""This repository provides a Minimal PyTorch implementation of Proximal Policy Optimization (PPO) with clipped objective for OpenAI gym environments. It is primarily intended for beginners in [Reinforcement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning) for understanding the PPO algorithm. It can still be used for complex environments but may require some hyperparameter-tuning or changes in the code.  To keep the training procedure simple :    - It has a **constant standard deviation** for the output action distribution (**multivariate normal with diagonal covariance matrix**) for the continuous environments  i.e. it is a hyperparameter and NOT a trainable parameter. However  it is **linearly decayed**. (action_std significantly affects performance)   - It uses simple **monte-carlo estimate** for calculating advantages and NOT Generalized Advantage Estimate (check out the OpenAI spinning up implementation for that).   - It is a **single threaded implementation**  i.e. only one worker collects experience. [One of the older forks](https://github.com/rhklite/Parallel-PPO-PyTorch) of this repository has been modified to have Parallel workers  A concise explaination of PPO algorithm can be found [here](https://stackoverflow.com/questions/46422845/what-is-the-way-to-understand-proximal-policy-optimization-algorithm-in-rl)    """;Reinforcement Learning;https://github.com/nikhilbarhate99/PPO-PyTorch
"""**[Update:]** I've further simplified the code to pytorch 1.5  torchvision 0.6  and replace the customized ops roipool and nms with the one from torchvision.  if you want the old version code  please checkout branch [v1.0](https://github.com/chenyuntc/simple-faster-rcnn-pytorch/tree/v1.0)    This project is a **Simplified** Faster R-CNN implementation based on [chainercv](https://github.com/chainer/chainercv) and other [projects](#acknowledgement) . I hope it can serve as an start code for those who want to know the detail of Faster R-CNN.  It aims to:  - Simplify the code (*Simple is better than complex*) - Make the code more straightforward (*Flat is better than nested*) - Match the performance reported in [origin paper](https://arxiv.org/abs/1506.01497) (*Speed Counts and mAP Matters*)  And it has the following features: - It can be run as pure Python code  no more build affair.  - It's a minimal implemention in around 2000 lines valid code with a lot of comment and instruction.(thanks to chainercv's excellent documentation) - It achieves higher mAP than the origin implementation (0.712 VS 0.699) - It achieve speed compariable with other implementation (6fps and 14fps for train and test in TITAN XP) - It's memory-efficient (about 3GB for vgg16)   ![img](imgs/faster-speed.jpg)     """;Computer Vision;https://github.com/chenyuntc/simple-faster-rcnn-pytorch
"""Pixel CNNs are a type of autoregressive generative models which try to model the generation of images as a sequence of generation of pixels. They use multiple convolutional layers to model the generation of next pixel conditioned on the pixels of the image which have already been generated. The layers preserve the spatial resolution of the input image in order to output the image of same size. During training phase  we start from the input image as shown below and perform convolution over it with the kernel of our first layer.   ![Representation of Convolution on the input without masking](images/Unmasked_Conv.png)  In the example above  we try to generate the pixel in the centre using the pixels which have already been generated. As described in the paper we are generating the pixels in the sequence as shown below:  ![Generating image as a sequence of Pixels](images/Sequence.png)  Clearly  pixel `a` should therefore not take into account the `b  f and g` since as per the sequence  during testing time  it won't have access to them. In order to replicate this even during the training stage as well  [A Oord et. al.](https://arxiv.org/abs/1601.06759) propose modification to the convolutional kernel by applying a mask to it. The mask will make that portion  which is not accessible to the model during testing time while generating the central pixel  0 as can be seen below:  ![Representation of Convolution on the input without masking](images/Masked_Conv.png)  Thus sequence by sequence we keep on generating the pixels one by one until the entire image is generated. This can be visualised very neatly with the help of the graphic image below:  ![Visualisation](images/Visualisation.gif)   """;Computer Vision;https://github.com/singh-hrituraj/PixelCNN-Pytorch
"""With the vast amount of landmark images on the Internet  the time has come to think about landmarks globally  namely to build a landmark prediction engine  on the scale of the entire earth.  Develop a technology that can predict landmark labels directly from image pixels  to help people better understand and organize the photo collections.The project challenges to build models that recognize the correct landmark in a dataset of challenging test images. The Kaggle challenge provides access to annotated data which consists of various links to google images along with their respective labeled classes.  Link to the Kaggle competition: [https://www.kaggle.com/c/landmark-recognition-challenge](https://www.kaggle.com/c/landmark-recognition-challenge)   """;General;https://github.com/ankit-vaghela30/Google-landmark-prediction
"""With the vast amount of landmark images on the Internet  the time has come to think about landmarks globally  namely to build a landmark prediction engine  on the scale of the entire earth.  Develop a technology that can predict landmark labels directly from image pixels  to help people better understand and organize the photo collections.The project challenges to build models that recognize the correct landmark in a dataset of challenging test images. The Kaggle challenge provides access to annotated data which consists of various links to google images along with their respective labeled classes.  Link to the Kaggle competition: [https://www.kaggle.com/c/landmark-recognition-challenge](https://www.kaggle.com/c/landmark-recognition-challenge)   """;Computer Vision;https://github.com/ankit-vaghela30/Google-landmark-prediction
"""This method aims at helping computer vision practitioners faced with an overfit problem. The idea is to replace  in a 3-branch ResNet  the standard summation of residual branches by a stochastic affine combination. The largest tested model improves on the best single shot published result on CIFAR-10 by reaching 2.72% test error.  ![shake-shake](https://s3.eu-central-1.amazonaws.com/github-xg/architecture3.png)  Figure 1: **Left:** Forward training pass. **Center:** Backward training pass. **Right:** At test time.   """;General;https://github.com/loshchil/AdamW-and-SGDW
"""AMLA is a common framework to run different AutoML algorithms for neural networks without changing  the underlying systems needed to configure  train and evaluate the generated networks. This has two benefits: * It ensures that different AutoML algorithms can be easily compared using the same set of hyperparameters and infrastructure  allowing for  easy evaluation  comparison and ablation studies of AutoML algorithms. * It provides a easy way to deploy AutoML algorithms on multi-cloud infrastructure.  With a framework  we can manage the lifecycle of autoML easily. Without this  hyperparameters and architecture design are spread out  some embedded in the code  others in config files and other as command line parameters  making it hard to compare two algorithms or perform ablation studies.  Some design principles of AMLA: * The network generation process is decoupled from the training/evaluation process. * The network specification model is independent of the implementation of the training/evaluation/generation code and ML library (i.e. whether it uses TensorFlow/PyTorch etc.).  AMLA currently supports the [NAC using EnvelopeNets](http://arxiv.org/pdf/1803.06744) AutoML algorithm  and we are actively adding newer algorithms to the framework. More information on AutoML algorithms for Neural Networks can be found [here](https://github.com/hibayesian/awesome-automl-papers)   """;General;https://github.com/CiscoAI/amla
"""Optax is a gradient processing and optimization library for JAX.  Optax is designed to facilitate research by providing building blocks that can be easily recombined in custom ways.  Our goals are to:  *   Provide simple  well-tested  efficient implementations of core components. *   Improve research productivity by enabling to easily combine low level     ingredients into custom optimisers (or other gradient processing components). *   Accelerate adoption of new ideas by making it easy for anyone to contribute.  We favour focusing on small composable building blocks that can be effectively combined into custom solutions. Others may build upon these basic components more complicated abstractions. Whenever reasonable  implementations prioritise readability and structuring code to match standard equations  over code reuse.  An initial prototype of this library was made available in JAX's experimental folder as `jax.experimental.optix`. Given the wide adoption across DeepMind of `optix`  and after a few iterations on the API  `optix` was eventually moved out of `experimental` as a standalone open-source library  renamed `optax`.  Documentation on Optax can be found at [optax.readthedocs.io](https://optax.readthedocs.io/).   """;General;https://github.com/deepmind/optax
"""MNC is an instance-aware semantic segmentation system based on deep convolutional networks  which won the first place in COCO segmentation challenge 2015  and test at a fraction of a second per image. We decompose the task of instance-aware semantic segmentation into related sub-tasks  which are solved by multi-task network cascades (MNC) with shared features. The entire MNC network is trained end-to-end with error gradients across cascaded stages.   ![example](data/readme_img/example.png)   MNC was initially described in a [CVPR 2016 oral paper](http://arxiv.org/abs/1512.04412).  This repository contains a python implementation of MNC  which is ~10% slower than the original matlab implementation.  This repository includes a bilinear RoI warping layer  which enables gradient back-propagation with respect to RoI coordinates.   """;Computer Vision;https://github.com/daijifeng001/MNC
"""The hyper-parameters are divided in 4 categories.    This pipeline's purpose is to train a neural network to segment NifTi files from examples.   Since the training requires example  the first step consists in producing manual segmentations of a fraction of the files. 10 to 50% of the files should be a good proportion  however this sample must be representative of the rest of the dataset. Datasets with great variability might require bigger fractions to be manually segmented.   The network is trained through a gradient back-propagation algorithm on the loss. The loss quantifies the difference between the predictions of the network and the manual segementations.   Once trained  the network can be used to automtically segment the entire dataset.  For training and inference  the volumes are sliced along the vertical axis and treated as collections of 2D images. Thus the image processing operations are 2D operations. Data augmentation is used on the training data. It consists in random modifications of the images and their corresponding GT to create more various examples.   <img src=""./media/process.png"" alt=""process schema"" width=""600""/>    """;Computer Vision;https://github.com/neuropoly/multiclass-segmentation
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/zgx0534/pointnet_win
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Satan012/BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Satan012/BERT
"""This repository contains the original modified residual network which modified the classical resnet ""Deep Residual Learning for Image Recognition"" (http://arxiv.org/abs/1512.03385). Original residual block was modified to improve model performance.    """;General;https://github.com/xinkuansong/modified-resnet-acc-0.9638-10.7M-parameters
"""This repository contains the original modified residual network which modified the classical resnet ""Deep Residual Learning for Image Recognition"" (http://arxiv.org/abs/1512.03385). Original residual block was modified to improve model performance.    """;Computer Vision;https://github.com/xinkuansong/modified-resnet-acc-0.9638-10.7M-parameters
""" **BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/thanhlong1997/bert_quora
"""* The original paper of this code is: https://arxiv.org/abs/1603.00748 * The code is mainly based on: https://github.com/carpedm20/NAF-tensorflow/ * Additionally I added the prioritized experience replay: https://arxiv.org/abs/1511.05952 * Using the OpenAI baseline implementation: https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py  Thanks openAI and Kim!    """;Reinforcement Learning;https://github.com/MathPhysSim/PER-NAF
"""[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.   """;Natural Language Processing;https://github.com/mwydmuch/extremeText
"""Adds a short description for the log files.    """;General;https://github.com/felixSchober/ABSA-Transformer
"""Adds a short description for the log files.    """;Natural Language Processing;https://github.com/felixSchober/ABSA-Transformer
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/long95288/car-logo-detect
"""This project consists of a simulation that simulates a partially observable  multi-agent  dynamic  continuous in space  discrete in time and partly unknown (missing knowledge about laws of physics) environment.  There are two actors that can interact consciously with the environment: a cow and wolf.  Additionally  there is another entity called grass.  Each entity has a certain energy level. The cow gets energy by touching grass  the wolf by touching cows. Each entity loses energy by touching its counterpart or moving around. The goal of each actor is to obtain as much energy as possible. If the energy level of the cow or the grass drops below zero the environment is reset. An actor perceives its environment  by sending out rays with a limited reach.  The rays return the color of the actor they intersect with  black if they intersected with the game border or white if they did not intersect with anything. The next figure shows a visualisation of the rays  the cow (brown)  the wolf (blue)  the grass (red) and a visualisation of the rays.  ![figure1](screenshot.png)  The little black circles represent their head. To implement the actors' AI deep Q learning as described in the lecture was used  however it does not achieve wanted results as of yet.    """;Reinforcement Learning;https://github.com/Stippler/cow-simulator
"""> **UPDATE 24/01/2020:** Thank you for your e-mails asking about _batchboost_. As promised  I will update the results soon and present comparisons with other solutions (paperswithcode.com). This is a draft and research needs to be continued to be complete work  if someone is interested in helping me  please contact.   """;Computer Vision;https://github.com/maciejczyzewski/batchboost
"""<img src=""demo/24.png"" width=""81%"">   """;Computer Vision;https://github.com/Yuliang-Liu/bezier_curve_text_spotting
"""I will describe the layout of the dataset. The CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes  with 6000 images per class. There are 50000 training images and 10000 test images.  The dataset is divided into five training batches and one test batch  each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order  but some training batches may contain more images from one class than another. Between them  the training batches contain exactly 5000 images from each class. The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. ""Automobile"" includes sedans  SUVs  things of that sort. ""Truck"" includes only big trucks. Neither includes pickup trucks. The archive contains the files data_batch_1  data_batch_2  ...  data_batch_5  as well as test_batch. For each batch files: 	data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 color image. The first 1024 entries contain the red channel values  the next 1024 the green  and the final 1024 the blue. The image is stored in row-major order  so that the first 32 entries of the array are the red channel values of the first row of the image. 	labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data. The dataset contains another file  called batches.meta. It too contains a Python dictionary object. It has the following entries: 	label_names -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example  label_names[0] == ""airplane""  label_names[1] == ""automobile""  etc.  """;Computer Vision;https://github.com/Xinyi6/CIFAR10-CNN-by-Keras
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   """;Computer Vision;https://github.com/qq330488563/TEST
""" Currently support tensorflow in   - **ResNeSt**  2d&3d  - **RegNet**  - **DETR** (modified classfication)  - **GENet** (2020 GPU-Efficient Network)   model only  no pertrain model for download (simply not enough free time and resource).   easy to read and modified. welcome for using it  ask question  test it  find some bugs maybe.  ResNeSt based on [offical github](https://github.com/zhanghang1989/ResNeSt) .   """;Computer Vision;https://github.com/QiaoranC/tf_ResNeSt_RegNet_model
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/lennonzurich/lalala
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/lennonzurich/lalala
"""I will describe the layout of the dataset. The CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes  with 6000 images per class. There are 50000 training images and 10000 test images.  The dataset is divided into five training batches and one test batch  each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order  but some training batches may contain more images from one class than another. Between them  the training batches contain exactly 5000 images from each class. The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. ""Automobile"" includes sedans  SUVs  things of that sort. ""Truck"" includes only big trucks. Neither includes pickup trucks. The archive contains the files data_batch_1  data_batch_2  ...  data_batch_5  as well as test_batch. For each batch files: 	data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 color image. The first 1024 entries contain the red channel values  the next 1024 the green  and the final 1024 the blue. The image is stored in row-major order  so that the first 32 entries of the array are the red channel values of the first row of the image. 	labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data. The dataset contains another file  called batches.meta. It too contains a Python dictionary object. It has the following entries: 	label_names -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example  label_names[0] == ""airplane""  label_names[1] == ""automobile""  etc.  """;General;https://github.com/Xinyi6/CIFAR10-CNN-by-Keras
"""Tensorflow's sequential model is a very intuitive way to start learning about Deep Neural Networks. However it is quite hard to dive into more complex networks without learning more about Keras.  Well it won't be hard anymore with Fast-layers! Define your Sequences and start building complex layers in a sequential fashion.  I created fast-layers for beginners who wants to build more advanced networks and for experimented users who needs to quickly build and test complex module architectures.   """;Computer Vision;https://github.com/AlexandreMahdhaoui/fast_layers
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/wj1tr0y/SSD
"""Welcome to PaddleSeg! PaddleSeg is an end-to-end image segmentation development kit developed based on [PaddlePaddle](https://www.paddlepaddle.org.cn)  which covers a large number of high-quality segmentation models in different directions such as *high-performance* and *lightweight*. With the help of modular design  we provide two application methods: *Configuration Drive* and *API Calling*. So one can conveniently complete the entire image segmentation application from training to deployment through configuration calls or API calls.  *  """;Computer Vision;https://github.com/PaddlePaddle/PaddleSeg
"""Welcome to PaddleSeg! PaddleSeg is an end-to-end image segmentation development kit developed based on [PaddlePaddle](https://www.paddlepaddle.org.cn)  which covers a large number of high-quality segmentation models in different directions such as *high-performance* and *lightweight*. With the help of modular design  we provide two application methods: *Configuration Drive* and *API Calling*. So one can conveniently complete the entire image segmentation application from training to deployment through configuration calls or API calls.  *  """;General;https://github.com/PaddlePaddle/PaddleSeg
"""The pickled data is a dictionary with 4 key/value pairs:  - `'features'` is a 4D array containing raw pixel data of the traffic sign images  (num examples  width  height  channels). - `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id. - `'sizes'` is a list containing tuples  (width  height) representing the original width and height the image. - `'coords'` is a list containing tuples  (x1  y1  x2  y2) representing coordinates of a bounding box around the sign in the image.  **First  we will use `numpy` provide the number of images in each subset  in addition to the image size  and the number of unique classes.** Number of training examples:  34799 Number of testing examples:  12630 Number of validation examples:  4410 Image data shape = (32  32  3) Number of classes = 43  **Then  we used `matplotlib` plot sample images from each subset.**  **And finally  we will use `numpy` to plot a histogram of the count of images in each unique class.** ---   """;General;https://github.com/rahulsonone1234/Traffic-Sign-Recognition
"""If you want to go somewhere regarding implementation  please skip this part.    YOLOv3 is a light-weight but powerful one-stage object detector  which means it regresses the positions of objects and predict the probability of objects directly from the feature maps of CNN. Typical example of one-state detector will be YOLO and SSD series.On the contrary   two stage detector like R-CNN  Fast R-CNN and Faster R-CNN may include Selective Search  Support Vector Machine (SVM) and Region Proposal Network (RPN) besides CNN. Two-stage detectors will be sightly more accurate but much slower.   YOLOv3 consists of 2 parts: feature extractor and detector. Feature extractor is a Darknet-53 without its fully connected layer  which is originally designed for classification task on ImageNet dataset.    ![darknet](/fig/Darknet.png)   *Darknet-53 architecture(Source: YOLOv3: An Incremental Improvement https://arxiv.org/abs/1804.02767)*  Detector uses multi-scale fused features to predict the position and the class of the corresponding object.   ![yolov3](/fig/yolo.png)*(Source: https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)*  As you can see from the picture above  there are 3 prediction scales in total. For example  if the spatial resolution of an input image is 32N X 32N  the output of the first prediction convolution layer(strides32) will be N X N X (B X (C+5)). B indicates amount of anchors at this scale and C stands for probabilities of different classes. 5 represents 5 different regressions  the  horizontal offset t_x  the vertical offset t_y  resizing factor of the given anchor height t_hand width t_wand objectness score o (whether an object exists in this square of the checkerboard). The second prediction layer will output feature maps of 2N X 2N X (B X (C+5)). And the third prediction output will be much finer  which is 4N X 4N X (B X (C+5).  Reading papers of YOLO  YOLOv2 and YOLOv3  I summarize the loss function of YOLOv3 as follows:   ![](/fig/loss1.PNG) <!-- $$ L_{Localization} = \lambda_1\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}[(t_{x} - t_{\hat{x}})^2 + (t_{y} - t_{\hat{y}})^2] \\L_{Shaping} =\lambda_2\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}[(t_{w} - t_{\hat{w}})^2 + (t_{h} - t_{\hat{h}})^2]\\ L_{objectness-obj} =\lambda_3\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}\log(o_{ij})$$ $$L_{objectness-noobj} =\lambda_4\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}\log(1-o_{ij}) \\L_{class} =\lambda_5\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}\sum_{c\in classes}[p_{\hat{ij}}(c)\log(p_{ij}(c))+ (1-p_{\hat{ij}}(c))\log(1-p_{ij}(c))]) \\ L_{Scale_{1}} = L_{Localization} + L_{Shaping} + L_{objectness-obj} + L_{objectness-noobj} + L_{class} \\ L_{total} = L_{Scale_{1}}+L_{Scale_{2}}+L_{Scale_{3}}$$ -->   This is my implementation of YOLOv3 using TensorFlow 2.0 backend. The main purpose of this project is to get me familiar with deep learning and specific concepts in domain object detection. Two usages are provided: * Object detection based on official pre-trained weights in COCO * Object detection of optic nerve on Indian Diabetic Retinopathy Image Dataset (IDRiD) using fine tuning.  ![nerve](/fig/optics_nerve.png) *Fundus and the corresponding optic nerve*  The following content will be provided in this repo: * Introduction of YOLOv3 * Object detection based on the official pre-trained weights * Object detection - fine tuning on IDRiD       """;Computer Vision;https://github.com/LEGO999/YOLOV3-TF2
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/adairliulei/yolov3
"""CenterNet is a framework for object detection with deep convolutional neural networks. You can use the code to train and evaluate a network for object detection on the MS-COCO dataset.  * It achieves state-of-the-art performance (an AP of 47.0%) on one of the most challenging dataset: MS-COCO.  * Our code is written in Python  based on [CornerNet](https://github.com/princeton-vl/CornerNet).  *More detailed descriptions of our approach and code will be made available soon.*  **If you encounter any problems in using our code  please contact Kaiwen Duan: kaiwen.duan@vipl.ict.ac.cn.**   """;Computer Vision;https://github.com/tekeburak/CenterNet
"""The authors of the paper present gMLP  an an attention-free all-MLP architecture based on spatial gating units. gMLP achieves parity with transformer models such as ViT and BERT on language and vision downstream tasks. The authors also show that gMLP scales with increased data and number of parameters  suggesting that self-attention is not a necessary component for designing performant models.   """;Computer Vision;https://github.com/jaketae/g-mlp
"""The authors of the paper present gMLP  an an attention-free all-MLP architecture based on spatial gating units. gMLP achieves parity with transformer models such as ViT and BERT on language and vision downstream tasks. The authors also show that gMLP scales with increased data and number of parameters  suggesting that self-attention is not a necessary component for designing performant models.   """;General;https://github.com/jaketae/g-mlp
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/zzfpython/yolov3-change
"""You can find an example of using this caption generator at [Inference.ipynb](https://github.com/alex-f1tor/Image-Caption/blob/master/Inference.ipynb) notebook.  Few examples of generated captions for images:  ![Image](https://github.com/alex-f1tor/Image-Caption/blob/master/imgs/bird_sample.png)  ![Image](https://github.com/alex-f1tor/Image-Caption/blob/master/imgs/pizza_sample.png)   You can also:   - Train your own caption network with MS-COCO dataset based on pipeline at [Training.ipynb](https://github.com/alex-f1tor/Image-Caption/blob/master/Training.ipynb)   - Estimate model performance at [cocoEvalCap.ipynb](https://github.com/alex-f1tor/Image-Caption/blob/master/cocoEvalCap.ipynb) via different [metrics](https://github.com/tylin/coco-caption)  like CIDEr  Rouge-L and etc.      - This project based on original arxiv paper [Show and Tell: A Neural Image Caption Generator (2015)](https://arxiv.org/abs/1411.4555) paper;   - The encoder is pretrained [resnet50](https://arxiv.org/abs/1512.03385) deep CNN [available](https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html#resnet50) in pyTorch;    - The caption generator was trained on [MS-COCO 2014 dataset](http://cocodataset.org/#download).    """;General;https://github.com/alex-f1tor/Image-Caption
"""You can find an example of using this caption generator at [Inference.ipynb](https://github.com/alex-f1tor/Image-Caption/blob/master/Inference.ipynb) notebook.  Few examples of generated captions for images:  ![Image](https://github.com/alex-f1tor/Image-Caption/blob/master/imgs/bird_sample.png)  ![Image](https://github.com/alex-f1tor/Image-Caption/blob/master/imgs/pizza_sample.png)   You can also:   - Train your own caption network with MS-COCO dataset based on pipeline at [Training.ipynb](https://github.com/alex-f1tor/Image-Caption/blob/master/Training.ipynb)   - Estimate model performance at [cocoEvalCap.ipynb](https://github.com/alex-f1tor/Image-Caption/blob/master/cocoEvalCap.ipynb) via different [metrics](https://github.com/tylin/coco-caption)  like CIDEr  Rouge-L and etc.      - This project based on original arxiv paper [Show and Tell: A Neural Image Caption Generator (2015)](https://arxiv.org/abs/1411.4555) paper;   - The encoder is pretrained [resnet50](https://arxiv.org/abs/1512.03385) deep CNN [available](https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html#resnet50) in pyTorch;    - The caption generator was trained on [MS-COCO 2014 dataset](http://cocodataset.org/#download).    """;Computer Vision;https://github.com/alex-f1tor/Image-Caption
"""For this project  we train an agent to navigate (and collect bananas!) in a large  square world.    ![Trained Agent][image1]  A reward of +1 is provided for collecting a yellow banana  and a reward of -1 is provided for collecting a blue banana.  Thus  the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.    The state space has 37 dimensions and contains the agent's velocity  along with ray-based perception of objects around agent's forward direction.  Given this information  the agent has to learn how to best select actions.  Four discrete actions are available  corresponding to: - **`0`** - move forward. - **`1`** - move backward. - **`2`** - turn left. - **`3`** - turn right.  The task is episodic  and in order to solve the environment  the agent must get an average score of +13 over 100 consecutive episodes.   """;Reinforcement Learning;https://github.com/shehrum/RL_Navigation
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/mengfu188/insightface.bak
"""The project is an implementation of Wasserstein GAN in Tensorflow 2.0.  Paper link: https://arxiv.org/abs/1701.07875   """;Computer Vision;https://github.com/WangZesen/WGAN-GP-Tensorflow-v2
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/cfl0122/face
"""**GCNet** is initially described in [arxiv](https://arxiv.org/abs/1904.11492). Via absorbing advantages of Non-Local Networks (NLNet) and Squeeze-Excitation Networks (SENet)   GCNet provides a simple  fast and effective approach for global context modeling  which generally outperforms both NLNet and SENet on major benchmarks for various recognition tasks.   """;Computer Vision;https://github.com/zhusiling/GCNet
"""**GCNet** is initially described in [arxiv](https://arxiv.org/abs/1904.11492). Via absorbing advantages of Non-Local Networks (NLNet) and Squeeze-Excitation Networks (SENet)   GCNet provides a simple  fast and effective approach for global context modeling  which generally outperforms both NLNet and SENet on major benchmarks for various recognition tasks.   """;General;https://github.com/zhusiling/GCNet
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/yushuz/me599_yolo
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/kfx7577/Caffe_VehicleCounter
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/zhang-huihui/git-repository
"""Welcome to the GitHub repo for the introduction course in Introduction to Machine Learning at Uppsala University. This repo contains all necessary material and information for the course.   """;Natural Language Processing;https://github.com/MansMeg/IntroML
"""This repository implements mulitple popular object detection algorithms  including Faster R-CNN  R-FCN  FPN  and our recently proposed Cascade R-CNN  on the MS-COCO and PASCAL VOC datasets. Multiple choices are available for backbone network  including AlexNet  VGG-Net and ResNet. It is written in C++ and powered by [Caffe](https://github.com/BVLC/caffe) deep learning toolbox.   [Cascade R-CNN](http://www.svcl.ucsd.edu/publications/conference/2018/cvpr/cascade-rcnn.pdf) is a multi-stage extension of the popular two-stage R-CNN object detection framework. The goal is to obtain high quality object detection  which can effectively reject close false positives. It consists of a sequence of detectors trained end-to-end with increasing IoU thresholds  to be sequentially more selective against close false positives. The output of a previous stage detector is forwarded to a later stage detector  and the detection results will be improved stage by stage. This idea can be applied to any detector based on the two-stage R-CNN framework  including Faster R-CNN  R-FCN  FPN  Mask R-CNN  etc  and reliable gains are available independently of baseline strength. A vanilla Cascade R-CNN on FPN detector of ResNet-101 backbone network  without any training or inference bells and whistles  achieved state-of-the-art results on the challenging MS-COCO dataset.   """;Computer Vision;https://github.com/zhaoweicai/cascade-rcnn
"""You have an audio recording  and you want to know where certain classes of sounds are.  SongExplorer is trained to recognize such words by manually giving it a few examples.  It will then automatically calculate the probability  over time  of when those words occur in all of your recordings.  Applications suitable for SongExplorer include quantifying the rate or pattern of words emitted by a particular species  distinguishing a recording of one species from another  and discerning whether individuals of the same species produce different song.  Underneath the hood is a deep convolutional neural network.  The input is the raw audio stream  and the output is a set of mutually-exclusive probability waveforms corresponding to each word of interest.  Training begins by first thresholding one of your recordings in the time- and frequency-domains to find sounds that exceed the ambient noise. These sounds are then clustered into similar categories for you to manually annotate with however many word labels naturally occur.  A classifier is then trained on this corpus of ground truth  and a new recording is analyzed by it.  The words it automatically finds are then clustered as before  but this time are displayed with predicted labels.  You manually correct the mistakes  both re-labeling words that it got wrong  as well as labeling words it missed.  These new annotations are added to the ground truth  and the process of retraining the classifier and analyzing and correcting new recordings is repeated until the desired accuracy is reached.    """;General;https://github.com/JaneliaSciComp/SongExplorer
"""You have an audio recording  and you want to know where certain classes of sounds are.  SongExplorer is trained to recognize such words by manually giving it a few examples.  It will then automatically calculate the probability  over time  of when those words occur in all of your recordings.  Applications suitable for SongExplorer include quantifying the rate or pattern of words emitted by a particular species  distinguishing a recording of one species from another  and discerning whether individuals of the same species produce different song.  Underneath the hood is a deep convolutional neural network.  The input is the raw audio stream  and the output is a set of mutually-exclusive probability waveforms corresponding to each word of interest.  Training begins by first thresholding one of your recordings in the time- and frequency-domains to find sounds that exceed the ambient noise. These sounds are then clustered into similar categories for you to manually annotate with however many word labels naturally occur.  A classifier is then trained on this corpus of ground truth  and a new recording is analyzed by it.  The words it automatically finds are then clustered as before  but this time are displayed with predicted labels.  You manually correct the mistakes  both re-labeling words that it got wrong  as well as labeling words it missed.  These new annotations are added to the ground truth  and the process of retraining the classifier and analyzing and correcting new recordings is repeated until the desired accuracy is reached.    """;Computer Vision;https://github.com/JaneliaSciComp/SongExplorer
"""---  >**Question answering**  (**QA**) is a computer science discipline within the fields of  [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval ""Information retrieval"")  and  [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing ""Natural language processing"")  (NLP)  which is concerned with building systems that automatically answer questions posed by humans in a  [natural language](https://en.wikipedia.org/wiki/Natural_language ""Natural language""). > >Source: [Wikipedia](https://en.wikipedia.org/wiki/Question_answering)  >**Extractive QA** is a popular task for natural language processing (NLP) research  where models must extract a short snippet from a document in order to answer a natural language question. > >  Source: [Facebook AI](https://ai.facebook.com/blog/research-in-brief-unsupervised-question-answering-by-cloze-translation/)    **Context-Based Question Answering (CBQA)** is an inference web-based Extractive QA search engine  mainly dependent on [Haystack](https://github.com/deepset-ai/haystack) and [Transformers](https://github.com/huggingface/transformers) library.  The CBQA application allows the user to add context and perform Question Answering(QA) in that context.  The main components in this application use [Haystack's](https://github.com/deepset-ai/haystack) core components   > - **FileConverter**: Extracts pure text from files (pdf  docx  pptx  html and many more). > -  **PreProcessor**: Cleans and splits texts into smaller chunks. > -   **DocumentStore**: Database storing the documents  metadata and vectors for our search. We recommend Elasticsearch or FAISS  but have also more light-weight options for fast prototyping (SQL or In-Memory). >   - **Retriever**: Fast algorithms that identify candidate documents for a given query from a large collection of documents. Retrievers narrow down the search space significantly and are therefore key for scalable QA. Haystack supports sparse methods (TF-IDF  BM25  custom Elasticsearch queries) and state of the art dense methods (e.g. sentence-transformers and Dense Passage Retrieval) > - **Reader**: Neural network (e.g. BERT or RoBERTA) that reads through texts in detail to find an answer. The Reader takes multiple passages of text as input and returns top-n answers. Models are trained via  [FARM](https://github.com/deepset-ai/FARM)  or [Transformers](https://github.com/huggingface/transformers)  on SQuAD like tasks. You can just load a pretrained model from  [Hugging Face's model hub](https://huggingface.co/models)  or fine-tune it on your own domain data. > >Source: [Haystack's Key Components docs](https://github.com/deepset-ai/haystack/#key-components)  In CBQA the allowed formats for adding the context are    - Textual Context (Using the TextBox field)  - File Uploads (.pdf  .txt  and .docx)  These contexts are uploaded to a temporary directory for each user for pre-processing and deletes after uploading them to **Elasticsearch ** which is the only **DocumentStore type** used in this system.  Each user has a separate Elasticsearch index to store the context documents. Using the **PreProcessor** and the **FileConverter** modules from [Haystack](https://github.com/deepset-ai/haystack)  the pre-processing and the extraction of text from context files is done.  **Elasticsearcher Retriever** is used in CBQA to retrieve relevant documents based on the search query.  **Transformers-based Readers** are used to extracting answers from the retrieved documents. The Readers used in CBQA are the pre-trained Transformers models hosted on the [Hugging Face's model hub](https://huggingface.co/models) .  Currently  CBQA provides the interface to perform QA in **English** and **French**  using four Transformers based models    - BERT  - RoBERTa  - DistilBERT  - CamemBERT  The interface provides an option to choose the inference device between CPU and GPU.  The output is in a tabular form containing the following headers   -   **Answers** (Extracted answers based on the question and context) -   **Context** (Specifies the context window  related to the answer) -   **Document Title** (Specifies the title of context file  related to the answer)   """;Natural Language Processing;https://github.com/Karthik-Bhaskar/Context-Based-Question-Answering
"""Thanks to Kaggle and a lot of amazing data enthusiasm people sharing their notebooks so I had a chance to learn Transformer and really use it to a real-world task!         Saint+ is a **Transformer** based knowledge-tracing model which takes students' exercise history information to predict future performance. As classical Transformer  it has an Encoder-Decoder structure that Encoder applied self-attention to a stream of exercise embeddings; Decoder applied self-attention to responses embeddings and encoder-decoder attention to encoder output.  """;General;https://github.com/Chang-Chia-Chi/SaintPlus-Knowledge-Tracing-Pytorch
"""Thanks to Kaggle and a lot of amazing data enthusiasm people sharing their notebooks so I had a chance to learn Transformer and really use it to a real-world task!         Saint+ is a **Transformer** based knowledge-tracing model which takes students' exercise history information to predict future performance. As classical Transformer  it has an Encoder-Decoder structure that Encoder applied self-attention to a stream of exercise embeddings; Decoder applied self-attention to responses embeddings and encoder-decoder attention to encoder output.  """;Natural Language Processing;https://github.com/Chang-Chia-Chi/SaintPlus-Knowledge-Tracing-Pytorch
"""``` yolo.ai ├── cfg                 #: The directory where model config file is located (darknet  efficientnet  etc) ├── tests               #: Implmentation test cases ├── yolo                #: YOLO implementation code bases │   ├── data            #: Base data directory │   │   ├── datasets    #: Contain datasets such as pascal-voc │   │   └── transforms  #: Custom transforms for Dataset │   ├── models          #: Base model directory │   │   ├── arch        #: YOLO model assembly place │   │   ├── backbones   #: All backbone network gathering here │   │   ├── detectors   #: Assembly of all types of detectors │   │   ├── losses      #: The gathering place of all loss functions │   │   ├── metrics     #: Metrics functions for bounding boxes and losses │   │   └── modules     #: Individuals modules for network building │   └── utils           #: Utilites file for visualization and network ```  """;Computer Vision;https://github.com/DavianYang/yolo.ai
"""Follow experimental summary [here](https://bit.ly/3arUw9q).   """;General;https://github.com/sayakpaul/EvoNorms-in-TensorFlow-2
"""Spell corrector.ipynb   We tackle the problem of OCR post processing. In OCR  we map the image form of the document into the text domain. This is done first using an CNN+LSTM+CTC model  in our case based on tesseract. Since this output maps only image to text  we need something on top to validate and correct language semantics.  The idea is to build a language model  that takes the OCRed text and corrects it based on language knowledge. The langauge model could be: - Char level: the aim is to capture the word morphology. In which case it's like a spelling correction system. - Word level: the aim is to capture the sentence semnatics. But such systems suffer from the OOV problem. - Fusion: to capture semantics and morphology language rules. The output has to be at char level  to avoid the OOV. However  the input can be char  word or both.   """;Natural Language Processing;https://github.com/ahmadelsallab/spell_corrector
""" **BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/thanhlong1997/bert_quora
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/meizi1114/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/meizi1114/bert
"""High localization accuracy is crucial in many real-world applications. We propose a novel single stage end-to-end object detection network (RRC) to produce high accuracy detection results. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our paper (https://arxiv.org/abs/1704.05776).  | method | KITTI test *mAP* car (moderate)| | :-------: | :-----: | | [Mono3D](http://3dimage.ee.tsinghua.edu.cn/cxz/mono3d)| 88.66% | | [SDP+RPN](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Exploit_All_the_CVPR_2016_paper.pdf)| 88.85% | | [MS-CNN](https://github.com/zhaoweicai/mscnn) | 89.02% | | [Sub-CNN](https://arxiv.org/pdf/1604.04693.pdf) | 89.04% | | RRC (single model) | **89.85%** |    [KITTI ranking](http://www.jimmyren.com/papers/rrc_kitti.pdf)   """;Computer Vision;https://github.com/xiaohaoChen/rrc_detection
"""This repository is for the CVPR 2018 Spotlight paper  '[Path Aggregation Network for Instance Segmentation](https://arxiv.org/abs/1803.01534)'  which ranked 1st place of [COCO Instance Segmentation Challenge 2017](http://cocodataset.org/#detections-leaderboard)   2nd place of [COCO Detection Challenge 2017](http://cocodataset.org/#detections-leaderboard) (Team Name: [UCenter](https://places-coco2017.github.io/#winners)) and 1st place of 2018 [Scene Understanding Challenge for Autonomous Navigation in Unstructured Environments](http://cvit.iiit.ac.in/scene-understanding-challenge-2018/benchmarks.php#instance) (Team Name: TUTU).   """;Computer Vision;https://github.com/ShuLiu1993/PANet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/zhang-huihui/git-repository
"""The goal of Detectron is to provide a high-quality  high-performance codebase for object detection *research*. It is designed to be flexible in order to support rapid implementation and evaluation of novel research. Detectron includes implementations of the following object detection algorithms:  - [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017* - [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017* - [Faster R-CNN](https://arxiv.org/abs/1506.01497) - [RPN](https://arxiv.org/abs/1506.01497) - [Fast R-CNN](https://arxiv.org/abs/1504.08083) - [R-FCN](https://arxiv.org/abs/1605.06409)  using the following backbone network architectures:  - [ResNeXt{50 101 152}](https://arxiv.org/abs/1611.05431) - [ResNet{50 101 152}](https://arxiv.org/abs/1512.03385) - [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt) - [VGG16](https://arxiv.org/abs/1409.1556)  Additional backbone architectures may be easily implemented. For more details about these models  please see [References](#references) below.   """;Computer Vision;https://github.com/jiajunhua/facebookresearch-Detectron
"""The goal of Detectron is to provide a high-quality  high-performance codebase for object detection *research*. It is designed to be flexible in order to support rapid implementation and evaluation of novel research. Detectron includes implementations of the following object detection algorithms:  - [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017* - [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017* - [Faster R-CNN](https://arxiv.org/abs/1506.01497) - [RPN](https://arxiv.org/abs/1506.01497) - [Fast R-CNN](https://arxiv.org/abs/1504.08083) - [R-FCN](https://arxiv.org/abs/1605.06409)  using the following backbone network architectures:  - [ResNeXt{50 101 152}](https://arxiv.org/abs/1611.05431) - [ResNet{50 101 152}](https://arxiv.org/abs/1512.03385) - [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt) - [VGG16](https://arxiv.org/abs/1409.1556)  Additional backbone architectures may be easily implemented. For more details about these models  please see [References](#references) below.   """;General;https://github.com/jiajunhua/facebookresearch-Detectron
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/Soldie/insightface-Rec.Face
"""  ![Summary](/images/summary.PNG)    """;Natural Language Processing;https://github.com/Raman-Raje/Machine-Reading-Comprehension-Neural-Question-Answer-
"""This repository is for '[Pyramid Scene Parsing Network](https://arxiv.org/abs/1612.01105)'  which ranked 1st place in [ImageNet Scene Parsing Challenge 2016](http://image-net.org/challenges/LSVRC/2016/results). The code is modified from Caffe version of  [DeepLab v2](https://bitbucket.org/aquariusjay/deeplab-public-ver2) and [yjxiong](https://github.com/yjxiong/caffe/tree/mem) for evaluation. We merge the batch normalization layer named 'bn_layer' in the former one into the later one while keep the original 'batch_norm_layer' in the later one unchanged for compatibility. The difference is that 'bn_layer' contains four parameters as 'slope bias mean variance' while 'batch_norm_layer' contains two parameters as 'mean variance'. Several evaluation code is borrowed from [MIT Scene Parsing](https://github.com/CSAILVision/sceneparsing).   """;Computer Vision;https://github.com/hszhao/PSPNet
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   """;Computer Vision;https://github.com/beassssry/U
"""Two experiments are included in this repository  where benchmarks are from the paper [Generalized Sliced Wasserstein Distances](http://papers.nips.cc/paper/8319-generalized-sliced-wasserstein-distances) and the paper [Distributional Sliced-Wasserstein and Applications to Generative Modeling](https://arxiv.org/pdf/2002.07367.pdf)  respectively. The first one is on the task of sliced Wasserstein flow  and the second one is on generative modellings with GANs. For more details and setups  please refer to the original paper **[Augmented Sliced Wasserstein Distances](https://arxiv.org/abs/2006.08812)**.  """;General;https://github.com/xiongjiechen/ASWD
"""The demos in this folder are designed to give straightforward samples of using TensorFlow in mobile applications.  Inference is done using the [TensorFlow Android Inference Interface](../../../tensorflow/contrib/android)  which may be built separately if you want a standalone library to drop into your existing application. Object tracking and YUV -> RGB conversion is handled by libtensorflow_demo.so.  A device running Android 5.0 (API 21) or higher is required to run the demo due to the use of the camera2 API  although the native libraries themselves can run on API >= 14 devices.   """;Computer Vision;https://github.com/GenesisDCarmen/C_Reconocimiento_Facial
"""The demos in this folder are designed to give straightforward samples of using TensorFlow in mobile applications.  Inference is done using the [TensorFlow Android Inference Interface](../../../tensorflow/contrib/android)  which may be built separately if you want a standalone library to drop into your existing application. Object tracking and YUV -> RGB conversion is handled by libtensorflow_demo.so.  A device running Android 5.0 (API 21) or higher is required to run the demo due to the use of the camera2 API  although the native libraries themselves can run on API >= 14 devices.   """;General;https://github.com/GenesisDCarmen/C_Reconocimiento_Facial
"""In this research project  to solve real world problems with machine learning  I noted that there is a limit to the traditional Deep Learning application  which is highly dependent on existing datasets because it is still difficult to obtain enough labled data.  The basis for judgment must be clear in the biomedical field  so I decided to use image data among various types for the reason of being visualized intuitively.  Using just one labeled image data for training  I wanted to categorize a lot of unseen data based on it by the basic concept of one shot learning through reinforcement learning.  In this project  I redefined the one shot image segmentation problem as a reinforcement learning and solved it using PPO. I found that there was actually a dramatic performance.  <p align=""center""> <img src=""oneshotgo/data/res/un.png"" width=70%/> </p>   """;Reinforcement Learning;https://github.com/decoderkurt/research_project_school_of_ai_2019
"""There are numerous other things that I have learned and failed through so far. I went into this semester with barely any knowledge of anything in this realm  and I really feel like I have learned a lot up to now  and am excited to continue to learn more and more.  The first few weeks of my semester were spent trying to learn and understand the mathematics and qualities that modern day style transfer is based off of. I read a few of the dominant papers in the field that discussed the methods.  To summarize  when creating a stylized image  there are three main players. The content photo (c)  the style photo (s)  and the new pistache (p).  At a high level  we want to make (p) as similar in content to (c)  and as similar in style to (s). So how can we define  what ‘content’ and ‘style’ are? [4] found that Convolutional Neural Networks (CNN’s)  when trained for object recognition  extract different types of information at different layers. If you then try to minimize the feature reconstruction for particular layers  you can extract different information. Importantly  minimizing early layers in the CNN seems to capture the texture and color images  whereas  minimizing higher layers in the CNN seem to capture image content and overall spatial structure [5].  There is a lot of math that is involved in reconstructing those layers  and minimizing the differences between images  that I won’t get into here. After the initial paper was published  an important addition was made that enabled the creation of the Android application that I ended up editing. [5] demonstrated a way of not only creating stylized images  but also creating them in real time. To create the pistache in [4]  there was both a forward and backward pass through a pretrained network. To fix the problem of speed  [5] trained a different neural network to quickly approximate solutions to their problem.  Finally  [6] resolved an issue that allowed you to use the same network for N distinct styles instead of 1  thus saving a ton of space. This allowed the Google Codelab to contain so many distinct styles  as well as allow some of the more well known style transfer apps  like Prisma to work.    A pistache is an artistic work that imitates the style of another one.  This git repo is the culumination of half a semester of work for my independent study. To date  I have edited android app  by implemented image-segmented background blurring  and increased luminance matching to a Google Codelab [1]  that focuses on creating pistaches.  After getting to this point  and remembering that this independent study was supposed to be about machine learning and art  and I hadn’t made any art projects  I began to think about what I wanted to make with this unique application.  I quickly realized that my moms birthday was coming up  and that I have historically never gotten her anything. For those who don’t know my mom  I personally believe that she has a manic addiction to her children. That belief is entirely centered around the life she wishes to portray through her Facebook.  Facebook photos tell your story  either intentionally or unintentionally  to everyone that kinda knows you.  Either way  in deciding how to celebrate the birth of my mom  I came to the conclusion that I wanted to tell her story. The one she tells through Facebook that is. It isn’t necessarily the story I would have chosen to portray  but it’s the one she has.  So  I had the pleasure of scouring Janice’s photos to build my collage  and I grew increasingly fearful for my privacy in the process. Regardless  I grabbed about 60 photos from her Facebook (and some others I wanted to throw in)  and created 26 different pistaches with styles ranging from Picasso to Van Gogh  resulting in over 1500 total photos.  Then  I went through the 26 pistaches for each of the 60 images  and chose the one that I thought most truly represented that memory to me. The photo that I think best captures that frozen moment. Thus  trying to portray how I view the world that she is portraying. Seeing her world through my eyes.  Also  for fun  I took all 1500 photos and put it in a super-collage  I call the meg. That photo is 170MB. That image is fucking awesome! But I had to scale down the image so I can have it on github. So the resolution is wack. But I have the real version and its literally the best thing ever.  <p align=""center"">  <img src=""scaled_down_mega_collage.jpg?""> </p>   """;General;https://github.com/chrismgeorge/Artistic_Additions_To_Style_Transfer
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/zengzhaoyang/trident
"""InfoGAN is an information-theoretic extension to the simple Generative Adversarial Networks that is able to learn disentangled representations in a completely unsupervised manner. What this means is that InfoGAN successfully disentangle wrirting styles from digit shapes on th MNIST dataset and discover visual concepts such as hair styles and gender on the CelebA dataset. To achieve this an information-theoretic regularization is added to the loss function that enforces the maximization of mutual information between latent codes  c  and the generator distribution G(z  c).   """;Computer Vision;https://github.com/Natsu6767/InfoGAN-PyTorch
"""This is a GAN demo for creating anime character faces from random noise.   """;Computer Vision;https://github.com/RikoLi/gan-acgface
"""Siamese Mask R-CNN is designed as a minimal variation of Mask R-CNN which can perform the visual search task described above. For more details please read the [paper](https://arxiv.org/abs/1811.11507).   <p align=""center"">  <img src=""figures/siamese-mask-rcnn-sketch.png"" width=50%> </p>   """;Computer Vision;https://github.com/bethgelab/siamese-mask-rcnn
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/guzhang480/Google_BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/guzhang480/Google_BERT
"""Despite the plethora of different models for deep learning on graphs  few approaches have been proposed thus far for dealing with graphs that present some sort of dynamic nature (e.g. evolving features or connectivity over time).   In this paper  we present Temporal Graph Networks (TGNs)  a generic  efficient framework for deep learning on dynamic graphs represented as sequences of timed events. Thanks to a novel combination of memory modules and graph-based operators  TGNs are able to significantly outperform previous approaches being at the same time more computationally efficient.   We furthermore show that several previous models for learning on dynamic graphs can be cast as specific instances of our framework. We perform a detailed ablation study of different components of our framework and devise the best configuration that achieves state-of-the-art performance on several transductive and inductive prediction tasks for dynamic graphs.    """;Graphs;https://github.com/twitter-research/tgn
"""- We have 3 different networks: a) Discriminator  b) Encoder  and c) Generator - A cGAN-VAE (Conditional Generative Adversarial Network- Variational Autoencoder) is used to encode the ground truth output image B to latent vector z which is then used to reconstruct the output image B' i.e.  B -> z -> B' - For inverse mapping (z->B'->z')  we use LR-GAN (Latent Regressor Generative Adversarial Networks) in which a Generator is used to generate B' from input image A and z. - Combining both these models  we get BicycleGAN. - The architecture of Generator is same as U-net in which there are encoder and decoder nets with symmetric skip connections. - For Encoder  we use several residual blocks for an efficient encoding of the input image. - The model is trained using Adam optimizer using BatchNormalization with batch size 1. - LReLU activation function is used for all types of networks.   """;Computer Vision;https://github.com/prakashpandey9/BicycleGAN
"""- We have 3 different networks: a) Discriminator  b) Encoder  and c) Generator - A cGAN-VAE (Conditional Generative Adversarial Network- Variational Autoencoder) is used to encode the ground truth output image B to latent vector z which is then used to reconstruct the output image B' i.e.  B -> z -> B' - For inverse mapping (z->B'->z')  we use LR-GAN (Latent Regressor Generative Adversarial Networks) in which a Generator is used to generate B' from input image A and z. - Combining both these models  we get BicycleGAN. - The architecture of Generator is same as U-net in which there are encoder and decoder nets with symmetric skip connections. - For Encoder  we use several residual blocks for an efficient encoding of the input image. - The model is trained using Adam optimizer using BatchNormalization with batch size 1. - LReLU activation function is used for all types of networks.   """;General;https://github.com/prakashpandey9/BicycleGAN
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/lobznet/yolov3
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/zapplea/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/zapplea/bert
"""One example is shown below:  ```python {     ""content"": ""世锦赛的整体水平远高于亚洲杯，要如同亚洲杯那样“鱼与熊掌兼得”，就需要各方面密切配合、#:idiom#:。作为主帅的俞觉敏，除了得打破保守思想，敢于破格用人，还得巧于用兵、#:idiom#:、灵活排阵，指挥得当，力争通过比赛推新人、出佳绩、出新的战斗力。""       ""realCount"": 2      ""groundTruth"": [""通力合作""  ""有的放矢""]       ""candidates"": [         [""凭空捏造""  ""高头大马""  ""通力合作""  ""同舟共济""  ""和衷共济""  ""蓬头垢面""  ""紧锣密鼓""]           [""叫苦连天""  ""量体裁衣""  ""金榜题名""  ""百战不殆""  ""知彼知己""  ""有的放矢""  ""风流才子""]     ] } ```  - `content`: The given passage where the original idioms are replaced by placeholders `#idiom#` - `realCount`: The number of placeholders or blanks - `groundTruth`: The golden answers in the order of blanks - `candidates`: The given candidates in the order of blanks   """;Natural Language Processing;https://github.com/chujiezheng/ChID-Dataset
"""In this work  3D image features are learned in a self-supervised way by training pointNet network to recognize the 3d rotation that been applied to an input image. Those learned features are being used for classification task learned on top of this base network.  In this repository I uploaded code and data.   """;Computer Vision;https://github.com/aviros/pointnet_totations
"""This is an implementation of global context module in GCNet.(GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond)<https://arxiv.org/abs/1904.11492> I just modified it from the official code.I hope this can help some tensorflow users. **Please let me know if there is something wrong with my code.** Official code repository<https://github.com/xvjiarui/GCNet>  """;Computer Vision;https://github.com/xggIoU/GCNet_global_context_module_tensorflow
"""This is an implementation of global context module in GCNet.(GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond)<https://arxiv.org/abs/1904.11492> I just modified it from the official code.I hope this can help some tensorflow users. **Please let me know if there is something wrong with my code.** Official code repository<https://github.com/xvjiarui/GCNet>  """;General;https://github.com/xggIoU/GCNet_global_context_module_tensorflow
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/944284742/1.FaceRecognition
"""Image recognition and deep learning technologies using Convolutional Neural Networks (CNN) have demonstrated remarkable progress in the medical image analysis field. Traditionally radiologists with extensive clinical expertise visually asses medical images to detect and classify diseases. The task of lesion detection is particularly challenging because non-lesions and true lesions can appear similar.   For my capstone project I use a Mask R-CNN <sup>[1](https://arxiv.org/abs/1703.06870)</sup> with a ResNet-50 Feature Pyramid Network backbone to detect lesions in a CT scan. The model outputs a bounding box  instance segmentation mask and confidence score for each detected lesion. Mask R-CNN was built by the Facebook AI research team (FAIR) in April 2017.  The algorithms are implemented using PyToch and run on an Nvidia Quadro P4000 GPU.   """;Computer Vision;https://github.com/fsafe/Capstone
"""Given a mixed source signal  the task of source separation algorithm is to divide the signal into its original components. We test our method on music separation and specifically on the [MUSDB18 dataset](https://zenodo.org/record/1117372#.XiSY9Bco9QJ) where the sources consist of contemporary songs and the goal is to divide them into four stems:      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:drum::shark:&nbsp;&nbsp; **drums**      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:studio_microphone::rabbit2:&nbsp;&nbsp; **vocals**      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:guitar::eagle:&nbsp;&nbsp; **bass**      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:saxophone::snake:&nbsp;&nbsp; **other accompaniments**     Music source separation can not only be used as a preprocessing step to other MIR problems (like sound source identification)  but it can also be used more creatively: we can create backing tracks to any song for musical practice or just for fun (karaoke)  we can create ""smart"" equilizers that are able to make a new remix  or we can separate a single instrument to better study its intricacies (guitar players can more easily determine the exact chords for example).   <br>  <p align=""center"">   <img src=""img/spectrogram.png"" alt=""Spectrogram illustration."" width=""600""/>   </p>  <p align=""center"">   <sub><em>Illustration of a separated audio signal (projected on log-scaled spectrograms). The top spectrogram shows the mixed audio that is transformed into the four separated components at the bottom. Note that we use the spectrograms just to illustrate the task — our model operates directly on the audio waveforms.</em></sub> </p>  <br>   """;Audio;https://github.com/pfnet-research/meta-tasnet
"""The StellarGraph library offers state-of-the-art algorithms for [graph machine learning](https://medium.com/stellargraph/knowing-your-neighbours-machine-learning-on-graphs-9b7c3d0d5896)  making it easy to discover patterns and answer questions about graph-structured data. It can solve many machine learning tasks:  - Representation learning for nodes and edges  to be used for visualisation and various downstream machine learning tasks; - [Classification and attribute inference of nodes](https://medium.com/stellargraph/can-graph-machine-learning-identify-hate-speech-in-online-social-networks-58e3b80c9f7e) or edges; - Classification of whole graphs; - Link prediction; - [Interpretation of node classification](https://medium.com/stellargraph/https-medium-com-stellargraph-saliency-maps-for-graph-machine-learning-5cca536974da) [8].  Graph-structured data represent entities as nodes (or vertices) and relationships between them as edges (or links)  and can include data associated with either as attributes. For example  a graph can contain people as nodes and friendships between them as links  with data like a person's age and the date a friendship was established. StellarGraph supports analysis of many kinds of graphs:  - homogeneous (with nodes and links of one type)  - heterogeneous (with more than one type of nodes and/or links) - knowledge graphs (extreme heterogeneous graphs with thousands of types of edges) - graphs with or without data associated with nodes - graphs with edge weights  StellarGraph is built on [TensorFlow 2](https://tensorflow.org/) and its [Keras high-level API](https://www.tensorflow.org/guide/keras)  as well as [Pandas](https://pandas.pydata.org) and [NumPy](https://www.numpy.org). It is thus user-friendly  modular and extensible. It interoperates smoothly with code that builds on these  such as the standard Keras layers and [scikit-learn](http://scikit-learn.github.io/stable)  so it is easy to augment the core graph machine learning algorithms provided by StellarGraph. It is thus also [easy to install with `pip` or Anaconda](#installation).   """;Graphs;https://github.com/stellargraph/stellargraph
"""![](illustration.png)  Efficient attention is an attention mechanism that substantially optimizes the memory and computational efficiency while retaining **exactly** the same expressive power as the conventional dot-product attention. The illustration above compares the two types of attention. The efficient attention module is a drop-in replacement for the non-local module ([Wang et al.  2018](https://arxiv.org/abs/1711.07971))  while it:  - uses less resources to achieve the same accuracy; - achieves higher accuracy with the same resource constraints (by allowing more insertions); and - is applicable in domains and models where the non-local module is not (due to resource constraints).   """;General;https://github.com/cmsflash/efficient-attention
"""![](illustration.png)  Efficient attention is an attention mechanism that substantially optimizes the memory and computational efficiency while retaining **exactly** the same expressive power as the conventional dot-product attention. The illustration above compares the two types of attention. The efficient attention module is a drop-in replacement for the non-local module ([Wang et al.  2018](https://arxiv.org/abs/1711.07971))  while it:  - uses less resources to achieve the same accuracy; - achieves higher accuracy with the same resource constraints (by allowing more insertions); and - is applicable in domains and models where the non-local module is not (due to resource constraints).   """;Computer Vision;https://github.com/cmsflash/efficient-attention
"""![](illustration.png)  Efficient attention is an attention mechanism that substantially optimizes the memory and computational efficiency while retaining **exactly** the same expressive power as the conventional dot-product attention. The illustration above compares the two types of attention. The efficient attention module is a drop-in replacement for the non-local module ([Wang et al.  2018](https://arxiv.org/abs/1711.07971))  while it:  - uses less resources to achieve the same accuracy; - achieves higher accuracy with the same resource constraints (by allowing more insertions); and - is applicable in domains and models where the non-local module is not (due to resource constraints).   """;Natural Language Processing;https://github.com/cmsflash/efficient-attention
"""The goal of **pycls** is to provide a simple and flexible codebase for image classification. It is designed to support rapid implementation and evaluation of research ideas. **pycls** also provides a large collection of baseline results ([Model Zoo](MODEL_ZOO.md)).  The codebase supports efficient single-machine multi-gpu training  powered by the PyTorch distributed package  and provides implementations of standard models including [ResNet](https://arxiv.org/abs/1512.03385)  [ResNeXt](https://arxiv.org/abs/1611.05431)  [EfficientNet](https://arxiv.org/abs/1905.11946)  and [RegNet](https://arxiv.org/abs/2003.13678).   """;Computer Vision;https://github.com/facebookresearch/pycls
"""`SNEMI3D_mito` contains 50 training images  20 validation iamges  and 30 test images. Each of them is an 8-bit grayscale png file and has a size of 1024 x 1024 pixels. [CLAHE](https://imagej.net/Enhance_Local_Contrast_(CLAHE)) was applied beforehand to prevent the amplification of noise. Below is the directory structure of `SNEMI3D_mito`  and when you input your own EM images to the U-Net  your datasets should have the same structure.  ``` SNEMI3D_mito/ ├── train/ │   ├── images/  │   │    ├── 0000.png │   │    ├── 0001.png │   │    ├── ... │   │    └── 0049.png │   └── labels/ │        ├── 0000.png │        ├── 0001.png │        ├── ... │        └── 0049.png ├── valid/ │   ├── images/  │   │    ├── 0050.png │   │    ├── 0051.png │   │    ├── ... │   │    └── 0069.png │   └── labels/  │        ├── 0050.png │        ├── 0051.png │        ├── ... │        └── 0069.png └── test/     ├── images/      │    ├── 0070.png     │    ├── 0071.png     │    ├── ...     │    └── 0099.png     └── labels/          ├── 0070.png          ├── 0071.png          ├── ...          └── 0099.png ```   **U-Net** is a CNN used to segment areas of an image by class  and known for higher RAND index score of mitochodnria segmentation from electron microscopy images [1 2]. This repository provides the Python code for automatic segmentation of mitochondria using (2D) U-Net from EM images by [SNEMI3D](http://brainiac2.mit.edu/SNEMI3D/) [3].  The code for U-Net model was adapted from [this repositroy](https://github.com/YunYang1994/TensorFlow2.0-Examples) [4]. Keras has a built-in class `ImageDataGenerator` for data augmentation  but I have also tried to use the combination of more flexible tools: [`albumentations`](https://github.com/albumentations-team/albumentations) and [`ImageDataAugmentor`](https://github.com/mjkvaak/ImageDataAugmentor) (see [`seg_mito_albumentations.ipynb`](https://github.com/sumiya-kuroda/EMsegmentation-mito/blob/master/notebooks/seg_mito_albumentations.ipynb))[5 6].   <img src=""https://github.com/sumiya-kuroda/EMsegmentation-mito/blob/master/misc/fig.png"" alt=""example"" title=""example"">   """;Computer Vision;https://github.com/sumiya-kuroda/EMsegmentation-mito
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/owainwest/uniprot_bert
"""OpenCV library provides methods to create and detect aruco markers. The drawmarker() method is defined for generating markers. Dictionary should be chosen beforehand. Example:  ```c cv::Mat markerImage; cv::Ptr<cv::aruco::Dictionary> dictionary cv::aruco::getPredefinedDictionary(cv::aruco::DICT_6X6_250); cv::aruco::drawMarker(dictionary  23  200  markerImage  1); cv::imwrite(""marker23.png""  markerImage);` ```  Parameters: -      Dictionary object  created from getPredefinedDictionary method -      marker id – should be in the range  defined for the selected dictionary -      the size of the marker in pixels -      output image -      black border width  expressed by the number of internal bits (optional  default equals 1)  There are also online aruco generators  one of which was used in this project. As an example  http://chev.me/arucogen/ website can be mentioned.   On the wave of current trends in computer vision and drone applications  we are introducing you to a project that we worked on “Landing of Drone on an object”. In general  this means making a drone land on any object by using a landing algorithm and a deep learning algorithm for the detection of an object. We choose the state-of-the-art YOLO algorithm as the object detection algorithm. In this project  our final goal was to land a drone on an object. Therefore  in the first phase  we researched and found an existing solution where the drone was landed on an Aruco marker. In the second phase  we trained the YOLO algorithm to recognize an object (Banana) and modified the landing algorithm to land on that object. Then we further modified the landing algorithm to hover over an object.  In this git laboratory  all the procedures and steps needed to reach the goals of this project were written in detail. During the different phases of our project  we faced a lot of challenges. A special thanks to Tiziano Fiorenzani for whom this project was successful. His YouTube videos and tutorials from GitHub helped us a lot.  References:   Tiziano Fiorenzani - Drone Tutorials  https://www.youtube.com/watch?v=TFDWs_DG2QY&list=PLuteWQUGtU9BcXXr3jCG00uVXFwQJkLRa  AlexeyAB's Darknet repo:  https://github.com/AlexeyAB/darknet/#how-to-train-tiny-yolo-to-detect-your-custom-objects  YOLv3 Series by Ivan Goncharov  https://www.youtube.com/watch?v=R0hipZXJjlI&list=PLZBN9cDu0MSk4IFFnTOIDihvhnHWhAa8W   Ivangrov's Darknet repo:  https://github.com/ivangrov/YOLOv3-Series  Colab Like a Pro by Ivan Goncharov  https://www.youtube.com/watch?v=yPSphbibqrI&list=PLZBN9cDu0MSnxbfY8Pb1gRtR0rKW4RKBw  DroneKit-Python project documentation:  https://dronekit-python.readthedocs.io/en/latest/    """;Computer Vision;https://github.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing
"""To tackle the object confusion problem in few-shot detection  we propose a novel Context-Transformer within a concise deep transfer framework. Specifically  Context-Transformer can effectively leverage source-domain object knowledge as guidance  and automatically formulate relational context clues to enhance the detector's generalization capcity to the target domain. It can be flexibly embedded in the popular SSD-style detectors  which makes it a plug-and-play module for end-to-end few-shot learning. For more details  please refer to our [original paper](https://arxiv.org/pdf/2003.07304.pdf).  <p align=center><img width=""80%"" src=""doc/Motivation.png""/></p>    """;Computer Vision;https://github.com/Ze-Yang/Context-Transformer
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   In this [paper](http://arxiv.org/abs/2005.03572)  we propose Complete-IoU (CIoU) loss and Cluster-NMS for enhancing geometric factors in both bounding box regression and Non-Maximum Suppression (NMS)  leading to notable gains of average precision (AP) and average recall (AR)  without the sacrifice of inference efficiency. In particular  we consider three geometric factors  i.e.  overlap area  normalized central point distance and aspect ratio  which are crucial for measuring bounding box regression in object detection and instance segmentation. The three geometric factors are then incorporated into CIoU loss for better distinguishing difficult regression cases. The training of deep models using CIoU loss results in consistent AP and AR improvements in comparison to widely adopted Ln-norm loss and IoU-based loss. Furthermore  we propose Cluster-NMS  where NMS during inference is done by implicitly clustering detected boxes and usually requires less iterations. Cluster-NMS is very efficient due to its pure GPU implementation  and geometric factors can be incorporated to improve both AP and AR. In the experiments  CIoU loss and Cluster-NMS have been applied to state-of-the-art instance segmentation (e.g.  YOLACT)  and object detection (e.g.  YOLO v3  SSD and Faster R-CNN) models.   """;Computer Vision;https://github.com/Zzh-tju/ultralytics-YOLOv3-Cluster-NMS
"""This project maps tree extent at the ten-meter scale using open source artificial intelligence and satellite imagery. The data enables accurate reporting of tree cover in urban areas  tree cover on agricultural lands  and tree cover in open canopy and dry forest ecosystems.   This repository contains the source code for the project. A full description of the methodology can be found [on arXiv](https://arxiv.org/abs/2005.08702). The data product specifications can be accessed on the wiki page. *  [Background](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#background) *  [Data Extent](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#data-extent) *  [Methodology](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#methodology) *  [Validation and Analysis](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#validation-and-analysis) | [Jupyter Notebook](https://github.com/wri/restoration-mapper/blob/master/notebooks/analysis/validation-analysis.ipynb) *  [Definitions](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#definitions) *  [Limitations](https://github.com/wri/restoration-mapper/wiki/Product-Specifications#limitations)    """;General;https://github.com/wri/sentinel-tree-cover
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/algharak/BERTenhance
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/algharak/BERTenhance
"""An important field within computer vision is medical imaging. However  a main problem within this area of research is that it is difficult to obtain a large sample of training images. Limitations to obtaining brain MRIs include: the low availability of participants  the time it takes to obtain and process high resolution MRI brain images  as well as the fact that participants have to stay still for long periods of time (whichkes it difficult to obtain a good image). Therefore it is useful to implement a generative adversarial network (GAN) that can be trained on existing brain MRIs and then if trained successfully  it can generate an infinite number of plausible brain MRIs. This would aid the training of computer vision techniques such as brain segmentation which would require much more expansive datasets that may otherwise not exist without many man-hours of medical imaging.   In particular  I have implemented a deep convolutional generative adversarial network (DCGAN) with reference DCGAN specifications in the paper written by Radford  Metz and Chintala [1]. In the DCGAN  the use of convolutional layers allows higher quality feature mapping and recognition relative to the traditional GAN which is only connected by dense layers. In my GAN implementation  I followed specifications such as: * using LeakyReLU in the discriminator * using strided convolutions in the discriminator and fractional-strided convolutions in the generator * using batchnorm in both the generator and discriminator * remove fully connected hidden layers * scaling training image pixel values from -1 to 1 * in LeakyReLU  the slope of the leak was set to 0.2 * using an Adam optimiser with learning rate of 0.0002 for both generator and discriminator (I used 0.0002 for the generator and 0.0001 for the discriminator) * no pooling layers  I also did not follow several specifications as I found they either did not work or produced lower quality results: * They suggested the use of a ReLU activation function in the generator  however  I found LeakyReLU worked better as they are more effective in preventing vanishing gradients. They also suggested the use of a Tanh activation function in the final layer of the generator  however  I found my model worked better without any activation functions in the generator and discriminator. * Instead of using a batch size of 128  I used a batch size of 10 (i.e. 10 real and 10 fake images in each batch). I found larger batch sizes would overload the GPU. * The paper suggested the use of beta_1=0.5 for the Adam optimiser  however I found that using the default beta_1=0.9 worked fine. * I decided to use a latent space of 256 instead of 100 for no real reason and this worked quite well * For the Conv2DTranspose layers  when using the depicted kernel size of (5 5) with stride (2 2) (Figure 1 in [1]) I got very aliased generator images with grid artifacts. This was remedied by using a kernel size of (4 4) with stride (2 2) * Also in reference to Figure 1 in [1]  I tried using four fractionally-strided convolutions (Conv2DTranspose) layers with one convolutional layer after and ended up with mode collapse. My model was working and produced very high quality brain images (SSIM>0.6) however  my generator would only produce the same images regardless of the noise input. I later fixed this by using three Conv2DTranspose layers and two convolutional layers instead. * I used dropout layers in my discriminator to make my discriminator learn more slowly. I did not try running the GAN without dropout layers so I'm not sure if this had any real effect  but the current model is quite effective. * In contrast to the number of filters in the generator in Figure 1 in [1] (which had filters 1024  512  256  128 for the Conv2DTranspose layers)  I used a maximum of 256 for the filters in my layers. I originally implemented the same number of filters in my generator as the paper  however  I found that my GPU would run out of memory due to the large number of filters. Also the brain MRI images are quite simple so may not require the larger number of filters.   """;Computer Vision;https://github.com/amyzhao11/GANBrain
"""Can we infer intentions and goals from a person's actions? As an example of this family of problems  we consider here whether it is possible to decipher what a person is searching for by decoding their eye movement behavior. We conducted two human psychophysics experiments on object arrays and natural images where we monitored subjects' eye movements while they were looking for a target object. Using as input the pattern of ""error"" fixations on non-target objects before the target was found  we developed a model (InferNet) whose goal was to infer what the target was. ""Error"" fixations share similar features with the sought target. The Infernet model uses a pre-trained 2D convolutional architecture to extract features from the error fixations and computes a 2D similarity map between the error fixation and all locations across the search image by modulating the search image via convolution across layers. InferNet consolidates the modulated response maps across layers via max pooling to keep track of the sub-patterns highly similar to features at error fixations and integrates these maps across all error fixations. InferNet successfully identifies the subject's goal and outperforms all the competitive null models  even without any object-specific training on the inference task.   [![problemintro](img/Capture.JPG)](img/Capture.JPG)  We present an example below. The figure shows the error fixations indicated by yellow number in the visual search process of one human subject (Column 1)  the inference maps predicted by our InferNet based on human error fixations (Column 2) and the inferred target location denoted by green numbers (Column 3) from the inference maps. Red color denotes higher probability of being the search target.  [![problemintro](img/1-teaser.gif)](img/1-teaser.gif)   """;Computer Vision;https://github.com/kreimanlab/HumanIntentionInferenceZeroShot
"""|                              |                          |                            | | :--------------------------: | :----------------------: | :------------------------: | |         AutoEncoder          | Variational AutoEncoder  |           BiGAN            | |   ![](metrics/ae/roc.png)    | ![](metrics/vae/roc.png) | ![](metrics/bigan/roc.png) | |           Seq2Seq            |           PCA            |           OCSVM            | | ![](metrics/seq2seq/roc.png) | ![](metrics/pca/roc.png) | ![](metrics/ocsvm/roc.png) |  For each model  we use labeled test data to first select a threshold that yields the best accuracy and then report on metrics such as f1  f2  precision  and recall at that threshold. We also report on ROC (area under the curve) to evaluate the overall skill of each model. Given that the dataset we use is not extremely complex (18 features)  we see that most models perform relatively well. Deep models (BiGAN  AE) are more robust (precision  recall  ROC AUC)  compared to PCA and OCSVM. The sequence-to-sequence model is not particularly competitive  given the data is not temporal. On a more complex dataset (e.g.  images)  we expect to see (similar to existing research)  more pronounced advantages in using a deep learning model.  For additional details on each model  see our [report](https://ff12.fastforwardlabs.com/). Note that models implemented here are optimized for tabular data. For example  extending this to work with image data will usually require the use of convolutional layers (as opposed to dense layers) within the neural network models to achieve performant results.  |                                  |                              |                                | | :------------------------------: | :--------------------------: | :----------------------------: | |           AutoEncoder            |   Variational AutoEncoder    |             BiGAN              | |   ![](metrics/ae/metrics.png)    | ![](metrics/vae/metrics.png) | ![](metrics/bigan/metrics.png) | |             Seq2Seq              |             PCA              |             OCSVM              | | ![](metrics/seq2seq/metrics.png) | ![](metrics/pca/metrics.png) | ![](metrics/ocsvm/metrics.png) |   """;Computer Vision;https://github.com/fastforwardlabs/deepad
"""|                              |                          |                            | | :--------------------------: | :----------------------: | :------------------------: | |         AutoEncoder          | Variational AutoEncoder  |           BiGAN            | |   ![](metrics/ae/roc.png)    | ![](metrics/vae/roc.png) | ![](metrics/bigan/roc.png) | |           Seq2Seq            |           PCA            |           OCSVM            | | ![](metrics/seq2seq/roc.png) | ![](metrics/pca/roc.png) | ![](metrics/ocsvm/roc.png) |  For each model  we use labeled test data to first select a threshold that yields the best accuracy and then report on metrics such as f1  f2  precision  and recall at that threshold. We also report on ROC (area under the curve) to evaluate the overall skill of each model. Given that the dataset we use is not extremely complex (18 features)  we see that most models perform relatively well. Deep models (BiGAN  AE) are more robust (precision  recall  ROC AUC)  compared to PCA and OCSVM. The sequence-to-sequence model is not particularly competitive  given the data is not temporal. On a more complex dataset (e.g.  images)  we expect to see (similar to existing research)  more pronounced advantages in using a deep learning model.  For additional details on each model  see our [report](https://ff12.fastforwardlabs.com/). Note that models implemented here are optimized for tabular data. For example  extending this to work with image data will usually require the use of convolutional layers (as opposed to dense layers) within the neural network models to achieve performant results.  |                                  |                              |                                | | :------------------------------: | :--------------------------: | :----------------------------: | |           AutoEncoder            |   Variational AutoEncoder    |             BiGAN              | |   ![](metrics/ae/metrics.png)    | ![](metrics/vae/metrics.png) | ![](metrics/bigan/metrics.png) | |             Seq2Seq              |             PCA              |             OCSVM              | | ![](metrics/seq2seq/metrics.png) | ![](metrics/pca/metrics.png) | ![](metrics/ocsvm/metrics.png) |   """;General;https://github.com/fastforwardlabs/deepad
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/somiltg/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Yipeng91/text_classifier_pub
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/owainwest/uniprot_bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Misoknisky/Bert-MultiGpu
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Misoknisky/Bert-MultiGpu
"""Verification and regression are two general methodologies for prediction in neural networks. Each has its own strengths: verification can be easier to infer accurately  and regression is more efficient and applicable to continuous target variables. Hence  it is often beneficial to carefully combine them to take advantage of their benefits. We introduce verification tasks into the localization prediction of RepPoints  producing **RepPoints v2**.   RepPoints v2 aims for object detection and it achieves `52.1 bbox mAP` on COCO test-dev by a single model. Dense RepPoints v2 aims for instance segmentation and it achieves `45.9 bbox mAP` and `39.0 mask mAP` on COCO test-dev by using a ResNet-50 model.  <div align=""center"">   <img src=""demo/reppointsv2.png"" width=""1178"" /> </div>   """;Computer Vision;https://github.com/Scalsol/RepPointsV2
"""Here  we implement basic data-handling tools for [SARAS-ESAD](https://saras-esad.grand-challenge.org/Dataset/) dataset with FPN training process. We implement a pure pytorch code for train FPN with [Focal-Loss](https://arxiv.org/pdf/1708.02002.pdf) or [OHEM/multi-box-loss](https://arxiv.org/pdf/1512.02325.pdf) paper.  <!-- Aim of this repository try different loss functions and make a fair comparison in terms of performance on SARAR-ESAD dataset. -->  We hope this will help kick start more teams to get up to the speed and allow the time for more innovative solutions. We want to eliminate the pain of building data handling and training process from scratch. Our final aim is to get this repository the level of [realtime-action-detection](https://github.com/gurkirt/realtime-action-detection).  At the moment we support the latest pytorch and ubuntu with Anaconda distribution of python. Tested on a single machine with 2/4/8 GPUs.  You can found out about architecture and loss function on parent repository  i.e. [RetinaNet implementation in pytorch.1.x](https://github.com/gurkirt/RetinaNet.pytorch.1.x).  ResNet is used as a backbone network (a) to build the pyramid features (b).  Each classification (c) and regression (d) subnet is made of 4 convolutional layers and finally a convolutional layer to predict the class scores and bounding box coordinated respectively.  Similar to the original paper  we freeze the batch normalisation layers of ResNet based backbone networks. Also  few initial layers are also frozen  see `fbn` flag in training arguments.    """;Computer Vision;https://github.com/Viveksbawa/SARAS-ESAD-Baseline
"""Here  we implement basic data-handling tools for [SARAS-ESAD](https://saras-esad.grand-challenge.org/Dataset/) dataset with FPN training process. We implement a pure pytorch code for train FPN with [Focal-Loss](https://arxiv.org/pdf/1708.02002.pdf) or [OHEM/multi-box-loss](https://arxiv.org/pdf/1512.02325.pdf) paper.  <!-- Aim of this repository try different loss functions and make a fair comparison in terms of performance on SARAR-ESAD dataset. -->  We hope this will help kick start more teams to get up to the speed and allow the time for more innovative solutions. We want to eliminate the pain of building data handling and training process from scratch. Our final aim is to get this repository the level of [realtime-action-detection](https://github.com/gurkirt/realtime-action-detection).  At the moment we support the latest pytorch and ubuntu with Anaconda distribution of python. Tested on a single machine with 2/4/8 GPUs.  You can found out about architecture and loss function on parent repository  i.e. [RetinaNet implementation in pytorch.1.x](https://github.com/gurkirt/RetinaNet.pytorch.1.x).  ResNet is used as a backbone network (a) to build the pyramid features (b).  Each classification (c) and regression (d) subnet is made of 4 convolutional layers and finally a convolutional layer to predict the class scores and bounding box coordinated respectively.  Similar to the original paper  we freeze the batch normalisation layers of ResNet based backbone networks. Also  few initial layers are also frozen  see `fbn` flag in training arguments.    """;General;https://github.com/Viveksbawa/SARAS-ESAD-Baseline
"""SWA is a simple DNN training method that can be used as a drop-in replacement for SGD with improved generalization  faster convergence  and essentially no overhead. The key idea of SWA is to average multiple samples produced by SGD with a modified learning rate schedule. We use a constant or cyclical learning rate schedule that causes SGD to _explore_ the set of points in the weight space corresponding to high-performing networks. We observe that SWA converges more quickly than SGD  and to wider optima that provide higher test accuracy.   In this repo we implement the constant learning rate schedule that we found to be most practical on CIFAR datasets.  <p align=""center"">   <img src=""https://user-images.githubusercontent.com/14368801/37633888-89fdc05a-2bca-11e8-88aa-dd3661a44c3f.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633885-89d809a0-2bca-11e8-8d57-3bd78734cea3.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633887-89e93784-2bca-11e8-9d71-a385ea72ff7c.png"" width=250> </p>  Please cite our work if you find this approach useful in your research: ```latex @article{izmailov2018averaging    title={Averaging Weights Leads to Wider Optima and Better Generalization}    author={Izmailov  Pavel and Podoprikhin  Dmitrii and Garipov  Timur and Vetrov  Dmitry and Wilson  Andrew Gordon}    journal={arXiv preprint arXiv:1803.05407}    year={2018} } ```    """;General;https://github.com/izmailovpavel/torch_swa_examples
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/kiko441500/google_bert
"""**ELECTRA** is a method for self-supervised language representation learning. It can be used to pre-train transformer networks using relatively little compute. ELECTRA models are trained to distinguish ""real"" input tokens vs ""fake"" input tokens generated by another neural network  similar to the discriminator of a [GAN](https://arxiv.org/pdf/1406.2661.pdf). At small scale  ELECTRA achieves strong results even when trained on a single GPU. At large scale  ELECTRA achieves state-of-the-art results on the [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset.  For a detailed description and experimental results  please refer to our ICLR 2020 paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/pdf?id=r1xMH1BtvB).  This repository contains code to pre-train ELECTRA  including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g . [GLUE](https://gluebenchmark.com/))  QA tasks (e.g.  [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/))  and sequence tagging tasks (e.g.  [text chunking](https://www.clips.uantwerpen.be/conll2000/chunking/)).  This repository also contains code for **Electric**  a version of ELECTRA inspired by [energy-based models](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf). Electric provides a more principled view of ELECTRA as a ""negative sampling"" [cloze model](https://en.wikipedia.org/wiki/Cloze_test). It can also efficiently produce [pseudo-likelihood scores](https://arxiv.org/pdf/1910.14659.pdf) for text  which can be used to re-rank the outputs of speech recognition or machine translation systems. For details on Electric  please refer to out EMNLP 2020 paper [Pre-Training Transformers as Energy-Based Cloze Models](https://www.aclweb.org/anthology/2020.emnlp-main.20.pdf).     """;Natural Language Processing;https://github.com/google-research/electra
"""This is an official pytorch implementation of [*Deep High-Resolution Representation Learning for Human Pose Estimation*](https://arxiv.org/abs/1902.09212).  In this work  we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods **recover high-resolution representations from low-resolution representations** produced by a high-to-low resolution network. Instead  our proposed network **maintains high-resolution representations** through the whole process. We start from a high-resolution subnetwork as the first stage  gradually add high-to-low resolution subnetworks one by one to form more stages  and connect the mutli-resolution subnetworks **in parallel**. We conduct **repeated multi-scale fusions** such that each of the high-to-low resolution representations receives information from other parallel representations over and over  leading to rich high-resolution representations. As a result  the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. </br>  ![Illustrating the architecture of the proposed HRNet](/figures/hrnet.png)  """;Computer Vision;https://github.com/baoshengyu/deep-high-resolution-net.pytorch
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/PiseyYou/MobileNet_Caffe_Configuration
"""This model is a semantic image segmentation model  which assigns label to each pixel of an image to partition different objects into segments. The whole model is composed of two parts  namely backbone part and classifier part. The backbone part is resnet101 which has been pre-trained  and the classifier part (DeepLabV3+ head  implemented by https://github.com/jfzhang95/pytorch-deeplab-xception using PyTorch) is fine-tuned based on this specific task.    """;General;https://github.com/sdyy6211/plant-segmentation
"""In this project  my aim was to develop a model that could regenerate patched/covered parts of human faces  and achieve believable results. I used the [Celeb-A](https://www.kaggle.com/jessicali9530/celeba-dataset) dataset  and created a Generative Adversarial Network with a Denoising Autoencoder as the Generator and a Deep Convolutional Network as the Discriminator. I chose this architecture based on *Avery Allen and Wenchen Li*'s [Generative Adversarial Denoising Autoencoder for Face Completion](https://www.cc.gatech.edu/~hays/7476/projects/Avery_Wenchen/).  The Denoising Autoencoder has 'relu' activations in the middle layers while the output layer had a 'tanh' activation. Each Convolution layer was followed by a BatchNormalization layer. The Discriminator has 'LeakyReLU' activations for the Convolution part  with a BatchNormalization layer following every Conv layer. At the end  the output from the CNN segment was flattened and connected to a Dense layer with 1 node  having 'sigmoid' as the activation function. This would enable the discrimator to predict the probability that the input image is real.  I added distortions to the images in two ways:- - Added random Gaussian noise. - Added random sized Black Patches.  The entire training was done on a GTX 1080 GPU  and took about 12days.  The latest checkpoints and the saved generator and discriminator can be found [here](https://drive.google.com/drive/folders/13wUgCcENajkPZ4MHz2bHrJtQepyVDvtb?usp=sharing).  A few sample generated images are present in `saved_imgs`.   """;Computer Vision;https://github.com/rdutta1999/Patched-Face-Regeneration-GAN
"""This is a work by Charles R. Qi  Hao Su  Kaichun Mo  Leonidas J. Guibas from Stanford University.You can find the link to their paper here[https://arxiv.org/abs/1612.00593]. Their original work is done using tensorflow1.x and few other packages which are deprecated in newer version. So  I implemented their model using tensorflow 2.0 and python 3.7. Shown below is an example of 3D point cloud objects in ModelNet40 Dataset. You need to do few more steps to train /test the model.  Step_1 -> Install h5py<br>           sudo apt-get install libhdf5-dev<br>           sudo pip install h5py<br> Step_2 -> Download the ModelNet40 dataset and copy the files to data folder. Type the following command in the terminal and unzip the file.<br> <b>wget --no-check-certificate -P ""pointnet/data"" https://shapenet.cs.stanford.edu/ericyi/shapenetcore_partanno_v0.zip</b><br>  Step_3 -> In terminal use ""python train.py"". For evaluation run  ""python evaluate.py --visu""<br> <b> I have not included the weight files here cause of it's size. But if you need the weight files you can pull a request  I can share via my drive </b>   ![3D point cloud vase](https://github.com/SonuDileep/3-D-Object-Detection-using-PointNet/blob/master/vase.jpg)   """;Computer Vision;https://github.com/SonuDileep/3-D-Object-Detection-using-PointNet
"""A brief content description is provided here  for detailed descriptions check the notebook comments     """;Computer Vision;https://github.com/5m0k3/gwd-efficientdet-pytorch
"""The purpose of this project is to develop deep learning approaches for the segmentation of brain tissues. These segmentations are useful for measuring and visualizing anatomical structures  but also to analyze brain changes in case of diseases like Alzheimer. Today different automatic segmentations are available thanks to FAST (FSL)  Freesurfer and ANTS. But these approaches are often inaccurate and require additional manual segmentations which are both time consuming and challenging.    """;Computer Vision;https://github.com/sophieloiz/brain-tissues-segmentation
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/y2kmz/pointnetv2
""" - Demo.ipynb - allows you to check enviroment and see working agent example  - Solver.ipynb - reproduces the training procedure  - agent.py - TD3 agent implementation  - networks.py - actor and critic Pytorch definitions  - replay_byffer.py - Replay Buffer implementation from OpenAI Baselines   - actor.pth - Saved weights for Actor network from TD3  - critic.pth - Saved weights from Critic networks from TD3    """;General;https://github.com/crazyleg/TD3-reacher
""" - Demo.ipynb - allows you to check enviroment and see working agent example  - Solver.ipynb - reproduces the training procedure  - agent.py - TD3 agent implementation  - networks.py - actor and critic Pytorch definitions  - replay_byffer.py - Replay Buffer implementation from OpenAI Baselines   - actor.pth - Saved weights for Actor network from TD3  - critic.pth - Saved weights from Critic networks from TD3    """;Reinforcement Learning;https://github.com/crazyleg/TD3-reacher
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Ewillingfly/MyBert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Ewillingfly/MyBert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/TonyX19/bert_hyperpartisan
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/TonyX19/bert_hyperpartisan
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Yipeng91/text_classifier_pub
"""- Positional encoding is important (it's quite obvious) to make it work - It seems to be more robust than other approaches I tried before - Surprisingly it works with one head and with only one layer  also without having the residual connection and normalization - Vanilla SGD did not work that well   """;General;https://github.com/krocki/np-transformer
"""- Positional encoding is important (it's quite obvious) to make it work - It seems to be more robust than other approaches I tried before - Surprisingly it works with one head and with only one layer  also without having the residual connection and normalization - Vanilla SGD did not work that well   """;Natural Language Processing;https://github.com/krocki/np-transformer
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/AmitNativ1984/yolov3
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/kiko441500/google_bert
"""This is fairseq  a sequence-to-sequence learning toolkit for [Torch](http://torch.ch/) from Facebook AI Research tailored to Neural Machine Translation (NMT). It implements the convolutional NMT models proposed in [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) and [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344) as well as a standard LSTM-based model. It features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU. We provide pre-trained models for English to French  English to German and English to Romanian translation.  Note  there is now a PyTorch version [fairseq-py](https://github.com/facebookresearch/fairseq-py) of this toolkit and new development efforts will focus on it.  ![Model](fairseq.gif)   """;Natural Language Processing;https://github.com/siyuofzhou/CNNSeqToSeq
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/tade0726/yolov3_ui_error
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/FaskyCC/TextClassification
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/yrouphail/yolov3
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/vanpersie32/Multigpu-Bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/FaskyCC/TextClassification
"""---    """;Computer Vision;https://github.com/J-woooo/Mask_Remover
"""This repository contains the author's implementation of ECCV 2018 paper ""License Plate Detection and Recognition in Unconstrained Scenarios"".  * Paper webpage: http://sergiomsilva.com/pubs/alpr-unconstrained/  If you use results produced by our code in any publication  please cite our paper:  ``` @INPROCEEDINGS{silva2018a    author={S. M. Silva and C. R. Jung}     booktitle={2018 European Conference on Computer Vision (ECCV)}     title={License Plate Detection and Recognition in Unconstrained Scenarios}     year={2018}     pages={580-596}     doi={10.1007/978-3-030-01258-8_36}     month={Sep} } ```   """;Computer Vision;https://github.com/sergiomsilva/alpr-unconstrained
"""R-CNN is a state-of-the-art visual object detection system that combines bottom-up region proposals with rich features computed by a convolutional neural network. At the time of its release  R-CNN improved the previous best detection performance on PASCAL VOC 2012 by 30% relative  going from 40.9% to 53.3% mean average precision. Unlike the previous best results  R-CNN achieves this performance without using contextual rescoring or an ensemble of feature types.  R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1311.2524) and will appear in a forthcoming CVPR 2014 paper.   """;Computer Vision;https://github.com/rbgirshick/rcnn
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/vanpersie32/Multigpu-Bert
"""CenterNet is a framework for object detection with deep convolutional neural networks. You can use the code to train and evaluate a network for object detection on the MS-COCO dataset.  * It achieves state-of-the-art performance (an AP of 47.0%) on one of the most challenging dataset: MS-COCO.  * Our code is written in Python  based on [CornerNet](https://github.com/princeton-vl/CornerNet).  *More detailed descriptions of our approach and code will be made available soon.*  **If you encounter any problems in using our code  please contact Kaiwen Duan: kaiwen.duan@vipl.ict.ac.cn.**   """;Computer Vision;https://github.com/jiajunhua/Duankaiwen-CenterNet
"""This is a reimplementation of the proximal policy algorithm    """;Reinforcement Learning;https://github.com/Gregory-Eales/proximal-policy-optimization
"""Neural Radiance Fields (NeRF) is a method for synthesizing novel views of complex scenes  by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Views are synthesized by querying 5D coordinates (spatial location (*x*  *y*  *z*) and viewing direction (*θ*  *ϕ*)) along camera rays and using classic volume rendering techniques to project the output colors and densities into an image.  This implementation tries to be as close as possible to the original source  bringing some code optimizations and using the flexibility and native multi device (GPUs and TPUs) support in JAX.  Most of the comments are from the original work  which are very helpful for understanding the model steps.   """;Computer Vision;https://github.com/myagues/flax_nerf
"""Get a conceptual overview of image classification  object localization  object detection  and image segmentation. Also be able to describe multi-label classification  and distinguish between semantic segmentation and instance segmentation. In the rest of this course  you will apply TensorFlow to build object detection and image segmentation models.  Learning Objectives:  - Distinguish between object localization and object detection - Distinguish between object detection and image segmentation - Distinguish between semantic segmentation and instance segmentation - Explain what is transfer learning and why it's used - Describe design options when using transfer learning - Implement object localization with a CNN - Implement an image classifier with transfer learning  Papers about Image segmentation models:  Fully Convolutional Networks for Semantic Segmentation https://people.eecs.berkeley.edu/~shelhamer/data/fcn.pdf  U-Net: Convolutional Networks for Biomedical Image Segmentation https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/  DeepLab: Semantic Image Segmentation with Deep Convolutional Nets  Atrous Convolution  and Fully Connected CRFs http://liangchiehchen.com/projects/DeepLab.html  Mask R-CNN: https://arxiv.org/abs/1703.06870    """;Computer Vision;https://github.com/polospeter/TensorFlow-Advanced-Techniques-Specialization
"""The master branch works with **PyTorch 1.1** or higher.  mmdetection is an open source object detection toolbox based on PyTorch. It is a part of the open-mmlab project developed by [Multimedia Laboratory  CUHK](http://mmlab.ie.cuhk.edu.hk/).  ![demo image](demo/coco_test_12510.jpg)   """;Computer Vision;https://github.com/OceanPang/Libra_R-CNN
"""The master branch works with **PyTorch 1.1** or higher.  mmdetection is an open source object detection toolbox based on PyTorch. It is a part of the open-mmlab project developed by [Multimedia Laboratory  CUHK](http://mmlab.ie.cuhk.edu.hk/).  ![demo image](demo/coco_test_12510.jpg)   """;General;https://github.com/OceanPang/Libra_R-CNN
"""The goal of **pycls** is to provide a simple and flexible codebase for image classification. It is designed to support rapid implementation and evaluation of research ideas. **pycls** also provides a large collection of baseline results ([Model Zoo](MODEL_ZOO.md)).  The codebase supports efficient single-machine multi-gpu training  powered by the PyTorch distributed package  and provides implementations of standard models including [ResNet](https://arxiv.org/abs/1512.03385)  [ResNeXt](https://arxiv.org/abs/1611.05431)  [EfficientNet](https://arxiv.org/abs/1905.11946)  and [RegNet](https://arxiv.org/abs/2003.13678).   """;General;https://github.com/facebookresearch/pycls
"""*Source Separation* is a repository to extract speeches from various recorded sounds. It focuses to adapt more real-like dataset for training models.   **This project aims at building a speech enhancement system to attenuate environmental noise.**  <img src=""denoise_10classes.gif"" alt=""Spectrogram denoising"" title=""Speech enhancement""/>    Audios have many different ways to be represented  going from raw time series to time-frequency decompositions. The choice of the representation is crucial for the performance of your system. Among time-frequency decompositions  Spectrograms have been proved to be a useful representation for audio processing. They consist in 2D images representing sequences of Short Time Fourier Transform (STFT) with time and frequency as axes  and brightness representing the strength of a frequency component at each time frame. In such they appear a natural domain to apply the CNNS architectures for images directly to sound. Between magnitude and phase spectrograms  magnitude spectrograms contain most the structure of the signal. Phase spectrograms appear to show only little temporal and spectral regularities.  In this project  I will use magnitude spectrograms as a representation of sound (cf image below) in order to predict the noise model to be subtracted to a noisy voice spectrogram.  <img src=""sound_to_spectrogram.png"" alt=""sound representation"" title=""sound representation"" />  The project is decomposed in three modes: `data creation`  `training` and `prediction`.   """;Computer Vision;https://github.com/tarit21/Speech-Enhancement-module
"""As stated above  the data is separated into a training set and a validation set. Since all parameters are provided by the paper clearly  there is no model tuning process that depend on validation results. The validation set is used as the function of the testing set. The code below will print out the training process. Graphs of loss and accuracy through training and other Keras supported metrics could be accessed fro mthe history variable.  ```python DeepLOB_model = initiate_DeepLOB_model(lookback_timestep  feature_num  conv_filter_num  inception_num  LSTM_num  leaky_relu_alpha                            loss  optimizer  metrics)  #: definte the training stop criteria (no new max validation accuracy in 20 consecutive epochs) es = EarlyStopping(monitor='val_accuracy'  mode='max'  patience = stop_epoch_num  verbose=1) history = DeepLOB_model.fit(X_train  y_train  epochs=num_epoch  batch_size=batch_size  verbose=2  validation_data=(X_test  y_test)  callbacks = [es]) ```   """;Computer Vision;https://github.com/yuxiangalvin/DeepLOB-Model-Implementation-Project
"""1. **`'method'`**: It is reserved for future use. 2. **`'regu_method'`**: We introduced three normalization methods in the paper  namely  `MPN-COV` `MPN-COV+matrix-l2` `MPN-COV+matrix-Fro`. As the latter two normalizations produced unsatisfactory performance  we only support MPN-COV  designated by `'power'`. 3. **`'alpha'`**: It denotes the exponent of matrix power function(equivalently  the power of eigenvalues  see the paper)  whose values should be positive. The default value is 0.5 producing the best performance. 4. **`'epsilon'`**: It is a small positive number added to eigenvalues of covariance matrices. It is set to 0 as the Power-E metric allows the eigenvalue to be non-negative.   This repository contains the source code and models trained on ImageNet 2012 dataset for the following paper:      @article{Li2017          author = {Peihua Li Jiangtao Xie Qilong Wang and Wangmeng Zuo}          title  = {Is Second-order Information Helpful for Large-scale Visual Recognition?}          journal= {International Conference on Computer Vision (ICCV)}          year   = {2017}     }  We proposed the second-order pooling to replace the common first-order  max/average pooling after the last conv. layer. The proposed networks  called MPN-COV ConvNets  achieved consistent  nontrivial improvements over their counterparts. The key to our method is **Matrix Power Normalization of COVariance**  which  1. amounts to robust covariance estimation given a small number of large-dimensional features(a.k.a. small sample/large dimension)  as commonly seen in the last convolutional layers in state-of-the-art ConvNets; 2. appropriately exploits Riemannian geometry which allows zero eigenvalues  overcoming the downside of the well-known Log-Euclidean metric in this scenario.      ![result](doc/figures/results.jpg)    - Figure 1: Error(% 10-crop) comparison of MPN-COV ConvNets with the counterparts. We can see our method can improve the performance of top-1 1.6% ~ 6.8% and top-5 1.0% ~ 4.0%.  You can visit our [project page](http://peihuali.org/iSQRT-COV/index.html) for more details.  """;General;https://github.com/jiangtaoxie/MPN-COV
"""![](./data/UncerGuidedI2I_Model.gif)  This repository provides the code for the **MICCAI-2021** paper titled ""[Uncertainty-guided Progressive GANs for Medical Image Translation](https://arxiv.org/abs/2106.15542)"".  We take inspiration from the progressive learning scheme demonstrated at [MedGAN](https://arxiv.org/abs/1806.06397) and [Progressive GANs](https://arxiv.org/abs/1710.10196)  and augment the learning with the estimation of intermediate uncertainty maps (as presented [here](http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-061.pdf) and [here](https://arxiv.org/pdf/2102.11747.pdf))  that are used as attention map to focus the image translation in poorly generated (highly uncertain) regions  progressively improving the images over multiple phases.  ![](./data/UncerGuidedI2I_res.gif)  The structure of the repository is as follows: ``` root  |-ckpt/ (will save all the checkpoints)  |-data/ (save your data and related script)  |-src/ (contains all the source code)     |-ds.py      |-networks.py     |-utils.py     |-losses.py ```   """;Computer Vision;https://github.com/ExplainableML/UncerGuidedI2I
"""The articulated 3D pose of the human body is high-dimensional and complex.  Many applications make use of a prior distribution over valid human poses  but modeling this distribution is difficult. Here we present VPoser  a learning based variational human pose prior trained from a large dataset of human poses represented as SMPL bodies. This body prior can be used as an Inverse Kinematics (IK) solver for many tasks such as fitting a body model to images  as the main contribution of this repository for [SMPLify-X](https://smpl-x.is.tue.mpg.de/).  VPoser has the following features:   - defines a prior of SMPL pose parameters  - is end-to-end differentiable  - provides a way to penalize impossible poses while admitting valid ones  - effectively models correlations among the joints of the body  - introduces an efficient  low-dimensional  representation for human pose  - can be used to generate valid 3D human poses for data-dependent tasks   """;Computer Vision;https://github.com/nghorbani/human_body_prior
"""The articulated 3D pose of the human body is high-dimensional and complex.  Many applications make use of a prior distribution over valid human poses  but modeling this distribution is difficult. Here we present VPoser  a learning based variational human pose prior trained from a large dataset of human poses represented as SMPL bodies. This body prior can be used as an Inverse Kinematics (IK) solver for many tasks such as fitting a body model to images  as the main contribution of this repository for [SMPLify-X](https://smpl-x.is.tue.mpg.de/).  VPoser has the following features:   - defines a prior of SMPL pose parameters  - is end-to-end differentiable  - provides a way to penalize impossible poses while admitting valid ones  - effectively models correlations among the joints of the body  - introduces an efficient  low-dimensional  representation for human pose  - can be used to generate valid 3D human poses for data-dependent tasks   """;General;https://github.com/nghorbani/human_body_prior
"""Among all the skin cancer type  melanoma is the least common skin cancer  but it is responsible for **75%** of death [SIIM-ISIC Melanoma Classification  2020](https://www.kaggle.com/c/siim-isic-melanoma-classification). Being a less common skin cancer type but is spread very quickly to other body parts if not diagnosed early. The **International Skin Imaging Collaboration (ISIC)** is facilitating skin images to reduce melanoma mortality. Melanoma can be cured if diagnosed and treated in the early stages. Digital skin lesion images can be used to make a teledermatology automated diagnosis system that can support clinical decision.  Currently  deep learning has revolutionised the future as it can solve complex problems. The motivation is to develop a solution that can help dermatologists better support their diagnostic accuracy by ensembling contextual images and patient-level information  reducing the variance of predictions from the model.   """;Computer Vision;https://github.com/Tirth27/Skin-Cancer-Classification-using-Deep-Learning
"""The goal of this project is to enable users to create cool web demos using the newly released OpenAI GPT-3 API **with just a few lines of Python.**   This project addresses the following issues:  1. Automatically formatting a user's inputs and outputs so that the model can effectively pattern-match 2. Creating a web app for a user to deploy locally and showcase their idea  Here's a quick example of priming GPT to convert English to LaTeX:  ``` #: Construct GPT object and show some examples gpt = GPT(engine=""davinci""            temperature=0.5            max_tokens=100) gpt.add_example(Example('Two plus two equals four'  '2 + 2 = 4')) gpt.add_example(Example('The integral from zero to infinity'  '\\int_0^{\\infty}')) gpt.add_example(Example('The gradient of x squared plus two times x with respect to x'  '\\nabla_x x^2 + 2x')) gpt.add_example(Example('The log of two times x'  '\\log{2x}')) gpt.add_example(Example('x squared plus y squared plus equals z squared'  'x^2 + y^2 = z^2'))  #: Define UI configuration config = UIConfig(description=""Text to equation""                    button_text=""Translate""                    placeholder=""x squared plus 2 times x"")  demo_web_app(gpt  config) ```  Running this code as a python script would automatically launch a web app for you to test new inputs and outputs with. There are already 3 example scripts in the `examples` directory.  You can also prime GPT from the UI. for that  pass `show_example_form=True` to `UIConfig` along with other parameters.  Technical details: the backend is in Flask  and the frontend is in React. Note that this repository is currently not intended for production use.   """;General;https://github.com/shreyashankar/gpt3-sandbox
"""The goal of this project is to enable users to create cool web demos using the newly released OpenAI GPT-3 API **with just a few lines of Python.**   This project addresses the following issues:  1. Automatically formatting a user's inputs and outputs so that the model can effectively pattern-match 2. Creating a web app for a user to deploy locally and showcase their idea  Here's a quick example of priming GPT to convert English to LaTeX:  ``` #: Construct GPT object and show some examples gpt = GPT(engine=""davinci""            temperature=0.5            max_tokens=100) gpt.add_example(Example('Two plus two equals four'  '2 + 2 = 4')) gpt.add_example(Example('The integral from zero to infinity'  '\\int_0^{\\infty}')) gpt.add_example(Example('The gradient of x squared plus two times x with respect to x'  '\\nabla_x x^2 + 2x')) gpt.add_example(Example('The log of two times x'  '\\log{2x}')) gpt.add_example(Example('x squared plus y squared plus equals z squared'  'x^2 + y^2 = z^2'))  #: Define UI configuration config = UIConfig(description=""Text to equation""                    button_text=""Translate""                    placeholder=""x squared plus 2 times x"")  demo_web_app(gpt  config) ```  Running this code as a python script would automatically launch a web app for you to test new inputs and outputs with. There are already 3 example scripts in the `examples` directory.  You can also prime GPT from the UI. for that  pass `show_example_form=True` to `UIConfig` along with other parameters.  Technical details: the backend is in Flask  and the frontend is in React. Note that this repository is currently not intended for production use.   """;Natural Language Processing;https://github.com/shreyashankar/gpt3-sandbox
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/ZhihaoZhu/PointNet-Implementation-Tensorflow
"""This code tries to implement the Neural Turing Machine  as found in  https://arxiv.org/abs/1410.5401  as a backend neutral recurrent keras layer.  A very default experiment  the copy task  is provided  too.  In the end there is a TODO-List. Help would be appreciated!  NOTE: * There is a nicely formatted paper describing the rough idea of the NTM  implementation difficulties and which discusses the   copy experiment. It is available here in the repository as The_NTM_-_Introduction_And_Implementation.pdf.  * You may want to change the LOGDIR_BASE in testing_utils.py to something that works for you or just set a symbolic   link.    """;General;https://github.com/flomlo/ntm_keras
"""This code tries to implement the Neural Turing Machine  as found in  https://arxiv.org/abs/1410.5401  as a backend neutral recurrent keras layer.  A very default experiment  the copy task  is provided  too.  In the end there is a TODO-List. Help would be appreciated!  NOTE: * There is a nicely formatted paper describing the rough idea of the NTM  implementation difficulties and which discusses the   copy experiment. It is available here in the repository as The_NTM_-_Introduction_And_Implementation.pdf.  * You may want to change the LOGDIR_BASE in testing_utils.py to something that works for you or just set a symbolic   link.    """;Sequential;https://github.com/flomlo/ntm_keras
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Nstats/bert_senti_analysis_ch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Nstats/bert_senti_analysis_ch
"""**ELECTRA** is a method for self-supervised language representation learning. It can be used to pre-train transformer networks using relatively little compute. ELECTRA models are trained to distinguish ""real"" input tokens vs ""fake"" input tokens generated by another neural network  similar to the discriminator of a [GAN](https://arxiv.org/pdf/1406.2661.pdf). At small scale  ELECTRA achieves strong results even when trained on a single GPU. At large scale  ELECTRA achieves state-of-the-art results on the [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset.  For a detailed description and experimental results  please refer to our ICLR 2020 paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/pdf?id=r1xMH1BtvB).  This repository contains code to pre-train ELECTRA  including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g . [GLUE](https://gluebenchmark.com/))  QA tasks (e.g.  [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/))  and sequence tagging tasks (e.g.  [text chunking](https://www.clips.uantwerpen.be/conll2000/chunking/)).  This repository also contains code for **Electric**  a version of ELECTRA inspired by [energy-based models](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf). Electric provides a more principled view of ELECTRA as a ""negative sampling"" [cloze model](https://en.wikipedia.org/wiki/Cloze_test). It can also efficiently produce [pseudo-likelihood scores](https://arxiv.org/pdf/1910.14659.pdf) for text  which can be used to re-rank the outputs of speech recognition or machine translation systems. For details on Electric  please refer to out EMNLP 2020 paper [Pre-Training Transformers as Energy-Based Cloze Models](https://www.aclweb.org/anthology/2020.emnlp-main.20.pdf).     """;Computer Vision;https://github.com/google-research/electra
"""To practice what you've learned  a good idea would be to spend an hour on 3 of the following (3-hours total  you could through them all if you want) and then write a blog post about what you've learned.  * For an overview of the different problems within NLP and how to solve them read through:    * [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)   * [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e) * Go through [MIT's Recurrent Neural Networks lecture](https://youtu.be/SEnXr6v2ifU). This will be one of the greatest additions to what's happening behind the RNN model's you've been building. * Read through the [word embeddings page on the TensorFlow website](https://www.tensorflow.org/tutorials/text/word_embeddings). Embeddings are such a large part of NLP. We've covered them throughout this notebook but extra practice would be well worth it. A good exercise would be to write out all the code in the guide in a new notebook.  * For more on RNN's in TensorFlow  read and reproduce [the TensorFlow RNN guide](https://www.tensorflow.org/guide/keras/rnn). We've covered many of the concepts in this guide  but it's worth writing the code again for yourself. * Text data doesn't always come in a nice package like the data we've downloaded. So if you're after more on preparing different text sources for being with your TensorFlow deep learning models  it's worth checking out the following:   * [TensorFlow text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text).   * [Reading text files with Python](https://realpython.com/read-write-files-python/) by Real Python. * This notebook has focused on writing NLP code. For a mathematically rich overview of how NLP with Deep Learning happens  read [Standford's Natural Language Processing with Deep Learning lecture notes Part 1](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).     * For an even deeper dive  you could even do the whole [CS224n](http://web.stanford.edu/class/cs224n/) (Natural Language Processing with Deep Learning) course.  * Great blog posts to read:   * Andrei Karpathy's [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) dives into generating Shakespeare text with RNNs.   * [Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) by Mauro Di Pietro. An overview of different techniques for turning text into numbers and then classifying it.   * [What are word embeddings?](https://machinelearningmastery.com/what-are-word-embeddings/) by Machine Learning Mastery. * Other topics worth looking into:   * [Attention mechanisms](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/). These are a foundational component of the transformer architecture and also often add improvments to deep NLP models.   * [Transformer architectures](http://jalammar.github.io/illustrated-transformer/). This model architecture has recently taken the NLP world by storm  achieving state of the art on many benchmarks. However  it does take a little more processing to get off the ground  the [HuggingFace Models (formerly HuggingFace Transformers) library](https://huggingface.co/models/) is probably your best quick start.  ---   1. Rebuild  compile and train `model_1`  `model_2` and `model_5` using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) instead of the Functional API. 2. Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data? 3. Try fine-tuning the TF Hub Universal Sentence Encoder model by setting `training=True` when instantiating it as a Keras layer.  ``` #: We can use this encoding layer in place of our text_vectorizer and embedding layer sentence_encoder_layer = hub.KerasLayer(""https://tfhub.dev/google/universal-sentence-encoder/4""                                          input_shape=[]                                          dtype=tf.string                                          trainable=True) #: turn training on to fine-tune the TensorFlow Hub model ``` 4. Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the `sample_submission.csv` file from Kaggle (see the Files tab in Colab for what the `sample_submission.csv` file looks like). Once you've done this  [make a submission to the Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/data)  how did your model perform? 5. Combine the ensemble predictions using the majority vote (mode)  how does this perform compare to averaging the prediction probabilities of each model? 6. Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels.   """;General;https://github.com/joelweber97/Python3_TF_Certificate
"""To practice what you've learned  a good idea would be to spend an hour on 3 of the following (3-hours total  you could through them all if you want) and then write a blog post about what you've learned.  * For an overview of the different problems within NLP and how to solve them read through:    * [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)   * [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e) * Go through [MIT's Recurrent Neural Networks lecture](https://youtu.be/SEnXr6v2ifU). This will be one of the greatest additions to what's happening behind the RNN model's you've been building. * Read through the [word embeddings page on the TensorFlow website](https://www.tensorflow.org/tutorials/text/word_embeddings). Embeddings are such a large part of NLP. We've covered them throughout this notebook but extra practice would be well worth it. A good exercise would be to write out all the code in the guide in a new notebook.  * For more on RNN's in TensorFlow  read and reproduce [the TensorFlow RNN guide](https://www.tensorflow.org/guide/keras/rnn). We've covered many of the concepts in this guide  but it's worth writing the code again for yourself. * Text data doesn't always come in a nice package like the data we've downloaded. So if you're after more on preparing different text sources for being with your TensorFlow deep learning models  it's worth checking out the following:   * [TensorFlow text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text).   * [Reading text files with Python](https://realpython.com/read-write-files-python/) by Real Python. * This notebook has focused on writing NLP code. For a mathematically rich overview of how NLP with Deep Learning happens  read [Standford's Natural Language Processing with Deep Learning lecture notes Part 1](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).     * For an even deeper dive  you could even do the whole [CS224n](http://web.stanford.edu/class/cs224n/) (Natural Language Processing with Deep Learning) course.  * Great blog posts to read:   * Andrei Karpathy's [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) dives into generating Shakespeare text with RNNs.   * [Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) by Mauro Di Pietro. An overview of different techniques for turning text into numbers and then classifying it.   * [What are word embeddings?](https://machinelearningmastery.com/what-are-word-embeddings/) by Machine Learning Mastery. * Other topics worth looking into:   * [Attention mechanisms](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/). These are a foundational component of the transformer architecture and also often add improvments to deep NLP models.   * [Transformer architectures](http://jalammar.github.io/illustrated-transformer/). This model architecture has recently taken the NLP world by storm  achieving state of the art on many benchmarks. However  it does take a little more processing to get off the ground  the [HuggingFace Models (formerly HuggingFace Transformers) library](https://huggingface.co/models/) is probably your best quick start.  ---   1. Rebuild  compile and train `model_1`  `model_2` and `model_5` using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) instead of the Functional API. 2. Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data? 3. Try fine-tuning the TF Hub Universal Sentence Encoder model by setting `training=True` when instantiating it as a Keras layer.  ``` #: We can use this encoding layer in place of our text_vectorizer and embedding layer sentence_encoder_layer = hub.KerasLayer(""https://tfhub.dev/google/universal-sentence-encoder/4""                                          input_shape=[]                                          dtype=tf.string                                          trainable=True) #: turn training on to fine-tune the TensorFlow Hub model ``` 4. Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the `sample_submission.csv` file from Kaggle (see the Files tab in Colab for what the `sample_submission.csv` file looks like). Once you've done this  [make a submission to the Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/data)  how did your model perform? 5. Combine the ensemble predictions using the majority vote (mode)  how does this perform compare to averaging the prediction probabilities of each model? 6. Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels.   """;Natural Language Processing;https://github.com/joelweber97/Python3_TF_Certificate
"""This project show cases the use of Tensorflow JS  a JavaScript library released by Tensorflow  for Image Conversion using GANs (Pix2Pix architecture here).   """;Computer Vision;https://github.com/santhtadi/Colorize-Images-Pix2Pix-in-TensorflowJS
"""The ultimate goal of identifying and classifying signs within a larger image was broken into two parts. Use of a basic CNN and a Mask RCNN was used to tackle the challenge. When trying to classify signs using the GTSRB set alone  a CNN vastly outperformed the Mask RCNN  while the CNN would not be easily implemented on the GTSDB dataset. To accomplish the ultimate goal  a pipeline was made where a CNN was trained for the GTSRB dataset  Mask RCNN was used to identify and extract signs from teh GTSDB dataset  and thoes extracted signs were fed back into the trained net from the GTSRB.   [Download data and initial EDA](code/DATA_and_EDA.ipynb)  [Image Modification](code/Image_modification.ipynb)   """;Computer Vision;https://github.com/scolocke/Traffic_Sign_ID_GTSRB_GTSDB
"""Paper Link: [cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information](http://www.statnlp.org/wp-content/uploads/papers/2018/cw2vec/cw2vec.pdf)    Paper Detail Summary: [cw2vec理论及其实现](https://bamtercelboo.github.io/2018/05/11/cw2vec/)   """;Natural Language Processing;https://github.com/bamtercelboo/cw2vec
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/guanfuchen/Deformable-ConvNets
"""26684 [DICOM](https://es.wikipedia.org/wiki/DICOM) files. (3.53 GB)       - stage_2_train_labels.csv (26684  6) (1.49 MB)         - Patient Id: nominal variable. File names correspond to patient Id.         - X and Y: the center of the boxes for the segmentation part.         - height and width: the dimension of the box for the segmentation part.         - target: binary. 0-normal and 1-pneumonia.              - stage_2_detailed_class_info.csv (26684  2) (1.69 MB)         - Patient Id: nominal variable. File names correspond to patient Id.         - Class: nominal variable with three possible values:             - 'Normal'             - 'Not normal/Not pneumonia'             - 'Pneumonia'   """;Computer Vision;https://github.com/EiroaMD/final_project_IH_pneumonia
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   """;Computer Vision;https://github.com/rh01/fast-rnn
"""This project turns edge devices such as Raspberry Pi into an intelligent gateway with deep learning running on it. No internet connection is required  everything is done locally on the edge device itself. Further  multiple edge devices can create a distributed AIoT network.  At DT42  we believe that bringing deep learning to edge devices is the trend towards the future. It not only saves costs of data transmission and storage but also makes devices able to respond according to the events shown in the images or videos without connecting to the cloud.  ![Figure 1](https://user-images.githubusercontent.com/292790/45943626-a3d28b80-c019-11e8-829c-5eb6afd3faa4.png)  <p align=""center"">Figure 1: BerryNet architecture</p>  Figure 1 shows the software architecture of the project  we use Node.js/Python  MQTT and an AI engine to analyze images or video frames with deep learning. So far  there are two default types of AI engines  the classification engine (with Inception v3 [[1]](https://arxiv.org/pdf/1512.00567.pdf) model) and the object detection engine (with TinyYOLO [[2]](https://pjreddie.com/media/files/papers/YOLO9000.pdf) model or MobileNet SSD [[3]](https://arxiv.org/pdf/1704.04861.pdf) model). Figure 2 shows the differences between classification and object detection.  ![Figure 2](https://cloud.githubusercontent.com/assets/292790/25520013/d9497738-2c2c-11e7-9693-3840647f2e1e.jpg)  <p align=""center"">Figure 2: Classification vs detection</p>  One of the application of this intelligent gateway is to use the camera to monitor the place you care about. For example  Figure 3 shows the analyzed results from the camera hosted in the DT42 office. The frames were captured by the IP camera and they were submitted into the AI engine. The output from the AI engine will be shown in the dashboard. We are working on the Email and IM notification so you can get a notification when there is a dog coming into the meeting area with the next release.  ![Figure 3](https://cloud.githubusercontent.com/assets/292790/25498294/0ab79976-2bba-11e7-9114-46e328d15a18.gif)  <p align=""center"">Figure 3: Object detection result example</p>  To bring easy and flexible edge AI experience to user  we keep expending support of the AI engines and the reference HWs.  ![Figure 4](https://user-images.githubusercontent.com/292790/64026655-c2b69780-cb71-11e9-90b9-6269319012f1.png)  <p align=""center"">Figure 4: Reference hardwares</p>    """;General;https://github.com/DT42/BerryNet
"""This project turns edge devices such as Raspberry Pi into an intelligent gateway with deep learning running on it. No internet connection is required  everything is done locally on the edge device itself. Further  multiple edge devices can create a distributed AIoT network.  At DT42  we believe that bringing deep learning to edge devices is the trend towards the future. It not only saves costs of data transmission and storage but also makes devices able to respond according to the events shown in the images or videos without connecting to the cloud.  ![Figure 1](https://user-images.githubusercontent.com/292790/45943626-a3d28b80-c019-11e8-829c-5eb6afd3faa4.png)  <p align=""center"">Figure 1: BerryNet architecture</p>  Figure 1 shows the software architecture of the project  we use Node.js/Python  MQTT and an AI engine to analyze images or video frames with deep learning. So far  there are two default types of AI engines  the classification engine (with Inception v3 [[1]](https://arxiv.org/pdf/1512.00567.pdf) model) and the object detection engine (with TinyYOLO [[2]](https://pjreddie.com/media/files/papers/YOLO9000.pdf) model or MobileNet SSD [[3]](https://arxiv.org/pdf/1704.04861.pdf) model). Figure 2 shows the differences between classification and object detection.  ![Figure 2](https://cloud.githubusercontent.com/assets/292790/25520013/d9497738-2c2c-11e7-9693-3840647f2e1e.jpg)  <p align=""center"">Figure 2: Classification vs detection</p>  One of the application of this intelligent gateway is to use the camera to monitor the place you care about. For example  Figure 3 shows the analyzed results from the camera hosted in the DT42 office. The frames were captured by the IP camera and they were submitted into the AI engine. The output from the AI engine will be shown in the dashboard. We are working on the Email and IM notification so you can get a notification when there is a dog coming into the meeting area with the next release.  ![Figure 3](https://cloud.githubusercontent.com/assets/292790/25498294/0ab79976-2bba-11e7-9114-46e328d15a18.gif)  <p align=""center"">Figure 3: Object detection result example</p>  To bring easy and flexible edge AI experience to user  we keep expending support of the AI engines and the reference HWs.  ![Figure 4](https://user-images.githubusercontent.com/292790/64026655-c2b69780-cb71-11e9-90b9-6269319012f1.png)  <p align=""center"">Figure 4: Reference hardwares</p>    """;Computer Vision;https://github.com/DT42/BerryNet
"""This project is a tensorflow implementation of the impressive work  [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/pdf/1609.04802.pdf). <br /> The result is obtained following to same setting from the v5 edition of the [paper on arxiv](https://arxiv.org/pdf/1609.04802.pdf). However  due to limited resources  I train my network on the [RAISE dataset](http://mmlab.science.unitn.it/RAISE/) which contains 8156 high resoution images captured by good cameras. As the results showed below  the performance is close to the result presented in the paper without using the imagenet training set. <br /> The result on BSD100  Set14  Set5 will be reported later. The code is highly inspired by the [pix2pix-tensorflow](https://github.com/affinelayer/pix2pix-tensorflow).   """;Computer Vision;https://github.com/brade31919/SRGAN-tensorflow
"""This project is a tensorflow implementation of the impressive work  [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/pdf/1609.04802.pdf). <br /> The result is obtained following to same setting from the v5 edition of the [paper on arxiv](https://arxiv.org/pdf/1609.04802.pdf). However  due to limited resources  I train my network on the [RAISE dataset](http://mmlab.science.unitn.it/RAISE/) which contains 8156 high resoution images captured by good cameras. As the results showed below  the performance is close to the result presented in the paper without using the imagenet training set. <br /> The result on BSD100  Set14  Set5 will be reported later. The code is highly inspired by the [pix2pix-tensorflow](https://github.com/affinelayer/pix2pix-tensorflow).   """;General;https://github.com/brade31919/SRGAN-tensorflow
"""We introduce a new CLEVR Baseline based on FiLM layers and Conditional Batch Normalization technique.   * [Introduction](#introduction) * [Installation](#installation)     * [Download](#Download)     * [Requirements](#requirements)     * [File architecture](#file-architecture)     * [Data](#data)     * [Pretrained models](#pretrained-models) * [Reproducing results](#reproducing-results)     * [Process Data](#data)     * [Train Model](#train-model) * [FAQ](#faq) * [Citation](#citation)   """;Computer Vision;https://github.com/GuessWhatGame/clevr
"""We introduce a new CLEVR Baseline based on FiLM layers and Conditional Batch Normalization technique.   * [Introduction](#introduction) * [Installation](#installation)     * [Download](#Download)     * [Requirements](#requirements)     * [File architecture](#file-architecture)     * [Data](#data)     * [Pretrained models](#pretrained-models) * [Reproducing results](#reproducing-results)     * [Process Data](#data)     * [Train Model](#train-model) * [FAQ](#faq) * [Citation](#citation)   """;General;https://github.com/GuessWhatGame/clevr
"""The demos in this folder are designed to give straightforward samples of using TensorFlow in mobile applications.  Inference is done using the [TensorFlow Android Inference Interface](../../../tensorflow/contrib/android)  which may be built separately if you want a standalone library to drop into your existing application. Object tracking and YUV -> RGB conversion is handled by libtensorflow_demo.so.  A device running Android 5.0 (API 21) or higher is required to run the demo due to the use of the camera2 API  although the native libraries themselves can run on API >= 14 devices.   """;Computer Vision;https://github.com/Adren98/EczemaApp
"""This package provides a simple way to boost the performance of image captioning task by reranking the hypotheses sentences according to the captions of nearest neighbor images in the training set. We denote this method as consensus reranking for the rest of this document.  It also provides images features (both refined by the [m-RNN model](www.stat.ucla.edu/~junhua.mao/m-RNN.html) and the original [VGG feature](http://arxiv.org/abs/1409.1556) ) on MS COCO Train2014  Val2014  and Test2014 dataset. We take the [m-RNN model](www.stat.ucla.edu/~junhua.mao/m-RNN.html) as an example to generate the hypotheses descriptions of an image.  The details is described in Section 8 of the latest version of the m-RNN paper: [Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)](http://arxiv.org/abs/1412.6632). The work is inspired by the [nearest neighbor captions retrieval method](http://arxiv.org/abs/1505.04467).   """;Computer Vision;https://github.com/mjhucla/mRNN-CR
"""In cytogenetics  experiments typically starts from chromosomal preparations fixed on glass slides. Occasionally a chromosome can fall on another one  yielding overlapping chromosomes in the image. Before computers and images processing with photography  chromosomes were cut from a paper picture and then classified (at least two paper pictures were required when chromosomes are overlapping). More recently  automatic segmentation methods were developed to overcome this problem. Most of the time these methods rely on a geometric analysis of the chromosome contour and require some human intervention when partial overlap occurs. Modern deep learning techniques have the potential to provide a more reliable  fully-automated solution.   """;Computer Vision;https://github.com/LilyHu/image_segmentation_chromosomes
"""The purpose of the project is to generate human faces that do not exist in the training set. Four models are compared: An autoencoder (AE)  a variational autoencoder (VAE)  a Deep Convolutional Generative Adversarial Network (DC-GAN)  and a Variational Autoencoder Generative Adversarial Network (VAE-GAN). All models are trained on a processed version of the publicly available LFW face dataset.  Our motivation is to learn about different face generation architectures since there are many applications related to face generation  including police sketching and data augmentation. We are also interested in assessing the effects of GANs on face generation. We would like to determine if GANs create sharper reconstructions and whether a VAE-GAN has more control over generations due to the restrictions on its latent space [1].  Considering the complexity of the task  machine learning is an appropriate tool for it; face generation is a task that does not lend itself to rules-based systems. Unlike traditional algorithms  machine learning algorithms can learn from large amounts of input data and create new data based on the structure of existing data [2].   """;Computer Vision;https://github.com/OsvaldN/APS360_Project
"""The purpose of the project is to generate human faces that do not exist in the training set. Four models are compared: An autoencoder (AE)  a variational autoencoder (VAE)  a Deep Convolutional Generative Adversarial Network (DC-GAN)  and a Variational Autoencoder Generative Adversarial Network (VAE-GAN). All models are trained on a processed version of the publicly available LFW face dataset.  Our motivation is to learn about different face generation architectures since there are many applications related to face generation  including police sketching and data augmentation. We are also interested in assessing the effects of GANs on face generation. We would like to determine if GANs create sharper reconstructions and whether a VAE-GAN has more control over generations due to the restrictions on its latent space [1].  Considering the complexity of the task  machine learning is an appropriate tool for it; face generation is a task that does not lend itself to rules-based systems. Unlike traditional algorithms  machine learning algorithms can learn from large amounts of input data and create new data based on the structure of existing data [2].   """;General;https://github.com/OsvaldN/APS360_Project
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/YYGXjpg/BERT_WL
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/YYGXjpg/BERT_WL
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/larsoncs/face_detect
"""This project contains the code (Note: The code is test in the environment with python=3.6  cuda=9.0  PyTorch-0.4.1  also support Pytorch-0.4.1+) for:  [**LEDNet: A Lightweight Encoder-Decoder Network for Real-time Semantic Segmentation**](https://arxiv.org/pdf/1905.02423.pdf)  by [Yu Wang](https://github.com/xiaoyufenfei).  <p align=""center""><img width=""100%"" src=""./images/LEDNet_overview.png"" /></p>  The extensive computational burden limits the usage of CNNs in mobile devices for dense estimation tasks  a.k.a semantic segmentation. In this paper  we present a lightweight network to address this problem  namely **LEDNet**  which employs an asymmetric encoder-decoder architecture for the task of real-time semantic segmentation.More specifically  the encoder adopts a ResNet as backbone network  where two new operations  channel split and shuffle  are utilized in each residual block to greatly reduce computation cost while maintaining higher segmentation accuracy. On the other hand  an attention pyramid network (APN) is employed in the decoder to further lighten the entire network complexity. Our model has less than 1M parameters  and is able to run at over 71 FPS on a single GTX 1080Ti GPU card. The comprehensive experiments demonstrate that our approach achieves state-of-the-art results in terms of speed and accuracy trade-off on Cityscapes dataset. and becomes an effective method for real-time semantic segmentation tasks.   """;General;https://github.com/alalagong/LEDNet
"""This project contains the code (Note: The code is test in the environment with python=3.6  cuda=9.0  PyTorch-0.4.1  also support Pytorch-0.4.1+) for:  [**LEDNet: A Lightweight Encoder-Decoder Network for Real-time Semantic Segmentation**](https://arxiv.org/pdf/1905.02423.pdf)  by [Yu Wang](https://github.com/xiaoyufenfei).  <p align=""center""><img width=""100%"" src=""./images/LEDNet_overview.png"" /></p>  The extensive computational burden limits the usage of CNNs in mobile devices for dense estimation tasks  a.k.a semantic segmentation. In this paper  we present a lightweight network to address this problem  namely **LEDNet**  which employs an asymmetric encoder-decoder architecture for the task of real-time semantic segmentation.More specifically  the encoder adopts a ResNet as backbone network  where two new operations  channel split and shuffle  are utilized in each residual block to greatly reduce computation cost while maintaining higher segmentation accuracy. On the other hand  an attention pyramid network (APN) is employed in the decoder to further lighten the entire network complexity. Our model has less than 1M parameters  and is able to run at over 71 FPS on a single GTX 1080Ti GPU card. The comprehensive experiments demonstrate that our approach achieves state-of-the-art results in terms of speed and accuracy trade-off on Cityscapes dataset. and becomes an effective method for real-time semantic segmentation tasks.   """;Computer Vision;https://github.com/alalagong/LEDNet
"""vedaseg is an open source semantic segmentation toolbox based on PyTorch.   """;Computer Vision;https://github.com/Media-Smart/vedaseg
"""Convolutional neural networks ahve become famous in cmoputer vision ever since *AlexNet* popularized deep convolutional neural networks by winning IamgeNet Challenge: ILSVRC 2012. The general trend has been to make deeper and more complicated networks in roder to achieve higher accuracy. However  these advances to improve accuracy are not necessarily making networks more efficient with respect to size and spped. In many real world applications such as robotics  self-driving  the recognition tasks need to be carried out in a timely fashion on a computationally limited platform. That's inspire me to build some effient convolutional neural networks which can be used on Mobile/portable device.    """;Computer Vision;https://github.com/forcefulowl/image_classification
"""it has all kinds of baseline models for text classification.  it also support for multi-label classification where multi labels associate with an sentence or document.  although many of these models are simple  and may not get you to top level of the task. but some of these models are very   classic  so they may be good to serve as baseline models. each model has a test function under model class. you can run   it to performance toy task first. the model is independent from data set.  <a href='https://github.com/brightmart/text_classification/blob/master/multi-label-classification.pdf'>check here for formal report of large scale multi-label text classification with deep learning</a>  several models here can also be used for modelling question answering (with or without context)  or to do sequences generating.   we explore two seq2seq model(seq2seq with attention transformer-attention is all you need) to do text classification.   and these two models can also be used for sequences generating and other tasks. if your task is a multi-label classification    you can cast the problem to sequences generating.  we implement two memory network. one is dynamic memory network. previously it reached state of art in question   answering  sentiment analysis and sequence generating tasks. it is so called one model to do several different tasks    and reach high performance. it has four modules. the key component is episodic memory module. it use gate mechanism to   performance attention  and use gated-gru to update episode memory  then it has another gru( in a vertical direction) to   performance hidden state update. it has ability to do transitive inference.  the second memory network we implemented is recurrent entity network: tracking state of the world. it has blocks of   key-value pairs as memory  run in parallel  which achieve new state of art. it can be used for modelling question   answering with contexts(or history). for example  you can let the model to read some sentences(as context)  and ask a   question(as query)  then ask the model to predict an answer; if you feed story same as query  then it can do   classification task.   To discuss ML/DL/NLP problems and get tech support from each other  you can join QQ group: 836811304  Models: -------------------------------------------------------------------------  1) fastText 2) TextCNN  3) Bert:Pre-training of Deep Bidirectional Transformers for Language Understanding   4) TextRNN     5) RCNN      6) Hierarchical Attention Network     7) seq2seq with attention    8) Transformer(""Attend Is All You Need"") 9) Dynamic Memory Network 10) EntityNetwork:tracking state of the world 11) Ensemble models 12) Boosting:       for a single model  stack identical models together. each layer is a model. the result will be based on logits added together. the only connection between layers are label's weights. the front layer's prediction error rate of each label will become weight for the next layers. those labels with high error rate will have big weight. so later layer's will pay more attention to those mis-predicted labels  and try to fix previous mistake of former layer. as a result  we will get a much strong model.     check a00_boosting/boosting.py  and other models:  1) BiLstmTextRelation;  2) twoCNNTextRelation;  3) BiLstmTextRelationTwoRNN  Performance -------------------------------------------------------------------------  (mulit-label label prediction task ask to prediction top5  3 million training data full score:0.5)  Model   | fastText|TextCNN|TextRNN| RCNN | HierAtteNet|Seq2seqAttn|EntityNet|DynamicMemory|Transformer ---     | ---     | ---   | ---   |---   |---         |---        |---      |---          |---- Score   | 0.362   |  0.405| 0.358 | 0.395| 0.398      |0.322      |0.400    |0.392        |0.322 Training| 10m     |  2h   |10h    | 2h   | 2h         |3h         |3h       |5h           |7h --------------------------------------------------------------------------------------------------    Bert model achieves 0.368 after first 9 epoch from validation set.    Ensemble of TextCNN EntityNet DynamicMemory: 0.411    Ensemble EntityNet DynamicMemory: 0.403       --------------------------------------------------------------------------------------------------    Notice:     `m` stand for **minutes**; `h` stand for **hours**;   `HierAtteNet` means Hierarchical Attention Networkk;  `Seq2seqAttn` means Seq2seq with attention;  `DynamicMemory` means DynamicMemoryNetwork;  `Transformer` stand for model from 'Attention Is All You Need'.  Usage: ------------------------------------------------------------------------------------------------------- 1) model is in `xxx_model.py` 2) run python `xxx_train.py` to train the model 3) run python `xxx_predict.py` to do inference(test).  Each model has a test method under the model class. you can run the test method first to check whether the model can work properly.  -------------------------------------------------------------------------  Environment: ------------------------------------------------------------------------------------------------------- python 2.7+ tensorflow 1.8   (tensorflow 1.1 to 1.13 should also works; most of models should also work fine in other tensorflow version  since we   use very few features bond to certain version.  if you use python3  it will be fine as long as you change print/try catch function in case you meet any error.  TextCNN model is already transfomed to python 3.6   Sample data: <a href='https://pan.baidu.com/s/1yWZf2eAPxq15-r2hHk2M-Q'>cached file of baidu</a> or <a href=""https://drive.google.com/drive/folders/0AKEuT4gza2AlUk9PVA"">Google Drive:</a>send me an email ------------------------------------------------------------------------------------------------------- to help you run this repository  currently we re-generate training/validation/test data and vocabulary/labels  and saved   them as cache file using h5py. we suggest you to download it from above link.  it contain everything you need to run this repository: data is pre-processed  you can start to train the model in a minute.    it's a zip file about 1.8G  contains 3 million training data. although after unzip it's quite big  but with the help of   hdf5  it only need a normal size of memory of computer(e.g.8 G or less) during training.  we use jupyter notebook: <a href='https://github.com/brightmart/text_classification/blob/master/pre-processing.ipynb'>pre-processing.ipynb</a> to pre-process data. you can have a better understanding of this task and   data by taking a look of it. you can also generate data by yourself in the way your want  just change few lines of code   using this jupyter notebook.  If you want to try a model now  you can dowload cached file from above  then go to folder 'a02_TextCNN'  run                python  p7_TextCNN_train.py      it will use data from cached files to train the model  and print loss and F1 score periodically.  old sample data source: if you need some sample data and word embedding per-trained on word2vec  you can find it in closed issues  such as: <a href=""https://github.com/brightmart/text_classification/issues/3"">issue 3</a>.   you can also find some sample data at folder ""data"". it contains two files:'sample_single_label.txt'  contains 50k data   with single label; 'sample_multiple_label.txt'  contains 20k data with multiple labels. input and label of is separate by ""   __label__"".  if you want to know more detail about data set of text classification or task these models can be used  one of choose is below:  https://biendata.com/competition/zhihu/  Road Map ------------------------------------------------------------------------------------------------------- One way you can use this repository:   step 1: you can read through this article. you will get a general idea of various classic models used to do text classification.  step 2: pre-process data and/or download cached file.        a. take a look a look of jupyter notebook('pre-processing.ipynb')  where you can familiar with this text              classification task and data set. you will also know how we pre-process data and generate training/validation/test                         set. there are a list of things you can try at the end of this jupyter.         b. download zip file that contains cached files  so you will have all necessary data  and can start to train models.  step 3: run some of models list here  and change some codes and configurations as you want  to get a good performance.        record performances  and things you done that works  and things that are not.        for example  you can take this sequence to explore:               1) fasttext---> 2)TextCNN---> 3)Transformer---> 4)BERT  additionally  write your article about this topic  you can follow paper's style to write. you may need to read some papers                on the way  many of these papers list in the  """;General;https://github.com/brightmart/text_classification
"""it has all kinds of baseline models for text classification.  it also support for multi-label classification where multi labels associate with an sentence or document.  although many of these models are simple  and may not get you to top level of the task. but some of these models are very   classic  so they may be good to serve as baseline models. each model has a test function under model class. you can run   it to performance toy task first. the model is independent from data set.  <a href='https://github.com/brightmart/text_classification/blob/master/multi-label-classification.pdf'>check here for formal report of large scale multi-label text classification with deep learning</a>  several models here can also be used for modelling question answering (with or without context)  or to do sequences generating.   we explore two seq2seq model(seq2seq with attention transformer-attention is all you need) to do text classification.   and these two models can also be used for sequences generating and other tasks. if your task is a multi-label classification    you can cast the problem to sequences generating.  we implement two memory network. one is dynamic memory network. previously it reached state of art in question   answering  sentiment analysis and sequence generating tasks. it is so called one model to do several different tasks    and reach high performance. it has four modules. the key component is episodic memory module. it use gate mechanism to   performance attention  and use gated-gru to update episode memory  then it has another gru( in a vertical direction) to   performance hidden state update. it has ability to do transitive inference.  the second memory network we implemented is recurrent entity network: tracking state of the world. it has blocks of   key-value pairs as memory  run in parallel  which achieve new state of art. it can be used for modelling question   answering with contexts(or history). for example  you can let the model to read some sentences(as context)  and ask a   question(as query)  then ask the model to predict an answer; if you feed story same as query  then it can do   classification task.   To discuss ML/DL/NLP problems and get tech support from each other  you can join QQ group: 836811304  Models: -------------------------------------------------------------------------  1) fastText 2) TextCNN  3) Bert:Pre-training of Deep Bidirectional Transformers for Language Understanding   4) TextRNN     5) RCNN      6) Hierarchical Attention Network     7) seq2seq with attention    8) Transformer(""Attend Is All You Need"") 9) Dynamic Memory Network 10) EntityNetwork:tracking state of the world 11) Ensemble models 12) Boosting:       for a single model  stack identical models together. each layer is a model. the result will be based on logits added together. the only connection between layers are label's weights. the front layer's prediction error rate of each label will become weight for the next layers. those labels with high error rate will have big weight. so later layer's will pay more attention to those mis-predicted labels  and try to fix previous mistake of former layer. as a result  we will get a much strong model.     check a00_boosting/boosting.py  and other models:  1) BiLstmTextRelation;  2) twoCNNTextRelation;  3) BiLstmTextRelationTwoRNN  Performance -------------------------------------------------------------------------  (mulit-label label prediction task ask to prediction top5  3 million training data full score:0.5)  Model   | fastText|TextCNN|TextRNN| RCNN | HierAtteNet|Seq2seqAttn|EntityNet|DynamicMemory|Transformer ---     | ---     | ---   | ---   |---   |---         |---        |---      |---          |---- Score   | 0.362   |  0.405| 0.358 | 0.395| 0.398      |0.322      |0.400    |0.392        |0.322 Training| 10m     |  2h   |10h    | 2h   | 2h         |3h         |3h       |5h           |7h --------------------------------------------------------------------------------------------------    Bert model achieves 0.368 after first 9 epoch from validation set.    Ensemble of TextCNN EntityNet DynamicMemory: 0.411    Ensemble EntityNet DynamicMemory: 0.403       --------------------------------------------------------------------------------------------------    Notice:     `m` stand for **minutes**; `h` stand for **hours**;   `HierAtteNet` means Hierarchical Attention Networkk;  `Seq2seqAttn` means Seq2seq with attention;  `DynamicMemory` means DynamicMemoryNetwork;  `Transformer` stand for model from 'Attention Is All You Need'.  Usage: ------------------------------------------------------------------------------------------------------- 1) model is in `xxx_model.py` 2) run python `xxx_train.py` to train the model 3) run python `xxx_predict.py` to do inference(test).  Each model has a test method under the model class. you can run the test method first to check whether the model can work properly.  -------------------------------------------------------------------------  Environment: ------------------------------------------------------------------------------------------------------- python 2.7+ tensorflow 1.8   (tensorflow 1.1 to 1.13 should also works; most of models should also work fine in other tensorflow version  since we   use very few features bond to certain version.  if you use python3  it will be fine as long as you change print/try catch function in case you meet any error.  TextCNN model is already transfomed to python 3.6   Sample data: <a href='https://pan.baidu.com/s/1yWZf2eAPxq15-r2hHk2M-Q'>cached file of baidu</a> or <a href=""https://drive.google.com/drive/folders/0AKEuT4gza2AlUk9PVA"">Google Drive:</a>send me an email ------------------------------------------------------------------------------------------------------- to help you run this repository  currently we re-generate training/validation/test data and vocabulary/labels  and saved   them as cache file using h5py. we suggest you to download it from above link.  it contain everything you need to run this repository: data is pre-processed  you can start to train the model in a minute.    it's a zip file about 1.8G  contains 3 million training data. although after unzip it's quite big  but with the help of   hdf5  it only need a normal size of memory of computer(e.g.8 G or less) during training.  we use jupyter notebook: <a href='https://github.com/brightmart/text_classification/blob/master/pre-processing.ipynb'>pre-processing.ipynb</a> to pre-process data. you can have a better understanding of this task and   data by taking a look of it. you can also generate data by yourself in the way your want  just change few lines of code   using this jupyter notebook.  If you want to try a model now  you can dowload cached file from above  then go to folder 'a02_TextCNN'  run                python  p7_TextCNN_train.py      it will use data from cached files to train the model  and print loss and F1 score periodically.  old sample data source: if you need some sample data and word embedding per-trained on word2vec  you can find it in closed issues  such as: <a href=""https://github.com/brightmart/text_classification/issues/3"">issue 3</a>.   you can also find some sample data at folder ""data"". it contains two files:'sample_single_label.txt'  contains 50k data   with single label; 'sample_multiple_label.txt'  contains 20k data with multiple labels. input and label of is separate by ""   __label__"".  if you want to know more detail about data set of text classification or task these models can be used  one of choose is below:  https://biendata.com/competition/zhihu/  Road Map ------------------------------------------------------------------------------------------------------- One way you can use this repository:   step 1: you can read through this article. you will get a general idea of various classic models used to do text classification.  step 2: pre-process data and/or download cached file.        a. take a look a look of jupyter notebook('pre-processing.ipynb')  where you can familiar with this text              classification task and data set. you will also know how we pre-process data and generate training/validation/test                         set. there are a list of things you can try at the end of this jupyter.         b. download zip file that contains cached files  so you will have all necessary data  and can start to train models.  step 3: run some of models list here  and change some codes and configurations as you want  to get a good performance.        record performances  and things you done that works  and things that are not.        for example  you can take this sequence to explore:               1) fasttext---> 2)TextCNN---> 3)Transformer---> 4)BERT  additionally  write your article about this topic  you can follow paper's style to write. you may need to read some papers                on the way  many of these papers list in the  """;Natural Language Processing;https://github.com/brightmart/text_classification
"""The demos in this folder are designed to give straightforward samples of using TensorFlow in mobile applications.  Inference is done using the [TensorFlow Android Inference Interface](../../../tensorflow/contrib/android)  which may be built separately if you want a standalone library to drop into your existing application. Object tracking and YUV -> RGB conversion is handled by libtensorflow_demo.so.  A device running Android 5.0 (API 21) or higher is required to run the demo due to the use of the camera2 API  although the native libraries themselves can run on API >= 14 devices.   """;General;https://github.com/Adren98/EczemaApp
"""Nowadays in computer vision  deep learning approach is performing superior result and even better than human. I decided to use deep learning approach to solve computer vision challenge in Grab AI For Sea. There is already some [published kernels in Kaggle](https://www.kaggle.com/jutrera/stanford-car-dataset-by-classes-folder/kernels) which I have referred to [their approach](https://www.kaggle.com/deepbear/pytorch-car-classifier-90-accuracy) as my starting point. I have made some changes on training scheme and network architecture. My approach of training scheme is better than baseline from the Kaggle kernel by 0.27% while performing another two tasks. Using state-of-the-art achitecture  performance is improved by 1.66%. I have also shown that not only we need to focus on performance  but also focus on size and computational power  I switched to lower resolution and state-of-the-art deep architecture for mobile  I managed to create a model that is efficient in terms of performance and computational power.    """;Computer Vision;https://github.com/kamwoh/Car-Model-Classification
"""FastText embeds words by adding word's n-grams to the word embedding and then normalizes by total token count i.e. <b>fastText(word)<sub></sub> = (v<sub>word</sub> + &Sigma;<sub>g &isin; ngrams(word)</sub>v<sub>g</sub>) / (1 + |ngrams(word)|)</b>. However if the word is not present in the dictionary (OOV) only n-grams are used i.e. <b>(&Sigma;<sub>g &isin; ngrams(word)</sub>v<sub>g</sub>) / |ngrams(word)|</b>. For purpose of studying OOV words this asymmetry between vocabulary and out of vocabulary words is removed by only utilizing word's n-grams regardless if the word is OOV or not.  In order to study contrast between common english words e.g. ""apple"" and noise-words (usually some parsing artifacts or unusual tokens with very specific meaning) e.g. ""wales-2708"" or ""G705"" [MIT 10K Common words dataset is used](https://www.mit.edu/~ecprice/wordlist.10000).  Entire code for this post in available in [this repository in file ""main.py""](https://github.com/vackosar/fasttext-vector-norms-and-oov-words/blob/master/main.py). FastText model used is [5-gram English 2M ""cc.en.300.bin""](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz).    """;Natural Language Processing;https://github.com/vackosar/fasttext-vector-norms-and-oov-words
"""This plugin is meant to make it easy to replace the background in portrait images and video. It is using a neural network to predict the mask of the portrait and remove the background pixels. It's easily composable with other OBS plugins to replace the background with e.g. an image or a transparent color.  ![](demo.gif)  The models used for background detection are SINet: https://arxiv.org/abs/1911.09099 and MODNet: https://arxiv.org/pdf/2011.11961.pdf The pre-trained model weights were taken from: - https://github.com/anilsathyan7/Portrait-Segmentation/tree/master/SINet - https://github.com/ZHKKKe/MODNet  Some more information about how I built it: https://www.morethantechnical.com/2021/04/15/obs-plugin-for-portrait-background-removal-with-onnx-sinet-model/   """;Computer Vision;https://github.com/royshil/obs-backgroundremoval
"""Given an image of a person’s face  the task of classifying the ID of the face is known as **face classification**. Whereas the problem of determining whether two face images are of the same person is known as **face verification** and this has several important applications. This mini-project uses convolutional neural networks (CNNs) to design an end-to-end system for face classification and face verification.   """;Computer Vision;https://github.com/anjandeepsahni/face_classification
"""Given an image of a person’s face  the task of classifying the ID of the face is known as **face classification**. Whereas the problem of determining whether two face images are of the same person is known as **face verification** and this has several important applications. This mini-project uses convolutional neural networks (CNNs) to design an end-to-end system for face classification and face verification.   """;General;https://github.com/anjandeepsahni/face_classification
"""This is an implementation of submitted paper:   Desai  S. V.  Balasubramanian  V. N.  Fukatsu  T.  Ninomiya  S.  & Guo  W. (2019). Automatic estimation of heading date of paddy rice using deep learning. Plant Methods  15(1)  76. https://doi.org/10.1186/s13007-019-0457-1   To plan the perfect time for harvest of rice crops  we detect and quantify the flowering of paddy rice. The dataset of rice crop images used in this work is taken from [1].   """;Computer Vision;https://github.com/svdesai/heading-date-estimation
"""The implementation is based on two papers:  - Simple Online and Realtime Tracking with a Deep Association Metric https://arxiv.org/abs/1703.07402 - YOLOv4: Optimal Speed and Accuracy of Object Detection https://arxiv.org/pdf/2004.10934.pdf   This repository contains a moded version of PyTorch YOLOv5 (https://github.com/ultralytics/yolov5).   It filters out every detection that is not a person. The detections of persons are then passed to a Deep Sort algorithm (https://github.com/ZQPei/deep_sort_pytorch) which tracks the persons. The reason behind the fact that it just tracks persons is that the deep association metric is trained on a person ONLY datatset.   """;Computer Vision;https://github.com/RunzhaoHuang/DeepSort_YOLOV5_OnScreen
"""BERT-base and BERT-large are respectively 110M and 340M parameters models and it can be difficult to fine-tune them on a single GPU with the recommended batch size for good performance (in most case a batch size of 32).  To help with fine-tuning these models  we have included several techniques that you can activate in the fine-tuning scripts [`run_classifier.py`](./examples/run_classifier.py) and [`run_squad.py`](./examples/run_squad.py): gradient-accumulation  multi-gpu training  distributed training and 16-bits training . For more details on how to use these techniques you can read [the tips on training large batches in PyTorch](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255) that I published earlier this month.  Here is how to use these techniques in our scripts:  - **Gradient Accumulation**: Gradient accumulation can be used by supplying a integer greater than 1 to the `--gradient_accumulation_steps` argument. The batch at each step will be divided by this integer and gradient will be accumulated over `gradient_accumulation_steps` steps. - **Multi-GPU**: Multi-GPU is automatically activated when several GPUs are detected and the batches are splitted over the GPUs. - **Distributed training**: Distributed training can be activated by supplying an integer greater or equal to 0 to the `--local_rank` argument (see below). - **16-bits training**: 16-bits training  also called mixed-precision training  can reduce the memory requirement of your model on the GPU by using half-precision training  basically allowing to double the batch size. If you have a recent GPU (starting from NVIDIA Volta architecture) you should see no decrease in speed. A good introduction to Mixed precision training can be found [here](https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/) and a full documentation is [here](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html). In our scripts  this option can be activated by setting the `--fp16` flag and you can play with loss scaling using the `--loss_scale` flag (see the previously linked documentation for details on loss scaling). The loss scale can be zero in which case the scale is dynamically adjusted or a positive power of two in which case the scaling is static.  To use 16-bits training and distributed training  you need to install NVIDIA's apex extension [as detailed here](https://github.com/nvidia/apex). You will find more information regarding the internals of `apex` and how to use `apex` in [the doc and the associated repository](https://github.com/nvidia/apex). The results of the tests performed on pytorch-BERT by the NVIDIA team (and my trials at reproducing them) can be consulted in [the relevant PR of the present repository](https://github.com/huggingface/pytorch-pretrained-BERT/pull/116).  Note: To use *Distributed Training*  you will need to run one training script on each of your machines. This can be done for example by running the following command on each server (see [the above mentioned blog post]((https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)) for more details): ```bash python -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=$THIS_MACHINE_INDEX --master_addr=""192.168.1.1"" --master_port=1234 run_classifier.py (--arg1 --arg2 --arg3 and all other arguments of the run_classifier script) ``` Where `$THIS_MACHINE_INDEX` is an sequential index assigned to each of your machine (0  1  2...) and the machine with rank 0 has an IP address `192.168.1.1` and an open port `1234`.   """;General;https://github.com/cedrickchee/pytorch-pretrained-BERT
"""BERT-base and BERT-large are respectively 110M and 340M parameters models and it can be difficult to fine-tune them on a single GPU with the recommended batch size for good performance (in most case a batch size of 32).  To help with fine-tuning these models  we have included several techniques that you can activate in the fine-tuning scripts [`run_classifier.py`](./examples/run_classifier.py) and [`run_squad.py`](./examples/run_squad.py): gradient-accumulation  multi-gpu training  distributed training and 16-bits training . For more details on how to use these techniques you can read [the tips on training large batches in PyTorch](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255) that I published earlier this month.  Here is how to use these techniques in our scripts:  - **Gradient Accumulation**: Gradient accumulation can be used by supplying a integer greater than 1 to the `--gradient_accumulation_steps` argument. The batch at each step will be divided by this integer and gradient will be accumulated over `gradient_accumulation_steps` steps. - **Multi-GPU**: Multi-GPU is automatically activated when several GPUs are detected and the batches are splitted over the GPUs. - **Distributed training**: Distributed training can be activated by supplying an integer greater or equal to 0 to the `--local_rank` argument (see below). - **16-bits training**: 16-bits training  also called mixed-precision training  can reduce the memory requirement of your model on the GPU by using half-precision training  basically allowing to double the batch size. If you have a recent GPU (starting from NVIDIA Volta architecture) you should see no decrease in speed. A good introduction to Mixed precision training can be found [here](https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/) and a full documentation is [here](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html). In our scripts  this option can be activated by setting the `--fp16` flag and you can play with loss scaling using the `--loss_scale` flag (see the previously linked documentation for details on loss scaling). The loss scale can be zero in which case the scale is dynamically adjusted or a positive power of two in which case the scaling is static.  To use 16-bits training and distributed training  you need to install NVIDIA's apex extension [as detailed here](https://github.com/nvidia/apex). You will find more information regarding the internals of `apex` and how to use `apex` in [the doc and the associated repository](https://github.com/nvidia/apex). The results of the tests performed on pytorch-BERT by the NVIDIA team (and my trials at reproducing them) can be consulted in [the relevant PR of the present repository](https://github.com/huggingface/pytorch-pretrained-BERT/pull/116).  Note: To use *Distributed Training*  you will need to run one training script on each of your machines. This can be done for example by running the following command on each server (see [the above mentioned blog post]((https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)) for more details): ```bash python -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=$THIS_MACHINE_INDEX --master_addr=""192.168.1.1"" --master_port=1234 run_classifier.py (--arg1 --arg2 --arg3 and all other arguments of the run_classifier script) ``` Where `$THIS_MACHINE_INDEX` is an sequential index assigned to each of your machine (0  1  2...) and the machine with rank 0 has an IP address `192.168.1.1` and an open port `1234`.   """;Natural Language Processing;https://github.com/cedrickchee/pytorch-pretrained-BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/saurabhkulkarni77/BERT_multilabel
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/saurabhkulkarni77/BERT_multilabel
"""`DenserNet` uses multiple-semantics fusion for image-based localization (as shown in the above figure)  which leverages the image-level supervision (positive and negative image pairs) without feature correspondences. This repo is the PyTorch implementation of AAAI2021 paper ""DenserNet: Weakly Supervised Visual Localization Using Multi-scale Feature Aggregation.""  [[pdf](https://arxiv.org/abs/2012.02366)] [[project page](https://yimingcuicuicui.github.io/DenserNet/)]   """;Computer Vision;https://github.com/goodproj13/DenserNet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/fancyerii/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/fancyerii/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/LeoWood/bert-horovod
"""[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.   """;Natural Language Processing;https://github.com/amymariaparker2401/new
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/kingcheng2000/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/kingcheng2000/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/felix-do-wizardry/bert_felix
"""The master branch works with **PyTorch 1.1 to 1.4**.  mmdetection is an open source object detection toolbox based on PyTorch. It is a part of the open-mmlab project developed by [Multimedia Laboratory  CUHK](http://mmlab.ie.cuhk.edu.hk/).  ![demo image](demo/coco_test_12510.jpg)   We propose a novel building block for CNNs  namely Res2Net  by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer.  |    Backbone     |Params. | GFLOPs  | top-1 err. | top-5 err. | | :-------------: |:----:  | :-----: | :--------: | :--------: | | ResNet-101      |44.6 M  | 7.8     |  22.63     |  6.44      | | ResNeXt-101-64x4d |83.5M | 15.5    |  20.40     |  -         | | HRNetV2p-W48    | 77.5M  | 16.1    |  20.70     |  5.50      | | Res2Net-101     | 45.2M  | 8.3     |  18.77     |  4.64      |  Compared with other backbone networks  Res2Net requires fewer parameters and FLOPs.  **Note:** - GFLOPs for classification are calculated with image size (224x224).    """;Computer Vision;https://github.com/Res2Net/mmdetection
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/DeligientSloth/QQsim
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/DeligientSloth/QQsim
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/LeoWood/bert-horovod
"""The goal of PySlowFast is to provide a high-performance  light-weight pytorch codebase provides state-of-the-art video backbones for video understanding research on different tasks (classification  detection  and etc). It is designed in order to support rapid implementation and evaluation of novel video research ideas. PySlowFast includes implementations of the following backbone network architectures:  - SlowFast - Slow - C2D - I3D - Non-local Network - X3D   """;General;https://github.com/facebookresearch/SlowFast
"""The goal of PySlowFast is to provide a high-performance  light-weight pytorch codebase provides state-of-the-art video backbones for video understanding research on different tasks (classification  detection  and etc). It is designed in order to support rapid implementation and evaluation of novel video research ideas. PySlowFast includes implementations of the following backbone network architectures:  - SlowFast - Slow - C2D - I3D - Non-local Network - X3D   """;Computer Vision;https://github.com/facebookresearch/SlowFast
"""For ArXiv PDF / abstract tabs:  - Renames the title to paper's title automatically in the background. (Originally is meaningless paper id  or start with paper id) - Add a browser button to open its corresponding abstract / PDF page. (Originally is hard to get back to abstract page from PDF page) - Add a direct download link on abstract page  click it to download the PDF with the title as filename. (Originally is paper id as filename) - Better title even for bookmarks and the [OneTab](https://www.one-tab.com/) plugin! - Firefox has [strict restrictions on PDF.js](https://bugzilla.mozilla.org/show_bug.cgi?id=1454760). So it doesn't work well with OneTab  the PDF renaming is achieved by intercepting requests and show the PDF in a container. The bookmark works well though. - Works well with native tab search (credits: [@The Rooler](https://addons.mozilla.org/en-US/firefox/addon/arxiv-utils/reviews/1674567/))   - [Tab search on Firefox](https://support.mozilla.org/en-US/kb/search-open-tabs-firefox)   - [Enable Tab search on Chrome](https://www.howtogeek.com/722640/how-to-enable-or-disable-the-tab-search-icon-in-chrome/)  [Tab search on Chrome](https://www.howtogeek.com/704212/how-to-search-open-tabs-on-google-chrome/)   - [Enable Tab search on Edge](https://www.makeuseof.com/microsoft-edge-chrome-tab-search/)   ArXiv is a really nice website for researchers  but I think it has 3 main shortages:  1. Unable to link to abstract page from PDF page if the PDF page is accessed directly. 2. No meaningful title for the PDF page  the abstract page have a redundant paper id as the prefix of the title. Bookmarking the PDF page is useless for later bookmark searches. 3. Downloading PDF requires a manual renaming afterwards.  This extension provides a solution to all of them!   """;Computer Vision;https://github.com/j3soon/arxiv-utils
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/felix-do-wizardry/bert_felix
"""This is the codebase that was used to train and evaluate reinforcement learning agents that can play [Heroic - Magic Duel](https://www.heroicgame.com/)  a real-time strategy player-versus-player mobile game.   """;Reinforcement Learning;https://github.com/Nordeus/heroic-rl
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   """;Computer Vision;https://github.com/devsoft123/fast-cnn
"""| Version | Validation Accuracy | Model Size | Training Time | | ------- | ------------------- | ---------- | ------------- | | 1       | 71.88%              | 219 MB     | 3 Hours       | | 2       | 39.53%              | 16 MB      | 7.6 Minutes   | | 3       | 48.59%              | 16 MB      | 14.9 Minutes  | | 4       | 67.19%              | 7 MB       | 10 Minutes    |   """;Computer Vision;https://github.com/2manslkh/korean-food-classification-flask
"""| Version | Validation Accuracy | Model Size | Training Time | | ------- | ------------------- | ---------- | ------------- | | 1       | 71.88%              | 219 MB     | 3 Hours       | | 2       | 39.53%              | 16 MB      | 7.6 Minutes   | | 3       | 48.59%              | 16 MB      | 14.9 Minutes  | | 4       | 67.19%              | 7 MB       | 10 Minutes    |   """;General;https://github.com/2manslkh/korean-food-classification-flask
"""English | [简体中文](README_CN.md)  [![Documentation](https://readthedocs.org/projects/mmpose/badge/?version=latest)](https://mmpose.readthedocs.io/en/latest/?badge=latest) [![actions](https://github.com/open-mmlab/mmpose/workflows/build/badge.svg)](https://github.com/open-mmlab/mmpose/actions) [![codecov](https://codecov.io/gh/open-mmlab/mmpose/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmpose) [![PyPI](https://img.shields.io/pypi/v/mmpose)](https://pypi.org/project/mmpose/) [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/master/LICENSE) [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues) [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)  MMPose is an open-source toolbox for pose estimation based on PyTorch. It is a part of the [OpenMMLab project](https://github.com/open-mmlab).  The master branch works with **PyTorch 1.5+**.  https://user-images.githubusercontent.com/15977946/124654387-0fd3c500-ded1-11eb-84f6-24eeddbf4d91.mp4   """;Computer Vision;https://github.com/open-mmlab/mmpose
"""English | [简体中文](README_CN.md)  [![Documentation](https://readthedocs.org/projects/mmpose/badge/?version=latest)](https://mmpose.readthedocs.io/en/latest/?badge=latest) [![actions](https://github.com/open-mmlab/mmpose/workflows/build/badge.svg)](https://github.com/open-mmlab/mmpose/actions) [![codecov](https://codecov.io/gh/open-mmlab/mmpose/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmpose) [![PyPI](https://img.shields.io/pypi/v/mmpose)](https://pypi.org/project/mmpose/) [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/master/LICENSE) [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues) [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)  MMPose is an open-source toolbox for pose estimation based on PyTorch. It is a part of the [OpenMMLab project](https://github.com/open-mmlab).  The master branch works with **PyTorch 1.5+**.  https://user-images.githubusercontent.com/15977946/124654387-0fd3c500-ded1-11eb-84f6-24eeddbf4d91.mp4   """;General;https://github.com/open-mmlab/mmpose
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/JimmyLauren/caffe_ssd_mofified
"""PaddleOCR aims to create multilingual  awesome  leading  and practical OCR tools that help users train better models and apply them into practice.   """;Computer Vision;https://github.com/raychiu0202/paddleOCR_flask
"""In this work  we first point out that the essential difference between anchor-based and anchor-free detection is actually **how to define positive and negative training samples**. Then we propose an Adaptive Training Sample Selection (ATSS) to automatically select positive and negative samples according to statistical characteristics of object  which significantly improves the performance of anchor-based and anchor-free detectors and bridges the gap between them. Finally  we demonstrate that tiling multiple anchors per location on the image to detect objects is a thankless operation under current situations. Extensive experiments conducted on MS COCO support our aforementioned analysis and conclusions. With the newly introduced ATSS  we improve state-of-the-art detectors by a large margin to 50.7% AP without introducing any overhead. For more details  please refer to our [paper](https://arxiv.org/abs/1912.02424).  *Note: The lite version of our ATSS has been merged to the official code of [FCOS](https://github.com/tianzhi0549/FCOS) as the [center sampling](https://github.com/tianzhi0549/FCOS/blob/master/fcos_core/modeling/rpn/fcos/loss.py#L166-L173) improvement  which improves its performance by ~0.8%. The full version of our ATSS can further improve its performance.*   """;General;https://github.com/sfzhang15/ATSS
"""Bounding box regression is the crucial step in object detection. In existing methods  while $\ell_n$-norm loss is widely adopted for bounding box regression  it is not tailored to the evaluation metric  i.e.  Intersection over Union (IoU). Recently  IoU loss and generalized IoU (GIoU) loss have been proposed to benefit the IoU metric  but still suffer from the problems of slow convergence and inaccurate regression. In this paper  we propose a Distance-IoU (DIoU) loss by incorporating the normalized distance between the predicted box and the target box  which converges much faster in training than IoU and GIoU losses. Furthermore  this paper summarizes three geometric factors in bounding box regression  i.e.  overlap area  central point distance and aspect ratio  based on which a Complete IoU (CIoU) loss is proposed  thereby leading to faster convergence and better performance. By incorporating DIoU and CIoU losses into state-of-the-art object detection algorithms  e.g.  YOLO v3  SSD and Faster RCNN  we achieve notable performance gains in terms of not only IoU metric but also GIoU metric. Moreover  DIoU can be easily adopted into non-maximum suppression (NMS) to act as the criterion  further boosting performance improvement.    """;Computer Vision;https://github.com/Zzh-tju/DIoU
"""This repository contains a full implementation of the T-Fixup algorithm implemented with the fairseq library  and includes both training and evaluation routines on the IWSLT'14 De-En dataset.  T-Fixup was used by [Javier Martin](https://www.kaggle.com/bacterio) and [Andres Torrubia](https://www.kaggle.com/antorsae) in their 3'rd place solution (out of 3395 teams) for the [""Riiid Answer Correctness Prediction""](https://www.kaggle.com/c/riiid-test-answer-prediction) Kaggle challenge. See [this blogpost](https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/209585).  <a name=""env""/>   """;General;https://github.com/layer6ai-labs/T-Fixup
"""Presented here is a project involving an implementation of a neural network architecture that has the aim of detecting rooftops in a typical 'object detection' task. But why is it called Zeus? For two main reasons; firstly Zeus is the god of the sky and we are detecting rooftops from aerial drone photographs. Secondly  because Zeus was the first god to inhabit Olympus and birth the future gods. This project was my first ambitious AI-related project and I hope like Zeus it will spawn many others that follow in its footsteps. I'm not going to dwell too long on the background  theory  or exact nature of neural networks. Many articles and resources are available for this and my attempt at rehashing this history would not do it justice. Instead  I will focus on the problems I faced with this project and how I overcame them. I find the most useful resources online (especially while attempting this project) were those that focussed on the implementation rather than getting caught up in theory. I will walk through the project and code in chronological order which also closely follows the order of implementation of each component.  A brief explainer on the origins of this project: originally it was a code test sent by a company  but I felt that I needed to finish it. As I progressed through the project I continued to learn about how exactly to implement a neural network and obtain results from working code. This was very valuable to me  as while I have education in neural network theory  the practical nature of actual implementation I feel is something you can only get from experience. As for the tools I chose  Mask-RCNN is a commonly used and well understood network that performs well in object detection tasks. Therefore  I decided to use Mask-RCNN as it had pre-trained weights available and would have lots of documentation (for specific resources used  see references). Faster R-CNN was also considered  but Mask R-CNN also has the option of outputting masks for each object (i.e. performing object segmentation as well as detection).   """;Computer Vision;https://github.com/Alexander-Whelan/Zeus
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/chenjian123456/caffe-ssd
"""This repo holds the code for [DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution](https://arxiv.org/pdf/2006.02334.pdf). The project is based on [mmdetection codebase](https://github.com/open-mmlab/mmdetection). Please refer to [mmdetection readme](README.mmdet.md) for installation and running scripts. The code is tested with PyTorch 1.4.0. It may not run with other versions. See [conda_env.md](conda_env.md) for the versions of all the packages.   """;Computer Vision;https://github.com/TeamA2020/Practice
"""|            |                   |        | ---------- |-------------------| | **Author**       | Trần Vĩnh Toàn- Foundation 8 - VTC AI| | **Title**        | Computer Vision - EfficientNet For Fruits & Vegetables Detection App  | | **Topics**       | Ứng dụng trong computer vision  sử dụng thuật toán chính là CNN| | **Descriptions** | Input sẽ là tấm hình với các loại quả khác nhau và file labels-v2.txt chứa danh sách tên của 130 loại quả tương ứng. Dữ liệu dùng để train là dataset của 130 loại quả có kích thước (100px X 100px). Train toàn bộ dữ liệu này bằng cấu trúc mạng CNN  sử dụng model EfficientNet ( Chi tiết về model : https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet  Paper: https://arxiv.org/abs/1905.11946). khi train xong sẽ trả ra output là file trọng số ```weights```. Ta sẽ sử dụng trọng số ```weights``` đã train để predict name của các object trong hình| | **Links**        | https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet   https://github.com/rwightman/pytorch-image-models| | **Framework**    | PyTorch| | **Pretrained Models**  | sử dụng weight đã được train sẵn [https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra-a5e2fbc7.pth](https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra-a5e2fbc7.pth) [https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_es_ra-f111e99c.pth](https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_es_ra-f111e99c.pth)| | **Datasets**     |Mô hình được train với bộ dữ liệu 130 loại quả tại: https://www.kaggle.com/moltean/fruits/data| | **Level of difficulty**| Normal +  có thể train lại với tập dữ liệu khác và model khác tốc độ tùy thuộc vào CPU & GPU & data input|   This is an neural network web app visualizing the training of the network and testing accuracy ~ 99.5% accuracy. The neural network uses pretrained EfficientNet_B3 and then trained to classify images of fruits and vegetables. It is built using Pytorch framework using Python as primary language. App is built using Flask. **© Toàn Vinh**   """;Computer Vision;https://github.com/toanbkmt/EfficientnetFruitDetect
"""Skin cancer is the most prevalent type of cancer. Melanoma  specifically  is responsible for 75% of skin cancer deaths  despite being the least common skin cancer. As with other cancers  early and accurate detection—potentially aided by data science—can make treatment more effective.  Currently  dermatologists evaluate every one of a patient's moles to identify outlier lesions or “ugly ducklings” that are most likely to be melanoma. Existing AI approaches have not adequately considered this clinical frame of reference. Dermatologists could enhance their diagnostic accuracy if detection algorithms take into account “contextual” images within the same patient to determine which images represent a melanoma. If successful  classifiers would be more accurate and could better support dermatological clinic work.  Melanoma is a deadly disease  but if caught early  most melanomas can be cured with minor surgery. Image analysis tools that automate the diagnosis of melanoma will improve dermatologists' diagnostic accuracy. Better detection of melanoma has the opportunity to positively impact millions of people.   """;Computer Vision;https://github.com/ArivCR7/Melanoma_Classifier
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/necla-ml/Deformable-ConvNets-py3
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.    """;Computer Vision;https://github.com/jellywangjie/yolov3
"""This project will be the backbone for the latest updates coming to the GTA5 self driving car project. (https://github.com/Will-J-Gale/GTA5-Self-Driving-Car)   """;Computer Vision;https://github.com/Will-J-Gale/Self-Driving-Car-Vision
"""Nowadays  we have so many high resolution camera  from camera to smartphone  every devices can take photos or videos at high resolution. But sometime we miss a great moments because of blurring or we want to get a photo from video but all the frames is blury.  This project is to build an web-application use Scale-Recurrent Network for deblur that blurred images to restore the value that we want to get from that images.   """;General;https://github.com/natuan310/scale-recurrent-network-images-deblurring
"""Nowadays  we have so many high resolution camera  from camera to smartphone  every devices can take photos or videos at high resolution. But sometime we miss a great moments because of blurring or we want to get a photo from video but all the frames is blury.  This project is to build an web-application use Scale-Recurrent Network for deblur that blurred images to restore the value that we want to get from that images.   """;Computer Vision;https://github.com/natuan310/scale-recurrent-network-images-deblurring
"""Vega is an AutoML algorithm tool chain developed by Noah's Ark Laboratory  the main features are as follows:  1. Full pipeline capailities: The AutoML capabilities cover key functions such as Hyperparameter Optimization  Data Augmentation  Network Architecture Search (NAS)  Model Compression  and Fully Train. These functions are highly decoupled and can be configured as required  construct a complete pipeline. 2. Industry-leading AutoML algorithms: Provides Noah's Ark Laboratory's self-developed **[industry-leading algorithm (Benchmark)](./docs/benchmark.md)** and **[Model Zoo](./docs/model_zoo.md)** to download the state-of-the-art (SOTA) models. 3. Fine-grained network search space: The network search space can be freely defined  and rich network architecture parameters are provided for use in the search space. The network architecture parameters and model training hyperparameters can be searched at the same time  and the search space can be applied to Pytorch  TensorFlow and MindSpore. 4. High-concurrency neural network training capability: Provides high-performance trainers to accelerate model training and evaluation. 5. Multi-Backend: PyTorch (GPU and Ascend 910)  TensorFlow (GPU and Ascend 910)  MindSpore (Ascend 910). 6. Ascend platform: Search and training on the Ascend 910 and model evaluation on the Ascend 310.   """;General;https://github.com/huawei-noah/vega
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/madamak/ultrapy-yolov3-chess
"""Chess has been studied as a subject of artificial intelligence for many decades  which is why it has been described as  “the most widely-studied domain in the history of artificial intelligence” (Silver et al. 1). Ranging back all the way to the 1950s  various aspects of the game have been studied; one of the most notable instances of this is IBM's Deep Blue computer. In 1997  Deep Blue beat chess champion Garry Kasparov (""Deep Blue"")  proving AI's ability to compete against (and beat) the best human chess players.  Given that IBM was able to achieve this over 20 years ago and technology has only continued to evolve  our team chose to focus on creating something that has the ability to derive conclusions from prior player performance and the opening moves used in a chess match. Chess always begins with the same initial board layout; as a result  different combinations of opening moves occur frequently in play and have the potential to be used to predict the outcome of the match at a very early stage. Point systems in chess are often used as the game progresses as a measure of which player holds an advantage; however  our work studies the earliest moves in the game (before players generally begin trading pieces).  Our goal is to predict the likelihood that the result of a chess game can be predicted using only the knowledge of the first x moves and the ratings of the two players involved in the match. Initially  we believed that this may be beneficial for helping players to choose certain openings; though our work may be useful in this respect  the choice of moves should be viewed as a causality of the player's experience. As a result  an inexperienced player making the moves in a bid to play better may yield moderate performance gains  but ultimately the opening moves lead to victory not because they are superior  but because the player making them is superior and recognizes the advantages of using certain openings.   """;Reinforcement Learning;https://github.com/samiamkhan/4641Project
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/rhllasag/ultralyticsYOLOv3
"""Large-scale language models show promising text generation capabilities  but users cannot easily control this generation process. We release *CTRL*  a 1.6 billion-parameter conditional transformer language model  trained to condition on control codes that specify domain  subdomain  entities  relationships between entities  dates  and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text  preserving the advantages of unsupervised learning while providing more explicit control over text generation.  Paper link: https://arxiv.org/abs/1909.05858  Blog link: https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/  The code currently supports two functionalities: 1. Generating from a trained model  two models are available for download - one with a sequence length of 256 and another with a sequence length of 512 -- they are trained with word-level vocabularies and through a sliding window approach can generate well beyond their trained sequence lengths.  2. Source attribution - given a prompt  prints the perplexity of the prompt conditional on each domain control code (see Section 5 of the paper).   Please refer to the argument flags for more details regarding the options available for either.    """;Natural Language Processing;https://github.com/algharak/CTRL
"""In this project  you'll label the pixels of a road in images using a Fully Convolutional Network (FCN).   The outline of the code implementation is shown below. The VGG16 model was downloaded  and the architecture was modified in the last layers. The final model was run with 50 Epochs and a batch size of 4.  ![alt text][image1]   This project is focused on the segmentation of images from a color camera for perception of the driving environment for autonomous vehicle design. The VGG16 model was used with further training on the [Kitti Road](http://www.cvlibs.net/datasets/kitti/eval_road.php) dataset in order to optimize the segmentation using a convolutional neural network. The approach is based on the following research work:  Fully Convolutional Networks for Semantic Segmentation https://arxiv.org/abs/1605.06211   """;Computer Vision;https://github.com/DrBoltzmann/CarND-Semantic-Segmentation
"""A tensorflow re-implementation of Self-Paced Network Embedding use random walk to get positive pair  """;Graphs;https://github.com/liuxinkai94/Graph-embedding
"""In this module  we provide training data  network settings and loss designs for deep face recognition. The training data includes  but not limited to the cleaned MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  etc.. The loss functions include Softmax  SphereFace  CosineFace  ArcFace  Sub-Center ArcFace and Triplet (Euclidean/Angular) Loss.  You can check the detail page of our work [ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/ArcFace)(which accepted in CVPR-2019) and [SubCenter-ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/SubCenter-ArcFace)(which accepted in ECCV-2020).  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this module  you can simply achieve LFW 99.83%+ and Megaface 98%+ by a single model. This module can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   InsightFace is an open source 2D&3D deep face analysis toolbox  mainly based on MXNet.   The master branch works with **MXNet 1.2 to 1.6**  with **Python 3.x**.     """;General;https://github.com/zhaoxin111/insightface
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._  | method           | VOC2007 test *mAP*(VOC07+12+COCO) | VOC2012 test *mAP* (VOC07++12+COCO) |COCO test-dev| | :--------------- | :-------------------------------: | :---------------------------------: | :---------------------------------:| | FSSD300* (VGG16) |               82.7                |                82.0                 |27.1:47.7:27.8| | FSSD512* (VGG16) |               84.5                |                84.2                 |31.8:52.8:33.5|   """;Computer Vision;https://github.com/lzx1413/CAFFE_SSD
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/liuliu408/caffe-ssd
"""<img src=""./fig/fig.png"" width=""1000"">  X3D is an efficient video architecture  searched/optimized for learning video representations. Here  the author expands a tiny base network along axes: space and time (of the input)  width and depth (of the network)  optimizing for the performace at a given complexity (params/FLOPs). It further relies on depthwise-separable 3D convolutions [[1]](https://arxiv.org/pdf/1704.04861.pdf)  inverted-bottlenecks in residual blocks [[2]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)  squeeze-and-excitation blocks [[3]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf)  swish (soft) activations [[4]](https://arxiv.org/pdf/1710.05941;%20http://arxiv.org/abs/1710.05941.pdf) and sparse clip sampling (at inference) to improve its efficiency.  Multigrid training is a mechanism to train video architectures efficiently. Instead of using a fixed batch size for training  this method proposes to use varying batch sizes in a defined schedule  yet keeping the computational budget approximately unchaged by keeping `batch x time x height x width` a constant. Hence  this follows a coarse-to-fine training process by having lower spatio-temporal resolutions at higher batch sizes and vice-versa. In contrast to conventioanl training with a fixed batch size  Multigrid training benefit from 'seeing' more inputs during a training schedule at approximately the same computaional budget.  Our implementaion achieves 62.62% Top-1 accuracy (3-view) on Kinetics-400 when trained for ~200k iterations from scratch (a 4x shorter schedule compared to the original  when adjusted with the linear scaling rule [[5]](https://arxiv.org/pdf/1706.02677.pdf%5B3%5D%20ImageNet))  which takes only ~2.8 days on 4 Titan RTX GPUs. This is much faster than previous Kinetics-400 training schedules on a single machine. Longer schedules can achieve SOTA results. We port and include the weights trained by FAIR for a longer schedule on 128 GPUs  which achieves 71.48% Top-1 accuracy (3-view) on Kinetics-400. This can be used for fine-tuning on other datasets. For instance  we can train on Charades classification (35.01% mAP) and localization (17.71% mAP) within a few hours on 2 Titan RTX GPUs. All models and training logs are included in the repository.   Note: the Kinetics-400 dataset that we trained on contains ~220k (~240k) training and ~17k (~20k) validation clips compared to (original dataset) due to availability.     """;General;https://github.com/kkahatapitiya/X3D-Multigrid
"""<img src=""./fig/fig.png"" width=""1000"">  X3D is an efficient video architecture  searched/optimized for learning video representations. Here  the author expands a tiny base network along axes: space and time (of the input)  width and depth (of the network)  optimizing for the performace at a given complexity (params/FLOPs). It further relies on depthwise-separable 3D convolutions [[1]](https://arxiv.org/pdf/1704.04861.pdf)  inverted-bottlenecks in residual blocks [[2]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)  squeeze-and-excitation blocks [[3]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf)  swish (soft) activations [[4]](https://arxiv.org/pdf/1710.05941;%20http://arxiv.org/abs/1710.05941.pdf) and sparse clip sampling (at inference) to improve its efficiency.  Multigrid training is a mechanism to train video architectures efficiently. Instead of using a fixed batch size for training  this method proposes to use varying batch sizes in a defined schedule  yet keeping the computational budget approximately unchaged by keeping `batch x time x height x width` a constant. Hence  this follows a coarse-to-fine training process by having lower spatio-temporal resolutions at higher batch sizes and vice-versa. In contrast to conventioanl training with a fixed batch size  Multigrid training benefit from 'seeing' more inputs during a training schedule at approximately the same computaional budget.  Our implementaion achieves 62.62% Top-1 accuracy (3-view) on Kinetics-400 when trained for ~200k iterations from scratch (a 4x shorter schedule compared to the original  when adjusted with the linear scaling rule [[5]](https://arxiv.org/pdf/1706.02677.pdf%5B3%5D%20ImageNet))  which takes only ~2.8 days on 4 Titan RTX GPUs. This is much faster than previous Kinetics-400 training schedules on a single machine. Longer schedules can achieve SOTA results. We port and include the weights trained by FAIR for a longer schedule on 128 GPUs  which achieves 71.48% Top-1 accuracy (3-view) on Kinetics-400. This can be used for fine-tuning on other datasets. For instance  we can train on Charades classification (35.01% mAP) and localization (17.71% mAP) within a few hours on 2 Titan RTX GPUs. All models and training logs are included in the repository.   Note: the Kinetics-400 dataset that we trained on contains ~220k (~240k) training and ~17k (~20k) validation clips compared to (original dataset) due to availability.     """;Computer Vision;https://github.com/kkahatapitiya/X3D-Multigrid
"""Pytorch implementation for “Towards Accurate and Compact Architectures via Neural Architecture Transformer”.  <p align=""center""> <img src=""./imgs/nat_scheme.jpg"" alt=""darts"" width=""100%""> </p> <p align=""aligned""> Figure. The scheme of NAT++. Our NAT++ takes an arbitrary architecture as input and produces the optimized architecture as the output. We use blue arrows to represent the process of architecture optimization. Red arrows and boxes denote the computation of reward and gradients. </p>     """;General;https://github.com/guoyongcs/NATv2
"""A general YOLOv4/v3/v2 object detection pipeline inherited from [keras-yolo3-Mobilenet](https://github.com/Adamdad/keras-YOLOv3-mobilenet)/[keras-yolo3](https://github.com/qqwweee/keras-yolo3) and [YAD2K](https://github.com/allanzelener/YAD2K). Implement with tf.keras  including data collection/annotation  model training/tuning  model evaluation and on device deployment. Support different architecture and different technologies:   """;Computer Vision;https://github.com/Gavino7/YOLOv3set
"""Panoptic-PolarNet is a fast and robust LiDAR point cloud panoptic segmentation framework. We learn both semantic segmentation and class-agnostic instance clustering in a single inference network using a polar Bird's Eye View (BEV) representation. Predictions from the semantic and instance head are then fused through a majority voting to create the final panopticsegmentation.  <p align=""center"">         <img src=""imgs/CVPR_pipeline.png"" width=""100%"">  </p>  We test Panoptic-PolarNet on SemanticKITTI and nuScenes datasets. Experiment shows that Panoptic-PolarNet reaches state-of-the-art performances with a real-time inference speed.  <p align=""center"">         <img src=""imgs/result.png"" width=""100%"">  </p>    """;Computer Vision;https://github.com/edwardzhou130/Panoptic-PolarNet
"""PyTorch Implementation of **Bottleneck Transformers for Visual Recognition**.   Link to paper: https://arxiv.org/abs/2101.11605  Structure of Self-Attention layer in paper:  ![self-attention layer](https://github.com/CandiceD17/Bottleneck-Transformers-for-Visual-Recognition/blob/master/asset/self-attention-layer.png)   """;General;https://github.com/CandiceD17/Bottleneck-Transformers-for-Visual-Recognition
"""PyTorch Implementation of **Bottleneck Transformers for Visual Recognition**.   Link to paper: https://arxiv.org/abs/2101.11605  Structure of Self-Attention layer in paper:  ![self-attention layer](https://github.com/CandiceD17/Bottleneck-Transformers-for-Visual-Recognition/blob/master/asset/self-attention-layer.png)   """;Computer Vision;https://github.com/CandiceD17/Bottleneck-Transformers-for-Visual-Recognition
"""Minecraft is a popular sandbox video game that contains a number of hostile non-player entities known as “mobs”; these entities are meant to attack and kill the player character. Our agent will have to learn strategies to deal with each type of hostile mob with the goal of defeating as many mobs and surviving as long as possible. Additionally  the environment in a Minecraft “world” can be randomly generated using an algorithm or built by the player. To create a closed environment for our agent to learn and fight against these mobs  we will be using Microsoft’s Project Malmo. Using machine learning in minecraft is the focus of a large competition called MineRL  which provides rigorous guidelines towards achieving an agent that can operate autonomously in minecraft. It is our hope that methods like the ones we are using to train an agent in a simulated environment can be extrapolated to real life applications like robotics in the physical world. Since minecraft as an environment is completely customizable  it makes it ideal for entry level testing of potential real world use cases.   """;Reinforcement Learning;https://github.com/rishavb123/MineRL
"""Many of the deep learning models designed to aid in drug discovery show improvement over traditional machine learning methods  but are limited due to two main reasons: first  they rely on hand-crafted features which prevents structural information to be learned directly from raw inputs  and  second  the existing architectures are not conducive for use on structured data such as molecules. Extraction of relevant features from images have already proven highly successful using convolutional neural networks (CNNs). Molecules can be represented as fully connected graphs in which atoms and bonds can be represented as nodes and edges  respectively. Graphs are irregularly shaped thereby making CNNs  which rely on convolution on regular grid-like structures  unsuitable for feature extraction [1].   Efforts have been made to generalize the convolution operation for graphs  resulting in the development of graph convolutional neural networks (GCNs). As Kipf and Welling describe in their seminal paper [2]  the idea behind graph convolutional neural networks (GCNs)  as shown in Fig. 1  is to perform convolutions on a graph by aggregating (through sum  average  etc) each node’s neighborhood feature vectors. This new aggregated vector is then passed through a neural network layer  and the output is the new vector representation of the node. Additional neural network layers repeat this same process  except the input is the updated vectors from the first layer.   <p align=""center"">   <img src=""images/Figure 1.png""  alt=""drawing"" width=""600""/> </p>   The drug discovery process is one of the most challenging and expensive endeavors in biomedicine. While there are about <img src=""https://render.githubusercontent.com/render/math?math=10^56""> atoms in the solar system  there are about <img src=""https://render.githubusercontent.com/render/math?math=10^60""> chemical compounds with drug-like features that can be made. Since it is unfeasible for chemists to synthesize and evaluate every molecule  they’ve grown to rely on virtual screening to narrow down promising candidates. However  the challenge of searching this almost infinite space of potential molecules is the perfect substrate for deep learning techniques to improve the drug discovery process even further. While the growing number of large datasets for molecules has already enabled the creation of several useful models  the application of deep learning to drug discovery is still in its infancy. Some useful predictions that could expedite drug discovery include toxicity  ability to bind with a given protein  and quantum properties.  Researchers commonly use Nuclear Magnetic Resonance (NMR) to gain insight into a molecule’s structure and dynamics. NMR’s functionality largely depends on its ability to accurately predict scalar couplings which are the strength of magnetic interactions between pairs of atoms in a given molecule. It is possible to compute scalar couplings on an inputted 3D molecular structure using advanced quantum mechanical simulation methods such as Density Functional Theory (DFT) which approximate Schrödinger’s equation. However  these methods are limited by their high computational cost  and are therefore reserved for use on small systems  or other  less approximate  methods are adopted instead. My goal for this project was to develop a fast  reliable  and cheaper method to perform this task through the use of a graph convolutional neural network (GCN). In particular  I focused on implementing and optimizing SchNet: a novel GCN that has been shown to achieve state-of-the-art performance on quantum chemical property benchmarks. As a byproduct  I hoped to learn more about GCNs and how they could be used for chemical applications.   """;Graphs;https://github.com/jmg764/Molecular-Scalar-Coupling-Constant-Prediction-using-SchNet
"""Many of the deep learning models designed to aid in drug discovery show improvement over traditional machine learning methods  but are limited due to two main reasons: first  they rely on hand-crafted features which prevents structural information to be learned directly from raw inputs  and  second  the existing architectures are not conducive for use on structured data such as molecules. Extraction of relevant features from images have already proven highly successful using convolutional neural networks (CNNs). Molecules can be represented as fully connected graphs in which atoms and bonds can be represented as nodes and edges  respectively. Graphs are irregularly shaped thereby making CNNs  which rely on convolution on regular grid-like structures  unsuitable for feature extraction [1].   Efforts have been made to generalize the convolution operation for graphs  resulting in the development of graph convolutional neural networks (GCNs). As Kipf and Welling describe in their seminal paper [2]  the idea behind graph convolutional neural networks (GCNs)  as shown in Fig. 1  is to perform convolutions on a graph by aggregating (through sum  average  etc) each node’s neighborhood feature vectors. This new aggregated vector is then passed through a neural network layer  and the output is the new vector representation of the node. Additional neural network layers repeat this same process  except the input is the updated vectors from the first layer.   <p align=""center"">   <img src=""images/Figure 1.png""  alt=""drawing"" width=""600""/> </p>   The drug discovery process is one of the most challenging and expensive endeavors in biomedicine. While there are about <img src=""https://render.githubusercontent.com/render/math?math=10^56""> atoms in the solar system  there are about <img src=""https://render.githubusercontent.com/render/math?math=10^60""> chemical compounds with drug-like features that can be made. Since it is unfeasible for chemists to synthesize and evaluate every molecule  they’ve grown to rely on virtual screening to narrow down promising candidates. However  the challenge of searching this almost infinite space of potential molecules is the perfect substrate for deep learning techniques to improve the drug discovery process even further. While the growing number of large datasets for molecules has already enabled the creation of several useful models  the application of deep learning to drug discovery is still in its infancy. Some useful predictions that could expedite drug discovery include toxicity  ability to bind with a given protein  and quantum properties.  Researchers commonly use Nuclear Magnetic Resonance (NMR) to gain insight into a molecule’s structure and dynamics. NMR’s functionality largely depends on its ability to accurately predict scalar couplings which are the strength of magnetic interactions between pairs of atoms in a given molecule. It is possible to compute scalar couplings on an inputted 3D molecular structure using advanced quantum mechanical simulation methods such as Density Functional Theory (DFT) which approximate Schrödinger’s equation. However  these methods are limited by their high computational cost  and are therefore reserved for use on small systems  or other  less approximate  methods are adopted instead. My goal for this project was to develop a fast  reliable  and cheaper method to perform this task through the use of a graph convolutional neural network (GCN). In particular  I focused on implementing and optimizing SchNet: a novel GCN that has been shown to achieve state-of-the-art performance on quantum chemical property benchmarks. As a byproduct  I hoped to learn more about GCNs and how they could be used for chemical applications.   """;General;https://github.com/jmg764/Molecular-Scalar-Coupling-Constant-Prediction-using-SchNet
"""Data augmentation (DA) plays a critical role in improving the generalization of deep learning models. Recent works on automatically searching for DA policies from data have achieved great success. However  existing automated DA methods generally perform the search at the image level  which limits the exploration of diversity in local regions. In this paper  we propose a more fine-grained automated DA approach  dubbed Patch AutoAugment  to divide an image into a grid of patches and search for the joint optimal augmentation policies for the patches. We formulate it as a multi-agent reinforcement learning (MARL) problem  where each agent learns an augmentation policy for each patch based on its content together with the semantics of the whole image. The agents cooperate with each other to achieve the optimal augmentation effect of the entire image by sharing a team reward. We show the effectiveness of our method on multiple benchmark datasets of image classification and fine-grained image recognition (e.g.  CIFAR-10  CIFAR-100  ImageNet  CUB-200-2011  Stanford Cars and FGVC-Aircraft). Extensive experiments demonstrate that our method outperforms the state-of-the-art DA methods while requiring fewer computational resources. <div align=center> <img src=https://github.com/LinShiqi047/PatchAutoAugment/blob/main/figure/PAA.png /> </div>   """;Computer Vision;https://github.com/LinShiqi047/PatchAutoAugment
"""This is a MATLAB implementation of a 2 hidden-layers neural network that recognizes handwritten digit with 97% accuracy on MNIST database. The architecture and training parameters of the network is configurable (including number of layers  number of neurons in each layer  number of training rounds  learning rate  and mini-batch length). The network integrates input normalization  He weight initialization [1]  and Swish activation function [2].  """;Computer Vision;https://github.com/phogbinh/handwritten-digit-recognition
"""This is a MATLAB implementation of a 2 hidden-layers neural network that recognizes handwritten digit with 97% accuracy on MNIST database. The architecture and training parameters of the network is configurable (including number of layers  number of neurons in each layer  number of training rounds  learning rate  and mini-batch length). The network integrates input normalization  He weight initialization [1]  and Swish activation function [2].  """;General;https://github.com/phogbinh/handwritten-digit-recognition
"""A general YOLOv4/v3/v2 object detection pipeline inherited from [keras-yolo3-Mobilenet](https://github.com/Adamdad/keras-YOLOv3-mobilenet)/[keras-yolo3](https://github.com/qqwweee/keras-yolo3) and [YAD2K](https://github.com/allanzelener/YAD2K). Implement with tf.keras  including data collection/annotation  model training/tuning  model evaluation and on device deployment. Support different architecture and different technologies:   """;Computer Vision;https://github.com/Lebhoryi/keras-YOLOv3-model-set
"""Can we automatically group images into semantically meaningful clusters when ground-truth annotations are absent? The task of unsupervised image classification remains an important  and open challenge in computer vision. Several recent approaches have tried to tackle this problem in an end-to-end fashion. In this paper  we deviate from recent works  and advocate a two-step approach where feature learning and clustering are decoupled.  We outperform state-of-the-art methods by large margins  in particular +26.6% on CIFAR10  +25.0% on CIFAR100-20 and +21.3% on STL10 in terms of classification accuracy.  Our method is the first to perform well on ImageNet (1000 classes). __Check out the benchmarks on the [Papers-with-code](https://paperswithcode.com/paper/learning-to-classify-images-without-labels) website for [Image Clustering](https://paperswithcode.com/task/image-clustering) and [Unsupervised Image Classification](https://paperswithcode.com/task/unsupervised-image-classification).__   """;General;https://github.com/wvangansbeke/Unsupervised-Classification
"""YOLOX is an anchor-free version of YOLO  with a simpler design but better performance! It aims to bridge the gap between research and industrial communities. For more details  please refer to our [report on Arxiv](https://arxiv.org/abs/2107.08430).  This repo is an implementation of PyTorch version YOLOX  there is also a [MegEngine implementation](https://github.com/MegEngine/YOLOX).  <img src=""assets/git_fig.png"" width=""1000"" >   """;Computer Vision;https://github.com/xiyie/yolox
"""We included two Jupyter notebooks to demonstrate how the HDF5 datasets are created * For the medium scale datasets view `create_hdf_benchmarking_datasets.ipynb`. You will need `pytorch`  `ogb==1.1.1` and `dgl==0.4.2` libraries to run the notebook. The notebook is also runnable on Google Colaboratory. * For the large scale pcqm4m dataset view `create_hdf_pcqm4m.ipynb`. You will need `pytorch`  `ogb>=1.3.0` and `rdkit>=2019.03.1` to run the notebook.   This is the official implementation of the **Edge-augmented Graph Transformer (EGT)** as described in https://arxiv.org/abs/2108.03348  which augments the Transformer architecture with residual edge channels. The resultant architecture can directly process graph-structured data and acheives good results on supervised graph-learning tasks as presented by [Dwivedi et al.](https://arxiv.org/abs/2003.00982). It also achieves good performance on the large-scale [PCQM4M-LSC](https://arxiv.org/abs/2103.09430) (`0.1263 MAE` on val) dataset. EGT beats convolutional/message-passing graph neural networks on a wide range of supervised tasks and thus demonstrates that convolutional aggregation is not an essential inductive bias for graphs.   """;Natural Language Processing;https://github.com/shamim-hussain/egt
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/shaikhzhas/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/shaikhzhas/bert
"""Unofficial!  A replication of [Octave Convoltuion](https://arxiv.org/abs/1904.05049).  Require TensorFlow 2.0 to run the code.  Model and layers are from [SUL](https://github.com/ddddwee1/sul).   """;Computer Vision;https://github.com/ddddwee1/Octave-Convolution
"""Pegasus is a large Transformer-based encoder-decoder model with a new pre-training objective which is adapted to abstractive summarization. More specifically  the pre-training objective  called ""Gap Sentence Generation (GSG)""  consists of masking important sentences from a document and generating these gap-sentences.  On the other hand  the Longformer is a Transformer which replaces the full-attention mechanism (quadratic dependency) with a novel attention mechanism which scale linearly with the input sequence length. Consequently  Longformer can process sequences up to 4 096 tokens long (8 times longer than BERT which is limited to 512 tokens).  This project plugs Longformer's attention mechanism to Pegasus in order to perform abstractive summarization on long documents. The conversion is done in loading_scripts/Pegasus_to_4k.py which enables Pegasus to process sequences up to 4 096 tokens long (rather than 512 tokens). Note that the `max_pos` parameter can be changed to accept even longer sequences (e.g `max_pos=16384`). The new Pegasus model is then fine-tuned on BigPatent dataset. To assess the model's performance on long documents  all training examples are filtered such that they have a minimum length of 4000 tokens.   This project was built using HuggingFace's Transformers library. The model is trained using model partitioning (with fairscale) and parallel batch processing on a cluster of 8 GPUs.   """;Natural Language Processing;https://github.com/amoramine/Pegasus_with_Longformer_summarization
"""We included two Jupyter notebooks to demonstrate how the HDF5 datasets are created * For the medium scale datasets view `create_hdf_benchmarking_datasets.ipynb`. You will need `pytorch`  `ogb==1.1.1` and `dgl==0.4.2` libraries to run the notebook. The notebook is also runnable on Google Colaboratory. * For the large scale pcqm4m dataset view `create_hdf_pcqm4m.ipynb`. You will need `pytorch`  `ogb>=1.3.0` and `rdkit>=2019.03.1` to run the notebook.   This is the official implementation of the **Edge-augmented Graph Transformer (EGT)** as described in https://arxiv.org/abs/2108.03348  which augments the Transformer architecture with residual edge channels. The resultant architecture can directly process graph-structured data and acheives good results on supervised graph-learning tasks as presented by [Dwivedi et al.](https://arxiv.org/abs/2003.00982). It also achieves good performance on the large-scale [PCQM4M-LSC](https://arxiv.org/abs/2103.09430) (`0.1263 MAE` on val) dataset. EGT beats convolutional/message-passing graph neural networks on a wide range of supervised tasks and thus demonstrates that convolutional aggregation is not an essential inductive bias for graphs.   """;Graphs;https://github.com/shamim-hussain/egt
"""**Adaptive Notes Generator** *is a tool that helps us attend online classes effectively:star_struck:. Due to the Online class culture  taking notes in pen and paper is not a good idea  the only options left are to click screenshots or struggle to note down everything in your notebook:unamused:. Our application will make your life easier  once a meeting video:film_projector: is provided  we will create the notes that will save you time:stopwatch: of research and gathering resources. We will divide your meeting into useful segments and add additional data to make it easy to understand any concept.:bookmark_tabs::bookmark_tabs:*   """;Sequential;https://github.com/chakravarthi-v/Polaroid-1
"""This is a classifier to predict label of hand drawn doodle images in real time. The idea is based on [QuickDraw](https://quickdraw.withgoogle.com/#) by Google. The [dataset](https://github.com/googlecreativelab/quickdraw-dataset) they provide contains 50 million images across 345 categories! I am using a subset of 50 categories for my model because of limited resources but one can use the same code with small tweaks to train on all 345 categories.  I built and trained the model in Pytorch and converted it to onnx format to use it in the browser. Initially my plan was to perform the classification on the backend. After drawing  the user would press a button and the request would be sent to the server for classification. That is how I built the app. However  it was very expensive on the server because for every image there would a request. Hence I decided to move the classification on the frontend. Also it is a lot more fun to see the model try to classify the image in real time :grin:.  It took me a very long time to train and tweak the model to obtain a good accuracy particularly because the dataset was huge even though I was using only a subset of it and training the model on the GPU. How someone draws a certain object varies a lot. It is all based on imagination and perception of that person about that object. Hence it was necessary to use lots of images per category to capture maximum variations.   For the record  88% was the average test accuracy(averaged out across the classes). Had it been allowed to train for longer  I believe it could have crossed 90% mark but I had already spent days on training it so I let it go. Yes days! Colab has a limit on the usage of GPUs after which the runtime gets disconnected. So I had to train the model for some hours every day for about 2-3 days to get good accuracy. Then I would make tweaks to the model and restart the process.     """;Computer Vision;https://github.com/deeplearning987/DoodleClassifier
"""We present DEAR  a DL-based approach that supports auto-fixing for the bugs that require changes at once to one or multiple hunks and one or multiple consecutive statements.We first design a novel fault localization (FL) technique for multi-hunk  multi-statement fixes that combines traditional  spectrum-based (SB) FL with deep learning and data-flow analysis. It takes the buggy statements returned by the SBFL  and detects the buggy hunks to be fixed at once and expands a buggy statement s in a hunk to include other suspicious statements from s. We enhance a two-tier  tree-based LSTM model that incorporates cycle training and uses a divide-andconquer strategy to learn proper code transformations for fixing multiple statements in the suitable fixing context consisting of surrounding subtrees. We conducted several experiments to evaluate DEAR on three datasets: Defects4J (395 bugs)  BigFix (+26k bugs)  and CPatMiner (+44k bugs). In CPatMiner  DEAR fixes 71 and 164 more bugs  including 52 and 61 more multi-hunk/multistatement bugs  than existing DL-based APR tools. Among 667 fixed bugs  there are 169 (25.3%) multi-hunk/multi-statement ones. On Defects4J  it outperforms those tools from 42–683% in terms of the number of auto-fixed bugs with only Top-1 ranked patches.   """;Natural Language Processing;https://github.com/AutomatedProgramRepair-2021/dear-auto-fix
"""As many others  this paper buids on recent work on linear attention that is calculated as <img src=""https://render.githubusercontent.com/render/math?math=\phi(Q) \left(\phi(K)^T V\right)""> instead of <img src=""https://render.githubusercontent.com/render/math?math=\text{softmax}\left(Q K^T\right)V"">  where <img src=""https://render.githubusercontent.com/render/math?math=\phi""> is a kernel function. This reduces the complexity from <img src=""https://render.githubusercontent.com/render/math?math=\mathcal{O}(N^2 D)""> to <img src=""https://render.githubusercontent.com/render/math?math=\mathcal{O}(ND^2)"">. The authors propose to extend this mechanism by including relative distance information in the Q  K product as <img src=""https://render.githubusercontent.com/render/math?math=\phi(Q_i)\phi(K_j)^T\cos\left(\frac{\pi}{2}\times\frac{i-j}{M}\right)"">. After expanding the trigonometric identity  the full equation becomes:  <p align=""center"">   <img src=""https://render.githubusercontent.com/render/math?math=\text{Attention}(Q  K  V)  =  Q^{\cos} \left(K^{\cos} V\right) %2B Q^{\sin} \left(K^{\sin} V\right)""> </p>  where <img src=""https://render.githubusercontent.com/render/math?math=Q_i^{\cos} = \phi(Q_i)\cos\left(\frac{\pi i}{2M}\right)  Q_i^{\sin} = \phi(Q_i)\sin\left(\frac{\pi i}{2M}\right)""> etc.  As the author of this repo possesses neither the time nor the ability  only the non-causal version of this approach is implemented.       """;General;https://github.com/davidsvy/cosformer-pytorch
"""This repository contains a TensorFlow-based implementation of **[4PP-EUSR (""Deep learning-based image super-resolution considering quantitative and perceptual quality"")](http://arxiv.org/abs/1809.04789)**  which considers both the quantitative (e.g.  PSNR) and perceptual quality (e.g.  NIQE) of the upscaled images. Our method **won the 2nd place and got the highest human opinion score for Region 2** in the [2018 PIRM Challenge on Perceptual Image Super-resolution at ECCV 2018](https://arxiv.org/abs/1809.07517).  ![BSD100 - 37073](figures/bsd100_37073.png) ※ The perceptual index is calculated by ""0.5 * ((10 - [Ma](https://sites.google.com/site/chaoma99/sr-metric)) + [NIQE](https://doi.org/10.1109/LSP.2012.2227726))""  which is used in the [PIRM Challenge](https://www.pirm2018.org/PIRM-SR.html). Lower is better.  Followings are the performance comparison evaluated on the [BSD100](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/) dataset.  ![BSD100 PSNR vs. NIQE](figures/bsd100_psnr_niqe.png)  Method | PSNR (dB) (↓) | Perceptual Index ------------ | :---: | :---: [EDSR](https://github.com/thstkdgus35/EDSR-PyTorch) | 27.796 | 5.326 [MDSR](https://github.com/thstkdgus35/EDSR-PyTorch) | 27.771 | 5.424 [EUSR](https://github.com/ghgh3269/EUSR-Tensorflow) | 27.674 | 5.307 [SRResNet-MSE](https://arxiv.org/abs/1609.04802) | 27.601 | 5.217 **4PP-EUSR (PIRM Challenge)** | **26.569** | **2.683** [SRResNet-VGG22](https://arxiv.org/abs/1609.04802) | 26.322 | 5.183 [SRGAN-MSE](https://arxiv.org/abs/1609.04802) | 25.981 | 2.802 Bicubic interpolation | 25.957 | 6.995 [SRGAN-VGG22](https://arxiv.org/abs/1609.04802) | 25.697 | 2.631 [SRGAN-VGG54](https://arxiv.org/abs/1609.04802) | 25.176 | 2.351 [CX](https://arxiv.org/abs/1803.04626) | 24.581 | 2.250  Please cite following papers when you use the code  pre-trained models  or results: - J.-H. Choi  J.-H. Kim  M. Cheon  J.-S. Lee: **Deep learning-based image super-resolution considering quantitative and perceptual quality**. Neurocomputing (In Press) [[Paper]](https://doi.org/10.1016/j.neucom.2019.06.103) [[arXiv]](http://arxiv.org/abs/1809.04789) ``` @article{choi2018deep    title={Deep learning-based image super-resolution considering quantitative and perceptual quality}    author={Choi  Jun-Ho and Kim  Jun-Hyuk and Cheon  Manri and Lee  Jong-Seok}    journal={Neurocomputing}    year={2019}    publisher={Elsevier} } ``` - J.-H. Kim  J.-S. Lee: **Deep residual network with enhanced upscaling module for super-resolution**. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops  pp. 913-921 (2018) [[Paper]](http://openaccess.thecvf.com/content_cvpr_2018_workshops/w13/html/Kim_Deep_Residual_Network_CVPR_2018_paper.html) ``` @inproceedings{kim2018deep    title={Deep residual network with enhanced upscaling module for super-resolution}    author={Kim  Jun-Hyuk and Lee  Jong-Seok}    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops}    year={2018} } ```   """;General;https://github.com/idearibosome/tf-perceptual-eusr
"""This repository contains a TensorFlow-based implementation of **[4PP-EUSR (""Deep learning-based image super-resolution considering quantitative and perceptual quality"")](http://arxiv.org/abs/1809.04789)**  which considers both the quantitative (e.g.  PSNR) and perceptual quality (e.g.  NIQE) of the upscaled images. Our method **won the 2nd place and got the highest human opinion score for Region 2** in the [2018 PIRM Challenge on Perceptual Image Super-resolution at ECCV 2018](https://arxiv.org/abs/1809.07517).  ![BSD100 - 37073](figures/bsd100_37073.png) ※ The perceptual index is calculated by ""0.5 * ((10 - [Ma](https://sites.google.com/site/chaoma99/sr-metric)) + [NIQE](https://doi.org/10.1109/LSP.2012.2227726))""  which is used in the [PIRM Challenge](https://www.pirm2018.org/PIRM-SR.html). Lower is better.  Followings are the performance comparison evaluated on the [BSD100](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/) dataset.  ![BSD100 PSNR vs. NIQE](figures/bsd100_psnr_niqe.png)  Method | PSNR (dB) (↓) | Perceptual Index ------------ | :---: | :---: [EDSR](https://github.com/thstkdgus35/EDSR-PyTorch) | 27.796 | 5.326 [MDSR](https://github.com/thstkdgus35/EDSR-PyTorch) | 27.771 | 5.424 [EUSR](https://github.com/ghgh3269/EUSR-Tensorflow) | 27.674 | 5.307 [SRResNet-MSE](https://arxiv.org/abs/1609.04802) | 27.601 | 5.217 **4PP-EUSR (PIRM Challenge)** | **26.569** | **2.683** [SRResNet-VGG22](https://arxiv.org/abs/1609.04802) | 26.322 | 5.183 [SRGAN-MSE](https://arxiv.org/abs/1609.04802) | 25.981 | 2.802 Bicubic interpolation | 25.957 | 6.995 [SRGAN-VGG22](https://arxiv.org/abs/1609.04802) | 25.697 | 2.631 [SRGAN-VGG54](https://arxiv.org/abs/1609.04802) | 25.176 | 2.351 [CX](https://arxiv.org/abs/1803.04626) | 24.581 | 2.250  Please cite following papers when you use the code  pre-trained models  or results: - J.-H. Choi  J.-H. Kim  M. Cheon  J.-S. Lee: **Deep learning-based image super-resolution considering quantitative and perceptual quality**. Neurocomputing (In Press) [[Paper]](https://doi.org/10.1016/j.neucom.2019.06.103) [[arXiv]](http://arxiv.org/abs/1809.04789) ``` @article{choi2018deep    title={Deep learning-based image super-resolution considering quantitative and perceptual quality}    author={Choi  Jun-Ho and Kim  Jun-Hyuk and Cheon  Manri and Lee  Jong-Seok}    journal={Neurocomputing}    year={2019}    publisher={Elsevier} } ``` - J.-H. Kim  J.-S. Lee: **Deep residual network with enhanced upscaling module for super-resolution**. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops  pp. 913-921 (2018) [[Paper]](http://openaccess.thecvf.com/content_cvpr_2018_workshops/w13/html/Kim_Deep_Residual_Network_CVPR_2018_paper.html) ``` @inproceedings{kim2018deep    title={Deep residual network with enhanced upscaling module for super-resolution}    author={Kim  Jun-Hyuk and Lee  Jong-Seok}    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops}    year={2018} } ```   """;Computer Vision;https://github.com/idearibosome/tf-perceptual-eusr
"""PatternNet and PatternLRP are methods that help to interpret decision of non-linear neural networks. They are in a line with the methods DeConvNet  GuidedBackprop and LRP:  ![An overview of the different explanation methods.](https://raw.githubusercontent.com/pikinder/nn-patterns/master/images/fig2.png)  and improve on them:  ![Different explanation methods on ImageNet.](https://raw.githubusercontent.com/pikinder/nn-patterns/master/images/fig5.png)  For more details we refer to the paper:  ``` PatternNet and PatternLRP -- Improving the interpretability of neural networks Pieter-Jan Kindermans  Kristof T. Schütt  Maximilian Alber  Klaus-Robert Müller  Sven Dähne https://arxiv.org/abs/1705.05598 ```  If you use this code please cite the following paper: ``` TODO: Add link to SW paper. ```    """;Computer Vision;https://github.com/pikinder/nn-patterns
"""This code provides a framework to easily add architectures and datasets  in order to  train and deploy CNNs for a robot. It contains a full training pipeline in python using Tensorflow and OpenCV  and it also some C++ apps to deploy a frozen protobuf in ROS and standalone. The C++ library is made in a way which allows to add other backends (such as TensorRT and MvNCS)  but only Tensorflow and TensorRT are implemented for now. For now  we will keep it this way because we are mostly interested in deployment for the Jetson and Drive platforms  but if you have a specific need  we accept pull requests!  The networks included is based of of many other architectures (see below)  but not exactly a copy of any of them. As seen in the videos  they run very fast in both GPU and CPU  and they are designed with performance in mind  at the cost of a slight accuracy loss. Feel free to use it as a model to implement your own architecture.  All scripts have been tested on the following configurations: - x86 Ubuntu 16.04 with an NVIDIA GeForce 940MX GPU (nvidia-384  CUDA9  CUDNN7  TF 1.7  TensorRT3) - x86 Ubuntu 16.04 with an NVIDIA GTX1080Ti GPU (nvidia-375  CUDA9  CUDNN7  TF 1.7  TensorRT3) - x86 Ubuntu 16.04 and 14.04 with no GPU (TF 1.7  running on CPU in NHWC mode  no TensorRT support) - Jetson TX2 (full Jetpack 3.2)  We also provide a Dockerfile to make it easy to run without worrying about the dependencies  which is based on the official nvidia/cuda image containing cuda9 and cudnn7. In order to build and run this image with support for X11 (to display the results)  you can run this in the repo root directory ([nvidia-docker](https://github.com/NVIDIA/nvidia-docker) should be used instead of vainilla docker):  ```sh   $ docker pull tano297/bonnet:cuda9-cudnn7-tf17-trt304   $ nvidia-docker build -t bonnet .   $ nvidia-docker run -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v $HOME/.Xauthority:/home/developer/.Xauthority -v /home/$USER/data:/shared --net=host --pid=host --ipc=host bonnet /bin/bash ```  _-v /home/$USER/data:/share_ can be replaced to point to wherever you store the data and trained models  in order to include the data inside the container for inference/training.   """;Computer Vision;https://github.com/PRBonn/bonnet
"""Which benefits:  - PCB structure  - PCB randomly update  - batchnorm  - random erasing  zero paddding crop  - warm-up learning rate  - global branch  - small batchsize  Which might helps:  - feature erasing  - feature mask  - tri-loss  - balanced sampling  - multi-gpu training (differs in BN layer)    Not working:  - adam  - am-softmax  - bias in FC layer or BN     """;Computer Vision;https://github.com/xuxu116/pytorch-reid-lite
"""This is an implementation of the object detection architecture ""YOLOv3"" using the deep learning framework ""Pytorch"". YOLOv3 is a neural network architecture for performing object detection on images and videos  it's a refinement of the previous  YOLO9000.    """;Computer Vision;https://github.com/DarkGeekMS/Pytorch-YOLOv3-Implementation
"""This tutorial implements [Learning Structured Output Representation using Deep Conditional Generative Models](http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generati) paper  which introduced Conditional Variational Auto-encoders in 2015  using Pyro PPL.  Supervised deep learning has been successfully applied for many recognition problems in machine learning and computer vision.  Although it can approximate a complex many-to-one function very well when large number of training data is provided  the lack of probabilistic inference of the current supervised deep learning methods makes it difficult to model a complex structured output representations.  In this work  Kihyuk Sohn  Honglak Lee and Xinchen Yan develop a scalable deep conditional generative model for structured output variables using Gaussian latent variables.  The model is trained efficiently in the framework of stochastic gradient variational Bayes  and allows a fast prediction using stochastic feed-forward inference.  They called the model Conditional Variational Auto-encoder (CVAE).  The CVAE is a conditional directed graphical model whose input observations modulate the prior on Gaussian latent variables that generate the outputs.  It is trained to maximize the conditional marginal log-likelihood.  The authors formulate the variational learning objective of the CVAE in the framework of stochastic gradient variational Bayes (SGVB).  In experiments  they demonstrate the effectiveness of the CVAE in comparison to the deterministic neural network counterparts in generating diverse but realistic output predictions using stochastic inference.  Here  we will implement their proof of concept: an artificial experimental setting for structured output prediction using MNIST database.   """;Computer Vision;https://github.com/ucals/cvae
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/stupidZZ/pyc_repo
"""In seismic exploration  the features of wave propagation in elastic media are studied. Processing of seismic data  allows you to determine the structure and elastic properties of the studied medium.  To obtain seismic data  a source of excitation of elastic waves and a recording system are required. In small-scale ground research  a sledgehammer is usually used as a source  and geophones as receivers.  A geophone is a device that consists of an electric coil and a permanent magnet. During the propagation of elastic waves  the magnet inside the geophone oscillates  thereby creating an alternating current in the coil. The signal is recorded for some time. The waveform is usually similar to the velocity or acceleration of the  oscillation of the particles of the medium (depending on the design of the geophone).  The figure below schematically shows the process of obtaining data. Geophones are placed on the profile with some distance from the metal plate (offset). An engineer hits a metal plate with a sledgehammer  creating an elastic wave that  propagates in the medium under study. At this time  geophones record the amplitude of the signal for some time.  Records of each geophone are called a seismic trace (1D data).  ![](examples/seismic_survey.svg)  Since the data on adjacent seismic traces have similarities in the features of the wave field  it is convenient to consider the data together. Therefore  the traces are combined into a seismogram (2D data). The trace number is  indicated on the horizontal axis  and the time of registration (in the number of samples) is indicated on  the vertical axis.  When visualizing traces in a seismogram  positive or negative amplitudes are usually indicated in black. Also   the amplitudes of each trace are normalized to the maximum amplitude of the seismogram  or each trace is normalized  individually. Waves with large amplitudes are usually clip at the threshold.  When only the amplitude  and not the waveform  is important  a seismogram can be displayed using color.  Usually a color seismogram is drawn in grayscale.  The figure below shows the equal seismograms in wiggle and color mode.  ![](examples/color.png)  Note that despite the square shape of the picture  it is not square. Usually  the number of samples in a trace is several thousand  and the number of trace is 24 or more. Therefore  when displayed in color  interpolation is used.   """;Computer Vision;https://github.com/DaloroAT/first_break_picking
"""In seismic exploration  the features of wave propagation in elastic media are studied. Processing of seismic data  allows you to determine the structure and elastic properties of the studied medium.  To obtain seismic data  a source of excitation of elastic waves and a recording system are required. In small-scale ground research  a sledgehammer is usually used as a source  and geophones as receivers.  A geophone is a device that consists of an electric coil and a permanent magnet. During the propagation of elastic waves  the magnet inside the geophone oscillates  thereby creating an alternating current in the coil. The signal is recorded for some time. The waveform is usually similar to the velocity or acceleration of the  oscillation of the particles of the medium (depending on the design of the geophone).  The figure below schematically shows the process of obtaining data. Geophones are placed on the profile with some distance from the metal plate (offset). An engineer hits a metal plate with a sledgehammer  creating an elastic wave that  propagates in the medium under study. At this time  geophones record the amplitude of the signal for some time.  Records of each geophone are called a seismic trace (1D data).  ![](examples/seismic_survey.svg)  Since the data on adjacent seismic traces have similarities in the features of the wave field  it is convenient to consider the data together. Therefore  the traces are combined into a seismogram (2D data). The trace number is  indicated on the horizontal axis  and the time of registration (in the number of samples) is indicated on  the vertical axis.  When visualizing traces in a seismogram  positive or negative amplitudes are usually indicated in black. Also   the amplitudes of each trace are normalized to the maximum amplitude of the seismogram  or each trace is normalized  individually. Waves with large amplitudes are usually clip at the threshold.  When only the amplitude  and not the waveform  is important  a seismogram can be displayed using color.  Usually a color seismogram is drawn in grayscale.  The figure below shows the equal seismograms in wiggle and color mode.  ![](examples/color.png)  Note that despite the square shape of the picture  it is not square. Usually  the number of samples in a trace is several thousand  and the number of trace is 24 or more. Therefore  when displayed in color  interpolation is used.   """;General;https://github.com/DaloroAT/first_break_picking
"""Here we introduce RainNet -- a convolutional neural network for radar-based precipitation nowcasting. RainNet was trained to predict continuous precipitation intensities at a lead time of five minutes  using several years of quality-controlled weather radar composites provided by the German Weather Service (DWD).   The source code of the RainNet model written using [_Keras_](https://keras.io) functional API is in the file `rainnet.py`.  The pretrained instance of `keras` `Model` for RainNet  as well as RainNet's pretrained weights are available on Zenodo:   [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3630429.svg)](https://doi.org/10.5281/zenodo.3630429)   """;Computer Vision;https://github.com/hydrogo/rainnet
"""This is an unofficial submission to ICLR 2019 Reproducibility Challenge. The central theme of the work by the authors is to reduce the computations while improving the accuracy in the case of Object Recognition and Speech Recognition by using multiple branches with different scales in the CNN architecture. This helps in feature detection at different scales. The authors claim that in the case of Object Recognition they can improve the accuracy by 1% while reducing the computations by 1/3rd of the original.  **Update**: For the official code for Big Little Net  checkout [IBM's](https://github.com/IBM/BigLittleNet) Official repository. This repository is being archived.    """;Computer Vision;https://github.com/apoorvagnihotri/big-little-net
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/semal/bert
"""&emsp;&emsp;This project is about movive reviews sentiment analysis based on Transformer and ULMFiT model.   **You can browse the full report from [here](https://github.com/PrideLee/sentiment-analysis/blob/master/Different%20Deep%20Learning%20Models%20Applied%20to%20Sentiment%20Analysis.pdf).**   """;General;https://github.com/PrideLee/sentiment-analysis
"""&emsp;&emsp;This project is about movive reviews sentiment analysis based on Transformer and ULMFiT model.   **You can browse the full report from [here](https://github.com/PrideLee/sentiment-analysis/blob/master/Different%20Deep%20Learning%20Models%20Applied%20to%20Sentiment%20Analysis.pdf).**   """;Natural Language Processing;https://github.com/PrideLee/sentiment-analysis
"""This post describes how I used the eo-learn and fastai libraries to create a machine learning data pipeline that can classify crop types from satellite imagery. I used this pipeline to enter Zindi’s [Farm Pin Crop Detection Challenge](https://zindi.africa/competitions/farm-pin-crop-detection-challenge). I may not have won the contest but I learnt some great techniques for working with remote-sensing data which I detail in this post.  Here are the preprocessing steps I followed:  1. divided an area of interest into a grid of ‘patches’   1. loaded imagery from disk   1. masked out cloud cover   1. added NDVI and euclidean norm features   1. resampled the imagery to regular time intervals   1. added raster layers with the targets and identifiers.  I reframed the problem of crop type classification as a semantic segmentation task and trained a U-Net with a ResNet50 encoder on multi-temporal multi-spectral data using image augmentation and mixup to prevent over-fitting.  My solution borrows heavily from the approach outlined by [Matic Lubej](undefined) in his [three](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-1-2471e8098195) [excellent](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-2-bd9aa86f8500) [posts](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-3-c62ed9ecd405) on land cover classification with [eo-learn](https://github.com/sentinel-hub/eo-learn).  The python notebooks I created can be found in this github repository: [https://github.com/simongrest/farm-pin-crop-detection-challenge](https://github.com/simongrest/farm-pin-crop-detection-challenge)   """;Computer Vision;https://github.com/simongrest/farm-pin-crop-detection-challenge
"""This post describes how I used the eo-learn and fastai libraries to create a machine learning data pipeline that can classify crop types from satellite imagery. I used this pipeline to enter Zindi’s [Farm Pin Crop Detection Challenge](https://zindi.africa/competitions/farm-pin-crop-detection-challenge). I may not have won the contest but I learnt some great techniques for working with remote-sensing data which I detail in this post.  Here are the preprocessing steps I followed:  1. divided an area of interest into a grid of ‘patches’   1. loaded imagery from disk   1. masked out cloud cover   1. added NDVI and euclidean norm features   1. resampled the imagery to regular time intervals   1. added raster layers with the targets and identifiers.  I reframed the problem of crop type classification as a semantic segmentation task and trained a U-Net with a ResNet50 encoder on multi-temporal multi-spectral data using image augmentation and mixup to prevent over-fitting.  My solution borrows heavily from the approach outlined by [Matic Lubej](undefined) in his [three](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-1-2471e8098195) [excellent](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-2-bd9aa86f8500) [posts](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-3-c62ed9ecd405) on land cover classification with [eo-learn](https://github.com/sentinel-hub/eo-learn).  The python notebooks I created can be found in this github repository: [https://github.com/simongrest/farm-pin-crop-detection-challenge](https://github.com/simongrest/farm-pin-crop-detection-challenge)   """;General;https://github.com/simongrest/farm-pin-crop-detection-challenge
"""Recent techniques built on Generative Adversarial Networks (GANs) like [CycleGAN](https://arxiv.org/abs/1703.10593) are able to learn mappings between domains from unpaired datasets through min-max optimization games between generators and discriminators. However  it remains challenging to stabilize the training process and diversify generated results. To address these problems  we present the non-trivial Bayesian extension of cyclic model and an integrated cyclic framework for inter-domain mappings.  The proposed method stimulated by [Bayesian GAN](https://arxiv.org/abs/1705.09558) explores the full posteriors of Bayesian cyclic model (with latent sampling) and optimizes the model with maximum a posteriori (MAP) estimation. By exploring the full posteriors over model parameters  the Bayesian marginalization could alleviate the risk of model collapse and boost multimodal distribution learning. Besides  we deploy a combination of L1 loss and GANLoss between reconstructed images and source images to enhance the reconstructed learning  we also prove that this variation has a global optimality theoretically and show its effectiveness in experiments.   """;Computer Vision;https://github.com/ranery/Bayesian-CycleGAN
"""Recent techniques built on Generative Adversarial Networks (GANs) like [CycleGAN](https://arxiv.org/abs/1703.10593) are able to learn mappings between domains from unpaired datasets through min-max optimization games between generators and discriminators. However  it remains challenging to stabilize the training process and diversify generated results. To address these problems  we present the non-trivial Bayesian extension of cyclic model and an integrated cyclic framework for inter-domain mappings.  The proposed method stimulated by [Bayesian GAN](https://arxiv.org/abs/1705.09558) explores the full posteriors of Bayesian cyclic model (with latent sampling) and optimizes the model with maximum a posteriori (MAP) estimation. By exploring the full posteriors over model parameters  the Bayesian marginalization could alleviate the risk of model collapse and boost multimodal distribution learning. Besides  we deploy a combination of L1 loss and GANLoss between reconstructed images and source images to enhance the reconstructed learning  we also prove that this variation has a global optimality theoretically and show its effectiveness in experiments.   """;General;https://github.com/ranery/Bayesian-CycleGAN
"""Cutout is a simple regularization method for convolutional neural networks which consists of masking out random sections of input images during training. This technique simulates occluded examples and encourages the model to take more minor features into consideration when making decisions  rather than relying on the presence of a few major features.      ![Cutout applied to CIFAR-10](https://github.com/uoguelph-mlrg/Cutout/blob/master/images/cutout_on_cifar10.jpg ""Cutout applied to CIFAR-10"")  Bibtex:   ``` @article{devries2017cutout      title={Improved Regularization of Convolutional Neural Networks with Cutout}      author={DeVries  Terrance and Taylor  Graham W}      journal={arXiv preprint arXiv:1708.04552}      year={2017}   } ```   """;Computer Vision;https://github.com/dishen12/Cuout_modify
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/charlesq34/pointnet
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  <center>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | |:-------|:-----:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | 300 | | [Faster R-CNN (ZF)](https://github.com/ShaoqingRen/faster_rcnn) | 62.1 | 17 | 300 | | [YOLO](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | | [Fast YOLO](http://pjreddie.com/darknet/yolo/) | 52.7 | 155 | 98 | | SSD300 (VGG16) | 72.1 | 58 | 7308 | | SSD300 (VGG16  cuDNN v5) | 72.1 | 72 | 7308 | | SSD500 (VGG16) | **75.1** | 23 | 20097 |  </center>   """;Computer Vision;https://github.com/MonsterPeng/caffe_ssd
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/zsweet/BERT_zsw
"""Welcome to the code repository of [How to train your MAML](https://arxiv.org/abs/1810.09502). This repository includes code for training both MAML and MAML++ models  as well as data providers and the datasets for both. By using this codebase you agree to the terms  and conditions in the [LICENSE](https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/blob/master/LICENSE) file. If you choose to use the Mini-Imagenet dataset  you must abide by the terms and conditions in the [ImageNet LICENSE](https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/blob/master/imagenet_license.md)   """;General;https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch
"""* They learned that CNN’s must be scaled up in multiple dimensions. Scaling CNN’s only in one direction (eg depth only) will result in rapidly deteriorating gains relative to the computational increase needed. As shown in the image below.  ![them_gains](https://github.com/PotatoSpudowski/CactiNet/blob/master/Images/gains.jpeg)  * In order to scale up efficiently  all dimensions of depth  width and resolution have to be scaled together  and there is an optimal balance for each dimension relative to the others.    """;Computer Vision;https://github.com/PotatoSpudowski/CactiNet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/svakulenk0/response_eval
"""This project was conducted as part of my engineering degree. The goal was to build a lip reading AI that could output words or sentences from a silent video input.   """;Computer Vision;https://github.com/khazit/Lip2Word
"""This project was conducted as part of my engineering degree. The goal was to build a lip reading AI that could output words or sentences from a silent video input.   """;General;https://github.com/khazit/Lip2Word
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/zsweet/BERT_zsw
"""The goal of this project is to develop a bot using Reinforcement Learning that can play Connect 4 against humans.   """;Reinforcement Learning;https://github.com/RandyDeng/gym_connect4
"""For this project  we work with the [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment.  ![Trained Agent][image1]  In this environment  a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus  the goal of your agent is to maintain its position at the target location for as many time steps as possible.  The observation space consists of 33 variables corresponding to position  rotation  velocity  and angular velocities of the arm. Each action is a vector with four numbers  corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.   """;Reinforcement Learning;https://github.com/shehrum/RL_Continous-Control
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/takooctopus/Yolov3-Tako
"""This repository has scripts and Jupyter-notebooks to perform all the different steps involved in [**Transforming Delete  Retrieve  Generate Approach for Controlled Text Style Transfer**](https://www.aclweb.org/anthology/D19-1322/)   This mechanism is used for **_text style transfer_** when a **_parallel corpus_** for both the styles is not available. This mechanism works on the assumption that the text of any style is made of **two** parts: **1. Content** and **2. Attributes** . Below is a simpe example of a resturent review. ``` The food was great and the service was excellent. Content: The food was and the service was Attributes: great  excellent ``` We transfer the style of a given content in in two different ways. The first is referred to as the **Delete and Generate** approach (referred to as Blind Generative Style Transformer - B-GST in the paper) in which the model transfers the style of the text by choosing attributes automatically learnt during training. The second is referred to as the **Delete  Retrieve and Generate** (referred to as Guided Generative Style Transformer - G-GST in the paper) approach in which the model uses attributes retrieved from the target corpus to generate target sentences from the content. Below are a few examples.  **Generative a negative sentiment sentence from content (neutral) with Delete and Generate** ``` Content: The food was and the service was Output: The food tasteless and the service was horrible. ```  **Generative a negative sentiment sentence from content (neutral) with Delete  Retrieve and Generate** ``` Content: The food was and the service was Attributes: blend  slow Output: The food was blend and the service was slow. ``` The names **Delete and Generate** and **Delete  Retrieve and Generate** are based on the steps involved in preparing training and test(reference) data.  In **Delete and Generate**   we prepare the training data by removing the attribute words from the text and during training teach the model to generate the sentence given content and target style. This is trained the same way a language model is trained. Below is an example. ``` The food was great and the service was excellent. Content: The food was and the service was Training input: <POS> <CON_START> The food was and the service was <START> The food was great and the service was excellent . <END>  The food was awful and the service was slow. Content: The food was and the service was Training input: <NEG> <CON_START> The food was and the service was <START> The food was awful and the service was slow . <END> ``` Cross entropy loss is calculated for all the tokens predicted after **_\<START\>_** token. For inference  we represent target style using the same tags as used in training  and provide the content as inputs to the model. For the case of sentiment style transfer  all the positive sentiment test data sentences will have **_\<NEG\>_** and all negative sentiment sentences will have **_\<POS\>_** token before the content. Below is an example. ``` Negative test data: <POS> <CON_START> the food was and the service was <START>  Positive test data: <NEG> <CON_START> the food was and the service was <START>  ```  In **Delete  Retrieve and Generate**  we prepare the training data similar to the **Delete and Generate** but insted of target text style we specify the exact attributes to be used for generating the sentence from the content. Below is an example. ``` The food was great and the service was excellent. Content: The food was and the service was Training input: <ATTR_WORDS> great excellent <CON_START> The food was and the service was <START> The food was great and the service was excellent . <END>  The food was awful and the service was slow. Content: The food was and the service was Training input: <ATTR_WORDS> awful slow <CON_START> The food was and the service was <START> The food was awful and the service was slow . <END> ``` Otherwise the training is same as the **Delete and Generate**. During inference  to perform style transfer we need to get the attributes of opposite text style  we get it by retrieving similar content from opposite train corpus and use the attribute associated with that. Below can be a good example.     ``` Negative test data: <ATTR_WORDS> great tasty  <CON_START> the food was and the service was <START>  Positive test data: <ATTR_WORDS> blend disappointing <CON_START> the food was and the service was <START>  ```   **The process of style transfer consists of multiple steps.**   **_1. Prepare Training data_**   * Train a classifier which uses attention mechanism. Here we have used [BERT](https://arxiv.org/abs/1810.04805) classifier.   * Use attention scores to prepare data for **Delete and Generate** trainig and test.   * Use the training and testing data of **Delete and Generate** to prepare training and test data for **Delete  Retrieve and Generate** .      **_2. Generator Training_**   * We have use modified version of [OpenAI GPT](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_openai_gpt.py)    * Run training of **Delete and Generate** and **Delete  Retrieve and Generate** .    **_3. Generate sentences_**   * Generate sentences from the test(reference) files.  The following section describes steps required from preparing the data to running inference.    """;Natural Language Processing;https://github.com/agaralabs/transformer-drg-style-transfer
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/saurabhnlp/bert
"""A basic project for chinese chess using Alphazero. https://github.com/walker8088/cchesslib as the cchesslib.  """;Reinforcement Learning;https://github.com/mengyangbai/CchessGo
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/svakulenk0/response_eval
"""Deep Convolutional GAN is one of the most coolest and popular deep learning technique. It is a great improvement upon the [original GAN network](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) that was first introduced by Ian Goodfellow at NIPS 2014. (DCGANs are much more stable than Vanilla GANs) DCGAN uses the same framework of generator and discriminator. This is analogous to solving a two player minimax game: Ideally the goal of the discriminator is to be very sharp in distinguishing between the real and fake data  whereas  generator aims at faking data in such a way that it becomes nearly impossible for the discriminator to classify it as a fake. The below gif shows how quickly dcgan learns the distribution of mnist and generates real looking digits.  ![](https://github.com/AKASHKADEL/dcgan-mnist/blob/master/results/fixed_noise/animated.gif)   """;Computer Vision;https://github.com/AKASHKADEL/dcgan-mnist
"""The hexadecimal 3 byte web color encoding can represent 16<sup>6</sup> ≈ 16.8 million different colors. With a combination of 3 colors there are (16.8x10<sup>6</sup>)<sup>3</sup> ≈ 4.7 billion possible palettes to choose from. This is definitely too much for a person to go through. One possible solution of the problem of finding good matches for a persons preferences of color combination is to let a recommendation system do the bidding.   """;General;https://github.com/Orchidaceae/AI_palette_recommendation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/junhahyung/bert_finetune
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/junhahyung/bert_finetune
"""In this work  we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN  including only the most essential component in GCN—neighborhood aggregation—for collaborative filtering.   """;Graphs;https://github.com/KueipoH/MoreLighterGCN
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/goldenbili/bert_lamb_pretrain
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/goldenbili/bert_lamb_pretrain
"""https://biomedical-engineering-online.biomedcentral.com<br> https://www.kceyeclinic.com<br> https://uvahealth.com<br> https://www.asrs.org<br> https://entokey.com/retina-4/<br> https://www.atlasophthalmology.net/<br>    In this project  deep learning methods were used to predict abnormalities in fundus images from Kaggle. The images in the dataset were augmented and different colorschemes were tried.   Aproach - Collection of information on the field of Funduns Images Illnesses and Epidemiology  for Benchmarking and Orientation - Decision on tasks  metrics  tools and methods - Exploration of dataset - Preprocessing of data - Augmentation of data - Training of Model on various variants of architecture  hyperparameters and data - Summarize results <br> <br>   """;Computer Vision;https://github.com/daimonae/Fundus-Image-Classification
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.   ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    """;Natural Language Processing;https://github.com/Sikun/dlam_project
"""    This program performs segmentation of the left vertricle  myocardium  right ventricle     and backgound of Cardiovascular Magnetic Resonance Images  with use of a convolutional neural network based on the well-known U-Net     architecture  as described by [https://arxiv.org/pdf/1505.04597.pdf](Ronneberger et al.) For each patient  both a 3D end systolic       image and a 3D end diastolic image with its corresponding ground truth segmentation of the left ventricle  myocardium and right         ventricle is available.           The available code first divides the patients data into a training set and a test set. The training data is then loaded from the        stored location and subsequently preprocessed. Preprocessing steps include resampling the image to the same voxel spacing  removal of outliers  normalization  cropping and one-hot encoding of the labels. Before training  the trainingset is subdivided again for training and validation of the model.          For training  a network based on the U-Net architecture is used and implemented with keras. For training  many different variables       can be tweaked  which are described in some detail below. After training  the network is evaluated using the test dataset. This data is loaded and preprocessed in the same way as the training dataset and propagated through the network to obtain pixel-wise predictions for each class. These predictions are probabilities and are thresholded to obtain a binary segmentation.           The binary segmentations are then evaluated by computing the (multiclass) softdice coefficient and the Hausdorff distance between the obtained segmentations and the ground truth segmentations. The softdice coefficients and Hausdorff distances are computed for each image for each individual class and the multiclass softdice for all the classes together. These results are all automatically saved in a text file. Furthermore  the obtained segmentations as an overlay with the original images  the training log and corresponding plots and the model summary are also saved automatically.         Lastly  from the segmentations of the left ventricular cavity during the end systole and end diastole  the ejection fraction is calculated. This value is  alongside the ejection fraction computed from the ground truth segmentations  stored in the same text file with results.       """;Computer Vision;https://github.com/jellevankerk/Team-Challenge
"""``` The task is to generate real images from landscape paintings. ``` Folder structure --------------  ``` ├── datasets/Train/a       - this folder contains landscape images. │   ├── image1001.png │   └── image1002.png │   └── -------------------- │ │ ├── datasets/Train/b      - this folder contains Real-world images. │   ├── image1001.png │   └── image1002.png │   └── --------------------   │ ├── datasets/Test-set/a             - this folder contains Test images(landscapes). │   └── image1001.png │   └── --------------------  │ ├── save_model    -- this folder contains saved model │ │── Python-scripts      - this folder contains  python files(can be run driectly in Jupyter notebook/IDE) │ ├──  train-MANET.py        - this file is used for training image. │    ├──  testing.py         - this file is used for generating test images. │    ├──  result        - this folder contains generated test images. │  └──logs/tensorlogs       ```  """;Computer Vision;https://github.com/Nisnab/Pix2Pix
"""``` The task is to generate real images from landscape paintings. ``` Folder structure --------------  ``` ├── datasets/Train/a       - this folder contains landscape images. │   ├── image1001.png │   └── image1002.png │   └── -------------------- │ │ ├── datasets/Train/b      - this folder contains Real-world images. │   ├── image1001.png │   └── image1002.png │   └── --------------------   │ ├── datasets/Test-set/a             - this folder contains Test images(landscapes). │   └── image1001.png │   └── --------------------  │ ├── save_model    -- this folder contains saved model │ │── Python-scripts      - this folder contains  python files(can be run driectly in Jupyter notebook/IDE) │ ├──  train-MANET.py        - this file is used for training image. │    ├──  testing.py         - this file is used for generating test images. │    ├──  result        - this folder contains generated test images. │  └──logs/tensorlogs       ```  """;General;https://github.com/Nisnab/Pix2Pix
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/duyunshu/bert-sentiment-analysis
"""MMSegmentation is an open source semantic segmentation toolbox based on PyTorch. It is a part of the OpenMMLab project.  The master branch works with **PyTorch 1.5+**.  ![demo image](resources/seg_demo.gif)   """;Computer Vision;https://github.com/open-mmlab/mmsegmentation
"""MMSegmentation is an open source semantic segmentation toolbox based on PyTorch. It is a part of the OpenMMLab project.  The master branch works with **PyTorch 1.5+**.  ![demo image](resources/seg_demo.gif)   """;General;https://github.com/open-mmlab/mmsegmentation
"""This work has been published in the Journal of Neuroscience Methods 2019. MV-LEAP is a framework for boosting the classification of imbalanced multi-view data. MV-LEAP comprises two key steps addressing two major machine learning problems in classification tasks:  **Issue 1: Training data imbalance.**    **Proposed solution ==>** Manifold learning-based proliferator  which enables to generate synthetic data for each view  is proposed to handle imbalanced data.  **Issue 2: Heterogeneity of the input multi-view data to learn from.**  **Proposed solution ==>** A multi-view manifold data alignment leveraging tensor canonical correlation analysis is proposed to map all original (i.e.  ground truth) and proliferated (i.e.  synthesized) views into a shared subspace where their distributions are aligned for the target classification task.  More details can be found at: https://www.sciencedirect.com/science/article/pii/S016502701930202X) or https://www.researchgate.net/publication/334162522_Multi-View_Learning-Based_Data_Proliferator_for_Boosting_Classification_Using_Highly_Imbalanced_Classes  In this repository  we release the MV-LEAP source code trained and tested on a simulated heterogeneous multi-view dataset drawn from 4 Gaussian distributions as shown below:   ![Simulated heterogeneous multi-view dataset](http://basira-lab.com/mvleap_1/)  The classification results by comparison methods and MV-LEAP (ours) are displayed below:  ![Demo classification results](http://basira-lab.com/mvleap_2/)   """;Computer Vision;https://github.com/basiralab/MV-LEAP
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/duyunshu/bert-sentiment-analysis
"""This is a partially official pytorch implementation accompanying the publication  [Flattenet: A Simple and Versatile Framework for Dense Pixelwise Prediction]( http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8932465&isnumber=8600701). This code repo only contains the **semantic segmentation part**. It is worth noting that  a number of **modifications** have been made after the paper has been published in order to  improve the performance. We evaluate the adapted method on PASCAL-Context and PASCAL VOC 2012.  To deal with the reduced feature resolution problem  we introduce a novel Flattening Module  which takes as input the coarse-grained feature maps (patch-wise visual descriptors) produced by  a Fully Convolutional Network (FCN) and then outputs dense pixel-wise visual descriptors.  The process described above is represented in the schematic diagram below.  A FCN equipped with the Flattening Module  which we refer to as FlatteNet  can accomplish various  dense prediction tasks in an effective and efficient manner.  <p align=""center""> <img src=""figures/flattenmodule-1.png"" width=""800""> </p>  An illustration of the structure of Flattening Module is displayed below. We have newly incorporated a context aggregation component into the design  which is implemented as a pyramid pooling module or self-attention module.  <p align=""center""> <img src=""figures/gconv-1.png"" width=""100""> </p>  The overall architecture is displayed below.  <p align=""center"">   <img src=""figures/flattenet-1.png"" width=""200""> </p>   """;Computer Vision;https://github.com/TotalVariation/Flattenet
"""Welcome to the first ""Norwegian Blue Parrot"" project article. For each AI project  I write the code and show you how it works. You can test it using a mobile phone  tablet  and laptop (on the website  https://nbp3-webclient-2020.web.app/). Furthermore  you can see for yourself the effectiveness and the shortcoming of each AI model.  <img src=""https://nbp3-webclient-2020.web.app/image/undraw_true_love_cy8x.svg"" width=""60%"" style=""margin:2rem;"" />  I have developed dozens of AI projects  and with each AI model  I learn a bit more insights  into the world of Artificial Intelligence.  The audience or point of view  (POV) for demystifying AI is primary for AI enthusiasts and friends with a curious mind. However  I am a full-stack programmer  a solution architect  and an AI scientist  and therefore  I will not shy away from the math and the coding  but I will remain steadfast to the primary POV.  Before digging into the technicalities  we will have fun test-driving  the AI model. After we have a firm grasp on ""what"" we are trying to demystify  we will travel a full journey from coding to gathering the data. We will take a break to understand the particular data biases   both the conscious biases and the unforeseen consequences. We will reach the journey conclusion at the use-cases  i.e.  what is the purpose of this AI project  and what other possibilities could this AI model can be used either ethically or  feloniously.  > """;General;https://github.com/duchaba/Norwegian_Blue_Parrot_k2fa_AI
"""Welcome to the first ""Norwegian Blue Parrot"" project article. For each AI project  I write the code and show you how it works. You can test it using a mobile phone  tablet  and laptop (on the website  https://nbp3-webclient-2020.web.app/). Furthermore  you can see for yourself the effectiveness and the shortcoming of each AI model.  <img src=""https://nbp3-webclient-2020.web.app/image/undraw_true_love_cy8x.svg"" width=""60%"" style=""margin:2rem;"" />  I have developed dozens of AI projects  and with each AI model  I learn a bit more insights  into the world of Artificial Intelligence.  The audience or point of view  (POV) for demystifying AI is primary for AI enthusiasts and friends with a curious mind. However  I am a full-stack programmer  a solution architect  and an AI scientist  and therefore  I will not shy away from the math and the coding  but I will remain steadfast to the primary POV.  Before digging into the technicalities  we will have fun test-driving  the AI model. After we have a firm grasp on ""what"" we are trying to demystify  we will travel a full journey from coding to gathering the data. We will take a break to understand the particular data biases   both the conscious biases and the unforeseen consequences. We will reach the journey conclusion at the use-cases  i.e.  what is the purpose of this AI project  and what other possibilities could this AI model can be used either ethically or  feloniously.  > """;Computer Vision;https://github.com/duchaba/Norwegian_Blue_Parrot_k2fa_AI
"""The implementation is based on two papers:  - Simple Online and Realtime Tracking with a Deep Association Metric https://arxiv.org/abs/1703.07402 - YOLOv3: An Incremental Improvement https://arxiv.org/abs/1804.02767   This repository contains a moded version of PyTorch YOLOv3 (https://github.com/ultralytics/yolov3). It filters out every detection that is not a person. The detections of persons are then passed to a Deep Sort algorithm (https://github.com/ZQPei/deep_sort_pytorch) which tracks the persons. The reason behind the fact that it just tracks persons is that the deep association metric is trained on a person ONLY datatset.   """;Computer Vision;https://github.com/mikel-brostrom/Yolov3_DeepSort_Pytorch
"""- シビックテックに新しく興味をもってくれた人に、各地ブリゲードの特徴をわかりやすく伝えたい。 - 既にシビックテック活動をしている人にも、他の地域のブリゲードの特徴がわかるようにしたい。 ![ブリゲードマッピングのきっかけ](img/brigade_mapping_trigger.png) - そこで、各地のブリゲードの性格や得意分野がわかるような「俯瞰図」「得意分野マップ」みたいなものを作れば、どのブリゲードと相性が良いかわかるようになるのでは?   """;Natural Language Processing;https://github.com/siramatu/brigade-visualizer
"""We provide the code for reproducing experiment results of [SCNet](https://arxiv.org/abs/2012.10150).   """;Computer Vision;https://github.com/thangvubk/SCNet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/lehoanganh298/BERT-Question-Answering
"""**Swin Transformer** (the name `Swin` stands for **S**hifted **win**dow) is initially described in [arxiv](https://arxiv.org/abs/2103.14030)  which capably serves as a general-purpose backbone for computer vision. It is basically a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection.  Swin Transformer achieves strong performance on COCO object detection (`58.7 box AP` and `51.1 mask AP` on test-dev) and ADE20K semantic segmentation (`53.5 mIoU` on val)  surpassing previous models by a large margin.  ![teaser](figures/teaser.png)   """;General;https://github.com/microsoft/Swin-Transformer
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/saurabhnlp/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/TSLNIHAOGIT/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/TSLNIHAOGIT/bert
"""  * `hyperparams.py` includes all hyper parameters that are needed.   * `module.py` contains all methods  including attention  feed-forward network and so on.   * `network.py` contains networks including encoder  decoder.   * `symbols.py` contains the definition of vocabulary.   * `utils.py` contains the position encoding method.   * `train.py` is a simple example for training a Transformer.    * `test.py` is a simple example for generating a sentence.   * directory `checkpoints/` contains the models   * directory `runs/` contains the training log file   """;General;https://github.com/DJJune/Transformer
"""  * `hyperparams.py` includes all hyper parameters that are needed.   * `module.py` contains all methods  including attention  feed-forward network and so on.   * `network.py` contains networks including encoder  decoder.   * `symbols.py` contains the definition of vocabulary.   * `utils.py` contains the position encoding method.   * `train.py` is a simple example for training a Transformer.    * `test.py` is a simple example for generating a sentence.   * directory `checkpoints/` contains the models   * directory `runs/` contains the training log file   """;Natural Language Processing;https://github.com/DJJune/Transformer
"""Here we introduce RainNet -- a convolutional neural network for radar-based precipitation nowcasting. RainNet was trained to predict continuous precipitation intensities at a lead time of five minutes  using several years of quality-controlled weather radar composites provided by the German Weather Service (DWD).   The source code of the RainNet model written using [_Keras_](https://keras.io) functional API is in the file `rainnet.py`.  The pretrained instance of `keras` `Model` for RainNet  as well as RainNet's pretrained weights are available on Zenodo:   [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3630429.svg)](https://doi.org/10.5281/zenodo.3630429)   """;General;https://github.com/hydrogo/rainnet
"""This is a implemention of the customization yolov3 object detection algorithm based on darknet framework. Features are listed bollow. * 1st: Adding the Attributes classification based on the Pre_box  such as overlapping  head or tail  day or night  etc.  * 2nd: Several image augmentation schemes have been added  such as: image block overlap  to make yolo higher recall. Ground glass effect is applied to make the edge of object more stable and steady. * 3rd: Based on the existing model  write the bounding boxes of the objects in a image into the xml file to assist further annotation. we provide all related source code  and corresponding executable file can be generated. For complete darknet code click here:https://pjreddie.com/darknet/install/  """;Computer Vision;https://github.com/FlyingAnt2018/darknet-modify
"""DenseNet is a network architecture where each layer is directly connected to every other layer in a feed-forward fashion (within each *dense block*). For each layer  the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. This connectivity pattern yields state-of-the-art accuracies on CIFAR10/100 (with or without data augmentation) and SVHN. On the large scale ILSVRC 2012 (ImageNet) dataset  DenseNet achieves a similar accuracy as ResNet  but using less than half the amount of parameters and roughly half the number of FLOPs.  <img src=""https://cloud.githubusercontent.com/assets/8370623/17981494/f838717a-6ad1-11e6-9391-f0906c80bc1d.jpg"" width=""480"">  Figure 1: A dense block with 5 layers and growth rate 4.    ![densenet](https://cloud.githubusercontent.com/assets/8370623/17981496/fa648b32-6ad1-11e6-9625-02fdd72fdcd3.jpg) Figure 2: A deep DenseNet with three dense blocks.     """;General;https://github.com/liuzhuang13/DenseNet
"""DenseNet is a network architecture where each layer is directly connected to every other layer in a feed-forward fashion (within each *dense block*). For each layer  the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. This connectivity pattern yields state-of-the-art accuracies on CIFAR10/100 (with or without data augmentation) and SVHN. On the large scale ILSVRC 2012 (ImageNet) dataset  DenseNet achieves a similar accuracy as ResNet  but using less than half the amount of parameters and roughly half the number of FLOPs.  <img src=""https://cloud.githubusercontent.com/assets/8370623/17981494/f838717a-6ad1-11e6-9391-f0906c80bc1d.jpg"" width=""480"">  Figure 1: A dense block with 5 layers and growth rate 4.    ![densenet](https://cloud.githubusercontent.com/assets/8370623/17981496/fa648b32-6ad1-11e6-9625-02fdd72fdcd3.jpg) Figure 2: A deep DenseNet with three dense blocks.     """;Computer Vision;https://github.com/liuzhuang13/DenseNet
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/bleedingfight/caffe-env
"""![alt text](https://github.com/saeedabi1/deep_learning_project/blob/master/pictures/Screen%20Shot%202020-05-23%20at%205.08.25%20PM.png?raw=true)    ![alt text](https://github.com/saeedabi1/deep_learning_project/blob/master/pictures/Screen%20Shot%202020-05-23%20at%205.08.39%20PM.png?raw=true)    Attention mechanisms are broadly used in present image captioning encoder / decoder frameworks  where at each step a weighted average is generated on encoded vectors to direct the process of caption decoding.  However  the decoder has no knowledge of whether or how well the vector being attended and the attention question being given are related  which may result in the decoder providing erroneous results.  Image captioning  that is to say generating natural automatic descriptions of language images are useful for visually impaired images and for the quest of natural language related pictures.  It is significantly more demanding than traditional vision tasks recognition of objects and classification of images for two guidelines.  First  well formed structured output space natural language sentences are considerably more challenging than just a set of class labels to predict.  Secondly  this dynamic output space enables a more thin understanding of the visual scenario  and therefore also a more informative one visual scene analysis to do well on this task.   """;General;https://github.com/varun-bhaseen/Image-caption-generation-using-attention-model
"""Pytorch based implementation of faster rcnn framework.For details about faster R-CNN please refer to the paper [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497) by Shaoqing Ren  Kaiming He  Ross Girshick  Jian Sun   This detection framework has the following features:   * It can be run as pure python code  and also pure based on pytorch framework  no need to build * It is easily trained by only running a train.py script  just set the data root dir * It has many backbone networks. like vgg  resnet-fpn  mobilenet  high resolution net(HRNet) * It can be a really detection framework. You only need to change super parameters in config file and get different models to compare different model * It's memory-efficient (about 3GB for vgg16)  """;Computer Vision;https://github.com/AlphaJia/pytorch-faster-rcnn
"""**RepPoints**  initially described in [arXiv](https://arxiv.org/abs/1904.11490)  is a new representation method for visual objects  on which visual understanding tasks are typically centered. Visual object representation  aiming at both geometric description and appearance feature extraction  is conventionally achieved by `bounding box + RoIPool (RoIAlign)`. The bounding box representation is convenient to use; however  it provides only a rectangular localization of objects that lacks geometric precision and may consequently degrade feature quality. Our new representation  RepPoints  models objects by a `point set` instead of a `bounding box`  which learns to adaptively position themselves over an object in a manner that circumscribes the object’s `spatial extent` and enables `semantically aligned feature extraction`. This richer and more flexible representation maintains the convenience of bounding boxes while facilitating various visual understanding applications. This repo demonstrated the effectiveness of RepPoints for COCO object detection.  Another feature of this repo is the demonstration of an `anchor-free detector`  which can be as effective as state-of-the-art anchor-based detection methods. The anchor-free detector can utilize either `bounding box` or `RepPoints` as the basic object representation.  <div align=""center"">   <img src=""demo/reppoints.png"" width=""400px"" />   <p>Learning RepPoints in Object Detection.</p> </div>   """;Computer Vision;https://github.com/nuannuanhcc/ps_reppoint
"""[![Watch the video](figures/video_figure.png)](https://www.youtube.com/watch?v=a_OeT8MXzWI&feature=youtu.be)   """;Computer Vision;https://github.com/seulkiyeom/once-for-all
"""[![Watch the video](figures/video_figure.png)](https://www.youtube.com/watch?v=a_OeT8MXzWI&feature=youtu.be)   """;General;https://github.com/seulkiyeom/once-for-all
"""* Deep Convolution Generative Adversarial Networks (DCGANs) belong to a set of algorithms called generative models  which are widely used for unupervised learning tasks which aim to learn the underlying structure of the given data.   * Simple GANs allow you to generate new unseen data that mimic the actual given real data. However  GANs pose problems in training and require carefullly tuned hyperparameters.  * DCGAN aims to solve this problem by explicitly using convolutional and convolutional-transpose layers in the discriminator and generator  respectively.   * DCGANs basically convert the laplacian pyramid technique (many pairs of G and D to progressively upscale an image) to a single pair of G and D.   """;Computer Vision;https://github.com/nakul-jindal/dcgan-in-pytorch
"""This is an official pytorch implementation of [*Deep High-Resolution Representation Learning for Human Pose Estimation*](https://arxiv.org/abs/1902.09212).  In this work  we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods **recover high-resolution representations from low-resolution representations** produced by a high-to-low resolution network. Instead  our proposed network **maintains high-resolution representations** through the whole process. We start from a high-resolution subnetwork as the first stage  gradually add high-to-low resolution subnetworks one by one to form more stages  and connect the mutli-resolution subnetworks **in parallel**. We conduct **repeated multi-scale fusions** such that each of the high-to-low resolution representations receives information from other parallel representations over and over  leading to rich high-resolution representations. As a result  the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. </br>  ![Illustrating the architecture of the proposed HRNet](/figures/hrnet.png)  """;Computer Vision;https://github.com/gox-ai/hrnet-pose-api
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/VishalChak/temp
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/lehoanganh298/BERT-Question-Answering
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/ravenagcaoili/yolov3
"""Population Based Augmentation (PBA) is a algorithm that quickly and efficiently learns data augmentation functions for neural network training. PBA matches state-of-the-art results on CIFAR with one thousand times less compute  enabling researchers and practitioners to effectively learn new augmentation policies using a single workstation GPU.  This repository contains code for the work ""Population Based Augmentation: Efficient Learning of Augmentation Schedules"" (http://arxiv.org/abs/1905.05393) in TensorFlow and Python. It includes training of models with the reported augmentation schedules and discovery of new augmentation policy schedules.  See below for a visualization of our augmentation strategy.  <p align=""center""> <img src=""figs/augs_v2_crop.png"" width=""40%""> </p>   """;Computer Vision;https://github.com/Zhiwei-Z/pba_experiment
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/hoangtrungchinh/Bert-SQuAD-v2
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/hoangtrungchinh/Bert-SQuAD-v2
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/fanneng/caffe
"""The `neural_tangents` (`nt`) package contains the following modules and functions:  * `stax` - primitives to construct neural networks like `Conv`  `Relu`  `serial`  `parallel` etc.  * `predict` - predictions with infinite networks:    * `predict.gradient_descent_mse` - inference with a single infinite width / linearized network trained on MSE loss with continuous gradient descent for an arbitrary finite or infinite (`t=None`) time. Computed in closed form.    * `predict.gradient_descent` - inference with a single infinite width / linearized network trained on arbitrary loss with continuous (momentum) gradient descent for an arbitrary finite time. Computed using an ODE solver.    * `predict.gradient_descent_mse_ensemble` - inference with an infinite ensemble of infinite width networks  either fully Bayesian (`get='nngp'`) or inference with MSE loss using continuous gradient descent (`get='ntk'`). Finite-time Bayesian inference (e.g. `t=1.  get='nngp'`) is interpreted as gradient descent on the top layer only [[11]](#11-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent)  since it converges to exact Gaussian process inference with NNGP (`t=None  get='nngp'`). Computed in closed form.    * `predict.gp_inference` - exact closed form Gaussian process inference using NNGP (`get='nngp'`)  NTK (`get='ntk'`)  or both (`get=('nngp'  'ntk')`). Equivalent to `predict.gradient_descent_mse_ensemble` with `t=None` (infinite training time)  but has a slightly different API (accepting precomputed kernel matrix `k_train_train` instead of `kernel_fn` and `x_train`).  * `monte_carlo_kernel_fn` - compute a Monte Carlo kernel estimate  of _any_ `(init_fn  apply_fn)`  not necessarily specified via `nt.stax`  enabling the kernel computation of infinite networks without closed-form expressions.  * Tools to investigate training dynamics of _wide but finite_ neural networks  like `linearize`  `taylor_expand`  `empirical_kernel_fn` and more. See [Training dynamics of wide but finite networks](#training-dynamics-of-wide-but-finite-networks) for details.    """;General;https://github.com/google/neural-tangents
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/sguo35/yolov3
"""The paper Image-to-Image Translation with Conditional Adversarial Networks (https://arxiv.org/pdf/1611.07004.pdf) showed that a general purpose solution could be made for image-to-image translation. Since the time that this paper was published  multiple artists and researchers have made their own models and experiments  with stunning results. These models range from creating cats out of drawings to creating videos of the sea by using an input from general household appliances.<br> The objective of the pix2pix model is to find a model that can map one picture to a desired paired image  which is indistinguishable from the real thing. An example is shown in Figure 1  where 4 different models attempt the mapping from the pixelated image to the real image. Pix2pix uses Conditional Generative Adversarial Networks to achieve this objective. Conditional means that the loss here is structured  there exists a conditional dependency between the pixels  meaning that the loss of one pixel is influenced by the loss of another. The loss function that is used by the model is shown in Equation 1.  <p align=""center"">   <img src=""/ImagesInText/LossFunction.png"" width=""60%"" height=""60%""><br>   Equation 1 [P. Isola et al. (https://arxiv.org/pdf/1611.07004.pdf)] </p>  The model can then be trained and results evaluated  which has been done for a great variety of experiments. We wanted to see if another application could be made  that of image restoration of blurry pictures. We have most likely all seen a movie or tv-series in which a spy agency needed someone to ""enhance"" a photo in order to see smaller details  with the advent of deep learning these techniques are becoming more of a reality. We wanted to see if this general architecture of pix2pix for image translation could also be used for this application to see if you could enhance your images by using pix2pix.  This blog first starts with the method of our project  what exactly is the type of data that we investigate and in what type of datasets they are stored. This is followed up by an explanation about the hyperparameter tuning that was performed and why this was important. The last part of the method is how we would evaluate our results. The method is followed up by our experiments  which also gives a sample of the data that we used as well as the result of some of our experiments  these results are then discussed in our discussion as well with our conclusion about the experiment if pix2pix can be used for image restoration.   """;Computer Vision;https://github.com/PieterBijl/Group28
"""The paper Image-to-Image Translation with Conditional Adversarial Networks (https://arxiv.org/pdf/1611.07004.pdf) showed that a general purpose solution could be made for image-to-image translation. Since the time that this paper was published  multiple artists and researchers have made their own models and experiments  with stunning results. These models range from creating cats out of drawings to creating videos of the sea by using an input from general household appliances.<br> The objective of the pix2pix model is to find a model that can map one picture to a desired paired image  which is indistinguishable from the real thing. An example is shown in Figure 1  where 4 different models attempt the mapping from the pixelated image to the real image. Pix2pix uses Conditional Generative Adversarial Networks to achieve this objective. Conditional means that the loss here is structured  there exists a conditional dependency between the pixels  meaning that the loss of one pixel is influenced by the loss of another. The loss function that is used by the model is shown in Equation 1.  <p align=""center"">   <img src=""/ImagesInText/LossFunction.png"" width=""60%"" height=""60%""><br>   Equation 1 [P. Isola et al. (https://arxiv.org/pdf/1611.07004.pdf)] </p>  The model can then be trained and results evaluated  which has been done for a great variety of experiments. We wanted to see if another application could be made  that of image restoration of blurry pictures. We have most likely all seen a movie or tv-series in which a spy agency needed someone to ""enhance"" a photo in order to see smaller details  with the advent of deep learning these techniques are becoming more of a reality. We wanted to see if this general architecture of pix2pix for image translation could also be used for this application to see if you could enhance your images by using pix2pix.  This blog first starts with the method of our project  what exactly is the type of data that we investigate and in what type of datasets they are stored. This is followed up by an explanation about the hyperparameter tuning that was performed and why this was important. The last part of the method is how we would evaluate our results. The method is followed up by our experiments  which also gives a sample of the data that we used as well as the result of some of our experiments  these results are then discussed in our discussion as well with our conclusion about the experiment if pix2pix can be used for image restoration.   """;General;https://github.com/PieterBijl/Group28
"""* **train.py** description for parameters(partly)    | Parameters           | Default                                            | Description                              |   | -------------------- | -------------------------------------------------- | ---------------------------------------- |   | **--image_dir**      | str: ‘/media/gallifrey/DJW/Dataset/Imagenet/train’ | Path for training data                   |   | **--continue_train** | bool: False                                        | wheather to continue training            |   | **--which_epoch**    | str: 'latest'                                      | start checkpoint                         |   | **--num_epoch**      | int: 20                                            | number of epoches                        |   | **--lr**             | float: 3.16e-5                                     | initial learning rate                    |   | **--rebalance**      | bool: True                                         | color rebalance or not                   |   | **--NN**             | int: 5                                             | number of neighor for KNN                |   | **--sigma**          | float: 5.0                                         | kernal size for gussian                  |   | **--gamma**          | float: 0.5                                         | coefficient for mixture of distributions |  * **test.py** description for parameters(partly)    | Parameters        | Default                                           | Description            |   | ----------------- | ------------------------------------------------- | ---------------------- |   | **--model_path**  | str： './model'                                   | path for models        |   | **--image_dir**   | str： ‘/media/gallifrey/DJW/Dataset/Imagenet/val' | path for testing data  |   | **--load_model**  | str: '19'                                         | checkpoint ID          |   | **--result_path** | str: './result'                                   | result path            |   | **--max_samples** | int: int(sys.maxsize)                             | max number to generate |     ![architecture](./imgs/architecture.png)  This project is based on the PaddlePaddle framework to reproduce the classical image colorization paper CIC (Colorful Image Colorization)  CIC is able to model the color channels for grayscale input and recover the color of the image. The innovation of this paper is to consider the prediction of color channels (ab) as a classification task  i.e.  the real ab channels are first encoded into 313 bins  and the forward process of the model is equivalent to performing 313 class classification. At the same time  in order to solve the problem that the image recovered color is affected by a large unsaturated area such as the background  the loss of each pixel is weighted according to the prior distribution of ab  which is essentially equivalent to doing color class balancing.  **Paper**  * [1] Zhang R    Isola P    Efros A A . Colorful Image Colorization[C]// European Conference on Computer Vision. Springer International Publishing  2016.  **Projects**  * [Official Caffe Implement](https://github.com/richzhang/colorization/tree/caffe) * [Unofficial Pytorch implement](https://github.com/Epiphqny/Colorization)  **Online Operation**  * Ai Studio job project：[https://aistudio.baidu.com/aistudio/clusterprojectdetail/2304371](https://aistudio.baidu.com/aistudio/clusterprojectdetail/2304371)     """;General;https://github.com/Callifrey/Paddle-CIC
"""Official Implementation of our pSp paper for both training and evaluation. The pSp method extends the StyleGAN model to  allow solving different image-to-image translation problems using its encoder.   """;Computer Vision;https://github.com/eladrich/pixel2style2pixel
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/tair-ai/caffe
"""This notebook contains the deep convolutiona neural network trained to distinguish the numbers shown by fingers on images. It develops on existing VGG neural network and uses the altered loss function to generate the novel images. The original network could be found: https://arxiv.org/abs/1512.03385 The original resnet weights could be found: https://www.kaggle.com/keras/resnet50  It has been developed as a part of the course deeplearning.ai The notebook contains the code and explanations.   Copyright 2020 Deeplearning.ai  """;General;https://github.com/abuchin/resnet_sign
"""This notebook contains the deep convolutiona neural network trained to distinguish the numbers shown by fingers on images. It develops on existing VGG neural network and uses the altered loss function to generate the novel images. The original network could be found: https://arxiv.org/abs/1512.03385 The original resnet weights could be found: https://www.kaggle.com/keras/resnet50  It has been developed as a part of the course deeplearning.ai The notebook contains the code and explanations.   Copyright 2020 Deeplearning.ai  """;Computer Vision;https://github.com/abuchin/resnet_sign
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/wangrui1996/caffe-ssd
"""This repo is an unofficial implementation of [IoU Loss for 2D/3D Object Detection](https://arxiv.org/pdf/1908.03851.pdf). It contains the Pytorch function which calculates the intersection area of oriented rectangles using GPU.   """;Computer Vision;https://github.com/lilanxiao/Rotated_IoU
"""Image super-resolution (SR)  which refers to the process of recovering high- resolution (HR) images from low-resolution (LR) images  is an important class of image processing techniques in computer vision. In general  this problem is very challenging and inherently ill posed since there are always multiple HR images for a single LR image but with the rapid development of deep learning techniques  Super-resolution models based on Deep learning have been extensively explored and they often achieve state-of-the-art performance on different benchmarks of Super-Resolution.   """;General;https://github.com/aba450/Super-Resolution
"""Image super-resolution (SR)  which refers to the process of recovering high- resolution (HR) images from low-resolution (LR) images  is an important class of image processing techniques in computer vision. In general  this problem is very challenging and inherently ill posed since there are always multiple HR images for a single LR image but with the rapid development of deep learning techniques  Super-resolution models based on Deep learning have been extensively explored and they often achieve state-of-the-art performance on different benchmarks of Super-Resolution.   """;Computer Vision;https://github.com/aba450/Super-Resolution
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/xyfer17/ssd-caffe
"""WGAN networks were argued to have higher quality output thanks to the use of the Wasser-Stein loss  which: . avoids the vanishing/exploding gradient issue normally associated with the KL/JA divergence losses . Adds a score to the realism of an image  as opposed to the sigmoid binary output which effectively performs classification.    """;Computer Vision;https://github.com/triple7/Keras-WGAN-RGB-128x128
"""> A team of radiologists from New Orleans studied the usefulness of Chest Radiographs for diagnosing COVID-19 compared to the reverse-transcription polymerase chain reaction (RT-PCR) and found out they could aid rapid diagnosis  especially in areas with limited testing facilities [[1]](https://pubs.rsna.org/doi/10.1148/ryct.2020200280 ""A Characteristic Chest Radiographic Pattern in the Setting of the COVID-19 Pandemic"").<br> > Another study found out that the radiographs of different viral cases of pneumonia are comparative  and they overlap with other infectious and inflammatory lung diseases  making it hard for radiologists to recognize COVID‐19 from other viral pneumonia cases [[2]](https://pubs.rsna.org/doi/10.1148/rg.2018170048 ""Radiographic and CT Features of Viral Pneumonia"").<br> > This project aims to make the former study a reality while dealing with the intricacies in the latter  with the help of Deep Learning.<br>   """;General;https://github.com/priyavrat-misra/xrays-and-gradcam
"""> A team of radiologists from New Orleans studied the usefulness of Chest Radiographs for diagnosing COVID-19 compared to the reverse-transcription polymerase chain reaction (RT-PCR) and found out they could aid rapid diagnosis  especially in areas with limited testing facilities [[1]](https://pubs.rsna.org/doi/10.1148/ryct.2020200280 ""A Characteristic Chest Radiographic Pattern in the Setting of the COVID-19 Pandemic"").<br> > Another study found out that the radiographs of different viral cases of pneumonia are comparative  and they overlap with other infectious and inflammatory lung diseases  making it hard for radiologists to recognize COVID‐19 from other viral pneumonia cases [[2]](https://pubs.rsna.org/doi/10.1148/rg.2018170048 ""Radiographic and CT Features of Viral Pneumonia"").<br> > This project aims to make the former study a reality while dealing with the intricacies in the latter  with the help of Deep Learning.<br>   """;Computer Vision;https://github.com/priyavrat-misra/xrays-and-gradcam
"""- The wrong connection and lack joint error is 256px smaller than 321px. Sometimes there is missing part.  - In terms of time  256px is almost twice as fast as 321px.  - PifPaf is often wrong in the wrong connection  missing part error. In addition  MPII and COCO dataset 256px and 321px focus on other error.  - In sports environment  PifPaf focuses on wrong connection error (60-70% of total error).   The recent methods of estimating the human posture in two-dimensional space based on deep learning have shown better applicability and results than before. However  the problem also faced many different challenges such as in crowds  resolution  lighting  ... In this project  I analyze and evaluate pros and cons of the article “Pifpaf: Composite Fields For Human Pose Estimation ”  the author has focused on solving the challenge of occluded and low resolution. I analyze on different datasets: COCO dataset  in addition I analyze errors on 1000 MPII images  2000 images of sports dataset collected by us. Thereby having a more general view of the article  and thereby giving directions to develop research to improve the article.   This project based on paper : https://arxiv.org/abs/1903.06593 and code: https://github.com/vita-epfl/openpifpaf  I analyze and evaluate on datasets:4000 images COCO test-dev 2000 images sports 1000 images MPII datasets:  Because the author focused on solving the challenge on low resolution and obscuration. I resized the images to 3 different resolutions: 256px  321px  641px. The author have suggested that 321px is the best but I want to experiment with 256px more  then I can conclude that 321px is the best or not.  Dataset: https://drive.google.com/drive/folders/19xFqlgraUi7BZp9VgBUY_UfC6u4gYNyY?usp=sharing  Analysis: https://drive.google.com/drive/folders/1BUkAbabOqjFUhi2_RlYds0vFS7Wg4ryB?usp=sharing   """;Computer Vision;https://github.com/thanhtrung98/Pose_estimation
"""This package contains the source code implementation of the paper ""Discovering Autoregressive Orderings with Variational Inference"".  It implements the Variational Order Inference (VOI) algorithm mentioned in the paper  where the encoder generates nonsequential autoregressive orders as the latent variable  and the decoder maximizes the joint probability of generating the target sequence under these nonsequential orders. In conditional text generation tasks  the encoder is implemented as non-causal Transformer  and the decoder is implemented as Transformer-InDIGO (Gu et al.  2019) which generates target sequences through insertion.  Taking away the encoder Transformer (we also call it as the Permutation Transformer (PT)  which outputs latent orderings) and the VOI algorithm  this repo is also a standalone implementation of Transformer-INDIGO. Training Transformer-INDIGO with left-to-right ordering is equivalent to training a Transformer with relative position representations ([Link](https://arxiv.org/abs/1803.02155)) (Shaw et al.  2018).   """;General;https://github.com/anonymouscode115/autoregressive_inference
"""**Swin Transformer** (the name `Swin` stands for **S**hifted **win**dow) is initially described in [arxiv](https://arxiv.org/abs/2103.14030)  which capably serves as a general-purpose backbone for computer vision. It is basically a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection.  Swin Transformer achieves strong performance on COCO object detection (`58.7 box AP` and `51.1 mask AP` on test-dev) and ADE20K semantic segmentation (`53.5 mIoU` on val)  surpassing previous models by a large margin.  ![teaser](figures/teaser.png)   """;Computer Vision;https://github.com/microsoft/Swin-Transformer
"""Compact Convolutional Transformers not only use the sequence pooling but also replace the patch embedding with a convolutional embedding  allowing for better inductive bias and making positional embeddings optional. CCT achieves better accuracy than ViT-Lite and CVT and increases the flexibility of the input parameters.  ![Comparison](images/comparison.png)   Compact Vision Transformers better utilize information with Sequence Pooling post  encoder  eliminating the need for the class token while achieving better accuracy.   	 [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/escaping-the-big-data-paradigm-with-compact/image-classification-on-flowers-102)](https://paperswithcode.com/sota/image-classification-on-flowers-102?p=escaping-the-big-data-paradigm-with-compact)  Preprint Link: [Escaping the Big Data Paradigm with Compact Transformers ](https://arxiv.org/abs/2104.05704)  By [Ali Hassani<sup>[1]</sup><span>&#42;</span>](https://alihassanijr.com/)  [Steven Walton<sup>[1]</sup><span>&#42;</span>](https://stevenwalton.github.io/)  [Nikhil Shah<sup>[1]</sup>](https://itsshnik.github.io/)  [Abulikemu Abuduweili<sup>[1]</sup>](https://github.com/Walleclipse)  [Jiachen Li<sup>[1 2]</sup>](https://chrisjuniorli.github.io/)   and [Humphrey Shi<sup>[1 2 3]</sup>](https://www.humphreyshi.com/)   <small><span>&#42;</span>Ali Hassani and Steven Walton contributed equal work</small>  In association with SHI Lab @ University of Oregon<sup>[1]</sup> and UIUC<sup>[2]</sup>  and Picsart AI Research (PAIR)<sup>[3]</sup>   ![model-sym](images/model_sym.png)   """;Computer Vision;https://github.com/SHI-Labs/Compact-Transformers
"""Dassl is a [PyTorch](https://pytorch.org) toolbox initially developed for our project [Domain Adaptive Ensemble Learning (DAEL)](https://arxiv.org/abs/2003.07325) to support research in domain adaptation and generalization---since in DAEL we study how to unify these two problems in a single learning framework. Given that domain adaptation is closely related to semi-supervised learning---both study how to exploit unlabeled data---we also incorporate components that support research for the latter.  Why the name ""Dassl""? Dassl combines the initials of domain adaptation (DA) and semi-supervised learning (SSL)  which sounds natural and informative.  Dassl has a modular design and unified interfaces  allowing fast prototyping and experimentation of new DA/DG/SSL methods. With Dassl  a new method can be implemented with only a few lines of code. Don't believe? Take a look at the [engine](https://github.com/KaiyangZhou/Dassl.pytorch/tree/master/dassl/engine) folder  which contains the implementations of many existing methods (then you will come back and star this repo). :-)  Basically  Dassl is perfect for doing research in the following areas: - Domain adaptation - Domain generalization - Semi-supervised learning  BUT  thanks to the neat design  Dassl can also be used as a codebase to develop any deep learning projects  like [this](https://github.com/KaiyangZhou/CoOp). :-)  A drawback of Dassl is that it doesn't (yet? hmm) support distributed multi-GPU training (Dassl uses `DataParallel` to wrap a model  which is less efficient than `DistributedDataParallel`).  We don't provide detailed documentations for Dassl  unlike another [project](https://kaiyangzhou.github.io/deep-person-reid/) of ours. This is because Dassl is developed for research purpose and as a researcher  we think it's important to be able to read source code and we highly encourage you to do so---definitely not because we are lazy. :-)   """;General;https://github.com/KaiyangZhou/Dassl.pytorch
"""This method now is just support for Inverted-Residual block[[1]](#ref) and Squeeze-and-Excitation block[[2]](#ref). Here use the characteristics of these two blocks  the internal channels of the block are pruned without affecting the output channel size  as shown below.   """;Computer Vision;https://github.com/Gideon0805/Tensorflow1.15-Model-Pruning
"""This method now is just support for Inverted-Residual block[[1]](#ref) and Squeeze-and-Excitation block[[2]](#ref). Here use the characteristics of these two blocks  the internal channels of the block are pruned without affecting the output channel size  as shown below.   """;General;https://github.com/Gideon0805/Tensorflow1.15-Model-Pruning
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Chonwai/Learning_BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Chonwai/Learning_BERT
"""MMTracking is an open source video perception toolbox based on PyTorch. It is a part of the OpenMMLab project.  The master branch works with **PyTorch1.5+**.  <div align=""left"">   <img src=""https://user-images.githubusercontent.com/24663779/103343312-c724f480-4ac6-11eb-9c22-b56f1902584e.gif"" width=""800""/> </div>   """;Computer Vision;https://github.com/open-mmlab/mmtracking
"""This repository is a fork of the Nathan Sprague implementation of the deep Q-learning algorithm described in:  [Playing Atari with Deep Reinforcement Learning](http://arxiv.org/abs/1312.5602) Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Alex Graves  Ioannis Antonoglou  Daan Wierstra  Martin Riedmiller  and   Mnih  Volodymyr  et al. ""Human-level control through deep reinforcement learning."" Nature 518.7540 (2015): 529-533.  We use the DQN algorithm to learn the strategies for Atari games using the RAM state of the machine.   """;Reinforcement Learning;https://github.com/sygi/deep_q_rl
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/FengJiaChunFromSYSU/Bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/FengJiaChunFromSYSU/Bert
"""We trained the model on the dataset with NSFW images as positive and SFW(suitable for work) images as negative. These images were editorially labelled. We cannot release the dataset or other details due to the nature of the data.   We use [CaffeOnSpark](https://github.com/yahoo/CaffeOnSpark) which is a wonderful framework for distributed learning that brings deep learning to Hadoop and Spark clusters for training models for our experiments. Big thanks to the CaffeOnSpark team!  The deep model was first pretrained on ImageNet 1000 class dataset. Then we finetuned the weights on the NSFW dataset. We used the thin resnet 50 1by2 architecture as the pretrained network. The model was generated using [pynetbuilder](https://github.com/jay-mahadeokar/pynetbuilder) tool and replicates the [residual network](https://arxiv.org/pdf/1512.03385v1.pdf) paper's 50 layer network (with half number of filters in each layer).  You can find more details on how the model was generated and trained [here](https://github.com/jay-mahadeokar/pynetbuilder/tree/master/models/imagenet)  Please note that deeper networks  or networks with more filters can improve accuracy. We train the model using a thin residual network architecture  since it provides good tradeoff in terms of accuracy  and the model is light-weight in terms of runtime (or flops) and memory (or number of parameters).   """;General;https://github.com/yahoo/open_nsfw
"""We trained the model on the dataset with NSFW images as positive and SFW(suitable for work) images as negative. These images were editorially labelled. We cannot release the dataset or other details due to the nature of the data.   We use [CaffeOnSpark](https://github.com/yahoo/CaffeOnSpark) which is a wonderful framework for distributed learning that brings deep learning to Hadoop and Spark clusters for training models for our experiments. Big thanks to the CaffeOnSpark team!  The deep model was first pretrained on ImageNet 1000 class dataset. Then we finetuned the weights on the NSFW dataset. We used the thin resnet 50 1by2 architecture as the pretrained network. The model was generated using [pynetbuilder](https://github.com/jay-mahadeokar/pynetbuilder) tool and replicates the [residual network](https://arxiv.org/pdf/1512.03385v1.pdf) paper's 50 layer network (with half number of filters in each layer).  You can find more details on how the model was generated and trained [here](https://github.com/jay-mahadeokar/pynetbuilder/tree/master/models/imagenet)  Please note that deeper networks  or networks with more filters can improve accuracy. We train the model using a thin residual network architecture  since it provides good tradeoff in terms of accuracy  and the model is light-weight in terms of runtime (or flops) and memory (or number of parameters).   """;Computer Vision;https://github.com/yahoo/open_nsfw
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/semal/bert
"""This project is a pytorch implementation of the baseline  RFCN in the Detect to Track paper.  This repository is influenced by the following implementations:  * [jwyang/faster-rcnn.pytorch](https://github.com/jwyang/faster-rcnn.pytorch)  based on Pytorch  * [rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn)  based on Pycaffe + Numpy  * [longcw/faster_rcnn_pytorch](https://github.com/longcw/faster_rcnn_pytorch)  based on Pytorch + Numpy  * [endernewton/tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn)  based on TensorFlow + Numpy  * [ruotianluo/pytorch-faster-rcnn](https://github.com/ruotianluo/pytorch-faster-rcnn)  Pytorch + TensorFlow + Numpy  Our implementation stems heavily from the work  [jwyang/faster-rcnn.pytorch](https://github.com/jwyang/faster-rcnn.pytorch).  As in that implementation  this repository has the following qualities:   * **It is pure Pytorch code**. We convert all the numpy implementations to pytorch!  * **It supports multi-image batch training**. We revise all the layers  including dataloader  rpn  roi-pooling  etc.  to support multiple images in each minibatch.  * **It supports multiple GPUs training**. We use a multiple GPU wrapper (nn.DataParallel here) to make it flexible to use one or more GPUs  as a merit of the above two features.  * **It is memory efficient**. We limit the aspect ratio of the images in each roidb and group images  with similar aspect ratios into a minibatch. As such  we can train resnet101 with batchsize = 2 (4 images) on a 2 Titan X (12 GB).   * **Supports 4 pooling methods**. roi pooling  roi alignment  roi cropping  and position-sensitive roi pooling.  More importantly  we modify all of them to support multi-image batch training.   """;Computer Vision;https://github.com/Feynman27/pytorch-detect-rfcn
"""This repository reproduces ""Zhu et al. Feature Selective Anchor-Free Module for Single-Shot Object Detection. CVPR  2019."" (FSAF) [PDF](https://arxiv.org/pdf/1903.00621.pdf) in PyTorch. The implementation is based on MMDetection framework. All the codes for the FSAF model follow the original paper.    """;Computer Vision;https://github.com/hdjang/Feature-Selective-Anchor-Free-Module-for-Single-Shot-Object-Detection
"""* [`temporal_classification.py`](https://github.com/eminorhan/baby-vision/blob/master/temporal_classification.py): trains temporal classification models as described in the paper. This file uses code recycled from the PyTorch ImageNet training [example](https://github.com/pytorch/examples/tree/master/imagenet). * [`read_saycam.py`](https://github.com/eminorhan/baby-vision/blob/master/read_saycam.py): SAYCam video-to-image reader. * [`moco`](https://github.com/eminorhan/baby-vision/tree/master/moco) directory contains helper files for training static and temporal MoCo models. The code here was modified from [Facebook's MoCo repository](https://github.com/facebookresearch/moco). * [`moco_img.py`](https://github.com/eminorhan/baby-vision/blob/master/moco_img.py): trains an image-based MoCo model as described in the paper. This code was modified from [Facebook's MoCo repository](https://github.com/facebookresearch/moco). * [`moco_temp.py`](https://github.com/eminorhan/baby-vision/blob/master/moco_temp.py): trains a temporal MoCo model as described in the paper. This code was also modified from [Facebook's MoCo repository](https://github.com/facebookresearch/moco). * [`moco_utils.py`](https://github.com/eminorhan/baby-vision/blob/master/moco_utils.py): some utility functions for MoCo training. * [`linear_decoding.py`](https://github.com/eminorhan/baby-vision/blob/master/linear_decoding.py): evaluates self-supervised models on downstream linear classification tasks. * [`linear_combination_maps.py`](https://github.com/eminorhan/baby-vision/blob/master/linear_combination_maps.py): plots spatial attention maps as in Figure 4b and Figure 6 in the paper. * [`highly_activating_imgs.py`](https://github.com/eminorhan/baby-vision/blob/master/highly_activating_imgs.py): finds highly activating images for a given feature as in Figure 7b in the paper. * [`selectivities.py`](https://github.com/eminorhan/baby-vision/blob/master/selectivities.py): measures the class selecitivity indices of all features in a given layer as in Figure 7a in the paper. * [`hog_baseline.py`](https://github.com/eminorhan/baby-vision/blob/master/hog_baseline.py): runs the HOG baseline model as described in the paper. * [`imagenet_finetuning.py`](https://github.com/eminorhan/baby-vision/blob/master/imagenet_finetuning.py): ImageNet evaluations. * [`feature_animation.py`](https://github.com/eminorhan/baby-vision/blob/master/feature_animation.py) and [`feature_animation_class.py`](https://github.com/eminorhan/baby-vision/blob/master/feature_animation_class.py): Some tools for visualizing the learned features.  For specific usage examples  please see the slurm scripts provided in the [`scripts`](https://github.com/eminorhan/baby-vision/tree/master/scripts) directory.   """;General;https://github.com/eminorhan/baby-vision
"""**TL; DR.** Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism.    ![deformable_detr](./figs/illustration.png)  ![deformable_detr](./figs/convergence.png)  **Abstract.** DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However  it suffers from slow convergence and limited feature spatial resolution  due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues  we proposed Deformable DETR  whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10× less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach.   """;Computer Vision;https://github.com/Cedric-Perauer/Deformable_Detr_PIL
"""**TL; DR.** Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism.    ![deformable_detr](./figs/illustration.png)  ![deformable_detr](./figs/convergence.png)  **Abstract.** DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However  it suffers from slow convergence and limited feature spatial resolution  due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues  we proposed Deformable DETR  whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10× less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach.   """;General;https://github.com/Cedric-Perauer/Deformable_Detr_PIL
"""This repository contains the implementation of 2D UNet architecture for fetal brain segmentation   """;Computer Vision;https://github.com/koriavinash1/Fetal-Brain-Segmentation
"""The *Show and Tell* model is a deep neural network that learns how to describe the content of images. For example:  ![Example captions](g3doc/example_captions.jpg)   """;Computer Vision;https://github.com/21-projects-for-deep-learning/image2text
"""[ImageNet](http://www.image-net.org/) is a common academic data set in machine learning for training an image recognition system. Code in this directory demonstrates how to use TensorFlow to train and evaluate a type of convolutional neural network (CNN) on this data set.      http://arxiv.org/abs/1512.00567      This network achieves 21.2% top-1 and 5.6% top-5 error for single frame evaluation with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. Below is a visualization of the model architecture.   ![image](https://github.com/r-karthik/images/blob/master/detection_of_pests/layers.png)            """;Computer Vision;https://github.com/r-karthik/Detection-of-pests
"""""Several areas of Earth with large accumulations of oil and gas also have huge deposits of salt below the surface. But unfortunately  knowing where large salt deposits are precisely is very difficult. Professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective  highly variable renderings. More alarmingly  it leads to potentially dangerous situations for oil and gas company drillers."" - from competition description.  In this competition TGS provided images collected using seismic reflection of the ground in various locations. Thus in the data we are given training data as the images and their appropriate masks highlighting the salt deposit within that image as labels. The goal of the competition is to build a model that best performs this image segmentation task.   """;Computer Vision;https://github.com/JHLee0513/Salt_detection_challenge
"""![Trained Agent][image1]  The goal of the project is to create an agent that learns how to efficiently solve a Tennis environment made with Unity-ML agents. While active the agent is trying to approximate the policy that defines his behaviour and tries to maximize the performance in the context of the environment.  In this environment  two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net  it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds  it receives a reward of -0.01. Thus  the goal of each agent is to keep the ball in play.  The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own  local observation. Two continuous actions are available  corresponding to movement toward (or away from) the net  and jumping.  The environment is considered solved  when the average (over 100 episodes) of those **scores** is at least +0.5.   """;Reinforcement Learning;https://github.com/prajwalgatti/DRL-Collaboration-and-Competition
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/yydai/bert_test
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/yydai/bert_test
"""For this project  you will train an agent to navigate (and collect bananas!) in a large  square world.    ![Trained Agent][image1]  A reward of +1 is provided for collecting a yellow banana  and a reward of -1 is provided for collecting a blue banana.  Thus  the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.    The state space has 37 dimensions and contains the agent's velocity  along with ray-based perception of objects around agent's forward direction.  Given this information  the agent has to learn how to best select actions.  Four discrete actions are available  corresponding to: - **`0`** - move forward. - **`1`** - move backward. - **`2`** - turn left. - **`3`** - turn right.  The task is episodic  and in order to solve the environment  your agent must get an average score of +13 over 100 consecutive episodes.   """;Reinforcement Learning;https://github.com/MEOWMEOW114/nd893-p1-navigation-banana
"""""Several areas of Earth with large accumulations of oil and gas also have huge deposits of salt below the surface. But unfortunately  knowing where large salt deposits are precisely is very difficult. Professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective  highly variable renderings. More alarmingly  it leads to potentially dangerous situations for oil and gas company drillers."" - from competition description.  In this competition TGS provided images collected using seismic reflection of the ground in various locations. Thus in the data we are given training data as the images and their appropriate masks highlighting the salt deposit within that image as labels. The goal of the competition is to build a model that best performs this image segmentation task.   """;General;https://github.com/JHLee0513/Salt_detection_challenge
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/chenbys/GuidedOffset
"""This repository contains Pytorch implementation of paper [""Safe Exploration in Continuous Action Spaces"" [Dalal et al.]](https://arxiv.org/pdf/1801.08757.pdf) along with [""Continuous Control With Deep Reinforcement Learning"" [Lillicrap et al.]](https://arxiv.org/pdf/1509.02971.pdf). Dalal et al. present a closed form analytically optimal solution to ensure safety in continuous action space. The proposed ""safety layer""  makes the smallest possible perturbation to the original action such that safety constraints are satisfied.  ![safety layer](./images/safety_layer.png)  Dalal et al. also propose two new domains BallND and Spaceship which are governed by first and second order dynamics respectively. In Spaceship domain agent receives a reward only on task completion  while BallND has continuous reward based distance from the target. Implementation of both of these tasks extend OpenAI gym's environment interface (`gym.Env`).   """;Reinforcement Learning;https://github.com/AgrawalAmey/safe-explorer
"""The user draws an image using given colors. Each of these colors represents a ""segment"" or type of object being present at that pixel. When they are satisfied with their sketch  they can click a button and the image will be uploaded into the backend of our program hosted on the Google Cloud. The program converts the image to a more readable form and passes it into a folder where we use NVIDIA's pre-trained models to create a synthesized image using learned traits about each texture and object and the pixel map the user submitted. This image is then displayed on the website for the user to see.   ...  One thing our program does not do is generate an instance map. The instance map segments the image into different instances of objects - and makes sure that if two of the same object are overlapping each other  they have different pixel values. Detecting and working past overlapping images is a hard problem for computer vision  and this helps generate higher fidelity images. However  we thought that the end user would not enjoy having to sketch another image  and would probably not be submitting very complex drawing with many overlapping figures  so we omitted using this in the model.    """;General;https://github.com/tinawu-23/smart-sketch
"""```python #:#:#: To start off let's do a basic data summary.  #: Number of training examples n_train = augmented_X_train.shape[0]  #: Number of validation examples n_validation = X_valid.shape[0]  #: Number of testing examples n_test = X_test.shape[0]  #: What's the shape of an image? image_shape = augmented_X_train[0].shape  #: How many classes are in the dataset n_classes = np.unique(augmented_y_train).shape[0]  print(""Number of training examples =""  n_train) print(""Number of validation examples =""  n_validation) print(""Number of testing examples =""  n_test) print(""Image data shape =""  image_shape) print(""Number of classes =""  n_classes) ```      Number of training examples = 155275     Number of validation examples = 4410     Number of testing examples = 12630     Image data shape = (32  32  3)     Number of classes = 43    ```python X_train = augmented_X_train y_train = augmented_y_train  ```   ```python #:#:#: To start off let's do a basic data summary.  #: Number of training examples n_train = X_train.shape[0]  #: Number of validation examples n_validation = X_valid.shape[0]  #: Number of testing examples n_test = X_test.shape[0]  #: What's the shape of an image? image_shape = X_train[0].shape  #: How many classes are in the dataset n_classes = np.unique(y_train).shape[0]  print(""Number of training examples =""  n_train) print(""Number of validation examples =""  n_validation) print(""Number of testing examples =""  n_test) print(""Image data shape =""  image_shape) print(""Number of classes =""  n_classes) ```      Number of training examples = 34799     Number of validation examples = 4410     Number of testing examples = 12630     Image data shape = (32  32  3)     Number of classes = 43    """;Computer Vision;https://github.com/ibabbar/Traffic-Sign-Classifier
"""```python #:#:#: To start off let's do a basic data summary.  #: Number of training examples n_train = augmented_X_train.shape[0]  #: Number of validation examples n_validation = X_valid.shape[0]  #: Number of testing examples n_test = X_test.shape[0]  #: What's the shape of an image? image_shape = augmented_X_train[0].shape  #: How many classes are in the dataset n_classes = np.unique(augmented_y_train).shape[0]  print(""Number of training examples =""  n_train) print(""Number of validation examples =""  n_validation) print(""Number of testing examples =""  n_test) print(""Image data shape =""  image_shape) print(""Number of classes =""  n_classes) ```      Number of training examples = 155275     Number of validation examples = 4410     Number of testing examples = 12630     Image data shape = (32  32  3)     Number of classes = 43    ```python X_train = augmented_X_train y_train = augmented_y_train  ```   ```python #:#:#: To start off let's do a basic data summary.  #: Number of training examples n_train = X_train.shape[0]  #: Number of validation examples n_validation = X_valid.shape[0]  #: Number of testing examples n_test = X_test.shape[0]  #: What's the shape of an image? image_shape = X_train[0].shape  #: How many classes are in the dataset n_classes = np.unique(y_train).shape[0]  print(""Number of training examples =""  n_train) print(""Number of validation examples =""  n_validation) print(""Number of testing examples =""  n_test) print(""Image data shape =""  image_shape) print(""Number of classes =""  n_classes) ```      Number of training examples = 34799     Number of validation examples = 4410     Number of testing examples = 12630     Image data shape = (32  32  3)     Number of classes = 43    """;General;https://github.com/ibabbar/Traffic-Sign-Classifier
"""In this project  from the pre-trained VGG model ""HyperColumns"" is harvested and is used to colorize gray images. The major target of this project is to explore HyperColumns and how it can be used in such computer vision tasks as image auto-colorizations. The training data is flower data set which is separated into train  validation and test sets. The trained model is also tested on images that are not from the flower data set. The project is done in Tensorflow 1.0 and Python. ![](pics/head.jpg)   """;General;https://github.com/BerenLuthien/HyperColumns_ImageColorization
"""**RNN based seq2seq model**:  A seq2seq model is mainly used in NLP tasks such as machine translation and often based on LSTM or GRU structure. It has encoder  decoder and intermediate step as its main components  mapping an arbitrarily long input sequence to an arbitrarily long output sequence with an intermediate encoded state.:  <p align=""center"">   <img src=""figures/seq2seq.png"">  </p>  In comparison to fully connected feed forward neural networks  recurrent neural networks has no longer the requirement a fixed-sized input and considers naturally the relation between previous and current time steps. In addition  LSTM or GRU are advanced RNN structures  which increase the ability of capturing long-term dependencies  by forcing a approximately constant back-propagation error flow during training.  However  due to the recurrent calculation for each time step  parrellelization is impossible for training theses networks. And it's a big disadvantage in the big data era. Even the input time range for a LSTM can not be arbitrary long in reality  and it is in fact severly limited by the training mechanism of RNN.  **Wavenet based approach**:  With Wavenet  the training procedure for all the time steps in the input can be parrellelized. We just let the output sequence be one time step ahead of the input sequence  and at every time step of the output  the value is only influenced by the previous steps in the input.  As for the inference stage  it yields every time only the prediction one step ahead as in the LSTM approach. But we don't need to define a distinct model for inferencing here. In each Iteration  the last point of the output sequence is selected as the prediction one step ahead of the previous iteration  and it is in turn concatenated to the input sequence  in order to predict one step further in the future.    The model architecture is similar to WaveNet  consisting of a stack of dilated causal convolutions  as demonstrated in the [diagram](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) below. For more details  see van den Oord's [paper](https://arxiv.org/abs/1609.03499).  <p align=""center"">   <img src=""figures/wavenet.gif"">  </p>  **Causal Convolution**:  The figure below shows a causal structure  which guarantees that the current time step is only influenced by the previous time steps. Then an expression of the conditional probability could be established. That is to say  we assume that the current value is conditioned on the previous values in a time sequence.    <p align=""center"">   <img src=""figures/WaveNet_causalconv.png"">  </p>  **Dilated Convolution**:  But as can be seen  the reception field is quite small with a limited number of stacks  and it results in poor performance handling long-term dependencies. So the idea of dilated convolution is employed. In a dilated convolution layer  filters are not applied to inputs in a simple sequential manner  but instead skip a constant dilation rate inputs in between each of the inputs they process  as in the WaveNet diagram below. By increasing the dilation rate multiplicatively at each layer (e.g. 1  2  4  8  …)  we can achieve the exponential relationship between layer depth and receptive field size that we desire. The figure below ilustrates the effect of dilation.  <p align=""center"">   <img src=""figures/WaveNet_dilatedconv.png"">  </p>   """;Audio;https://github.com/ZhouYuxuanYX/Wavenet-in-Keras-for-Kaggle-Competition-Web-Traffic-Time-Series-Forecasting
"""**RNN based seq2seq model**:  A seq2seq model is mainly used in NLP tasks such as machine translation and often based on LSTM or GRU structure. It has encoder  decoder and intermediate step as its main components  mapping an arbitrarily long input sequence to an arbitrarily long output sequence with an intermediate encoded state.:  <p align=""center"">   <img src=""figures/seq2seq.png"">  </p>  In comparison to fully connected feed forward neural networks  recurrent neural networks has no longer the requirement a fixed-sized input and considers naturally the relation between previous and current time steps. In addition  LSTM or GRU are advanced RNN structures  which increase the ability of capturing long-term dependencies  by forcing a approximately constant back-propagation error flow during training.  However  due to the recurrent calculation for each time step  parrellelization is impossible for training theses networks. And it's a big disadvantage in the big data era. Even the input time range for a LSTM can not be arbitrary long in reality  and it is in fact severly limited by the training mechanism of RNN.  **Wavenet based approach**:  With Wavenet  the training procedure for all the time steps in the input can be parrellelized. We just let the output sequence be one time step ahead of the input sequence  and at every time step of the output  the value is only influenced by the previous steps in the input.  As for the inference stage  it yields every time only the prediction one step ahead as in the LSTM approach. But we don't need to define a distinct model for inferencing here. In each Iteration  the last point of the output sequence is selected as the prediction one step ahead of the previous iteration  and it is in turn concatenated to the input sequence  in order to predict one step further in the future.    The model architecture is similar to WaveNet  consisting of a stack of dilated causal convolutions  as demonstrated in the [diagram](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) below. For more details  see van den Oord's [paper](https://arxiv.org/abs/1609.03499).  <p align=""center"">   <img src=""figures/wavenet.gif"">  </p>  **Causal Convolution**:  The figure below shows a causal structure  which guarantees that the current time step is only influenced by the previous time steps. Then an expression of the conditional probability could be established. That is to say  we assume that the current value is conditioned on the previous values in a time sequence.    <p align=""center"">   <img src=""figures/WaveNet_causalconv.png"">  </p>  **Dilated Convolution**:  But as can be seen  the reception field is quite small with a limited number of stacks  and it results in poor performance handling long-term dependencies. So the idea of dilated convolution is employed. In a dilated convolution layer  filters are not applied to inputs in a simple sequential manner  but instead skip a constant dilation rate inputs in between each of the inputs they process  as in the WaveNet diagram below. By increasing the dilation rate multiplicatively at each layer (e.g. 1  2  4  8  …)  we can achieve the exponential relationship between layer depth and receptive field size that we desire. The figure below ilustrates the effect of dilation.  <p align=""center"">   <img src=""figures/WaveNet_dilatedconv.png"">  </p>   """;Sequential;https://github.com/ZhouYuxuanYX/Wavenet-in-Keras-for-Kaggle-Competition-Web-Traffic-Time-Series-Forecasting
"""Generative Adversarial Networks (GANs) are one of the most popular (and coolest) Machine Learning algorithms developed in recent times. They belong to a set of algorithms called generative models  which are widely used for unupervised learning tasks which aim to learn the uderlying structure of the given data. As the name suggests GANs allow you to generate new unseen data that mimic the actual given real data. However  GANs pose problems in training and require carefullly tuned hyperparameters.This paper aims to solve this problem.  DCGAN is one of the most popular and succesful network design for GAN. It mainly composes of convolution layers  without max pooling or fully connected layers. It uses strided convolutions and transposed convolutions  for the downsampling and the upsampling respectively.  **Generator architecture of DCGAN** <p align=""center""> <img src=""images/Generator.png"" title=""DCGAN Generator"" alt=""DCGAN Generator""> </p>  **Network Design of DCGAN:** * Replace all pooling layers with strided convolutions. * Remove all fully connected layers. * Use transposed convolutions for upsampling. * Use Batch Normalization after every layer except after the output layer of the generator and the input layer of the discriminator. * Use ReLU non-linearity for each layer in the generator except for output layer use tanh. * Use Leaky-ReLU non-linearity for each layer of the disciminator excpet for output layer use sigmoid.   """;Computer Vision;https://github.com/rajprakrit/DCGAN
"""lr - learning rate can be scalar or function  in second case relative step size is using.  beta1  beta2 - is also can be scalar or functions  in first case algorithm works as AMSGrad. Setting beta1 to zero is turning off moments updates.  non_constant_decay - boolean  has effect if betas are scalars. If True using functions for betas (from section 7.1)  enable_factorization - boolean. Factorization works on 2D weights.  clipping_threshold - scalar. Threshold value for update clipping (from section 6)  """;General;https://github.com/DeadAt0m/adafactor-pytorch
"""This is a half-semester project  an ongoing attempt to extend the original work by:   1. Using LIDAR point cloud data instead of RGB-D data   2. Using bird's eye view representation of LIDAR data to generate additional 3D bounding box proposals from the top view  The below figure shows the *proposed modification* to the original work. The components (at the top) connected by red arrows are what we are going to add.  ![proposed_architecture](https://github.com/witignite/Frustum-PointNets/blob/master/doc/Fig_3_ProposedArchitecture.PNG)  The purpose is to experiment on using the bird's eye view LIDAR data to help improving the performance in some extreme condition  such as low light or occlusion  where it may be difficult to detect the 2D bounding boxes in the image (and hence no 3D object will be detected). By using the bird's eye view LIDAR data  we expect to obtain additional 3D box proposals  which will be combined with the original proposals to improve the performance.   """;Computer Vision;https://github.com/witignite/Frustum-PointNet
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.   ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    """;Natural Language Processing;https://github.com/habibullah-araphat/BERT-GPU
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/minar09/yolov3-pytorch
"""This repository focuses on implementing the project 'Twitter Sentiment Analysis using ULMFiT' which is consisted of a thorough analysis of the dataset and prediction of Twitter sentiments. A sentiment analysis job has to be done regarding the problems of each major U.S. airline and contributors. We have first to classify positive  negative  and neutral tweets  and then categorize the negative tweets according to the reasons provided  i.e.  ""late flight"" or ""rude service."" This problem along with the Dataset is available on Kaggle: https://www.kaggle.com/crowdflower/twitter-airline-sentiment   """;General;https://github.com/RajasSU/Twitter-Sentiment-Analysis-using-ULMFiT
"""This repository focuses on implementing the project 'Twitter Sentiment Analysis using ULMFiT' which is consisted of a thorough analysis of the dataset and prediction of Twitter sentiments. A sentiment analysis job has to be done regarding the problems of each major U.S. airline and contributors. We have first to classify positive  negative  and neutral tweets  and then categorize the negative tweets according to the reasons provided  i.e.  ""late flight"" or ""rude service."" This problem along with the Dataset is available on Kaggle: https://www.kaggle.com/crowdflower/twitter-airline-sentiment   """;Natural Language Processing;https://github.com/RajasSU/Twitter-Sentiment-Analysis-using-ULMFiT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/TYTYTYTYTYTYTYTYTY/558-project
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Kevin-Vora/bert-embedding-gluonnlp-edit-
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/TYTYTYTYTYTYTYTYTY/558-project
"""Here we study the problem of learning generators from noisy data on MNIST handwritten digit dataset that gives insights about the robustness of Generative Adversarial Networks towards Noisy Data. Perturbed discriminator or noisy adversarial inference with it's feedback is expected to introduce errors in GAN training. The dataset is corrupted for experiments in two scenarios; Corruption by Gaussian noise and corruption by EMNIST dataset. We show experimentally that Generative Adversarial Networks are robust to noise in the data upto a certain threshold and various artifacts are introduced in the output generated images on increasing the noise above the threshold. Here  we resort to visual analysis of the results. ![MNIST_GAN](https://miro.medium.com/max/1416/1*6zMZBE6xtgGUVqkaLTBaJQ.png)   """;Computer Vision;https://github.com/SuchismitaSahu1993/Studying-the-Robustness-of-GANs-on-Noisy-Data-using-MNIST-and-EMNIST-Datasets
"""[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.   """;Natural Language Processing;https://github.com/ericxsun/fastText
"""Google AI in 2017 proposed a new simple network architecture  the Transformer  based solely on attention mechanisms  dispensing with recurrence and convolutions entirely. As you see  this model architecture becomes the base stone of the following state-of-the-art pre-trained models in Natural Language Processing (NLP)  such as GPT  BERT  Transformer-XL XLnet  RoBERTa.   This repo will walk through the implementation of Transformer. Code is simple  clean and easy to understand. Some of these codes are based on The [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html).  Currently this project is working in process  as always  PRs are welcome :)   """;General;https://github.com/walkacross/transformer-pytorch
"""Google AI in 2017 proposed a new simple network architecture  the Transformer  based solely on attention mechanisms  dispensing with recurrence and convolutions entirely. As you see  this model architecture becomes the base stone of the following state-of-the-art pre-trained models in Natural Language Processing (NLP)  such as GPT  BERT  Transformer-XL XLnet  RoBERTa.   This repo will walk through the implementation of Transformer. Code is simple  clean and easy to understand. Some of these codes are based on The [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html).  Currently this project is working in process  as always  PRs are welcome :)   """;Natural Language Processing;https://github.com/walkacross/transformer-pytorch
"""1.Description:  A generative adversarial network (GAN) is a class of machine learning system invented by Ian Goodfellow in 2014. Two neural networks compete with each other in a game. Given a training set  this technique learns to generate new data with the same statistics as the training set.  In this competition  you’ll be training generative models to create images of dogs. Only this time… there’s no ground truth data for you to predict. Here  you’ll submit the images and be scored based on how well those images are classified as dogs from pre-trained neural networks. Take these images  for example. Can you tell which are real vs. generated?    2.Evaluation:MiFID  用生成模型所常用的指标[FID](https://baijiahao.baidu.com/s?id=1647349368499780367&wfr=spider&for=pc) 之前的Mi是Memorization-informed的简称 官方解释如下:The memorization distance is defined as the minimum cosine distance of all training samples in the feature space  averaged across all user generated image samples. This distance is thresholded  and it's assigned to 1.0 if the distance exceeds a pre-defined epsilon.通俗来说就是即一个衡量你生成图片和原始图片的distance 的惩罚系数(防止你不做训练直接将原图提交上去生成""极为逼真""的小狗)。     """;General;https://github.com/Morxrc/kaggle_generate-dogs_public-1st-private-16th-solution
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Kevin-Vora/bert-embedding-gluonnlp-edit-
"""This repo is a [PyTorch](https://www.pytorch.org/) implementation of Vanilla DQN  Double DQN  and Dueling DQN based off these papers.  - [Human-level control through deep reinforcement learning](http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html) - [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) - [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)  Starter code is used from [Berkeley CS 294 Assignment 3](https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3) and modified for PyTorch with some guidance from [here](https://github.com/transedward/pytorch-dqn). Tensorboard logging has also been added (thanks [here](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/04-utils/tensorboard) for visualization during training in addition to what the Gym Monitor already does).   """;Reinforcement Learning;https://github.com/dxyang/DQN_pytorch
"""An unofficical low-resolution (32 x 32) implementation of BigBiGAN   Paper: https://arxiv.org/abs/1907.02544  """;General;https://github.com/LEGO999/BigBiGAN-TensorFlow2.0
"""An unofficical low-resolution (32 x 32) implementation of BigBiGAN   Paper: https://arxiv.org/abs/1907.02544  """;Computer Vision;https://github.com/LEGO999/BigBiGAN-TensorFlow2.0
"""An easy-to-use and efficient system to support the Mixture of Experts (MoE)  model for PyTorch.    """;General;https://github.com/laekov/fastmoe
"""Mixup is a generic and straightforward data augmentation principle. In essence  mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so  mixup regularizes the neural network to favor simple linear behavior in-between training examples.  This repository contains the implementation used for the results in our paper (https://arxiv.org/abs/1710.09412).   """;Computer Vision;https://github.com/facebookresearch/mixup-cifar10
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/coronazap/bert_client
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/coronazap/bert_client
"""_Accompanying repository for the paper [Aspect-Controlled Neural Argument Generation](https://aclanthology.org/2021.naacl-main.34/)._  We rely on arguments in our daily lives to deliver our opinions and base them on evidence  making them more convincing  in turn. However  finding and formulating arguments can be challenging.  To tackle this challenge  we trained a language model (based on the CTRL by  [Keskar et al. (2019)](https://arxiv.org/abs/1909.05858)) for argument generation that can be controlled on a fine-grained  level to generate sentence-level arguments for a given topic  stance  and aspect.  We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset  with 5 032 arguments annotated with aspects. We release this dataset  as well as the training data for the argument generation model  its weights  and the arguments generated with the model.  The following figure shows how the argument generation model was trained:  ![Image description](arg_gen_pipeline.png)  (1) We gather several million documents for eight different topics from two large data sources.  All sentences are classified into pro-  con-  and non-arguments. We detect aspects of all arguments with a model trained on a novel dataset and concatenate arguments with the same topic  stance  and aspect into training documents.  (2) We use the collected classified data to condition the CTRL model on the topics  stances  and aspects of all gathered arguments. (3) At inference  passing the control code  _[Topic]_ _[Stance]_ _[Aspect]_ will generate an argument that follows these commands.   """;Natural Language Processing;https://github.com/UKPLab/controlled-argument-generation
"""An one-stage object detection model is implemented on paddle 2.1.0 and paddledetection 2.1.0   """;Computer Vision;https://github.com/FL77N/RetinaNet-Based-on-PPdet
"""An one-stage object detection model is implemented on paddle 2.1.0 and paddledetection 2.1.0   """;General;https://github.com/FL77N/RetinaNet-Based-on-PPdet
"""In this work  we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN including only the most essential component in GCN—neighborhood aggregation—for collaborative filtering   """;Graphs;https://github.com/Wuyxin/LightGCN-parallelized-version
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/leodotnet/bert-old
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/xiaopp123/bert_explain
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/xiaopp123/bert_explain
"""MLP Mixer is based on multi layer perceptron it does not use modern days CNN   It has two kinds of multi layer preceptrons one is directly applied to image patches   which are created original image then we transpose the layer and apply MLP layer across patches In the extreme case  Multi layer perceptron architecture can be seen as a very special CNN  which uses 1×1 convolutions for channel mixing  and single-channel depth-wise convolutions of a full receptive field and parameter sharing for token mixing. However  the converse is not true as typical CNNs are not special cases of Mixer. Furthermore  a convolution is more complex than the plain matrix multiplication in MLPs as it requires an additional costly reduction to matrix multiplication and/or specialized implementation. If you want to see training of cifar10 dataset using above architecture you can refer [here](https://github.com/imad08/MLP-Mixer/blob/main/MLP.ipynb)   """;Computer Vision;https://github.com/imad08/MLP-Mixer
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/weidi1024/caffe-ssd-correct
"""Py**T**orch **Im**age **M**odels (`timm`) is a collection of image models  layers  utilities  optimizers  schedulers  data-loaders / augmentations  and reference training / validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.  The work of many others is present here. I've tried to make sure all source material is acknowledged via links to github  arxiv papers  etc in the README  documentation  and code docstrings. Please let me know if I missed anything.   """;General;https://github.com/rwightman/pytorch-image-models
"""Inspired by the structure of Receptive Fields (RFs) in human visual systems  we propose a novel RF Block (RFB) module  which takes the relationship between the size and eccentricity of RFs into account  to enhance the discriminability and robustness of features. We further  assemble the RFB module to the top of SSD with a lightweight CNN model  constructing the RFB Net detector. You can use the code to train/evaluate the RFB Net for object detection. For more details  please refer to our [arXiv paper](https://arxiv.org/pdf/1711.07767.pdf).   <img align=""right"" src=""https://github.com/ruinmessi/RFBNet/blob/master/doc/rfb.png"">  &nbsp; &nbsp;   """;Computer Vision;https://github.com/dishen12/py03
"""How to initialize a sparse deep neural network without compromising its performance? Our paper tackles this issue by instantiating sparse neural networks whose training dynamics in function space are as close as possible to the training dynamics of a dense one.  ![Schematic illustration of neural networks' output evolution during supervised training from time t = 0 (starting point) to t = T (endpoint) ](schematic.png)  We achieve this by minimizing the mismatch between the neural tangent kernels of the sparse and a dense teacher network  a method we refer to as *Neural Tangent Transfer* (NTT). NTT has two key advantages: (i) it only requires label-free data  and (ii) it can be used to find trainable layerwise sparse networks  e.g.  CNNs with sparse convolutional filters  which are desirable for energy-efficient inference.     """;General;https://github.com/fmi-basel/neural-tangent-transfer
"""This repo will release the two proposed benchmarks GFR-R and GFR-V in our paper. I hope the proposed benchmarks will attract researchers on _Generalized Face Recognition_ problem. More details can be referred to our paper [Learning Meta Face Recognition in Unseen Domains](https://me.guojianzhu.com/assets/pdfs/05997.pdf)  accepted to CVPR2020.    """;Computer Vision;https://github.com/cleardusk/MFR
"""This is a PyTorch implementation of [Data-Driven Neuron Allocation for Scale Aggregation Networks](https://arxiv.org/pdf/1904.09460.pdf).(CVPR2019) with pretrained models.    """;Computer Vision;https://github.com/Eli-YiLi/ScaleNet
"""This repo contains the implementation of YOLOv2 in Keras with Tensorflow backend.  For details about YOLO and YOLOv2 please refer to their [project page](https://pjreddie.com/darknet/yolo/)  and the [paper](https://arxiv.org/abs/1612.08242): **YOLO9000: Better  Faster  Stronger by Joseph Redmon and Ali Farhadi**.  ---   """;Computer Vision;https://github.com/FMsunyh/keras-yolo2
"""<strong>Generative adversarial network</strong> contains the two components: generator and discriminator. The training process is just like zero-sum game  and it can be simply shown in Figure below.  <img src=""/asset/gan.png""/>  For generator  it should generate the image which is just like the real one. On the contrary  the discriminator should distinguish the image is fake or not. During the training  the generator should make itself have more capability to generate image which is more and more like the actual one  and the discriminator should make itself realize the difference with more and more accuracy.  The problem this paper is concerned with is that of unsupervised learning.Authors direct their attention towards<em>various ways to measure how close the model distribution and the real distribution are</em>  or equvalently on the various ways to define a distance or divergence  ρ(P<sub>θ</sub>  P<sub>r</sub>) where the real data distribution P<sub>r</sub> admits a density and P<sub>θ</sub> is the distribution of the parametrized density. The most fundamental difference between such distances is their impact on the convergence of sequences of probability distributions. In order to optimize the parameter θ  it is of course desirable to define our model distribution P<sub>θ</sub> in a manner that makes the mapping θ→P<sub>θ</sub> is continuous.   """;Computer Vision;https://github.com/akshay-gupta123/WGAN
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/kusti8/yolov3
"""Deep-trace is a graphsage-based machine-learning pipeline for contact tracing. Conventional methods can only exploit knowledge of an individual person's contacts. Taken over the set of all individuals  this contact set is essentially a graph with nodes representing people and edges connecting contact between people. The proposed method allows us to utilize information stored in the graph contacts as well as node features to develop a method to classify individuals in the contact set as either susceptible  exposed or infected. In this particular case we use the covid vulnerability index to assign a feature vector to each node. We are then able to learn the contact network based not only on the graph node and edgelist specification  but also the vulnerability feature mapping. Thus we create a three-dimensional node embedding for new contacts that shows an assessment of their likelihood of being in one of three exposure categories – Infected  Exposed or Susceptible. This low dimensional embedding allows contact tracing personnel to prioritize which individuals they should contact and test in situations where a pandemic is evolving too quickly under limited personnel and test resources to correspond with everyone in the contact set. Thu one can quickly identify and prioritize which persons to contact and isolate. <br /> #  Figure 1 below shows a TSNE projection of the data onto three dimensions for a simulated case study of 27 infected  519 susceptible  and 419 exposed individuals:  ![alt text][image_2]  This is a 2-D projection of the same TSNE embedding:  ![alt text][image_3]   """;Graphs;https://github.com/silent-code/deep-trace
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/bt77/pointnet
"""This is the system that does the object segmentation on frames/images.    """;Computer Vision;https://github.com/jklife3/maskrcnn-impl
""" The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.    This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/phnk/yolov3
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/leodotnet/bert-old
"""The goal of the project is to build a chatbot which can improve people's motivation and confidence to speak English.  An AI-based sound chatbot was built based on Pytorch framework and trained on the Cornell Movie-Dialogs Corpus.      """;General;https://github.com/T9-LIN/MSc-Project
"""This notebook focuses on the so-called Universal Language Model Fine-tuning (ULMFiT) introduced by  Jeremy Howard and Sebastian Ruder [[1](https://arxiv.org/abs/1801.06146)].  The ULMFiT model consists of three main stages in the building a language model (LM):  1. **General-domain LM  pre-training**: Similar to the ImageNet database used in computer vision   the idea is to pre-train a large corpus of text. The ULMFiT has a pre-trained model called the  `Wikitext-103` where more than 20 000 Wikipedia articles was trained on.  This is alreadly included in the `fastai.text` API  thus  it is not necessary to carry out this step.  2. **LM fine-tuning**: Because the target data (typically) comes from a different  distribution from the general-domain  it is necessary to fine-tune the LM to adapt to  the idosyncrasies of the target data. Howard and Ruder suggested *discriminative fine-tuning* and *slanted triangular learning rates* for fine-tuning the LM. These techniques are available  in `fastai` and are described in [[1](https://arxiv.org/abs/1801.06146)].  3. **Classifier fine-tuning**: Using the updated weights from the previous step  a classifier can be fine-tuned. Howard and Ruder suggested a few techniques which include *concat pooling*  and *gradual unfreezing*. The latter in particular is used in this demonstration.  Again  the `fastai` framework allows one to perform this technique.  **Reference**  [1] J. Howard and S. Ruder. 2018.*Universal Language Model Fine-tuning for Text Classification*. [arXiv:1801.06146](https://arxiv.org/abs/1801.06146).  """;General;https://github.com/anthonyckleung/Transfer-Learning-in-Sentiment-Tweets
"""This notebook focuses on the so-called Universal Language Model Fine-tuning (ULMFiT) introduced by  Jeremy Howard and Sebastian Ruder [[1](https://arxiv.org/abs/1801.06146)].  The ULMFiT model consists of three main stages in the building a language model (LM):  1. **General-domain LM  pre-training**: Similar to the ImageNet database used in computer vision   the idea is to pre-train a large corpus of text. The ULMFiT has a pre-trained model called the  `Wikitext-103` where more than 20 000 Wikipedia articles was trained on.  This is alreadly included in the `fastai.text` API  thus  it is not necessary to carry out this step.  2. **LM fine-tuning**: Because the target data (typically) comes from a different  distribution from the general-domain  it is necessary to fine-tune the LM to adapt to  the idosyncrasies of the target data. Howard and Ruder suggested *discriminative fine-tuning* and *slanted triangular learning rates* for fine-tuning the LM. These techniques are available  in `fastai` and are described in [[1](https://arxiv.org/abs/1801.06146)].  3. **Classifier fine-tuning**: Using the updated weights from the previous step  a classifier can be fine-tuned. Howard and Ruder suggested a few techniques which include *concat pooling*  and *gradual unfreezing*. The latter in particular is used in this demonstration.  Again  the `fastai` framework allows one to perform this technique.  **Reference**  [1] J. Howard and S. Ruder. 2018.*Universal Language Model Fine-tuning for Text Classification*. [arXiv:1801.06146](https://arxiv.org/abs/1801.06146).  """;Natural Language Processing;https://github.com/anthonyckleung/Transfer-Learning-in-Sentiment-Tweets
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/dwtstore/sfm1
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/TravisTorch/Pytorch
"""The attention mechanism is a popular easy-to-implement model architecture designed to perform well on different NLP tasks  including machine translation.  Empowered with pretrained [SpaCy](#https://spacy.io/) word model this approach makes a strong baseline with comparable to state-of-the-art perfomance. In this repo the translator from German to English is trained and demonstrated. In case you need other languages support  thanks to SpaCy team there are  plenty of available language models to train the exact same model on.  Pretrained German-English model is already available after setup.     """;General;https://github.com/Andrey885/Machine_translation_PyTorch
"""The attention mechanism is a popular easy-to-implement model architecture designed to perform well on different NLP tasks  including machine translation.  Empowered with pretrained [SpaCy](#https://spacy.io/) word model this approach makes a strong baseline with comparable to state-of-the-art perfomance. In this repo the translator from German to English is trained and demonstrated. In case you need other languages support  thanks to SpaCy team there are  plenty of available language models to train the exact same model on.  Pretrained German-English model is already available after setup.     """;Natural Language Processing;https://github.com/Andrey885/Machine_translation_PyTorch
"""FoveaBox is an accurate  flexible and completely anchor-free object detection system for object detection framework  as presented in our paper [https://arxiv.org/abs/1904.03797](https://arxiv.org/abs/1904.03797): Different from previous anchor-based methods  FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility  and (b) producing category-agnostic bounding box for each position that potentially contains an object.  <div align=""center"">   <img src=""demo/foveabox.jpg"" width=""300px"" />   <p>FoveaBox detection process.</p> </div>   """;Computer Vision;https://github.com/marinarierav-uab/foveabox
"""Two experiments are included in this repository  where benchmarks are from the paper [Generalized Sliced Wasserstein Distances](http://papers.nips.cc/paper/8319-generalized-sliced-wasserstein-distances) and the paper [Distributional Sliced-Wasserstein and Applications to Generative Modeling](https://arxiv.org/pdf/2002.07367.pdf)  respectively. The first one is on the task of sliced Wasserstein flow  and the second one is on generative modellings with GANs. For more details and setups  please refer to the original paper **Augmented Sliced Wasserstein Distances**.  """;General;https://github.com/ShwanMario/ASWD
"""R-CNN is a state-of-the-art visual object detection system that combines bottom-up region proposals with rich features computed by a convolutional neural network. At the time of its release  R-CNN improved the previous best detection performance on PASCAL VOC 2012 by 30% relative  going from 40.9% to 53.3% mean average precision. Unlike the previous best results  R-CNN achieves this performance without using contextual rescoring or an ensemble of feature types.  R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1311.2524) and will appear in a forthcoming CVPR 2014 paper.   """;Computer Vision;https://github.com/jiangbestone/DetectRCNN
"""* Implement layer normalization GRU in pytorch  followed the instruction from the paper [Layer normalization](https://arxiv.org/abs/1607.06450). * Code modified from [this repository](https://github.com/seba-1511/lstms.pth/blob/master/lstms/lstm.py). * Our research has exerted this technique in predicting kinematic variables from invasive brain-computer interface (BCI) dataset  [Nonhuman Primate Reaching with Multichannel Sensorimotor Cortex Electrophysiology](https://zenodo.org/record/583331). For more information regarding this dataset  please see the article [Superior arm-movement decoding from cortex with a new  unsupervised-learning algorithm](https://iopscience.iop.org/article/10.1088/1741-2552/aa9e95/meta). * Environment: [Official pytorch docker image](https://hub.docker.com/r/pytorch/pytorch/tags) from [Docker Hub](https://hub.docker.com/) ```pytorch/pytorch:1.4-cuda10.1-cudnn7-runtime```   """;General;https://github.com/ElektrischesSchaf/LayerNorm_GRU
"""Inspired by the structure of Receptive Fields (RFs) in human visual systems  we propose a novel RF Block (RFB) module  which takes the relationship between the size and eccentricity of RFs into account  to enhance the discriminability and robustness of features. We further  assemble the RFB module to the top of SSD with a lightweight CNN model  constructing the RFB Net detector. You can use the code to train/evaluate the RFB Net for object detection. For more details  please refer to our [arXiv paper](https://arxiv.org/pdf/1711.07767.pdf).   <img align=""right"" src=""https://github.com/ruinmessi/RFBNet/blob/master/doc/rfb.png"">  &nbsp; &nbsp;   """;General;https://github.com/dishen12/py03
"""The implementation is based on two papers:  - Simple Online and Realtime Tracking with a Deep Association Metric https://arxiv.org/abs/1703.07402 - YOLOv4: Optimal Speed and Accuracy of Object Detection https://arxiv.org/pdf/2004.10934.pdf   This repository contains a moded version of PyTorch YOLOv5 (https://github.com/ultralytics/yolov5). It filters out every detection that is not a person. The detections of persons are then passed to a Deep Sort algorithm (https://github.com/ZQPei/deep_sort_pytorch) which tracks the persons. The reason behind the fact that it just tracks persons is that the deep association metric is trained on a person ONLY datatset.   """;Computer Vision;https://github.com/ajcohen1/CoVision-Social-Distance-Violation-Detection
"""Py**T**orch **Im**age **M**odels (`timm`) is a collection of image models  layers  utilities  optimizers  schedulers  data-loaders / augmentations  and reference training / validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.  The work of many others is present here. I've tried to make sure all source material is acknowledged via links to github  arxiv papers  etc in the README  documentation  and code docstrings. Please let me know if I missed anything.   """;Computer Vision;https://github.com/rwightman/pytorch-image-models
"""FairScale is a PyTorch extension library for high performance and large scale training. This library extends basic PyTorch capabilities while adding new SOTA scaling techniques. FairScale makes available the latest distributed training techniques in the form of composable modules and easy to use APIs. These APIs are a fundamental part of a researcher's toolbox as they attempt to scale models with limited resources.  FairScale was designed with the following values in mind:  * **Usability** -  Users should be able to understand and use FairScale APIs with minimum cognitive overload.  * **Modularity** - Users should be able to combine multiple FairScale APIs as part of their training loop seamlessly.  * **Performance** - FairScale APIs provide the best performance in terms of scaling and efficiency.   """;General;https://github.com/facebookresearch/fairscale
"""We provide a jupyter notebook example to show how to use the anycost generator for image synthesis at diverse costs: `notebooks/intro.ipynb`.  We also provide a colab version of the notebook: [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mit-han-lab/anycost-gan/blob/master/notebooks/intro_colab.ipynb). Be sure to select the GPU as the accelerator in runtime options.     """;Computer Vision;https://github.com/mit-han-lab/anycost-gan
"""We utilize state-of-the-art object detection and tracking algorithm in surveillance videos. Our best object detection model basically uses Faster RCNN with a backbone of Resnet-101 with dilated CNN and FPN. The tracking algo (Deep SORT) uses ROI features from the object detection model. The ActEV trained models are good for small object detection in outdoor scenes. For indoor cameras  COCO trained models are better.   <div align=""center"">   <div style="""">       <img src=""images/Person_vis_video.gif"" height=""300px"" />       <img src=""images/Vehicle_vis_video.gif"" height=""300px"" />   </div> </div>  Also supports multi-camera tracking and ReID:  <div align=""center"">   <div style="""">       <img src=""images/multi-camera-reid.gif"" height=""450px"" />       <img src=""images/vehicle_multi_reid.gif"" height=""350px"" />       <img src=""images/person_multi_reid2.gif"" height=""338px"" />   </div> </div>   """;Computer Vision;https://github.com/JunweiLiang/Object_Detection_Tracking
"""Here is a brief summary of the three MobileNet versions. Only MobileNetV3 is implemented.  """;General;https://github.com/akrapukhin/MobileNetV3
"""Here is a brief summary of the three MobileNet versions. Only MobileNetV3 is implemented.  """;Computer Vision;https://github.com/akrapukhin/MobileNetV3
"""Question-answering problem is currenlty one of the most chellenging task in Natural Language Processing domain. In purpose to solve it transfer learning is state of the art method. Thanks to huggingface-transformers which made avaiable pretrained NLP most advanced models (like: BERT   GPT-2  XLNet  RoBERTa  DistilBERT) relatively easy to be used in different language tasks.   Original [akensert](https://www.kaggle.com/akensert/quest-bert-base-tf2-0) code was tested with different parameters and changed base models. From both implemented (XLNet  RoBERTa) the second one resulted in better score. Further improvement ccould be made by implementation of combined model version. For example it could consists of BERT  RoBERTa and XLNet.   Change of main algorithm from BERT to RoBERTa was justified by the fact that second one is an improved version of the first one. The expansion of the algorithm name is Robustly Optimized BERT Pretraining Approach  it modifications consists of [5]: - training the model longer  with bigger batches  over more data;  - removing the next sentence prediction objective;  - training on longer sequences;  - dynamically changing the masking pattern applied to the training data.  Use of RoBERTa consequently causes the need of configuration change and implementation of RoBERTa sepcific tokenizer. It constructs a RoBERTa BPE tokenizer  derived from the GPT-2 tokenizer  using byte-level Byte-Pair-Encoding. Which works in that order: 1. Prepare a large enough training data (i.e. corpus) 2. Define a desired subword vocabulary size 3. Split word to sequence of characters and appending suffix “</w>” to end of word with word frequency. So the basic unit is character in this stage. For example  the frequency of “low” is 5  then we rephrase it to “l o w </w>”: 5 4. Generating a new subword according to the high frequency occurrence. 5. Repeating step 4 until reaching subword vocabulary size which is defined in step 2 or the next highest frequency pair is 1.  Values of the tuning parameters (folds  epochs  batch_size) was mostly implicated by the kaggle GPU power and competition constrain of kernel computation limitation to 2 hours run-time.  Final step was calculation of predicitons taking into acount results averaged results for folds. Weights have been assigned by empricialy tring different values. The change of particular ones was based on the prediction score. Limitation was only the summing up of weights to one. Change of arithmetic mean of folds predictions to weighted average improved results in public leaderboard from 0.38459 to 0.38798. On the other hand as the final scores on the private leaderboard showed it was not good choice. Finally  soo strictly assignment of weights caused the decrease in final result from 0.36925 to 0.36724.  XLNet was also tested (to show it  the code with it was left commented). In theory XLNet should overcome BERT limitations. Relying on corrupting the input with masks  BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons XLNet  which is characterised by: - learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order; - overcomes the limitations of BERT thanks to its autoregressive formulation; - integrates ideas from Transformer-XL  the state-of-the-art autoregressive model  into pretraining. Empirically  under comparable experiment settings  XLNet outperforms BERT on 20 tasks  often by a large margin  including question answering  natural language inference  sentiment analysis  and document ranking [6].    After all public score for XLNet version was lower (0.36310) than score (0.37886) for base BERT model and was rejected.     ""In this competition  you’re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites  in a ""common-sense"" fashion. Our raters received minimal guidance and training  and relied largely on their subjective interpretation of the prompts. As such  each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines  we hope to increase the re-use value of this data set.""[1]     """;Natural Language Processing;https://github.com/bluejurand/Kaggle_QA_Google_Labeling
"""Question-answering problem is currenlty one of the most chellenging task in Natural Language Processing domain. In purpose to solve it transfer learning is state of the art method. Thanks to huggingface-transformers which made avaiable pretrained NLP most advanced models (like: BERT   GPT-2  XLNet  RoBERTa  DistilBERT) relatively easy to be used in different language tasks.   Original [akensert](https://www.kaggle.com/akensert/quest-bert-base-tf2-0) code was tested with different parameters and changed base models. From both implemented (XLNet  RoBERTa) the second one resulted in better score. Further improvement ccould be made by implementation of combined model version. For example it could consists of BERT  RoBERTa and XLNet.   Change of main algorithm from BERT to RoBERTa was justified by the fact that second one is an improved version of the first one. The expansion of the algorithm name is Robustly Optimized BERT Pretraining Approach  it modifications consists of [5]: - training the model longer  with bigger batches  over more data;  - removing the next sentence prediction objective;  - training on longer sequences;  - dynamically changing the masking pattern applied to the training data.  Use of RoBERTa consequently causes the need of configuration change and implementation of RoBERTa sepcific tokenizer. It constructs a RoBERTa BPE tokenizer  derived from the GPT-2 tokenizer  using byte-level Byte-Pair-Encoding. Which works in that order: 1. Prepare a large enough training data (i.e. corpus) 2. Define a desired subword vocabulary size 3. Split word to sequence of characters and appending suffix “</w>” to end of word with word frequency. So the basic unit is character in this stage. For example  the frequency of “low” is 5  then we rephrase it to “l o w </w>”: 5 4. Generating a new subword according to the high frequency occurrence. 5. Repeating step 4 until reaching subword vocabulary size which is defined in step 2 or the next highest frequency pair is 1.  Values of the tuning parameters (folds  epochs  batch_size) was mostly implicated by the kaggle GPU power and competition constrain of kernel computation limitation to 2 hours run-time.  Final step was calculation of predicitons taking into acount results averaged results for folds. Weights have been assigned by empricialy tring different values. The change of particular ones was based on the prediction score. Limitation was only the summing up of weights to one. Change of arithmetic mean of folds predictions to weighted average improved results in public leaderboard from 0.38459 to 0.38798. On the other hand as the final scores on the private leaderboard showed it was not good choice. Finally  soo strictly assignment of weights caused the decrease in final result from 0.36925 to 0.36724.  XLNet was also tested (to show it  the code with it was left commented). In theory XLNet should overcome BERT limitations. Relying on corrupting the input with masks  BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons XLNet  which is characterised by: - learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order; - overcomes the limitations of BERT thanks to its autoregressive formulation; - integrates ideas from Transformer-XL  the state-of-the-art autoregressive model  into pretraining. Empirically  under comparable experiment settings  XLNet outperforms BERT on 20 tasks  often by a large margin  including question answering  natural language inference  sentiment analysis  and document ranking [6].    After all public score for XLNet version was lower (0.36310) than score (0.37886) for base BERT model and was rejected.     ""In this competition  you’re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites  in a ""common-sense"" fashion. Our raters received minimal guidance and training  and relied largely on their subjective interpretation of the prompts. As such  each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines  we hope to increase the re-use value of this data set.""[1]     """;General;https://github.com/bluejurand/Kaggle_QA_Google_Labeling
"""This repo contains the code to train and evaluate FCN8 network as described in [A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images](https://arxiv.org/submit/1741331). We investigate the use of [Fully Convolutional Neural Networks](https://arxiv.org/abs/1608.06993) for Endoluminal Scene Segmentation  and report state of the art results on EndoScene dataset.   """;General;https://github.com/jbernoz/deeppolyp
"""This repo contains the code to train and evaluate FCN8 network as described in [A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images](https://arxiv.org/submit/1741331). We investigate the use of [Fully Convolutional Neural Networks](https://arxiv.org/abs/1608.06993) for Endoluminal Scene Segmentation  and report state of the art results on EndoScene dataset.   """;Computer Vision;https://github.com/jbernoz/deeppolyp
"""This is the official code of [High-Resolution Representations for Object Detection](https://arxiv.org/pdf/1904.04514.pdf). We extend the high-resolution representation (HRNet) [1] by augmenting the high-resolution representation by aggregating the (upsampled) representations from all the parallel convolutions  leading to stronger representations. We build a multi-level representation from the high resolution and apply it to the Faster R-CNN  Mask R-CNN and Cascade R-CNN framework. This proposed approach achieves superior results to existing single-model networks  on COCO object detection. The code is based on [mmdetection](https://github.com/open-mmlab/mmdetection)  <div align=center>  ![](images/hrnetv2p.png)  </div>     """;Computer Vision;https://github.com/HRNet/HRNet-Object-Detection
"""UniformAugment is an automated data augmentation approach that completely avoids a search phase. UniformAugment’s effectiveness is comparable to the known methods  while still being highly efficient by virtue of not requiring any search.   """;Computer Vision;https://github.com/tgilewicz/uniformaugment
"""https://dreamtolearn.com/ryan/cognitivewingman/18/en  Character Cartridges - Embodied Identity  We are entering a magical convergence phase in media and technology. The next dozen years through 2030 are going to be very interesting as we begin to understand how to develop character and identity for sensemaking systems  leveraging AI.  Multiple technologies – including in mobile  AR  and deep learning - are rapidly converging to enable organizations to compose AI-powered systems only dreamt of in Sci-Fi novels and Hollywood movies.    These sensemaking (and empathetic) systems will quickly enable assistants who can play roles that include a “Cognitive Wingman” – similar to the automated intelligence seen in media: JARVIS (Iron Man); KITT (Knight Rider); HAL (Space Odyssey); Samantha (Her); TARS (Interstellar).      These systems will: · Talk and listen · Have identity · Have relationships · Are situationally aware · Reason  Understand and Learn · Understand context and remember things · Can hold state for multiple ‘conversation turns’ · Behave in a manner that simulates emotional intelligence     With readily available technology - can build alpha versions of these systems today.  Mind you many POC's quite crappy – but it’s a start - and demonstrates feasibility.  And with widely available ML tools and techniques  and our human tendency to improve on things – good stuff will happen soon.     A key component for the creation of Digital Humans (for applications extending well beyond gaming) is a sense of identity and character.   Empathetic systems that embody cognitive elements need personality.   The best rendered face and eyes  is still just a collection of high resolution pixels – until we add voice  emotion  identity and soul.    """;Natural Language Processing;https://github.com/rustyoldrake/Character-Cartridges-Embodied-Identity
"""该系统实现了基于深度框架的语音识别中的声学模型和语言模型建模，其中声学模型包括CNN-CTC、GRU-CTC、CNN-RNN-CTC，语言模型包含[transformer](https://jalammar.github.io/illustrated-transformer/)、[CBHG](https://github.com/crownpku/Somiao-Pinyin)，数据集包含stc、primewords、Aishell、thchs30四个数据集。  本系统更整体介绍：https://blog.csdn.net/chinatelecom08/article/details/82557715  本项目现已训练一个迷你的语音识别系统，将项目下载到本地上，下载[thchs数据集](http://www.openslr.org/resources/18/data_thchs30.tgz)并解压至data，运行`test.py`，不出意外能够进行识别，结果如下：       the  0 th example.     文本结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文汉字： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然     识别结果： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然  若自己建立模型则需要删除现有模型，重新配置参数训练，具体实现流程参考本页最后。   """;General;https://github.com/swimmingCreative/DeepSpeechRecognition-master
"""该系统实现了基于深度框架的语音识别中的声学模型和语言模型建模，其中声学模型包括CNN-CTC、GRU-CTC、CNN-RNN-CTC，语言模型包含[transformer](https://jalammar.github.io/illustrated-transformer/)、[CBHG](https://github.com/crownpku/Somiao-Pinyin)，数据集包含stc、primewords、Aishell、thchs30四个数据集。  本系统更整体介绍：https://blog.csdn.net/chinatelecom08/article/details/82557715  本项目现已训练一个迷你的语音识别系统，将项目下载到本地上，下载[thchs数据集](http://www.openslr.org/resources/18/data_thchs30.tgz)并解压至data，运行`test.py`，不出意外能够进行识别，结果如下：       the  0 th example.     文本结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文汉字： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然     识别结果： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然  若自己建立模型则需要删除现有模型，重新配置参数训练，具体实现流程参考本页最后。   """;Natural Language Processing;https://github.com/swimmingCreative/DeepSpeechRecognition-master
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/abhiagg92/bert_sentiment
"""This project provides an implementation for our CVPR2021 paper ""[OPANAS: One-Shot Path Aggregation Network Architecture Search for Object Detection](https://arxiv.org/abs/2103.04507)"" on PyTorch. The search code is coming soon.   """;Computer Vision;https://github.com/VDIGPKU/OPANAS
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/abhiagg92/bert_sentiment
"""**This project aims at building a speech enhancement system to attenuate environmental noise.**  <img src=""img/denoise_10classes.gif"" alt=""Spectrogram denoising"" title=""Speech enhancement""/>    Audios have many different ways to be represented  going from raw time series to time-frequency decompositions. The choice of the representation is crucial for the performance of your system. Among time-frequency decompositions  Spectrograms have been proved to be a useful representation for audio processing. They consist in 2D images representing sequences of Short Time Fourier Transform (STFT) with time and frequency as axes  and brightness representing the strength of a frequency component at each time frame. In such they appear a natural domain to apply the CNNS architectures for images directly to sound. Between magnitude and phase spectrograms  magnitude spectrograms contain most the structure of the signal. Phase spectrograms appear to show only little temporal and spectral regularities.  In this project  I will use magnitude spectrograms as a representation of sound (cf image below) in order to predict the noise model to be subtracted to a noisy voice spectrogram.  <img src=""img/sound_to_spectrogram.png"" alt=""sound representation"" title=""sound representation"" />  The project is decomposed in three modes: `data creation`  `training` and `prediction`.   """;Computer Vision;https://github.com/44aayush/Denoising-and-Enhancement
"""This repository contains solution of NER task based on PyTorch [reimplementation](https://github.com/huggingface/pytorch-pretrained-BERT) of [Google's TensorFlow repository for the BERT model](https://github.com/google-research/bert) that was released together with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin  Ming-Wei Chang  Kenton Lee and Kristina Toutanova.  This implementation can load any pre-trained TensorFlow checkpoint for BERT (in particular [Google's pre-trained models](https://github.com/google-research/bert)).  Old version is in ""old"" branch.   """;Natural Language Processing;https://github.com/sberbank-ai/ner-bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/JohannLee1996/bert
"""**GCNet** is initially described in [arxiv](https://arxiv.org/abs/1904.11492). Via absorbing advantages of Non-Local Networks (NLNet) and Squeeze-Excitation Networks (SENet)   GCNet provides a simple  fast and effective approach for global context modeling  which generally outperforms both NLNet and SENet on major benchmarks for various recognition tasks.   """;Computer Vision;https://github.com/xvjiarui/GCNet
"""**GCNet** is initially described in [arxiv](https://arxiv.org/abs/1904.11492). Via absorbing advantages of Non-Local Networks (NLNet) and Squeeze-Excitation Networks (SENet)   GCNet provides a simple  fast and effective approach for global context modeling  which generally outperforms both NLNet and SENet on major benchmarks for various recognition tasks.   """;General;https://github.com/xvjiarui/GCNet
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This is my adjustment of the code base in https://github.com/ultralytics/yolov3 for using YOLOv3 in PyTorch. This model has been customized to discriminate between drones and airplanes  with future plans for including birds and other categories that may assist monitoring technologies in airports and defense applications.  The existing dataset must be enriched for the drone category to encompass more numerous settings and machine models.   This directory contains PyTorch YOLOv3 software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/sergiosonline/transfercv
"""    - In this project I tried to train autoencoder from scratch which can colorize grayscale images.     - For encoder I used Resnet-18 Model [0-6]  and for decoder I used upsampling in pytorch.     - This project is inspired from paper Colorful Image Colorization by Richard Zhang.       """;General;https://github.com/pravesh-bawangade/Image-colorization-using-Autoencoders
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/coconutzs/PointNet_zs
"""BERT-base and BERT-large are respectively 110M and 340M parameters models and it can be difficult to fine-tune them on a single GPU with the recommended batch size for good performance (in most case a batch size of 32).  To help with fine-tuning these models  we have included several techniques that you can activate in the fine-tuning scripts [`run_classifier.py`](./examples/run_classifier.py) and [`run_squad.py`](./examples/run_squad.py): gradient-accumulation  multi-gpu training  distributed training and 16-bits training . For more details on how to use these techniques you can read [the tips on training large batches in PyTorch](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255) that I published earlier this month.  Here is how to use these techniques in our scripts:  - **Gradient Accumulation**: Gradient accumulation can be used by supplying a integer greater than 1 to the `--gradient_accumulation_steps` argument. The batch at each step will be divided by this integer and gradient will be accumulated over `gradient_accumulation_steps` steps. - **Multi-GPU**: Multi-GPU is automatically activated when several GPUs are detected and the batches are splitted over the GPUs. - **Distributed training**: Distributed training can be activated by supplying an integer greater or equal to 0 to the `--local_rank` argument (see below). - **16-bits training**: 16-bits training  also called mixed-precision training  can reduce the memory requirement of your model on the GPU by using half-precision training  basically allowing to double the batch size. If you have a recent GPU (starting from NVIDIA Volta architecture) you should see no decrease in speed. A good introduction to Mixed precision training can be found [here](https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/) and a full documentation is [here](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html). In our scripts  this option can be activated by setting the `--fp16` flag and you can play with loss scaling using the `--loss_scale` flag (see the previously linked documentation for details on loss scaling). The loss scale can be zero in which case the scale is dynamically adjusted or a positive power of two in which case the scaling is static.  To use 16-bits training and distributed training  you need to install NVIDIA's apex extension [as detailed here](https://github.com/nvidia/apex). You will find more information regarding the internals of `apex` and how to use `apex` in [the doc and the associated repository](https://github.com/nvidia/apex). The results of the tests performed on pytorch-BERT by the NVIDIA team (and my trials at reproducing them) can be consulted in [the relevant PR of the present repository](https://github.com/huggingface/pytorch-pretrained-BERT/pull/116).  Note: To use *Distributed Training*  you will need to run one training script on each of your machines. This can be done for example by running the following command on each server (see [the above mentioned blog post]((https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)) for more details): ```bash python -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=$THIS_MACHINE_INDEX --master_addr=""192.168.1.1"" --master_port=1234 run_classifier.py (--arg1 --arg2 --arg3 and all other arguments of the run_classifier script) ``` Where `$THIS_MACHINE_INDEX` is an sequential index assigned to each of your machine (0  1  2...) and the machine with rank 0 has an IP address `192.168.1.1` and an open port `1234`.   """;Natural Language Processing;https://github.com/dnanhkhoa/pytorch-pretrained-BERT
"""Attention: I started this work as I thought this would be a great idea and a good application of deep learning in a health setting. However  it turned out that the availability of (labelled) data is a major issue  as it is for a lot of ML projects. I did not succeed in getting access to enough data so that I stopped working on it. Based on the results  I'm pretty sure this should work  but you can't really tell unless you experiement with enough real world data. In September 2020  there was a publication on general wound classification (8) which has a broader look into this problem. This work also refers to the Medetec Wound Database (9).  If you are interested in continuing this work and have access to data  I'd be happy to exchange ideas.  One further idea was to not only to classify the burn wounds  but also to determine the percentage of the body that is burnt with a specific grade. Based on recent work around dense pose estimation (7) and 3D body part reconstruction  this should also be valid based on existing work.   """;Computer Vision;https://github.com/CarstenIsert/DeepBurn
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/bingnoi/yolo_py
"""***""the discriminator estimates the probability that the given real data is more realistic than a randomly sampled fake data""***   *= RGAN*  ***""the discriminator estimates the probability that the given real data is more realistic than fake data  on average""***   *= RaGAN*  """;Computer Vision;https://github.com/taki0112/RelativisticGAN-Tensorflow
"""This project aims to implement biomedical image segmentation with the use of U-Net model. The below image briefly explains the output we want:  <p align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/segmentation_image.jpg"">   The dataset we used is Transmission Electron Microscopy (ssTEM) data set of the Drosophila first instar larva ventral nerve cord (VNC)  which is dowloaded from [ISBI Challenge: Segmentation of of neural structures in EM stacks](http://brainiac2.mit.edu/isbi_challenge/home)  The dataset contains 30 images (.png) of size 512x512 for each train  train-labels and test.    """;Computer Vision;https://github.com/shermanhung/U-Net
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/zengzhaoyang/Weak_Detection
"""The aim of this write up is to describe the process of training a convolutional neural network to complete at least one lap of bot tracks provided. I will begin by setting the goals of the project  then I will give a walkthrough of the process  the decisions I made and parameters chosen. I will then describe the results of the process  followed by a discussion about the limitations and possible improvements.  ![Original image](./images/orig_image.jpg)   """;General;https://github.com/barrykidney/CarND-Behavioral-Cloning-P3
"""I used the shape() property to get the shapes of of training  validation and test datasets. Shape can also be used to find the shape of traffic sign images. Number of classes can be found out using signnames.csv or finding unique entries in the training set - I use the latter  * The size of training set is 34799 * The size of the validation set is 4410 * The size of test set is 12630 * The shape of a traffic sign image is (32  32  3) * The number of unique classes/labels in the data set is 43   """;Computer Vision;https://github.com/utsawk/CarND-Traffic-Sign-Classifier-Project
"""The Adversarial Autoencoder behaves similarly to [Variational Autoencoders](https://arxiv.org/abs/1312.6114)  forcing the latent space of an autoencoder to follow a predefined prior. In the case of the Adversarial Autoencoder  this latent space can be defined arbitrarily and easily sampled and fed into the Discriminator in the network.  <div> <img src=""https://raw.githubusercontent.com/greentfrapp/keras-aae/master/images/aae_latent.png"" alt=""Latent space from Adversarial Autoencoder"" width=""whatever"" height=""300px"" style=""display: inline-block;""> <img src=""https://raw.githubusercontent.com/greentfrapp/keras-aae/master/images/regular_latent.png"" alt=""Latent space from regular Autoencoder"" width=""whatever"" height=""300px"" style=""display: inline-block;""> </div>  *The left image shows the latent space of an unseen MNIST test set after training with an Adversarial Autoencoder for 50 epochs  which follows a 2D Gaussian prior. Contrast this with the latent space of the regular Autoencoder trained under the same conditions  with a far more irregular latent distribution.*   """;Computer Vision;https://github.com/greentfrapp/keras-aae
"""The Adversarial Autoencoder behaves similarly to [Variational Autoencoders](https://arxiv.org/abs/1312.6114)  forcing the latent space of an autoencoder to follow a predefined prior. In the case of the Adversarial Autoencoder  this latent space can be defined arbitrarily and easily sampled and fed into the Discriminator in the network.  <div> <img src=""https://raw.githubusercontent.com/greentfrapp/keras-aae/master/images/aae_latent.png"" alt=""Latent space from Adversarial Autoencoder"" width=""whatever"" height=""300px"" style=""display: inline-block;""> <img src=""https://raw.githubusercontent.com/greentfrapp/keras-aae/master/images/regular_latent.png"" alt=""Latent space from regular Autoencoder"" width=""whatever"" height=""300px"" style=""display: inline-block;""> </div>  *The left image shows the latent space of an unseen MNIST test set after training with an Adversarial Autoencoder for 50 epochs  which follows a 2D Gaussian prior. Contrast this with the latent space of the regular Autoencoder trained under the same conditions  with a far more irregular latent distribution.*   """;General;https://github.com/greentfrapp/keras-aae
"""I used the shape() property to get the shapes of of training  validation and test datasets. Shape can also be used to find the shape of traffic sign images. Number of classes can be found out using signnames.csv or finding unique entries in the training set - I use the latter  * The size of training set is 34799 * The size of the validation set is 4410 * The size of test set is 12630 * The shape of a traffic sign image is (32  32  3) * The number of unique classes/labels in the data set is 43   """;General;https://github.com/utsawk/CarND-Traffic-Sign-Classifier-Project
"""该工作处理的是图像的二分类问题。判断是否为人脸，做为人脸检测的一个模块。主体模型是[ResNet](https://arxiv.org/abs/1512.03385)   """;General;https://github.com/xcmyz/FaceDetection
"""该工作处理的是图像的二分类问题。判断是否为人脸，做为人脸检测的一个模块。主体模型是[ResNet](https://arxiv.org/abs/1512.03385)   """;Computer Vision;https://github.com/xcmyz/FaceDetection
"""A simple implementation of Generative Adversarial Networks (https://arxiv.org/pdf/1406.2661.pdf) in tensorflow 2 (alpha) on simple 2d point datasets as well as MNIST Goal is just to see the basics of GAN training as well as tensorflow 2.  Currently  only network models are mulilayer perceptrons (no CNNs). ![Alt text](readme_images/sin.gif?raw=true ""Sin data"") ![Alt text](readme_images/mnist.gif?raw=true ""MNIST data"")   """;Computer Vision;https://github.com/alectryonexamples/gan_tf2
"""SqueezeNet is focused on size and performance over outright accuracy  however  it still achieved AlexNet-level accuracy in the paper by Iandola in 2016. The actual SqueezeNet architecture is different than what I will refer to as 'Squeeze Net' so I encourage you to read the paper (cited below) and visit the [Deepscale/SqueezeNet github page](https://github.com/deepscale/squeezenet). My model did not reach [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)-level accuracy (89%) but did reach approximately 80% with only 122k parameters (AlexNet is ~ 60million  VGG is 130million+). Additionally  my model is much smaller than even that referenced in the SqueezeNet paper.    """;Computer Vision;https://github.com/zshancock/SqueezeNet_vs_CIFAR10
"""The masks we generate are of two kinds and they both used OpenCV to draw their shapes. The first method contained in the script *utils/Mask_generator.py* creates masks composed of ellipses  lines and circle of random size and position. The second method contained in the script *utils/Mask_generator_circle.py* creates circular masks with center and radius that can vary according to one's preference. For simplicity  in the analysis done so far  the circular masks were always centered at the center of the images.   """;Computer Vision;https://github.com/GabrieleMonte/CoMBInE
"""A simple implementation of Generative Adversarial Networks (https://arxiv.org/pdf/1406.2661.pdf) in tensorflow 2 (alpha) on simple 2d point datasets as well as MNIST Goal is just to see the basics of GAN training as well as tensorflow 2.  Currently  only network models are mulilayer perceptrons (no CNNs). ![Alt text](readme_images/sin.gif?raw=true ""Sin data"") ![Alt text](readme_images/mnist.gif?raw=true ""MNIST data"")   """;General;https://github.com/alectryonexamples/gan_tf2
"""With the rapid growth of Artificial Intelligence (AI)  multimodal emotion recognition has become a major research topic  primarily due to its potential applications in many challenging tasks  such as dialogue generation  user behavior understanding  multimodal interaction  and others. A conversational emotion recognition system can be used to generate appropriate responses by analyzing user emotions (Zhou et al.  2017; Rashkin et al.  2018). In Dialogue Conversations  the utterance by a speaker y (say) depends on the utterance/statement given previously by a speaker x. That there occurs a correlation between the sequence of utterances. This is depicted in Figure-1  where 2 speakers are conversing and there is an emotional shift visible as the conversation proceeds. <br>  ![Figure 1: MELD_about_data](MELD.PNG) <br> **Figure 1: MELD_about_data** <br>   Conversation in its natural form is multimodal. In dialogues  we rely on others’ facial expressions  vocal tonality  language  and gestures to anticipate their stance. For emotion recognition  multimodality is particularly important. For the utterances with language that is difficult to understand  we often resort to other modalities  such as prosodic and visual cues  to identify their emotions. Figure-2 presents examples from the dataset where the presence of multimodal signals in addition to the text itself is necessary in order to make correct predictions of their emotions and sentiments. Multimodal emotion recognition of sequential turns encounters several other challenges. One such example is the classification of short utterances. Utterances like “yeah”  “okay”  “no” can express varied emotions depending on the context and discourse of the dialogue. However  due to the difficulty of perceiving emotions from text alone  most models resort to assigning the majority class. <br> ![Figure 2: Importance of Multimodal Cues](MELD_fig2.png) <br> **Figure 2: Importance of Multimodal Cues** <br> We use MELD [Multimodal Multi-party Dataset for Emotion Recognitions in Conversations Dataset](https://arxiv.org/pdf/1810.02508.pdf). The [dataset](https://github.com/SenticNet/MELD) contains the same dialogue instances available in EmotionLines  but it also encompasses audio and visual modality along with the text. MELD has more than 1400 dialogues and 13000 utterances from Friends TV series. Multiple speakers participated in the dialogues. Each utterance in dialogue has been labeled by any of these seven emotions -- Anger  Disgust  Sadness  Joy  Neutral  Surprise and Fear. MELD also has sentiment (positive  negative and neutral) annotation for each utterance. The other publicly available multimodal emotion and sentiment recognition datasets are MOSEI  MOSI  MOUD. However  none of those datasets is conversational. <br>   """;General;https://github.com/ankurbhatia24/MULTIMODAL-EMOTION-RECOGNITION
"""With the rapid growth of Artificial Intelligence (AI)  multimodal emotion recognition has become a major research topic  primarily due to its potential applications in many challenging tasks  such as dialogue generation  user behavior understanding  multimodal interaction  and others. A conversational emotion recognition system can be used to generate appropriate responses by analyzing user emotions (Zhou et al.  2017; Rashkin et al.  2018). In Dialogue Conversations  the utterance by a speaker y (say) depends on the utterance/statement given previously by a speaker x. That there occurs a correlation between the sequence of utterances. This is depicted in Figure-1  where 2 speakers are conversing and there is an emotional shift visible as the conversation proceeds. <br>  ![Figure 1: MELD_about_data](MELD.PNG) <br> **Figure 1: MELD_about_data** <br>   Conversation in its natural form is multimodal. In dialogues  we rely on others’ facial expressions  vocal tonality  language  and gestures to anticipate their stance. For emotion recognition  multimodality is particularly important. For the utterances with language that is difficult to understand  we often resort to other modalities  such as prosodic and visual cues  to identify their emotions. Figure-2 presents examples from the dataset where the presence of multimodal signals in addition to the text itself is necessary in order to make correct predictions of their emotions and sentiments. Multimodal emotion recognition of sequential turns encounters several other challenges. One such example is the classification of short utterances. Utterances like “yeah”  “okay”  “no” can express varied emotions depending on the context and discourse of the dialogue. However  due to the difficulty of perceiving emotions from text alone  most models resort to assigning the majority class. <br> ![Figure 2: Importance of Multimodal Cues](MELD_fig2.png) <br> **Figure 2: Importance of Multimodal Cues** <br> We use MELD [Multimodal Multi-party Dataset for Emotion Recognitions in Conversations Dataset](https://arxiv.org/pdf/1810.02508.pdf). The [dataset](https://github.com/SenticNet/MELD) contains the same dialogue instances available in EmotionLines  but it also encompasses audio and visual modality along with the text. MELD has more than 1400 dialogues and 13000 utterances from Friends TV series. Multiple speakers participated in the dialogues. Each utterance in dialogue has been labeled by any of these seven emotions -- Anger  Disgust  Sadness  Joy  Neutral  Surprise and Fear. MELD also has sentiment (positive  negative and neutral) annotation for each utterance. The other publicly available multimodal emotion and sentiment recognition datasets are MOSEI  MOSI  MOUD. However  none of those datasets is conversational. <br>   """;Natural Language Processing;https://github.com/ankurbhatia24/MULTIMODAL-EMOTION-RECOGNITION
"""This project aims to implement biomedical image segmentation with the use of U-Net model. The below image briefly explains the output we want:  <p align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/segmentation_image.jpg"">   The dataset we used is Transmission Electron Microscopy (ssTEM) data set of the Drosophila first instar larva ventral nerve cord (VNC)  which is dowloaded from [ISBI Challenge: Segmentation of of neural structures in EM stacks](http://brainiac2.mit.edu/isbi_challenge/home)  The dataset contains 30 images (.png) of size 512x512 for each train  train-labels and test.    """;Computer Vision;https://github.com/rickyHong/UNet-segmentation-pytorch-repl
"""This repository contains an example program to demonstrate the functionality of Gaussian processes and Bayesian Neural Networks who approximate Gaussian Processes under certain conditions  as shown by (Gal  2016): https://arxiv.org/pdf/1506.02142.pdf. \ In this example  a Gaussian Process for a simple regression task is implemented to demonstrate its prior and posterior function distribution. Then  a Bayesian Neural Network is trained which approximates the Gaussian process by variational inference. Again  the posterior distribution is plotted.    """;General;https://github.com/arneschmidt/bayesian_deep_learning
"""------------ Reimplementation of [Soft Actor-Critic Algorithms and Applications](https://arxiv.org/pdf/1812.05905.pdf) and a deterministic variant of SAC from [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/pdf/1801.01290.pdf).  Added another branch for [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/pdf/1801.01290.pdf) -> [SAC_V](https://github.com/pranz24/pytorch-soft-actor-critic/tree/SAC_V).   """;Reinforcement Learning;https://github.com/pranz24/pytorch-soft-actor-critic
""" **BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/MOHAMEDELDAKDOUKY/bert_adjusted
""" **BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/MOHAMEDELDAKDOUKY/bert_adjusted
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/kelly2016/multi-label-bert
"""This is an implementation of our SV-X-Softmax loss by **Pytorch** library. The repository contains the fc_layers.py and loss.py The old version: ""Support Vector Guided Softmax Loss for Face Recognition"" [arxiv](https://arxiv.org/abs/1812.11317) \ is implemented by **Caffe** library and does not remove the overlaps between training set and test set. The performance comparsion  may not be fair in the old version.   """;General;https://github.com/xiaoboCASIA/SV-X-Softmax
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/autobotasia/vibert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/autobotasia/vibert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/EuphoriaYan/bert_component
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/EuphoriaYan/bert_component
"""- Implementations of RL algorithms with tensorflow - Run in ""CartPole-v1"" environment.   """;Reinforcement Learning;https://github.com/InSpaceAI/RL-Zoo
"""- Implementations of RL algorithms with tensorflow - Run in ""CartPole-v1"" environment.   """;General;https://github.com/InSpaceAI/RL-Zoo
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/kelly2016/multi-label-bert
"""This repository walks through an example of LIME(Local Interpretable Model-Agnostic Explanations). Orginial LIME paper - https://arxiv.org/abs/1602.04938  LIME provides a means to explain any black-box classifier or regressor.   Models can be difficult to analyze on a global level but may be possible to analyze for a specific instance.  Desirable characterstics of an explainable model: - Interpretable - Local Fidelity - Model-Agnostic - Global Perspective   """;General;https://github.com/blazecolby/PyTorch-LIME
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/ricardordb/bert
"""This repo contains imlementation of the the `Pointer Sentinel Mixture Model`  as described in the [paper](https://arxiv.org/abs/1609.07843) by Stephen Merity et al.  See my [blog post](https://elanmart.github.io/2018-02-10-psmm) for details.  See [model.py](https://github.com/elanmart/psmm/blob/master/psmm/model.py) for the core of this architecture.  """;Sequential;https://github.com/elanmart/psmm
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/coco60/bert-test
"""This Repository includes YOLOv3 with some lightweight backbones (***ShuffleNetV2  GhostNet  VoVNet***)  some computer vision attention mechanism (***SE Block  CBAM Block  ECA Block***)  pruning quantization and distillation for GhostNet.  """;Computer Vision;https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention
"""**Dense RepPoints** utilizes a dense point set to describe the multi-grained object representation of both box level and pixel level. The following figure illustrates the representation of object segments in different forms using Dense RepPoints. The key techniques to learn such representation are a **distance transform sampling (DTS)** method combined with a **set-to-set supervision** method. In inference  both the **concave hull** and **triangulation** methods are supported. The method is also efficient  achieving near constant complexity with increasing point number. Please touch [arXiv](https://arxiv.org/abs/1912.11473) for more details.   <div align=""center"">   <img src=""demo/dense_reppoints.png"" width=""250px"" />  <img src=""demo/inference_mask.png"" width=""250px"" /> <img src=""demo/pts_seg.png"" width=""295px"" />     <p>Learning Dense RepPoints in Object Detection and Instance Segmentation.</p> </div>   """;General;https://github.com/justimyhxu/Dense-RepPoints
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/socc-io/piqaboo
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/socc-io/piqaboo
"""This Repository includes YOLOv3 with some lightweight backbones (***ShuffleNetV2  GhostNet  VoVNet***)  some computer vision attention mechanism (***SE Block  CBAM Block  ECA Block***)  pruning quantization and distillation for GhostNet.  """;General;https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention
"""```python device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') model = ResNetUNet(n_class=6) model = model.to(device)  #: check keras-like model summary using torchsummary from torchsummary import summary summary(model  input_size=(3  224  224)) ```      ----------------------------------------------------------------             Layer (type)               Output Shape         Param #     ================================================================                 Conv2d-1         [-1  64  224  224]           1 792                   ReLU-2         [-1  64  224  224]               0                 Conv2d-3         [-1  64  224  224]          36 928                   ReLU-4         [-1  64  224  224]               0                 Conv2d-5         [-1  64  112  112]           9 408            BatchNorm2d-6         [-1  64  112  112]             128                   ReLU-7         [-1  64  112  112]               0              MaxPool2d-8           [-1  64  56  56]               0                 Conv2d-9           [-1  64  56  56]           4 096           BatchNorm2d-10           [-1  64  56  56]             128                  ReLU-11           [-1  64  56  56]               0                Conv2d-12           [-1  64  56  56]          36 864           BatchNorm2d-13           [-1  64  56  56]             128                  ReLU-14           [-1  64  56  56]               0                Conv2d-15          [-1  256  56  56]          16 384           BatchNorm2d-16          [-1  256  56  56]             512                Conv2d-17          [-1  256  56  56]          16 384           BatchNorm2d-18          [-1  256  56  56]             512                  ReLU-19          [-1  256  56  56]               0            Bottleneck-20          [-1  256  56  56]               0                Conv2d-21           [-1  64  56  56]          16 384           BatchNorm2d-22           [-1  64  56  56]             128                  ReLU-23           [-1  64  56  56]               0                Conv2d-24           [-1  64  56  56]          36 864           BatchNorm2d-25           [-1  64  56  56]             128                  ReLU-26           [-1  64  56  56]               0                Conv2d-27          [-1  256  56  56]          16 384           BatchNorm2d-28          [-1  256  56  56]             512                  ReLU-29          [-1  256  56  56]               0            Bottleneck-30          [-1  256  56  56]               0                Conv2d-31           [-1  64  56  56]          16 384           BatchNorm2d-32           [-1  64  56  56]             128                  ReLU-33           [-1  64  56  56]               0                Conv2d-34           [-1  64  56  56]          36 864           BatchNorm2d-35           [-1  64  56  56]             128                  ReLU-36           [-1  64  56  56]               0                Conv2d-37          [-1  256  56  56]          16 384           BatchNorm2d-38          [-1  256  56  56]             512                  ReLU-39          [-1  256  56  56]               0            Bottleneck-40          [-1  256  56  56]               0                Conv2d-41          [-1  128  56  56]          32 768           BatchNorm2d-42          [-1  128  56  56]             256                  ReLU-43          [-1  128  56  56]               0                Conv2d-44          [-1  128  28  28]         147 456           BatchNorm2d-45          [-1  128  28  28]             256                  ReLU-46          [-1  128  28  28]               0                Conv2d-47          [-1  512  28  28]          65 536           BatchNorm2d-48          [-1  512  28  28]           1 024                Conv2d-49          [-1  512  28  28]         131 072           BatchNorm2d-50          [-1  512  28  28]           1 024                  ReLU-51          [-1  512  28  28]               0            Bottleneck-52          [-1  512  28  28]               0                Conv2d-53          [-1  128  28  28]          65 536           BatchNorm2d-54          [-1  128  28  28]             256                  ReLU-55          [-1  128  28  28]               0                Conv2d-56          [-1  128  28  28]         147 456           BatchNorm2d-57          [-1  128  28  28]             256                  ReLU-58          [-1  128  28  28]               0                Conv2d-59          [-1  512  28  28]          65 536           BatchNorm2d-60          [-1  512  28  28]           1 024                  ReLU-61          [-1  512  28  28]               0            Bottleneck-62          [-1  512  28  28]               0                Conv2d-63          [-1  128  28  28]          65 536           BatchNorm2d-64          [-1  128  28  28]             256                  ReLU-65          [-1  128  28  28]               0                Conv2d-66          [-1  128  28  28]         147 456           BatchNorm2d-67          [-1  128  28  28]             256                  ReLU-68          [-1  128  28  28]               0                Conv2d-69          [-1  512  28  28]          65 536           BatchNorm2d-70          [-1  512  28  28]           1 024                  ReLU-71          [-1  512  28  28]               0            Bottleneck-72          [-1  512  28  28]               0                Conv2d-73          [-1  128  28  28]          65 536           BatchNorm2d-74          [-1  128  28  28]             256                  ReLU-75          [-1  128  28  28]               0                Conv2d-76          [-1  128  28  28]         147 456           BatchNorm2d-77          [-1  128  28  28]             256                  ReLU-78          [-1  128  28  28]               0                Conv2d-79          [-1  512  28  28]          65 536           BatchNorm2d-80          [-1  512  28  28]           1 024                  ReLU-81          [-1  512  28  28]               0            Bottleneck-82          [-1  512  28  28]               0                Conv2d-83          [-1  256  28  28]         131 072           BatchNorm2d-84          [-1  256  28  28]             512                  ReLU-85          [-1  256  28  28]               0                Conv2d-86          [-1  256  14  14]         589 824           BatchNorm2d-87          [-1  256  14  14]             512                  ReLU-88          [-1  256  14  14]               0                Conv2d-89         [-1  1024  14  14]         262 144           BatchNorm2d-90         [-1  1024  14  14]           2 048                Conv2d-91         [-1  1024  14  14]         524 288           BatchNorm2d-92         [-1  1024  14  14]           2 048                  ReLU-93         [-1  1024  14  14]               0            Bottleneck-94         [-1  1024  14  14]               0                Conv2d-95          [-1  256  14  14]         262 144           BatchNorm2d-96          [-1  256  14  14]             512                  ReLU-97          [-1  256  14  14]               0                Conv2d-98          [-1  256  14  14]         589 824           BatchNorm2d-99          [-1  256  14  14]             512                 ReLU-100          [-1  256  14  14]               0               Conv2d-101         [-1  1024  14  14]         262 144          BatchNorm2d-102         [-1  1024  14  14]           2 048                 ReLU-103         [-1  1024  14  14]               0           Bottleneck-104         [-1  1024  14  14]               0               Conv2d-105          [-1  256  14  14]         262 144          BatchNorm2d-106          [-1  256  14  14]             512                 ReLU-107          [-1  256  14  14]               0               Conv2d-108          [-1  256  14  14]         589 824          BatchNorm2d-109          [-1  256  14  14]             512                 ReLU-110          [-1  256  14  14]               0               Conv2d-111         [-1  1024  14  14]         262 144          BatchNorm2d-112         [-1  1024  14  14]           2 048                 ReLU-113         [-1  1024  14  14]               0           Bottleneck-114         [-1  1024  14  14]               0               Conv2d-115          [-1  256  14  14]         262 144          BatchNorm2d-116          [-1  256  14  14]             512                 ReLU-117          [-1  256  14  14]               0               Conv2d-118          [-1  256  14  14]         589 824          BatchNorm2d-119          [-1  256  14  14]             512                 ReLU-120          [-1  256  14  14]               0               Conv2d-121         [-1  1024  14  14]         262 144          BatchNorm2d-122         [-1  1024  14  14]           2 048                 ReLU-123         [-1  1024  14  14]               0           Bottleneck-124         [-1  1024  14  14]               0               Conv2d-125          [-1  256  14  14]         262 144          BatchNorm2d-126          [-1  256  14  14]             512                 ReLU-127          [-1  256  14  14]               0               Conv2d-128          [-1  256  14  14]         589 824          BatchNorm2d-129          [-1  256  14  14]             512                 ReLU-130          [-1  256  14  14]               0               Conv2d-131         [-1  1024  14  14]         262 144          BatchNorm2d-132         [-1  1024  14  14]           2 048                 ReLU-133         [-1  1024  14  14]               0           Bottleneck-134         [-1  1024  14  14]               0               Conv2d-135          [-1  256  14  14]         262 144          BatchNorm2d-136          [-1  256  14  14]             512                 ReLU-137          [-1  256  14  14]               0               Conv2d-138          [-1  256  14  14]         589 824          BatchNorm2d-139          [-1  256  14  14]             512                 ReLU-140          [-1  256  14  14]               0               Conv2d-141         [-1  1024  14  14]         262 144          BatchNorm2d-142         [-1  1024  14  14]           2 048                 ReLU-143         [-1  1024  14  14]               0           Bottleneck-144         [-1  1024  14  14]               0               Conv2d-145          [-1  512  14  14]         524 288          BatchNorm2d-146          [-1  512  14  14]           1 024                 ReLU-147          [-1  512  14  14]               0               Conv2d-148            [-1  512  7  7]       2 359 296          BatchNorm2d-149            [-1  512  7  7]           1 024                 ReLU-150            [-1  512  7  7]               0               Conv2d-151           [-1  2048  7  7]       1 048 576          BatchNorm2d-152           [-1  2048  7  7]           4 096               Conv2d-153           [-1  2048  7  7]       2 097 152          BatchNorm2d-154           [-1  2048  7  7]           4 096                 ReLU-155           [-1  2048  7  7]               0           Bottleneck-156           [-1  2048  7  7]               0               Conv2d-157            [-1  512  7  7]       1 048 576          BatchNorm2d-158            [-1  512  7  7]           1 024                 ReLU-159            [-1  512  7  7]               0               Conv2d-160            [-1  512  7  7]       2 359 296          BatchNorm2d-161            [-1  512  7  7]           1 024                 ReLU-162            [-1  512  7  7]               0               Conv2d-163           [-1  2048  7  7]       1 048 576          BatchNorm2d-164           [-1  2048  7  7]           4 096                 ReLU-165           [-1  2048  7  7]               0           Bottleneck-166           [-1  2048  7  7]               0               Conv2d-167            [-1  512  7  7]       1 048 576          BatchNorm2d-168            [-1  512  7  7]           1 024                 ReLU-169            [-1  512  7  7]               0               Conv2d-170            [-1  512  7  7]       2 359 296          BatchNorm2d-171            [-1  512  7  7]           1 024                 ReLU-172            [-1  512  7  7]               0               Conv2d-173           [-1  2048  7  7]       1 048 576          BatchNorm2d-174           [-1  2048  7  7]           4 096                 ReLU-175           [-1  2048  7  7]               0           Bottleneck-176           [-1  2048  7  7]               0               Conv2d-177           [-1  1024  7  7]       2 098 176                 ReLU-178           [-1  1024  7  7]               0             Upsample-179         [-1  1024  14  14]               0               Conv2d-180          [-1  512  14  14]         524 800                 ReLU-181          [-1  512  14  14]               0               Conv2d-182          [-1  512  14  14]       7 078 400                 ReLU-183          [-1  512  14  14]               0             Upsample-184          [-1  512  28  28]               0               Conv2d-185          [-1  512  28  28]         262 656                 ReLU-186          [-1  512  28  28]               0               Conv2d-187          [-1  512  28  28]       4 719 104                 ReLU-188          [-1  512  28  28]               0             Upsample-189          [-1  512  56  56]               0               Conv2d-190          [-1  256  56  56]          65 792                 ReLU-191          [-1  256  56  56]               0               Conv2d-192          [-1  256  56  56]       1 769 728                 ReLU-193          [-1  256  56  56]               0             Upsample-194        [-1  256  112  112]               0               Conv2d-195         [-1  64  112  112]           4 160                 ReLU-196         [-1  64  112  112]               0               Conv2d-197        [-1  128  112  112]         368 768                 ReLU-198        [-1  128  112  112]               0             Upsample-199        [-1  128  224  224]               0               Conv2d-200         [-1  64  224  224]         110 656                 ReLU-201         [-1  64  224  224]               0               Conv2d-202          [-1  6  224  224]             390     ================================================================     Total params: 40 549 382     Trainable params: 40 549 382     Non-trainable params: 0     ----------------------------------------------------------------    """;Computer Vision;https://github.com/usuyama/pytorch-unet
"""**AMDIM** (Augmented Multiscale Deep InfoMax) is an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple *views* of a shared *context*.   Our paper describing AMDIM is available at: https://arxiv.org/abs/1906.00910.   """;Computer Vision;https://github.com/cfld/amdim
"""Parameters related to training and evaluation can be set in `train.py`  as follows:  |  Parameters   | default  | description | other | |  ----  |  ----  |  ----  |  ----  | | config| None  Mandatory| Configuration file path || | --eval| None  Optional| Evaluate after an epoch |If you don't select this  you might have trouble finding the best_model| | --fp16| None  Optional| Semi-precision training |If this option is not selected  32GB of video memory may not be sufficient| | --resume| None  Optional | Recovery training |For example: --resume output/yolov2_voc/66|   """;Computer Vision;https://github.com/nuaaceieyty/Paddle-YOLOv2
"""An example diagram of our Cluster-NMS  where X denotes IoU matrix which is calculated by `X=jaccard(boxes boxes).triu_(diagonal=1) > nms_thresh` after sorted by score descending. (Here use 0 1 for visualization.)  <img src=""cluster-nms01.png"" width=""1150px""/> <img src=""cluster-nms02.png"" width=""1150px""/>  The inputs of NMS are `boxes` with size [n 4] and `scores` with size [80 n]. (take coco as example)  There are two ways for NMS. One is that all classes have the same number of boxes. First  we use top k=200 to select the top 200 detections for every class. Then `boxes` will be [80 m 4]  where m<=200. Do Cluster-NMS and keep the boxes with `scores>0.01`. Finally  return top 100 boxes across all classes.  The other approach is that different classes have different numbers of boxes. First  we use a score threshold (e.g. 0.01) to filter out most low score detection boxes. It results in the number of remaining boxes in different classes may be different. Then put all the boxes together and sorted by score descending. (Note that the same box may appear more than once  because its scores of multiple classes are greater than the threshold 0.01.) Adding offset for all the `boxes` according to their class labels. (use `torch.arange(0 80)`.) For example  since the coordinates (x1 y1 x2 y2) of all the boxes are on interval (0 1). By adding offset  if a box belongs to class 61  its coordinates will on interval (60 61). After that  the IoU of boxes belonging to different classes will be 0. (because they are treated as different clusters.) Do Cluster-NMS and return top 100 boxes across all classes. (For this method  please refer to another our repository https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/detection/detection.py)   """;Computer Vision;https://github.com/Zzh-tju/CIoU
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/chenglibo123/123123
"""This repo holds the code for [DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution](https://arxiv.org/pdf/2006.02334.pdf). The project is based on [mmdetection codebase](https://github.com/open-mmlab/mmdetection). Please refer to [mmdetection readme](README.mmdet.md) for installation and running scripts. The code is tested with PyTorch 1.4.0. It may not run with other versions. See [conda_env.md](conda_env.md) for the versions of all the packages.   """;Computer Vision;https://github.com/joe-siyuan-qiao/DetectoRS
"""The full Github link would be in here: https://github.com/StephenEkaputra/Mask_RCNN-TinyPascalVOC We used the repository from: https://github.com/matterport/Mask_RCNN   """;Computer Vision;https://github.com/StephenEkaputra/Mask_RCNN-TinyPascalVOC
"""This work is accepted at the PRIME workshop in MICCAI 2021.  > **Investigating and Quantifying the Reproducibility of Graph Neural Networks in Predictive Medicine** > > Mohammed Amine Gharsallaoui  Furkan Tornaci and Islem Rekik > > BASIRA Lab  Faculty of Computer and Informatics  Istanbul Technical University  Istanbul  Turkey > > **Abstract:** *Graph neural networks (GNNs) have gained an unprecedented attention in many domains including dysconnectivity disorder diagnosis thanks to their high performance in tackling graph classification tasks. Despite the large stream of GNNs developed recently  prior efforts invariably focus on boosting the classification accuracy while ignoring the model reproducibility and interpretability  which are vital in pinning down disorder-specific biomarkers. Although less investigated  the discriminativeness of the original input features -biomarkers  which is reflected by their learnt weights using a GNN gives informative insights about their reliability. Intuitively  the reliability of a given biomarker is emphasized if it belongs to the sets of top discriminative regions of interest (ROIs) using different models. Therefore  we define the first axis in our work as \emph{reproducibility across models}  which evaluates the commonalities between sets of top discriminative biomarkers for a pool of GNNs. This task mainly answers this question: \emph{How likely can two models be congruent in terms of their respective sets of top discriminative biomarkers?} The second axis of research in our work is to investigate \emph{reproducibility in generated connectomic datasets}. This is addressed by answering this question: \emph{how likely would the set of top discriminative biomarkers by a trained model for a ground-truth dataset be consistent with a predicted dataset by generative learning?} In this paper  we propose a reproducibility assessment framework  a method for quantifying the commonalities in the GNN-specific learnt feature maps across models  which can complement explanatory approaches of GNNs and provide new ways to assess predictive medicine via biomarkers reliability. We evaluated our framework using four multiview connectomic datasets of healthy neurologically disordered subjects with five GNN architectures and two different learning mindsets: (a) conventional training on all samples (resourceful) and (b) a few-shot training on random samples (frugal).*    """;Graphs;https://github.com/basiralab/Reproducible-Generative-Learning
"""This demo is a Swift program that uses a pre-trained Inception model (http://arxiv.org/abs/1512.00567).  It can load the prebuilt model into a Perfect TensorFlow Session object  like this:  ``` swift let g = try TF.Graph() let def = try TF.GraphDef(serializedData: model) try g.import(definition: def) ```  Accompanied by this model  a known object name list also would be loaded into memory if success.  ``` swift try fTag.open(.read) let lines = try fTag.readString() tags = lines.utf8.split(separator: 10).map { String(describing: $0) } // the tags should be looks like this if success: // tags = [""dummy""  ""kit fox""  ""English setter""  ""Siberian husky"" ...] ```  Once received a picture from client  it will decode the picture (in jpeg format) and normalize it into a specific form:  ``` swift   public func constructAndExecuteGraphToNormalizeImage(imageBytes: Data) throws -> TF.Tensor {     let H:Int32 = 224     let W:Int32 = 224     let mean:Float = 117     let scale:Float = 1     let input = try g.constant(name: ""input2""  value: imageBytes)     let batch = try g.constant( name: ""make_batch""  value: Int32(0))     let scale_v = try g.constant(name: ""scale""  value: scale)     let mean_v = try g.constant(name: ""mean""  value: mean)     let size = try g.constantArray(name: ""size""  value: [H W])     let jpeg = try g.decodeJpeg(content: input  channels: 3)     let cast = try g.cast(value: jpeg  dtype: TF.DataType.dtFloat)     let images = try g.expandDims(input: cast  dim: batch)     let resizes = try g.resizeBilinear(images: images  size: size)     let subbed = try g.sub(x: resizes  y: mean_v)     let output = try g.div(x: subbed  y: scale_v)     let s = try g.runner().fetch(TF.Operation(output)).run()     guard s.count > 0 else { throw TF.Panic.INVALID }     return s[0]   }//end normalize ```  Then you can run a TensorFlow session from this picture input:  ``` swift let result = try g.runner().feed(""input""  tensor: image).fetch(""output"").run() ```  The result is actually a possibility array which matches the known object name list  i.e.  each object in this name list will have a possibility prediction in the corresponding array slot. So checking the max possibility throughout the array may get the most possible object that the input image could be:  ``` swift public func match(image: Data) throws -> (Int  Int) {     let normalized = try constructAndExecuteGraphToNormalizeImage(imageBytes: image)     let possibilities = try executeInceptionGraph(image: normalized)     guard let m = possibilities.max()  let i = possibilities.index(of: m) else {       throw TF.Panic.INVALID     }//end guard     return (i  Int(m * 100))   } ```  The final step is translating the result object index into the tag name of the object and sending it back to the client:  ``` swift let tag = tags[result.0]       let p = result.1       response.setHeader(.contentType  value: ""text/json"")         .appendBody(string: ""{\""value\"": \""Is it a \(tag)? (Possibility: \(p)%)\""}"")         .completed() ```   """;Computer Vision;https://github.com/PerfectExamples/Perfect-TensorFlow-Demo-Vision
"""``` pos-tagger-bert-tensorflow |____ pos_tagger_bert_tensorflow.ipynb  #: Notebook with all actions required to download UD Treebank dataset and BERT model  fine-tune BERT  and evaluate POS tagger |____ bert_pos.py           #: Main code |____ data                  #: Train data |____ middle_data           #: Middle data (label id map) |____ output                #: Output (final model  predicted results) |____ uncased_L-12_H-768_A-12	#: BERT model downloaded from -> https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip |____ run_pos.sh    		#: Run model and evaluate results  ```   """;Natural Language Processing;https://github.com/soutsios/pos-tagger-bert-tensorflow
""" DeepLab is a state-of-art deep learning system for semantic image segmentation built on top of [Caffe](http://caffe.berkeleyvision.org).  It combines densely-computed deep convolutional neural network (CNN) responses with densely connected conditional random fields (CRF).  This distribution provides a publicly available implementation for the key model ingredients first reported in an [arXiv paper](http://arxiv.org/abs/1412.7062)  accepted in revised form as conference publication to the ICLR-2015 conference.  It also contains implementations for methods supporting model learning using only weakly labeled examples  described in a second follow-up [arXiv paper](http://arxiv.org/abs/1502.02734). Please consult and consider citing the following papers:      @inproceedings{chen14semantic        title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        booktitle={ICLR}        url={http://arxiv.org/abs/1412.7062}        year={2015}     }      @article{papandreou15weak        title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation}        author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille}        journal={arxiv:1502.02734}        year={2015}     }  Note that if you use the densecrf implementation  please consult and cite the following paper:      @inproceedings{KrahenbuhlK11        title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}        author={Philipp Kr{\""{a}}henb{\""{u}}hl and Vladlen Koltun}        booktitle={NIPS}              year={2011}     }   """;Computer Vision;https://github.com/TheLegendAli/DeepLab-Context
"""The objective of this post is to implement a music genre classification model by comparing two popular architectures for sequence modeling: Recurrent Neural networks and Transformers.  RNNs are popular for all sorts of 1D sequence processing tasks  they re-use the same weights at each time step and pass information from a time-step to the next by keeping an internal state and using a gating mechanism (LSTM  GRUs … ). Since they use recurrence  those models can suffer from vanishing/exploding gradients which can make training and learning long-range patterns harder.  ![](https://cdn-images-1.medium.com/max/800/1*3gB5yUL9lqQBuEY7qFIH2A.png)  <span class=""figcaption_hack"">[Source: https://en.wikipedia.org/wiki/Recurrent_neural_network](https://en.wikipedia.org/wiki/Recurrent_neural_network) by [fdeloche](https://commons.wikimedia.org/wiki/User:Ixnay) Under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0)</span>  Transformers are a relatively newer architecture that can process sequences without using any recurrence or convolution [[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)]. The transformer layer is mostly point-wise feed-forward operations and self-attention. These types of networks are having some great success in natural language processing  especially when pre-trained on a large amount of unlabeled data [[https://arxiv.org/pdf/1810.04805](https://arxiv.org/pdf/1810.04805)].  ![](https://cdn-images-1.medium.com/max/800/1*SW0xA1VEJZd3XSqc3NvxNw.png)  <span class=""figcaption_hack"">Transformer Layer — Image by author</span>   """;General;https://github.com/CVxTz/music_genre_classification
"""The objective of this post is to implement a music genre classification model by comparing two popular architectures for sequence modeling: Recurrent Neural networks and Transformers.  RNNs are popular for all sorts of 1D sequence processing tasks  they re-use the same weights at each time step and pass information from a time-step to the next by keeping an internal state and using a gating mechanism (LSTM  GRUs … ). Since they use recurrence  those models can suffer from vanishing/exploding gradients which can make training and learning long-range patterns harder.  ![](https://cdn-images-1.medium.com/max/800/1*3gB5yUL9lqQBuEY7qFIH2A.png)  <span class=""figcaption_hack"">[Source: https://en.wikipedia.org/wiki/Recurrent_neural_network](https://en.wikipedia.org/wiki/Recurrent_neural_network) by [fdeloche](https://commons.wikimedia.org/wiki/User:Ixnay) Under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0)</span>  Transformers are a relatively newer architecture that can process sequences without using any recurrence or convolution [[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)]. The transformer layer is mostly point-wise feed-forward operations and self-attention. These types of networks are having some great success in natural language processing  especially when pre-trained on a large amount of unlabeled data [[https://arxiv.org/pdf/1810.04805](https://arxiv.org/pdf/1810.04805)].  ![](https://cdn-images-1.medium.com/max/800/1*SW0xA1VEJZd3XSqc3NvxNw.png)  <span class=""figcaption_hack"">Transformer Layer — Image by author</span>   """;Natural Language Processing;https://github.com/CVxTz/music_genre_classification
"""Most of the progress in computer vision has centered around object detection and  semantic segmentation in images. For image classification  popular networks have  been ResNet  VGG Network  and GoogleNet. We have seen strong image  segmentation architectures such as FCN  SegNet  UNet  and PSPNet. When it has come to video data  the most common approach has been to deploy fast  object detection algorithms on each frame of the video  such as YOLO and  RetinaNet. While this approach is effective  there is certainly room for  improvement. By performing fast object detection frame-by-frame  all of the previous  timestep information is lost  and each timestep is just a brand-new image to the   object detection algorithm. The goal of this project was to investigate the   incorporation of previous timestep information to increase object detection   in video data. This project also provides code for performing object detection  on video data.   """;Computer Vision;https://github.com/RichardMathewsII/YOLBO
"""Most of the progress in computer vision has centered around object detection and  semantic segmentation in images. For image classification  popular networks have  been ResNet  VGG Network  and GoogleNet. We have seen strong image  segmentation architectures such as FCN  SegNet  UNet  and PSPNet. When it has come to video data  the most common approach has been to deploy fast  object detection algorithms on each frame of the video  such as YOLO and  RetinaNet. While this approach is effective  there is certainly room for  improvement. By performing fast object detection frame-by-frame  all of the previous  timestep information is lost  and each timestep is just a brand-new image to the   object detection algorithm. The goal of this project was to investigate the   incorporation of previous timestep information to increase object detection   in video data. This project also provides code for performing object detection  on video data.   """;General;https://github.com/RichardMathewsII/YOLBO
"""To generate the summaries use `generate_summary.py` script: ``` python generate_summary.py \     --model_path experiments/checkpoints/translation/summarization_title_fr/barthez/ms4096_mu60000_lr1e-04_me50_dws1/1/checkpoint_best.pt \     --output_path experiments/checkpoints/translation/summarization_title_fr/barthez/ms4096_mu60000_lr1e-04_me50_dws1/1/output.txt \      --source_text summarization_data_title_barthez/test-article.txt \     --data_path summarization_data_title_barthez/data-bin/ \     --sentence_piece_model barthez.base/sentence.bpe.model ``` we use [rouge-score](https://pypi.org/project/rouge-score/) to compute ROUGE score. No stemming is applied before evaluation.   A french sequence to sequence pretrained model based on [BART](https://github.com/pytorch/fairseq/tree/master/examples/bart). <br> BARThez is pretrained by learning to reconstruct a corrupted input sentence. A corpus of 66GB of french raw text is used to carry out the pretraining. <br> Unlike already existing BERT-based French language models such as CamemBERT and FlauBERT  BARThez is particularly well-suited for generative tasks  since not only its encoder but also its decoder is pretrained.   In addition to BARThez that is pretrained from scratch  we continue the pretraining of a multilingual BART [mBART](https://github.com/pytorch/fairseq/tree/master/examples/mbart) which boosted its performance in both discriminative and generative tasks. We call the french adapted version mBARThez.  | Model         | Architecture  | #layers | #params | Link  | | ------------- |:-------------:| :-----:|:-----:|:-----:| | BARThez       | BASE          | 12     | 216M  | [Link](https://www.dropbox.com/s/a1y5avgb8uh2v3s/barthez.base.zip?dl=1) | | mBARThez      | LARGE         | 24     | 561M  |[Link](https://www.dropbox.com/s/oo9tokh09rioq0m/mbarthez.large.zip?dl=1) |   """;Sequential;https://github.com/moussaKam/BARThez
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.    """;Computer Vision;https://github.com/xinshasha/YoloV3
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.    """;Computer Vision;https://github.com/GreatTommy/yolov3
"""IMPORTANT: For each experiment  contents of its `.yaml` file is stored in *Text* section in tensorboard!  There are multiple metrics stored for each model and in this section you can find their description. * **accuracy measured at N**: after learning N-th task  evaluation on all previous tasks is performed. In other words *accuracy measured at 10* is PermutedMNIST-10 and *accuracy measured at 100* is PermutedMNIST-100 * **task 0 accuracy**: as we don't want to evaluate all tasks too often  to check if the catastrophic forgetting happened  after learning each task we just check what is its accuracy at task number 0  **References:** 1. Continual learning with hypernetworks  <https://arxiv.org/abs/1906.00695> 2. Three scenarios for continual learning  <https://arxiv.org/abs/1904.07734> 3. HyperNetworks  <https://arxiv.org/abs/1609.09106>  """;General;https://github.com/gahaalt/continual-learning-with-hypernets
"""The idea of this notebook is to investigate what Shapley values are and how they can be used.   All the details can be found in the notebook Shapley values classification.ipynb    """;General;https://github.com/LaurentLava/SHAP
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/ricardordb/bert
""" The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.    This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/TamirShasha/Mars_yolo
"""PaddleOCR aims to create multilingual  awesome  leading  and practical OCR tools that help users train better models and apply them into practice.   """;Computer Vision;https://github.com/zhaoyantao-murray/PaddleOCR-pra
""">Input: [96 x 96 x 3] >Patch: [24 x 24 x 3]  >Overlap size 12. 1 input image is composed with 49 patches.  >Flow: Input Image -> Make Patches -> Encoding -> Pixel CNN -> CPC  >Encoder: ResDense Block + Global AVG Pool  No Pooling Layer(Conv Only)  Batch Norm(Fine Tune Only) and Self Attention.  >***ResDense Block is*** dense convolution block with residual connection(See JointCenterLoss).   ***Contact: seonghobaek@gmail.com***  """;General;https://github.com/SeonghoBaek/CPC
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/wonderland-dsg/pointnet-grid
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/YiruS/pointnet_adversarial
"""Salt formations are important in geology because they form one of the most important traps for hydrocarbons.  Interpretation on seismic images has long used texture attributes  to identify and highlight areas of interest. These can be seen like feature maps on the texture of the seismic. The texture of salt in a siesmic image is unique in the output images of collected siesmic data.   Geologists use 2D or 3D images of seismic images that have been heavily processed to study the subsurface for salt formations. Here I am going to use maching learning and computer vision to identify salt in a seismic image   """;Computer Vision;https://github.com/dbailey00/Semantic-Segmentation-With-Seismic-Images
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/iRmantou/car_bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/iRmantou/car_bert
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/MSJawad/yolov3
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/NBvinay/vinb-ghost-car
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/zheng1weiyu/Insightface_for_FaceRecog
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/Smallflyfly/yolov3_ultralytics
"""A general YOLOv4/v3/v2 object detection pipeline inherited from [keras-yolo3-Mobilenet](https://github.com/Adamdad/keras-YOLOv3-mobilenet)/[keras-yolo3](https://github.com/qqwweee/keras-yolo3) and [YAD2K](https://github.com/allanzelener/YAD2K). Implement with tf.keras  including data collection/annotation  model training/tuning  model evaluation and on device deployment. Support different architecture and different technologies:   """;Computer Vision;https://github.com/symoon94/YOLO-keras
"""This project is based on our CVPR2019 paper. You can find the [arXiv](https://arxiv.org/abs/1811.07246) version here.  ``` @inproceedings{wu2019pointconv    title={Pointconv: Deep convolutional networks on 3d point clouds}    author={Wu  Wenxuan and Qi  Zhongang and Fuxin  Li}    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}    pages={9621--9630}    year={2019} } ```  Unlike images which are represented in regular dense grids  3D point clouds are irregular and unordered  hence applying convolution on them can be difficult. In this paper  we extend the dynamic filter to a new convolution operation  named PointConv. PointConv can be applied on point clouds to build deep convolutional networks. We treat convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight and density functions. With respect to a given point  the weight functions are learned with multi-layer perceptron networks and the density functions through kernel density estimation. A novel reformulation is proposed for efficiently computing the weight functions  which allowed us to dramatically scale up the network and significantly improve its performance. The learned convolution kernel can be used to compute translation-invariant and permutation-invariant convolution on any point set in the 3D space. Besides  PointConv can also be used as deconvolution operators to propagate features from a subsampled point cloud back to its original resolution. Experiments on ModelNet40  ShapeNet  and ScanNet show that deep convolutional neural networks built on PointConv are able to achieve state-of-the-art on challenging semantic segmentation benchmarks on 3D point clouds. Besides  our experiments converting CIFAR-10 into a point cloud showed that networks built on PointConv can match the performance of convolutional networks in 2D images of a similar structure.   """;Computer Vision;https://github.com/THHHomas/mls
"""This is an attempt at the classification task featured in the [Kaggle Data Science Bowl 2017](https://www.kaggle.com/c/data-science-bowl-2017). The task consists on predicting from CT lung scans whether a patient will develop cancer or not within a year. This is a particularly challenging problem given the very high dimensionality of data and the very limited number of samples.  The competition saw many creative approaches  such as those reported by the winning entries [here](https://github.com/lfz/DSB2017) (1st place)  [here](http://blog.kaggle.com/2017/06/29/2017-data-science-bowl-predicting-lung-cancer-2nd-place-solution-write-up-daniel-hammack-and-julian-de-wit/) (2nd place) and [here](http://blog.kaggle.com/2017/05/16/data-science-bowl-2017-predicting-lung-cancer-solution-write-up-team-deep-breath/) (9th place). These approaches have in common that:  1. they are based on deep CNNs; 2. leverage external data  in particular the [LUNA dataset](https://luna16.grand-challenge.org/); 3. make extensive use of ensemble of models.  What I'm attempting here is a rather more ""purist"" (for lack of a better word) approach that uses no ensemble models and no external data. The purpose of this is simply to explore the possibility of achieving a decent classification accuracy using a single model and using solely the provided data. The current attempt consists of a combination of two neural networks:  * Gaussian Mixture Convolutional AutoEncoder (GMCAE): A convolutional autoencoder cast as Mixture Density Network ([Bishop  1994](https://www.microsoft.com/en-us/research/publication/mixture-density-networks/)). This network is used to learn high-level features of patches of lung scans (3D arrays of CT scans in Hounsfield Units)  using unsupervised learning and maximum likelihood on a mixture of Gaussians. * CNN classifier: Performs binary classification upon the features extracted by the encoding layers of the GMCAE.  ![model_overview](illustrations/model_overview.png ""Model overview"")   """;Computer Vision;https://github.com/alegonz/kdsb17
"""In this repository  we present the datasets and the toolkits of [ViP-DeepLab](https://arxiv.org/abs/2012.05258). ViP-DeepLab is a unified model attempting to tackle the long-standing and challenging inverse projection problem in vision  which we model as restoring the point clouds from perspective image sequences while providing each point with instance-level semantic interpretations. Solving this problem requires the vision models to predict the spatial location  semantic class  and temporally consistent instance label for each 3D point. ViP-DeepLab approaches it by jointly performing monocular depth estimation and video panoptic segmentation. We name this joint task as Depth-aware Video Panoptic Segmentation (DVPS)  and propose a new evaluation metric along with two derived datasets for it. This repository includes the datasets SemKITTI-DVPS and Cityscapes-DVPS along with the evaluation toolkits.  [![Demo](readme_srcs/ViP-DeepLab.gif)](https://youtu.be/XR4HFiwwao0)    """;Computer Vision;https://github.com/joe-siyuan-qiao/ViP-DeepLab
"""![](figures/ppl.png) > Video datasets are usually scene-dominated  We propose to decouple the scene and the motion (DSM) with two simple operations  so that the model attention towards the motion information is better paid.   The generated triplet is as below: ![](figures/triplet_visualization.png)    """;Computer Vision;https://github.com/TencentYoutuResearch/SelfSupervisedLearning-DSM
"""Recent research has shown that numerous human-interpretable directions exist in the latent space of GANs. In this paper  we develop an automatic procedure for finding directions that lead to foreground-background image separation  and we use these directions to train an image segmentation model without human supervision. Our method is generator-agnostic  producing strong segmentation results with a wide range of different GAN architectures. Furthermore  by leveraging GANs pretrained on large datasets such as ImageNet  we are able to segment images from a range of domains without further training or finetuning. Evaluating our method on image segmentation benchmarks  we compare favorably to prior work while using neither human supervision nor access to the training data. Broadly  our results demonstrate that automatically extracting foreground-background structure from pretrained deep generative models can serve as a remarkably effective substitute for human supervision.    """;General;https://github.com/lukemelas/unsupervised-image-segmentation
"""This part contains the implementation of the [Stand-alone self attention research paper](https://arxiv.org/pdf/1906.05909.pdf) in tensorflow.         This is divided into parts as:-          **readme**            **attention** containg the module of attention stem and attention(used in convolutions core blocks)            **config** containg the arguments we will pass in the command line            **main** containg the execution of all the functions       **model** containg the architecture of the model          **preprocess** containg the arguments that will preprocess the images   """;Computer Vision;https://github.com/MaheepChaudhary/Stand-Alone_Self-Attention
"""Properties of dilated convolution are discussed in our [ICLR 2016 conference paper](http://arxiv.org/abs/1511.07122). This repository contains the network definitions and the trained models. You can use this code together with vanilla Caffe to segment images using the pre-trained models. If you want to train the models yourself  please check out the [document for training](https://github.com/fyu/dilation/blob/master/docs/training.md).  **If you are looking for dilation models with state-of-the-art performance and Python implementation  please check out [Dilated Residual Networks](https://github.com/fyu/drn).**   """;Computer Vision;https://github.com/fyu/dilation
"""+ Implementation  Implemented based on [THUMT-TensorFlow](https://github.com/THUNLP-MT/THUMT)  an open-source toolkit for neural machine translation developed by the Natural Language Processing Group at Tsinghua University which was implemented strictly referring to [Vaswani et al. (2017)](https://arxiv.org/pdf/1706.03762.pdf).  + Data  [WMT14 English-German](https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-wmt14en2de.sh)  [WMT19 Chinese-English](https://drive.google.com/file/d/1LvUPsIZ_xRwuB1vHlvi1COeZEOxfbYy0/view?usp=sharing)    """;General;https://github.com/xydaytoy/BMI-NMT
"""+ Implementation  Implemented based on [THUMT-TensorFlow](https://github.com/THUNLP-MT/THUMT)  an open-source toolkit for neural machine translation developed by the Natural Language Processing Group at Tsinghua University which was implemented strictly referring to [Vaswani et al. (2017)](https://arxiv.org/pdf/1706.03762.pdf).  + Data  [WMT14 English-German](https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-wmt14en2de.sh)  [WMT19 Chinese-English](https://drive.google.com/file/d/1LvUPsIZ_xRwuB1vHlvi1COeZEOxfbYy0/view?usp=sharing)    """;Natural Language Processing;https://github.com/xydaytoy/BMI-NMT
"""In dataset.py  we set the seed inside ```create_dataset``` function. In var_init.py  we set seed for weight initialization   """;Computer Vision;https://github.com/Moonforeva/mindspore_yolov4_modified
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/JohannLee1996/bert
"""You-Only-Look-Once (YOLO) newtork was introduced by Joseph Redmon et al.  Three versions were implemented in C  with the framework called [darknet](https://github.com/pjreddie/darknet) (paper: [v1](https://arxiv.org/abs/1506.02640)  [v2](https://arxiv.org/abs/1612.08242)  [v3](https://arxiv.org/abs/1804.02767)).  This repo implements the Nueral Network (NN) model of YOLOv3 in the PyTorch framework  aiming to ease the pain when the network needs to be modified or retrained.  There are a number of implementations existing in the open source domain   e.g.  [eriklindernoren/PyTorch-YOLOv3](https://github.com/eriklindernoren/PyTorch-YOLOv3)  [ayooshkathuria/pytorch-yolo-v3](https://github.com/ayooshkathuria/pytorch-yolo-v3)  [ultralytics/yolov3](https://github.com/ultralytics/yolov3)  etc. However  majority of them relies on ""importing"" the configuration file from the original darknet framework. In this work  the model is built from scratch using PyTorch.  Additionally  both inference and training part are implemented.  The original weights trained by the authors are converted to .pt file. It can be used as a baseline for transfer learning.  **This project is licensed under BSD 3-Clause ""Revised"" License.**   """;Computer Vision;https://github.com/westerndigitalcorporation/YOLOv3-in-PyTorch
"""PyTorchVideo is a deeplearning library with a focus on video understanding work. PytorchVideo provides reusable  modular and efficient components needed to accelerate the video understanding research. PyTorchVideo is developed using [PyTorch](https://pytorch.org) and supports different deeplearning video components like video models  video datasets  and video-specific transforms.  Key features include:  - **Based on PyTorch:** Built using PyTorch. Makes it easy to use all of the PyTorch-ecosystem components. - **Reproducible Model Zoo:** Variety of state of the art pretrained video models and their associated benchmarks that are ready to use.   Complementing the model zoo  PyTorchVideo comes with extensive data loaders supporting different datasets. - **Efficient Video Components:** Video-focused fast and efficient components that are easy to use. Supports accelerated inference on hardware.   """;Computer Vision;https://github.com/facebookresearch/pytorchvideo
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M and VGG2 datasets  which were already packed in the MxNet binary format. The network backbones include ResNet  InceptionResNet_v2  DenseNet  DPN and MobiletNet. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss. * loss-type=0:  Softmax * loss-type=1:  SphereFace * loss-type=2:  CosineFace * loss-type=4:  ArcFace * loss-type=5:  Combined Margin * loss-type=12: TripletLoss  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/eric-erki/insightface
"""We build a plant disease diagnosis system on Android  by implementing a deep convolutional neural network with Tensorflow to detect disease from various plant leave images.   Generally  due to the size limitation of the dataset  we adopt the transefer learning in this system. Specifically  we retrain the MobileNets [[1]](https://arxiv.org/pdf/1704.04861.pdf)  which is first trained on ImageNet dataset  on the plant disease datasets. Finally  we port the trained model to Android.   """;General;https://github.com/sabrisangjaya/plantdoctor
"""We build a plant disease diagnosis system on Android  by implementing a deep convolutional neural network with Tensorflow to detect disease from various plant leave images.   Generally  due to the size limitation of the dataset  we adopt the transefer learning in this system. Specifically  we retrain the MobileNets [[1]](https://arxiv.org/pdf/1704.04861.pdf)  which is first trained on ImageNet dataset  on the plant disease datasets. Finally  we port the trained model to Android.   """;Computer Vision;https://github.com/sabrisangjaya/plantdoctor
"""This paper presents a normalization mechanism called Instance-Level Meta Normalization (ILM-Norm) to address a learning-to-normalize problem. ILM-Norm learns to predict the normalization parameters via both the feature feed-forward and the gradient back-propagation paths. ILM-Norm provides a meta normalization mechanism and has several good properties. It can be easily plugged into existing instance-level normalization schemes such as Instance Normalization  Layer Normalization  or Group Normalization. ILM-Norm normalizes each instance individually and therefore maintains high performance even when small mini-batch is used. The experimental results show that ILM-Norm well adapts to different network architectures and tasks  and it consistently improves the performance of the original models.    """;General;https://github.com/Gasoonjia/ILM-Norm
"""Google AI's BERT paper shows the amazing result on various NLP task (new 17 NLP tasks SOTA)   This paper proved that Transformer(self-attention) based encoder can be powerfully used as  alternative of previous language model with proper language model training method.  This repo is implementation of Mask LM in BERT. Code is very simple and easy to understand fastly. Some of these codes are based on [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)    """;Natural Language Processing;https://github.com/huanghonggit/Mask-Language-Model
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/coco60/bert-test
"""This is an official implementation of [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808). We present a new architecture  named Convolutional vision Transformers (CvT)  that improves Vision Transformers (ViT) in performance and efficienty by introducing convolutions into ViT to yield the best of both disignes. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding  and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (e.g. shift  scale  and distortion invariance) while maintaining the merits of Transformers (e.g. dynamic attention  global context  and better generalization). We validate CvT by conducting extensive experiments  showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k  with fewer parameters and lower FLOPs. In addition  performance gains are maintained when pretrained on larger dataset (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained on ImageNet-22k  our CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k val set. Finally  our results show that the positional encoding  a crucial component in existing Vision Transformers  can be safely removed in our model  simplifying the design for higher resolution vision tasks.   ![](figures/pipeline.svg)   """;Computer Vision;https://github.com/leoxiaobin/CvT
"""Recent research has shown that numerous human-interpretable directions exist in the latent space of GANs. In this paper  we develop an automatic procedure for finding directions that lead to foreground-background image separation  and we use these directions to train an image segmentation model without human supervision. Our method is generator-agnostic  producing strong segmentation results with a wide range of different GAN architectures. Furthermore  by leveraging GANs pretrained on large datasets such as ImageNet  we are able to segment images from a range of domains without further training or finetuning. Evaluating our method on image segmentation benchmarks  we compare favorably to prior work while using neither human supervision nor access to the training data. Broadly  our results demonstrate that automatically extracting foreground-background structure from pretrained deep generative models can serve as a remarkably effective substitute for human supervision.    """;Computer Vision;https://github.com/lukemelas/unsupervised-image-segmentation
"""FastSeq provides efficient implementation of popular sequence models (e.g. [Bart](https://arxiv.org/pdf/1910.13461.pdf)  [ProphetNet](https://github.com/microsoft/ProphetNet)) for text generation  summarization  translation tasks etc. It automatically optimizes inference speed based on popular NLP toolkits (e.g. [FairSeq](https://github.com/pytorch/fairseq) and [HuggingFace-Transformers](https://github.com/huggingface/transformers)) without accuracy loss. All these can be easily done (no need to change any code/model/data if using our command line tool  or simply add one-line code `import fastseq` if using source code).   """;Sequential;https://github.com/microsoft/fastseq
"""| Name           | Type  | Default                       | Description                         | | -------------- | ----- | ----------------------------- | ----------------------------------- | | data_dir       | str   | ""modelnet40_normal_resampled"" | train & test data dir               | | num_point      | int   | 1024                          | sample number of points             | | batch_size     | int   | 32                            | batch size in training              | | num_category   | int   | 40                            | ModelNet10/40                       | | learning_rate  | float | 1e-3                          | learning rate in training           | | max_epochs     | int   | 200                           | max epochs in training              | | num_workers    | int   | 32                            | number of workers in dataloader     | | log_batch_num  | int   | 50                            | log info per log_batch_num          | | model_path     | str   | ""pointnet.pdparams""           | save/load model in training/testing | | lr_decay_step  | int   | 20                            | step_size in StepDecay              | | lr_decay_gamma | float | 0.7                           | gamma in StepDecay                  |   This project reproduces PointNet based on paddlepaddle framework.  PointNet provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  **Paper:** [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/pdf/1612.00593.pdf)  **Competition Page:** [PaddlePaddle AI Studio](https://aistudio.baidu.com/aistudio/competition/detail/106)  **PointNet Architecture:** ![arch](arch.png)  **Other Version Implementation:**  - [TensorFlow (Official)](https://github.com/charlesq34/pointnet) - [PyTorch](https://github.com/yanx27/Pointnet_Pointnet2_pytorch)  **Acceptance condition**  - Classification Accuracy 89.2 on ModelNet40 Dataset   """;Computer Vision;https://github.com/Phimos/Paddle-PointNet
"""**Swin Transformer** (the name `Swin` stands for **S**hifted **win**dow) is initially described in [arxiv](https://arxiv.org/abs/2103.14030)  which capably serves as a general-purpose backbone for computer vision. It is basically a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection.  Swin Transformer achieves strong performance on COCO object detection (`58.7 box AP` and `51.1 mask AP` on test-dev) and ADE20K semantic segmentation (`53.5 mIoU` on val)  surpassing previous models by a large margin.  ![teaser](figures/teaser.png)   """;Computer Vision;https://github.com/DominickZhang/Distillation-Swin-Transformer
"""We provide a simple introduction in [Motivation](#motivation)  and more details can be found in our [paper](https://arxiv.org/abs/1908.03265). There are some unofficial introductions available (with better writings)  and they are listed here for reference only (contents/claims in our paper are more accurate):  [Medium Post](https://medium.com/@lessw/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b) > [related Twitter Post](https://twitter.com/jeremyphoward/status/1162118545095852032?ref_src=twsrc%5Etfw)  [CSDN Post (in Chinese)](https://blog.csdn.net/u014248127/article/details/99696029)   <h5 align=""center""><i>If warmup is the answer  what is the question?</i></h5>  The learning rate warmup for Adam is a must-have trick for stable training in certain situations (or eps tuning). But the underlying mechanism is largely unknown. In our study  we suggest one fundamental cause is __the large variance of the adaptive learning rates__  and provide both theoretical and empirical support evidence.  In addition to explaining __why we should use warmup__  we also propose __RAdam__  a theoretically sound variant of Adam.    """;General;https://github.com/LiyuanLucasLiu/RAdam
""" This is a Mindspore implementation of our paper [SKFAC: Training Neural Networks With Faster Kronecker-Factored Approximate Curvature](https://openaccess.thecvf.com/content/CVPR2021/html/Tang_SKFAC_Training_Neural_Networks_With_Faster_Kronecker-Factored_Approximate_Curvature_CVPR_2021_paper.html) <!--  This is an example of training ResNet-50 V1.5 with ImageNet2012 dataset by second-order optimizer [SKFAC](https://openaccess.thecvf.com/content/CVPR2021/html/Tang_SKFAC_Training_Neural_Networks_With_Faster_Kronecker-Factored_Approximate_Curvature_CVPR_2021_paper.html). This example is based on modifications to the [THOR optimizer](https://gitee.com/mindspore/mindspore/tree/r1.1/model_zoo/official/cv/resnet_thor) on the [MindSpore framework](https://www.mindspore.cn/en). -->   <!-- TOC -->  - [ResNet-50-SKFAC Example](#resnet-50-skfac-example)  	- [Description](#description)  	- [Model Architecture](#model-architecture)  	- [Dataset](#dataset)  	- [Environment Requirements](#environment-requirements)  		- Hardware  		- Framework  	- [Quick Start](#quick-start)  	- [Script Parameters](#script-parameters) 	- [Optimize Performance](#optimize-performance) 	- [References](#references)  <!-- TOC -->     """;General;https://github.com/fL0n9/SKFAC-MindSpore
""" This is a Mindspore implementation of our paper [SKFAC: Training Neural Networks With Faster Kronecker-Factored Approximate Curvature](https://openaccess.thecvf.com/content/CVPR2021/html/Tang_SKFAC_Training_Neural_Networks_With_Faster_Kronecker-Factored_Approximate_Curvature_CVPR_2021_paper.html) <!--  This is an example of training ResNet-50 V1.5 with ImageNet2012 dataset by second-order optimizer [SKFAC](https://openaccess.thecvf.com/content/CVPR2021/html/Tang_SKFAC_Training_Neural_Networks_With_Faster_Kronecker-Factored_Approximate_Curvature_CVPR_2021_paper.html). This example is based on modifications to the [THOR optimizer](https://gitee.com/mindspore/mindspore/tree/r1.1/model_zoo/official/cv/resnet_thor) on the [MindSpore framework](https://www.mindspore.cn/en). -->   <!-- TOC -->  - [ResNet-50-SKFAC Example](#resnet-50-skfac-example)  	- [Description](#description)  	- [Model Architecture](#model-architecture)  	- [Dataset](#dataset)  	- [Environment Requirements](#environment-requirements)  		- Hardware  		- Framework  	- [Quick Start](#quick-start)  	- [Script Parameters](#script-parameters) 	- [Optimize Performance](#optimize-performance) 	- [References](#references)  <!-- TOC -->     """;Computer Vision;https://github.com/fL0n9/SKFAC-MindSpore
"""PaddleDetection is an end-to-end object detection development kit based on PaddlePaddle  which implements varied mainstream object detection  instance segmentation  tracking and keypoint detection algorithms in modular designwhich with configurable modules such as network components  data augmentations and losses  and release many kinds SOTA industry practice models  integrates abilities of model compression and cross-platform high-performance deployment  aims to help developers in the whole end-to-end development in a faster and better way.   """;Computer Vision;https://github.com/PaddlePaddle/PaddleDetection
"""This is a PyTorch implementation of the autofocus convolutional layer proposed for semantic segmentation with the objective of enhancing the capabilities of neural networks for multi-scale processing. Autofocus layers adaptively change the size of the effective receptive field  based on the processed context to generate more powerful features. The proposed autofocus layer can be easily integrated into existing networks to improve a model's representational power.   Here we apply the autofocus convolutional layer to deep neural networks for 3D semantic segmentation. We run experiments on the [Brain Tumor Image Segmentation dataset (BRATS2015)](https://www.smir.ch/BRATS/Start2015) as an example to show how the models work. In addition  we also implement a series of deep learning based models used for 3D Semantic Segmentation. The details of all the models implemented here can be found in our paper: [Autofocus Layer for Semantic Segmentation](https://arxiv.org/pdf/1805.08403.pdf).  <img src=""./src/autofocus.png"" width=""900""/>  Figure 1. An autofocus convolutional layer with four candidate dilation rates. (a) The attention model. (b) A weighted summation of activations from parallel dilated convolutions. (c) An example of attention maps for a small (r^1) and large (r^2) dilation rate. The first row is the input and the segmentation result of AFN6.    """;Computer Vision;https://github.com/yaq007/Autofocus-Layer
"""DFNet introduce **Fusion Block** for generating a flexible alpha composition map to combine known and unknown regions. It builds a bridge for structural and texture information  so that information in known region can be naturally propagated into completion area. With this technology  the completion results will have smooth transition near the boundary of completion area.  Furthermore  the architecture of fusion block enable us to apply **multi-scale constraints**. Multi-scale constrains improves the performance of DFNet a lot on structure consistency.  Moreover  **it is easy to apply this fusion block and multi-scale constrains to other existing deep image completion models**. A fusion block feed with feature maps and input image  will give you a completion result in the same resolution as given feature maps.  More detail can be found in the [paper](https://arxiv.org/abs/1904.08060)  The illustration of a fusion block in the paper:  <p align=""center"">   <img width=""600"" src=""imgs/fusion-block.jpg""> </p>  Examples of corresponding images:  ![](imgs/github_teaser.jpg)  If you find this code useful for your research  please cite:  ``` @inproceedings{DFNet2019    title={Deep Fusion Network for Image Completion}    author={Xin Hong and Pengfei Xiong and Renhe Ji and Haoqiang Fan}    journal={arXiv preprint}    year={2019}  } ```   """;Computer Vision;https://github.com/iankuoli/DFNet_TF2
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/KhusDM/PointNetTree
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO** (https://pjreddie.com/darknet/yolo/) and to **Erik Lindernoren for the PyTorch implementation** this work is based on (https://github.com/eriklindernoren/PyTorch-YOLOv3).   This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/vobecant/yolov3_WannaSeeU
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/frankcgq105/BERTCHEN
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/frankcgq105/BERTCHEN
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/fciannel/bert_fciannel
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/mayurnewase/Quora-Bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/mayurnewase/Quora-Bert
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/Lw510107/pointnet-2018.6.27-
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/fciannel/bert_fciannel
"""<div align=""left"">   <img src=""https://insightface.ai/assets/img/custom/thumb_sdunet.png"" width=""600""/> </div>  In this module  we provide datasets and training/inference pipelines for face alignment.  Supported methods:  - [x] [SDUNets (BMVC'2018)](alignment/heatmap) - [x] [SimpleRegression](alignment/coordinate_reg)   [SDUNets](alignment/heatmap) is a heatmap based method which accepted on [BMVC](http://bmvc2018.org/contents/papers/0051.pdf).  [SimpleRegression](alignment/coordinate_reg) provides very lightweight facial landmark models with fast coordinate regression. The input of these models is loose cropped face image while the output is the direct landmark coordinates.    <div align=""left"">   <img src=""https://insightface.ai/assets/img/github/11513D05.jpg"" width=""640""/> </div>  In this module  we provide training data with annotation  network settings and loss designs for face detection training  evaluation and inference.  The supported methods are as follows:  - [x] [RetinaFace (CVPR'2020)](detection/retinaface) - [x] [SCRFD (Arxiv'2021)](detection/scrfd) - [x] [blazeface_paddle](detection/blazeface_paddle)  [RetinaFace](detection/retinaface) is a practical single-stage face detector which is accepted by [CVPR 2020](https://openaccess.thecvf.com/content_CVPR_2020/html/Deng_RetinaFace_Single-Shot_Multi-Level_Face_Localisation_in_the_Wild_CVPR_2020_paper.html). We provide training code  training dataset  pretrained models and evaluation scripts.   [SCRFD](detection/scrfd) is an efficient high accuracy face detection approach which is initialy described in [Arxiv](https://arxiv.org/abs/2105.04714). We provide an easy-to-use pipeline to train high efficiency face detectors with NAS supporting.    In this module  we provide training data  network settings and loss designs for deep face recognition.  The supported methods are as follows:  - [x] [ArcFace_mxnet (CVPR'2019)](recognition/arcface_mxnet) - [x] [ArcFace_torch (CVPR'2019)](recognition/arcface_torch) - [x] [SubCenter ArcFace (ECCV'2020)](recognition/subcenter_arcface) - [x] [PartialFC_mxnet (Arxiv'2020)](recognition/partial_fc) - [x] [PartialFC_torch (Arxiv'2020)](recognition/arcface_torch) - [x] [VPL (CVPR'2021)](recognition/vpl) - [x] [OneFlow_face](recognition/oneflow_face) - [x] [ArcFace_Paddle (CVPR'2019)](recognition/arcface_paddle)  Commonly used network backbones are included in most of the methods  such as IResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  etc..    [InsightFace](https://insightface.ai) is an open source 2D&3D deep face analysis toolbox  mainly based on PyTorch and MXNet.   Please check our [website](https://insightface.ai) for detail.  The master branch works with **PyTorch 1.6+** and/or **MXNet=1.6-1.8**  with **Python 3.x**.  InsightFace efficiently implements a rich variety of state of the art algorithms of face recognition  face detection and face alignment  which optimized for both training and deployment.   """;General;https://github.com/deepinsight/insightface
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  <center>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | |:-------|:-----:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | 300 | | [Faster R-CNN (ZF)](https://github.com/ShaoqingRen/faster_rcnn) | 62.1 | 17 | 300 | | [YOLO](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | | [Fast YOLO](http://pjreddie.com/darknet/yolo/) | 52.7 | 155 | 98 | | SSD300 (VGG16) | 72.1 | 58 | 7308 | | SSD300 (VGG16  cuDNN v5) | 72.1 | 72 | 7308 | | SSD500 (VGG16) | **75.1** | 23 | 20097 |  </center>   """;Computer Vision;https://github.com/inisis/caffe
"""we propose what point affect to accuracy from between Python Package and C raw programming without Package.   """;General;https://github.com/jooyounghun/Separable-CNN
"""we propose what point affect to accuracy from between Python Package and C raw programming without Package.   """;Computer Vision;https://github.com/jooyounghun/Separable-CNN
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO** (https://pjreddie.com/darknet/yolo/) and to **Erik Lindernoren for the PyTorch implementation** this work is based on (https://github.com/eriklindernoren/PyTorch-YOLOv3).   This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information on Ultralytics projects please visit: https://www.ultralytics.com.   """;Computer Vision;https://github.com/thecryboy/yolov3_ultralytics_deconv
"""We present 1-dimensional (1D) convolutional neural networks (CNN) for the classification of soil texture based on hyperspectral data. The following CNN models are included:  * `LucasCNN` * `LucasResNet` * `LucasCoordConv` * `HuEtAl`: 1D CNN by Hu et al. (2015)  DOI: [10.1155/2015/258619](http://dx.doi.org/10.1155/2015/258619) * `LiuEtAl`: 1D CNN by Liu et al. (2018)  DOI: [10.3390/s18093169](https://dx.doi.org/10.3390%2Fs18093169)  These 1D CNNs are optimized for the soil texture classification based on the hyperspectral data of the *Land Use/Cover Area Frame Survey* (LUCAS) topsoil dataset. It is available [here](https://esdac.jrc.ec.europa.eu/projects/lucas). For more information have a look in our publication (see below).  **Introducing paper:** [arXiv:1901.04846](https://arxiv.org/abs/1901.04846)  **Licence:** [MIT](LICENSE)  **Authors:**  * [Felix M. Riese](mailto:felix.riese@kit.edu) * [Sina Keller](mailto:sina.keller@kit.edu)  **Citation of the code and the paper:** see [below](#citation) and in the [bibtex](bibliography.bib) file   """;Computer Vision;https://github.com/felixriese/CNN-SoilTextureClassification
"""This repository holds the code framework used in the paper Reg R-CNN: Lesion Detection and Grading under Noisy Labels [1]. The framework is a fork of MIC's [medicaldetectiontoolkit](https://github.com/MIC-DKFZ/medicaldetectiontoolkit) with added regression capabilities.  As below figure shows  the regression capability allows for the preservation of ordinal relations in the training signal as opposed to a standard categorical classification loss like the cross entropy loss (see publication for details). <p align=""center""><img src=""assets/teaser.png""  width=50%></p><br> Network Reg R-CNN is a version of Mask R-CNN [2] but with a regressor in place of the object-class head (see figure below). In this scenario  the first stage makes foreground (fg) vs background (bg) detections  then the regression head determines the class on an ordinal scale. Consequently  prediction confidence scores are taken from the first stage as opposed to the head in the original Mask R-CNN. <p align=""center""><img src=""assets/regrcnn.png""  width=50%></p><br>  In the configs file of a data set in the framework  you may set attribute self.prediction_tasks = [""task""] to a value ""task"" from [""class""  ""regression_bin""  ""regression""]. ""class"" produces the same behavior as the original framework  i.e.  standard object-detection behavior. ""regression"" on the other hand  swaps the class head of network Mask R-CNN [2] for a regression head. Consequently  objects are identified as fg/bg and then the class is decided by the regressor. For the sake of comparability  ""regression_bin"" produces a similar behavior but with a classification head. Both methods should be evaluated with the (implemented) Average Viewpoint Precision instead of only Average Precision.  Below you will found a description of the general framework operations and handling. Basic framework functionality and description are for the most part identical to the original [medicaldetectiontoolkit](https://github.com/MIC-DKFZ/medicaldetectiontoolkit).  <br/> [1] Ramien  Gregor et al.  <a href=""https://arxiv.org/abs/1907.12915"">""Reg R-CNN: Lesion Detection and Grading under Noisy Labels""</a>. In: UNSURE Workshop at MICCAI  2019.<br> [2] He  Kaiming  et al.  <a href=""https://arxiv.org/abs/1703.06870"">""Mask R-CNN""</a> ICCV  2017<br> <br>   """;Computer Vision;https://github.com/MIC-DKFZ/RegRCNN
"""This repository holds the code framework used in the paper Reg R-CNN: Lesion Detection and Grading under Noisy Labels [1]. The framework is a fork of MIC's [medicaldetectiontoolkit](https://github.com/MIC-DKFZ/medicaldetectiontoolkit) with added regression capabilities.  As below figure shows  the regression capability allows for the preservation of ordinal relations in the training signal as opposed to a standard categorical classification loss like the cross entropy loss (see publication for details). <p align=""center""><img src=""assets/teaser.png""  width=50%></p><br> Network Reg R-CNN is a version of Mask R-CNN [2] but with a regressor in place of the object-class head (see figure below). In this scenario  the first stage makes foreground (fg) vs background (bg) detections  then the regression head determines the class on an ordinal scale. Consequently  prediction confidence scores are taken from the first stage as opposed to the head in the original Mask R-CNN. <p align=""center""><img src=""assets/regrcnn.png""  width=50%></p><br>  In the configs file of a data set in the framework  you may set attribute self.prediction_tasks = [""task""] to a value ""task"" from [""class""  ""regression_bin""  ""regression""]. ""class"" produces the same behavior as the original framework  i.e.  standard object-detection behavior. ""regression"" on the other hand  swaps the class head of network Mask R-CNN [2] for a regression head. Consequently  objects are identified as fg/bg and then the class is decided by the regressor. For the sake of comparability  ""regression_bin"" produces a similar behavior but with a classification head. Both methods should be evaluated with the (implemented) Average Viewpoint Precision instead of only Average Precision.  Below you will found a description of the general framework operations and handling. Basic framework functionality and description are for the most part identical to the original [medicaldetectiontoolkit](https://github.com/MIC-DKFZ/medicaldetectiontoolkit).  <br/> [1] Ramien  Gregor et al.  <a href=""https://arxiv.org/abs/1907.12915"">""Reg R-CNN: Lesion Detection and Grading under Noisy Labels""</a>. In: UNSURE Workshop at MICCAI  2019.<br> [2] He  Kaiming  et al.  <a href=""https://arxiv.org/abs/1703.06870"">""Mask R-CNN""</a> ICCV  2017<br> <br>   """;General;https://github.com/MIC-DKFZ/RegRCNN
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/why2000/DuReader-bert
"""![data-example](images/data-example.png)  Dataset consists of collected from public available chest X-Ray (CXR) images. Overall amount of images is 800 meanwhile labeled only 704 of them. Whole dataset was randomly divided into train (0.8 of total) validation (0.1 splited from train) and test parts. Splits were saved into ```splits.pk```.  The main task is to implement pixel-wise segmentation on the available data to detect lung area. Download link on the dataset https://drive.google.com/file/d/1ffbbyoPf-I3Y0iGbBahXpWqYdGd7xxQQ/view.   """;Computer Vision;https://github.com/IlliaOvcharenko/lung-segmentation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Gaozhen0816/BERT_QA_For_AILaw
"""*student: Carlos Báez*   Project report for the Deep learning postgrade at UPC tech talent (Barcelona). This report explains all the work done  results and extracted conclusions  The main idea in the project was the implementation of an end-to-end person recognition system. For this  I decided to split the project in two parts:   - **Detection**. Study of different implemented algorithms and different datasets to choose the best option for us     - **Face Recognition**. It is implemented and modified four different solutions with a saimese architecture.   """;Computer Vision;https://github.com/carlosb1/upc-aidl-19-team4
"""Implementation of a neural network that can transfer the style of an arbitrary image to another photo. <br /> Much of the code (e.g the layers) is implemented in my [neural network toolbox](https://github.com/nhatsmrt/nn-toolbox/tree/experimental). The training procedure can be found [here](https://github.com/nhatsmrt/nn-toolbox/blob/experimental/nntoolbox/vision/learner/style.py). This repository contains only the testing code. To replicate my work  please also clone the experimental branch of my nntoolbox repository.  """;Computer Vision;https://github.com/nhatsmrt/torch-styletransfer
"""Implementation of a neural network that can transfer the style of an arbitrary image to another photo. <br /> Much of the code (e.g the layers) is implemented in my [neural network toolbox](https://github.com/nhatsmrt/nn-toolbox/tree/experimental). The training procedure can be found [here](https://github.com/nhatsmrt/nn-toolbox/blob/experimental/nntoolbox/vision/learner/style.py). This repository contains only the testing code. To replicate my work  please also clone the experimental branch of my nntoolbox repository.  """;General;https://github.com/nhatsmrt/torch-styletransfer
"""This is a proposition for the Capstone project for the EPFL Extension School Applied Machine Learning program. The objective is to train a neural net for custom class object detection and run inference at the edge by: - building a custom data set and annotate it; - train a network using data augmentation techniques and transfer learning with fine-tuning of the last layers; - (if possible) running inference at the edge on a device with limited computing power.  I will thoroughly document each phase of the project and draw conclusions on the best techniques to use.   """;Computer Vision;https://github.com/petrum01/Capstone_project_object_detection
"""This is a proposition for the Capstone project for the EPFL Extension School Applied Machine Learning program. The objective is to train a neural net for custom class object detection and run inference at the edge by: - building a custom data set and annotate it; - train a network using data augmentation techniques and transfer learning with fine-tuning of the last layers; - (if possible) running inference at the edge on a device with limited computing power.  I will thoroughly document each phase of the project and draw conclusions on the best techniques to use.   """;General;https://github.com/petrum01/Capstone_project_object_detection
"""This is the code base for our work of Generative Map at <https://arxiv.org/abs/1902.11124>.   It is an effort to combine generative model (in particular  [Variational Auto-Encoders](https://arxiv.org/abs/1312.6114)) and the classic Kalman filter for generation with localization.   For more details  please refer to [our paper on arXiv](https://arxiv.org/abs/1902.11124).   """;Computer Vision;https://github.com/Mingpan/generative_map
"""This is the code base for our work of Generative Map at <https://arxiv.org/abs/1902.11124>.   It is an effort to combine generative model (in particular  [Variational Auto-Encoders](https://arxiv.org/abs/1312.6114)) and the classic Kalman filter for generation with localization.   For more details  please refer to [our paper on arXiv](https://arxiv.org/abs/1902.11124).   """;General;https://github.com/Mingpan/generative_map
"""Districts and cities have periods of high demand for electricity  which raise electricity prices and the overall cost of the power distribution networks. Flattening  smoothening  and reducing the overall curve of electrical demand helps reduce operational and capital costs of electricity generation  transmission  and distribution networks. Demand response is the coordination of electricity consuming agents (i.e. buildings) in order to reshape the overall curve of electrical demand. CityLearn allows the easy implementation of reinforcement learning agents in a multi-agent setting to reshape their aggregated curve of electrical demand by controlling the storage of energy by every agent. Currently  CityLearn allows controlling the storage of domestic hot water (DHW)  and chilled water (for sensible cooling and dehumidification). CityLearn also includes models of air-to-water heat pumps  electric heaters  solar photovoltaic arrays  and the pre-computed energy loads of the buildings  which include space cooling  dehumidification  appliances  DHW  and solar generation.  """;General;https://github.com/intelligent-environments-lab/CityLearn
"""Districts and cities have periods of high demand for electricity  which raise electricity prices and the overall cost of the power distribution networks. Flattening  smoothening  and reducing the overall curve of electrical demand helps reduce operational and capital costs of electricity generation  transmission  and distribution networks. Demand response is the coordination of electricity consuming agents (i.e. buildings) in order to reshape the overall curve of electrical demand. CityLearn allows the easy implementation of reinforcement learning agents in a multi-agent setting to reshape their aggregated curve of electrical demand by controlling the storage of energy by every agent. Currently  CityLearn allows controlling the storage of domestic hot water (DHW)  and chilled water (for sensible cooling and dehumidification). CityLearn also includes models of air-to-water heat pumps  electric heaters  solar photovoltaic arrays  and the pre-computed energy loads of the buildings  which include space cooling  dehumidification  appliances  DHW  and solar generation.  """;Reinforcement Learning;https://github.com/intelligent-environments-lab/CityLearn
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/DeokO/bert-excercise-ongoing
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/DeokO/bert-excercise-ongoing
"""Work in progress       """;Graphs;https://github.com/WiktorJ/msnode2vec
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.      ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    """;Natural Language Processing;https://github.com/kyzhouhzau/BERT-NER
"""* This is an assignment of [Deep Learning basic class](https://deeplearning.jp/lectures/dlb2018/) arranged a little.  * Training CIFAR-10 by small ResNet on Google Colaboratory with TensorFlow 2.0 Alpha. * Data is augmented by ImageDataGenerator of Keras.   """;General;https://github.com/shoji9x9/CIFAR-10-By-small-ResNet
"""* This is an assignment of [Deep Learning basic class](https://deeplearning.jp/lectures/dlb2018/) arranged a little.  * Training CIFAR-10 by small ResNet on Google Colaboratory with TensorFlow 2.0 Alpha. * Data is augmented by ImageDataGenerator of Keras.   """;Computer Vision;https://github.com/shoji9x9/CIFAR-10-By-small-ResNet
"""This code applies [dynamic evaluation](https://arxiv.org/abs/1709.07432) to pretrained Transformer-XL models from this [paper](https://arxiv.org/abs/1901.02860). Our codebase is a modified version of [their codebase](https://github.com/kimiyoung/transformer-xl). We used this code to obtain state of the art results on WikiText-103 (perplexity: 16.4)  enwik8 (bits/char: 0.94)  and text8 (bits/char: 1.04).   """;Natural Language Processing;https://github.com/benkrause/dynamiceval-transformer
"""<img width=""516"" alt=""스크린샷 2021-01-28 오후 4 50 19"" src=""https://user-images.githubusercontent.com/22078438/106106482-f04da900-6188-11eb-8f15-820811c2f908.png"">   """;General;https://github.com/leaderj1001/BottleneckTransformers
"""<img width=""516"" alt=""스크린샷 2021-01-28 오후 4 50 19"" src=""https://user-images.githubusercontent.com/22078438/106106482-f04da900-6188-11eb-8f15-820811c2f908.png"">   """;Computer Vision;https://github.com/leaderj1001/BottleneckTransformers
"""This package is a re-implementation of the [m-RNN](http://www.stat.ucla.edu/~junhua.mao/m-RNN.html) image captioning method using [TensorFlow](https://www.tensorflow.org/). The training speed is optimized with buckets of different lengths of the training sentences. It also support the *Beam Search* method to decode image features into  sentences.   """;Computer Vision;https://github.com/mjhucla/TF-mRNN
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   """;Computer Vision;https://github.com/BlackAngel1111/Fast-RCNN
"""The repository contains an implementation of DeepMask and SharpMask models.  DeepMask model predicts class agnostic object mask and object score  which is positive if an object is centered and fully contained in an image. SharpMask is an extension of DeepMask architecture  which uses  a top-down refinement module to compute more precise object mask proposal.  The implementation is based on TensorFlow official ResNet-v2 [3] model implementation and requires pre-trained ResNet [weights](http://download.tensorflow.org/models/official/resnet_v2_imagenet_checkpoint.tar.gz ""TensorFlow ResNet-v2 checkpoint"").  ResNet model implementation is copied from the official TensorFlow [repository](https://github.com/tensorflow/models/tree/master/official/resnet).  Note  that variables in ResNet model checkpoint are explicitly placed on a GPU device.  So they won't be restored on a CPU.   """;General;https://github.com/aby2s/sharpmask
"""The repository contains an implementation of DeepMask and SharpMask models.  DeepMask model predicts class agnostic object mask and object score  which is positive if an object is centered and fully contained in an image. SharpMask is an extension of DeepMask architecture  which uses  a top-down refinement module to compute more precise object mask proposal.  The implementation is based on TensorFlow official ResNet-v2 [3] model implementation and requires pre-trained ResNet [weights](http://download.tensorflow.org/models/official/resnet_v2_imagenet_checkpoint.tar.gz ""TensorFlow ResNet-v2 checkpoint"").  ResNet model implementation is copied from the official TensorFlow [repository](https://github.com/tensorflow/models/tree/master/official/resnet).  Note  that variables in ResNet model checkpoint are explicitly placed on a GPU device.  So they won't be restored on a CPU.   """;Computer Vision;https://github.com/aby2s/sharpmask
"""Integration of (https://arxiv.org/abs/1506.02640) and Redmon and Farhadi  2016 (https://arxiv.org/abs/1612.08242).   YOLO stands for You Only Look Once. It's an object detector that uses features learned by a deep convolutional neural network to detect an object. Before we get out hands dirty with code  we must understand how YOLO works.     """;Computer Vision;https://github.com/alia21/VechicleDetection-and-Traccking
"""WaveNet replication study. Before stepping up to WaveNet implementation it was decided to implement PixelCNN first as WaveNet based on its architecture.  This repository contains two modes: [Gated PixelCNN][pixelcnn-paper] and [WaveNet][wavenet-paper]  see class definitions in `wavenet/models.py`.  For detailed explanation of how these model work see [my blog post](http://sergeiturukin.com/2017/02/22/pixelcnn.html).   """;Computer Vision;https://github.com/rampage644/wavenet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/dbonner/tf1_bert
"""WaveNet replication study. Before stepping up to WaveNet implementation it was decided to implement PixelCNN first as WaveNet based on its architecture.  This repository contains two modes: [Gated PixelCNN][pixelcnn-paper] and [WaveNet][wavenet-paper]  see class definitions in `wavenet/models.py`.  For detailed explanation of how these model work see [my blog post](http://sergeiturukin.com/2017/02/22/pixelcnn.html).   """;Audio;https://github.com/rampage644/wavenet
"""WaveNet replication study. Before stepping up to WaveNet implementation it was decided to implement PixelCNN first as WaveNet based on its architecture.  This repository contains two modes: [Gated PixelCNN][pixelcnn-paper] and [WaveNet][wavenet-paper]  see class definitions in `wavenet/models.py`.  For detailed explanation of how these model work see [my blog post](http://sergeiturukin.com/2017/02/22/pixelcnn.html).   """;Sequential;https://github.com/rampage644/wavenet
"""This is a project based on [InsightFace: 2D and 3D Face Analysis Project](https://github.com/deepinsight/insightface).   """;General;https://github.com/nv-quan/insightface-attendance
"""  - Why do we use word embeddings?   - The difference between Distributed and Distributional embeddings   - An easy way of approaching word embeddings using count-based methods   - Walkthrough of simple one-hot representations  building till we get to Word2Vec and GLoVe embeddings   - simple walkthrough of training your own word embedding using a given corpora     - followed by using GluonNLP for a simple OOTB approach for retrieving WE       - Outline and breakdown of notebooks and how they're structured and what you can find in each   - Introduction to Syntax  semantics  and pragmatics (definitions  examples of the differences using project gutenburg)     - include references to English syntax and a brief overview of the key terminologies with regards to English syntax with examples using code to extract them     - Outline POS  phrase structure etc.   - Ambiguity (examples with project gutenburg text) and Compositionality (with code examples of compositionality  again using project gutenburg free texts)   - Briefly explain how these cause complications for ML with regard to NL with demonstrable examples from the Gutenberg texts     """;General;https://github.com/faramarzmunshi/d2l-nlp
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Gaozhen0816/BERT_QA_For_AILaw
"""This repository contains code for the [model-agnostic meta-learning algorithm](https://arxiv.org/pdf/1703.03400.pdf) (Finn et al.) including investigation into modifying the task sampling regime. Rather than sampling uniformly  we can sample according to a probability distribution that we construct using a priority queue containing losses previously seen during training. So far the following sampling regimes have been implemented:  * Uniform (vanilla) * Epsilon Greedy (max with certain probability  random otherwise) * Weighted Sample (sample according to probability distribution given by priority queue) * Importance Weighted Sample (same as above but meta update is now weighted by importance weights) * Delta Sample (sample according to probability distribution given by change in priority queue - biases sample to parameter space in which progress has been made) * Importance Delta Sample (same as above but meta update is now weighted by importance weights)  The priority queue is essentially a mapping between parameter tuples and losses where the parameters are those governing the task distribution (e.g. in sinusoidal regression the parameters are the phase shift and amplitude scaling). Each time a task is sampled in the inner loop  the parameter associated with this task in the priority queue will be updated with the loss incurred.  So far the following tasks have been implemented:  * 2D sinusoidal regression (parameters: phase shift  amplitude) * 3D sinusoidal regression (parameters: phase shift  amplitude & frequency scaling)  Implementations for image classification and control tasks will hopefully be added soon.   This repository uses [Jax](https://github.com/google/jax) for the MAML implementation.     """;General;https://github.com/seblee97/task_weighted_maml
"""Inspired by the structure of Receptive Fields (RFs) in human visual systems  we propose a novel RF Block (RFB) module  which takes the relationship between the size and eccentricity of RFs into account  to enhance the discriminability and robustness of features. We further  assemble the RFB module to the top of SSD with a lightweight CNN model  constructing the RFB Net detector. You can use the code to train/evaluate the RFB Net for object detection. For more details  please refer to our [arXiv paper](https://arxiv.org/pdf/1711.07767.pdf).   <img align=""right"" src=""https://github.com/ruinmessi/RFBNet/blob/master/doc/rfb.png"">  &nbsp; &nbsp;   """;Computer Vision;https://github.com/ZTao-z/multiflow-resnet-ssd
"""This is the official code of [High-Resolution Representations for Object Detection](https://arxiv.org/pdf/1904.04514.pdf). We extend the high-resolution representation (HRNet) [1] by augmenting the high-resolution representation by aggregating the (upsampled) representations from all the parallel convolutions  leading to stronger representations. We build a multi-level representation from the high resolution and apply it to the Faster R-CNN  Mask R-CNN and Cascade R-CNN framework. This proposed approach achieves superior results to existing single-model networks on COCO object detection. The code is based on [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark)  <div align=center>  ![](images/hrnetv2p.png)  </div>    """;Computer Vision;https://github.com/HRNet/HRNet-MaskRCNN-Benchmark
"""ODTK is a single shot object detector with various backbones and detection heads. This allows performance/accuracy trade-offs.  It is optimized for end-to-end GPU processing using: * The [PyTorch](https://pytorch.org) deep learning framework with [ONNX](https://onnx.ai) support * NVIDIA [Apex](https://github.com/NVIDIA/apex) for mixed precision and distributed training * NVIDIA [DALI](https://github.com/NVIDIA/DALI) for optimized data pre-processing * NVIDIA [TensorRT](https://developer.nvidia.com/tensorrt) for high-performance inference * NVIDIA [DeepStream](https://developer.nvidia.com/deepstream-sdk) for optimized real-time video streams support   """;General;https://github.com/NVIDIA/retinanet-examples
"""ODTK is a single shot object detector with various backbones and detection heads. This allows performance/accuracy trade-offs.  It is optimized for end-to-end GPU processing using: * The [PyTorch](https://pytorch.org) deep learning framework with [ONNX](https://onnx.ai) support * NVIDIA [Apex](https://github.com/NVIDIA/apex) for mixed precision and distributed training * NVIDIA [DALI](https://github.com/NVIDIA/DALI) for optimized data pre-processing * NVIDIA [TensorRT](https://developer.nvidia.com/tensorrt) for high-performance inference * NVIDIA [DeepStream](https://developer.nvidia.com/deepstream-sdk) for optimized real-time video streams support   """;Computer Vision;https://github.com/NVIDIA/retinanet-examples
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/why2000/DuReader-bert
"""This repository contains examples and best practices for building recommendation systems  provided as Jupyter notebooks. The examples detail our learnings on five key tasks:  - [Prepare Data](examples/01_prepare_data): Preparing and loading data for each recommender algorithm - [Model](examples/00_quick_start): Building models using various classical and deep learning recommender algorithms such as Alternating Least Squares ([ALS](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html#ALS)) or eXtreme Deep Factorization Machines ([xDeepFM](https://arxiv.org/abs/1803.05170)). - [Evaluate](examples/03_evaluate): Evaluating algorithms with offline metrics - [Model Select and Optimize](examples/04_model_select_and_optimize): Tuning and optimizing hyperparameters for recommender models - [Operationalize](examples/05_operationalize): Operationalizing models in a production environment on Azure  Several utilities are provided in [recommenders](recommenders) to support common tasks such as loading datasets in the format expected by different algorithms  evaluating model outputs  and splitting training/test data. Implementations of several state-of-the-art algorithms are included for self-study and customization in your own applications. See the [recommenders documentation](https://readthedocs.org/projects/microsoft-recommenders/).  For a more detailed overview of the repository  please see the documents on the [wiki page](https://github.com/microsoft/recommenders/wiki/Documents-and-Presentations).   """;Graphs;https://github.com/microsoft/recommenders
"""Inspired by the structure of Receptive Fields (RFs) in human visual systems  we propose a novel RF Block (RFB) module  which takes the relationship between the size and eccentricity of RFs into account  to enhance the discriminability and robustness of features. We further  assemble the RFB module to the top of SSD with a lightweight CNN model  constructing the RFB Net detector. You can use the code to train/evaluate the RFB Net for object detection. For more details  please refer to our [arXiv paper](https://arxiv.org/pdf/1711.07767.pdf).   <img align=""right"" src=""https://github.com/ruinmessi/RFBNet/blob/master/doc/rfb.png"">  &nbsp; &nbsp;   """;General;https://github.com/ZTao-z/multiflow-resnet-ssd
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/chrisleunglokhin/Capstone-BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/chrisleunglokhin/Capstone-BERT
"""Pneumonia is a disease in which the air sacs in one or both lungs get infected and inflame. The air sacs may fill with fluid or pus (purulent material)  causing cough with phlegm or pus  fever  chills  and difficulty breathing. Diseases such as Pneumonia are responsible for over 1 million hospitalizations and 50 000 deaths a year in the US alone. Currently radiologists use Chest X-Rays to detect diseases such as Pneumonia. Other diseases detected in this manner include Atelectasis  Consolidation  Infiltration  Pneumothorax  Edema  Emphysema  Fibrosis  Effusion  Pleural Thickening  Cardiomegaly  Nodule  Hernia and Mass. Once detected  the patient can be treated. However if the disease is not detected at an early stage  the consequences can be severe.   Luckily algorithms can be trained to detect diseases and assist medical personel. In fact algorithms can be trained to detect diseases such as Pneumonia with greater accuracy than any human radiologist from chest X-Rays. Therfore  through decreasing human error in detection  countless lives can be saved!  Further an estimated two thirds of the global population lacks access to radiology diagnostics. These diagnostics include as mentioned above detection of diseases. With the automation of radiology experts  healthcare delivery can be improved and access to medical imaging expertise can be increased in many parts of the world. Therefore  through automating radiology experts  many parts of the world will gain radiology diagnostics and countless lives can be saved!  We set out to build an algorithm that could take as input a chest X-ray image and return probabilities for a collection of diseases detectable through chest X-rays (Atelectasis  Consolidation  Infiltration  Pneumothorax  Edema  Emphysema  Fibrosis  Effusion  Pleural Thickening  Cardiomegaly  Nodule  Hernia  Mass) and the probability of no disease being present.   ![Image of chest X-Ray and heatmap](https://github.com/thibaultwillmann/CheXNet-Pytorch/blob/master/chest_x_ray_example.png)  Image of a chest X-Ray left and heatmap highlighting areas with high probalility of a disease being present right   """;Computer Vision;https://github.com/thibaultwillmann/CheXNet-Pytorch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/cospplay/bert-master
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/tms2003/yolov3
"""| TaskID | Description                                                                          | Examples                                                                | | ------- | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------| | 1       | Detection of an aspect in a review.                                                  | Is sleep quality mentioned in this review?                             | | 2       | Prediction of the customer general satisfaction.                                     | Is the client satisfied by this hotel?                                   | | 3       | Prediction of the global trend of an aspect in a given review.                       | Is the client satisfied with the cleanliness of the hotel?               | | 4       | Prediction of whether the rating of a given aspect is above or under a given value.  | Is the rating of location under 4?                                     | | 5       | Prediction of the exact rating of an aspect in a review.                             | What is the rating of the aspect Value in this review?                 | | 6       | Prediction of the list of all the positive/negative aspects mentioned in the review. | Can you give me a list of all the positive aspects in this review?     | | 7.0     | Comparison between aspects.                                                          | Is the sleep quality better than the service in this hotel?            | | 7.1     | Comparison between aspects.                                                          | Which one of these two aspects  service  location has the best rating? | | 8       | Prediction of the strengths and weaknesses in a review.                              | What is the best aspect rated in this comment?                         |   """;General;https://github.com/qgrail/ReviewQA
"""This code tries to implement the Neural Turing Machine  as found in  https://arxiv.org/abs/1410.5401  as a backend neutral recurrent keras layer.  A very default experiment  the copy task  is provided  too.  In the end there is a TODO-List. Help would be appreciated!  NOTE: * There is a nicely formatted paper describing the rough idea of the NTM  implementation difficulties and which discusses the   copy experiment. It is available here in the repository as The_NTM_-_Introduction_And_Implementation.pdf.  * You may want to change the LOGDIR_BASE in testing_utils.py to something that works for you or just set a symbolic   link.    """;General;https://github.com/fuchason/NTM-keras
"""This code tries to implement the Neural Turing Machine  as found in  https://arxiv.org/abs/1410.5401  as a backend neutral recurrent keras layer.  A very default experiment  the copy task  is provided  too.  In the end there is a TODO-List. Help would be appreciated!  NOTE: * There is a nicely formatted paper describing the rough idea of the NTM  implementation difficulties and which discusses the   copy experiment. It is available here in the repository as The_NTM_-_Introduction_And_Implementation.pdf.  * You may want to change the LOGDIR_BASE in testing_utils.py to something that works for you or just set a symbolic   link.    """;Sequential;https://github.com/fuchason/NTM-keras
"""The description made above remains valid here ``` #:#: main parameters reload_model     #: model to reload for encoder decoder #:#: data location / training objective ae_steps          #: denoising auto-encoder training steps bt_steps          #: back-translation steps mt_steps          #: parallel training steps word_shuffle      #: noise for auto-encoding loss word_dropout      #: noise for auto-encoding loss word_blank        #: noise for auto-encoding loss lambda_ae         #: scheduling on the auto-encoding coefficient  #:#: transformer parameters encoder_only      #: use a decoder for MT  #:#: optimization tokens_per_batch  #: use batches with a fixed number of words eval_bleu         #: also evaluate the BLEU score ```  ``` #:#: main parameters exp_name                     #: experiment name exp_id                       #: Experiment ID dump_path                    #: where to store the experiment (the model will be stored in $dump_path/$exp_name/$exp_id)  #:#: data location / training objective data_path                    #: data location  lgs                          #: considered languages/meta-tasks clm_steps                    #: CLM objective mlm_steps                    #: MLM objective  #:#: transformer parameters emb_dim                      #: embeddings / model dimension n_layers                     #: number of layers n_heads                      #: number of heads dropout                      #: dropout attention_dropout            #: attention dropout gelu_activation              #: GELU instead of ReLU  #:#: optimization batch_size                   #: sequences per batch bptt                         #: sequences length optimizer                    #: optimizer epoch_size                   #: number of sentences per epoch max_epoch                    #: Maximum epoch size validation_metrics           #: validation metric (when to save the best model) stopping_criterion           #: end experiment if stopping criterion does not improve  #:#: dataset #:#:#:#: These three parameters will always be rounded to an integer number of batches  so don't be surprised if you see different values than the ones provided. train_n_samples              #: Just consider train_n_sample train data valid_n_samples              #: Just consider valid_n_sample validation data  test_n_samples               #: Just consider test_n_sample test data for #:#:#:#: If you don't have enough RAM/GPU or swap memory  leave these three parameters to True  otherwise you may get an error like this when evaluating : #:#:#:#:#:#: RuntimeError: copy_if failed to synchronize: cudaErrorAssert: device-side assert triggered remove_long_sentences_train #: remove long sentences in train dataset       remove_long_sentences_valid #: remove long sentences in valid dataset   remove_long_sentences_test  #: remove long sentences in test dataset   ```   """;Natural Language Processing;https://github.com/Tikquuss/meta_XLM
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/xuzhezhaozhao/bert_reading
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/xuzhezhaozhao/bert_reading
"""The description made above remains valid here ``` #:#: main parameters reload_model     #: model to reload for encoder decoder #:#: data location / training objective ae_steps          #: denoising auto-encoder training steps bt_steps          #: back-translation steps mt_steps          #: parallel training steps word_shuffle      #: noise for auto-encoding loss word_dropout      #: noise for auto-encoding loss word_blank        #: noise for auto-encoding loss lambda_ae         #: scheduling on the auto-encoding coefficient  #:#: transformer parameters encoder_only      #: use a decoder for MT  #:#: optimization tokens_per_batch  #: use batches with a fixed number of words eval_bleu         #: also evaluate the BLEU score ```  ``` #:#: main parameters exp_name                     #: experiment name exp_id                       #: Experiment ID dump_path                    #: where to store the experiment (the model will be stored in $dump_path/$exp_name/$exp_id)  #:#: data location / training objective data_path                    #: data location  lgs                          #: considered languages/meta-tasks clm_steps                    #: CLM objective mlm_steps                    #: MLM objective  #:#: transformer parameters emb_dim                      #: embeddings / model dimension n_layers                     #: number of layers n_heads                      #: number of heads dropout                      #: dropout attention_dropout            #: attention dropout gelu_activation              #: GELU instead of ReLU  #:#: optimization batch_size                   #: sequences per batch bptt                         #: sequences length optimizer                    #: optimizer epoch_size                   #: number of sentences per epoch max_epoch                    #: Maximum epoch size validation_metrics           #: validation metric (when to save the best model) stopping_criterion           #: end experiment if stopping criterion does not improve  #:#: dataset #:#:#:#: These three parameters will always be rounded to an integer number of batches  so don't be surprised if you see different values than the ones provided. train_n_samples              #: Just consider train_n_sample train data valid_n_samples              #: Just consider valid_n_sample validation data  test_n_samples               #: Just consider test_n_sample test data for #:#:#:#: If you don't have enough RAM/GPU or swap memory  leave these three parameters to True  otherwise you may get an error like this when evaluating : #:#:#:#:#:#: RuntimeError: copy_if failed to synchronize: cudaErrorAssert: device-side assert triggered remove_long_sentences_train #: remove long sentences in train dataset       remove_long_sentences_valid #: remove long sentences in valid dataset   remove_long_sentences_test  #: remove long sentences in test dataset   ```   """;General;https://github.com/Tikquuss/meta_XLM
"""Fire is very useful discovery of the humanity. Like any other human invention  mis-handeling of fire can cause huge damage to the humanity and nature. Every year  urban fire results huge life and property damage. Similary the tragic loss of natural resources in uncontroleld wildfire is well known. The control of wildfire has becoming a huge challenge by using the treditional technologies. On the other hand  recently  the advancement in technology and especially the machine learning have benifited the society a lot in diverse aspects  ranging from self driving car to cancer research. Hence  it can surely contribute to the technology to early detect the fire so that we can react it earlier before getting it worse and making a lots of damage or being it out of control. We can train a deep neural network to distinguish fire and non-fire situation with very good accuracy. This technology aided with suitable hardware design can be vary useful to minimize the fire hazard. In this work we train a convolutional neural network which can achieve out of sample accuracy of 97% classifying the fire and non-fire images.      """;Computer Vision;https://github.com/roshankoirala/Fire_Detection_Model
"""CenterNet is a framework for object detection with deep convolutional neural networks. You can use the code to train and evaluate a network for object detection on the MS-COCO dataset.  * It achieves state-of-the-art performance (an AP of 47.0%) on one of the most challenging dataset: MS-COCO.  * Our code is written in Python  based on [CornerNet](https://github.com/princeton-vl/CornerNet).  *More detailed descriptions of our approach and code will be made available soon.*  **If you encounter any problems in using our code  please contact Kaiwen Duan: kaiwen.duan@vipl.ict.ac.cn.**   """;Computer Vision;https://github.com/Duankaiwen/CenterNet
"""This project solves the problem of detecting stress fibers (parallel actin filaments) located above and below nuclei within a single cell using a confocal microscope image of this cell. ![alt text](https://github.com/ninanikitina/BioLab/blob/master/readme_pic/research_project.png?raw=true)  There are two parts of the project: 1) Test part - detection volume of all nuclei in the picture with multiple nuclei. UNet model is used to detect countors of each nucleus on a picture  and the Alexnet model was used to detect real vs. reflected nucleus images on each layer. UNet model was trained from scratch with 300 images of 30 nuclei on different slices (no data augmentation). The result was compared with the result produced by the Imaris program. Test part overview and results: https://github.com/ninanikitina/BioLab/blob/master/readme_pic/Presentarion_12.18.2020.pdf 2) Main part - detecting stress fibers. Slices of the 3D image of a single nucleus are converted from XY axis to ZY axis  and a UNet model is used to detect countors of each fiber on each slice.   The UNet model was trained from scratch with 40 images of different ZY slices of one nucleus (no data augmentation image).     """;Computer Vision;https://github.com/ninanikitina/BioLab
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/KaidongLi/tf-3d-alpha
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/dbonner/tf1_bert
"""R-CNN is a state-of-the-art visual object detection system that combines bottom-up region proposals with rich features computed by a convolutional neural network. At the time of its release  R-CNN improved the previous best detection performance on PASCAL VOC 2012 by 30% relative  going from 40.9% to 53.3% mean average precision. Unlike the previous best results  R-CNN achieves this performance without using contextual rescoring or an ensemble of feature types.  R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1311.2524) and will appear in a forthcoming CVPR 2014 paper.   """;Computer Vision;https://github.com/jiangbestone/DetectRccn
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.    """;Computer Vision;https://github.com/liuji93/yolo_v3
"""The idea of this notebook is to investigate what Lime allows to do and how it can be used. There is also a small description of how Lime works in the vincity of a specific sample.   """;General;https://github.com/LaurentLava/Lime
"""Prenez un data scientist qui aurait conçu un algorithme de classification de données avec des résultats corrects. Demandez-lui comment l’améliorer. Neuf fois sur dix  vous vous entendrez répondre : “Il me faut plus de données !”.   En général  avoir plus de données permet de créer des modèles plus proches de la réalité  qui se généraliseront mieux aux nouvelles données (qui n'ont pas servi à l'apprentissage) et permettront donc d’obtenir de meilleurs résultats. Mais le problème c’est que pour réaliser l’apprentissage de modèles sur de gros volumes de données  il faut mettre en place une architecture de stockage et de calcul appropriée.   """;Computer Vision;https://github.com/andersoncarlosfs/image_classification
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/rickyHong/Google-BERT-repl
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/rickyHong/Google-BERT-repl
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/wuryantoAji/POINTNET
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/cospplay/bert-master
"""* **What:**    * The authors propose an alternative generator architecture for generative adversarial    networks  borrowing from style transfer literature    * The new generator improves the state-of-the-art in terms    of traditional distribution quality metrics   * The authors propose two new  automated methods to quantify interpolation quality    and disentanglement  that are applicable to any generator architecture   * The authors introduce a new  highly varied and high-quality dataset of    human faces The (FFHQ  Flickr-Faces-HQ).    * The authors proposed the way to control different features of the generated image and made   the model more explainable. See theirs official video (https://www.youtube.com/watch?v=kSLJriaOumA&feature=youtu.be) * **Method:**  The idea of creating Mapping Network with the transformed/inner latent space is used  instead of using input  latent code. This mapping part consists of 8 MLP blocks(8 gave the best performance)  which gives __new__ `W-space`  latent code  that is passed to the __Generator__ as a style which consists of 18 conv layers(two for each resolution 4 ** 2 - 1024 ** 2).  See [#Architecture] figure.    Through the experimental process it was found out that starting image at the top of `g-Generator` can be learn   constant tensor instead of a real image. Following the architecture this image then in each block   follows the next process. First conv layer. Then summed with _per channel scaled  noise_ (scaling is learned as B-parameter). Then again conv. Then AdaIN block.    AdaIN Block can be described the next way:  ![AdaIN](assets/adain.png)  where `x_i` is input per channel feature passed vertically from top and `(y_s_i  y_b_i)` are _style_ parameters  got from transformation of `w-vector` from input `W` latent space. To get these style parameters this vector is   first passed through learned affine transformation.       **To sum it all up**  the main interesting points are: the input `4x4x512` tensor is always the same and we   only get styles from our latent variable. Moreover they say that at each scale changing our _style_ inputs   we can change different **very** localised parts of generated image (See video).      > Copying the styles corresponding to coarse spatial resolutions (4 ** 2–8 ** 2) brings high-level aspects such as pose  general hair style  face shape  and eyeglasses from source B  while all colors(eyes  hair  lighting) and finer facial features resemble A. If we instead copy the styles of middle resolutions (16 ** 2–32 ** 2) from B  we inheritsmaller scale facial features  hair style  eyes open/closed from B  while the pose  general face shape  and eyeglasses from A are preserved.Finally  copying the fine styles (64 ** 2–1024 ** 2)   By the way most of the architecture/training process is based on https://arxiv.org/pdf/1710.10196.pdf (Baseline configuration is the Progressive GAN).  The changes made to architecture: - The baseline using bilinear up/downsampling operations  longer training  and tuned hyperparameters - Adding the mapping network and AdaIN operations - Simplify the architecture by removing the traditional input layer and starting the image synthesis from a learned 4 × 4 × 512 constant tensor - Adding the noise inputs - Adding the mixing regularization(I didn't mention it. The idea is the following: in the learning process we don't  take one latent variable  but take two. Then for one part of the network we feed as style first variable and then the other. That way the model separate the influence and doesn't entangle it. At test time see [Mixing] figure below)  ![Architecture](assets/photo5197353128075307948.jpg)  Figure _Architecture_ * **W** - an intermediate latent space * **AdaIN** - adaptive *style* instance normalization * **A** - learned affine transform * **B** - learned per-channel scaling fac-tors to the noise input  ![Mixture](assets/mixture.png)  _Figure Mixing_. Two sets of images were generated from their respective latent codes (sources A and B); the rest of the images were generated bycopying a specified subset of styles from source B and taking the rest from source A.   * **Proposed metrics:**        - Perceptual path length:     ![Path_length](assets/path_length.png)     where `z1  z2` are latent variables; `t` is random value in `(0 1)` interval;     `eps` = `1e-4`; `G` stands for generator; `slerp` - spherical      interpolation; `d(x1  x2)` - `l2(vgg(x1)  vgg(x2))`;     The idea is that for less entangled and informative latent code interpolation of latent-space      vectors should not yield surprisingly non-linear changes in the image. Quantative results in the Table 3.     - Linear separability. Tries to find out the separability in the input     latent code. Given the features of generated images as Male/Female  find out     how informative for this info is input latent code.     The idea is the next: given good dataset as CELEBA-HQ train good classifier of      a given feature; then generate images using our **Generator** and label using trained     classifier so we have <latent variable  class of interest> mapping; then train SVM on this     data and exponent of cross entropy of this model will be the score of Linear Separability.     See table:     ![Metrics](assets/metrics.png)  * **Results:**    * **Frechet inception distance (FID)**  ![FID](assets/photo5195447889172737473.jpg)       """;Computer Vision;https://github.com/irynakostyshyn/A-Style-Based-Generator-Architecture-for-GAN
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/VaanHUANG/CSCI5210HW1
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/ml-inory/Caffe
"""*Artistic Styling of images using CNNs  VGG*  *****************************************  """;Computer Vision;https://github.com/sarthakbaiswar/Artistic-Styling
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/shanry/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/shanry/bert
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/mike07026/insightface_20190620
"""```sh  Model: ""sequential_1"" _________________________________________________________________ Layer (type)                 Output Shape              Param #:    ================================================================= conv2d_1 (Conv2D)            (None  224  224  64)      1792       _________________________________________________________________ conv2d_2 (Conv2D)            (None  224  224  64)      36928      _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None  112  112  64)      0          _________________________________________________________________ conv2d_3 (Conv2D)            (None  112  112  128)     73856      _________________________________________________________________ conv2d_4 (Conv2D)            (None  112  112  128)     147584     _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None  56  56  128)       0          _________________________________________________________________ conv2d_5 (Conv2D)            (None  56  56  256)       295168     _________________________________________________________________ conv2d_6 (Conv2D)            (None  56  56  256)       590080     _________________________________________________________________ conv2d_7 (Conv2D)            (None  56  56  256)       590080     _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None  28  28  256)       0          _________________________________________________________________ conv2d_8 (Conv2D)            (None  28  28  512)       1180160    _________________________________________________________________ conv2d_9 (Conv2D)            (None  28  28  512)       2359808    _________________________________________________________________ conv2d_10 (Conv2D)           (None  28  28  512)       2359808    _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None  14  14  512)       0          _________________________________________________________________ conv2d_11 (Conv2D)           (None  14  14  512)       2359808    _________________________________________________________________ conv2d_12 (Conv2D)           (None  14  14  512)       2359808    _________________________________________________________________ conv2d_13 (Conv2D)           (None  14  14  512)       2359808    _________________________________________________________________ max_pooling2d_5 (MaxPooling2 (None  7  7  512)         0          _________________________________________________________________ flatten_1 (Flatten)          (None  25088)             0          _________________________________________________________________ dense_1 (Dense)              (None  4096)              102764544  _________________________________________________________________ dense_2 (Dense)              (None  4096)              16781312   _________________________________________________________________ dense_3 (Dense)              (None  2)                 8194       ================================================================= Total params: 134 268 738 Trainable params: 134 268 738 Non-trainable params: 0 _________________________________________________________________ ```   """;Computer Vision;https://github.com/ashushekar/VGG16
"""The model is a Convolutional Neural Network with shortcuts as showed in this paper: https://arxiv.org/pdf/1512.03385.pdf   """;General;https://github.com/LuigiRussoDev/ResNets
"""The model is a Convolutional Neural Network with shortcuts as showed in this paper: https://arxiv.org/pdf/1512.03385.pdf   """;Computer Vision;https://github.com/LuigiRussoDev/ResNets
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/LONG-9621/Extract_Point_3D
"""Code is for two robust multimodal two-stage object detection networks BIRANet and RANet. The two modalities used in these architectures are radar signals and RGB camera images. These two networks have the same base architecture with differences in anchor generation and RPN target generation methods  which are explained in the paper. Evaluation is done on NuScenes dataset[https://www.nuscenes.org]  and results are compared with Faster R-CNN with feature pyramid network for object detection(FFPN)[https://arxiv.org/pdf/1612.03144.pdf]. Both proposed networks proved to be robust in comparison to FFPN. BIRANet performs better than FFPN and also proved to be more robust. RANet is evaluated to be robust and works reasonably well with fewer anchors  which are merely based on radar points. For further details  please refer to our paper(https://ieeexplore.ieee.org/document/9191046).  <img src=""https://github.com/RituYadav92/NuScenes_radar_RGBFused-Detection/blob/master/Demo/Front.gif"" alt=""alt text"" width=""300"" height=""200""> <img src=""https://github.com/RituYadav92/NuScenes_radar_RGBFused-Detection/blob/master/Demo/Back_Cam.gif"" alt=""alt text"" width=""300"" height=""200"">   """;Computer Vision;https://github.com/RituYadav92/Radar-RGB-Attentive-Multimodal-Object-Detection
"""How can we use the causal knowledge obtained by causal discovery to improve machine learning? Are causal discovery methods beneficial for machine learning tasks?  * **Causal mechanism transfer** <sup>[1](#references)</sup>   Domain adaptation among systems sharing the same causal mechanism   can be performed by estimating the structural equations (reduced-form equations; estimated by nonlinear ICA).    <div align=""center"">   <img src=""./docs_src/figs/schema_problem_setup.png"" alt=""Problem setup schema"" width=""50%""/>   </div>    * Theoretically well-grounded domain adaptation (generalization error bound without the partial-distribution-shift assumption).   * Intuitively accessible transfer assumption: if the data come from the same causal mechanism  information can be transferred.   * Method to directly leverage the estimated structural equations: via data augmentation.   """;General;https://github.com/takeshi-teshima/few-shot-domain-adaptation-by-causal-mechanism-transfer
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Osobarako/alduswarrensewell
"""AlphaVideo is an open-sourced video understanding toolbox based on [PyTorch](https://pytorch.org/) covering multi-object tracking and action detection. In AlphaVideo  we released the first one-stage multi-object tracking (MOT) system **TubeTK** that can achieve 66.9 MOTA on [MOT-16](https://motchallenge.net/results/MOT16) dataset and 63 MOTA on [MOT-17](https://motchallenge.net/results/MOT17) dataset. For action detection  we released an efficient model **AlphAction**  which is the first open-source project that achieves 30+ mAP (32.4 mAP) with single model on [AVA](https://research.google.com/ava/) dataset.   """;Computer Vision;https://github.com/Alpha-Video/AlphaVideo
"""R-CNN is a state-of-the-art visual object detection system that combines bottom-up region proposals with rich features computed by a convolutional neural network. At the time of its release  R-CNN improved the previous best detection performance on PASCAL VOC 2012 by 30% relative  going from 40.9% to 53.3% mean average precision. Unlike the previous best results  R-CNN achieves this performance without using contextual rescoring or an ensemble of feature types.  R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1311.2524) and will appear in a forthcoming CVPR 2014 paper.   """;Computer Vision;https://github.com/jiangbestone/detect_rcnn
"""DCGAN implementation on Mnist and [icons-50](https://www.kaggle.com/danhendrycks/icons50?) on pytorch. Mostly following this paper https://arxiv.org/pdf/1511.06434.pdf and https://github.com/soumith/ganhacks.   """;Computer Vision;https://github.com/maxencealluin/mnist_DCGAN
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/Coldmooon/SSD-on-Custom-Dataset
"""The traditional methods of estimating uncertainties were mainly Bayesian methods. Bayesian methods should introduce the preor to estimate the distribution of the posteror. But DNN has too many parameters  so it's hard to calculate. So there are also non-bayesian methods that are most famous for modelling methods.  This method learns several DNNs to obtain an uncertainty with different degrees of prediction.  This method has a large computational cost because it requires the learning of several models.  Other methods have problems that cannot be measured by distinguishing between an entity's data from an entity's internal uncertainty.  Solve these problems using SDE-Net. SDE-Net alternates between drift net and diffuse net.  The drift net increases the accuracy of the prediction and allows the measurement of the analistic entity  and the epistemical entity is measured with the diffusion net.   """;General;https://github.com/Junghwan-brian/SDE-Net
"""**Facial detection**  Use case: face detection on heritage images    """;Computer Vision;https://github.com/altomator/Introduction_to_Deep_Learning-2-Face_Detection
"""------------ A weighted version of the [Soft Actor-Critic Algorithms](https://arxiv.org/pdf/1801.01290.pdf). The code is largely derived from a [Pytorch SAC Implementation](https://github.com/pranz24/pytorch-soft-actor-critic)  We support five sampling strategies:  - Uniform Sampling (as used in the original version of SAC) - [Emphasizing Recent Experience (ERE)](https://arxiv.org/abs/1906.04009) - Approximated version of ERE (ERE\_apx  Proposition 1 in our paper) - 1/age weighed sampling - [Prioritized Experience Replay (PER)](https://arxiv.org/abs/1511.05952)   This implementation demonstrates that ERE  ERE\_apx and 1/age share very similar performances and are better than uniform sampling and PER.   """;Reinforcement Learning;https://github.com/sunfex/weighted-sac
"""1. [requirements.txt](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/requirements.txt) - Includes all the required libraries for the Conda Environment. 2. [model.py](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/model.py) - Defines the QNetwork which is the nonlinear function approximator to calculate the value actions based directly on observation from the environment. 3. [dqn_agent.py](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/dqn_agent.py) -  Defines the Agent that uses Deep Learning to find the optimal parameters for the function approximators  determines the best action to take and maximizes the overall or total reward. 4. [Navigation.ipynb](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/Navigation.ipynb) - The main file that trains the Deep Q-Network and shows the trained agent in action. This file can be run in the Conda environment.   """;Reinforcement Learning;https://github.com/iDataist/Navigation-with-Deep-Q-Network
"""The implementation is based on the following: - Simple Online and Realtime Tracking with a Deep Association Metric https://arxiv.org/abs/1703.07402 - YOLOv4: Optimal Speed and Accuracy of Object Detection https://arxiv.org/pdf/2004.10934.pdf -YOLOv5 https://github.com/ultralytics/yolov5 - ESRGAN https://arxiv.org/abs/1809.00219   This repository contains a moded version of PyTorch YOLOv5 (https://github.com/ultralytics/yolov5) It filters out every detection that is not a Number Plate. The detections of Vehicle Number Plates are then passed to a Deep Sort algorithm (https://github.com/ZQPei/deep_sort_pytorch) which tracks the same along with Pytorch 1.7. The main reason to only detect Number plates is that the deep association metric is trained on a Vehicle Number Plate ONLY dataset.The detections are then cropped and subjected to Super Resolution Technique ESRGAN ( Training of the ESRGAN to get better Resolution Number Plate is done) to get high resolution number plates followed by application of EasyOCR on the same images.The registration number plates after being read are then logged into a CSV  .   """;Computer Vision;https://github.com/nabarunbaruaAIML/ATCC_Yolov5
"""Training.py：To train and evaluate the our proposed multi-focus network.  SurvmodelTMI2.py: The architecture of our proposed multi-focus network.  SurvmodelTMI_FPN18ori.py：The architecture of network based on the feature pyramid network and ResNet-18. More details will be found when our work is published.  SurvmodelTMI_FPN50ori.py The architecture of network based on the feature pyramid network and ResNet-50.  The directory of Methods contains the existing methods which are used for comparison.   """;Computer Vision;https://github.com/dreamenwalker/Multi-focusNet
"""JFastText is a Java wrapper for Facebook's [fastText](https://github.com/facebookresearch/fastText)   a library for efficient learning of word embeddings and fast sentence classification. The JNI interface is built using [javacpp](https://github.com/bytedeco/javacpp).  The library provides full fastText's command line interface. It also provides the API for loading trained model from file to do label prediction in memory. Model training and quantization are supported via the command line interface.  JFastText is ideal for building fast text classifiers in Java.   """;Natural Language Processing;https://github.com/vinhkhuc/JFastText
"""In the world of Machine Learning  Neural Networks have been successful to achieve state-of-art accuracy in multiple domains like image classification  voice recognition  autonomous driving and so on. In this project  I focus on training various neural network models for classifying images. The Datasets that were used are MNIST  FMNIST  CIFAR 10  and SVHN. Studied Various regularization techniques their effect while training. During the training phase  there are mulitple parameters which influence the validation accuracy. The algorithmic community is coming out with different optimization strategies to reduce the validation loss. My goal is to study the most recently used optimization strategies and observe their influence over the accuracy of the network.   """;Computer Vision;https://github.com/Spoorthy-gunda/Image-Classification
"""This commited code changes the user's description to an icon representing the current moon phase. I'd like to expand on this to update the username to the name of the phase of the mooon. As with the last script  to use this  skip the ""Glitch your avatar"" section - and remind me to redo this README.md to account for all the different scripts soon!   """;General;https://github.com/philcryer/randota
"""This commited code changes the user's description to an icon representing the current moon phase. I'd like to expand on this to update the username to the name of the phase of the mooon. As with the last script  to use this  skip the ""Glitch your avatar"" section - and remind me to redo this README.md to account for all the different scripts soon!   """;Computer Vision;https://github.com/philcryer/randota
"""> * The main results on these UCI datasets are summarized below. > * ARM-Net achieves overall best performance. > * More results and technical details can be found [here](https://github.com/nusdbsystem/ARM-Net/tree/uci#main-results-evaluated-on-first-36121-datasets-updating). >   | Model |  Rank(Best_Cnt)  | abalone|  acute-inflammation|  acute-nephritis|  adult|  annealing|  arrhythmia|  audiology-std|  balance-scale|  balloons|  bank|  blood|  breast-cancer|  breast-cancer-wisc|  breast-cancer-wisc-diag|  breast-cancer-wisc-prog|  breast-tissue|  car|  cardiotocography-10clases|  cardiotocography-3clases|  chess-krvk|  chess-krvkp|  congressional-voting|  conn-bench-sonar-mines-rocks|  conn-bench-vowel-deterding|  connect-4|  contrac|  credit-approval|  cylinder-bands|  dermatology|  echocardiogram|  ecoli|  energy-y1|  energy-y2|  fertility|  flags|  glass| |:-----------:|:-----------:|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------| | `n_samples` | - | 4177|  120|  120|  48842|  898|  452|  196|  625|  16|  4521|  748|  286|  699|  569|  198|  106|  1728|  2126|  2126|  28056|  3196|  435|  208|  990|  67557|  1473|  690|  512|  366|  131|  336|  768|  768|  100|  194|  214| | `n_features` | - | 9|  7|  7|  15|  32|  263|  60|  5|  5|  17|  5|  10|  10|  31|  34|  10|  7|  22|  22|  7|  37|  17|  61|  12|  43|  10|  16|  36|  35|  11|  8|  9|  9|  10|  29|  10| | LR | 6-th (0/36) | 0.6293/0.0080|  0.9833/0.0211|  0.9533/0.0552|  0.8423/0.0008|  0.1280/0.0172|  0.5442/0.0184|  0.7040/0.0480|  0.8718/0.0310|  0.7250/0.0935|  0.8904/0.0023|  0.7610/0.0043|  0.6923/0.0171|  0.9490/0.0090|  0.9641/0.0103|  0.6626/0.0656|  0.5283/0.1371|  0.8032/0.0052|  0.7595/0.0118|  0.8798/0.0120|  0.2743/0.0009|  0.9438/0.0035|  0.5705/0.0328|  0.7385/0.0186|  0.7121/0.0088|  0.7547/0.0004|  0.4829/0.0383|  0.8557/0.0119|  0.6305/0.0647|  0.9399/0.0313|  0.7600/0.0605|  0.7988/0.0510|  0.8391/0.0123|  0.8448/0.0297|  0.5800/0.1066|  0.4206/0.0365|  0.5290/0.0281| | FM | 5-th (3/36) | 0.6329/0.0067|  0.9767/0.0389|  0.8700/0.0945|  0.8443/0.0005|  0.1960/0.1493|  0.5283/0.0211|  0.4880/0.0588|  `0.9224/0.0087`|  0.5750/0.1275|  0.8882/0.0028|  0.7647/0.0000|  0.6909/0.0604|  0.9599/0.0048|  `0.9697/0.0048`|  0.6626/0.0849|  0.5094/0.0818|  0.8882/0.0097|  0.7616/0.0161|  0.8903/0.0172|  0.3127/0.0035|  0.9796/0.0038|  0.5705/0.0306|  `0.9502/0.0087`|  0.9502/0.0087|  0.8264/0.0005|  0.4524/0.0140|  0.8638/0.0093|  0.7016/0.0250|  0.9202/0.0350|  0.7846/0.0600|  0.7595/0.0680|  0.8823/0.0086|  0.8604/0.0283|  0.7720/0.0688|  0.3423/0.0200|  0.5907/0.0361| | DNN | 4-th (6/36) |0.6560/0.0051|  0.9900/0.0200|  0.9500/0.0316|  0.8519/0.0015|  0.4420/0.2346|  0.6442/0.0114|  0.6880/0.0466|  0.8987/0.0048|  0.5500/0.2318|  0.8900/0.0035|  0.7583/0.0050|  0.7147/0.0082|  0.9633/0.0033|  0.9648/0.0107|  0.7091/0.0475|  0.5849/0.0396|  0.9442/0.0034|  0.7797/0.0121|  0.9178/0.0031|  0.6842/0.0147|  0.9775/0.0032|  0.5834/0.0147|  0.7481/0.0377|  `0.9745/0.0063`|  0.8501/0.0023|  0.5084/0.0158|  0.8417/0.0187|  `0.7359/0.0386`|  `0.9639/0.0101`|  0.7846/0.0337|  `0.8524/0.0166`|  0.8688/0.0107|  `0.8865/0.0094`|  0.8320/0.0722|  `0.4969/0.0272`|  0.5850/0.0316| | SNN | 3rd (6/36) |0.6457/0.0043|  0.9567/0.0389|  0.9000/0.0548|  0.8489/0.0009|  0.2280/0.2671|  0.5841/0.0410|  `0.7200/0.0253`|  0.9058/0.0240|  0.7250/0.1225|  0.8885/0.0019|  0.8885/0.0019|  0.7105/0.0105|  `0.9656/0.0041`|  0.9690/0.0112|  0.6727/0.0903|  `0.6000/0.0690`|  `0.9632/0.0066`|  `0.8008/0.0125`|  0.9029/0.0086|  0.6796/0.0141|  0.9726/0.0061|  0.5779/0.0209|  0.7135/0.0300|  0.9693/0.0100|  0.8491/0.0013|  0.5106/0.0098|  `0.8719/0.0121`|  0.7000/0.0163|  0.9388/0.0269|  0.7877/0.0439|  0.8179/0.035|  0.8714/0.0142|  0.8854/0.0154|  0.7600/0.1180|  0.4804/0.0231|  0.5738/0.0602| | Perceiver-IO | 2nd (6/36) |0.6381/0.0143|  `1.0000/0.0000`|  0.9367/0.0531|  0.8521/0.0011|  `0.7600/0.0000`|  0.5602/0.0053|  0.0080/0.0160|  0.8821/0.0166|  `0.7750/0.0500`|  0.8850/0.0000|  0.7620/0.0000|  0.7063/0.0088|  0.9352/0.0313|  0.9556/0.0142|  `0.7596/0.0118`|  0.3208/0.0597|  0.9326/0.0120|  0.5325/0.0861|  0.7817/0.0035|  0.6834/0.0151|  0.8106/0.0895|  `0.6129/0.0000`|  0.5635/0.0817|  0.6732/0.0521|  0.7538/0.0000|  0.4457/0.0122|  0.7745/0.1075|  0.6133/0.0078|  0.4295/0.0754|  0.7662/0.0834|  0.6440/0.0239|  0.8417/0.0295|  0.8807/0.0325|  `0.8560/0.0480`|  0.3010/0.0247|  0.4093/0.0415| | `ARM-Net` | `1st (15/36)` |`0.6603/0.0034`|  0.9767/0.0389|  `0.9600/0.0800`|  `0.8562/0.0011`|  0.1500/0.1131|  `0.6487/0.0214`|  0.5520/0.0299|  0.9135/0.0070|  0.7500/0.0791|  `0.8922/0.0012`|  `0.8922/0.0012`|  `0.7203/0.0193`|  0.9530/0.0118|  0.9521/0.0186|  0.6828/0.0485|  0.5170/0.0638|  0.9463/0.0086|  0.7868/0.0054|  `0.9146/0.0051`|  `0.6982/0.0109`|  `0.9826/0.0040`|  0.5760/0.0193|  0.7712/0.0335|  0.9675/0.0115|  `0.8672/0.0028`|  `0.5228/0.0119`|  0.8620/0.0187|  0.7133/0.0305|  0.9497/0.0181|  `0.8338/0.0406`|  0.8214/0.0279|  `0.8844/0.0048`|  0.8750/0.0304|  0.8240/0.0528|  0.4330/0.0526|  `0.6150/0.0232`|    > * The main results on these large benchmark datasets are summarized below. > * ARM-Net achieves the overall best performance. > * More results and technical details can be found in the [paper](https://dl.acm.org/doi/10.1145/3448016.3457321). > * Note that all the results are reported with a *fixed embedding size* of **10** for a fair comparison  and higher AUC can be obtained by increasing the embedding size.   ```sh E.g.  with a larger embedding size of 100  ARM-Net (single head  without ensemble with a DNN) can obtain 0.9817 AUC on Frappe with only 10 exponential neurons.  CUDA_VISIBLE_DEVICES=0 python train.py --model armnet_1h --nemb 100 --h  10 --alpha 1.7 --lr 0.001 --exp_name frappe_armnet_1h_nemb --repeat 5  AUC and Model Size of this ARM-Net of different embedding sizes are compared below.  ``` | Embedding Size | 10  | 20  | 30  | 40  |  50  | 60  | 70  | 80  | 90  |  **100**   | 110  | 120  | |:--------------:|:---:|:---:|:---:|:---:|:----:|:---:|:---:|:---:|:---:|:----------:|:---:|:---:| |      AUC       | 0.9777  | 0.9779  | 0.9801  | 0.9803  | 0.9798  | 0.9807  | 0.9808  | 0.9810  | 0.9810  | **0.9817** | 0.9811  | 0.9805  | |   Model Size   | 177K  | 262K  | 348K  | 434K  | 520K  | 606K  | 692K  | 779K  | 866K  |  **953K**  | 1.04M |  1.13M  |    <img src=""https://user-images.githubusercontent.com/14588544/139670215-77544a4b-5bec-4ede-9b58-1ac1a24ff4cd.png"" width=""660"" />    """;General;https://github.com/nusdbsystem/ARM-Net
"""<div align=""left"">   <img src=""https://insightface.ai/assets/img/custom/thumb_sdunet.png"" width=""600""/> </div>  In this module  we provide datasets and training/inference pipelines for face alignment.  Supported methods:  - [x] [SDUNets (BMVC'2018)](alignment/heatmap) - [x] [SimpleRegression](alignment/coordinate_reg)   [SDUNets](alignment/heatmap) is a heatmap based method which accepted on [BMVC](http://bmvc2018.org/contents/papers/0051.pdf).  [SimpleRegression](alignment/coordinate_reg) provides very lightweight facial landmark models with fast coordinate regression. The input of these models is loose cropped face image while the output is the direct landmark coordinates.    <div align=""left"">   <img src=""https://insightface.ai/assets/img/github/11513D05.jpg"" width=""640""/> </div>  In this module  we provide training data with annotation  network settings and loss designs for face detection training  evaluation and inference.  The supported methods are as follows:  - [x] [RetinaFace (CVPR'2020)](detection/retinaface) - [x] [SCRFD (Arxiv'2021)](detection/scrfd) - [x] [blazeface_paddle](detection/blazeface_paddle)  [RetinaFace](detection/retinaface) is a practical single-stage face detector which is accepted by [CVPR 2020](https://openaccess.thecvf.com/content_CVPR_2020/html/Deng_RetinaFace_Single-Shot_Multi-Level_Face_Localisation_in_the_Wild_CVPR_2020_paper.html). We provide training code  training dataset  pretrained models and evaluation scripts.   [SCRFD](detection/scrfd) is an efficient high accuracy face detection approach which is initialy described in [Arxiv](https://arxiv.org/abs/2105.04714). We provide an easy-to-use pipeline to train high efficiency face detectors with NAS supporting.    In this module  we provide training data  network settings and loss designs for deep face recognition.  The supported methods are as follows:  - [x] [ArcFace_mxnet (CVPR'2019)](recognition/arcface_mxnet) - [x] [ArcFace_torch (CVPR'2019)](recognition/arcface_torch) - [x] [SubCenter ArcFace (ECCV'2020)](recognition/subcenter_arcface) - [x] [PartialFC_mxnet (Arxiv'2020)](recognition/partial_fc) - [x] [PartialFC_torch (Arxiv'2020)](recognition/arcface_torch) - [x] [VPL (CVPR'2021)](recognition/vpl) - [x] [OneFlow_face](recognition/oneflow_face) - [x] [ArcFace_Paddle (CVPR'2019)](recognition/arcface_paddle)  Commonly used network backbones are included in most of the methods  such as IResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  etc..    [InsightFace](https://insightface.ai) is an open source 2D&3D deep face analysis toolbox  mainly based on PyTorch and MXNet.   Please check our [website](https://insightface.ai) for detail.  The master branch works with **PyTorch 1.6+** and/or **MXNet=1.6-1.8**  with **Python 3.x**.  InsightFace efficiently implements a rich variety of state of the art algorithms of face recognition  face detection and face alignment  which optimized for both training and deployment.   """;Computer Vision;https://github.com/deepinsight/insightface
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Osobarako/alduswarrensewell
"""MMDetection is an open source object detection toolbox based on PyTorch. It is a part of the OpenMMLab project developed by [Multimedia Laboratory  CUHK](http://mmlab.ie.cuhk.edu.hk/).  The master branch works with **PyTorch 1.3 to 1.5**. The old v1.x branch works with PyTorch 1.1 to 1.4  but v2.0 is strongly recommended for faster speed  higher performance  better design and more friendly usage.  ![demo image](resources/coco_test_12510.jpg)   """;Computer Vision;https://github.com/Gugan0905/steel-defect-detection
"""Macaw (<b>M</b>ulti-<b>a</b>ngle <b>c</b>(q)uestion <b>a</b>ns<b>w</b>ering) is a ready-to-use model capable of general  question answering  showing robustness outside the domains it was  trained on. It has been trained in ""multi-angle"" fashion  which means it can handle a flexible set of input and output ""slots"" (like question  answer  explanation) .  Macaw was built on top of [T5](https://github.com/google-research/text-to-text-transfer-transformer) and  comes in different sizes:  [macaw-11b](https://huggingface.co/allenai/macaw-11b)  [macaw-3b](https://huggingface.co/allenai/macaw-3b)   and [macaw-large](https://huggingface.co/allenai/macaw-large)  as well as an answer-focused version featured on  various leaderboards: [macaw-answer-11b](https://huggingface.co/allenai/macaw-answer-11b) (see [below](#training-data)).   """;Natural Language Processing;https://github.com/allenai/macaw
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/zoeyuchao/caffe-ssd
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/tyxr/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/tyxr/bert
"""This repository provides the official training and development dataset for [the Chinese Natural Language Inference (CNLI) share task](http://www.cips-cl.org/static/CCL2018/call-evaluation.html). We evaluate the cnli\_1.0 corpus on two baseline models.     """;Sequential;https://github.com/blcunlp/CNLI
""" This repository includes:  - Training and inferring scripts using Python and MXNet - Pretrained models of *MaskFlownet-S* and *MaskFlownet* - ROS node for optical flow prediction with MaskFlownet  Code has been tested with Python 2.7.12 and MXNet 1.5.   """;General;https://github.com/AMatusV/MaskFlownet-ROS
""" If you simply want to use it in Python  you can install the latest release using [pip](https://pypi.org/project/pip/):  `pip install -U blingfire`     Hi  we are a team at Microsoft called Bling (Beyond Language Understanding)  we help Bing be smarter. Here we wanted to share with all of you our **FI**nite State machine and **RE**gular expression manipulation library (**FIRE**). We use Fire for many linguistic operations inside Bing such as Tokenization  Multi-word expression matching  Unknown word-guessing  Stemming / Lemmatization just to mention a few.   """;Natural Language Processing;https://github.com/microsoft/BlingFire
"""Need For Speed Underground 2 is a racing game  made by EA and published in 2004 in which the player mainly competes in races against other racers. To win a race  the player has to become first in each race. In the career mode on which we will focus  a player can also earn reputation. A larger lead at the end of the race yields more reputation. This motivates us to not only become first  but also win by the largest margin possible.  The initial idea of the project is to create an AI for Need For Speed Underground 2 that is able to beat the ingame opponents on easiest difficulty in one race in the modes 'Circuit'  'Sprint'  'Street X' and / or 'Underground Racing Leauge' ('URL'). To achieve this  we will develop a neural network to determine the lead over other opponents. With the help of this information  we can reward the AI for being in front of the other racers or punish it otherwise. Hence  we will use reinforcment learning in order to train the AI. The current plan is to utilise a (deep) neural network that uses the whole window of the game and the lead over enemies as inputs for the network. These inputs will be used to determine the most optimal button presses (outputs) to gain the largest lead.  The reason behind the limitation of racing modes is that here  the leads are constantly displayed in seconds. Moreover  they are easier to handle since in other ones  there are additional 'rules'. In the case of drag races  the player has to shift gears manually and is only informed on its over all position on the leaderboard. In addition  there is no information about leads available on the screen. For drift races  the goal is to perform drifts to obtain points. Here  the leads are displayed in points. We observe that the optimal way of driving in this mode is different compared to all other modes since the focus lies on drifting.   The goal of this project is to let a neural network drive cars and win races in the racing game Need for Speed: Underground 2. In order to achieve this task  we will use reinforcment learning in combination with neural networks. This project is inspired by the [work of the DeepMind team using the raw pixels to train a deep neural network to play Atari games](http://arxiv.org/pdf/1312.5602v1.pdf). In addition  at least the steering and the retrieval of the pixels will be done in a similar fashion as it was done by the YouTuber [sentdex](https://www.youtube.com/user/sentdex) (see also [here](https://pythonprogramming.net/next-steps-python-plays-gta-v/)).  <a name=""introduction""></a>  """;Reinforcement Learning;https://github.com/tlohr/nfsu2-ai
"""```You can run the model and the harness around it using: python main.py  Run evaluate over your test set python main.py --mode=evaluate  Configuration: config.py   config.TRAIN.batch_size = 8 #:Training batch size  config.TRAIN.lr_init = 1e-4 #:Initial Learning Rate  config.TRAIN.beta1 = 0.9 #:Beta1 parameter for batch normalization  config.TRAIN.n_epoch_init = 35 #:Number of epochs to run the generator before adversarial training  config.TRAIN.n_epoch = 56 #:Number of Epochs of Adversarial training config.TRAIN.lr_decay = 0.1 #:Learning rate decay through adversarial training config.TRAIN.decay_every = int(config.TRAIN.n_epoch / 2)   config.TRAIN.hr_img_path = '../train_data_out_2' config.TRAIN.lr_img_path = '../train_data_in'  config.VALID.hr_img_path = '../test_data_out_2/' config.VALID.lr_img_path = '../test_data_in/' ```  TRAIN.hr_img_path is the groundtruth path and TRAIN.lr_img_path is the input image path. In our case these are 128x128 slices of input image and binary masks.   """;Computer Vision;https://github.com/ankit-ai/GAN_breast_mammography_segmentation
"""The codes are [PyTorch](https://pytorch.org/) re-implement version for paper: SqueezeNext: Hardware-Aware Neural Network Design. (SqueezeNext)  > Gholami A  Kwon K  Wu B  et al. SqueezeNext: Hardware-Aware Neural Network Design[J]. 2018. [arXiv:1803.10615v1](https://arxiv.org/abs/1803.10615v1)  We implement this work from [amirgholami/SqueezeNext](https://github.com/amirgholami/SqueezeNext).   """;Computer Vision;https://github.com/luuuyi/SqueezeNext.PyTorch
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/Kev1nZheng/yolov_mask
"""[ImageNet](http://www.image-net.org/) and Alex Krizhevsky's [""AlexNet""](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks) sparked a revolution in machine learning. AlexNet marked the end of the era of mostly hand-crafted features for visual recognition problems. In just the few years that followed AlexNet  ""deep learning"" found great success in natural language processing  speech recognition  and reinforcement learning.  Any aspiring machine learning engineer should construct and train a deep convnet ""from scratch.""  Of course  there are varying degrees of ""from scratch."" I had already implemented many of the neural network primitives using NumPy (e.g. fully connected layers  cross-entropy loss  batch normalization  LSTM / GRU cells  and convolutional layers). So  here I use TensorFlow so the focus is on training a deep network on a large dataset.  Amazingly  with only 2 hours of GPU time (about $0.50 using an Amazon EC2 spot instance)  it was not difficult to reach 50% top-1 accuracy and almost 80% top-5 accuracy. At this accuracy  I was also making mistakes on the images that the model got wrong (and I even made mistakes on some that it got correct).   """;Computer Vision;https://github.com/pat-coady/tiny_imagenet
"""|                   | Description                                                                            | |-------------------|----------------------------------------------------------------------------------------| | Model             | Very Deep Convolutional Networks with 3x3 kernel [1] | | Data Augmentation | cropping  horizontal reflection [2] and scaling. see lib/data_augmentation.lua             | | Preprocessing     | Global Contrast Normalization (GCN) and ZCA whitening. see lib/preprocessing.lua       | | Training Time     | 20 hours on GTX760. | | Prediction Time   | 2.5 hours on GTX760. | | Result            | 0.93320 (single model). 0.94150 (average 6 models)|    """;Computer Vision;https://github.com/nagadomi/kaggle-cifar10-torch7
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/MaZhiyuanBUAA/bert-tf1.4.0
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/MaZhiyuanBUAA/bert-tf1.4.0
"""**leelaApplication** is a free open-source Android GO board game AI  experiment program.      It supports [Leela Zero](https://github.com/gcp/leela-zero)  can do some test easily.    It will create a SO lib  this lib can be used in [BlueGoAI](https://github.com/John-Yu/BluGo_Android).    You can download BlueGoAI source code from  [GITHUB](https://github.com/John-Yu/BluGo_Android)      """;Reinforcement Learning;https://github.com/John-Yu/leelaApplication
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/nmfisher/bert-modified
"""The *Show and Tell* model is a deep neural network that learns how to describe the content of images. For example:  ![Example captions](g3doc/example_captions.jpg)   """;Computer Vision;https://github.com/puxinhe/im2txt_v3
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO** (https://pjreddie.com/darknet/yolo/) and to **Erik Lindernoren for the PyTorch implementation** this work is based on (https://github.com/eriklindernoren/PyTorch-YOLOv3).   This directory contains software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information on Ultralytics projects please visit: https://www.ultralytics.com.   """;Computer Vision;https://github.com/BlackAngel1111/Yolo3_Pytorch
"""- Fast AutoAugment (hereafter FAA) finds the optimal set of data augmentation operations via density matching using Bayesian optimization. - FAA delivers comparable performance to <a href=""https://arxiv.org/abs/1805.09501"">AutoAugment</a> but in a much shorter period of time. - Unlike AutoAugment that discretizes the search space  FAA can handle continuous search space directly.  <br>     """;Computer Vision;https://github.com/junkwhinger/fastautoaugment_jsh
"""For this project  you will train an agent to navigate (and collect bananas!) in a large  square world.    ![Trained Agent][image1]  A reward of +1 is provided for collecting a yellow banana  and a reward of -1 is provided for collecting a blue banana.  Thus  the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.    The state space has 37 dimensions and contains the agent's velocity  along with ray-based perception of objects around agent's forward direction.  Given this information  the agent has to learn how to best select actions.  Four discrete actions are available  corresponding to: - **`0`** - move forward. - **`1`** - move backward. - **`2`** - turn left. - **`3`** - turn right.  The task is episodic  and in order to solve the environment  your agent must get an average score of +13 over 100 consecutive episodes.   """;Reinforcement Learning;https://github.com/JayLohokare/banana-world-navigation
"""Spriteworld is a python-based RL environment that consists of a 2-dimensional arena with simple shapes that can be moved freely. This environment was developed for the COBRA agent introduced in the paper [""COBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration"" (Watters et al.  2019)](https://arxiv.org/abs/1905.09275). The motivation for the environment was to provide as much flexibility for procedurally generating multi-object scenes while retaining as simple an interface as possible.  Spriteworld sprites come in a variety of shapes and can vary continuously in position  size  color  angle  and velocity. The environment has occlusion but no physics  so by default sprites pass beneath each other but do not collide or interact in any way. Interactions may be introduced through the action space  which can update all sprites each timestep. For example  the DiscreteEmbodied action space (see `spriteworld/action_spaces.py`) implements a rudimentary form of physics in which an agent's body sprite can adhere to and carry sprites underneath it.  There are a variety of action spaces  some of which are continuous (like a touch-screen) and others of which are discrete (like an embodied agent that takes discrete steps).   """;Computer Vision;https://github.com/deepmind/spriteworld
"""[![Watch the video](figures/video_figure.png)](https://www.youtube.com/watch?v=a_OeT8MXzWI&feature=youtu.be)   """;Computer Vision;https://github.com/mit-han-lab/once-for-all
"""[![Watch the video](figures/video_figure.png)](https://www.youtube.com/watch?v=a_OeT8MXzWI&feature=youtu.be)   """;General;https://github.com/mit-han-lab/once-for-all
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/flyliu2017/bert_modularized
"""This repo contains the code to train and evaluate FC-DenseNets as described in [The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/abs/1611.09326). We investigate the use of [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993) for semantic segmentation  and report state of the art results on datasets such as CamVid.   """;General;https://github.com/SimJeg/FC-DenseNet
"""This repo contains the code to train and evaluate FC-DenseNets as described in [The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/abs/1611.09326). We investigate the use of [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993) for semantic segmentation  and report state of the art results on datasets such as CamVid.   """;Computer Vision;https://github.com/SimJeg/FC-DenseNet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/vijay120/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/vijay120/bert
"""This is a Caffe implementation of Google's MobileNets (v1 and v2). For details  please read the following papers: - [v1] [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) - [v2] [Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification  Detection and Segmentation](https://arxiv.org/abs/1801.04381)    """;Computer Vision;https://github.com/shicai/MobileNet-Caffe
"""This is a Caffe implementation of Google's MobileNets (v1 and v2). For details  please read the following papers: - [v1] [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) - [v2] [Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification  Detection and Segmentation](https://arxiv.org/abs/1801.04381)    """;General;https://github.com/shicai/MobileNet-Caffe
"""GeDi is a method of using class-conditional language models (which we refer to as generative discriminators (GeDis)) to guide generation from other (potentially much larger) language models. This has several advantages over finetuning large language models directly including:  * significantly less training computation. * maintaining the diversity of the original language model (If we finetune a large pretrained language model to a specific attribute dataset  we will likely reduce the broad generation capabilities of the model). * teaching the language model what not to generate. This is especially useful for applications like detoxification.   GeDi is a form of discriminator guided generation. A discriminator that can classify an attribute could be used to guide language model generation towards that attribute by classifying the sequences that result from candidate next tokens. However  using a normal discriminator (such as [BERT](https://arxiv.org/abs/1810.04805)) to do this would be very computationally expensive during generation  since it would require feeding in every candidate next token one-by-one to the discriminator to be classified. However  using generative discriminators  we can very efficiently classify candidate next tokens during generation using Bayes rule (see Section 3.1 of the paper). As an added bonus  [generative discriminators can be used as zero shot classifiers](https://arxiv.org/abs/1703.01898)  and can therefore be used to guide generation towards unseen topics.    """;Natural Language Processing;https://github.com/salesforce/GeDi
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/flyliu2017/bert_modularized
"""NEMATODE is a light-weight neural machine translation toolkit built around the [transformer](https://arxiv.org/pdf/1706.03762.pdf) model. As the name suggests  it was originally derived from the [Nematus](https://github.com/EdinburghNLP/nematus) toolkit and eventually deviated from Nematus into a stand-alone project  by adopting the transformer model and a custom data serving pipeline. Many of its components (most notably the transformer implementation) were subsequently merged into Nematus.   """;General;https://github.com/demelin/nematode
"""NEMATODE is a light-weight neural machine translation toolkit built around the [transformer](https://arxiv.org/pdf/1706.03762.pdf) model. As the name suggests  it was originally derived from the [Nematus](https://github.com/EdinburghNLP/nematus) toolkit and eventually deviated from Nematus into a stand-alone project  by adopting the transformer model and a custom data serving pipeline. Many of its components (most notably the transformer implementation) were subsequently merged into Nematus.   """;Natural Language Processing;https://github.com/demelin/nematode
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/deepset-ai/bert-tensorflow
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/deepset-ai/bert-tensorflow
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/YuanEric88/bert-nlu
"""Pytorch implementation for ""NAT: Neural Architecture Transformer for Accurate and Compact Architectures"".   """;General;https://github.com/guoyongcs/NAT
"""This is the official code of [High-Resolution Representations for Facial Landmark Detection](https://arxiv.org/pdf/1904.04514.pdf).  We extend the high-resolution representation (HRNet) [1] by augmenting the high-resolution representation by aggregating the (upsampled)  representations from all the parallel convolutions  leading to stronger representations. The output representations are fed into classifier. We evaluate our methods on four datasets  COFW  AFLW  WFLW and 300W.  <div align=center>  ![](images/hrnet.jpg)  </div>   """;Computer Vision;https://github.com/HRNet/HRNet-Facial-Landmark-Detection
"""For calculating the Q Values we used a Neural Network  rather than just showing the current state to the network  the next 4 possible states(left  right  up  down) were also shown. This intuition was inspired from Monte Carlo Tree Search estimation where game is played till the end to determine the Q-Values. Instead of using a normal Q Network  a Double Q Network was used one for predicting Q values and other for predicting actions. This is done to try and reduce the large overestimations of action values which result form a positive bias introduced in Q Learning. For data preprocessing log2 normalisation  training was done using the Bellman's Equation. The policy used was Epsilon greedy  to allow exploration the value of epsilon was annealed down by 5%.    """;Reinforcement Learning;https://github.com/dsgiitr/rl_2048
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/YuanEric88/bert-nlu
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/paolanu/BERT_epitope
"""FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search(https://arxiv.org/abs/1812.03443)  The master branch works with (https://github.com/open-mmlab/mmdetection)edb03937964b583a59dd1bddf76eaba82df9e8c0  - **test_block_time**  python  tools/test_time.py configs/search_config/bottlenetck_kernel.py   - **fbnet_search**  ./search.sh configs/model_config/retinanet_fpn.py configs/search_config/bottlenetck_kernel.py speed/bottlenetck_kernel.py_speed_gpu.txt  - **fbnet_train** (according to search_result)  ./train.sh configs/model_config/retinanet_fpn.py configs/search_config/bottlenetck_kernel.py theta/bottlenetck_kernel_retinanet_fpn/base.txt --work_dir ./your_path_tosave  - **fbnet_test** (according to train_result)  ./test.sh configs/model_config/retinanet_fpn.py configs/search_config/bottlenetck_kernel.py theta/bottlenetck_kernel_retinanet_fpn/base.txt your_path_tosave/lastest.pth --show  """;General;https://github.com/anorthman/custom
"""FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search(https://arxiv.org/abs/1812.03443)  The master branch works with (https://github.com/open-mmlab/mmdetection)edb03937964b583a59dd1bddf76eaba82df9e8c0  - **test_block_time**  python  tools/test_time.py configs/search_config/bottlenetck_kernel.py   - **fbnet_search**  ./search.sh configs/model_config/retinanet_fpn.py configs/search_config/bottlenetck_kernel.py speed/bottlenetck_kernel.py_speed_gpu.txt  - **fbnet_train** (according to search_result)  ./train.sh configs/model_config/retinanet_fpn.py configs/search_config/bottlenetck_kernel.py theta/bottlenetck_kernel_retinanet_fpn/base.txt --work_dir ./your_path_tosave  - **fbnet_test** (according to train_result)  ./test.sh configs/model_config/retinanet_fpn.py configs/search_config/bottlenetck_kernel.py theta/bottlenetck_kernel_retinanet_fpn/base.txt your_path_tosave/lastest.pth --show  """;Computer Vision;https://github.com/anorthman/custom
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/joytianya/google_bert
"""  MegaFace dataset includes 1 027 060 faces  690 572 identities. [Link](http://megaface.cs.washington.edu/)   Challenge 1 is taken to test our model with 1 million distractors.   ![image](https://github.com/foamliu/InsightFace-v2/raw/master/images/megaface_stats.png)    Use Labeled Faces in the Wild (LFW) dataset for performance evaluation:  - 13233 faces - 5749 identities - 1680 identities with >=2 photo   MS-Celeb-1M dataset for training  3 804 846 faces over 85 164 identities.    """;General;https://github.com/nhanvu39/ArcFace
"""FoveaBox is an accurate  flexible and completely anchor-free object detection system for object detection framework  as presented in our paper [https://arxiv.org/abs/1904.03797](https://arxiv.org/abs/1904.03797): Different from previous anchor-based methods  FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility  and (b) producing category-agnostic bounding box for each position that potentially contains an object.  <div align=""center"">   <img src=""demo/foveabox.jpg"" width=""300px"" />   <p>FoveaBox detection process.</p> </div>    """;Computer Vision;https://github.com/taokong/FoveaBox
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/nmfisher/bert-modified
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/YuxuanXie/caffe_mobilessd
"""* [About mini-ImageNet](#about-mini-ImageNet) * [Requirements](#requirements) * [Installation](#installation) * [Usage](#usage) * [Performance](#performance) * [Acknowledgement](#acknowledgement)   """;General;https://github.com/yaoyao-liu/mini-imagenet-tools
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/1wy/bert
"""This repository is build for the proposed self-attention network (SAN)  which contains full training and testing code. The implementation of SA module with optimized CUDA kernels are also included.  <p align=""center""><img src=""./figure/sa.jpg"" width=""400""/></p>   """;Computer Vision;https://github.com/hszhao/SAN
"""**RepPoints**  initially described in [arXiv](https://arxiv.org/abs/1904.11490)  is a new representation method for visual objects  on which visual understanding tasks are typically centered. Visual object representation  aiming at both geometric description and appearance feature extraction  is conventionally achieved by `bounding box + RoIPool (RoIAlign)`. The bounding box representation is convenient to use; however  it provides only a rectangular localization of objects that lacks geometric precision and may consequently degrade feature quality. Our new representation  RepPoints  models objects by a `point set` instead of a `bounding box`  which learns to adaptively position themselves over an object in a manner that circumscribes the object’s `spatial extent` and enables `semantically aligned feature extraction`. This richer and more flexible representation maintains the convenience of bounding boxes while facilitating various visual understanding applications. This repo demonstrated the effectiveness of RepPoints for COCO object detection.  Another feature of this repo is the demonstration of an `anchor-free detector`  which can be as effective as state-of-the-art anchor-based detection methods. The anchor-free detector can utilize either `bounding box` or `RepPoints` as the basic object representation.  <div align=""center"">   <img src=""demo/reppoints.png"" width=""400px"" />   <p>Learning RepPoints in Object Detection.</p> </div>   """;Computer Vision;https://github.com/microsoft/RepPoints
"""A **PyTorch** implementation for our **AAAI 2020** paper **[""Adversarial-Learned Loss for Domain Adaptation"" (ALDA)](https://arxiv.org/abs/2001.01046)**.  In ALDA  we use a domain discriminator to correct the noise in the pseudo-label. ALDA outperforms state-of-the-art approaches in four standard unsupervised domain adaptation datasets.  ![pic1](./pics/pic0.png)   """;Computer Vision;https://github.com/ZJULearning/ALDA
"""This code contains two versions of the network architectures and hyper-parameters. The first one is based on the [TensorFlow implementation](https://github.com/hardikbansal/CycleGAN). The second one is based on the [official PyTorch implementation](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix). The differences are minor and we observed both versions produced good results. You may need to train several times as the quality of the results are sensitive to the initialization.    Below is a snapshot of our result at the 50th epoch on one training instance:  <img src='imgs/horse2zebra.png' width=""900px""/>   """;Computer Vision;https://github.com/leehomyc/cyclegan-1
"""This code contains two versions of the network architectures and hyper-parameters. The first one is based on the [TensorFlow implementation](https://github.com/hardikbansal/CycleGAN). The second one is based on the [official PyTorch implementation](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix). The differences are minor and we observed both versions produced good results. You may need to train several times as the quality of the results are sensitive to the initialization.    Below is a snapshot of our result at the 50th epoch on one training instance:  <img src='imgs/horse2zebra.png' width=""900px""/>   """;General;https://github.com/leehomyc/cyclegan-1
"""In this module  we provide training data  network settings and loss designs for deep face recognition. The training data includes  but not limited to the cleaned MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  etc.. The loss functions include Softmax  SphereFace  CosineFace  ArcFace  Sub-Center ArcFace and Triplet (Euclidean/Angular) Loss.  You can check the detail page of our work [ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/ArcFace)(which accepted in CVPR-2019) and [SubCenter-ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/SubCenter-ArcFace)(which accepted in ECCV-2020).  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this module  you can simply achieve LFW 99.83%+ and Megaface 98%+ by a single model. This module can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   InsightFace is an open source 2D&3D deep face analysis toolbox  mainly based on MXNet.   The master branch works with **MXNet 1.2 to 1.6**  with **Python 3.x**.     """;General;https://github.com/hqhoangvuong/paperspace_ml_01
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/Hiwyl/yolov3-attentation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Chenrj233/bert_pratice
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Chenrj233/bert_pratice
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Arthurizijar/Bert_Airport
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Arthurizijar/Bert_Airport
"""InfoGAN is an information-theoretic extension to the simple Generative Adversarial Networks that is able to learn disentangled representations in a completely unsupervised manner. What this means is that InfoGAN successfully disentangle wrirting styles from digit shapes on th MNIST dataset and discover visual concepts such as hair styles and gender on the CelebA dataset. To achieve this an information-theoretic regularization is added to the loss function that enforces the maximization of mutual information between latent codes  c  and the generator distribution G(z  c).   """;Computer Vision;https://github.com/vinoth654321/Casia-Webface
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/paolanu/BERT_epitope
"""Tensorflow object detection is very good  easy and free framework for object detection tasks.  It contains varous models trained on COCO  Kitty like large dataset and we use these pretrained models on our custom datset.    """;Computer Vision;https://github.com/vandangorade/BrandLOGO_detection
"""HAR-Web is a web application that can be utilized to carry out the task of Human Activity Recognition in real time on the web using GPU-enabled devices. The web application is based on a micro service architecture where the project has been divided into 4 basic services  each running on a different port and deployed using Docker containers. I would like to add support for Kubernetes but as of now it doesn't support native hardware access like Docker-Swarm. One way of solving this would be to write a host device plugin like https://github.com/honkiko/k8s-hostdev-plugin but specifically for webcam access. If anyone has an idea on how to enable local webcam access on K8S  I would love to hear about it.     """;General;https://github.com/ChetanTayal138/HAR-Web
"""Created model is working correctly  suprisingly well taking into acount the fact that it was trained only on computer with only one graphics processing unit. Analyzing image results one can conclude that model has tendency to leave image with faded colors. Another problem is leaving small object not colored. Possible solutions to that drawbacks could be to train model on computer with more powerfull GPUs  like Amazon EC2 p3.16xlarge. Furthermore good option could be addition of class rebalancing basing on probability like in [2] paper.     """;General;https://github.com/bluejurand/Photos-colorization
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/NovasMax/yolov3
""" BART model [https://arxiv.org/pdf/1910.13461.pdf](https://arxiv.org/pdf/1910.13461.pdf)  Fairseq [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)  Fairseq tutorial on fine-tuning BART on Seq2Seq task [https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md](https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md)  COVID Dialogue Dataset [https://github.com/UCSD-AI4H/COVID-Dialogue](https://github.com/UCSD-AI4H/COVID-Dialogue)   """;Sequential;https://github.com/huangxt39/BART_on_COVID_dialogue
""" This is the **ranked No.1** tensorflow based scene text spotting algorithm on [__ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text__](https://rrc.cvc.uab.es/?ch=14) (Latin Only  Latin and Chinese)  futhermore  the algorithm is also adopted in [__ICDAR2019 Robust Reading Challenge on Large-scale Street View Text with Partial Labeling__](https://rrc.cvc.uab.es/?ch=16) and [__ICDAR2019 Robust Reading Challenge on Reading Chinese Text on Signboard__](https://rrc.cvc.uab.es/?ch=12).   Scene text detection algorithm is modified from [__Tensorpack FasterRCNN__](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN)  and we only open source code in this repository for scene text recognition. I upload ICDAR2019 ArT competition model to docker hub  please refer to [Docker](#Docker). For more details  please refer to our [__arXiv technical report__](https://arxiv.org/abs/1912.04561).  Our text recognition algorithm not only recognizes Latin and Non-Latin characters  but also supports horizontal and vertical text recognition in one model. It is convenient for multi-lingual arbitrary-shaped text recognition.  **Note that the competition model in docker container as described in [__our technical report__](https://arxiv.org/abs/1912.04561) is slightly different from the recognition model trained from this updated repository.**   """;General;https://github.com/smisthzhu/attentionocr
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/1wy/bert
"""音声生成は、音楽制作や映画のSE音で実用的であると考えられています。   映画などの制作に携わっている音響監督の方達は、作品内で使用するSE音を選ぶ時、多数ある音の中からその場面に合う一つを見つけなければなりません。とても面倒臭い作業です。   そこで、音声生成があるとSE音を探したい場面の情報をインプットしただけで適した音を生成してくれるとその作業が楽になるのではないかと考えられます。   <img width=""625"" alt=""how_to_use_voice_synthesis"" src=""https://user-images.githubusercontent.com/39772824/71435632-22423f80-272d-11ea-985f-6a55735da5d9.png"">   従来の音声生成には自己回帰トレーニングによるニューラルネットワークモデルがあげられるが、これは出力が出るたびにフィードバックをしなければならないので、とても時間がかかる方法です。   画像生成で使われているGANを音声生成で使用するには、スペクトログラムに変換して画像として扱うと簡単になると考えられます。    この論文では、2種類のGANの提案をしています。  --- 1つ目はSpecGANと呼ばれるもので、これは入力のオーディオデータをスペクトログラムに直して扱うモデルです。   <img width=""776"" alt=""SpecGAN"" src=""https://user-images.githubusercontent.com/39772824/71434596-962e1900-2728-11ea-9a69-93b3c72d03e9.png"">    --- 2つ目はWaveGANと呼ばれるもので、これは画像生成に使われているDCGANを音声生成に対応するように作り替えたものです。入力データを別の形に変換せずにそのまま使えるのが特徴です。   <img width=""555"" alt=""WaveGAN"" src=""https://user-images.githubusercontent.com/39772824/71434590-91696500-2728-11ea-9958-3d52cec1892f.png"">     """;Audio;https://github.com/mahotani/ADVERSARIAL-AUDIO-SYNTHESIS
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/Atom-min/MLBigWork
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/AlvesRobot/Deep-Learning-on-Point-Cloud-for-3D-Classification-and-Segmentation
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/TangDL/DCN
"""CenterNet is a framework for object detection with deep convolutional neural networks. You can use the code to train and evaluate a network for object detection on the MS-COCO dataset.  * It achieves state-of-the-art performance (an AP of 47.0%) on one of the most challenging dataset: MS-COCO.  * Our code is written in Python  based on [CornerNet](https://github.com/princeton-vl/CornerNet).  *More detailed descriptions of our approach and code will be made available soon.*  **If you encounter any problems in using our code  please contact Kaiwen Duan: kaiwen.duan@vipl.ict.ac.cn.**   """;Computer Vision;https://github.com/DaiJianBo/CenterNet-duan-2080Ti
"""English | [简体中文](README_zh-CN.md)  The master branch works with **PyTorch 1.3+**.  MMDetection3D is an open source object detection toolbox based on PyTorch  towards the next-generation platform for general 3D detection. It is a part of the OpenMMLab project developed by [MMLab](http://mmlab.ie.cuhk.edu.hk/).  ![demo image](resources/mmdet3d_outdoor_demo.gif)   """;Computer Vision;https://github.com/open-mmlab/mmdetection3d
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/krantirk/BERT-Pretrained-model
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/krantirk/BERT-Pretrained-model
"""Torch implementation of the deep relational architecture from the paper [""Relational Deep Reinforcement Learning""](https://arxiv.org/pdf/1806.01830.pdf) together with (synchronous) advantage-actor-critic training as discussed for example [here](https://arxiv.org/abs/1602.01783).  The Box-World environment used in this script can be found at [this repo](https://github.com/mavischer/Box-World).  Training is performed in `a2c_fast.py`. The implementation is based on [this repo](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail) which turned out to be more clever and substantially faster than my own implementation `a2c_dist.py`. However this latter file contains routines to plot the gradients in the network and the computation graph.  The relational module and general architecture are both implemented as `torch.nn.Module` in `attention_module.py`. However  `a2c_fast.py` uses almost identical adaptations of these classes in `helper/a2c_ppo_acktr/model.yml` that comply with the training algorithm's `Policy` class.  An example YAML config file parsed from the arguments is `configs/exmpl_config.yml`. Training  the environment and network can be parameterized there. A copy of the loaded configuration file will be saved with checkpoints and logs for documentation.  A suitable environment can be created e.g. by  `conda env create -f environment.yml` or   `pip install -r requirements.txt`. Afterwards install and register the [Box-World environment](https://github.com/mavischer/Box-World) by cloning the repo and `pip install -e gym-boxworld`. *Remember that after changing the code you need to re-register the environment before the changes become effective.* You can find the details of state space  action space and reward structure there.  `visualize_results.ipynb` contains some plotting functionality.   """;Reinforcement Learning;https://github.com/mavischer/DRRL
"""Torch implementation of the deep relational architecture from the paper [""Relational Deep Reinforcement Learning""](https://arxiv.org/pdf/1806.01830.pdf) together with (synchronous) advantage-actor-critic training as discussed for example [here](https://arxiv.org/abs/1602.01783).  The Box-World environment used in this script can be found at [this repo](https://github.com/mavischer/Box-World).  Training is performed in `a2c_fast.py`. The implementation is based on [this repo](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail) which turned out to be more clever and substantially faster than my own implementation `a2c_dist.py`. However this latter file contains routines to plot the gradients in the network and the computation graph.  The relational module and general architecture are both implemented as `torch.nn.Module` in `attention_module.py`. However  `a2c_fast.py` uses almost identical adaptations of these classes in `helper/a2c_ppo_acktr/model.yml` that comply with the training algorithm's `Policy` class.  An example YAML config file parsed from the arguments is `configs/exmpl_config.yml`. Training  the environment and network can be parameterized there. A copy of the loaded configuration file will be saved with checkpoints and logs for documentation.  A suitable environment can be created e.g. by  `conda env create -f environment.yml` or   `pip install -r requirements.txt`. Afterwards install and register the [Box-World environment](https://github.com/mavischer/Box-World) by cloning the repo and `pip install -e gym-boxworld`. *Remember that after changing the code you need to re-register the environment before the changes become effective.* You can find the details of state space  action space and reward structure there.  `visualize_results.ipynb` contains some plotting functionality.   """;General;https://github.com/mavischer/DRRL
"""This codebase is to reproduce the results in the report submitted to APAC HPCAI 2020. We optimize the distributed performance by using gradient checkpointing. The baseline is run on 1 V100 GPU on NSCC DGX and the optimized code is run on 2 nodes with 4 GPUs on each node on NSCC DGX. The optimized code can achieve more than 8 times throughput compared to baseline experiment.    """;General;https://github.com/FrankLeeeee/HPCAI-2020-BERT-Submission
"""TabNet should **not act as a replacement to boosted methods** for typical data science use-cases. However  it may provide improved performance in use-cases where the labelled dataset is large (e.g.  millions of examples)  or in situations where only an unsupervised model is needed (e.g.  fraud detection).  - **Performance:** While the paper demonstrates promising results  my TabNet implemention underperformed XGBoost in `adult_census` and only slightly outperformed XGBoost in `forest_census` (likely driven by the larger size of the dataset). These results are produced without hyperparamater tuning. - **Training time:** The training time for TabNet models is considerably higher than the XGBoost counterpart on CPU  though this difference is lower on GPU. As such  TabNet should only be considered when plenty of samples are available (e.g.  as with `Forest Cover`) - **Interpretability:** Aside from (i) visualising the embedding space of categorical features  and (ii) providing some intuition on which features the model was attending to while predicting on a text example  the vanilla TabNet model does not provide much additional interpretability over the importance plots already available in XGBoost.     """;General;https://github.com/96imranahmed/TabNet
"""The solution is a web-service.  Users interact with it via a standard web browser on a smartphone or a desktop computer. Results are displayed on the screen as images and text and can be sent to the user's E-mail.  This solution can also be installed as a standalone program on a personal computer and can be used through a command-line interface.  Video presentation: https://youtu.be/_vcvxPtAzOM     This service is available at the address: http://angelina-reader.ru       """;Computer Vision;https://github.com/IlyaOvodov/AngelinaReader
"""The solution is a web-service.  Users interact with it via a standard web browser on a smartphone or a desktop computer. Results are displayed on the screen as images and text and can be sent to the user's E-mail.  This solution can also be installed as a standalone program on a personal computer and can be used through a command-line interface.  Video presentation: https://youtu.be/_vcvxPtAzOM     This service is available at the address: http://angelina-reader.ru       """;General;https://github.com/IlyaOvodov/AngelinaReader
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/SpikeKing/My-Bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/SpikeKing/My-Bert
"""Our UGC data include comments posted on news articles collected from 3 major Israeli news sites  between January 2020 to August 2020. The total size of the data is ~150 MB  including over 7 millions words and 350K sentences. ~2000 sentences were annotated by crowd members (3-10 annotators per sentence) for overall sentiment (polarity) and [eight emotions](https://en.wikipedia.org/wiki/Robert_Plutchik#Plutchik's_wheel_of_emotions): anger  disgust  expectation   fear  happy  sadness  surprise and trust.  The percentage of sentences in which each emotion appeared is found in the table below.  |       | anger | disgust | expectation | fear | happy | sadness | surprise | trust | sentiment | |------:|------:|--------:|------------:|-----:|------:|--------:|---------:|------:|-----------| | **ratio** |  0.78 |    0.83 |        0.58 | 0.45 |  0.12 |    0.59 |     0.17 |  0.11 | 0.25      |   """;Natural Language Processing;https://github.com/avichaychriqui/HeBERT
"""We built our approach on [FCOS](https://arxiv.org/abs/1904.01355)  A simple and strong anchor-free object detector  with [ResNeSt](https://arxiv.org/abs/2004.08955) as our backbone  to detect embedded and isolated formulas.  We employed [ATSS](https://arxiv.org/abs/1912.02424) as our sampling strategy instead of random sampling to eliminate the effects of sample imbalance. Moreover  we observed and revealed the influence of different FPN levels on the detection result.  [Generalized Focal Loss](https://arxiv.org/abs/2006.04388) is adopted to our loss. Finally  with a series of useful tricks and model ensembles  our method was ranked 1st in the MFD task.  ![Random Sampling(left) ATSS(right)](https://github.com/Yuxiang1995/ICDAR2021_MFD/blob/main/resources/sampling_strategy.png) **Random Sampling(left) ATSS(right)**    """;Computer Vision;https://github.com/Yuxiang1995/ICDAR2021_MFD
"""We built our approach on [FCOS](https://arxiv.org/abs/1904.01355)  A simple and strong anchor-free object detector  with [ResNeSt](https://arxiv.org/abs/2004.08955) as our backbone  to detect embedded and isolated formulas.  We employed [ATSS](https://arxiv.org/abs/1912.02424) as our sampling strategy instead of random sampling to eliminate the effects of sample imbalance. Moreover  we observed and revealed the influence of different FPN levels on the detection result.  [Generalized Focal Loss](https://arxiv.org/abs/2006.04388) is adopted to our loss. Finally  with a series of useful tricks and model ensembles  our method was ranked 1st in the MFD task.  ![Random Sampling(left) ATSS(right)](https://github.com/Yuxiang1995/ICDAR2021_MFD/blob/main/resources/sampling_strategy.png) **Random Sampling(left) ATSS(right)**    """;General;https://github.com/Yuxiang1995/ICDAR2021_MFD
"""The implementation is based on two papers & Github Repository: - Object Tracking(https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch) - Simple Online and Realtime Tracking with a Deep Association Metric https://arxiv.org/abs/1703.07402 - YOLOv4: Optimal Speed and Accuracy of Object Detection https://arxiv.org/pdf/2004.10934.pdf   """;Computer Vision;https://github.com/JisuHann/Object-Tracking
"""Distribuuuu is a Distributed Classification Training Framework powered by native PyTorch.  Please check [tutorial](./tutorial/) for detailed **Distributed Training** tutorials:  - Single Node Single GPU Card Training [[snsc.py](./tutorial/snsc.py)] - Single Node Multi-GPU Cards Training (with DataParallel) [[snmc_dp.py](./tutorial/snmc_dp.py)] - Multiple Nodes Multi-GPU Cards Training (with DistributedDataParallel)     - torch.distributed.launch [[mnmc_ddp_launch.py](./tutorial/mnmc_ddp_launch.py)]     - torch.multiprocessing [[mnmc_ddp_mp.py](./tutorial/mnmc_ddp_mp.py)]     - Slurm Workload Manager [[mnmc_ddp_slurm.py](./tutorial/mnmc_ddp_slurm.py)] - ImageNet training example [[imagenet.py](./tutorial/imagenet.py)]  For the complete training framework  please see [distribuuuu](./distribuuuu/).    """;General;https://github.com/BIGBALLON/distribuuuu
"""Distribuuuu is a Distributed Classification Training Framework powered by native PyTorch.  Please check [tutorial](./tutorial/) for detailed **Distributed Training** tutorials:  - Single Node Single GPU Card Training [[snsc.py](./tutorial/snsc.py)] - Single Node Multi-GPU Cards Training (with DataParallel) [[snmc_dp.py](./tutorial/snmc_dp.py)] - Multiple Nodes Multi-GPU Cards Training (with DistributedDataParallel)     - torch.distributed.launch [[mnmc_ddp_launch.py](./tutorial/mnmc_ddp_launch.py)]     - torch.multiprocessing [[mnmc_ddp_mp.py](./tutorial/mnmc_ddp_mp.py)]     - Slurm Workload Manager [[mnmc_ddp_slurm.py](./tutorial/mnmc_ddp_slurm.py)] - ImageNet training example [[imagenet.py](./tutorial/imagenet.py)]  For the complete training framework  please see [distribuuuu](./distribuuuu/).    """;Computer Vision;https://github.com/BIGBALLON/distribuuuu
"""Existing video polyp segmentation (VPS) models typically employ convolutional neural networks (CNNs) to extract features.  However  due to their limited receptive fields  CNNs can not fully exploit the global temporal and spatial information in successive video frames  resulting in false-positive segmentation results.  In this paper  we propose the novel PNS-Net (Progressively Normalized Self-attention Network)  which can efficiently learn representations from polyp videos with real-time speed (~140fps) on a single RTX 2080 GPU and no post-processing.   Our PNS-Net is based solely on a basic normalized self-attention block  dispensing with recurrence and CNNs entirely. Experiments on challenging VPS datasets demonstrate that the proposed PNS-Net achieves state-of-the-art performance.  We also conduct extensive experiments to study the effectiveness of the channel split  soft-attention  and progressive learning strategy.  We find that our PNS-Net works well under different settings  making it a promising solution to the VPS task.   """;Computer Vision;https://github.com/GewelsJI/PNS-Net
"""In the Pong environment  the agent has three related elements: action  reward and state.  Actions: agent takes the action at time t; there are six actions including going up  down  staying put  fire the ball  etc. Rewards: agent/environment receives/produces reward  when the opponent fails to hit the ball back towards the agent or the agent get 21 points and win. State: environment updates state St  which is defined by four game frames’ interfaces stacking together - the Pong involves motion of two paddles and one ball  and background features that the agent need to learn at the game. The network  suggested by https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf  is used to approximate the action values  which consists of three convolutional neural networks followed by two dense layers. In addition to a network used for training  the other network  which is architecture identical with the first one  gets its weights by copying them from the train network periodically during training and is used to compute the action value label. The other network (called the target network by the paper) is set up to avoid instability in training.  The model is trained using the following three frameworks.    """;Reinforcement Learning;https://github.com/gznyyb/deep_reinforcement_learning_Pong
"""MutualGuide is a compact object detector specially designed for embedded devices. Comparing to existing detectors  this repo contains two key features.   Firstly  the Mutual Guidance mecanism assigns labels to the classification task based on the prediction on the localization task  and vice versa  alleviating the misalignment problem between both tasks; Secondly  the teacher-student prediction disagreements guides the knowledge transfer in a feature-based detection distillation framework  thereby reducing the performance gap between both models.  For more details  please refer to our [ACCV paper](https://openaccess.thecvf.com/content/ACCV2020/html/Zhang_Localize_to_Classify_and_Classify_to_Localize_Mutual_Guidance_in_ACCV_2020_paper.html) and [BMVC paper](https://www.bmvc2021.com/).   """;Computer Vision;https://github.com/ZHANGHeng19931123/MutualGuide
"""The same learning algorithm was used to train agents for each of the ten OpenAI Gym MuJoCo continuous control environments. The only difference between evaluations was the number of episodes used per training batch  otherwise all options were the same. The exact code used to generate the OpenAI Gym submissions is in the **`aigym_evaluation`** branch.  Here are the key points:  * Proximal Policy Optimization (similar to TRPO  but uses gradient descent with KL loss terms)  \[1\] \[2\] * Value function approximated with 3 hidden-layer NN (tanh activations):     * hid1 size = obs_dim x 10     * hid2 size = geometric mean of hid1 and hid3 sizes     * hid3 size = 5 * Policy is a multi-variate Gaussian parameterized by a 3 hidden-layer NN (tanh activations):     * hid1 size = obs_dim x 10     * hid2 size = geometric mean of hid1 and hid3 sizes     * hid3 size = action_dim x 10     * Diagonal covariance matrix variables are separately trained * Generalized Advantage Estimation (gamma = 0.995  lambda = 0.98) \[3\] \[4\] * ADAM optimizer used for both neural networks * The policy is evaluated for 20 episodes between updates  except:     * 50 episodes for Reacher     * 5 episodes for Swimmer     * 5 episodes for HalfCheetah     * 5 episodes for HumanoidStandup * Value function is trained on current batch + previous batch * KL loss factor and ADAM learning rate are dynamically adjusted during training * Policy and Value NNs built with TensorFlow   """;Reinforcement Learning;https://github.com/magnusja/ppo
"""[Architecture]: Frame-Recurrent Video Super-Resolution  Sajjadi et al. (https://arxiv.org/pdf/1512.02134.pdf) [Dataset]: It was obtained from 30 video taken from Youtube by me.  ![Model](preview/dataset.png)  Implementation of [FRVSR](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/paper-MIFDB16.pdf) arhitecture with some tweaks such as:  * adding new PP Loss (references (https://arxiv.org/pdf/1811.09393.pdf)) * modifing arhitecture residual blocks by adding new spectral normalization layers (references (https://arxiv.org/pdf/1802.05957.pdf)) * adding depth to the model ( more channels and res blocks )  ![Model](preview/frvsr.png)    """;Computer Vision;https://github.com/adbobes/VideoSuperResolution
"""[Architecture]: Frame-Recurrent Video Super-Resolution  Sajjadi et al. (https://arxiv.org/pdf/1512.02134.pdf) [Dataset]: It was obtained from 30 video taken from Youtube by me.  ![Model](preview/dataset.png)  Implementation of [FRVSR](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/paper-MIFDB16.pdf) arhitecture with some tweaks such as:  * adding new PP Loss (references (https://arxiv.org/pdf/1811.09393.pdf)) * modifing arhitecture residual blocks by adding new spectral normalization layers (references (https://arxiv.org/pdf/1802.05957.pdf)) * adding depth to the model ( more channels and res blocks )  ![Model](preview/frvsr.png)    """;General;https://github.com/adbobes/VideoSuperResolution
"""Augmented Implementation of the [pSp implementation](https://github.com/eladrich/pixel2style2pixel) to train Images on white noise reduction   """;Computer Vision;https://github.com/rahuls02/Image-Noise-Reduction
"""The model is used to tackle the alpha matting problem. It is basically an encoder-decoder deep neural network. By feed the network an original image with tri-map  you can get the prediction alpha of the image.       Deep Image Matting     Ning Xu  Brian Price  Scott Cohen  and Thomas Huang.      CVPR  2017.  The input images should be mean pixel subtraction. And the channel should be BGR.   """;Computer Vision;https://github.com/ShawnNew/shape-alpha-matting
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/MonsterPeng/Deformable-ConvNets-master
"""Requires a training set of summaries that are not a headline. There is a wikipedia dataset that may be useful  https://github.com/tscheepers/Wikipedia-Summary-Dataset  part of this https://github.com/tscheepers/CompVec   """;Natural Language Processing;https://github.com/mlennox/summarisers
"""This capstone project would be aimed at building an image classification model using convulated neural networks that will identify the following 4 categories of products: backpacks  clothing  footwear and watches. The source data to construct this model will be based on 20 000 images scraped from Amazon  the world's largest online retailer. Based on this  a recognition model will be used to correctly class a certain product image. Secondly  a recommendation system would be built to promote the closest matches based on a select product image.  Stakeholders will be the e-commerce companies and the user of the services themselves. It will help the company improve the effectiveness of potential transactions. It will also improve the user experience with more accuracy and also to avoid problems arising from wrongly identifying products. Often  users may want to source for similar looking items and a recommender would help to efficiently match a user's preferences to similar postings  giving rise to increased transactions and sales turnover.  The detailed solution to this would be to make use of unsupervised machine learning via neural networks in order to perform multi-classification of product categories. Dimensional clustering can also be used in order to match similar looking product images for the recommender system.  Metrics used to measure the performance would be accuracy using majority class and also compared to Imagenet pre-trained model performance  to be within 5 percentage points. Challenges foreseen would be lack of sufficient data  complex background noise or poor resolution images.   """;Computer Vision;https://github.com/Lester1711/DSI15_Capstone
"""* https://arxiv.org/abs/1910.13302 (updated: 2020.08) * https://authors.elsevier.com/c/1ca0dxnVK3cWY   If you find this code useful please cite:  ``` @article{solovyev2021weighted    title={Weighted boxes fusion: Ensembling boxes from different object detection models}    author={Solovyev  Roman and Wang  Weimin and Gabruseva  Tatiana}    journal={Image and Vision Computing}    pages={1-6}    year={2021}    publisher={Elsevier} } ```  """;Computer Vision;https://github.com/ZFTurbo/Weighted-Boxes-Fusion
"""* this project using gym-style interface of ai2thor environment * objective is simply picking an apple in kitchen environment - FloorPlan28 * observation space is first-view RGB 128x128 image from agent's camera * maximum step in this project is 500 * reward fuction:      * -0.01 each time step     * 1 if agent can pick an apple  the env than terminate     * 0.01 if agent saw an apple (has been removed in latest code)  * a pre-train mobilenet-v2 model on image-net is used an feature extractor for later dense layer both actor and critic model * actor optimizer using Advantages + Entropy term to encourage exploration (https://arxiv.org/abs/1602.01783)   """;Reinforcement Learning;https://github.com/gungui98/deeprl-a3c-ai2thor
"""* this project using gym-style interface of ai2thor environment * objective is simply picking an apple in kitchen environment - FloorPlan28 * observation space is first-view RGB 128x128 image from agent's camera * maximum step in this project is 500 * reward fuction:      * -0.01 each time step     * 1 if agent can pick an apple  the env than terminate     * 0.01 if agent saw an apple (has been removed in latest code)  * a pre-train mobilenet-v2 model on image-net is used an feature extractor for later dense layer both actor and critic model * actor optimizer using Advantages + Entropy term to encourage exploration (https://arxiv.org/abs/1602.01783)   """;General;https://github.com/gungui98/deeprl-a3c-ai2thor
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/joytianya/google_bert
"""The project implement the clipped version of Proximal Policy Optimization Algorithms described here https://arxiv.org/pdf/1707.06347.pdf  Into the config.yaml file are defined some of the hyper parameter used into the various implementation. Those parameters are initilized with the values proposed in the article.   Here the key point: * Loss function parameters:   * epsilon = 0.2   * gamma = 0.99   * entropy loss = 1e-3    * Network size:   * state_size = 27 (25 laser scan + target heading + target distance)   * action_size (angular velocity) = 5   * action_size2 (linear velocity) = 3   * batch_size = 64   * output layer = 8 into 2 streams (5 nodes for angular and 3 for linear velocity)   * lossWeights for the output layer:      * 0 5     * 0.5      The values for the loss weights are the result of some test. With an equal weight the success rate is lower.    """;Reinforcement Learning;https://github.com/MatteoBrentegani/PPO
"""BNM v1: we prove in the paper that Batch Nuclear-norm Maximization (BNM) can ensure the prediction discriminability and diversity  which is an effective method under label insufficient situations.  BNM v2: we further devise Batch Nuclear-norm Minimization (BNMin) and Fast BNM (FBNM) for multiple domain adaptation scenarios.   """;General;https://github.com/cuishuhao/BNM
"""Sonnet has been designed and built by researchers at DeepMind. It can be used to construct neural networks for many different purposes (un/supervised learning  reinforcement learning  ...). We find it is a successful abstraction for our organization  you might too!  More specifically  Sonnet provides a simple but powerful programming model centered around a single concept: `snt.Module`. Modules can hold references to parameters  other modules and methods that apply some function on the user input. Sonnet ships with many predefined modules (e.g. `snt.Linear`  `snt.Conv2D`  `snt.BatchNorm`) and some predefined networks of modules (e.g. `snt.nets.MLP`) but users are also encouraged to build their own modules.  Unlike many frameworks Sonnet is extremely unopinionated about **how** you will use your modules. Modules are designed to be self contained and entirely decoupled from one another. Sonnet does not ship with a training framework and users are encouraged to build their own or adopt those built by others.  Sonnet is also designed to be simple to understand  our code is (hopefully!) clear and focussed. Where we have picked defaults (e.g. defaults for initial parameter values) we try to point out why.   """;Computer Vision;https://github.com/deepmind/sonnet
"""See also the animated  extended version of the summary at: [https://youtu.be/U2okTa0JGZg](https://youtu.be/U2okTa0JGZg). Some of the subtler changes are better visible when animated. For each of the features listed below  there is a link to the exact corresponding timestamp in the animation.   [Smile: upper lip](#smile-upper-lip-animation02m43s)   [Age](#age-animation04m22s)   [Look direction](#look-direction-animation03m16s)   [Hair color](#hair-color-animation00m06s)   [Left / right rotation](#left--right-rotation-animation00m15s)   [Face oval: size](#face-oval-size-animation00m28s)   [Hairstyle: background size](#hairstyle-background-size-animation01m08s)   [Lower jaw size](#lower-jaw-size-animation01m51s)   [Forward backward inclination](#forward--backward-inclination-animation01m41s)   [Mouth: open / closed](#mouth-open--closed-animation01m31s)   [Hair: wavy / straight](#hair-wavy--straight-animation02m11s)   [Eyebrows: up / down](#eyebrows-up--down-animation02m56s)   [Nose length](#nose-length-animation04m02s)   [Nose: upturned tip](#nose-upturned-tip-animation03m52s)   [Eyebrows shape](#eyebrows-shape-animation03m06s)   [Vertical face stretch](#vertical-face-stretch-animation03m32s)   [Color of the irises](#color-of-the-irises-animation05m07s)   [Shape of the nostrils](#shape-of-the-nostrils-animation05m18s)   [Hair texture](#hair-texture-animation06m10s)   [Lower eyelid](#lower-eyelid-animation05m54s)   [Wrinkles](#wrinkles-animation05m27s)     A comprehensive list of all isolated features is at: [https://youtu.be/mOckeVkM1jU](https://youtu.be/mOckeVkM1jU).    """;Computer Vision;https://github.com/jonasz/progressive_infogan
"""Puzzle RLE (Reinforcement Learning Environment) is an environment for learning the puzzle gameplay from the mobile game [Puzzle and Dragons](https://youtu.be/tLku-s20EBE) (Gungho Online Entertainment  Tokyo  Japan). The environment is a re-implemntation based on orb-matching and clearing mechanics encountered during normal gameplay.  The environment supports the following: * pygame environment visualization engine * 5 Actions: select-orb  move left  up  right  down * Baseline random agent * OpenAI Baselines agents   """;Reinforcement Learning;https://github.com/nathanin/pad
"""we propose some application providing face detection of specific person and face blurring of the others. There is two Paper for achieving job of project application.   """;Computer Vision;https://github.com/jooyounghun/AI-Team-5
""" [CK-Caffe](https://github.com/dividiti/ck-caffe) is an open framework for collaborative and reproducible optimisation of convolutional neural networks. It's based on the [Caffe](http://caffe.berkeleyvision.org) framework from the Berkeley Vision and Learning Center ([BVLC](http://bvlc.eecs.berkeley.edu)) and the [Collective Knowledge](http://cknowledge.org) framework for customizable cross-platform builds and experimental workflows with JSON API from the [cTuning Foundation](http://ctuning.org) (see CK intro for more details: [1](https://arxiv.org/abs/1506.06256)  [2](https://www.researchgate.net/publication/304010295_Collective_Knowledge_Towards_RD_Sustainability) ). In essence  CK-Caffe is an open-source suite of convenient wrappers and workflows with unified JSON API for simple and customized building  evaluation and multi-objective optimisation of various Caffe implementations (CPU  CUDA  OpenCL) across diverse platforms from mobile devices and IoT to supercomputers.  As outlined in our [vision](http://dx.doi.org/10.1145/2909437.2909449)  we invite the community to collaboratively design and optimize convolutional neural networks to meet the performance  accuracy and cost requirements for deployment on a range of form factors - from sensors to self-driving cars. To this end  CK-Caffe leverages the key capabilities of CK to crowdsource experimentation across diverse platforms  CNN designs  optimization options  and so on; exchange experimental data in a flexible JSON-based format; and apply leading-edge predictive analytics to extract valuable insights from the experimental data.  See [cKnowledge.org/ai](http://cKnowledge.org/ai)   [reproducible and CK-powered AI/SW/HW co-design competitions at ACM/IEEE conferences](http://cKnowledge.org/request)  [shared optimization statistics](http://cKnowledge.org/repo)  [reusable AI artifact in the CK format](http://cKnowledge.org/ai-artifacts) and [online demo of CK AI API with self-optimizing DNN](http://cKnowledge.org/ai/ck-api-demo) for more details.   """;Computer Vision;https://github.com/dividiti/ck-caffe
"""* This is an assignment of [Deep Learning basic class](https://deeplearning.jp/lectures/dlb2018/) arranged a little.  * Training Fashion-MNIST by ResNet on Google Colaboratory with TensorFlow 2.0 Alpha. * Data is augmented by ImageDataGenerator of Keras.   """;General;https://github.com/shoji9x9/Fashion-MNIST-By-ResNet
"""* This is an assignment of [Deep Learning basic class](https://deeplearning.jp/lectures/dlb2018/) arranged a little.  * Training Fashion-MNIST by ResNet on Google Colaboratory with TensorFlow 2.0 Alpha. * Data is augmented by ImageDataGenerator of Keras.   """;Computer Vision;https://github.com/shoji9x9/Fashion-MNIST-By-ResNet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/guoyaohua/BERT-Chinese-Annotation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/guoyaohua/BERT-Chinese-Annotation
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/zzdxfei/defor_conv_mxnet_code
"""The *Show and Tell* model is a deep neural network that learns how to describe the content of images. For example:  ![Example captions](g3doc/example_captions.jpg)   """;Computer Vision;https://github.com/SophiaYuSophiaYu/ImageCaption
"""This is the SSD model based on project by [Max DeGroot](https://github.com/amdegroot/ssd.pytorch/). I corrected some bugs in the code and successfully run the code on GPUs at Google Cloud.    [SSD (Single Shot MultiBox Object Detector)](https://arxiv.org/pdf/1512.02325.pdf) is able to detect objects in an image with bounding boxes. The method is faster than [faster-RCNN](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf) and [mask-RCNN](https://arxiv.org/pdf/1703.06870.pdf%20http://arxiv.org/abs/1703.06870.pdf) and still yield a good accuracy.   """;Computer Vision;https://github.com/yczhang1017/SSD_resnet_pytorch
"""SWA is a simple DNN training method that can be used as a drop-in replacement for SGD with improved generalization  faster convergence  and essentially no overhead. The key idea of SWA is to average multiple samples produced by SGD with a modified learning rate schedule. We use a constant or cyclical learning rate schedule that causes SGD to _explore_ the set of points in the weight space corresponding to high-performing networks. We observe that SWA converges more quickly than SGD  and to wider optima that provide higher test accuracy.   In this repo we implement the constant learning rate schedule that we found to be most practical on CIFAR datasets.  <p align=""center"">   <img src=""https://user-images.githubusercontent.com/14368801/37633888-89fdc05a-2bca-11e8-88aa-dd3661a44c3f.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633885-89d809a0-2bca-11e8-8d57-3bd78734cea3.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633887-89e93784-2bca-11e8-9d71-a385ea72ff7c.png"" width=250> </p>  Please cite our work if you find this approach useful in your research: ```bibtex @article{izmailov2018averaging    title={Averaging Weights Leads to Wider Optima and Better Generalization}    author={Izmailov  Pavel and Podoprikhin  Dmitrii and Garipov  Timur and Vetrov  Dmitry and Wilson  Andrew Gordon}    journal={arXiv preprint arXiv:1803.05407}    year={2018} } ```    """;General;https://github.com/timgaripov/swa
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/msracver/Deformable-ConvNets
"""**Faster** R-CNN is an object detection framework based on deep convolutional networks  which includes a Region Proposal Network (RPN) and an Object Detection Network. Both networks are trained for sharing convolutional layers for fast testing.   Faster R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1506.01497).  This repo contains a MATLAB re-implementation of Fast R-CNN. Details about Fast R-CNN are in: [rbgirshick/fast-rcnn](https://github.com/rbgirshick/fast-rcnn).  This code has been tested on Windows 7/8 64-bit  Windows Server 2012 R2  and Linux  and on MATLAB 2014a.  Python version is available at [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn).   """;Computer Vision;https://github.com/ShaoqingRen/faster_rcnn
"""This repository build on the work of https://github.com/tkipf/gae  (N.  Thomas  and Max. “Variational Graph Auto-Encoders.” ArXiv.org  21 Nov. 2016  arxiv.org/abs/1611.07308.) to learn representations of scientific citation data into a latent space. If latent space is then a representation of the the underlying *information* in the dataset  investigating the latent space and their correlations with features  we can begin to understand how the features interact with each other in a complex networks - such as the scientific citation network.   """;Graphs;https://github.com/Omairss/RepresentationLearning
"""The A3C algorithm was released by Google’s DeepMind group earlier this year  and it made a splash by essentially obsoleting DQN. It was faster  simpler  more robust  and able to achieve much better scores on the standard battery of Deep RL tasks. On top of all that it could work in continuous as well as discrete action spaces. Given this  it has become the go-to Deep RL algorithm for new challenging problems with complex state and action spaces        <a href= ""https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"" >Medium Article explaining A3c reinforcement learning </a> <br> ![A3C LSTM playing MsPacman-v0](https://github.com/Nasdin/ReinforcementLearning-AtariGame/blob/master/img/MsPacman.gif)  """;Reinforcement Learning;https://github.com/Nasdin/ReinforcementLearning-AtariGame
"""The A3C algorithm was released by Google’s DeepMind group earlier this year  and it made a splash by essentially obsoleting DQN. It was faster  simpler  more robust  and able to achieve much better scores on the standard battery of Deep RL tasks. On top of all that it could work in continuous as well as discrete action spaces. Given this  it has become the go-to Deep RL algorithm for new challenging problems with complex state and action spaces        <a href= ""https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"" >Medium Article explaining A3c reinforcement learning </a> <br> ![A3C LSTM playing MsPacman-v0](https://github.com/Nasdin/ReinforcementLearning-AtariGame/blob/master/img/MsPacman.gif)  """;General;https://github.com/Nasdin/ReinforcementLearning-AtariGame
"""In this study  we try to understand the limits of our system when running a Deep Learning training. The step to train a model is the most time consuming step of the model building process. With contraints put on the hardware  what can we do on the programming side to help us train models better? What if you had a limited amount of time? To try out our hand at augmentation  we will be using the Flickr27 dataset.     """;Computer Vision;https://github.com/premthomas/keras-image-classification
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/xesdiny/test-bert-master
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/xesdiny/test-bert-master
"""I've been searching for a Tensorflow implementation of YOLOv2 for a while but the darknet version and derivatives are not really easy to understand. This one is an hopefully easier-to-understand version of Tiny YOLOv2. The weight extraction  weights structure  weight assignment  network  inference and postprocessing are made as simple as possible.  The output of this implementation on the test image ""dog.jpg"" is the following:  ![alt text](https://github.com/simo23/tinyYOLOv2/blob/master/dog_output.jpg ""YOLOv2 output"")  Just to be clear  this implementation is called ""tiny-yolo-voc"" on pjreddie's site and can be found here:  ![alt text](https://github.com/simo23/tinyYOLOv2/blob/master/pjsite.png ""YOLOv2 site"")   """;Computer Vision;https://github.com/simo23/tinyYOLOv2
"""I've been searching for a Tensorflow implementation of YOLOv2 for a while but the darknet version and derivatives are not really easy to understand. This one is an hopefully easier-to-understand version of Tiny YOLOv2. The weight extraction  weights structure  weight assignment  network  inference and postprocessing are made as simple as possible.  The output of this implementation on the test image ""dog.jpg"" is the following:  ![alt text](https://github.com/simo23/tinyYOLOv2/blob/master/dog_output.jpg ""YOLOv2 output"")  Just to be clear  this implementation is called ""tiny-yolo-voc"" on pjreddie's site and can be found here:  ![alt text](https://github.com/simo23/tinyYOLOv2/blob/master/pjsite.png ""YOLOv2 site"")   """;General;https://github.com/simo23/tinyYOLOv2
"""1. MNIST  2. cifar10  3. ImageNet   """;General;https://github.com/horse007666/ResNet
"""1. MNIST  2. cifar10  3. ImageNet   """;Computer Vision;https://github.com/horse007666/ResNet
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/AndrewZhou924/YOLOv3_pytorch
"""The Follow Me project consists in making a UAV able to follow a *specific* human. To achieve this goal  we build and train a fully convolutional network (FCN).   """;Computer Vision;https://github.com/alexandrelewin/FollowMe
"""AutoInt：An effective and efficient algorithm to automatically learn high-order feature interactions for (sparse) categorical and numerical features.  <div align=center>   <img src=""https://github.com/shichence/AutoInt/blob/master/figures/model.png"" width = 50% height = 50% /> </div> The illustration of AutoInt. We first project all sparse features (both categorical and numerical features) into the low-dimensional space. Next  we feed embeddings of all fields into stacked multiple interacting layers implemented by self-attentive neural network. The output of the final interacting layer is the low-dimensional representation of learnt combinatorial features  which is further used for estimating the CTR via sigmoid function.   """;General;https://github.com/shichence/AutoInt
"""**This project aims at building a speech enhancement system to attenuate environmental noise.**  <img src=""img/denoise_10classes.gif"" alt=""Spectrogram denoising"" title=""Speech enhancement""/>    Audios have many different ways to be represented  going from raw time series to time-frequency decompositions. The choice of the representation is crucial for the performance of your system. Among time-frequency decompositions  Spectrograms have been proved to be a useful representation for audio processing. They consist in 2D images representing sequences of Short Time Fourier Transform (STFT) with time and frequency as axes  and brightness representing the strength of a frequency component at each time frame. In such they appear a natural domain to apply the CNNS architectures for images directly to sound. Between magnitude and phase spectrograms  magnitude spectrograms contain most the structure of the signal. Phase spectrograms appear to show only little temporal and spectral regularities.  In this project  I will use magnitude spectrograms as a representation of sound (cf image below) in order to predict the noise model to be subtracted to a noisy voice spectrogram.  <img src=""img/sound_to_spectrogram.png"" alt=""sound representation"" title=""sound representation"" />  The project is decomposed in three modes: `data creation`  `training` and `prediction`.   """;Computer Vision;https://github.com/vbelz/Speech-enhancement
"""1. end-to-end 2. extremely fast (base YOLO : 45 FPS  Fast YOLO : 155 FPS  other SOTA 7~20 FPS) 3. YOLOv1-VGG16 mAP : 66.4% vs FasterRCNN mAP : 73.2% (Pascal VOC 2007 Test) 4. small objects do not detect well. 5. each grid cell only predicts two boxes and can only have one class.   """;Computer Vision;https://github.com/OFRIN/Tensorflow_YOLOv1
"""Le NST permet  à partir d'une image de contenu (ex: une photo) et d'une image de référence de style (ex: une peinture)  de créer une image qui maintienne le contenu de la première tout en reproduisant le style de la seconde. [Source](https://www.tensorflow.org/tutorials/generative/style_transfer )  Autrement dit  l’algorithme NST manipule des images dans le but de leur donner l’apparence ou le style visuel d’une autre image. Ces algorithmes utilisent des réseaux neuronaux profonds pour pouvoir réaliser la transformation d’images.   Ce projet a pour objectif la découverte du Neural Style Transfer (NST) à travers l’utilisation des modèles de d2l et de Google Brain. L'objectif est de réaliser un transfert d'une image de style à une image de contenu  et et de créer un site web d’application du modèle de Google Brain.  Ce readMe permet d’expliquer la méthode de NST de d2l suivant un principe d’optimisation pour ensuite la comparer avec la méthode de Google Brain qui permet l’obtention en une boucle de l’image synthétisée sans entraînement préalable sur les images données en entrée. Ce dépôt contient la mise en place d'un site web / API pour faire la démonstration de l’application du modèle choisi  créé par Google Brain  prenant des images en entrée pour donner en sortie l’image synthétisée.   """;Computer Vision;https://github.com/RemiArbache/style-transfer-M2
""" Narcissus is an interactive art installation which takes an image of the viewer  compresses it into a set of descriptive features  and produces the ""reflection"" of those features as a neural network sees it. Particularly  this Generative Adversarial Network (GAN) draws the viewer's reflection from its artificial  distinctly neural ""latent"" space which it learns from its training dataset. I used a GAN trained on a well-known celebrity face dataset  CelebA-HQ  meaning that the neural network learns to reproduce faces of celebrities. Narcissus intends to raise more self-reflecting questions in the viewer than it answers  many of which are becoming increasingly relevant.  When the viewer looks into the mirror and sees a foreign face stare back  similarities and differences are visceral. What makes a celebrity’s face different from ours? How does the generated face staring back at us reflect what we deem as special  as 'celebrity'  or as beautiful? More broadly  how do the deep-seated biases inherent in our everyday society lie hidden in the datasets we collect?  These questions are all the more important in the new age of artificial intelligence; those biased datasets we use lead to inherently biased data-driven artificial intelligence models. Without sufficient checks and balances  data-driven systems can also create feedback loops resulting in an amplification of these biases  such as in [recidivism prediction](https://www.technologyreview.com/s/612775/algorithms-criminal-justice-ai/). As well as this  since data is necessarily a reduction of objective reality  models that learn only from training data will never have the full picture and there will always be cases where it lacks common sense. For example  if a model is trained on current recidivism data to predict recidivism rates  even if the data contains no information about race  the model might learn that certain crimes are committed by certain classes of people corresponding to race  and use that information to predict rates that might still be disproportionate across race. We must be careful not to blindly believe that these systems will not perpetuate the biases and mistakes of our current or previous societies.  As well as this  the model often makes mistakes that seem obvious to us  producing a celebrity that looks totally different from the viewer. Even though the celebrity is a product of the viewer's data  *is* it the viewer? This again brings to attention the point that data is simply a reduction of objective reality. How can we reduce the important features of a human to a representation in data? Is there a minimum amount of data that can represent a human? Corporations are collecting and consolidating data into unprecedented [data lakes]() with data sources from [amazon ring](https://www.usatoday.com/story/tech/2019/10/29/amazon-ring-doorbell-cams-police-home-security/2493974001/)  []()  and [](). While this data can be used to build powerful models  these models will never operate on objective reality  but just a reduction of it. As we progress into the age of big data and powerful models that are increasingly replacing humans in important decision-making including punishment sentencing  [facial recognition in HK](https://www.nytimes.com/2019/07/26/technology/hong-kong-protests-facial-recognition-surveillance.html)  and [healthcare](https://www.nytimes.com/2019/03/21/science/health-medicine-artificial-intelligence.html)  we need to be wary of the inadvertant effects caused by a general lack of human ""common sense"" in these models due to inherent properties of data.   """;Computer Vision;https://github.com/kyranstar/Narcissus
""" This is the **ranked No.1** tensorflow based scene text spotting algorithm on [__ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text__](https://rrc.cvc.uab.es/?ch=14) (Latin Only  Latin and Chinese)  futhermore  the algorithm is also adopted in [__ICDAR2019 Robust Reading Challenge on Large-scale Street View Text with Partial Labeling__](https://rrc.cvc.uab.es/?ch=16) and [__ICDAR2019 Robust Reading Challenge on Reading Chinese Text on Signboard__](https://rrc.cvc.uab.es/?ch=12).   Scene text detection algorithm is modified from [__Tensorpack FasterRCNN__](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN)  and we only open source code in this repository for scene text recognition. I upload ICDAR2019 ArT competition model to docker hub  please refer to [Docker](#Docker). For more details  please refer to our [__arXiv technical report__](https://arxiv.org/abs/1912.04561).  Our text recognition algorithm not only recognizes Latin and Non-Latin characters  but also supports horizontal and vertical text recognition in one model. It is convenient for multi-lingual arbitrary-shaped text recognition.  **Note that the competition model in docker container as described in [__our technical report__](https://arxiv.org/abs/1912.04561) is slightly different from the recognition model trained from this updated repository.**   """;General;https://github.com/zhang0jhon/AttentionOCR
"""Classifying unlabeled data is generally the final purpose of supervised learning problems. Fitting a classifier for accurate predictions of unlabeled data can not be achieved with supervised learning  unsupervised learning may be considered instead. The typical purpose of unsupervised learning is to find relations between datapoints but compared to supervised learning they do not output any target predictions.  Semi-supervised learning includes the challange to classify a large unlabeled dataset using a small labeled dataset for training the classifier. Reasons of pursuing semi-supervised learning rather than supervised may be to reduce the manual labour of having a domain expert label every datapoint.   """;General;https://github.com/stefangeneralao/exterior-interior-classifier
"""[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.   """;Natural Language Processing;https://github.com/Kinetikm/fasttextRelearnExperiment
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Shinya-Kouda/kgc
"""This is an official pytorch implementation of [*Deep High-Resolution Representation Learning for Human Pose Estimation*](https://arxiv.org/abs/1902.09212).  In this work  we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods **recover high-resolution representations from low-resolution representations** produced by a high-to-low resolution network. Instead  our proposed network **maintains high-resolution representations** through the whole process. We start from a high-resolution subnetwork as the first stage  gradually add high-to-low resolution subnetworks one by one to form more stages  and connect the mutli-resolution subnetworks **in parallel**. We conduct **repeated multi-scale fusions** such that each of the high-to-low resolution representations receives information from other parallel representations over and over  leading to rich high-resolution representations. As a result  the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. </br>  ![Illustrating the architecture of the proposed HRNet](/figures/hrnet.png)  """;Computer Vision;https://github.com/leoxiaobin/deep-high-resolution-net.pytorch
"""In this repo  I have written and refactored some code to mirror strings using the seq2seq model. For example  if the input is ABC  the ground-truth output is CBA.  """;Natural Language Processing;https://github.com/trqminh/seq2seq
"""We propose a novel building block for CNNs  namely Res2Net  by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models  e.g.   ResNet  ResNeXt  BigLittleNet  and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models. <p align=""center""> 	<img src=""https://mftp.mmcheng.net/imgs800/19Res2Net.jpg"" alt=""Sample""  width=""500""> 	<p align=""center""> 		<em>Res2Net module</em> 	</p> </p>   """;Computer Vision;https://github.com/Res2Net/Res2Net-PretrainedModels
"""This project extends the original [Google DeViSE](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf) paper to create a functioning image search engine with a focus on interpreting search results. We have extended the original paper in the following ways. First  we added an RNN to process variable length queries as opposed to single words. Next  to understand how the network responds to different parts of the query(like noun phrases) and the image  we leverage [Ribeiro et.al's LIME](https://arxiv.org/pdf/1602.04938v1.pdf) for model-agnostic interpretability. It has been tested on subsets of the [UIUC-PASCAL dataset](http://vision.cs.uiuc.edu/pascal-sentences/) and the final network has been trained on the [MSCOCO 2014 dataset](http://cocodataset.org/#home).   """;General;https://github.com/priyamtejaswin/devise-keras
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Nstats/bert_MRC
"""This work builds on the foundations laid by Irwan Bello in ""LambdaNetworks: Modeling long-range interactions without attention""  [(Bello  2021)](https://arxiv.org/abs/2102.08602). Bello proposes a method where long-range interactions are modeled by layers which  transform contexts into linear functions called lambdas  in order to avoid the use of attention maps. The great advantage  of lambda layers is that they require much less compute than self-attention mechanisms according to the original paper  by Bello. This is fantastic  because it does not only provide results faster  but also saves money and has a more  favorable carbon footprint! However  Bello still uses 32 [TPUv3s](https://cloud.google.com/tpu) and the  200 GB sized ImageNet classification dataset. Therefore  we started this reproducibility project wondering: Could lambda  layers be scaled to mainstream computers while keeping its attractive properties?  In 2021 the world did not only have to deal with the [COVID-19 epidemic](https://www.who.int/emergencies/diseases/novel-coronavirus-2019)  but was struck by [chip shortages](https://www.cnbc.com/2021/02/10/whats-causing-the-chip-shortage-affecting-ps5-cars-and-more.html)  as well due to increase in consumer electronics for working at home  shut down factories in China and the rising prices of crypto-currencies.  This has decreased supply to record lows and prices to record highs. Resulting in a situation  whereby researchers  academics   and students (who are all usually on a budget) are no longer able to quickly build a cluster out of COTS (commercial off-the-shelf)  GPUs resulting in having to deal with older  less  and less efficient hardware.  No official code was released at the time of starting the project mid-March. Therefore  in order to answer the  aforementioned question  it is up to us to reproduce the paper by Bello as accurately as possible while trying to scale  it down such that it can be run on an average consumer computer.     """;General;https://github.com/joigalcar3/LambdaNetworks
"""Sonnet has been designed and built by researchers at DeepMind. It can be used to construct neural networks for many different purposes (un/supervised learning  reinforcement learning  ...). We find it is a successful abstraction for our organization  you might too!  More specifically  Sonnet provides a simple but powerful programming model centered around a single concept: `snt.Module`. Modules can hold references to parameters  other modules and methods that apply some function on the user input. Sonnet ships with many predefined modules (e.g. `snt.Linear`  `snt.Conv2D`  `snt.BatchNorm`) and some predefined networks of modules (e.g. `snt.nets.MLP`) but users are also encouraged to build their own modules.  Unlike many frameworks Sonnet is extremely unopinionated about **how** you will use your modules. Modules are designed to be self contained and entirely decoupled from one another. Sonnet does not ship with a training framework and users are encouraged to build their own or adopt those built by others.  Sonnet is also designed to be simple to understand  our code is (hopefully!) clear and focussed. Where we have picked defaults (e.g. defaults for initial parameter values) we try to point out why.   """;General;https://github.com/deepmind/sonnet
"""```You can run the model and the harness around it using: python main.py  Run evaluate over your test set python main.py --mode=evaluate  Configuration: config.py   config.TRAIN.batch_size = 8 #:Training batch size  config.TRAIN.lr_init = 1e-4 #:Initial Learning Rate  config.TRAIN.beta1 = 0.9 #:Beta1 parameter for batch normalization  config.TRAIN.n_epoch_init = 35 #:Number of epochs to run the generator before adversarial training  config.TRAIN.n_epoch = 56 #:Number of Epochs of Adversarial training config.TRAIN.lr_decay = 0.1 #:Learning rate decay through adversarial training config.TRAIN.decay_every = int(config.TRAIN.n_epoch / 2)   config.TRAIN.hr_img_path = '../train_data_out_2' config.TRAIN.lr_img_path = '../train_data_in'  config.VALID.hr_img_path = '../test_data_out_2/' config.VALID.lr_img_path = '../test_data_in/' ```  TRAIN.hr_img_path is the groundtruth path and TRAIN.lr_img_path is the input image path. In our case these are 128x128 slices of input image and binary masks.   """;Computer Vision;https://github.com/Violet981/Breast_mass_Segmentation
"""**T-TA**  or **T**ransformer-based **T**ext **A**utoencoder   is a new deep bidirectional language model for unsupervised learning tasks. T-TA learns the straightforward learning objective  *language autoencoding*  which is to predict all tokens in a sentence at once using only their context. Unlike ""masked language model""  T-TA has *self-masking* mechanism in order to avoid merely copying the input to output. Unlike BERT (which is for fine-tuning the entire pre-trained model)  T-TA is especially beneficial to obtain contextual embeddings   which are fixed representations of each input token generated from the hidden layers of the trained language model.  T-TA model architecture is based on the [BERT](https://arxiv.org/abs/1810.04805) model architecture  which is mostly a standard [Transformer](https://arxiv.org/abs/1706.03762) architecture. Our code is based on [Google's BERT github](https://github.com/google-research/bert)  which includes methods for building customized vocabulary  preparing the Wikipedia dataset  etc.    """;General;https://github.com/joongbo/tta
"""**T-TA**  or **T**ransformer-based **T**ext **A**utoencoder   is a new deep bidirectional language model for unsupervised learning tasks. T-TA learns the straightforward learning objective  *language autoencoding*  which is to predict all tokens in a sentence at once using only their context. Unlike ""masked language model""  T-TA has *self-masking* mechanism in order to avoid merely copying the input to output. Unlike BERT (which is for fine-tuning the entire pre-trained model)  T-TA is especially beneficial to obtain contextual embeddings   which are fixed representations of each input token generated from the hidden layers of the trained language model.  T-TA model architecture is based on the [BERT](https://arxiv.org/abs/1810.04805) model architecture  which is mostly a standard [Transformer](https://arxiv.org/abs/1706.03762) architecture. Our code is based on [Google's BERT github](https://github.com/google-research/bert)  which includes methods for building customized vocabulary  preparing the Wikipedia dataset  etc.    """;Natural Language Processing;https://github.com/joongbo/tta
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/PaulZhangIsing/CaffeSSD-opencv4
"""Sentence simplification is a task that produces a simplified sentence from any input sentence. Sentence simplification systems can be useful tools for people whose first language is not English  children  or the elderly to help with understanding. Initial approaches to this task were primarily borrowed from neural machine translation systems with transformations at the sentence level or with architectures based on recurrent neural networks (RNNs).  The core building blocks are a conditional sequence generative adversarial net which comprises of two adversarial sub models : **_Generator_** and **_Discriminator_**. The **Generator** aims to generate sentences which are hard to be discriminated from human-translated sentences; the **Discriminator** makes efforts to discriminate the machine-generated sentences from humantranslated ones.   """;General;https://github.com/zhangyaoyuan/GAN-Simplification
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/JoyHsiao/Haitec
"""**Model Type:** Artificial Neural Network ( 37 717 Trainable params )  <img src='./Images/model.png'>  **Features used (Variable X):**  The features (total 45) are of three categories:  * Market Data Features * Engineered Features - Rolling averages and Exponential Weighted Moving averages of Open and Close Prices * News Data Features  <img src='./Images/X.png'> <img src='./Images/X2.png'>  Some of these features were further normalized with Scalers from SKLearn.  **Features Predicted (Variable Y):**   	returnsOpenNextMktres10(float64) - 10 day  market-residualized return.      You must predict a signed confidence value  (-1  1)  which is multiplied by the market-adjusted return of a given assetCode over a ten day window.    """;Natural Language Processing;https://github.com/diabhaque/Sixth-Sense
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/dugusiqing/caffe
"""NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR)  natural language processing (NLP)  and text-to-speech synthesis (TTS). The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models and make it easier to create new `conversational AI models <https://developer.nvidia.com/conversational-ai#started>`_.  `Pre-trained NeMo models. <https://catalog.ngc.nvidia.com/models?query=nemo&orderBy=weightPopularDESC>`_   `Introductory video. <https://www.youtube.com/embed/wBgpMf_KQVw>`_   """;Natural Language Processing;https://github.com/NVIDIA/NeMo
"""Spatial transformer networks are a generalization of differentiable attention to any spatial transformation. Spatial transformer networks (STN for short) allow a neural network to learn how to perform spatial transformations on the input image in order to enhance the geometric invariance of the model. For example  it can crop a region of interest  scale and correct the orientation of an image. It can be a useful mechanism because CNNs are not invariant to rotation and scale and more general affine transformations.   Goals of the project:  1. Investigate if using CoordConv layers instead of standard Conv will help to improve the performance. 2. Compare the performance of the new model in evaluation metric and motivate the choice of metrics. 3. Explore new ideas that might achieve better performance than conventional STNs.   """;Computer Vision;https://github.com/vicsesi/PyTorch-STN
"""**Funnel-Transformer** is a new self-attention model that gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More importantly  by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model  Funnel-Transformer usually has a higher capacity given the same FLOPs. In addition  with a decoder  Funnel-Transformer is able to recover the token-level deep representation for each token from the reduced hidden sequence  which enables standard pretraining.    For a detailed description of technical details and experimental results  please refer to our paper:  > [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) > > Zihang Dai\*  Guokun Lai*  Yiming Yang  Quoc V. Le  > > (*: equal contribution)  > > Preprint 2020 >     """;Natural Language Processing;https://github.com/laiguokun/Funnel-Transformer
"""**RepPoints**  initially described in [arXiv](https://arxiv.org/abs/1904.11490)  is a new representation method for visual objects  on which visual understanding tasks are typically centered. Visual object representation  aiming at both geometric description and appearance feature extraction  is conventionally achieved by `bounding box + RoIPool (RoIAlign)`. The bounding box representation is convenient to use; however  it provides only a rectangular localization of objects that lacks geometric precision and may consequently degrade feature quality. Our new representation  RepPoints  models objects by a `point set` instead of a `bounding box`  which learns to adaptively position themselves over an object in a manner that circumscribes the object’s `spatial extent` and enables `semantically aligned feature extraction`. This richer and more flexible representation maintains the convenience of bounding boxes while facilitating various visual understanding applications. This repo demonstrated the effectiveness of RepPoints for COCO object detection.  Another feature of this repo is the demonstration of an `anchor-free detector`  which can be as effective as state-of-the-art anchor-based detection methods. The anchor-free detector can utilize either `bounding box` or `RepPoints` as the basic object representation.  <div align=""center"">   <img src=""demo/reppoints.png"" width=""400px"" />   <p>Learning RepPoints in Object Detection.</p> </div>   """;Computer Vision;https://github.com/muditchaudhary/RepPoints-x-Libra-R-CNN-x-Transformer-self-attention
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/hellotrik/torch_yolov3
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/LONG-9621/PointNet
"""This repository contains an implementation of a Resnet-32 model as described in the paper ""Deep Residual Learning for Image Recognition"" (http://arxiv.org/abs/1512.03385) to solve Intel's Image Scene Classification.     """;General;https://github.com/Olayemiy/Image-Classification-With-Resnet
"""This repository contains an implementation of a Resnet-32 model as described in the paper ""Deep Residual Learning for Image Recognition"" (http://arxiv.org/abs/1512.03385) to solve Intel's Image Scene Classification.     """;Computer Vision;https://github.com/Olayemiy/Image-Classification-With-Resnet
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/Harut0726/my-pointnet-tensorflow
"""This project is about using the [Sign Language Digits Dataset](https://www.kaggle.com/ardamavi/sign-language-digits-dataset/data) to classify images of sign language digits. This is similar to the MNIST dataset that has been used throughout the years to classify a grayscale  handwritten digits between 0 to 9.   """;Computer Vision;https://github.com/francislata/Sign-Language-Digits-CNN
"""The following table summarizes the data provided  detailing the requested target variables and all other variables provided. For a detailed explanation of each of the variables  see the link provided in the Introduction. Please  in order to keep the size of the provided prediction files small  just deliver them in uint16 format with each variable within its own range. When computing the score   we will divide each channel by its maximum value to have them in the interval [0  1]:  Name | Folder | Variables | Target Variable | type | Range | Scale Factor | Offset :----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:| Temperature at Ground/Cloud | CTTH |  `temperature` <br /> `ctth_tempe` <br /> `ctth_pres` <br /> `ctth_alti` <br /> `ctth_effectiv` `ctth_method` <br /> `ctth_quality` <br /> `ishai_skt` <br /> `ishai_quality` | `temperature` | uint16 | 0-11000 | 10 | 0 Convective Rainfall Rate | CRR | `crr` <br /> `crr_intensity` <br /> `crr_accum` <br /> `crr_quality` | `crr_intensity` | uint16 | 0-500 | 0.1 | 0 Probability of Occurrence of Tropopause Folding | ASII | `asii_turb_trop_prob` <br /> `asiitf_quality` | `asii_turb_trop_prob` | uint8 | 0-100 | 1 | 0 Cloud Mask | CMA | `cma_cloudsnow` <br /> `cma` <br /> `cma_dust` <br /> `cma_volcanic` <br /> `cma_smoke` <br /> `cma_quality` | `cma` | uint8 | 0-1 | 1 | 0 Cloud Type | CT | `ct` <br /> `ct_cumuliform` <br /> `ct_multilayer` <br /> `ct_quality` | `None` | uint8 | `None` | `None` | `None`  Cloud Type is provided since it has rich information that might help the models but no variable is required from this product. For the other products  we expect predictions for [`temperature  crr_intensity  asii_turb_trop_prob  cma`] in this order for the channel dimension in the submitted tensor.  Data obtained in collaboration with AEMet - Agencia Estatal de Meteorología/ NWC SAF.   The aim of our core competition is to predict the next 32 images (8h ahead in 15 minutes intervals) in our weather movies  which encode four different variables: (i) Temperature from the [Cloud Top Temperature](https://www.nwcsaf.org/ctth2) or the ground [Skin Temperature](https://www.nwcsaf.org/ishai_description) if there are no clouds  (ii) [Convective Rainfall Rate](https://www.nwcsaf.org/crr3)  (iii) [Probability of Occurrence of Tropopause Folding](https://www.nwcsaf.org/asii-tf)  and (iv) [Cloud Mask](https://www.nwcsaf.org/cma3). Each image is an observation of these 4 channels in a 15 minutes period where pixels correspond to a spatial area of ~3km x 3km  and there are 11 regions of 256 x 256 pixels to provide test predictions. From these regions  5 of them contain also training and validation data for learning purposes but the other 6 only inference is requested  to assess the Transfer Learning capabilities of models.  ![Regions](/images/IEEE_BigData_regions.png?raw=true ""Train/Validation/test Regions"")  The submission format in each day of the test set is a multi-dimensional array (tensor) of shape (32  4  256  256) and the objective function of all submitted tensors (one for each day in the test set and region) is the **mean squared error** of all pixel channel values to pixel channel values derived from true observations. We note that we normalize these pixel channel values to lie between 0 and 1 by dividing the pixel channel value by the maximum value of each variable (see below).  There are **two competitions** running in parallel that expect independent submission (participants can join one or both of them): - [Core Competition](https://www.iarai.ac.at/weather4cast/competitions/ieee-big-data-core/): Train your models on these regions with the provided data and submit predictions on the test subset. - [Transfer Learning Competition](https://www.iarai.ac.at/weather4cast/competitions/ieee-big-data-transfer-learning/): Only the test subset is provided for these regions  test the generalization capacity of your models.   """;Computer Vision;https://github.com/iarai/weather4cast
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/gmt710/caffe-ssd
"""   First  you can go to the official site(https://www.cityscapes-dataset.com/downloads/).    Then  download `leftImg8bit_trainvaltest.zip`(11GB) and `gtFine_trainvaltest.zip`(241MB). Make sure that you've registered your email on this site before downloading and that email is not a gmail account.   I personally used 2975 images as train data  500 images as both validation and test data. I saved configuration details in files located at `Csv` dir.   Pyramid Scene Parsing Network (PSPNet) is the model architecture proposed by this paper(https://arxiv.org/abs/1612.01105).     <img src=""https://user-images.githubusercontent.com/51239551/98388785-8ac0b380-2096-11eb-8a61-44401b1ec8b6.png"" width=""230""/> <img src=""https://user-images.githubusercontent.com/51239551/98388903-af1c9000-2096-11eb-88bf-fd5ce39b1d2c.png"" width=""230""/> <img src=""https://user-images.githubusercontent.com/51239551/98388922-b5ab0780-2096-11eb-920b-f768001eb05e.png"" width=""230""/>      Input Image(left)  Output Image(center)  Ground Truth(right)   """;Computer Vision;https://github.com/Rintarooo/PSPNet
"""<p float=""center"">     <img src=""figures/eca_module.jpg"" width=""1000"" alt=""Struct."">     <br>     <em>Structural comparison of SE and ECA attention mechanism.</em> </p>  Efficient Channel Attention (ECA) is a simple efficient extension of the popular Squeeze-and-Excitation Attention Mechanism  which is based on the foundation concept of Local Cross Channel Interaction (CCI). Instead of using fully-connected layers with reduction ratio bottleneck as in the case of SENets  ECANet uses an adaptive shared (across channels) 1D convolution kernel on the downsampled GAP *C* x 1 x 1 tensor. ECA is an equivalently plug and play module similar to SE attention mechanism and can be added anywhere in the blocks of a deep convolutional neural networks. Because of the shared 1D kernel  the parameter overhead and FLOPs cost added by ECA is significantly lower than that of SENets while achieving similar or superior performance owing to it's capabilities of constructing adaptive kernels. This work was accepted at the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)  2020.    """;General;https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET
"""<p float=""center"">     <img src=""figures/eca_module.jpg"" width=""1000"" alt=""Struct."">     <br>     <em>Structural comparison of SE and ECA attention mechanism.</em> </p>  Efficient Channel Attention (ECA) is a simple efficient extension of the popular Squeeze-and-Excitation Attention Mechanism  which is based on the foundation concept of Local Cross Channel Interaction (CCI). Instead of using fully-connected layers with reduction ratio bottleneck as in the case of SENets  ECANet uses an adaptive shared (across channels) 1D convolution kernel on the downsampled GAP *C* x 1 x 1 tensor. ECA is an equivalently plug and play module similar to SE attention mechanism and can be added anywhere in the blocks of a deep convolutional neural networks. Because of the shared 1D kernel  the parameter overhead and FLOPs cost added by ECA is significantly lower than that of SENets while achieving similar or superior performance owing to it's capabilities of constructing adaptive kernels. This work was accepted at the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)  2020.    """;Computer Vision;https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET
"""**OCR-RCNN** is designed for elevator button recognition task based on [Faster RCNN](http://arxiv.org/abs/1506.01497)  which includes a Region Proposal Network (RPN)  an Object Detection Network and a Character Recognition Network. This framework aims to help solve inter-floor navigation problem of service robots.  In this package  a **button recognition service** is implemented based on a trained OCR-RCNN model. The service takes a raw image as input and returns the detection  localization and character recognition results. Besides  a **Multi-Tracker** is also implemented  which utilizes the outputs of recognition service to initialize the tracking process  yielding an on-line detection performance.  If you find it helpful to your project  please consider cite our paper:  ``` @inproceedings{zhu2018novel    title={A Novel OCR-RCNN for Elevator Button Recognition}    author={Zhu  Delong and Li  Tingguang and Ho  Danny and Zhou  Tong and Meng  Max QH}    booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}    pages={3626--3631}    year={2018}    organization={IEEE} } @ARTICLE{9324975    author={D. {Zhu} and Y. {Fang} and Z. {Min} and D. {Ho} and M. Q. . -H. {Meng}}    journal={IEEE Transactions on Industrial Electronics}     title={OCR-RCNN: An Accurate and Efficient Framework for Elevator Button Recognition}     year={2021}    volume={}    number={}    pages={1-1}    doi={10.1109/TIE.2021.3050357}} ```    """;Computer Vision;https://github.com/zhudelong/elevator_button_recognition
"""[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1hYHb0FTdKQCXZs3qCwVZnSuVGrZU2Z1w?usp=sharing)  It was in January of 2021 that **OpenAI** announced two new models: **DALL-E** and **CLIP**  both **multi-modality** models connecting **texts and images** in some way. In this article we are going to implement CLIP model from scratch in **PyTorch**. OpenAI has open-sourced some of the code relating to CLIP model but I found it intimidating and it was far from something short and simple. I also came across a good tutorial inspired by CLIP model on Keras code examples and I translated some parts of it into PyTorch to build this tutorial totally with our beloved PyTorch!  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1hYHb0FTdKQCXZs3qCwVZnSuVGrZU2Z1w?usp=sharing)   """;Computer Vision;https://github.com/moein-shariatnia/OpenAI-CLIP
"""Official Implementation of our ReStyle paper for both training and evaluation. ReStyle introduces an iterative refinement mechanism which can be applied over different StyleGAN encoders for solving the StyleGAN inversion task.    """;Computer Vision;https://github.com/yuval-alaluf/restyle-encoder
"""Convolutional Neural Networks (CNNs) and transformers are two mainstream model architectures currently dominating computer vision and natural language processing. The authors of the paper  however  empirically show that neither convolution nor self-attenion are necessary; in fact  muti-layered perceptrons (MLPs) can also serve as a strong baseline. The authors present MLP-Mixer  an all-MLP mode architecture  that contains two types of layers: a token-mixing layer and a channel-mixing layer. Each of the layers ""mix"" per-location and per-feature information in the input. MLP-Mixer performs comparably to other state-of-the-art models  such as [ViT](https://arxiv.org/abs/2010.11929) or [EfficientNet](https://arxiv.org/abs/1905.11946).   """;Computer Vision;https://github.com/jaketae/mlp-mixer
"""To submit a multi-GPU job  use the `submit_pm.sh` with the `-n` option set to the desired number of GPUs. For example  to launch a training with multiple GPUs  you will use commands like: ``` sbatch -n NUM_GPU submit_pm.sh [OPTIONS] ``` This script automatically uses the slurm flags `--ntasks-per-node 4`  `--cpus-per-task 32`  `--gpus-per-task 1`  so slurm will allocate one process for each GPU we request  and give each process 1/4th of the CPU resources available on a Perlmutter GPU node. This way  multi-node trainings can easily be launched simply by setting `-n` greater than 4.  *Question: why do you think we run 1 task (cpu process) per GPU  instead of 1 task per node (each running 4 GPUs)?*  PyTorch `DistributedDataParallel`  or DDP for short  is flexible and can initialize process groups with a variety of methods. For this code  we will use the standard approach of initializing via environment variables  which can be easily read from the slurm environment. Take a look at the `export_DDP_vars.sh` helper script  which is used by our job script to expose for PyTorch DDP the global rank and node-local rank of each process  along with the total number of ranks and the address and port to use for network communication. In the [`train.py`](train.py) script  near the bottom in the main script execution  we set up the distributed backend using these environment variables via `torch.distributed.init_proces_group`.  When distributing a batch of samples in DDP training  we must make sure each rank gets a properly-sized subset of the full batch. See if you can find where we use the `DistributedSampler` from PyTorch to properly partition the data in [`utils/data_loader.py`](utils/data_loader.py). Note that in this particular example  we are already cropping samples randomly form a large simulation volume  so the partitioning does not ensure each rank gets unique data  but simply shortens the number of steps needed to complete an ""epoch"". For datasets with a fixed number of unique samples  `DistributedSampler` will also ensure each rank sees a unique minibatch.  In `train.py`  after our U-Net model is constructed  we convert it to a distributed data parallel model by wrapping it as: ``` model = DistributedDataParallel(model  device_ids=[local_rank]) ```  The DistributedDataParallel (DDP) model wrapper takes care of broadcasting initial model weights to all workers and performing all-reduce on the gradients in the training backward pass to properly synchronize and update the model weights in the distributed setting.  *Question: why does DDP broadcast the initial model weights to all workers? What would happen if it didn't?*   """;Computer Vision;https://github.com/NERSC/sc21-dl-tutorial
"""This open source book is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See [LICENSE](LICENSE) file.  The sample and reference code within this open source book is made available under a modified MIT license. See the [LICENSE-SAMPLECODE](LICENSE-SAMPLECODE) file.  [Chinese version](https://github.com/d2l-ai/d2l-zh) | [Discuss and report issues](https://discuss.d2l.ai/) | [Code of conduct](CODE_OF_CONDUCT.md) | [Other Information](INFO.md)   """;Computer Vision;https://github.com/d2l-ai/d2l-en
"""YOLOX is an anchor-free version of YOLO  with a simpler design but better performance! It aims to bridge the gap between research and industrial communities. For more details  please refer to our [report on Arxiv](https://arxiv.org/abs/2107.08430).  This repo is an implementation of PyTorch version YOLOX  there is also a [MegEngine implementation](https://github.com/MegEngine/YOLOX).  <img src=""assets/git_fig.png"" width=""1000"" >   """;Computer Vision;https://github.com/Megvii-BaseDetection/YOLOX
"""• main.py script creates the whole model consiting of localisation network  Grid generator and sampler and Recognition network.<br /> • stn_network.py script crates spatial transformer network  Grid genrator and bilinearsampler.<br /> • resnet_stn.py script creates detection and recognition resnet network as proposed by the author.<br />   """;Computer Vision;https://github.com/vinod377/STN-OCR-Tensorflow
"""This is the implementation for [Anchor-Free Person Search](https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_Anchor-Free_Person_Search_CVPR_2021_paper.pdf) in CVPR2021 and its extended version [Efficient Person Search: An Anchor-Free Approach](https://arxiv.org/abs/2109.00211).  A brief introduction in Chinese can be found at https://zhuanlan.zhihu.com/p/359617800  ![demo image](demo/arch.jpg)    """;Computer Vision;https://github.com/daodaofr/AlignPS
"""The economy of online shopping is bigger than it has ever been and continues to grow each year. It follows that the market for being able to deliver goods quickly and reliably is becoming more and more competitive. In addition to improving the overall flow of transactions  knowing when a package will be delivered is a major factor in customer satisfaction  making the ability to accurately predict delivery dates essential to companies such as eBay.  The process of achieving this  however  poses many challenges. In the case of eBay  the majority of transactions are carried out between individual sellers and buyers  often resulting in the data for goods and their delivery being inconsistently recorded. This  in addition to packages being processed by a variety of different delivery services  means that data labels are frequently missing or incorrect. Further  the shipment date is largely left to the sole decision of each individual seller  resulting in a high degree of variability.  ![image of shipping process](/images/diagram2.png)  We worked to provide a solution to these problems and provide a model to enable the accurate prediction of delivery dates using machine learning.  To implement our model  a number of decisions were made and tested  including deciding the optimal means by which to clean the data (for instance  whether to omit training data that has missing or incorrect features or to assign it average values based on correctly labeled training data)  deciding whether to compute the estimation holistically from shipment to delivery date or to compute multiple separate estimates on separate legs of the delivery process  and deciding which features to include and in which leg.  Should our model have some error  it is important that it produces random rather than systematic error. Specifically  we want to avoid creating a model which might consistently predict early delivery dates  which could lead to sellers and delivery services rushing packages and resulting in the employment of more non-sustainable methods  such as shipping half-full boxes  as well as increasing the pressure on employees to have to work faster and faster.  Using a loss function of the weighted average absolute error of the delivery predictions in days that was provided by eBay  our model achieved a loss of 0.411 where a loss of 0.759 was the baseline of random guessing.    """;Computer Vision;https://github.com/milliemince/eBay-shipping-predictions
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Nstats/bert_MRC
"""The RadioTalk corpus is in JSONL format  with one json document per line. Each line represents one ""snippet"" of audio  may contain multiple sentences  and is represented as a dictionary object with the following keys: * `content`: The transcribed speech from the snippet. * `callsign`: The call letters of the station the snippet aired on. * `city`: The city the station is based in  as in FCCC filings. * `state`: The state the station is based in  as in FCCC filings. * `show_name`: The name of the show containing this snippet. * `signature`: The initial 8 bytes of an MD5 hash of the `content` field  after lowercasing and removing English stopwords (specifically the NLTK stopword list)  intended to help with deduplication. * `studio_or_telephone`: A flag for whether the underlying audio came from a telephone or studio audio equipment. (The most useful feature in distinguishing these is the [narrow frequency range](https://en.wikipedia.org/wiki/Plain_old_telephone_service#Characteristics) of telephone audio.) * `guessed_gender`: The imputed speaker gender. * `segment_start_time`: The Unix timestamp of the beginning of the underlying audio. * `segment_end_time`: The Unix timestamp of the end of the underlying audio. * `speaker_id`: A diarization ID for the person speaking in the audio snippet. * `audio_chunk_id`: An ID for the audio chunk this snippet came from (each chunk may be split into multiple snippets).  An example snippet from the corpus (originally on one line but pretty-printed here for readability): ``` {     ""content"": ""This would be used for housing programs and you talked a little bit about how the attorney""      ""callsign"": ""KABC""      ""city"": ""Los Angeles""      ""state"": ""CA""      ""show_name"": ""The Drive Home With Jillian Barberie & John Phillips""      ""signature"": ""afd7d2ee""      ""studio_or_telephone"": ""T""      ""guessed_gender"": ""F""      ""segment_start_time"": 1540945402.6      ""segment_end_time"": 1540945408.6      ""speaker_id"": ""S0""      ""audio_chunk_id"": ""2018-10-31/KABC/00_20_28/16"" } ```   """;Natural Language Processing;https://github.com/social-machines/RadioTalk
"""This repo uses *MaskRCNN* as the baseline method for Instance segmentation and Object detection. We use the [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark) as the baseline.   [Res2Net](https://github.com/gasvn/Res2Net) is a powerful backbone architecture that can be easily implemented into state-of-the-art models by replacing the bottleneck with Res2Net module. More detail can be found on [ ""Res2Net: A New Multi-scale Backbone Architecture""](https://arxiv.org/pdf/1904.01169.pdf) and our [project page](https://mmcheng.net/res2net) .    """;Computer Vision;https://github.com/Res2Net/Res2Net-maskrcnn
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Shinya-Kouda/kgc
"""This repository implements a MAML example for sinusoid regression in Flax. The idea of MAML is to learn the initial weight values of a model that can quickly adapt to new tasks. For more information  check the [paper](https://arxiv.org/abs/1703.03400).  This implementation uses only default Flax components like `flax.nn.Model` and `flax.nn.Module`  showing that this kind of optimization-based Meta Learning algorithms can easily be implemented in Flax/JAX.  It is based on the [MAML implementation in JAX by Eric Jang](https://blog.evjang.com/2019/02/maml-jax.html) and updated to use Flax components. I have only implemented the sinusoid example so far  but I intend to add the Omniglot example too.  There is also an implementation of a model that fits just to one sinusoid  without meta learning  useful to see the difference between the two approaches. This approach is implemented in `main_wo_maml.py`.   """;General;https://github.com/gcucurull/maml_flax
"""This repository provides a multi-thread implementation of node2vec random walk  with alias table based on LRU cache  it can process with limited memory usage  so that walking through a giant graph on a single machine can be possible.  Tested for a graph that contains 23 thousand nodes and 23 million edges  with parameter  `--walk_length=80 --num_walks=10 --workers=20 --max_nodes=50000 --max_edges=100000 --p=10 --q=0.01`  only 11GB memory used  and finished walking within 2 hours.  Visit https://blog.razrlele.com/p/2650 for more.   """;Graphs;https://github.com/razrLeLe/fastwalk
"""A general YOLOv4/v3/v2 object detection pipeline inherited from [keras-yolo3-Mobilenet](https://github.com/Adamdad/keras-YOLOv3-mobilenet)/[keras-yolo3](https://github.com/qqwweee/keras-yolo3) and [YAD2K](https://github.com/allanzelener/YAD2K). Implement with tf.keras  including data collection/annotation  model training/tuning  model evaluation and on device deployment. Support different architecture and different technologies:   """;Computer Vision;https://github.com/grifon-239/diploma
"""This project is based on our CVPR2019 paper. You can find the [arXiv](https://arxiv.org/abs/1811.07246) version here.  ``` @inproceedings{wu2019pointconv    title={Pointconv: Deep convolutional networks on 3d point clouds}    author={Wu  Wenxuan and Qi  Zhongang and Fuxin  Li}    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}    pages={9621--9630}    year={2019} } ```  Unlike images which are represented in regular dense grids  3D point clouds are irregular and unordered  hence applying convolution on them can be difficult. In this paper  we extend the dynamic filter to a new convolution operation  named PointConv. PointConv can be applied on point clouds to build deep convolutional networks. We treat convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight and density functions. With respect to a given point  the weight functions are learned with multi-layer perceptron networks and the density functions through kernel density estimation. A novel reformulation is proposed for efficiently computing the weight functions  which allowed us to dramatically scale up the network and significantly improve its performance. The learned convolution kernel can be used to compute translation-invariant and permutation-invariant convolution on any point set in the 3D space. Besides  PointConv can also be used as deconvolution operators to propagate features from a subsampled point cloud back to its original resolution. Experiments on ModelNet40  ShapeNet  and ScanNet show that deep convolutional neural networks built on PointConv are able to achieve state-of-the-art on challenging semantic segmentation benchmarks on 3D point clouds. Besides  our experiments converting CIFAR-10 into a point cloud showed that networks built on PointConv can match the performance of convolutional networks in 2D images of a similar structure.   """;Computer Vision;https://github.com/Young98CN/pointconv_pytorch
"""• main.py script creates the whole model consiting of localisation network  Grid generator and sampler and Recognition network.<br /> • stn_network.py script crates spatial transformer network  Grid genrator and bilinearsampler.<br /> • resnet_stn.py script creates detection and recognition resnet network as proposed by the author.<br />   """;General;https://github.com/vinod377/STN-OCR-Tensorflow
"""------------------ *BeautyGAN*: Instance-level Facial Makeup Transfer with Deep Generative Adversarial Network  - Website: [http://liusi-group.com/projects/BeautyGAN](http://liusi-group.com/projects/BeautyGAN)  *MobileNetV2*: Computer Vision and PatternRecognition  - Website: [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)  Usage  - Python 3.6+ - Tensorflow 1.x  Download pretrained models  - [https://pan.baidu.com/s/1wngvgT0qzcKJ5LfLMO7m8A](https://pan.baidu.com/s/1wngvgT0qzcKJ5LfLMO7m8A) - [https://drive.google.com/drive/folders/1pgVqnF2-rnOxcUQ3SO4JwHUFTdiSe5t9](https://drive.google.com/drive/folders/1pgVqnF2-rnOxcUQ3SO4JwHUFTdiSe5t9)  Save pretrained model  index  checkpoint to `models/model_p2`  ``` . +--docs +--mlib +--models |   +-- model_dlib |   +-- model_p2 -> make folder  here save models +--static ... ```  """;Computer Vision;https://github.com/eunki7/flask-deeplearning-service-demo
"""------------------ *BeautyGAN*: Instance-level Facial Makeup Transfer with Deep Generative Adversarial Network  - Website: [http://liusi-group.com/projects/BeautyGAN](http://liusi-group.com/projects/BeautyGAN)  *MobileNetV2*: Computer Vision and PatternRecognition  - Website: [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)  Usage  - Python 3.6+ - Tensorflow 1.x  Download pretrained models  - [https://pan.baidu.com/s/1wngvgT0qzcKJ5LfLMO7m8A](https://pan.baidu.com/s/1wngvgT0qzcKJ5LfLMO7m8A) - [https://drive.google.com/drive/folders/1pgVqnF2-rnOxcUQ3SO4JwHUFTdiSe5t9](https://drive.google.com/drive/folders/1pgVqnF2-rnOxcUQ3SO4JwHUFTdiSe5t9)  Save pretrained model  index  checkpoint to `models/model_p2`  ``` . +--docs +--mlib +--models |   +-- model_dlib |   +-- model_p2 -> make folder  here save models +--static ... ```  """;General;https://github.com/eunki7/flask-deeplearning-service-demo
"""  6 types of defects are made by photoshop  . The defects defined in the dataset are: missing hole  mouse bite  open circuit  short  spur  and spurious copper. The augmented dataset contains 10668 images and the corresponding annotation files.    """;Computer Vision;https://github.com/sudharshan2001/PCB-defect-detection-using-U-NET
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/fofore/yolov3-pyroch
"""Machine translation is a natural language processing task that aims to translate natural languages using computers automatically. Recent several years have witnessed the rapid development of end-to-end neural machine translation  which has become the new mainstream method in practical MT systems.  THUMT is an open-source toolkit for neural machine translation developed by [the Natural Language Processing Group at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/site2/index.php?lang=en). The website of THUMT is: [http://thumt.thunlp.org/](http://thumt.thunlp.org/).   """;General;https://github.com/THUNLP-MT/THUMT
"""Machine translation is a natural language processing task that aims to translate natural languages using computers automatically. Recent several years have witnessed the rapid development of end-to-end neural machine translation  which has become the new mainstream method in practical MT systems.  THUMT is an open-source toolkit for neural machine translation developed by [the Natural Language Processing Group at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/site2/index.php?lang=en). The website of THUMT is: [http://thumt.thunlp.org/](http://thumt.thunlp.org/).   """;Natural Language Processing;https://github.com/THUNLP-MT/THUMT
"""The current code trains an RNN ([Gated Recurrent Units](https://arxiv.org/abs/1406.1078)) to predict  at each timestep (i.e. visit)  the diagnosis codes occurring in the next visit. This is denoted as *Sequential Diagnoses Prediction* in the paper.  In the future  we will relases another version for making a single prediction for the entire visit sequence. (e.g. Predict the onset of heart failure given the visit record)  Note that the current code uses [Multi-level Clinical Classification Software for ICD-9-CM](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp) as the domain knowledge. We will release the one that uses ICD9 Diagnosis Hierarchy in the future. 	  """;Sequential;https://github.com/mp2893/gram
"""The goal of this tasks consists of generating a text about a specific topic or situation. We used two models in this experiment: GPT-2 and a Transformer model.  On the one hand  despite GPT-2 was only trained to predict the next token in a sentence  it surprisingly learned basic competence in some tasks like translating between languages and answering questions. On the other hand  we implemented a basic Transformer encoder-decoder architecture to analyse whether it is able to generate a similar text to the training data.   In Question-Answering tasks  the model receives a text and a question regarding to the text  and it should mark the beginning and end of the answer in the text.  In this case  it is necessary to fine-tune BERT. We will use XQuAD (Cross-lingual Question Answering Dataset) to fine-tune it  which consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 together with their professional translations into 10 languages. Source: https://github.com/deepmind/xquad   BERT's authors trained it in two tasks and one of them is **Masked Language Model** (MLM). The task consists of mask some input tokens randomly  and then try to predict those random tokens. Hence  we do not need to pretrain BERT and we can directly test it.  In this repository  I have implemented **a basic script to infer the masked token** in a sentence.  Read the BERT paper: https://arxiv.org/pdf/1810.04805.pdf   """;Natural Language Processing;https://github.com/DimasDMM/transformers
"""The objective of the project is to train the agents to play Tennis using the [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment.   ![tennis](images/trained_agent.gif)  """;Reinforcement Learning;https://github.com/vgudapati/DRLND_Collaboration_Competetion
"""---  Reinforcement learning (RL) is an area of machine learning where an agent aims to make the optimal decision in an uncertain environment in order to get the maximum cumulative reward. Since RL requires an agent to operate and learn in its environment  it's often easier to implement agents inside  simulations on computers than in real world situations.   A common technique to test various RL algorithms is to use an existing video game where the agent learns from its interactions with the game. Video games are an ideal environment to test RL algorithms because (1) they provide complex problems for the agent to solve  (2) they provide safe  controllable environments for the agent to act in  and (3) they provide instantaneous feedback which makes training agents much faster.  For the reasons listed above  we have chosen to implement an RL agent on the classic game Snake. Snake is an arcade game originally released in 1976 where the player maneuvers a snake-like line to eat food that both increases their score and the size of the snake. The game ends when the snake collides with a wall or with itself. The score of the game is simply the number of food the player's collected. Since each food collected increases the size of the snake  the game gets increasingly difficult as time goes on.   We chose to use Deep Q-Learning as our algorithm to allow our agent to make optimal decisions solely from raw input data. No rules about the Snake game are given to the agent. This approach consists of solely giving the agent information about the state of the game and providing either positive or negative feedback based on the actions it takes. Our agent was more performant than we expected  showing strategy and rapid improvement after only 200 games. We will go into more detail of our results later in the paper.   """;Reinforcement Learning;https://github.com/sourenaKhanzadeh/snakeAi
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/chiayewken/bert-qa
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/chiayewken/bert-qa
"""The *Show and Tell* model is a deep neural network that learns how to describe the content of images. For example:  ![Example captions](g3doc/example_captions.jpg)   """;Computer Vision;https://github.com/stoensin/IC
"""The method introduced in this paper aims at helping deep learning practitioners faced with an overfit problem. The idea is to replace  in a multi-branch network  the standard summation of parallel branches with a stochastic affine combination. Applied to 3-branch residual networks  shake-shake regularization improves on the best single shot published results on CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%.  ![shake-shake](https://s3.eu-central-1.amazonaws.com/github-xg/architecture3.png)  Figure 1: **Left:** Forward training pass. **Center:** Backward training pass. **Right:** At test time.  Bibtex:  ``` @article{Gastaldi17ShakeShake     title = {Shake-Shake regularization}     author = {Xavier Gastaldi}     journal = {arXiv preprint arXiv:1705.07485}     year = 2017  } ```   """;General;https://github.com/xgastaldi/shake-shake
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/longbowking/bert
"""Code for the labs of the [Deep Learning course](https://uvadlc.github.io/) offered in MSc. in Artificial Intelligence at the University of Amsterdam.   """;Computer Vision;https://github.com/KrishnaTarun/Deep-Learning-Lab
"""Code for the labs of the [Deep Learning course](https://uvadlc.github.io/) offered in MSc. in Artificial Intelligence at the University of Amsterdam.   """;General;https://github.com/KrishnaTarun/Deep-Learning-Lab
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/qilei123/sod_v1
"""This repository contains titles scraped from different subreddits and the task is to identify which subreddit it came from. It is organized in .csv files. All data can be found in the zip file here: https://drive.google.com/file/d/1bpfz10fbHWR56W__Fcu-hcl7WqdAkVvk/view?usp=sharing  I have also segregated them into two lists: coarse grained list (17 subreddits)  fine-grained list (1416 subreddits). These lists are obtained via thresholding on the number of active users  and choosing only those which support text data in the subreddit. More details can be found in the report. The train  validation  test set for the coarse and fine grained can be found here: https://drive.google.com/open?id=16WTab0JQGfPzs3MSOKJjHksz_rbLAtNX   Lastly  we also provide code using TF-IDF and ULMFiT (https://arxiv.org/abs/1801.06146). The latter is provided in the form of Jupyter notebook for easy replication.  """;General;https://github.com/TheShadow29/subreddit-classification-dataset
"""This repository contains titles scraped from different subreddits and the task is to identify which subreddit it came from. It is organized in .csv files. All data can be found in the zip file here: https://drive.google.com/file/d/1bpfz10fbHWR56W__Fcu-hcl7WqdAkVvk/view?usp=sharing  I have also segregated them into two lists: coarse grained list (17 subreddits)  fine-grained list (1416 subreddits). These lists are obtained via thresholding on the number of active users  and choosing only those which support text data in the subreddit. More details can be found in the report. The train  validation  test set for the coarse and fine grained can be found here: https://drive.google.com/open?id=16WTab0JQGfPzs3MSOKJjHksz_rbLAtNX   Lastly  we also provide code using TF-IDF and ULMFiT (https://arxiv.org/abs/1801.06146). The latter is provided in the form of Jupyter notebook for easy replication.  """;Natural Language Processing;https://github.com/TheShadow29/subreddit-classification-dataset
"""Running vision tasks such as object detection  segmentation in real time on mobile devices. Our goal is to implement video segmentation in real time at least 24 fps on Google Pixel 2. We use efficient deep learning network specialized in mobile/embedded devices and exploit data redundancy between consecutive frames to reduce unaffordable computational cost. Moreover  the network can be optimized with 8-bits quantization provided by tf-lite.  ![Real-Time Video Segmentation(Credit: Google AI)](https://raw.githubusercontent.com/tantara/JejuNet/master/docs/real_time_video_segmentation_google_ai.gif)  *Example: Reai-Time Video Segmentation(Credit: Google AI)*   """;Computer Vision;https://github.com/tantara/JejuNet
"""Running vision tasks such as object detection  segmentation in real time on mobile devices. Our goal is to implement video segmentation in real time at least 24 fps on Google Pixel 2. We use efficient deep learning network specialized in mobile/embedded devices and exploit data redundancy between consecutive frames to reduce unaffordable computational cost. Moreover  the network can be optimized with 8-bits quantization provided by tf-lite.  ![Real-Time Video Segmentation(Credit: Google AI)](https://raw.githubusercontent.com/tantara/JejuNet/master/docs/real_time_video_segmentation_google_ai.gif)  *Example: Reai-Time Video Segmentation(Credit: Google AI)*   """;General;https://github.com/tantara/JejuNet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/longbowking/bert
"""The file [acquisition.cpp](https://github.com/kinik93/Spatial-Transformer-Network-application/blob/master/acquisition.cpp) has the following dependecies: * [DepthSense SDK](https://www.sony-depthsensing.com/Support/DownloadLegacyDriver) * [OPENCV 2.4.9](https://opencv.org/releases.html)  and obviously a depth camera like *Senz3d* or *DepthSense*.  An example of depth frames (8 bit PNG encoding) acquisition is shown in the following gif:  ![Alt Text](https://github.com/kinik93/Spatial-Transformer-Network-application/blob/master/makingOfDataset.gif)   """;Computer Vision;https://github.com/kinik93/Spatial-Transformer-Network-application
"""Parking Space Detection is a very important task in the field of computer vision as there is a general dearth of parking spaces toady and it takes time to explore the parking spaces as parking spots start filling up. By solving this problem we can reduce the car emissions in urban centers by reducing the need for people to needlesly circle city blocks for parking. It also permits cities to carefully manage their parking supply and finally it reduces the daily stress associated with parking spaces.  My implementation is based on the methodology that with the continous advances in the field of computer vision a lot of new and more efficient and accurate models are being introduced and we should be able to use these models directly off the shelf without any fine-tuning. So I present my unsupervised method of doing parking detection.   """;Computer Vision;https://github.com/KushajveerSingh/Unsupervised-Parking-Lot-Detection
"""Code for the labs of the [Deep Learning course](https://uvadlc.github.io/) offered in MSc. in Artificial Intelligence at the University of Amsterdam.   """;Natural Language Processing;https://github.com/KrishnaTarun/Deep-Learning-Lab
"""In this project  you'll label the pixels of a road in images using a Fully Convolutional Network (FCN).   """;Computer Vision;https://github.com/hgjeon/CarND-Semantic-Segmentation
"""CenterNet is a framework for object detection with deep convolutional neural networks. You can use the code to train and evaluate a network for object detection on the MS-COCO dataset.  * It achieves state-of-the-art performance (an AP of 47.0%) on one of the most challenging dataset: MS-COCO.  * Our code is written in Python  based on [CornerNet](https://github.com/princeton-vl/CornerNet).  *More detailed descriptions of our approach and code will be made available soon.*  **If you encounter any problems in using our code  please contact Kaiwen Duan: kaiwen.duan@vipl.ict.ac.cn.**   """;Computer Vision;https://github.com/guohaoyuan/CenterNet-annotation
"""You can find an introduction to single-image super-resolution in [this article](https://krasserm.github.io/2019/09/04/super-resolution/).  It also demonstrates how EDSR and WDSR models can be fine-tuned with SRGAN (see also [this section](#srgan-for-fine-tuning-edsr-and-wdsr-models)).   """;General;https://github.com/krasserm/super-resolution
"""You can find an introduction to single-image super-resolution in [this article](https://krasserm.github.io/2019/09/04/super-resolution/).  It also demonstrates how EDSR and WDSR models can be fine-tuned with SRGAN (see also [this section](#srgan-for-fine-tuning-edsr-and-wdsr-models)).   """;Computer Vision;https://github.com/krasserm/super-resolution
"""Super-resolution (SR) of images refers to the process of generating or reconstructing the high- resolution (HR) images from low-resolution images (LR). This project mainly focuses on dealing with this problem of super-resolution using the generative adversarial network  named SRGAN  a deep learning framework. In this project  SRGAN was trained and evaluated using 'DIV2K'  ‘MS-COCO’ and ‘VID4’ [6] which are the popular datasets for image resolution tasks.  In total  datasets were merged to form:   1. 5800 training images  2. 100 validation images  3. 4 videos for testing  Apart from the datasets mentioned above  ‘LFW’  ‘Set5’ and ‘Set14’ datasets [6] were used to get inferences and compare the performance of models implemented in this project with the models from Ledig et al. [2].  Most of this project is built upon the ideas of Ledig et al [2]. Apart from that  I did some research on comparing the results obtained using different objective functions available in TensorFlow’s “TFGAN” library for loss optimizations of SRGAN. Different model implementations were evaluated for pixel quality through the peak signal-to-noise ratio (PSNR) scores as a metric. Intuitively  this metric does not capture the essence of the perceptual quality of an image. However  it is comparatively easy to use PSNR when evaluating the performance while training the model compared to mean-opinion-score (MOS) that has been used by Ledig et al [2]. To evaluate the perceptual quality of images  I have compared the generated images from both the models. This paper also proposes a method of super-resolution using SRGAN with “Per-Pix loss” which I defined in the losses section of this paper. Based on results from [2] and [5]  I have combined both MSE and VGG losses  named it “Per-Pix loss” that stands for ‘Perceptual and Pixel’ qualities of the image  which resulted in preserving the pixel quality besides improving the perceptual quality of images. Finally  I have compared the models built in this project with the models from Ledig et al. [2] to know the performance and effectiveness of models implemented in this project.   """;General;https://github.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN
"""Super-resolution (SR) of images refers to the process of generating or reconstructing the high- resolution (HR) images from low-resolution images (LR). This project mainly focuses on dealing with this problem of super-resolution using the generative adversarial network  named SRGAN  a deep learning framework. In this project  SRGAN was trained and evaluated using 'DIV2K'  ‘MS-COCO’ and ‘VID4’ [6] which are the popular datasets for image resolution tasks.  In total  datasets were merged to form:   1. 5800 training images  2. 100 validation images  3. 4 videos for testing  Apart from the datasets mentioned above  ‘LFW’  ‘Set5’ and ‘Set14’ datasets [6] were used to get inferences and compare the performance of models implemented in this project with the models from Ledig et al. [2].  Most of this project is built upon the ideas of Ledig et al [2]. Apart from that  I did some research on comparing the results obtained using different objective functions available in TensorFlow’s “TFGAN” library for loss optimizations of SRGAN. Different model implementations were evaluated for pixel quality through the peak signal-to-noise ratio (PSNR) scores as a metric. Intuitively  this metric does not capture the essence of the perceptual quality of an image. However  it is comparatively easy to use PSNR when evaluating the performance while training the model compared to mean-opinion-score (MOS) that has been used by Ledig et al [2]. To evaluate the perceptual quality of images  I have compared the generated images from both the models. This paper also proposes a method of super-resolution using SRGAN with “Per-Pix loss” which I defined in the losses section of this paper. Based on results from [2] and [5]  I have combined both MSE and VGG losses  named it “Per-Pix loss” that stands for ‘Perceptual and Pixel’ qualities of the image  which resulted in preserving the pixel quality besides improving the perceptual quality of images. Finally  I have compared the models built in this project with the models from Ledig et al. [2] to know the performance and effectiveness of models implemented in this project.   """;Computer Vision;https://github.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN
"""**R-FCN** is a region-based object detection framework leveraging deep fully-convolutional networks  which is accurate and efficient. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region sub-network hundreds of times  our region-based detector is fully convolutional with almost all computation shared on the entire image. R-FCN can natually adopt powerful fully convolutional image classifier backbones  such as [ResNets](https://github.com/KaimingHe/deep-residual-networks)  for object detection.  R-FCN was initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).  This code has been tested on Windows 7/8 64 bit  Windows Server 2012 R2  and Ubuntu 14.04  with Matlab 2014a.   """;Computer Vision;https://github.com/ishanashastri/RFCN-Breast-Cancer
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/flyliu2017/bert
"""[BERT](https://github.com/google-research/bert) is a pre-trained language model proposed by Google AI in 2018. It has achieved excellent results in many tasks in the NLP field  and it is also a turning point in the NLP field.  academic paper which describes BERT in detail and provides full results on a number of tasks can be found here:https://arxiv.org/abs/1810.04805.  Although after bert  a number of excellent models that have swept the NLP field  such as [RoBERTa](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md)  [XLNet](https://github.com/zihangdai/xlnet)  etc.  have also been improved on the basis of BERT.  BERT-Classifier is a general text classifier that is simple and easy to use. It has been improved on the basis of BERT and supports three paragraphs of sentences as input for prediction. The low-level API was used to reconstruct the overall pipline  which effectively solved the problem of weak flexibility of the tensorflow estimator. Optimize the training process  effectively reduce the model initialization time  solve the problem of repeatedly reloading the calculation graph during the estimator training process  and add a wealth of monitoring indicators during training  including (precision  recall  AUC  ROC curve  Confusion Matrix  F1 score  learning rate  loss  etc.)  which can effectively monitor model changes during training.  BERT-Classifier takes full advantage of the Python multi-process mechanism  multi-core speeds up the data preprocessing process  and the data preprocessing speed is more than 10 times faster than the original bert run_classifier (the specific speed increase is related to the number of CPU cores  frequency  and memory size).  Optimized the model checkpoint saving mechanism  which can save TOP N checkpoints according to different indicators  and adds the checkpoint average function  which can fuse model parameters generated in multiple different stages to further enhance model robustness.  It also supports packaging the trained models into services for use by downstream tasks.   """;Natural Language Processing;https://github.com/guoyaohua/BERT-Classifier
"""  """;General;https://github.com/OFRIN/Tensorflow_FixMatch
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/fanfancy/caffeSSD_compress
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/mithunpaul08/bert_tensorflow
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/mithunpaul08/bert_tensorflow
"""UmBERTo inherits from RoBERTa base model architecture which improves the initial BERT by identifying key hyperparameters for better results. Umberto extends Roberta and uses two innovative approaches: ***SentencePiece*** and ***Whole Word Masking***. SentencePiece Model (**SPM**) is a language-independent subword tokenizer and detokenizer designed for Neural-based text processing and creates sub-word units specifically to the size of the chosen vocabulary and the language of the corpus.  Whole Word Masking (**WWM**) applies mask to an entire word  if at least one of all tokens created by SentencePiece Tokenizer was originally chosen as mask. So only entire word are masked  not subwords.  Two models are released:   - **umberto-wikipedia-uncased-v1**  an uncased model trained on a relative small corpus (~7GB) extracted from    [Wikipedia-ITA](https://linguatools.org/tools/corpora/wikipedia-monolingual-corpora/).   - **umberto-commoncrawl-cased-v1**  a cased model trained on Commoncrawl ITA exploiting [OSCAR](https://traces1.inria.fr/oscar/) (Open Super-large Crawled ALMAnaCH coRpus) Italian large corpus ( ~69GB)  Both models have 12-layer  768-hidden  12-heads  110M parameters (BASE).   | Model | WWM | CASED | TOKENIZER | VOCAB SIZE  | TRAIN STEPS | FAIRSEQ  | TRANSFORMERS | | ------ | ------ | ------ | ------ | ------ |------ | ------ | --- | | `umberto-wikipedia-uncased-v1` | YES  | NO | SPM | 32K | 100k | [Link](http://bit.ly/2s7JmXh)| [Link](http://bit.ly/35wbSj6) | | `umberto-commoncrawl-cased-v1` | YES | YES | SPM | 32K | 125k | [Link](http://bit.ly/2TakHfJ)| [Link](http://bit.ly/35zO7GH) |  We trained both the models on 8 Nvidia V100 GPUs (p2.8xlarge P2 EC2 instance) during 4 days on [AWS Sagemaker](https://aws.amazon.com/it/sagemaker/).   """;Natural Language Processing;https://github.com/musixmatchresearch/umberto
"""English | [简体中文](README_zh-CN.md)  MMGeneration is a powerful toolkit for generative models  especially for GANs now. It is based on PyTorch and [MMCV](https://github.com/open-mmlab/mmcv). The master branch works with **PyTorch 1.5+**.  <div align=""center"">     <img src=""https://user-images.githubusercontent.com/12726765/114534478-9a65a900-9c81-11eb-8087-de8b6816eed8.png"" width=""800""/> </div>    """;General;https://github.com/open-mmlab/mmgeneration
"""English | [简体中文](README_zh-CN.md)  MMGeneration is a powerful toolkit for generative models  especially for GANs now. It is based on PyTorch and [MMCV](https://github.com/open-mmlab/mmcv). The master branch works with **PyTorch 1.5+**.  <div align=""center"">     <img src=""https://user-images.githubusercontent.com/12726765/114534478-9a65a900-9c81-11eb-8087-de8b6816eed8.png"" width=""800""/> </div>    """;Computer Vision;https://github.com/open-mmlab/mmgeneration
"""The plankton dataset that we use contains 712 491 images of plankton spread __unevenly__ accross 65 different species. These are in turn divided into train  validation and test sets with the following ratios: <ul> <li>Train: &emsp;&emsp; &emsp;&nbsp; 699 491 images</li> <li>Validation:&emsp;&nbsp; 6500 images</li> <li>Test: &emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp; 6500 images</li> </ul>  In short we used the *train*  *validate* and *test* folders inside the *data-65* folder from the dataset given to us for this assignment. <br>  Here is an overview of what the different species look like:   ```python class_names = ['Acantharea'  'Acartiidae'  'Actinopterygii'  'Annelida'  'Bivalvia__Mollusca'  'Brachyura'                'bubble'  'Calanidae'  'Calanoida'  'calyptopsis'  'Candaciidae'  'Cavoliniidae'  'Centropagidae'                'Chaetognatha'  'Copilia'  'Corycaeidae'  'Coscinodiscus'  'Creseidae'  'cyphonaute'  'cypris'                'Decapoda'  'Doliolida'  'egg__Actinopterygii'  'egg__Cavolinia_inflexa'  'Eucalanidae'  'Euchaetidae'                'eudoxie__Diphyidae'  'Evadne'  'Foraminifera'  'Fritillariidae'  'gonophore__Diphyidae'  'Haloptilus'                'Harpacticoida'  'Hyperiidea'  'larvae__Crustacea'  'Limacidae'  'Limacinidae'  'Luciferidae'  'megalopa'                'multiple__Copepoda'  'nauplii__Cirripedia'  'nauplii__Crustacea'  'nectophore__Diphyidae'  'nectophore__Physonectae'                 'Neoceratium'  'Noctiluca'  'Obelia'  'Oikopleuridae'  'Oithonidae'  'Oncaeidae'  'Ophiuroidea'  'Ostracoda'  'Penilia'                'Phaeodaria'  'Podon'  'Pontellidae'  'Rhincalanidae'  'Salpida'  'Sapphirinidae'  'scale'  'seaweed'  'tail__Appendicularia'                'tail__Chaetognatha'  'Temoridae'  'zoea__Decapoda'] link_list = ['https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/{}.jpg'.format(cn) for cn in class_names] html_list = [""<table>""] for i in range(8):     html_list.append(""<tr>"")     for j in range(8):         html_list.append(""<td><center>{}</center><img src='{}'></td>"".format(class_names[i*8+j]  link_list[i*8+j]))     html_list.append(""</tr>"") html_list.append(""</table>"")  display(HTML(''.join(html_list)))    ```   <table><tr><td><center>Acantharea</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Acantharea.jpg'></td><td><center>Acartiidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Acartiidae.jpg'></td><td><center>Actinopterygii</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Actinopterygii.jpg'></td><td><center>Annelida</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Annelida.jpg'></td><td><center>Bivalvia__Mollusca</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Bivalvia__Mollusca.jpg'></td><td><center>Brachyura</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Brachyura.jpg'></td><td><center>bubble</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/bubble.jpg'></td><td><center>Calanidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Calanidae.jpg'></td></tr><tr><td><center>Calanoida</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Calanoida.jpg'></td><td><center>calyptopsis</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/calyptopsis.jpg'></td><td><center>Candaciidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Candaciidae.jpg'></td><td><center>Cavoliniidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Cavoliniidae.jpg'></td><td><center>Centropagidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Centropagidae.jpg'></td><td><center>Chaetognatha</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Chaetognatha.jpg'></td><td><center>Copilia</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Copilia.jpg'></td><td><center>Corycaeidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Corycaeidae.jpg'></td></tr><tr><td><center>Coscinodiscus</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Coscinodiscus.jpg'></td><td><center>Creseidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Creseidae.jpg'></td><td><center>cyphonaute</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/cyphonaute.jpg'></td><td><center>cypris</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/cypris.jpg'></td><td><center>Decapoda</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Decapoda.jpg'></td><td><center>Doliolida</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Doliolida.jpg'></td><td><center>egg__Actinopterygii</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/egg__Actinopterygii.jpg'></td><td><center>egg__Cavolinia_inflexa</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/egg__Cavolinia_inflexa.jpg'></td></tr><tr><td><center>Eucalanidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Eucalanidae.jpg'></td><td><center>Euchaetidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Euchaetidae.jpg'></td><td><center>eudoxie__Diphyidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/eudoxie__Diphyidae.jpg'></td><td><center>Evadne</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Evadne.jpg'></td><td><center>Foraminifera</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Foraminifera.jpg'></td><td><center>Fritillariidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Fritillariidae.jpg'></td><td><center>gonophore__Diphyidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/gonophore__Diphyidae.jpg'></td><td><center>Haloptilus</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Haloptilus.jpg'></td></tr><tr><td><center>Harpacticoida</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Harpacticoida.jpg'></td><td><center>Hyperiidea</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Hyperiidea.jpg'></td><td><center>larvae__Crustacea</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/larvae__Crustacea.jpg'></td><td><center>Limacidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Limacidae.jpg'></td><td><center>Limacinidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Limacinidae.jpg'></td><td><center>Luciferidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Luciferidae.jpg'></td><td><center>megalopa</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/megalopa.jpg'></td><td><center>multiple__Copepoda</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/multiple__Copepoda.jpg'></td></tr><tr><td><center>nauplii__Cirripedia</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/nauplii__Cirripedia.jpg'></td><td><center>nauplii__Crustacea</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/nauplii__Crustacea.jpg'></td><td><center>nectophore__Diphyidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/nectophore__Diphyidae.jpg'></td><td><center>nectophore__Physonectae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/nectophore__Physonectae.jpg'></td><td><center>Neoceratium</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Neoceratium.jpg'></td><td><center>Noctiluca</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Noctiluca.jpg'></td><td><center>Obelia</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Obelia.jpg'></td><td><center>Oikopleuridae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Oikopleuridae.jpg'></td></tr><tr><td><center>Oithonidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Oithonidae.jpg'></td><td><center>Oncaeidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Oncaeidae.jpg'></td><td><center>Ophiuroidea</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Ophiuroidea.jpg'></td><td><center>Ostracoda</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Ostracoda.jpg'></td><td><center>Penilia</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Penilia.jpg'></td><td><center>Phaeodaria</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Phaeodaria.jpg'></td><td><center>Podon</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Podon.jpg'></td><td><center>Pontellidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Pontellidae.jpg'></td></tr><tr><td><center>Rhincalanidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Rhincalanidae.jpg'></td><td><center>Salpida</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Salpida.jpg'></td><td><center>Sapphirinidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Sapphirinidae.jpg'></td><td><center>scale</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/scale.jpg'></td><td><center>seaweed</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/seaweed.jpg'></td><td><center>tail__Appendicularia</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/tail__Appendicularia.jpg'></td><td><center>tail__Chaetognatha</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/tail__Chaetognatha.jpg'></td><td><center>Temoridae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Temoridae.jpg'></td></tr></table>    """;Computer Vision;https://github.com/JakobKallestad/InceptionV3-on-plankton-images
"""DeepLab is a state-of-art deep learning system for semantic image segmentation built on top of [Caffe](http://caffe.berkeleyvision.org).  It combines (1) *atrous convolution* to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks  (2) *atrous spatial pyramid pooling* to robustly segment objects at multiple scales with filters at multiple sampling rates and effective fields-of-views  and (3) densely connected conditional random fields (CRF) as post processing.  This distribution provides a publicly available implementation for the key model ingredients reported in our latest [arXiv paper](http://arxiv.org/abs/1606.00915). This version also supports the experiments (DeepLab v1) in our ICLR'15. You only need to modify the old prototxt files. For example  our proposed atrous convolution is called dilated convolution in CAFFE framework  and you need to change the convolution parameter ""hole"" to ""dilation"" (the usage is exactly the same). For the experiments in ICCV'15  there are some differences between our argmax and softmax_loss layers and Caffe's. Please refer to [DeepLabv1](https://bitbucket.org/deeplab/deeplab-public/) for details.  Please consult and consider citing the following papers:      @article{CP2016Deeplab        title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets  Atrous Convolution  and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        journal={arXiv:1606.00915}        year={2016}     }      @inproceedings{CY2016Attention        title={Attention to Scale: Scale-aware Semantic Image Segmentation}        author={Liang-Chieh Chen and Yi Yang and Jiang Wang and Wei Xu and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{CB2016Semantic        title={Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform}        author={Liang-Chieh Chen and Jonathan T Barron and George Papandreou and Kevin Murphy and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{PC2015Weak        title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation}        author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille}        booktitle={ICCV}        year={2015}     }      @inproceedings{CP2015Semantic        title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        booktitle={ICLR}        year={2015}     }   Note that if you use the densecrf implementation  please consult and cite the following paper:      @inproceedings{KrahenbuhlK11        title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}        author={Philipp Kr{\""{a}}henb{\""{u}}hl and Vladlen Koltun}        booktitle={NIPS}        year={2011}     }   """;Computer Vision;https://github.com/keb123keb/deeplabv2
"""Burrus is an AI programmed to learn how to play Othello. Othello is a two-player  perfect information game that is also known as Reversi. Read more about the game here: https://en.wikipedia.org/wiki/Reversi  Burrus uses reinforcement learning with a neural network to learn how to play Othello from scratch. The core learning algorithm is a re-implementation of the novel algorithm introduced in the paper ""Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"" published by Google DeepMind: https://arxiv.org/abs/1712.01815. A simple version of the learning loop is as follows:   1. Initialize neural network with random weights  1. Self-play the network against itself with random noise added for diversity in moves 1. Train next iteration of the neural network with the winner of games from step 2. 1. Repeat steps 2-4.   Burrus comes with an extremely performant Othello engine to compute legal moves and gamestates efficiently. Bitboard calculations are adapted and applied to this new game domain from the modern literature on writing performant chess engines (for example of performant chess engine  see: Stockfish https://github.com/official-stockfish/Stockfish)    """;Reinforcement Learning;https://github.com/aduerig/burrus
"""PaddleOCR aims to create multilingual  awesome  leading  and practical OCR tools that help users train better models and apply them into practice.   **Recent updates** - 2021.12.21 OCR open source online course starts. The lesson starts at 8:30 every night and lasts for ten days. Free registration: https://aistudio.baidu.com/aistudio/course/introduce/25207 - 2021.12.21 release PaddleOCR v2.4  release 1 text detection algorithm (PSENet)  3 text recognition algorithms (NRTR、SEED、SAR)  1 key information extraction algorithm (SDMGR  [tutorial](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.4/ppstructure/docs/kie.md)) and 3 DocVQA algorithms (LayoutLM  LayoutLMv2  LayoutXLM  [tutorial](https://github.com/PaddlePaddle/PaddleOCR/tree/release/2.4/ppstructure/vqa)). - PaddleOCR R&D team would like to share the key points of PP-OCRv2  at 20:15 pm on September 8th  [Course Address](https://aistudio.baidu.com/aistudio/education/group/info/6758). - 2021.9.7 release PaddleOCR v2.3  [PP-OCRv2](#PP-OCRv2) is proposed. The inference speed of PP-OCRv2 is 220% higher than that of PP-OCR server in CPU device. The F-score of PP-OCRv2 is 7% higher than that of PP-OCR mobile. - 2021.8.3 released PaddleOCR v2.2  add a new structured documents analysis toolkit  i.e.  [PP-Structure](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/ppstructure/README.md)  support layout analysis and table recognition (One-key to export chart images to Excel files). - 2021.4.8 release end-to-end text recognition algorithm [PGNet](https://www.aaai.org/AAAI21Papers/AAAI-2885.WangP.pdf) which is published in AAAI 2021. Find tutorial [here](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.1/doc/doc_en/pgnet_en.md)；release multi language recognition [models](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.1/doc/doc_en/multi_languages_en.md)  support more than 80 languages recognition; especically  the performance of [English recognition model](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.1/doc/doc_en/models_list_en.md#English) is Optimized.  - [more](./doc/doc_en/update_en.md)   """;Computer Vision;https://github.com/PaddlePaddle/PaddleOCR
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Charliesgithub20221030/BERT
"""Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent  we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. But if we instead take steps proportional to the positive of the gradient  we approach a local maximum of that function; the procedure is then known as gradient ascent. Gradient descent was originally proposed by Cauchy in 1847.    """;General;https://github.com/Arko98/Gradient-Descent-Algorithms
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/haidershaour/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/haidershaour/bert
"""To lead the way with reproducibility  Reaver is bundled with pre-trained weights and full Tensorboard summary logs for all six minigames.  Simply download an experiment archive from the [releases](https://github.com/inoryy/reaver-pysc2/releases) tab and unzip onto the `results/` directory.  You can use pre-trained weights by appending `--experiment` flag to `reaver.run` command:      python reaver.run --map <map_name> --experiment <map_name>_reaver --test 2> stderr.log  Tensorboard logs are available if you launch `tensorboard --logidr=results/summaries`.   You can also view them [directly online](https://boards.aughie.org/board/HWi4xmuvuOSuw09QBfyDD-oNF1U) via [Aughie Boards](https://boards.aughie.org/).   Reaver is a modular deep reinforcement learning framework with a focus on various StarCraft II based tasks  following in DeepMind's footsteps  who are pushing state-of-the-art of the field through the lens of playing a modern video game with human-like interface and limitations.  This includes observing visual features similar (though not identical) to what a human player would perceive and choosing actions from similar pool of options a human player would have. See [StarCraft II: A New Challenge for Reinforcement Learning](https://arxiv.org/abs/1708.04782) article for more details.  Though development is research-driven  the philosophy behind Reaver API is akin to StarCraft II game itself -  it has something to offer both for novices and experts in the field. For hobbyist programmers Reaver offers all the tools necessary to train DRL agents by modifying only a small and isolated part of the agent (e.g. hyperparameters). For veteran researchers Reaver offers simple  but performance-optimized codebase with modular architecture:  agent  model  and environment are decoupled and can be swapped at will.  While the focus of Reaver is on StarCraft II  it also has full support for other popular environments  notably Atari and MuJoCo.  Reaver agent algorithms are validated against reference results  e.g. PPO agent is able to match [ Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347). Please see [below](#but-wait-theres-more) for more details.   """;Reinforcement Learning;https://github.com/inoryy/reaver
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/111qqz/caffe-ssd
"""CascadePSP is a deep learning model for high-resolution segmentation refinement. This repository contains our PyTorch implementation with both training and testing functionalities. We also provide the annotated UHD dataset **BIG** and the pretrained model.  Here are some refinement results on high-resolution images. ![teaser](docs/images/teaser.jpg)   """;Computer Vision;https://github.com/hkchengrex/CascadePSP
"""![ ](Figures/Introduction.png)  This paper proposes a Hierarchical Learned Video Compression (HLVC) method with three hierarchical quality layers and a recurrent enhancement network. As illustrated in Figure 1  the frames in layers 1  2 and 3 are compressed with the highest  medium and the lowest quality  respectively. The benefits of hierarchical quality are two-fold: First  the high quality frames  which provide high quality references  are able to improve the compression performance of other frames at the encoder side; Second  because of the high correlation among neighboring frames  at the decoder side  the low quality frames can be enhanced by making use of the advantageous information in high quality frames. The enhancement improves quality without bit-rate overhead  thus improving the rate-distortion performance. For example  the frames 3 and 8 in Figure 1  which belong to layer 3  are compressed with low quality and bit-rate. Then  our recurrent enhancement network significantly improves their quality  taking advantage of higher quality frames  e.g.  frames 0 and 5. As a result  the frames 3 and 8 reach comparable quality to frame 5 in layer 2  but consume much less bit-rate. Therefore  our HLVC approach achieves efficient video compression.   """;Computer Vision;https://github.com/RenYang-home/HLVC
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/StoneGH/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/StoneGH/bert
""" - Most words are symbols for an extra-linguistic entity - a word is a signifier that maps to a signified (idea/thing) - Approx. 13m words in English language   - There is probably some N-dimensional space (such that N << 13m) that is sufficient to encode all semantics of our language - Most simple word vector - one-hot encoding   - Denotational semantics - the concept of representing an idea as a symbol - a word or one-hot vector - sparse  cannot capture similarity - localist encoding  Evaluation  - Intrinsic - evaluation on a specific  intermediate task   - Fast to compute   - Aids with understanding of the system   - Needs to be correlated with real task to provide a good measure of usefulness   - Word analogies - popular intrinsic evaluation method for word vectors     - Semantic - e.g. King/Man | Queen/Woman     - Syntactic - e.g. big/biggest | fast/fastest - Extrinsic - evaluation on a real task   - Slow   - May not be clear whether the problem with low performance is related to a particular subsystem  other subsystems  or interactions between subsystems   - If a subsystem is replaced and performance improves  the change is likely to be good   """;Natural Language Processing;https://github.com/jeremycz/word-vectors
"""The project provides a web app which allows users to post found hats as well as report their finds. <br> <br> The system automatically matches images and/or textual descriptions  notifying the owners of the lost items. There is the option to register their hats to be automatically matched if they are found. There is an element of gamification by means of awarding productive users with experience points as well as a very simple feed for viewing  bumping and reacting to posts about lost and found items. The project can also be modified to suit current market demand  for example handling face masks in addition to hats.   """;General;https://github.com/mvxxx/mimuw-hats
"""The project provides a web app which allows users to post found hats as well as report their finds. <br> <br> The system automatically matches images and/or textual descriptions  notifying the owners of the lost items. There is the option to register their hats to be automatically matched if they are found. There is an element of gamification by means of awarding productive users with experience points as well as a very simple feed for viewing  bumping and reacting to posts about lost and found items. The project can also be modified to suit current market demand  for example handling face masks in addition to hats.   """;Computer Vision;https://github.com/mvxxx/mimuw-hats
"""**Quasi-Recurrent Neural Networks (QRNNs)** are used for the RNA sub-cellular localization (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5210605/).   QRNNs embrace the benefits of both convolutional and recurrent neural networks alike. QRNNs beat other networks such as LSTM in both accuracy and speed.   The followind figure depicts the difference between an LSTM layer and a QRNN layer. The first is totally sequential; the second can be parallelized with convolutions and the sequential part in much lighter. ![layer](https://github.com/AllenMont/qrnn-rna-localization/blob/master/img/layer.PNG)  A description of QRNNs can be found in https://arxiv.org/abs/1611.01576. The PyTorch implementation follows https://github.com/salesforce/pytorch-qrnn.   """;Sequential;https://github.com/montallen/qrnn-rna-localization
"""Training a model for image-to-image translation typically requires a large dataset of paired examples.These datasets can be difficult and expensive to prepare  and in some cases impossible  such as photographs of paintings by long dead artists. The CycleGAN is a technique that involves the automatic training of image-to-image translation models without paired examples. The models are trained in an unsupervised manner using a collection of images from the source and target domain that do not need to be related in any way.  Pix2Pix GAN  a popular image to image translation uses paired images. Paired images dataset is small and hence it is hard to implement object transformation. Cycle GANs use unpaired images for training for image to image transformation.   """;Computer Vision;https://github.com/DeepikaKaranji/CycleGAN-Unpaired-Image-translation
"""Training a model for image-to-image translation typically requires a large dataset of paired examples.These datasets can be difficult and expensive to prepare  and in some cases impossible  such as photographs of paintings by long dead artists. The CycleGAN is a technique that involves the automatic training of image-to-image translation models without paired examples. The models are trained in an unsupervised manner using a collection of images from the source and target domain that do not need to be related in any way.  Pix2Pix GAN  a popular image to image translation uses paired images. Paired images dataset is small and hence it is hard to implement object transformation. Cycle GANs use unpaired images for training for image to image transformation.   """;General;https://github.com/DeepikaKaranji/CycleGAN-Unpaired-Image-translation
"""<img src=""./Report/25_inf.png"">     (first row ~ last row)<br>     0633: this flower has petals that are yellow with red blotches <br>     0194: the flower has white stringy petals with yellow and purple pollen tubes<br>     2014: this flower is pink in color with only one large petal<br>     4683: this flower is yellow in color with petals that are rounded<br>     3327: the flower has a several pieces of yellow colored petals that looks similar to its leaves  """;General;https://github.com/BeyondCloud/Comp04_ReverseImageCaption
"""<img src=""./Report/25_inf.png"">     (first row ~ last row)<br>     0633: this flower has petals that are yellow with red blotches <br>     0194: the flower has white stringy petals with yellow and purple pollen tubes<br>     2014: this flower is pink in color with only one large petal<br>     4683: this flower is yellow in color with petals that are rounded<br>     3327: the flower has a several pieces of yellow colored petals that looks similar to its leaves  """;Computer Vision;https://github.com/BeyondCloud/Comp04_ReverseImageCaption
"""InfoGAN is an information-theoretic extension to the simple Generative Adversarial Networks that is able to learn disentangled representations in a completely unsupervised manner. What this means is that InfoGAN successfully disentangle wrirting styles from digit shapes on th MNIST dataset and discover visual concepts such as hair styles and gender on the CelebA dataset. To achieve this an information-theoretic regularization is added to the loss function that enforces the maximization of mutual information between latent codes  c  and the generator distribution G(z  c).   """;Computer Vision;https://github.com/inkplatform/InfoGAN-PyTorch
"""In this two agent Unity environment the agents must learn to keep the ball in play and to hit the ball over the net enough times before it falls to the ground or outside the court.  An MADDPG agent is used to train the cooperating actor network policies.   """;Reinforcement Learning;https://github.com/petsol/MultiAgentCooperation_UnityAgent_MADDPG_Udacity
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/yuhangT/tf_bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/yuhangT/tf_bert
"""For this project  the unity ML [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment is used . The agent is trained to play [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis)     [![Unity ML-Agents Tennis Environment](https://video.udacity-data.com/topher/2018/May/5af7955a_tennis/tennis.png)Unity ML-Agents Tennis Environment](https://classroom.udacity.com/nanodegrees/nd893-ent/parts/0ba70f95-986b-400c-9b2e-59366cca2a49/modules/83e3a45a-a815-4dca-82bc-c6f1b46ac8cd/lessons/c03538e3-4024-41c5-9baa-3be2d91f250c/concepts/da65c741-cdeb-4f34-bb56-d8977385596e#)    In this environment  two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net  it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds  it receives a reward of -0.01. Thus  the goal of each agent is to keep the ball in play.  The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own  local observation. Two continuous actions are available  corresponding to movement toward (or away from) the net  and jumping.  The task is episodic  and in order to solve the environment  your agents must get an average score of +0.5 (over 100 consecutive episodes  after taking the maximum over both agents). Specifically   - After each episode  we add up the rewards that each agent received (without discounting)  to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores. - This yields a single **score** for each episode.  The environment is considered solved  when the average (over 100 episodes) of those **scores** is at least +0.5.   """;Reinforcement Learning;https://github.com/Zorrorulz/MultiAgentDDPG-Tennis
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Charliesgithub20221030/BERT
"""SSD(Single Shot MultiBox Detector) is a state-of-art object detection algorithm  brought by Wei Liu and other wonderful guys  see [SSD: Single Shot MultiBox Detector @ arxiv](https://arxiv.org/abs/1512.02325)  recommended to read for better understanding.  Also  SSD currently performs good at PASCAL VOC Challenge  see [http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=3](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=3)   """;Computer Vision;https://github.com/nirajdpandey/Object-detection-and-localization-using-SSD
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/XMdidi/SMP_Bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/XMdidi/SMP_Bert
"""SSD(Single Shot MultiBox Detector) is a state-of-art object detection algorithm  brought by Wei Liu and other wonderful guys  see [SSD: Single Shot MultiBox Detector @ arxiv](https://arxiv.org/abs/1512.02325)  recommended to read for better understanding.  Also  SSD currently performs good at PASCAL VOC Challenge  see [http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=3](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=3)     """;Computer Vision;https://github.com/rajs25/Object-Detection
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/anhnktp/yolov3
"""For this project  we will work with the [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment.  In this environment  two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net  it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds  it receives a reward of -0.01.  Thus  the goal of each agent is to keep the ball in play.  The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own  local observation.  Two continuous actions are available  corresponding to movement toward (or away from) the net  and jumping.   The task is episodic  and in order to solve the environment  your agents must get an average score of +0.5 (over 100 consecutive episodes  after taking the maximum over both agents). Specifically   - After each episode  we add up the rewards that each agent received (without discounting)  to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores. - This yields a single **score** for each episode.  The environment is considered solved  when the average (over 100 episodes) of those **scores** is at least +0.5.   """;Reinforcement Learning;https://github.com/biemann/Collaboration-and-Competition
"""Google AI's BERT paper shows the amazing result on various NLP task (new 17 NLP tasks SOTA)   including outperform the human F1 score on SQuAD v1.1 QA task.  This paper proved that Transformer(self-attention) based encoder can be powerfully used as  alternative of previous language model with proper language model training method.  And more importantly  they showed us that this pre-trained language model can be transfer  into any NLP task without making task specific model architecture.  This amazing result would be record in NLP history   and I expect many further papers about BERT will be published very soon.  This repo is implementation of BERT. Code is very simple and easy to understand fastly. Some of these codes are based on [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)  Currently this project is working on progress. And the code is not verified yet.   """;Natural Language Processing;https://github.com/nachiketaa/BERT-pytorch
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/OpenCv30/Yolov3
"""In object detection  determining which anchors to assign as positive or negative samples  known as __anchor assignment__  has been revealed as a core procedure that can significantly affect a model's performance. In this paper we propose a novel anchor assignment strategy that adaptively separates anchors into positive and negative samples for a ground truth bounding box according to the model's learning status such that it is able to reason about the separation in a probabilistic manner. To do so we first calculate the scores of anchors conditioned on the model and fit a probability distribution to these scores. The model is then trained with anchors separated into positive and negative samples according to their probabilities. Moreover  we investigate the gap between the training and testing objectives and propose to predict the Intersection-over-Unions of detected boxes as a measure of localization quality to reduce the discrepancy. <div align=""center"">   <img src=""eccv2020_fig1.jpg"" width=""1000""/> </div> <div align=""center"">   <img src=""box_vis_cat.jpg"" width=""500""/> </div>   """;Computer Vision;https://github.com/kkhoot/PAA
"""Summarization model for short texts based on pure [transformer model](https://arxiv.org/abs/1706.03762) with [bpe encoding](http://www.aclweb.org/anthology/P16-1162).   """;General;https://github.com/gooppe/transformer-summarization
"""Summarization model for short texts based on pure [transformer model](https://arxiv.org/abs/1706.03762) with [bpe encoding](http://www.aclweb.org/anthology/P16-1162).   """;Natural Language Processing;https://github.com/gooppe/transformer-summarization
"""Here collects my study notes  including programming languages  deep learning / machine learning papers  mathematical simulation notes  from a biochem major perspective.   """;Reinforcement Learning;https://github.com/liyiyuian/Deep-Learning
"""PaddleOCR aims to create multilingual  awesome  leading  and practical OCR tools that help users train better models and apply them into practice.   """;Computer Vision;https://github.com/Mushroomcat9998/PaddleOCR
"""**TL; DR.** Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism.    ![deformable_detr](./figs/illustration.png)  ![deformable_detr](./figs/convergence.png)  **Abstract.** DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However  it suffers from slow convergence and limited feature spatial resolution  due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues  we proposed Deformable DETR  whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10× less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach.   """;Computer Vision;https://github.com/fundamentalvision/Deformable-DETR
"""**TL; DR.** Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism.    ![deformable_detr](./figs/illustration.png)  ![deformable_detr](./figs/convergence.png)  **Abstract.** DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However  it suffers from slow convergence and limited feature spatial resolution  due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues  we proposed Deformable DETR  whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10× less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach.   """;General;https://github.com/fundamentalvision/Deformable-DETR
"""Official Implementation of our pSp paper for both training and evaluation. The pSp method extends the StyleGAN model to  allow solving different image-to-image translation problems using its encoder.   """;Computer Vision;https://github.com/liuliuliu11/pixel2style2pixel-liu
"""DeepLab is a series of image semantic segmentation models  whose latest version  i.e. v3+  proves to be the state-of-art. Its major contribution is the use of atrous spatial pyramid pooling (ASPP) operation at the end of the encoder. While the model works extremely well  its open source code is hard to read (at least from my personal perspective). Here we re-implemented DeepLab V3  the earlier version of v3+ (which only additionally employs the decoder architecture)  in a much simpler and more understandable way.   """;Computer Vision;https://github.com/leimao/DeepLab-V3
"""[![video summary](youtube.png)](https://www.youtube.com/watch?v=vUm6vurIwyM)   """;Computer Vision;https://github.com/boschresearch/OASIS
"""English | [简体中文](README_zh-CN.md)  [![build](https://github.com/open-mmlab/mmocr/workflows/build/badge.svg)](https://github.com/open-mmlab/mmocr/actions) [![docs](https://readthedocs.org/projects/mmocr/badge/?version=latest)](https://mmocr.readthedocs.io/en/latest/?badge=latest) [![codecov](https://codecov.io/gh/open-mmlab/mmocr/branch/main/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmocr) [![license](https://img.shields.io/github/license/open-mmlab/mmocr.svg)](https://github.com/open-mmlab/mmocr/blob/main/LICENSE) [![PyPI](https://badge.fury.io/py/mmocr.svg)](https://pypi.org/project/mmocr/) [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmocr.svg)](https://github.com/open-mmlab/mmocr/issues) [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmocr.svg)](https://github.com/open-mmlab/mmocr/issues)  MMOCR is an open-source toolbox based on PyTorch and mmdetection for text detection  text recognition  and the corresponding downstream tasks including key information extraction. It is part of the [OpenMMLab](https://openmmlab.com/) project.  The main branch works with **PyTorch 1.6+**.  Documentation: https://mmocr.readthedocs.io/en/latest/.  <div align=""left"">   <img src=""resources/illustration.jpg""/> </div>   """;Computer Vision;https://github.com/open-mmlab/mmocr
"""The foundational model that we use is Inception-V3 from Keras' pretrained models. However  we cut it off until 'mixed7' layer  and then add our own layers.  Read more about this model at: - https://keras.io/api/applications/inceptionv3/ - https://arxiv.org/abs/1512.00567     There are only 3 important files in this repository. - `modeling.ipynb` is a jupyter notebook which can be run on Google Colab (with GPU for faster training). It contains step-by-step on how to create the image classifier and export the model.  - `model_inception_weights.h5` is the trained weights of our deep learning model's layers. This is used to load the model in our web app. - `apps.py` is the python file to deploy our web app in Streamlit.     """;Computer Vision;https://github.com/myarist/Rock-Paper-Scissors
"""The embeddings generated by Deep Speaker can be used for many tasks  including speaker identification  verification  and clustering. We experiment with ResCNN architectures to extract the acoustic features  then mean pool to produce utterance-level speaker embeddings  and train using triplet loss based on cosine similarity.   """;Computer Vision;https://github.com/dodoproptit99/deep-speaker
"""The embeddings generated by Deep Speaker can be used for many tasks  including speaker identification  verification  and clustering. We experiment with ResCNN architectures to extract the acoustic features  then mean pool to produce utterance-level speaker embeddings  and train using triplet loss based on cosine similarity.   """;General;https://github.com/dodoproptit99/deep-speaker
"""This repository is the official implementation for [Pose Recognition with Cascade Transformers](https://openaccess.thecvf.com/content/CVPR2021/html/Li_Pose_Recognition_With_Cascade_Transformers_CVPR_2021_paper.html). It proposes two types of cascade Transformers  as follows  for pose recognition.   """;Computer Vision;https://github.com/mlpc-ucsd/PRTR
"""The output of the code will be saved in the location specified by the `--save_path` argument. The output is three main files. The progress file  the actors file  and archive files. The log file is saved under `args.save_path/progress_{file_name}.dat` with `file_name = PGA-MAP-Elites_{args.env}_{args.seed}_{args.dim_map}`. After each batch the progress file will log in each column:  - Nr of Evaluations - Coverage  - Max Fitness - Mean Fitness - Median Fitness - 5th Percentile Fitness - 95th Percentile Fitness - Averaged Max Fitness (10 Evaluations) - Averaged Max Fitness Behaviour Descriptor (10 Evaluations)  The actors file takes the form `args.save_path/actors_{file_name}.dat` and saves information about each actor added to the main archive.   - Nr of Evaluations - id - Fitness - Behaviour Descriptor - Associated CVT centroid - Parent 1 id - Parent 2 id - type (evo/gradient) - novel (bool) - delta fitness (compared to the previous solution in that cell)  Each `--save_period` evaluations the state of the archive is saved under `args.save_path/archive_{file_name}_{nr_of_evaluations}.dat`. The saves info about each actor currently in the archive:   - Fitness - Assosiated CVT centroid - Behaviour Descriptor  - id  The PyTorch network models are saved for all actors in the final archive under `args.save_path/models/{file_name}_actor_id`.   """;General;https://github.com/ollenilsson19/PGA-MAP-Elites
"""The output of the code will be saved in the location specified by the `--save_path` argument. The output is three main files. The progress file  the actors file  and archive files. The log file is saved under `args.save_path/progress_{file_name}.dat` with `file_name = PGA-MAP-Elites_{args.env}_{args.seed}_{args.dim_map}`. After each batch the progress file will log in each column:  - Nr of Evaluations - Coverage  - Max Fitness - Mean Fitness - Median Fitness - 5th Percentile Fitness - 95th Percentile Fitness - Averaged Max Fitness (10 Evaluations) - Averaged Max Fitness Behaviour Descriptor (10 Evaluations)  The actors file takes the form `args.save_path/actors_{file_name}.dat` and saves information about each actor added to the main archive.   - Nr of Evaluations - id - Fitness - Behaviour Descriptor - Associated CVT centroid - Parent 1 id - Parent 2 id - type (evo/gradient) - novel (bool) - delta fitness (compared to the previous solution in that cell)  Each `--save_period` evaluations the state of the archive is saved under `args.save_path/archive_{file_name}_{nr_of_evaluations}.dat`. The saves info about each actor currently in the archive:   - Fitness - Assosiated CVT centroid - Behaviour Descriptor  - id  The PyTorch network models are saved for all actors in the final archive under `args.save_path/models/{file_name}_actor_id`.   """;Reinforcement Learning;https://github.com/ollenilsson19/PGA-MAP-Elites
"""Object detection methods typically rely on only local evidence. For example  to detect the mouse in the image below  only the features extracted at/around the mouse are used. In contrast  HoughNet is able to utilize long-range (i.e. far away) evidence  too. Below  on the right  the votes that support the detection of the mouse are shown: in addition to the local evidence  far away but semantically relevant objects  the two keyboards  vote for the mouse.  <img src=""/readme/teaser.png"" width=""550"">  HoughNet is a one-stage  anchor-free  voting-based  bottom-up object detection method. Inspired by the Generalized Hough Transform  HoughNet determines the presence of an object at a certain location by the sum of the votes cast on that location. Votes are collected from both near and long-distance locations based on a log-polar vote field. Thanks to this voting mechanism  HoughNet is able to integrate both near and long-range  class-conditional evidence for visual recognition  thereby generalizing and enhancing current object detection methodology  which typically relies on only local evidence. On the COCO dataset  HoughNet achieves 46.4 AP (and 65.1 AP<sub>50</sub>)  performing on par with the state-of-the-art in bottom-up object detection and outperforming most  major one-stage and two-stage methods. We further validate the effectiveness of HoughNet in another task  namely  ""labels to photo"" image generation by integrating the voting module to two different GAN models and showing that the accuracy is significantly improved in both cases.   """;Computer Vision;https://github.com/nerminsamet/houghnet
"""This is a pytorch implementation for the Visformer models. This project is based on the training code in [Deit](https://github.com/facebookresearch/deit) and the tools in [timm](https://github.com/rwightman/pytorch-image-models).   """;Computer Vision;https://github.com/danczs/Visformer
"""This is an official implementation of [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808). We present a new architecture  named Convolutional vision Transformers (CvT)  that improves Vision Transformers (ViT) in performance and efficienty by introducing convolutions into ViT to yield the best of both disignes. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding  and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (e.g. shift  scale  and distortion invariance) while maintaining the merits of Transformers (e.g. dynamic attention  global context  and better generalization). We validate CvT by conducting extensive experiments  showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k  with fewer parameters and lower FLOPs. In addition  performance gains are maintained when pretrained on larger dataset (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained on ImageNet-22k  our CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k val set. Finally  our results show that the positional encoding  a crucial component in existing Vision Transformers  can be safely removed in our model  simplifying the design for higher resolution vision tasks.   ![](figures/pipeline.svg)   """;Computer Vision;https://github.com/microsoft/CvT
"""In this module  we provide training data  network settings and loss designs for deep face recognition. The training data includes  but not limited to the cleaned MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  etc.. The loss functions include Softmax  SphereFace  CosineFace  ArcFace  Sub-Center ArcFace and Triplet (Euclidean/Angular) Loss.  You can check the detail page of our work [ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/ArcFace)(which accepted in CVPR-2019) and [SubCenter-ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/SubCenter-ArcFace)(which accepted in ECCV-2020).  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this module  you can simply achieve LFW 99.83%+ and Megaface 98%+ by a single model. This module can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   InsightFace is an open source 2D&3D deep face analysis toolbox  mainly based on MXNet.   The master branch works with **MXNet 1.2 to 1.6**  with **Python 3.x**.     """;General;https://github.com/vladimirwest/insightface_cinematic
"""This is a super simple ConvNet architecture that achieves over **80% top-1 accuracy on ImageNet with a stack of 3x3 conv and ReLU**! This repo contains the **pretrained models**  code for building the model  training  and the conversion from training-time model to inference-time  and **an example of using RepVGG for semantic segmentation**.  The MegEngine version: https://github.com/megvii-model/RepVGG.  TensorRT implemention with C++ API by @upczww https://github.com/upczww/TensorRT-RepVGG. Great work!  Another PyTorch implementation by @zjykzj https://github.com/ZJCV/ZCls. He also presented detailed benchmarks at https://zcls.readthedocs.io/en/latest/benchmark-repvgg/. Nice work!  Included in a famous model zoo (over 7k stars) https://github.com/rwightman/pytorch-image-models.  Objax implementation and models by @benjaminjellis. Great work! https://github.com/benjaminjellis/Objax-RepVGG.  Citation:      @inproceedings{ding2021repvgg      title={Repvgg: Making vgg-style convnets great again}      author={Ding  Xiaohan and Zhang  Xiangyu and Ma  Ningning and Han  Jungong and Ding  Guiguang and Sun  Jian}      booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}      pages={13733--13742}      year={2021}     }   """;Computer Vision;https://github.com/DingXiaoH/RepVGG
"""The purpose of the project is to create a Spark application to separate images into learning and test sets  extract features from images from two different classes  learn a model using the learning data and measure the model's performance on the test data.  > For example  we are told that with 100 learning images in each class  it is possible to obtain 98% good classifications in the Wheaten Terrier vs Yorkshire Terrier task.   L'objectif du projet est de créer une application Spark permettant de séparer les images en jeux d'apprentissage et de test  d'extraire les features des images en provenance de deux classes différentes  d'apprendre un modèle sur les données d'apprentissage et de mesurer les performances du modèle sur les données de test.  > À titre d'exemple  il nous est indiqué qu'avec 100 images d'apprentissage dans chaque classe  il est possible d'obtenir 98% de bonnes classifications dans la tâche Wheaten Terrier vs Yorkshire Terrier.    """;Computer Vision;https://github.com/elliott-iadvize/ocr-aws-distributed
"""Pose estimation find the keypoints belong to the people in the image. There are two methods exist for pose estimation.  * **Bottom-Up** first finds the keypoints and associates them into different people in the image. (Generally faster and lower accuracy) * **Top-Down** first detect people in the image and estimate the keypoints. (Generally computationally intensive but better accuracy)  This repo will only include top-down pose estimation models.   """;Computer Vision;https://github.com/sithu31296/pose-estimation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/flyliu2017/bert
"""Google AI's BERT paper shows the amazing result on various NLP task (new 17 NLP tasks SOTA)   including outperform the human F1 score on SQuAD v1.1 QA task.  This paper proved that Transformer(self-attention) based encoder can be powerfully used as  alternative of previous language model with proper language model training method.  And more importantly  they showed us that this pre-trained language model can be transfer  into any NLP task without making task specific model architecture.  This amazing result would be record in NLP history   and I expect many further papers about BERT will be published very soon.  This repo is implementation of BERT. Code is very simple and easy to understand fastly. Some of these codes are based on [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)  Currently this project is working on progress. And the code is not verified yet.   """;Natural Language Processing;https://github.com/codertimo/BERT-pytorch
"""**NOTE:** The code has been refactored to use TensorFlow 2.0 and PyBullet (instead of MuJoCo). See the `tf1_mujoco` branch for old version.  The project's original goal was to use the same algorithm to ""solve"" [10 MuJoCo robotic control environments](https://gym.openai.com/envs/#mujoco). And  specifically  to achieve this without hand-tuning the hyperparameters (network sizes  learning rates  and TRPO settings) for each environment. This is challenging because the environments range from a simple cart pole problem with a single control input to a humanoid with 17 controlled joints and 44 observed variables. The project was successful  nabbing top spots on almost all of the AI Gym MuJoCo leaderboards.  With the release of TensorFlow 2.0  I decided to dust off this project and upgrade the code. And  while I was at it  I moved from the paid MuJoCo simulator to the free PyBullet simulator.  Here are the key points:  * Trust Region Policy Optimization \[1\] \[2\] * Value function approximated with 3 hidden-layer NN (tanh activations):     * hid1 size = obs_dim x 10     * hid2 size = geometric mean of hid1 and hid3 sizes     * hid3 size = 5 * Policy is a multi-variate Gaussian parameterized by a 3 hidden-layer NN (tanh activations):     * hid1 size = obs_dim x 10     * hid2 size = geometric mean of hid1 and hid3 sizes     * hid3 size = action_dim x 10     * Diagonal covariance matrix variables are separately trained * Generalized Advantage Estimation (gamma = 0.995  lambda = 0.98) \[3\] \[4\] * ADAM optimizer used for both neural networks * The policy is evaluated for 20 episodes between updates  except:     * 50 episodes for Reacher     * 5 episodes for Swimmer     * 5 episodes for HalfCheetah     * 5 episodes for HumanoidStandup * Value function is trained on current batch + previous batch * KL loss factor and ADAM learning rate are dynamically adjusted during training * Policy and Value NNs built with TensorFlow   """;Reinforcement Learning;https://github.com/pat-coady/trpo
"""This project mainly introduces the learning rate schemes provided by tensorflow and observes their influences on convolutional neural networks. The problem about how they work is not included as it is difficult to explain. Maybe in the future  I will post it once I get them straight. So  there are 15 learning rate schemes we will talk about: - 1. exponential_decay - 2. piecewise_constant_decay - 3. polynominal_decay - 4. inverse_time_decay - 5. cosine_decay - 6. cosine_decay_restarts - 7. linear_cosine_decay - 8. noisy_linear_cosine_decay - 9. tf.train.GradientDescentOptimizer - 10. tf.train.MomentumOptimizer - 11. tf.train.AdamOptimizer // tf.train.AdagradOptimizer // tf.train.AdadeletaOptimizer // tf.train.AdagradDAOptimizer - 12. tf.train.RMSPropOptimizer - 13. tf.train.FtrlOptimizer We conduct experiments on Cifar10 with these shemes  and then make analyses on different combinations among them.   """;General;https://github.com/souxun2015/Survery-of-Learning-Rate-Shemes
"""**Faster** R-CNN is an object detection framework based on deep convolutional networks  which includes a Region Proposal Network (RPN) and an Object Detection Network. Both networks are trained for sharing convolutional layers for fast testing.  **This repo contains a Python implementation of Faster-RCNN originally developed in Matlab. This code works with models trained using Matlab version of Faster-RCNN which is main difference between this and py-faster-rcnn.**  This code was developed for internal use in one of my projects at the end of 2015. I decided to publish it as is.   """;Computer Vision;https://github.com/smichalowski/faster_rcnn
"""This is the implementation of our model called Style-Restricted GAN (SRGAN)  which is designed for the unpaired image translation with multiple styles. The main features of this models are 1) the enhancement of diversification and 2) the restriction of diversification. As for the former one  while the base model ([SingleGAN](https://github.com/Xiaoming-Yu/SingleGAN)) employed KL divergence loss to restrict the distribution of encoded features like [VAE](https://arxiv.org/abs/1312.6114)  SRGAN exploits 3 new losses instead: batch KL divergence loss  correlation loss  and histogram imitation loss. When it comes to the restriction  in the previous  it wasn't explicitly designed to control how the generator diversifies the results  which can have an adverse effect on some applications. Therefore  in this paper  the encoder is pre-trained with the classification task before being used as an encoder.  We'll proceed this implementation in a notebook form. And we also share our docker environment in order for everybody to run the code as well as observing the implementation.  ---  """;Computer Vision;https://github.com/shinshoji01/Style-Restricted_GAN
"""This is the implementation of our model called Style-Restricted GAN (SRGAN)  which is designed for the unpaired image translation with multiple styles. The main features of this models are 1) the enhancement of diversification and 2) the restriction of diversification. As for the former one  while the base model ([SingleGAN](https://github.com/Xiaoming-Yu/SingleGAN)) employed KL divergence loss to restrict the distribution of encoded features like [VAE](https://arxiv.org/abs/1312.6114)  SRGAN exploits 3 new losses instead: batch KL divergence loss  correlation loss  and histogram imitation loss. When it comes to the restriction  in the previous  it wasn't explicitly designed to control how the generator diversifies the results  which can have an adverse effect on some applications. Therefore  in this paper  the encoder is pre-trained with the classification task before being used as an encoder.  We'll proceed this implementation in a notebook form. And we also share our docker environment in order for everybody to run the code as well as observing the implementation.  ---  """;General;https://github.com/shinshoji01/Style-Restricted_GAN
"""Environments where the extrinsic rewards are sparsely observed are harder to explore and to learn for RL agents since they may require more episodes to determine which sequences of actions lead to high rewards. Recently  it has been argued that adding intrinsic rewards (for instance through Random Network Distillation) can help to drive the exploration efficiently in Atari games [1][2][3] and can be simply implemented along with Proximal Policy Optimization [4]. As opposed to extrinsic rewards  intrinsic rewards are not necessarily constant over the episodes: such rewards can be seen as ""curiosity"" functions. Through this case study we aim at highlighting the advantages and limitations of curiosity-driven exploration.  [1] https://arxiv.org/pdf/1810.12894.pdf [2] https://arxiv.org/pdf/1705.05363.pdf [3] https://pathak22.github.io/large-scale-curiosity/resources/largeScaleCuriosity2018.pdf [4] https://arxiv.org/pdf/1707.06347.pdf   """;Reinforcement Learning;https://github.com/lgerrets/rl18-curiosity
"""The pickled data is a dictionary with 4 key/value pairs:  - `'features'` is a 4D array containing raw pixel data of the traffic sign images  (num examples  width  height  channels). - `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id. - `'sizes'` is a list containing tuples  (width  height) representing the original width and height the image. - `'coords'` is a list containing tuples  (x1  y1  x2  y2) representing coordinates of a bounding box around the sign in the image.  **First  we will use `numpy` provide the number of images in each subset  in addition to the image size  and the number of unique classes.** Number of training examples:  34799 Number of testing examples:  12630 Number of validation examples:  4410 Image data shape = (32  32  3) Number of classes = 43  **Then  we used `matplotlib` plot sample images from each subset.**   <figure>  <img src=""./traffic-signs-data/Screenshots/Train.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>   <figure>  <img src=""./traffic-signs-data/Screenshots/Test.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  <figure>  <img src=""./traffic-signs-data/Screenshots/Valid.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>   **And finally  we will use `numpy` to plot a histogram of the count of images in each unique class.**   <figure>  <img src=""./traffic-signs-data/Screenshots/TrainHist.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  <figure>  <img src=""./traffic-signs-data/Screenshots/TestHist.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  <figure>  <img src=""./traffic-signs-data/Screenshots/ValidHist.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  ---   """;General;https://github.com/mohamedameen93/German-Traffic-Sign-Classification-Using-TensorFlow
"""English | [简体中文](/README_zh-CN.md)  [![Documentation](https://readthedocs.org/projects/mmaction2/badge/?version=latest)](https://mmaction2.readthedocs.io/en/latest/) [![actions](https://github.com/open-mmlab/mmaction2/workflows/build/badge.svg)](https://github.com/open-mmlab/mmaction2/actions) [![codecov](https://codecov.io/gh/open-mmlab/mmaction2/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmaction2) [![PyPI](https://img.shields.io/pypi/v/mmaction2)](https://pypi.org/project/mmaction2/) [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/blob/master/LICENSE) [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues) [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)  MMAction2 is an open-source toolbox for video understanding based on PyTorch. It is a part of the [OpenMMLab](http://openmmlab.org/) project.  The master branch works with **PyTorch 1.3+**.  <div align=""center"">   <div style=""float:left;margin-right:10px;"">   <img src=""https://github.com/open-mmlab/mmaction2/raw/master/resources/mmaction2_overview.gif"" width=""380px""><br>     <p style=""font-size:1.5vw;"">Action Recognition Results on Kinetics-400</p>   </div>   <div style=""float:right;margin-right:0px;"">   <img src=""https://user-images.githubusercontent.com/34324155/123989146-2ecae680-d9fb-11eb-916b-b9db5563a9e5.gif"" width=""380px""><br>     <p style=""font-size:1.5vw;"">Skeleton-base Action Recognition Results on NTU-RGB+D-120</p>   </div> </div> <div align=""center"">   <img src=""https://github.com/open-mmlab/mmaction2/raw/master/resources/spatio-temporal-det.gif"" width=""800px""/><br>     <p style=""font-size:1.5vw;"">Spatio-Temporal Action Detection Results on AVA-2.1</p> </div>   """;General;https://github.com/open-mmlab/mmaction2
"""English | [简体中文](/README_zh-CN.md)  [![Documentation](https://readthedocs.org/projects/mmaction2/badge/?version=latest)](https://mmaction2.readthedocs.io/en/latest/) [![actions](https://github.com/open-mmlab/mmaction2/workflows/build/badge.svg)](https://github.com/open-mmlab/mmaction2/actions) [![codecov](https://codecov.io/gh/open-mmlab/mmaction2/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmaction2) [![PyPI](https://img.shields.io/pypi/v/mmaction2)](https://pypi.org/project/mmaction2/) [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/blob/master/LICENSE) [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues) [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)  MMAction2 is an open-source toolbox for video understanding based on PyTorch. It is a part of the [OpenMMLab](http://openmmlab.org/) project.  The master branch works with **PyTorch 1.3+**.  <div align=""center"">   <div style=""float:left;margin-right:10px;"">   <img src=""https://github.com/open-mmlab/mmaction2/raw/master/resources/mmaction2_overview.gif"" width=""380px""><br>     <p style=""font-size:1.5vw;"">Action Recognition Results on Kinetics-400</p>   </div>   <div style=""float:right;margin-right:0px;"">   <img src=""https://user-images.githubusercontent.com/34324155/123989146-2ecae680-d9fb-11eb-916b-b9db5563a9e5.gif"" width=""380px""><br>     <p style=""font-size:1.5vw;"">Skeleton-base Action Recognition Results on NTU-RGB+D-120</p>   </div> </div> <div align=""center"">   <img src=""https://github.com/open-mmlab/mmaction2/raw/master/resources/spatio-temporal-det.gif"" width=""800px""/><br>     <p style=""font-size:1.5vw;"">Spatio-Temporal Action Detection Results on AVA-2.1</p> </div>   """;Computer Vision;https://github.com/open-mmlab/mmaction2
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/qilei123/DEEPLAB_4_RETINA
""" This repository is build for PSANet  which contains source code for PSA module and related evaluation code. For installation  please merge the related layers and follow the description in [PSPNet](https://github.com/hszhao/PSPNet) repository (test with CUDA 7.0/7.5 + cuDNN v4).   """;Computer Vision;https://github.com/hszhao/PSANet
""" This repository is build for PSANet  which contains source code for PSA module and related evaluation code. For installation  please merge the related layers and follow the description in [PSPNet](https://github.com/hszhao/PSPNet) repository (test with CUDA 7.0/7.5 + cuDNN v4).   """;General;https://github.com/hszhao/PSANet
"""English | [简体中文](README_zh-CN.md)  MMDetection is an open source object detection toolbox based on PyTorch. It is a part of the [OpenMMLab](https://openmmlab.com/) project.  The master branch works with **PyTorch 1.5+**.  <details open> <summary>Major features</summary>  - **Modular Design**    We decompose the detection framework into different components and one can easily construct a customized object detection framework by combining different modules.  - **Support of multiple frameworks out of box**    The toolbox directly supports popular and contemporary detection frameworks  *e.g.* Faster RCNN  Mask RCNN  RetinaNet  etc.  - **High efficiency**    All basic bbox and mask operations run on GPUs. The training speed is faster than or comparable to other codebases  including [Detectron2](https://github.com/facebookresearch/detectron2)  [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark) and [SimpleDet](https://github.com/TuSimple/simpledet).  - **State of the art**    The toolbox stems from the codebase developed by the *MMDet* team  who won [COCO Detection Challenge](http://cocodataset.org/#detection-leaderboard) in 2018  and we keep pushing it forward. </details>   Apart from MMDetection  we also released a library [mmcv](https://github.com/open-mmlab/mmcv) for computer vision research  which is heavily depended on by this toolbox.   """;Computer Vision;https://github.com/open-mmlab/mmdetection
"""Prototype implementation in PyTorch of the NIPS'16 paper:<br> Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering<br> M Defferrard  X Bresson  P Vandergheynst<br> Advances in Neural Information Processing Systems  3844-3852  2016<br> ArXiv preprint: [arXiv:1606.09375](https://arxiv.org/pdf/1606.09375.pdf) <br> <br>   """;Graphs;https://github.com/xbresson/spectral_graph_convnets
"""|name|description| |-|-| |[elmo_tfhub_use_methods.ipynb](https://nbviewer.jupyter.org/github/yuanxiaosc/ELMo/blob/master/elmo_tfhub_use_methods.ipynb)|Summarize four usage methods of Elmo embedding.| |[IMDB_ELMo_As_Embedding_Layer.ipynb](https://github.com/yuanxiaosc/ELMo/blob/master/tfhub_elmo_use_examples/IMDB_ELMo_As_Embedding_Layer.ipynb)|IMDB movie review sentiment analysis example| |[elmo_sentence_level_embedding.ipynb](https://github.com/yuanxiaosc/ELMo/blob/master/tfhub_elmo_use_examples/elmo_sentence_level_embedding.ipynb)|Kaggle's movie review sentiment analysis example| |[elmo_word_level_embedding.ipynb](https://github.com/yuanxiaosc/ELMo/blob/master/tfhub_elmo_use_examples/elmo_word_level_embedding.ipynb)|Kaggle's movie review sentiment analysis example| |[IMDB_ELMo_Preprocessing_Data.ipynb](https://github.com/yuanxiaosc/ELMo/blob/master/allennlp_elmo_use_examples/IMDB_ELMo_Preprocessing_Data.ipynb)|Preprocessing data with Elmo| |Visualizing ELMo Contextual Vectors|[Visualizing...](https://nbviewer.jupyter.org/github/yuanxiaosc/ELMo/blob/master/Visualizing%20ELMo%20Contextual%20Vectors/Visualizing%20ELMo%20Contextual%20Vectors.ipynb)|   """;Natural Language Processing;https://github.com/yuanxiaosc/ELMo
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/rpuiggari/bert2
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/rpuiggari/bert2
"""One of the most important applications of seismic reflection is the hydrocarbon exploration which is closely related to salt deposits analysis. This problem is very important even nowadays due to it’s non-linear nature. Taking into account the recent developments in deep learning networks [TGS-NOPEC Geophysical Company](https://www.tgs.com/) hosted the Kaggle [competition](https://www.kaggle.com/c/tgs-salt-identification-challenge) for salt deposits segmentation problem in seismic image data. In this paper  we demonstrate the great performance of several novel deep learning techniques merged into a single neural network. Using a [U-Net](https://arxiv.org/abs/1505.04597) with [ResNeXt-50](https://arxiv.org/abs/1611.05431) encoder pretrained on [ImageNet](http://www.image-net.org/) as our base architecture  we implemented [Spatial-Channel Squeeze & Excitation](https://arxiv.org/abs/1803.02579)  [Lovasz loss](https://github.com/bermanmaxim/LovaszSoftmax)  analog of [CoordConv](https://eng.uber.com/coordconv/) and [Hypercolumn](https://arxiv.org/abs/1411.5752) methods.  This architecture was a part of the [solutiuon](https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/69274) (27th out of 3234 teams top 1%) in the [TGS Salt Identification Challenge](https://www.kaggle.com/c/tgs-salt-identification-challenge).   """;Computer Vision;https://github.com/K-Mike/Automatic-salt-deposits-segmentation
"""The demand of applying semantic segmentation model on mobile devices has been increasing rapidly. Current state-of-the-art networks have enormous amount of parameters  hence unsuitable for mobile devices  while other small memory footprint models follow the spirit of classification network and ignore the inherent characteristic of semantic segmentation. To tackle this problem  we propose a novel Context Guided Network (CGNet)  which is a light-weight and efficient network for semantic segmentation. We first propose the Context Guided (CG) block  which learns the joint feature of both local feature and surrounding context  and further improves the joint feature with the global context. Based on the CG block  we develop CGNet which captures contextual information in all stages of the network and is specially tailored for increasing segmentation accuracy. CGNet is also elaborately designed to reduce the number of parameters and save memory footprint. Under an equivalent number of parameters  the proposed CGNet significantly outperforms existing segmentation networks. Extensive experiments on Cityscapes and CamVid datasets verify the effectiveness of the proposed approach. Specifically  without any post-processing and multi-scale testing  the proposed CGNet achieves 64.8% mean IoU on Cityscapes with less than 0.5 M parameters.    """;Computer Vision;https://github.com/wutianyiRosun/CGNet
"""Migration learning is already hot in the CV space. However  there has been no development in the NLP field. According to the theory  migration learning can be used in the NLP field. In October last year  Google sent a migration study paper in the NLP field: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Although this is not the earliest paper in the field of NLP migration learning  the first papers are ELMo  fasiai's ULMFit and OpenAI's Transformer. Google created BERT based on predecessors  and BERT is much better than other models. Attention and transformer are mainly used in BERT. Detailed details can be seen in the original text. Https://arxiv.org/abs/1810.04805 This project is fine-tuned on this basis  mainly modifying the run_classify.py file. Refer to this blog. https://mp.weixin.qq.com/s/XmeDjHSFI0UsQmKeOgwnyA And the code has been modified to better suit our project needs.   """;Natural Language Processing;https://github.com/SkullFang/BERT_NLP_Classification
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Nstats/my_bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Nstats/my_bert
"""デジタル回路は通常では0か1の値しかとらないた通常であれば微分することはできません。  一方で、入出力を0や1ではなく「1になる確率」としてアナログ的に扱う手法があり、Stochastic計算と呼ばれます。 幸いな事に Neural Network は、学習において多くの対象の尤度を取り扱う為この考え方は相性のよい考え方です。  Stochastic計算を用いると、例えばANDゲートは二つの入力の両方が同時に1になる確率、すなわち確率の乗算器として振舞います。このようにすべてのデジタル回路をStochastic計算に置き換えることが可能です。  FPGAというデバイスは、LUTと呼ばれる小さなメモリとこのメモリを選択する集合体で、メモリを書き換えることでプログラマブルな回路記述を実現します。このLUT回路を微分可能回路記述に置き換えたのちに、メモリに相当する部分に学習対象の重み係数を置いて学習を行うネットワークが LUT-Network です。  BinaryBrain は LUT-Network の学習可能性を実証するために作られたプラットフォームです。    """;General;https://github.com/ryuz/BinaryBrain
"""The pickled data is a dictionary with 4 key/value pairs:  - `'features'` is a 4D array containing raw pixel data of the traffic sign images  (num examples  width  height  channels). - `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id. - `'sizes'` is a list containing tuples  (width  height) representing the original width and height the image. - `'coords'` is a list containing tuples  (x1  y1  x2  y2) representing coordinates of a bounding box around the sign in the image.  **First  we will use `numpy` provide the number of images in each subset  in addition to the image size  and the number of unique classes.** Number of training examples:  34799 Number of testing examples:  12630 Number of validation examples:  4410 Image data shape = (32  32  3) Number of classes = 43  **Then  we used `matplotlib` plot sample images from each subset.**   <figure>  <img src=""./traffic-signs-data/Screenshots/Train.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>   <figure>  <img src=""./traffic-signs-data/Screenshots/Test.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  <figure>  <img src=""./traffic-signs-data/Screenshots/Valid.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>   **And finally  we will use `numpy` to plot a histogram of the count of images in each unique class.**   <figure>  <img src=""./traffic-signs-data/Screenshots/TrainHist.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  <figure>  <img src=""./traffic-signs-data/Screenshots/TestHist.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  <figure>  <img src=""./traffic-signs-data/Screenshots/ValidHist.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  ---   """;Computer Vision;https://github.com/mohamedameen93/German-Traffic-Sign-Classification-Using-TensorFlow
"""該論文提出了一個通用的reconstruction network，希望能夠對輸入的任意style進行transfer，而不需要重新train model；換句話說，就是希望能夠使用任意的reference image來進行style transfer，擺脫傳統的style transfer對於style和content loss需要通過對layer的嘗試參數，來得到一個和style較爲匹配的表述纔能有較好的效果，且針對不同的style這一步驟需要重新training這樣的缺點。 該論文提出了Whitening & Coloring transform layer (WCT layer)，它的實作觀念在於，對於任何一種style image(reference image)，要能夠使content表現出style的風格，只需在feature map上分布表徵一致。 首先，將feature map減去平均值，然後乘上對自己的協方差矩陣的逆矩陣，來進行whitening的動作，以利將feature map拉到一個白話的分布空間。然後透過對reference image取得feature map的coloring協方差矩陣的方式，將其乘以content image whitening後的結果，並加上平均值，就可以將content image whitening後的feature map空間轉移到reference image圖片上平均分布；最後，透過Stylization Weight Control 的公式：  <a href=""https://www.codecogs.com/eqnedit.php?latex=\widehat{f_{cs}}&space;=&space;\alpha&space;\widehat{f_{cs}}&space;&plus;&space;(1&space;-&space;\alpha)\widehat{f_c}"" target=""_blank""><img src=""https://latex.codecogs.com/gif.latex?\widehat{f_{cs}}&space;=&space;\alpha&space;\widehat{f_{cs}}&space;&plus;&space;(1&space;-&space;\alpha)\widehat{f_c}"" title=""\widehat{f_{cs}} = \alpha \widehat{f_{cs}} + (1 - \alpha)\widehat{f_c}"" /></a>  就可以完成將reference image整合input image的動作。   該論文透過卷積神經網路，將圖片的內容及風格分開並重建，提供一個style transfer的做法。  該論文利用簡單的統計分析，將一張圖的顏色特徵轉移到另外一張圖上，其中，色彩校正的部分主要是藉由選擇合適的source image，並將其特徵應用到target image上來實現。   Cycle gan跟傳統的gan做圖像轉換的方式不同，它不需要配對的數據集(paired image data set)；利用兩個generator、discrimnator和轉換的一致性(consistency)，cycle gan只需要不同風格的unpaired image data set即可運作。   """;Computer Vision;https://github.com/eugene08976/hw1
"""該論文提出了一個通用的reconstruction network，希望能夠對輸入的任意style進行transfer，而不需要重新train model；換句話說，就是希望能夠使用任意的reference image來進行style transfer，擺脫傳統的style transfer對於style和content loss需要通過對layer的嘗試參數，來得到一個和style較爲匹配的表述纔能有較好的效果，且針對不同的style這一步驟需要重新training這樣的缺點。 該論文提出了Whitening & Coloring transform layer (WCT layer)，它的實作觀念在於，對於任何一種style image(reference image)，要能夠使content表現出style的風格，只需在feature map上分布表徵一致。 首先，將feature map減去平均值，然後乘上對自己的協方差矩陣的逆矩陣，來進行whitening的動作，以利將feature map拉到一個白話的分布空間。然後透過對reference image取得feature map的coloring協方差矩陣的方式，將其乘以content image whitening後的結果，並加上平均值，就可以將content image whitening後的feature map空間轉移到reference image圖片上平均分布；最後，透過Stylization Weight Control 的公式：  <a href=""https://www.codecogs.com/eqnedit.php?latex=\widehat{f_{cs}}&space;=&space;\alpha&space;\widehat{f_{cs}}&space;&plus;&space;(1&space;-&space;\alpha)\widehat{f_c}"" target=""_blank""><img src=""https://latex.codecogs.com/gif.latex?\widehat{f_{cs}}&space;=&space;\alpha&space;\widehat{f_{cs}}&space;&plus;&space;(1&space;-&space;\alpha)\widehat{f_c}"" title=""\widehat{f_{cs}} = \alpha \widehat{f_{cs}} + (1 - \alpha)\widehat{f_c}"" /></a>  就可以完成將reference image整合input image的動作。   該論文透過卷積神經網路，將圖片的內容及風格分開並重建，提供一個style transfer的做法。  該論文利用簡單的統計分析，將一張圖的顏色特徵轉移到另外一張圖上，其中，色彩校正的部分主要是藉由選擇合適的source image，並將其特徵應用到target image上來實現。   Cycle gan跟傳統的gan做圖像轉換的方式不同，它不需要配對的數據集(paired image data set)；利用兩個generator、discrimnator和轉換的一致性(consistency)，cycle gan只需要不同風格的unpaired image data set即可運作。   """;General;https://github.com/eugene08976/hw1
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/qilei123/sod_v1_demo
"""Siamese networks have drawn great attention in visual tracking because of their balanced accuracy and speed.  However  the backbone network utilized in these trackers is still the classical AlexNet  which does not fully take advantage of the capability of modern deep neural networks.     Our proposals improve the performances of fully convolutional siamese trackers by  1) introducing CIR and CIR-D units to unveil the power of deeper and wider networks like [ResNet](https://arxiv.org/abs/1512.03385) and [Inceptipon](https://arxiv.org/abs/1409.4842);  2) designing backbone networks according to the analysis on internal network factors (e.g. receptive field  stride  output feature size)  which affect tracking performances.  <div align=""center"">   <img src=""demo/vis.gif"" width=""800px"" />   <!-- <p>Example SiamFC  SiamRPN and SiamMask outputs.</p> --> </div>  <!-- :tada::tada: **Highlight !!** Siamese tracker is severely sensitive to hyper-parameter  which is a common sense in tracking field. Although significant progresses have been made in some works  the result is hard to reproduce. In this case  we provide a [parameter tuning toolkit]() to make our model being reproduced easily. We hope our efforts and supplies will be helpful to your work. -->   """;General;https://github.com/researchmm/SiamDW
"""Siamese networks have drawn great attention in visual tracking because of their balanced accuracy and speed.  However  the backbone network utilized in these trackers is still the classical AlexNet  which does not fully take advantage of the capability of modern deep neural networks.     Our proposals improve the performances of fully convolutional siamese trackers by  1) introducing CIR and CIR-D units to unveil the power of deeper and wider networks like [ResNet](https://arxiv.org/abs/1512.03385) and [Inceptipon](https://arxiv.org/abs/1409.4842);  2) designing backbone networks according to the analysis on internal network factors (e.g. receptive field  stride  output feature size)  which affect tracking performances.  <div align=""center"">   <img src=""demo/vis.gif"" width=""800px"" />   <!-- <p>Example SiamFC  SiamRPN and SiamMask outputs.</p> --> </div>  <!-- :tada::tada: **Highlight !!** Siamese tracker is severely sensitive to hyper-parameter  which is a common sense in tracking field. Although significant progresses have been made in some works  the result is hard to reproduce. In this case  we provide a [parameter tuning toolkit]() to make our model being reproduced easily. We hope our efforts and supplies will be helpful to your work. -->   """;Computer Vision;https://github.com/researchmm/SiamDW
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/ahmed-anas/thesis-pointnet
"""This paper’s objectives are to produce vibrant and realistic colorizations of grayscale photographs.  We chose this paper because it utilizes architecture we already know (CNNs) and the project seemed like an interesting one to everyone in the group. This is a supervised learning problem where  our inputs are the grey scale images and the labels are the corresponding original colored images. We hope to predict the colors for every pixel on the image  based on training our model to generate artificially coloured images that are similar to the real coloured images from a corresponding grayscale image.   """;General;https://github.com/hsalhab/Coloring-in-the-Deep
"""Implementation of the [paper](https://arxiv.org/pdf/1612.01474v1.pdf) **Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles**  ![Sinusoidal gaussian regressor](sinusoidal.png)   Predictive uncertainty estimates obtained by using the ensemble approach proposed in the paper. The value of the sinusoidal function for values between -4 and 4 was used in training and at test time  the trained model is used to predict function values between -8 and 8. The blue line represents the true function  red represents the mean and the other two curves represent (mean + 3*std)   ![Sinusoidal gaussian dropout regressor](sinusoidal_dropout.png)  Predictive uncertainty estimates obtained by using a dropout Gaussian regressor. During training  we train the model to minimize NLL with dropout. Hence  as a result  a larger ensemble of networks are trained simultaneously. At test time  we maintain the same dropout and average the predictions of a fixed number (in the above plot  50) of networks that are part of the trained ensemble.  ![Kink example](kink.png)  I also tried out the kink example given in the paper by breaking the cubic curve in the middle and introducing a kink (sinusoidal curve)  The implementation is partly inspired from [this](https://github.com/muupan/deep-ensemble-uncertainty) repository  **Author** : Anirudh Vemula  """;General;https://github.com/vvanirudh/deep-ensembles-uncertainty
"""Sampling from a latent space of images to produce entirely new images is currently one of the most prominent and successful application of creative artificial intelligence. In this context  generative adversarial networks (or GANs for short) (Goodfellow et al.  2014)  first introduced in 2014  have exploded in popularity as an alternative to variational autoencoders (VAEs) for learning latent spaces of images. They have been used in real-life applications for text/image/video generation  drug discovery and text-toimage synthesis.  GANs are a kind of generative model that allows us to generate a whole image in parallel  in contrast with recurrent networks where the model generates the image one pixel at a time. Along with several other kinds of generative models  GANs use a differentiable function represented by a neural network as a generator G network. The generator network takes random noise as input  then runs that noise through a differentiable function to transform the noise and reshape it to have recognizable structure. The output of the generator is a realistic image. The choice of the random input noise determines which image will come out of the generator network. Running the generator with many different input noise values produces many different realistic output images.  The goal is for these images to be as fair samples from the distribution over real data. Of course  the generator net doesn’t start out producing realistic images. It has to be trained. The training process for a generative model is very different from the training process for a supervised learning model. For a supervised learning model  we show the model an image of an object and we tell it  this is the label. For a generative model  there is no output to associate with each image. We just show the model a lot of images and ask it to make more images that come from the same probability distribution.  ##  <figure>   <img src=""fig1.png"">   <figcaption>Scheme representing the general structure of a GAN  using MNIST images as data. The latent sample is a random vector the generator uses to construct its fake images. As the generator learns through training  it figures out how to map these random vectors to recognizable images that can fool the discriminator. The output of the discriminator is a sigmoid function  where 0 indicates a fake image and 1 indicates a real image.   </figcaption> </figure>  ##  But how we actually get the model to do that? Most generative models are trained by adjusting the parameters to maximize the probability that the generator net will generate the training data set. Unfortunately for a lot of interesting models  it can be very difficult to compute this probability. Most generative models get around that with some kind of approximation. GANs use an approximation where a second network  called the discriminator D  learns to guide the generator. The discriminator is just a regular neural net classifier. During the training process  the discriminator is shown real images from the training data half the time and fake images from the generator the other half of the time. The discriminator is trained to output the probability that the input is real. So it tries to assign a probability near 1 to real images  and a probability near zero to fake images (see Fig. 1). Meanwhile  the generator tries to do the opposite. It is trained to try to output images that the discriminator will assign probability near one of being real. Over time  the generator is forced to produce more realistic output in order to fool the discriminator. The generator takes random noise values z and maps them to output values x. Wherever the generator maps more values of z  the probability distribution over x  represented by the model  becomes denser. The discriminator outputs high values wherever the density of real data is greater than the density of generated data. The generator changes the samples it produces to move uphill along the function learned by the discriminator (see Algorithm 1). In other words  the generator moves its samples into areas where the model distribution is not yet dense enough. Eventually  the generator’s distribution matches the real distribution  and the discriminator has to output a probability of one half everywhere because every point is equally likely to be generated by the real data set as to be generated by the model. The two densities are equal.   """;Computer Vision;https://github.com/vmartinezalvarez/Image-generation-GAN-vs-DCGAN
"""Human visual system starts from lower visual area and proceed to the higher areas. However  it is not a full story. Our lower visual areas are largely affected by various higher visual area interactively.   ![Retino and Non-retino images][incongOccluded]    """;Computer Vision;https://github.com/Ohyeon5/MultiscaleSegmentation
"""This is the code repo of our TMM 2019 work titled  [""COMIC: Towards A Compact Image Captioning Model with Attention""](https://arxiv.org/abs/1903.01072).  In this paper  we tackle the problem of compactness of image captioning models which is hitherto unexplored.  We showed competitive results on both MS-COCO and InstaPIC-1.1M datasets despite having an embedding vocabularly size that is 39x-99x smaller.  <img src=""TMM.png"" height=""200"">  **Some pre-trained model checkpoints are available at  [this repo](https://github.com/jiahuei/COMIC-Pretrained-Captioning-Models).**    Updated on 25 Feb 2021: [Object Relation Transformer](https://papers.nips.cc/paper/9293-image-captioning-transforming-objects-into-words.pdf)  with Radix encoding that can achieve CIDEr score of 1.291 after SCST training. [Code at this repo](https://github.com/jiahuei/sparse-image-captioning).  Updated on 12 June 2019: Self-Critical Sequence Training (SCST)  Updated on 06 June 2019: Pre-trained model repo  Released on 03 June 2019.    """;General;https://github.com/jiahuei/COMIC-Compact-Image-Captioning-with-Attention
"""English | [简体中文](README_zh-CN.md)  MMDetection is an open source object detection toolbox based on PyTorch. It is a part of the [OpenMMLab](https://openmmlab.com/) project.  The master branch works with **PyTorch 1.5+**.  <details open> <summary>Major features</summary>  - **Modular Design**    We decompose the detection framework into different components and one can easily construct a customized object detection framework by combining different modules.  - **Support of multiple frameworks out of box**    The toolbox directly supports popular and contemporary detection frameworks  *e.g.* Faster RCNN  Mask RCNN  RetinaNet  etc.  - **High efficiency**    All basic bbox and mask operations run on GPUs. The training speed is faster than or comparable to other codebases  including [Detectron2](https://github.com/facebookresearch/detectron2)  [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark) and [SimpleDet](https://github.com/TuSimple/simpledet).  - **State of the art**    The toolbox stems from the codebase developed by the *MMDet* team  who won [COCO Detection Challenge](http://cocodataset.org/#detection-leaderboard) in 2018  and we keep pushing it forward. </details>   Apart from MMDetection  we also released a library [mmcv](https://github.com/open-mmlab/mmcv) for computer vision research  which is heavily depended on by this toolbox.   """;General;https://github.com/open-mmlab/mmdetection
"""This is an example of running Lambda on GreenGrass with MXNet pretrained model Inception v3 for image classification  Details on Inception v3 can be found in https://arxiv.org/abs/1512.00567   """;Computer Vision;https://github.com/kangks/aws-greengrass-mxnet-inception
"""This repository is used to PULSE any image including not face.  You can do it from following link. Some example and result is on the `images` directory  https://colab.research.google.com/drive/15Oi0D6djIcWTufLRP03rT5ZOPWkhvnvx   """;Computer Vision;https://github.com/kingsj0405/pulse-any-image
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/goldenbili/Bert_Test2
"""This respository contains a number of tutorials on how to use OpenVINO. In particular  these tutorials teach how someone would like to get started using OpenVINO through the context of object detection and pose estimation. Feel free to flip through the Jupyter Notebooks in order to understand how OpenVINO's Python API works.   """;General;https://github.com/fcr3/OpenVINO_Tutorials
"""Spatial Transform layer is an algorithm proposed in paper (""Spatial Transformer Networks"")[https://arxiv.org/abs/1506.02025]. Spatial Transform Layer can enhance model's invariance to translation  scale  rotation and more generic warping. It usually serves as the first layer of neural network to preprocess input image with affine transformation to align object in the input image to the training samples automatically.    """;Computer Vision;https://github.com/breadbread1984/SpatialTransformLayer-tf2.0
"""Stochastic weight averaging (SWA) is build upon the same principle as [snapshot ensembling](https://arxiv.org/abs/1704.00109) and [fast geometric ensembling](https://arxiv.org/abs/1802.10026). The idea is that averaging select stages of training can lead to better models. Where as the two former methods average by sampling and ensembling models  SWA instead average weights. This has been shown to give comparable improvements confined into a single model.  [![Illustration](https://raw.githubusercontent.com/simon-larsson/keras-swa/master/swa_illustration.png)](https://raw.githubusercontent.com/simon-larsson/keras-swa/master/swa_illustration.png)   """;General;https://github.com/simon-larsson/keras-swa
"""**MacBERT** is an improved BERT with novel **M**LM **a**s **c**orrection pre-training task  which mitigates the discrepancy of pre-training and fine-tuning.  Instead of masking with [MASK] token  which never appears in the ﬁne-tuning stage  **we propose to use similar words for the masking purpose**. A similar word is obtained by using [Synonyms toolkit (Wang and Hu  2017)](https://github.com/chatopera/Synonyms)  which is based on word2vec (Mikolov et al.  2013) similarity calculations. If an N-gram is selected to mask  we will ﬁnd similar words individually. In rare cases  when there is no similar word  we will degrade to use random word replacement.  Here is an example of our pre-training task. |  | Example       | | -------------- | ----------------- | | **Original Sentence**  | we use a language model to predict the probability of the next word. | |  **MLM** | we use a language [M] to [M] ##di ##ct the pro [M] ##bility of the next word . | | **Whole word masking**   | we use a language [M] to [M] [M] [M] the [M] [M] [M] of the next word . | | **N-gram masking** | we use a [M] [M] to [M] [M] [M] the [M] [M] [M] [M] [M] next word . | | **MLM as correction** | we use a text system to ca ##lc ##ulate the po ##si ##bility of the next word . |  Except for the new pre-training task  we also incorporate the following techniques.  - Whole Word Masking (WWM) - N-gram masking - Sentence-Order Prediction (SOP)  **Note that our MacBERT can be directly replaced with the original BERT as there is no differences in the main neural architecture.**  For more technical details  please check our paper: [Revisiting Pre-trained Models for Chinese Natural Language Processing](https://arxiv.org/abs/2004.13922)    """;Natural Language Processing;https://github.com/ymcui/MacBERT
"""Walking pass an interesting restaurant/coffee shop  thinking about going in but not sure about what they offered and too lazy to search for it in Google?  With this app  all you have to do is to taking one picture  and all these information are there for you. You could also make a purchase for the drink you like. It's time to make your evening adventure much more easier and enjoyable.    """;Computer Vision;https://github.com/minhhuu291/Logo-Recognition
"""该系统实现了基于深度框架的语音识别中的声学模型和语言模型建模，其中声学模型包括CNN-CTC、GRU-CTC、CNN-RNN-CTC，语言模型包含[transformer](https://jalammar.github.io/illustrated-transformer/)、[CBHG](https://github.com/crownpku/Somiao-Pinyin)，数据集包含stc、primewords、Aishell、thchs30四个数据集。  本系统更整体介绍：https://blog.csdn.net/chinatelecom08/article/details/82557715  本项目现已训练一个迷你的语音识别系统，将项目下载到本地上，下载[thchs数据集](http://www.openslr.org/resources/18/data_thchs30.tgz)并解压至data，运行`test.py`，不出意外能够进行识别，结果如下：       the  0 th example.     文本结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文汉字： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然     识别结果： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然  若自己建立模型则需要删除现有模型，重新配置参数训练，具体实现流程参考本页最后。   """;General;https://github.com/StevenLai1994/AM_and_LM
"""该系统实现了基于深度框架的语音识别中的声学模型和语言模型建模，其中声学模型包括CNN-CTC、GRU-CTC、CNN-RNN-CTC，语言模型包含[transformer](https://jalammar.github.io/illustrated-transformer/)、[CBHG](https://github.com/crownpku/Somiao-Pinyin)，数据集包含stc、primewords、Aishell、thchs30四个数据集。  本系统更整体介绍：https://blog.csdn.net/chinatelecom08/article/details/82557715  本项目现已训练一个迷你的语音识别系统，将项目下载到本地上，下载[thchs数据集](http://www.openslr.org/resources/18/data_thchs30.tgz)并解压至data，运行`test.py`，不出意外能够进行识别，结果如下：       the  0 th example.     文本结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文汉字： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然     识别结果： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然  若自己建立模型则需要删除现有模型，重新配置参数训练，具体实现流程参考本页最后。   """;Natural Language Processing;https://github.com/StevenLai1994/AM_and_LM
"""[fastDNA](#continuous-embedding-of-dna-reads-and-application-to-metagenomics) is a library for classification of short DNA sequences. It is adapted from the [fastText](https://fasttext.cc/) library.    """;Natural Language Processing;https://github.com/rmenegaux/fastDNA
"""We release Tensorflow implementations of the following **two graph embedding models** from the paper:  - Linear Graph Autoencoders  - Linear Graph Variational Autoencoders  together with standard Graph Autoencoders (AE) and Graph Variational Autoencoders (VAE) models (with 2-layer or 3-layer Graph Convolutional Networks encoders) from [Kipf and Welling (2016)](https://arxiv.org/pdf/1611.07308.pdf).   We evaluate all models on the **link prediction** and **node clustering** tasks introduced in the paper. We provide the **Cora**  **Citeseer** and **Pubmed** datasets in the `data` folder  and refer to section 4 of the paper for direct link to the additional datasets used in our experiments.  Our code builds upon Thomas Kipf's [original Tensorflow implementation](https://github.com/tkipf/gae) of standard Graph AE/VAE.  ![Linear AE and VAE](figures/linearsummary.png)   """;Graphs;https://github.com/deezer/linear_graph_autoencoders
"""  MegaFace dataset includes 1 027 060 faces  690 572 identities. [Link](http://megaface.cs.washington.edu/)   Challenge 1 is taken to test our model with 1 million distractors.   ![image](https://github.com/foamliu/InsightFace-v2/raw/master/images/megaface_stats.png)    Use Labeled Faces in the Wild (LFW) dataset for performance evaluation:  - 13233 faces - 5749 identities - 1680 identities with >=2 photo   MS-Celeb-1M dataset for training  3 804 846 faces over 85 164 identities.    """;General;https://github.com/foamliu/InsightFace-v2
"""The primary objective of our project was to improve human and object detection on low-light/dark images. We've used [EnlightenGAN](https://arxiv.org/abs/1906.06972) and have also used EnlightenGAN with different combinations of image filters such as Contrast Limited Adaptive Histogram Equalization (CLAHE) and Unsharp Mask (USM) to enhance low-light/dark images. The model that we used for human and object detection on these enlightened images was [Faster R-CNN](https://arxiv.org/abs/1506.01497) and found significant improvement in the performance. For more details  please refer to the [project report](https://github.com/ksheeraj/CS256-AI-ObjectDetection/blob/master/CS256_ProjectReport.pdf) and [presentation slides](https://github.com/ksheeraj/CS256-AI-ObjectDetection/blob/master/Presentation_FinalMilestone.pdf).   """;Computer Vision;https://github.com/ksheeraj/CS256-AI-ObjectDetection
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/goldenbili/Bert_Test3
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/goldenbili/Bert_Test3
"""![Schematic Illustration MAS](https://github.com/SAP-samples/acl2019-commonsense/blob/master/img/mas_illustration.png) The recently introduced [BERT (Deep Bidirectional Transformers for Language Understanding)](https://github.com/google-research/bert) [1] model exhibits strong performance on several language understanding benchmarks. In this work  we describe a simple re-implementation of BERT for commonsense reasoning. We show that the attentions produced by BERT can be directly utilized for tasks such as the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). Our proposed attention-guided commonsense reasoning method is conceptually simple yet empirically powerful. Experimental analysis on multiple datasets demonstrates that our proposed system performs remarkably well on all cases while outperforming the previously reported state of the art by a margin. While results suggest that BERT seems to implicitly learn to establish complex relationships between entities  solving commonsense reasoning tasks might require more than unsupervised models learned from huge text corpora. The sample code provided within this repository allows to replicate the results reported in the paper for PDP and WSC.  """;Natural Language Processing;https://github.com/SAP-samples/acl2019-commonsense
"""The repository contains the entire project (including all the preprocessing) for one-stage space-time video super-resolution with Zooming Slow-Mo.  Zooming Slow-Mo is a recently proposed joint video frame interpolation (VFI) and video super-resolution (VSR) method  which directly synthesizes an HR slow-motion video from an LFR  LR video. It is going to be published in [CVPR 2020](http://cvpr2020.thecvf.com/). The most up-to-date paper with supplementary materials can be found at [arXiv](https://arxiv.org/abs/2002.11616).  In Zooming Slow-Mo  we firstly temporally interpolate features of the missing LR frame by the proposed feature temporal interpolation network. Then  we propose a deformable ConvLSTM to align and aggregate temporal information simultaneously. Finally  a deep reconstruction network is adopted to predict HR slow-motion video frames. If our proposed architectures also help your research  please consider citing our paper.  Zooming Slow-Mo achieves state-of-the-art performance by PSNR and SSIM in Vid4  Vimeo test sets.  ![framework](./dump/framework.png)   """;Computer Vision;https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020
"""For calculating the Q Values we used a Neural Network  rather than just showing the current state to the network  the next 4 possible states(left  right  up  down) were also shown. This intuition was inspired from Monte Carlo Tree Search where game is played till the end to determine the Q-Values.  For data preprocessing log2 normalisation  training was done using the Bellman's Equation. The policy used was Epsilon greedy  to allow exploration the value of epsilon was annealed down by 5%.    """;Reinforcement Learning;https://github.com/Ishan-Kumar2/Reinforcement-Learning-on-2048
""" Similar to generative adversarial networks  autoencoders work by learning the latent space representation of the data that is fed in. The latent space contains all the necessary information to recreate the features of the original data.   In this project  I used an autoencoder to remove salt and pepper noise from corrupted images. It is important to highlight that this type of neural network could be easily applied to another type of problems like increasing resolution of images  image colorization  film restoration  and background removal.     """;Computer Vision;https://github.com/fescobar96/Image-Noise-Removal
""" Similar to generative adversarial networks  autoencoders work by learning the latent space representation of the data that is fed in. The latent space contains all the necessary information to recreate the features of the original data.   In this project  I used an autoencoder to remove salt and pepper noise from corrupted images. It is important to highlight that this type of neural network could be easily applied to another type of problems like increasing resolution of images  image colorization  film restoration  and background removal.     """;General;https://github.com/fescobar96/Image-Noise-Removal
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/zzdang/match_fashion
"""The Neural Network used for this work is a Odenet neural network: https://arxiv.org/abs/1806.07366 In particular I used a Res-Ode that's mean a miniblock of resnet before the ODE block.    """;General;https://github.com/LuigiRussoDev/Covid19Detection
"""The Neural Network used for this work is a Odenet neural network: https://arxiv.org/abs/1806.07366 In particular I used a Res-Ode that's mean a miniblock of resnet before the ODE block.    """;Computer Vision;https://github.com/LuigiRussoDev/Covid19Detection
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/chaowang1994/caffe-ssd
"""This repository provides the official PyTorch implementation of UNAS that was presented at CVPR 2020.  The paper presents results in two search spaces including [DARTS](https://arxiv.org/abs/1806.09055)  and [ProxylessNAS](https://arxiv.org/abs/1812.00332) spaces. Our paper can be found [here](https://arxiv.org/abs/1912.07651).    """;Computer Vision;https://github.com/NVlabs/unas
"""This repository provides the official PyTorch implementation of UNAS that was presented at CVPR 2020.  The paper presents results in two search spaces including [DARTS](https://arxiv.org/abs/1806.09055)  and [ProxylessNAS](https://arxiv.org/abs/1812.00332) spaces. Our paper can be found [here](https://arxiv.org/abs/1912.07651).    """;General;https://github.com/NVlabs/unas
"""Model-definition is a deep learning application for fault detection in photovoltaic plants. In this repository you will find trained detection models that point out where the panel faults are by using radiometric thermal infrared pictures. In [Web-API](https://github.com/RentadroneCL/Web-API) contains a performant  production-ready reference implementation of this repository.  ![Data Flow](MLDataFlow.svg)   """;Computer Vision;https://github.com/RentadroneCL/Photovoltaic_Fault_Detector
"""*Comparison is done based on the best metric score (Test accuracy) across 3 runs.*  |Activation Function| Mish > Baseline Model | Mish < Baseline Model | |---|---|---| |Penalized TanH|5|0| |ELU|5|0| |Sigmoid|5|0| |SReLU|4|0| |TanH|4|1| |Swish|3|2| |ReLU|2|3| |Leaky ReLU|2|3| |GELU|1|2|   *Comparison is done based on the high priority metric  for image classification the Top-1 Accuracy while for Generative Networks and Image Segmentation the Loss Metric. Therefore  for the latter  Mish > Baseline is indicative of better loss and vice versa. For Embeddings  the AUC metric is considered.*  |Activation Function| Mish > Baseline Model | Mish < Baseline Model | |---|---|---| |ReLU|55|20| |Swish-1|53|22| |SELU|26|1| |Sigmoid|24|0| |TanH|24|0| |HardShrink(λ = 0.5)|23|0| |Tanhshrink|23|0| |PReLU(Default Parameters)	|23|2| |Softsign|22|1| |Softshrink (λ = 0.5)|22|1| |Hardtanh|21|2| |ELU(α=1.0)|21|7| |LogSigmoid|20|4| |GELU|19|3| |E-Swish (β=1.75)|19|7| |CELU(α=1.0)|18|5| |SoftPlus(β = 1)|17|7| |Leaky ReLU(α=0.3)|17|8| |Aria-2(β = 1  α=1.5)|16|2| |ReLU6|16|8| |SQNL|13|1| |Weighted TanH (Weight = 1.7145)|12|1| |RReLU|12|11| |ISRU (α=1.0)|11|1| |Le Cun's TanH|10|2| |Bent's Identity|10|5| |Hard ELisH|9|1| |Flatten T-Swish|9|3| |Soft Clipping (α=0.5)|9|3| |SineReLU (ε = 0.001)|9|4| |ISRLU (α=1.0)|9|4| |ELisH|7|3| |SReLU|7|6| |Hard Sigmoid|1|0| |Thresholded ReLU(θ=1.0)|1|0|   """;General;https://github.com/digantamisra98/Mish
"""This repository shows how the Object detection and Recognition task can be performed for a personal assistance robot based on **ROS (Robot Operating System)**.  The whole project can be found in this youtube [video](https://youtu.be/AEgZd6wD7dk)  The following are the detailed task steps: 1) Extracting the real time image from the Kinect Camera 2) Detect and Recognize different objects in each image frame from the camera 3) Check distance between the robot camera and each of the detected objects 4) Each of the detected objects within a certain distance from the robot camera is stored with its equivelant position on the envirnoment map extracted from the Rtabmap current odometry published topic 5) While performing the above 4 steps  the robot keep waiting for any voice command sent from the user using the mobile app including the name of an object  in order to start navigating towards the required it if exists in the stored dictionary of previously detected objects and their location  as shown in the figure below.  <p align=""center""> <img src=""https://github.com/youssef-kishk/ROS-Based-Robot-Object-Detection-Recognition-Module/blob/master/images/image.png"" width=""600"" height=""300"" />  </p>    In the Object detection and recognition task  we are dealing with a **Microsoft xbox 360 Kinect camera RGB D** to have a real time image of the environment of the robot  which is then used to detect different objects using **You Only Look Once (YOLO) version 2** model  pretrained on a dataset of 80 different object categories [Common Objects in Context (COCO) dataset](https://cocodataset.org/).   """;Computer Vision;https://github.com/youssef-kishk/ROS-Based-Robot-Object-Detection-Recognition
"""**This project aims at building a speech enhancement system to attenuate environmental noise.**  <img src=""img/denoise_10classes.gif"" alt=""Spectrogram denoising"" title=""Speech enhancement""/>    Audios have many different ways to be represented  going from raw time series to time-frequency decompositions. The choice of the representation is crucial for the performance of your system. Among time-frequency decompositions  Spectrograms have been proved to be a useful representation for audio processing. They consist in 2D images representing sequences of Short Time Fourier Transform (STFT) with time and frequency as axes  and brightness representing the strength of a frequency component at each time frame. In such they appear a natural domain to apply the CNNS architectures for images directly to sound. Between magnitude and phase spectrograms  magnitude spectrograms contain most the structure of the signal. Phase spectrograms appear to show only little temporal and spectral regularities.  In this project  I will use magnitude spectrograms as a representation of sound (cf image below) in order to predict the noise model to be subtracted to a noisy voice spectrogram.  <img src=""img/sound_to_spectrogram.png"" alt=""sound representation"" title=""sound representation"" />  The project is decomposed in three modes: `data creation`  `training` and `prediction`.   """;Computer Vision;https://github.com/OlgaFomin/Speech-enhancement
"""WIDER Face Dataset WIDER FACE dataset is a Face Mask Segmentation benchmark dataset  of which images are selected from the publicly available WIDER dataset.  This data have 32 203 images and 393 703 faces are labeled with a high degree of variability in scale  pose and occlusion as depicted in the sample images. In this project  we are using 409 images and around 1000 faces for ease of computation.    We will be using transfer learning on an already trained model to build our segmenter. We will perform transfer learning on the MobileNet model which is already trained to perform image segmentation. We will need to train the last 6-7 layers and freeze the remaining layers to train the model for face mask segmentation. To be able to train the MobileNet model for face mask segmentation  we will be using the WIDER FACE dataset for various images with a single face and multiple faces. The output of the model is the face mask segmented data which masks the face in an image. We learn to build a face mask segmentation model using Keras supported by Tensorflow.    """;General;https://github.com/amol-matkar/Face-Mask-Segmentation
"""WIDER Face Dataset WIDER FACE dataset is a Face Mask Segmentation benchmark dataset  of which images are selected from the publicly available WIDER dataset.  This data have 32 203 images and 393 703 faces are labeled with a high degree of variability in scale  pose and occlusion as depicted in the sample images. In this project  we are using 409 images and around 1000 faces for ease of computation.    We will be using transfer learning on an already trained model to build our segmenter. We will perform transfer learning on the MobileNet model which is already trained to perform image segmentation. We will need to train the last 6-7 layers and freeze the remaining layers to train the model for face mask segmentation. To be able to train the MobileNet model for face mask segmentation  we will be using the WIDER FACE dataset for various images with a single face and multiple faces. The output of the model is the face mask segmented data which masks the face in an image. We learn to build a face mask segmentation model using Keras supported by Tensorflow.    """;Computer Vision;https://github.com/amol-matkar/Face-Mask-Segmentation
"""This is a Project for Continuous Control Deep Reinforcement Learning Nanodegree @ Udacity. The Task is to follow a target with a multijoint robot arm. A DDPG model is applied to accomplish the task. The model achieves the desired +30 score on average per episode.   """;Reinforcement Learning;https://github.com/petsol/ContinuousControl_UnityAgent_DDPG_Udacity
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/yet124/yolov3-pytorch
"""In a very general way  recommender systems are algorithms aimed at suggesting relevant items to users (items being movies to watch  text to read  products to buy or anything else depending on industries).  **There are two main data selection methods:**  Collaborative-filtering: In collaborative-filtering items are recommended  for example hotels  based on how similar your user profile is to other users’  finds the users that are most similar to you and then recommends items that they have shown a preference for. This method suffers from the so-called cold-start problem: If there is a new hotel  no-one else would’ve yet liked or watched it  so you’re not going to have this in your list of recommended hotels  even if you’d love it.  Content-based filtering: This method uses attributes of the content to recommend similar content. It doesn’t have a cold-start problem because it works through attributes or tags of the content  such as views  Wi-Fi or room types  so that new hotels can be recommended right away.  The point of content-based is that we have to know the content of both user and item. Usually you construct user-profile and item-profile using the content of shared attribute space. For example  for a movie  you represent it with the movie stars in it and the genres (using a binary coding for example).  **There are a number of popular encoding schemes but the main ones are:**   - One-hot encoding - Term frequency–inverse document frequency (TF-IDF) encoding - Word embeddings  In this project  we will be discussing content-based filtering of recommender engine  turning implicit attributes into explicit features for hotel recommender engine.     """;General;https://github.com/alantancr/Hotel-Recommender
"""In a very general way  recommender systems are algorithms aimed at suggesting relevant items to users (items being movies to watch  text to read  products to buy or anything else depending on industries).  **There are two main data selection methods:**  Collaborative-filtering: In collaborative-filtering items are recommended  for example hotels  based on how similar your user profile is to other users’  finds the users that are most similar to you and then recommends items that they have shown a preference for. This method suffers from the so-called cold-start problem: If there is a new hotel  no-one else would’ve yet liked or watched it  so you’re not going to have this in your list of recommended hotels  even if you’d love it.  Content-based filtering: This method uses attributes of the content to recommend similar content. It doesn’t have a cold-start problem because it works through attributes or tags of the content  such as views  Wi-Fi or room types  so that new hotels can be recommended right away.  The point of content-based is that we have to know the content of both user and item. Usually you construct user-profile and item-profile using the content of shared attribute space. For example  for a movie  you represent it with the movie stars in it and the genres (using a binary coding for example).  **There are a number of popular encoding schemes but the main ones are:**   - One-hot encoding - Term frequency–inverse document frequency (TF-IDF) encoding - Word embeddings  In this project  we will be discussing content-based filtering of recommender engine  turning implicit attributes into explicit features for hotel recommender engine.     """;Natural Language Processing;https://github.com/alantancr/Hotel-Recommender
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Nimesh-Patel/BERT
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/Yang2446/pointnet
"""GANs have proven to be very powerful generative models. So  here's a well-structured **Tensorflow** project containing implementations of some GANs architectures.   """;Computer Vision;https://github.com/DarkGeekMS/Tensorflow-GANs-Architectures-Implementation
"""The goal of the project is to identify flowers belonging to five classes: daisy  tulip  rose  sunflower and dandelion.  The dataset is available for download at https://www.kaggle.com/alxmamaev/flowers-recognition. I have built a simple convolutional neural network (CNN) using tensorflow and keras to classify the flowers. Furthermore  I have implemented tranfer learning where I have used the weights of the InceptionV3 model (https://arxiv.org/abs/1512.00567) trained on the ImageNet Dataset (http://www.image-net.org/).   """;Computer Vision;https://github.com/manashpratim/Flower_Recognition
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/xxnote/stunning-doodle
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/hwillard98/htmlify-yolo-training
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/CalvinXu17/yolov3
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/ivychill/insightface
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/wchh127/yykf
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/wchh127/yykf
"""We've desinged a novel neural architecture search framework for generative adversarial networks (GANs)  dubbed AutoGAN. Experiments validate the effectiveness of AutoGAN on the task of unconditional image generation. Specifically  our discovered architectures achieve highly competitive performance on unconditional image generation task of CIFAR-10  which obtains a record FID score of **12.42**  a competitive Inception score of **8.55**.   **RNN controller:** <p align=""center"">   <img src=""imgs/ctrl.png"" alt=""ctrl"" width=""90%""> </p>  **Search space:** <p align=""center"">   <img src=""imgs/ss.png"" alt=""ss"" width=""30%""> </p>  **Discovered network architecture:** <p align=""center"">   <img src=""imgs/cifar_arch1.png"" alt=""cifar_arch1"" width=""75%""> </p>   """;General;https://github.com/VITA-Group/AutoGAN
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/liuky74/caffe_liuky74
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/xzgz/caffe-ssd
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Nimesh-Patel/BERT
"""It is project in KAIST CS492(I) course. With NSML of NAVER  implement shopping item object detection model.   ![Alt text](Image/0a5e810ae2cbbf0bdbce393ed8209498.jpg) ![Alt text](Image/0a70b8806168e481d63f8331bbdf00f8.jpg)  These are the example of data. Our team's approach is to exploit [FixMatch](https://arxiv.org/pdf/2001.07685.pdf) to [MixMatch](https://arxiv.org/pdf/1905.02249.pdf).   """;General;https://github.com/DonghwanKIM0101/CS492I_CV
"""GANs have proven to be very powerful generative models. So  here's a well-structured **Tensorflow** project containing implementations of some GANs architectures.   """;General;https://github.com/DarkGeekMS/Tensorflow-GANs-Architectures-Implementation
"""Remote sensing offers the unique opportunity to monitor the evolution of human settlements from space.   Using multi-spectral images from Landsat 5  Landsat 7  Landsat 8 along with nighttime images from DMSP-OLS and NPP-VIIRS  this project aims to:  - Build a model quantifying the footprint of cities and monitor their evolution over time. - Use the model to provide an empirical validation of the scaling law in the city size distribution  at difference scales (regional level  country level  worldwide).  Yearly images between 1992 to 2020 of Las Vegas  Nevada - USA   |RGB composites             |  Nighttime Lights        | |:-------------------------:|:-------------------------:| ![Alt Text](./data/demo/las_vegas_area_rgb.gif) |  ![Alt Text](./data/demo/las_vegas_area_nl.gif) |Image segmentation             |  Segmentation legend    | |![Alt Text](./data/demo/las_vegas_area_preds.gif) | ![Alt Text](./data/demo/legend.png) |   """;Computer Vision;https://github.com/badrbmb/cities-watch
"""PGPE is an algorithm for computing approximate policy gradients for Reinforcement Learning (RL) problems. `pgpelib` provides a clean  scalable and easily extensible implementation of PGPE  and also serves as a reference (re)implementation of **ClipUp** [[2](#references)]  an optimizer designed to work specially well with PGPE-style gradient estimation. Although they were developed in the context of RL  both PGPE and ClipUp are general purpose tools for solving optimization problems.  Here are some interesting RL agents trained in simulation with the [PGPE+ClipUp](#what-is-clipup) implementation in `pgpelib`.  <table>     <tr>         <td>HumanoidBulletEnv-v0<br />Score: 4853</td>         <td>             <img src=""images/pgpelib_HumanoidBulletEnv_score4853.gif"" alt=""HumanoidBulletEnv-v0"" />         </td>     <tr>     </tr>         <td>Humanoid-v2<br />Score: 10184</td>         <td>             <img src=""images/pgpelib_Humanoid_score10184.gif"" alt=""Humanoid-v2"" />         </td>     <tr>     </tr>         <td>Walker2d-v2<br />Score: 5232</td>         <td>             <img src=""images/pgpelib_Walker2d_score5232.gif"" alt=""Walker2d-v2"" />         </td>     </tr> </table>   """;General;https://github.com/nnaisense/pgpelib
""" Synapses play an important role in biological neural networks.  They're joint points of neurons where learning and memory happened. The picture below demonstrates that two neurons (red) connected through a branch chain of synapses which may  link to other neurons.   <p align='center'> <img src=""./picture/synapse.jpg"" alt=""synapse"" width=""80%"" /> </p>  Inspired by the synapse research of neuroscience  we construct a simple model that can describe some key properties of a synapse.   <p align='center'> <img src=""./picture/synapse-unit.png"" alt=""synpase"" width=""70%"" />  </p>  A Synaptic Neural Network (SynaNN) contains non-linear synapse networks that connect to neurons. A synapse consists of an input from the excitatory-channel  an input from the inhibitory-channel  and an output channel which sends a value to other synapses or neurons. The synapse function is  <p align='center'> <img src=""https://latex.codecogs.com/svg.latex?S(x y;\alpha \beta)=\alpha%20x(1-\beta%20y)"" </p>  where x∈(0 1) is the open probability of all excitatory channels and α >0 is the parameter of the excitatory channels; y∈(0 1) is the open probability of all inhibitory channels and β∈(0 1) is the parameter of the inhibitory channels. The surface of the synapse function is    <p align='center'> <img src=""./picture/synpase.png"" alt=""synpase"" width=""50%"" /> </p>  By combining deep learning  we expect to build ultra large scale neural networks to solve real-world AI problems. At the same time  we want to create an explainable neural network model to better understand what an AI model doing instead of a black box solution.  <p align='center'> <img src=""./picture/E425.tmp.png"" alt=""synpase"" width=""60%"" /> </p>  A synapse graph is a connection of synapses. In particular  a synapse tensor is fully connected synapses from input neurons to output neurons with some hidden layers. Synapse learning can work with gradient descent and backpropagation algorithms. SynaNN can be applied to construct MLP  CNN  and RNN models.  Assume that the total number of input of the synapse graph equals the total number of outputs  the fully-connected synapse graph is defined as   <p align='center'> <img src=""https://latex.codecogs.com/svg.latex?y_{i}(\textbf{x};%20\pmb\beta_i)%20=%20\alpha_i%20x_{i}{\prod_{j=1}^{n}(1-\beta_{ij}x_{j})} \%20for\%20all\%20i%20\in%20[1 n]""/> </p>  where   <p align='center'> <img src=""https://latex.codecogs.com/svg.latex?\textbf{x}=(x_1 \cdots x_n) \textbf{y}=(y_1 \cdots y_n) x_i y_i\in(0 1) \alpha_i \geq 1 \beta_{ij}\in(0 1))""/> </p>  Transformed to tensor/matrix representation  we have the synapse log formula    <p align='center'> <img src=""https://latex.codecogs.com/svg.latex?log(\textbf{y})=log(\textbf{x})+{\textbf{1}_{|x|}}*log(\textbf{1}_{|\beta|}-diag(\textbf{x})*\pmb{\beta}^T)""/> </p>  We are going to implement this formula for fully-connected synapse network with Tensorflow and PyTorch in the examples.  Moreover  we can design synapse graph like circuit below for some special applications.   <p align='center'> <img src=""./picture/synapse-flip.png"" alt=""synapse-flip"" width=""50%"" /> </p>   """;General;https://github.com/Neatware/SynaNN
"""This repository contains the official code and pretrained models for [CoaT: Co-Scale Conv-Attentional Image Transformers](http://arxiv.org/abs/2104.06399). It introduces (1) a co-scale mechanism to realize fine-to-coarse  coarse-to-fine and cross-scale attention modeling and (2) an efficient conv-attention module to realize relative position encoding in the factorized attention.  <img src=""./figures/model-acc.svg"" alt=""Model Accuracy"" width=""600"" />  For more details  please refer to [CoaT: Co-Scale Conv-Attentional Image Transformers](http://arxiv.org/abs/2104.06399) by [Weijian Xu*](https://weijianxu.com/)  [Yifan Xu*](https://yfxu.com/)  [Tyler Chang](https://tylerachang.github.io/)  and [Zhuowen Tu](https://pages.ucsd.edu/~ztu/).   """;Computer Vision;https://github.com/mlpc-ucsd/CoaT
"""<div align=center><img width=""100%"" src=""figs/FFB6D_overview.png""/></div>  [FFB6D](https://arxiv.org/abs/2103.02242v1) is a general framework for representation learning from a single RGBD image  and we applied it to the 6D pose estimation task by cascading downstream prediction headers for instance semantic segmentation and 3D keypoint voting prediction from PVN3D([Arxiv](https://arxiv.org/abs/1911.04231)  [Code](https://github.com/ethnhe/PVN3D)  [Video](https://www.bilibili.com/video/av89408773/)).  At the representation learning stage of FFB6D  we build **bidirectional** fusion modules in the **full flow** of the two networks  where fusion is applied to each encoding and decoding layer. In this way  the two networks can leverage local and global complementary information from the other one to obtain better representations. Moreover  at the output representation stage  we designed a simple but effective 3D keypoints selection algorithm considering the texture and geometry information of objects  which simplifies keypoint localization for precise pose estimation.  Please cite [FFB6D](https://arxiv.org/abs/2103.02242v1) & [PVN3D](https://arxiv.org/abs/1911.04231) if you use this repository in your publications:  ``` @InProceedings{He_2021_CVPR  author = {He  Yisheng and Huang  Haibin and Fan  Haoqiang and Chen  Qifeng and Sun  Jian}  title = {FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation}  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}  month = {June}  year = {2021} }  @InProceedings{He_2020_CVPR  author = {He  Yisheng and Sun  Wei and Huang  Haibin and Liu  Jianran and Fan  Haoqiang and Sun  Jian}  title = {PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation}  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}  month = {June}  year = {2020} } ```   """;Computer Vision;https://github.com/hz-ants/FFB6D
"""- *main.py*: main script to train or evaluate models - *train.py*: training and evaluation part of the code - *config*: storing configuration of datasets (and maybe other things in the future) - *utils.pypy*: useful functions - *getbest.py*: display the best validation error of each saving folder - *dataloader.py*: defines *getDataloaders* function which is used to load datasets - *models*: a folder storing all network models. Each script in it should contain a *createModel(\*\*kwargs)* function that takes the arguments and return a model (subclass of nn.Module) for training - *scripts*: a folder storing example training commands in UNIX shell scripts   """;General;https://github.com/felixgwu/img_classification_pk_pytorch
"""- *main.py*: main script to train or evaluate models - *train.py*: training and evaluation part of the code - *config*: storing configuration of datasets (and maybe other things in the future) - *utils.pypy*: useful functions - *getbest.py*: display the best validation error of each saving folder - *dataloader.py*: defines *getDataloaders* function which is used to load datasets - *models*: a folder storing all network models. Each script in it should contain a *createModel(\*\*kwargs)* function that takes the arguments and return a model (subclass of nn.Module) for training - *scripts*: a folder storing example training commands in UNIX shell scripts   """;Computer Vision;https://github.com/felixgwu/img_classification_pk_pytorch
"""自作関数’T’をtransformerで分類し   その根拠も示すプログラム   実装の大部分を http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks のプログラムを参考に実装した      Thanks to   Guillaume Klein and Yoon Kim and Yuntian Deng and Jean Senellart and Alexander M. Rush   and   https://arxiv.org/pdf/1706.03762.pdf    """;General;https://github.com/kotu931226/classifier_transformer_pytorch
"""自作関数’T’をtransformerで分類し   その根拠も示すプログラム   実装の大部分を http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks のプログラムを参考に実装した      Thanks to   Guillaume Klein and Yoon Kim and Yuntian Deng and Jean Senellart and Alexander M. Rush   and   https://arxiv.org/pdf/1706.03762.pdf    """;Natural Language Processing;https://github.com/kotu931226/classifier_transformer_pytorch
"""Author: Chih-Chung Hsu (cchsu@mail.npust.edu.tw) Department of Management Information Systems National Pingtung University of Science and Technology  This code is the implementation of our recent paper released at September 2018 -- Learning to Detect Fake Face Images in the Wild (ArXiv: https://arxiv.org/abs/1809.08754) and our recent paper published on ICIP 2019 -- Y. Zhuang and C. Hsu  ""Detecting Generated Image Based on a Coupled Network with Two-Step Pairwise Learning "" 2019 IEEE International Conference on Image Processing (ICIP)  Taipei  Taiwan  2019  pp. 3212-3216.  Any suggestion/problem is welcome.   - Our GAN synthesizers are based on https://github.com/LynnHo/DCGAN-LSGAN-WGAN-WGAN-GP-Tensorflow - The final results of the proposed method can be reproduced by executing ResNet_DeepUD.ipynb. - The results without contrastive loss of the proposed NN architecture can be reproduced by executing ResNet_DeepUD_noContrastive.ipynb.  ---   """;Computer Vision;https://github.com/jesse1029/Fake-Face-Images-Detection-Tensorflow
"""This is the Pytorch implementation for reproducing the results in   > CIKM 2021. Yifei Shen  Yongji Wu  Yao Zhang  Caihua Shan  Jun Zhang  Khaled B. Letaief  Dongsheng Li(2021). How Powerful is Graph Convolution for Recommendation? [Paper in arXiv](https://arxiv.org/abs/2108.07567)  The code is heavily built on LightGCN's code.  >SIGIR 2020. Xiangnan He  Kuan Deng  Xiang Wang  Yan Li  Yongdong Zhang  Meng Wang(2020). LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation  [Paper in arXiv](https://arxiv.org/abs/2002.02126).  (See Pytorch [implementation](https://github.com/gusye1234/LightGCN-PyTorch)) We also adopt exactly the same dataset and train/test splitting. The code is not optimized for speed but rather for simplicity.   """;Graphs;https://github.com/yshenaw/GF_CF
"""We build a plant disease diagnosis system on Android  by implementing a deep convolutional neural network with Tensorflow to detect disease from various plant leave images.   Generally  due to the size limitation of the dataset  we adopt the transefer learning in this system. Specifically  we retrain the MobileNets [[1]](https://arxiv.org/pdf/1704.04861.pdf)  which is first trained on ImageNet dataset  on the plant disease datasets. Finally  we port the trained model to Android.   """;General;https://github.com/nyak10/mwc
"""We build a plant disease diagnosis system on Android  by implementing a deep convolutional neural network with Tensorflow to detect disease from various plant leave images.   Generally  due to the size limitation of the dataset  we adopt the transefer learning in this system. Specifically  we retrain the MobileNets [[1]](https://arxiv.org/pdf/1704.04861.pdf)  which is first trained on ImageNet dataset  on the plant disease datasets. Finally  we port the trained model to Android.   """;Computer Vision;https://github.com/nyak10/mwc
"""Author: Chih-Chung Hsu (cchsu@mail.npust.edu.tw) Department of Management Information Systems National Pingtung University of Science and Technology  This code is the implementation of our recent paper released at September 2018 -- Learning to Detect Fake Face Images in the Wild (ArXiv: https://arxiv.org/abs/1809.08754) and our recent paper published on ICIP 2019 -- Y. Zhuang and C. Hsu  ""Detecting Generated Image Based on a Coupled Network with Two-Step Pairwise Learning "" 2019 IEEE International Conference on Image Processing (ICIP)  Taipei  Taiwan  2019  pp. 3212-3216.  Any suggestion/problem is welcome.   - Our GAN synthesizers are based on https://github.com/LynnHo/DCGAN-LSGAN-WGAN-WGAN-GP-Tensorflow - The final results of the proposed method can be reproduced by executing ResNet_DeepUD.ipynb. - The results without contrastive loss of the proposed NN architecture can be reproduced by executing ResNet_DeepUD_noContrastive.ipynb.  ---   """;General;https://github.com/jesse1029/Fake-Face-Images-Detection-Tensorflow
"""![image1]   """;Reinforcement Learning;https://github.com/cipher982/ppo-exploration
"""In the paper writers used 2 GAN types which are DCGAN and BiGAN classify images. DCGAN wass used for creating happy  sad  male  female faces. BiGAN  which employed from Adversarial Feature Learning  was ran on Imagenet ILSVRC 2012 dataset to classify images.   ---   """;General;https://github.com/COMP6248-Reproducability-Challenge/CapturingHumanCategoryRepresentation
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   """;Computer Vision;https://github.com/rbgirshick/fast-rcnn
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/goldenbili/Bert_Test2
"""DeepLab is a state-of-art deep learning system for semantic image segmentation built on top of [Caffe](http://caffe.berkeleyvision.org).  It combines (1) *atrous convolution* to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks  (2) *atrous spatial pyramid pooling* to robustly segment objects at multiple scales with filters at multiple sampling rates and effective fields-of-views  and (3) densely connected conditional random fields (CRF) as post processing.  This distribution provides a publicly available implementation for the key model ingredients reported in our latest [arXiv paper](http://arxiv.org/abs/1606.00915). It also contains implementations for **all** methods reported in all our previous papers.  Please consult and consider citing the following papers:      @article{CP2016Deeplab        title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets  Atrous Convolution  and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        journal={arXiv:1606.00915}        year={2016}     }      @inproceedings{CY2016Attention        title={Attention to Scale: Scale-aware Semantic Image Segmentation}        author={Liang-Chieh Chen and Yi Yang and Jiang Wang and Wei Xu and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{CB2016Semantic        title={Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform}        author={Liang-Chieh Chen and Jonathan T Barron and George Papandreou and Kevin Murphy and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{PC2015Weak        title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation}        author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille}        booktitle={ICCV}        year={2015}     }      @inproceedings{CP2015Semantic        title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        booktitle={ICLR}        year={2015}     }   Note that if you use the densecrf implementation  please consult and cite the following paper:      @inproceedings{KrahenbuhlK11        title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}        author={Philipp Kr{\""{a}}henb{\""{u}}hl and Vladlen Koltun}        booktitle={NIPS}        year={2011}     }   """;Computer Vision;https://github.com/liarba/caffe_dev
"""This repository contains my master's (ongoing) work on model compression techniques at YOLOv3. **It is freely available for redistribution under the GPL-3.0 license**.  This repository is based on [YOLOv3 Ultralytics](https://github.com/ultralytics/yolov3).  Currently evaluated approaches: * Lottery Tickets Hypothesis (Iterative Magnitude based Pruning) * Continuous Sparsification (Iterative Gradient based Pruning) * Knowledge Distillation (classical approach) * Generative Adversarial Network (GAN) based Knowledge Distillation * Neural Architecture Search (NAS) from MobileNet V3 * NAS from YOLO Nano   """;Computer Vision;https://github.com/AndreydeAguiarSalvi/yolo_compression
"""There are many repositories available with multi-agent RL implementation either in older version of tensorflow or in pytorch. Those repositories have lots of dependecies and need spend good amount of time to figure out code structure. Lots of dependencies and complicated code structure can make customization difficult for someone new in RL  with not much knowledge about different RL tools present in python and could be time consuming.  This work is a part of my MS project where I implemented MADDPG algorithm in keras. My efforts was to make code structure less complicated and implement it using very basic deep learning libraries. For customization of the code you just need basic knowledge of keras and understanding of Deep Reinforcement Learning and you will be good to go.  The project was build-up by getting motivation from keras implementation of DDPG algorithm on https://keras.io/examples/rl/ddpg_pendulum/  Their are many possible improvements possible in this work. With addition of new agent code needs bit of customization. Although it can be generalized. Project is still open and any contributions are most welcome.  I have added trained model for demosntration   https://user-images.githubusercontent.com/50385421/114270020-b837d180-9a27-11eb-89ac-635e01092d96.mp4   """;Reinforcement Learning;https://github.com/pr-shukla/maddpg-keras
"""* Guassian: http://cs229.stanford.edu/section/more_on_gaussians.pdf * http://www.scholarpedia.org/article/Bayesian * Check data normal: http://allendowney.blogspot.com/2013/08/are-my-data-normal.html   """;Computer Vision;https://github.com/taohu88/BayesianML
"""* Guassian: http://cs229.stanford.edu/section/more_on_gaussians.pdf * http://www.scholarpedia.org/article/Bayesian * Check data normal: http://allendowney.blogspot.com/2013/08/are-my-data-normal.html   """;General;https://github.com/taohu88/BayesianML
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/TSLNIHAOGIT/bert_run
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/TSLNIHAOGIT/bert_run
"""SSD(Single Shot MultiBox Detector) is a state-of-art object detection algorithm  brought by Wei Liu and other wonderful guys  see [SSD: Single Shot MultiBox Detector @ arxiv](https://arxiv.org/abs/1512.02325)  recommended to read for better understanding.  Also  SSD currently performs good at PASCAL VOC Challenge  see [http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=3](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=3)   """;Computer Vision;https://github.com/nirajdevpandey/Object-detection-and-localization-using-SSD-
"""The aim of this project is to buid a classifier that can identify if a remotely sensed target is a Ship or a drifting Iceberg. This is an attempt to solve the problem stated in one of the competitions at Kaggle.com ([here](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge)).  """;Computer Vision;https://github.com/singh-shakti94/Deep-Learning-Project
"""This repository is a PyTorch implementation for semantic segmentation / scene parsing. The code is easy to use for training and testing on various datasets. The codebase mainly uses ResNet50/101/152 as backbone and can be easily adapted to other basic classification structures. Implemented networks including [PSPNet](https://hszhao.github.io/projects/pspnet) and [PSANet](https://hszhao.github.io/projects/psanet)  which ranked 1st places in [ImageNet Scene Parsing Challenge 2016 @ECCV16](http://image-net.org/challenges/LSVRC/2016/results)  [LSUN Semantic Segmentation Challenge 2017 @CVPR17](https://blog.mapillary.com/product/2017/06/13/lsun-challenge.html) and [WAD Drivable Area Segmentation Challenge 2018 @CVPR18](https://bdd-data.berkeley.edu/wad-2018.html). Sample experimented datasets are [ADE20K](http://sceneparsing.csail.mit.edu)  [PASCAL VOC 2012](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6) and [Cityscapes](https://www.cityscapes-dataset.com).  <img src=""./figure/pspnet.png"" width=""900""/>   """;Computer Vision;https://github.com/hszhao/semseg
"""This repository is a PyTorch implementation for semantic segmentation / scene parsing. The code is easy to use for training and testing on various datasets. The codebase mainly uses ResNet50/101/152 as backbone and can be easily adapted to other basic classification structures. Implemented networks including [PSPNet](https://hszhao.github.io/projects/pspnet) and [PSANet](https://hszhao.github.io/projects/psanet)  which ranked 1st places in [ImageNet Scene Parsing Challenge 2016 @ECCV16](http://image-net.org/challenges/LSVRC/2016/results)  [LSUN Semantic Segmentation Challenge 2017 @CVPR17](https://blog.mapillary.com/product/2017/06/13/lsun-challenge.html) and [WAD Drivable Area Segmentation Challenge 2018 @CVPR18](https://bdd-data.berkeley.edu/wad-2018.html). Sample experimented datasets are [ADE20K](http://sceneparsing.csail.mit.edu)  [PASCAL VOC 2012](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6) and [Cityscapes](https://www.cityscapes-dataset.com).  <img src=""./figure/pspnet.png"" width=""900""/>   """;General;https://github.com/hszhao/semseg
"""This project is an pytorch implement R-FCN and CoupleNet  large part code is reference from [jwyang/faster-rcnn.pytorch](https://github.com/jwyang/faster-rcnn.pytorch). The R-FCN structure is refer to [Caffe R-FCN](https://github.com/daijifeng001/R-FCN) and [Py-R-FCN](https://github.com/YuwenXiong/py-R-FCN)  - For R-FCN  mAP@0.5 reached 73.2 in VOC2007 trainval dataset - For CoupleNet  mAP@0.5 reached 75.2 in VOC2007 trainval dataset   """;Computer Vision;https://github.com/princewang1994/R-FCN.pytorch
"""This is a quick implementation of the DenseNet model described in the paper *""Densely Connected Convolutional Networks""* by Huang et al. ([arXiv](https://arxiv.org/abs/1608.06993))  It has only been tested on the Cifar-10 dataset without data augmentation  but it should work fine on any dataset.   """;General;https://github.com/satishjasthi/Densenet_smplified
"""This is a quick implementation of the DenseNet model described in the paper *""Densely Connected Convolutional Networks""* by Huang et al. ([arXiv](https://arxiv.org/abs/1608.06993))  It has only been tested on the Cifar-10 dataset without data augmentation  but it should work fine on any dataset.   """;Computer Vision;https://github.com/satishjasthi/Densenet_smplified
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/qilei123/DeformableConvV2_crop
"""- parameters.py contains the parameter environment setting - Transformer package comtains the main architecture of Transformer Encoder-Decoder Model - to be continued       """;General;https://github.com/shuaishuaij/Machine-Translation
"""- parameters.py contains the parameter environment setting - Transformer package comtains the main architecture of Transformer Encoder-Decoder Model - to be continued       """;Natural Language Processing;https://github.com/shuaishuaij/Machine-Translation
"""- preprocess.py: a Python script to index the data. - train.lua: a Lua script to train and load the QA model. - interact.lua: a Lua script to load the saved model and querying user requirements. - model.dot: a grahical representation of the model configuration. - output.png: a sample output screenshot.  """;General;https://github.com/rakeshbm/QA-using-Torch
"""CGMM is a generative approach to learning contexts in graphs. It combines information diffusion and local computation through the use of a deep architecture and stationarity assumptions. The model does NOT preprocess the graph into a fixed structure before learning. Instead  it works with graphs of any size and shape while retaining scalability. Experiments show that this model works well compared to expensive kernel methods that extensively analyse the entire input structure in order to extract relevant features. In contrast  CGMM extract more abstract features as the architecture is built (incrementally).   We hope that the exploitation of the proposed framework  which can be extended in many directions  can contribute to the extensive use of both generative and discriminative approaches to the adaptive processing of structured data.   """;Graphs;https://github.com/diningphil/CGMM
"""**XLNet** is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally  XLNet employs [Transformer-XL](https://arxiv.org/abs/1901.02860) as the backbone model  exhibiting excellent performance for language tasks involving long context. Overall  XLNet achieves state-of-the-art (SOTA) results on various downstream language tasks including question answering  natural language inference  sentiment analysis  and document ranking.  For a detailed description of technical details and experimental results  please refer to our paper:  ​        [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)  ​        Zhilin Yang\*  Zihang Dai\*  Yiming Yang  Jaime Carbonell  Ruslan Salakhutdinov  Quoc V. Le   ​        (*: equal contribution)   ​        Preprint 2019      """;Natural Language Processing;https://github.com/huggingface/xlnet
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  <center>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | |:-------|:-----:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | 300 | | [Faster R-CNN (ZF)](https://github.com/ShaoqingRen/faster_rcnn) | 62.1 | 17 | 300 | | [YOLO](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | | [Fast YOLO](http://pjreddie.com/darknet/yolo/) | 52.7 | 155 | 98 | | SSD300 (VGG16) | 72.1 | 58 | 7308 | | SSD300 (VGG16  cuDNN v5) | 72.1 | 72 | 7308 | | SSD500 (VGG16) | **75.1** | 23 | 20097 |  </center>   """;Computer Vision;https://github.com/freeniliang/caffe-ssd
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/xiaoxd16/Palm
"""Space exploration is now the fastest-growing subject. This was  however at the cost of billions worth of projects and twice the amount effort. Obi-Wan Kenobi is an integrated system of a space rover updated and modified to use minimum energy with the greatest efficiency possible. It aims to revolutionize the entire industry  rendering it a profitable organization. This is managed by a new arm designed to indulge the biggest and most accurate amount of information while also not sacrificing the energy and a brand new software that takes the camera’s 2-D image and process it with sensors on the rover to produce a high-quality 3-D environment. How We Addressed This Challenge Obi-Wan Kenobi is space rover designed to gather information about foreign plants and there composition in a new way. We have made it with many implementations improving on the previous rovers using new technics  mechanics  and software in astronomical ways. Obi-Wan Kenobi aims to take the prior models of space rovers and modify on them in ways that revolutionize the entire subject of space exploration by maximizing the amount of information it can indulge and allaying every drop of wasted energy putting it into more work.     """;Computer Vision;https://github.com/ahmedabdel-hady/Space-Xplores-Nasaspaceapp2020
"""Welcome to the code repository of [How to train your MAML](https://arxiv.org/abs/1810.09502). This repository includes code for training both MAML and MAML++ models  as well as data providers and the datasets for both. By using this codebase you agree to the terms  and conditions in the [LICENSE](https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/blob/master/LICENSE) file. If you choose to use the Mini-Imagenet dataset  you must abide by the terms and conditions in the [ImageNet LICENSE](https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/blob/master/imagenet_license.md)   """;General;https://github.com/dkalpakchi/ReproducingSCAPytorch
"""Using DCGAN model to generate Asian faces (traing set： ＡＦＡＤ).   """;Computer Vision;https://github.com/eecsdanny/AFAD_DCGAN
"""Solution developed using this environment:  - Python 3 (based on Anaconda installation)  - Pytorch 1.1.0+ and torchvision 0.3.0+   - Nvidia apex https://github.com/NVIDIA/apex  - https://github.com/skvark/opencv-python  - https://github.com/aleju/imgaug   Hardware: Current training batch size requires at least 2 GPUs with 12GB each. (Initially trained on Titan V GPUs). For 1 GPU batch size and learning rate should be found in practice and changed accordingly.  ""train""  ""tier3"" and ""test"" folders from competition dataset should be placed to the current folder.  Use ""train.sh"" script to train all the models. (~7 days on 2 GPUs). To generate predictions/submission file use ""predict.sh"". ""evalution-docker-container"" folder contains code for docker container used for final evalution on hold out set (CPU version).   """;Computer Vision;https://github.com/DIUx-xView/xView2_first_place
"""This repository contains an extended caffe wich is modified from caffe version of [yjxiong](https://github.com/yjxiong/caffe/tree/mem) and introduces many new features.      """;Computer Vision;https://github.com/xiamenwcy/extended-caffe
"""* This is the unofficial  implementation of the ""CenterNet:Objects as Points"".In my experiment  it was not based on the DLA34  Hourglass and other networks in the original paper. I simply modified shufflenetv2_1.0x and yolov3  and kept their feature extraction part  then connected to centernet_detect_head  and did not use dcn convolution. * This is just a simple attempt to the effect of the algorithm.I only have one 1080ti **I did not use any data augmentation and any other tricks during training，so the model is not very good still need more work to get good results**.If it helps you  please give me a star.You can read my Chinese notes.(1)<https://zhuanlan.zhihu.com/p/68383078>         (2)<https://zhuanlan.zhihu.com/p/76378871> * Official implementation:<https://github.com/xingyizhou/CenterNet> * CenterNet:Objects as Points:<https://arxiv.org/pdf/1904.07850.pdf> * Shufflenetv2 is modified from:<https://github.com/timctho/shufflenet-v2-tensorflow> * Shufflenetv2:<https://arxiv.org/abs/1807.11164> * Yolov3 is is modified from:<https://github.com/wizyoung/YOLOv3_TensorFlow> * Yolov3:<https://pjreddie.com/media/files/papers/YOLOv3.pdf> * ExFuse:<https://arxiv.org/abs/1804.03821>  """;Computer Vision;https://github.com/xggIoU/centernet_tensorflow_wilderface_voc
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/wayalhruhi/gogle_bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/wayalhruhi/gogle_bert
"""The end goal of this project is to try and get the area of leaves from a picture. To do this  the part of the image that is the plant needs to be segmented from the background. There are many ways to do image segmentation such as thresholding the RGB values or using clustering to find patches of similar color  both of which are often paired with edge detection methods to try and find the edges of objects and only keep the pixels that are of interest. However  these methods are sensitive to different lighting conditions and often require color space transformations from RGB to HSV in order to be robust. These methods also don't determine which class the foreground of the image is a part of. However  with a segmentation neural network  there would not have to be any color space transformation of the image and it would output predictions for each class in the foreground that it was told to try and predict.  Once the image is segmented  an estimation of the leaf area could be produced by properly scaling the proportion of the image that contains the desired class to some reference object that has known dimensions. Since there are no reference objects in this data set  the goal for this project would be to get the proportion of each class in the image.   """;Computer Vision;https://github.com/jkhadley/capstone-project
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/bhavitvyamalik/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/bhavitvyamalik/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/theQuert/inlpfun
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/theQuert/inlpfun
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/tcnguyen/bert
"""**BERT** (Bidirectional Encoder Representations from Transformers) is a NLP model developed by Google that follows the structure of a transformer. That structure was used to create models that NLP practicioners can then download and use for free such as RoBerta  XLNet or AlBert. You can either use these models to extract high quality language features from your text data  or you can fine-tune these models on a specific task (classification  entity recognition  question answering  etc.) with your own data to produce state of the art predictions.  In this repository  we will try to modify and fine-tune BERT to create a powerful NLP model for Question Answering  which means giving a text and a question  the model will be able to find the answer to the question in the text.  The dataset that we will be using is **SQuAD 2.0**. The dataset consist in a series of questions posed by crowdworkers on a set of Wikipedia articles  where the answer to every question is a segment of text  or span  from the corresponding reading passage (sometimes the question might be unanswerable).   """;Natural Language Processing;https://github.com/JMSaindon/NlpSquad
"""      probabilistic implementation of https://arxiv.org/abs/1410.5401 in julia        main.jl encapsulate of the function       con.jl contains definition of controller       utils.jl contains utility functions like content_finding  gated_interpolation etc        ntm.jl encapsulate all the modules       heads.jl contains read_head and write_head function's  """;General;https://github.com/shanyaanand/ntm
"""      probabilistic implementation of https://arxiv.org/abs/1410.5401 in julia        main.jl encapsulate of the function       con.jl contains definition of controller       utils.jl contains utility functions like content_finding  gated_interpolation etc        ntm.jl encapsulate all the modules       heads.jl contains read_head and write_head function's  """;Sequential;https://github.com/shanyaanand/ntm
"""This is an implementation for the paper **""Learning Invariant Representation for Unsupervised Image Restoration"" (CVPR 2020)**  a simple and efficient framework for unsupervised image restoration  which is injected into the general domain transfer architecture. More details could be found in the original paper.    """;Computer Vision;https://github.com/Wenchao-Du/LIR-for-Unsupervised-IR
"""This is an implementation for the paper **""Learning Invariant Representation for Unsupervised Image Restoration"" (CVPR 2020)**  a simple and efficient framework for unsupervised image restoration  which is injected into the general domain transfer architecture. More details could be found in the original paper.    """;General;https://github.com/Wenchao-Du/LIR-for-Unsupervised-IR
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/xitianxiaofeixue/BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/xitianxiaofeixue/BERT
"""For a receipt  each text bbox can be viewed as a node of a graph. Its positions  the attributes of bounding box  and the corresponding text are used as the node feature. Our goal is to label each node (text bounding box) with five different classes  including Company  Date  Address  Total and Other. Sample images are shown below:  <img src=""./docs/sroie.png"" width=""750""/>   """;Computer Vision;https://github.com/cyh1112/GraphNormalization
"""For a receipt  each text bbox can be viewed as a node of a graph. Its positions  the attributes of bounding box  and the corresponding text are used as the node feature. Our goal is to label each node (text bounding box) with five different classes  including Company  Date  Address  Total and Other. Sample images are shown below:  <img src=""./docs/sroie.png"" width=""750""/>   """;General;https://github.com/cyh1112/GraphNormalization
"""This repository contains an extended caffe wich is modified from caffe version of [yjxiong](https://github.com/yjxiong/caffe/tree/mem) and introduces many new features.      """;General;https://github.com/xiamenwcy/extended-caffe
"""| Parameter | Default | | --------- | ------- | | Config    | None    |   ***  This project is based on the pixel2style2pixel (pSp). pSp framework generates a series of style vectors based on a novel encoder network  which is fed into a pre-trained style generator to form an extended W + potential space. The encoder can directly reconstruct real input images.   """;Computer Vision;https://github.com/771979972/Paddle_pSp
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/SCismycat/bert_code_view
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/SCismycat/bert_code_view
"""This repository contains a two-stage-tracker. The detections generated by [YOLOv5](https://github.com/ultralytics/yolov5)  a family of object detection architectures and models pretrained on the COCO dataset  are passed to a [Deep Sort algorithm](https://github.com/ZQPei/deep_sort_pytorch) which tracks the objects. It can track any object that your Yolov5 model was trained to detect.    """;Computer Vision;https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch
"""This repo uses [*Simple Baselines*](http://openaccess.thecvf.com/content_ECCV_2018/html/Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper.html) as the baseline method for Pose Estimation.   [Res2Net](https://github.com/gasvn/Res2Net) is a powerful backbone architecture that can be easily implemented into state-of-the-art models by replacing the bottleneck with Res2Net module. More detail can be found on [ ""Res2Net: A New Multi-scale Backbone Architecture""](https://arxiv.org/pdf/1904.01169.pdf)   """;Computer Vision;https://github.com/Res2Net/Res2Net-Pose-Estimation
"""The two modified architectures are as follows: 1. A dual-transformer model that utilizes both sentence and token level embeddings. 2. A convolutional model that halves the dimensionality of BERT embeddings.   """;Natural Language Processing;https://github.com/TidalPaladin/neural-summarizer
"""An application for people who want to know the proper makeup.   """;Computer Vision;https://github.com/skkuse02/2019fall_41class_team8
"""This is an *unofficial* [PyTorch](https://github.com/pytorch/pytorch) implementation of the recent  paper ['A Simple Framework for Contrastive Learning of Visual  Representations'](https://arxiv.org/pdf/2002.05709.pdf). The arXiv version of this paper can be cited as ``` @article{chen2020simple    title={A simple framework for contrastive learning of visual representations}    author={Chen  Ting and Kornblith  Simon and Norouzi  Mohammad and Hinton  Geoffrey}    journal={arXiv preprint arXiv:2002.05709}    year={2020} } ``` The focus of this repository is to accurately reproduce the results in the paper using PyTorch. We use the original paper and the official [tensorflow repo](https://github.com/google-research/simclr) as our sources.    """;General;https://github.com/ae-foster/pytorch-simclr
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/pgtinsley/insightface_oge
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/wangkingkingking/ssd_mathocr
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/pesong/ssd-caffe
"""Essentially CIDA asks the question of whether and how to **go beyond current (categorical) domain adaptation regime** and proposes **the first approach to adapt across continuously indexed domains**. For example  instead of adapting from domain A to domain B  we would like to simultaneously adapt across infintely many domains in a manifold. This allows us to go beyond domain adaption and perform both [domain interpolation](https://github.com/hehaodele/CIDA#intra-dataset-results-on-real-world-medical-datasets) and [domain extrapolation](https://github.com/hehaodele/CIDA#intra-dataset-results-on-real-world-medical-datasets). See the following toy example.  <p align=""center""> <img src=""fig/blog-circle.png"" alt="""" data-canonical-src=""fig/blog-circle.png"" width=""95%""/> </p>  For a more **visual** introduction  feel free to take a look at this [video](https://www.youtube.com/watch?v=KtZPSCD-WhQ).   """;General;https://github.com/hehaodele/CIDA
"""Deep convolution-based single image super-resolution (SISR) networks embrace the benefits of learning from large-scale external image resources for local recovery  yet most existing works have ignored the long-range feature-wise similarities in natural images. Some recent works have successfully leveraged this intrinsic feature correlation by exploring non-local attention modules. However  none of the current deep models have studied another inherent property of images: cross-scale feature correlation. In this paper  we propose the first Cross-Scale Non-Local (CS-NL) attention module with integration into a recurrent neural network. By combining the new CS-NL prior with local and in-scale non-local priors in a powerful recurrent fusion cell  we can find more cross-scale feature correlations within a single low-resolution (LR) image. The performance of SISR is significantly improved by exhaustively integrating all possible priors. Extensive experiments demonstrate the effectiveness of the proposed CS-NL module by setting new state-of-the-arts on multiple SISR benchmarks.  ![CS-NL Attention](/Figs/Attention.png)  Cross-Scale Non-Local Attention.  ![CSNLN](/Figs/CSNLN.png)  The recurrent architecture with Self-Exemplars Mining (SEM) Cell.   """;General;https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention
"""Glaucoma is a retinal disease caused due to increased intraocular pressure in the eyes. It is the second most dominant cause of irreversible blindness after cataract  and if this remains undiagnosed  it may become the first common cause. Ophthalmologists use different comprehensive retinal examinations such as ophthalmoscopy  tonometry  perimetry  gonioscopy and pachymetry to diagnose glaucoma. But all these approaches are manual and time-consuming. Thus  a computer-aided diagnosis system may aid as an assistive measure for the initial screening of glaucoma for diagnosis purposes  thereby reducing the computational complexity. This paper presents a deep learning-based disc cup segmentation glaucoma network (DC-Gnet) for the extraction of structural features namely cup-to-disc ratio  disc damage likelihood scale and inferior superior nasal temporal regions for diagnosis of glaucoma. The proposed approach of segmentation has been trained and tested on RIM-One and Drishti-GS dataset. Further  based on experimental analysis  the DC-Gnet is found to outperform U-net  Gnet and Deep-lab architectures.  ![**Glaucomatous Eye**](Glaucoma.jpg)   """;Computer Vision;https://github.com/archit31uniyal/DC-Gnet
"""Data scientists often choose the uniform distribution or the normal distribution as the latent variable distribution when they build representative models of datasets. For example  the studies of the GANs [1] and the VAEs[2] used the uniform random distribution and the normal one  respectively.  As the approximate function implemented by neural networks is usually continuous  the topological structure of the latent variable distribution is preserved after the transformation from the latent variables space to the observable variables space. Given that the observed variables are distributed on a torus and that networks  for example the GANs  are trained with the latent variables sampled from the normal distribution  the structure of the projected distribution by the trained networks does not meet with the torus  even if residual error is small enough. Imagine another example where the observable variables follow a mixture distribution  of which clusters separate each other  trained variational autoencoder can encode the feature on the latent variable space with high precision  however  the decoded distribution consists of connected clusters since the latent variable is topologically equal with the ball. This means that the topology of the given dataset is not represented by the projection of the trained networks.  In this short text  we study the consequence of autoencoders' training due to  the topological mismatch. We use the SAE[4] as autoencoders  which is enhanced based on the WAE[3] owing to the sinkhorn algorithm.   """;Computer Vision;https://github.com/allnightlight/ConditionalWassersteinAutoencoderPoweredBySinkhornDistance
"""Data scientists often choose the uniform distribution or the normal distribution as the latent variable distribution when they build representative models of datasets. For example  the studies of the GANs [1] and the VAEs[2] used the uniform random distribution and the normal one  respectively.  As the approximate function implemented by neural networks is usually continuous  the topological structure of the latent variable distribution is preserved after the transformation from the latent variables space to the observable variables space. Given that the observed variables are distributed on a torus and that networks  for example the GANs  are trained with the latent variables sampled from the normal distribution  the structure of the projected distribution by the trained networks does not meet with the torus  even if residual error is small enough. Imagine another example where the observable variables follow a mixture distribution  of which clusters separate each other  trained variational autoencoder can encode the feature on the latent variable space with high precision  however  the decoded distribution consists of connected clusters since the latent variable is topologically equal with the ball. This means that the topology of the given dataset is not represented by the projection of the trained networks.  In this short text  we study the consequence of autoencoders' training due to  the topological mismatch. We use the SAE[4] as autoencoders  which is enhanced based on the WAE[3] owing to the sinkhorn algorithm.   """;General;https://github.com/allnightlight/ConditionalWassersteinAutoencoderPoweredBySinkhornDistance
""" The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.    """;Computer Vision;https://github.com/GodofCCode/yolo3
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/molyswu/knowledge-graph-embeding
"""This is an official pytorch implementation of [*Deep High-Resolution Representation Learning for Human Pose Estimation*](https://arxiv.org/abs/1902.09212).  In this work  we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods **recover high-resolution representations from low-resolution representations** produced by a high-to-low resolution network. Instead  our proposed network **maintains high-resolution representations** through the whole process. We start from a high-resolution subnetwork as the first stage  gradually add high-to-low resolution subnetworks one by one to form more stages  and connect the mutli-resolution subnetworks **in parallel**. We conduct **repeated multi-scale fusions** such that each of the high-to-low resolution representations receives information from other parallel representations over and over  leading to rich high-resolution representations. As a result  the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. </br>  ![Illustrating the architecture of the proposed HRNet](/figures/hrnet.png)  """;Computer Vision;https://github.com/anshky/HR-NET
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/darkAlert/yolov3-rt
"""1. Train **[DCGAN](https://arxiv.org/pdf/1703.05921.pdf)** with solely on image data of **healthy cases** with the aim to model the variety of healthy appearance. 2. Map new images to the *latent space* and find the most similar image *G*(**z**) via applying backpropagation iteratively. 3. Compute Anomaly score *A*(**x**) which is a weighted sum of *residual loss* and *discrimination loss*.   """;Computer Vision;https://github.com/seungjunlee96/AnoGAN-pytorch
"""- [X] InsightFace inference example (production ready architecture)  - [X] Face recognition demo with insightface (visualization missing  add later)  - [ ] InsightFace training pipeline   """;General;https://github.com/bingxinhu/arcface
"""Deep Convolutional GAN is one of the most coolest and popular deep learning technique. It is a great improvement upon the [original GAN network](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) that was first introduced by Ian Goodfellow at NIPS 2014. (DCGANs are much more stable than Vanilla GANs) DCGAN uses the same framework of generator and discriminator. This is analogous to solving a two player minimax game: Ideally the goal of the discriminator is to be very sharp in distinguishing between the real and fake data  whereas  generator aims at faking data in such a way that it becomes nearly impossible for the discriminator to classify it as a fake. The below gif shows how quickly dcgan learns the distribution of celebrity images and generates real looking people. The gif is created for both  a fixed noise and variable noise:-  <p float=""left"">   <img src=""https://github.com/AKASHKADEL/dcgan-celeba/blob/master/results/variable_noise/animated.gif"" width=""400"" height=""400"" />   <img src=""https://github.com/AKASHKADEL/dcgan-celeba/blob/master/results/fixed_noise/animated.gif"" width=""400"" height=""400"" /> </p>   """;Computer Vision;https://github.com/AKASHKADEL/dcgan-celeba
"""Machine translation is a natural language processing task that aims to translate natural languages using computers automatically. Recent several years have witnessed the rapid development of end-to-end neural machine translation  which has become the new mainstream method in practical MT systems.  THUMT is an open-source toolkit for neural machine translation developed by [the Natural Language Processing Group at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/site2/index.php?lang=en). The website of THUMT is: [http://thumt.thunlp.org/](http://thumt.thunlp.org/).   """;General;https://github.com/wangmz15/Chinese-Error-Correction-with-THUMT
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/rap-music/-
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/Eltomad/insightface
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/hukangli/https-github.com-deepinsight-insightface
"""Intention Classifier is a module that analyzes the intention of the user’s utterance. This module modifies and combines “bi-RNN” and “Attention mechanism” to implement an Intention classification model.   - 2.1 Maintainer status: maintained - 2.2 Maintainer: Yuri Kim  [yurikim@hanyang.ac.kr]() - 2.3 Author: Yuri Kim  [yurikim@hanyang.ac.kr]() - 2.4 License (optional):  - 2.5 Source git: https://github.com/DeepTaskHY/DM_Intent   """;Natural Language Processing;https://github.com/DeepTaskHY/DM_Intent
"""MuarAugment is the easiest way to a state-of-the-art data augmentation pipeline. Neural nets learn most from data they struggle with. MuarAugment uses the model to select the data augmentations that it has most trouble with  and uses only those most difficult data for training. This has been shown empirically. Perhaps more difficult data reduces overfitting. Perhaps it exposes parts of the data distribution which the model did not see yet. Either way  MuarAugment works.  It adapts the leading pipeline search algorithms  RandAugment<sup>[1]</sup> and the model uncertainty-based augmentation scheme<sup>[2]</sup> (called MuAugment here)  and modifies them to work batch-wise  on the GPU. Kornia<sup>[3]</sup> and albumentations are used for batch-wise and item-wise transforms respectively.  If you are seeking SOTA data augmentation pipelines without laborious trial-and-error  MuarAugment is the package for you.   """;Computer Vision;https://github.com/adam-mehdi/MuarAugment
"""Object Cut is an online image background removal service that uses [BASNet](https://github.com/NathanUA/BASNet). Removing the background from an image is a common operation in the daily work of professional photographers and image editors. This process is usually a repeatable and manual task that requires a lot of effort and human time. However  thanks to [BASNet](https://github.com/NathanUA/BASNet)  one of the most robust and fastest performance deep learning models in image segmentation  Object Cut was able to turn it into an easy and automatic process.   It was built as an API to make it as easy as possible to integrate. APIs  also known as Application Programming Interfaces  are already a common way to integrate different types of solutions to improve systems without actually knowing what is happening inside. Specifically  RESTful APIs are a standard in the Software Engineering field for designing and specifying APIs. Making it substantially easier to adapt your desired APIs to your workflows.  <br> <p align=""center"">   <img alt=""Pipeline"" src=""docs/images/pipeline.png"" width=""100%""/> </p> <br>  Object Cut was born to power up the designing and image editing process from the people who work with images daily. Integrating the Object Cut API removes the necessity of understanding the complex inner workings behind it and automates the process of removing the background from images in a matter of seconds.   """;Computer Vision;https://github.com/AlbertSuarez/object-cut
"""In this work we have shown how to augment an images training using rotations. Furthermore  we have presented the convolutional neural network SegNet which yields a fairly good prediction for road segmentation on satellite images. However  one must pay attention to overfitting very carefully.    The goal of this work is to segment roads on satellite images (Figure 1) by using machine learning techniques. In other words  we want to assign a label (road or background) to each pixel of the image. Before selecting the best algorithm  an effort is made on how to augment a small image data set and how to get the most relevant features out of it. Then we present 2 different classes of algorithm. The first one is a linear logistic regression whereas the second one  called SegNet [1] uses a more complicated scheme based on a convolutional neural network (CNN).  ![Fig1. Exampel of satellite image ](projectRoadSegmentation/report/pics/satImage.png)  Fig1. Exampel of satellite image   A set of N = 100 training images of size 400 × 400 pixels is provided. The set contains different aerial pictures of urban areas. Together with this training set  the Figure 2. Ground truth of satellite image example. corresponding ground truth grayscale images (Figure 2) are also available. Note that the ground truth images need to be converted into label images. Concretely  each pixel y i can only take one of the two possible values corresponding to the classes: road label (y i = 1) or background label (y i = 0). In order to binarize the ground truth images  a threshold of 25% is set. This means that every pixel with an intensity lower than 75% of the maximum possible value is set to 0 and the rest is set to 1. With 8 bits images  the maximum value is 255 which sets the threshold to 191. This pixel threshold has a direct impact on the width of the road label in the computed label image.    ![Fig2. Ground truth of satellite image example ](projectRoadSegmentation/report/pics/satImage_gt.png)    Fig2. Ground truth of satellite image example     The problem that arises with such a small training set (100 images only) is overfitting. Moreover in order to train any convolutional neural network properly it is necessary to augment the dataset. Analysing the training set  it is obvious that it contains mainly pictures with strictly vertical and horizontal roads. For that reason  creating new images by rotating the original ones allows to increase the size of the dataset and generates data which will be useful to better train the algorithm. Specifically  we rotate each image by angles going from 5 to 355 degrees every 5 degrees (i.e. 5  10  15 ...  355). That way we generate a set of images with roads in every directions. In summary  for each image of the original training set  70 images are generated using therotations  resulting in a new training set of 7100 images. This augmented training set is then suitable for the training of the CNN.   """;Computer Vision;https://github.com/gaelmoccand/RoadSegmentation_CNN
"""This repo contain pytorch implementation of Compact Convolution Transformers as explained in the [Escaping the Big Data Paradigm with Compact Transformers](https://arxiv.org/abs/2104.05704) paper  for official implementation of this paper visit [here](https://github.com/SHI-Labs/Compact-Transformers) ![](model.PNG)  """;Computer Vision;https://github.com/rishikksh20/compact-convolution-transformer
"""This repo contains a sample code to show how to create a Flask API server by deploying our remote sensing imagery cloud removal PyTorch model that is trained on RICE dataset.  """;Computer Vision;https://github.com/XavierJiezou/cloud-removal-deploy
"""This repo contains a sample code to show how to create a Flask API server by deploying our remote sensing imagery cloud removal PyTorch model that is trained on RICE dataset.  """;General;https://github.com/XavierJiezou/cloud-removal-deploy
"""**TL; DR.** Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism.    ![deformable_detr](./figs/illustration.png)  ![deformable_detr](./figs/convergence.png)  **Abstract.** DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However  it suffers from slow convergence and limited feature spatial resolution  due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues  we proposed Deformable DETR  whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10× less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach.   """;Computer Vision;https://github.com/duongnv0499/Explain-Deformable-DETR
"""**TL; DR.** Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism.    ![deformable_detr](./figs/illustration.png)  ![deformable_detr](./figs/convergence.png)  **Abstract.** DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However  it suffers from slow convergence and limited feature spatial resolution  due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues  we proposed Deformable DETR  whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10× less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach.   """;General;https://github.com/duongnv0499/Explain-Deformable-DETR
"""•	UNet++ aims to improve segmentation accuracy  with a series of nested  dense skip pathways.  •	Redesigned skip pathways make optimization easier by getting the semantically similar feature maps.  •	Dense skip connections improve segmentation accuracy and make the gradient flow smoother.  •	Deep supervision allows for model complexity tuning to balance between speed and performance optimization by allowing the model to toggle between 2 different training modes in the fast mode and the accurate mode.  •	UNet++ differs from the original U-Net in three ways - (refer architecture diagram above)    - It has convolution layers (green)on skip pathways  which bridges the semantic gap between encoder and decoder feature maps.        - It has dense skip connections on skip pathways (blue)  which improves gradient flow.        - It employs deep supervision (red)  which enables model pruning and improves or in the worst case achieves comparable performance to using only one loss layer.   UNet++  a convolutional neural network dedicated for biomedical image segmentation  was designed  and applied in 2018 by (Zhou et al.  2018). UNet++ was basically designed to overcome some of the short comings of the UNet architecture. UNet works on the idea of skip connections. U-Net concatenates them and add convolutions and non-linearities between each up-sampling block. The skip connections recover the full spatial resolution at the network output  making fully convolutional methods suitable for semantic segmentation. UNet and other segmentation models based on the encoder-decoder architecture tend to fuse semantically dissimilar feature maps from the encoder and decoder sub-networks  which may degrade segmentation performance. This is where UNet++ is shown to have an edge over the other players as it bridges the semantic gap between the feature maps of the encoder and decoder prior to fusion thus improving the segmentation performance and output.   """;Computer Vision;https://github.com/sauravmishra1710/UNet-Plus-Plus---Brain-Tumor-Segmentation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/brightmart/bert_original
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/brightmart/bert_original
"""This model was trained from scratch with 5k images and scored a [Dice coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) of 0.988423 on over 100k test images.  It can be easily used for multiclass segmentation  portrait segmentation  medical segmentation  ...    """;Computer Vision;https://github.com/milesial/Pytorch-UNet
"""We present Sandwich Batch Normalization (SaBN)  an extremely easy improvement of Batch Normalization (BN) with only a few lines of code changes.  ![method](imgs/architect.png)  We demonstrate the prevailing effectiveness of SaBN as a drop-in replacement in four tasks: 1. **conditional image generation**  2. **neural architecture search**  3. **adversarial training**  4. **arbitrary neural style transfer**.   """;General;https://github.com/VITA-Group/Sandwich-Batch-Normalization
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/tcnguyen/bert
"""The main purpose is to study how well the word order information learned by different neural networks. Specifically  we randomly move one word to another position  and examine whether a trained model can detect both the original and inserted positions. Our codes were built upon [THUMT-MT](https://github.com/THUNLP-MT/THUMT). We compare self-attention networks (SAN  [Vaswani et al.  2017](https://arxiv.org/pdf/1706.03762.pdf)) with re-implemented RNN ([Chen et al.  2018](https://www.aclweb.org/anthology/P18-1008))  as well as directional SAN (DiSAN [Shen et al.  2018](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16126/16099)) that augments SAN with recurrence modeling.   """;General;https://github.com/baosongyang/WRD
"""The main purpose is to study how well the word order information learned by different neural networks. Specifically  we randomly move one word to another position  and examine whether a trained model can detect both the original and inserted positions. Our codes were built upon [THUMT-MT](https://github.com/THUNLP-MT/THUMT). We compare self-attention networks (SAN  [Vaswani et al.  2017](https://arxiv.org/pdf/1706.03762.pdf)) with re-implemented RNN ([Chen et al.  2018](https://www.aclweb.org/anthology/P18-1008))  as well as directional SAN (DiSAN [Shen et al.  2018](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16126/16099)) that augments SAN with recurrence modeling.   """;Natural Language Processing;https://github.com/baosongyang/WRD
"""This project is based on our CVPR2019 paper. You can find the [arXiv](https://arxiv.org/abs/1811.07246) version here.  ``` @inproceedings{wu2019pointconv    title={Pointconv: Deep convolutional networks on 3d point clouds}    author={Wu  Wenxuan and Qi  Zhongang and Fuxin  Li}    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}    pages={9621--9630}    year={2019} } ```  Unlike images which are represented in regular dense grids  3D point clouds are irregular and unordered  hence applying convolution on them can be difficult. In this paper  we extend the dynamic filter to a new convolution operation  named PointConv. PointConv can be applied on point clouds to build deep convolutional networks. We treat convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight and density functions. With respect to a given point  the weight functions are learned with multi-layer perceptron networks and the density functions through kernel density estimation. A novel reformulation is proposed for efficiently computing the weight functions  which allowed us to dramatically scale up the network and significantly improve its performance. The learned convolution kernel can be used to compute translation-invariant and permutation-invariant convolution on any point set in the 3D space. Besides  PointConv can also be used as deconvolution operators to propagate features from a subsampled point cloud back to its original resolution. Experiments on ModelNet40  ShapeNet  and ScanNet show that deep convolutional neural networks built on PointConv are able to achieve state-of-the-art on challenging semantic segmentation benchmarks on 3D point clouds. Besides  our experiments converting CIFAR-10 into a point cloud showed that networks built on PointConv can match the performance of convolutional networks in 2D images of a similar structure.   """;Computer Vision;https://github.com/DylanWusee/pointconv_pytorch
"""**QA_PreProcess.py\QA_PreProcess.ipynb:** Converts the raw induction tasks data set to separate ndarrays containing questions  answers  and facts with all words being in the form of GloVe pre-trained vector representations.    **DMN+.py\DMN+.ipynb:** The DMN+ model  along with training  validation and testing.    """;General;https://github.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M and VGG2 datasets  which were already packed in the MxNet binary format. The network backbones include ResNet  InceptionResNet_v2  DenseNet  DPN and MobileNet. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss. * loss-type=0:  Softmax * loss-type=1:  SphereFace * loss-type=2:  CosineFace * loss-type=4:  ArcFace * loss-type=5:  Combined Margin * loss-type=12: TripletLoss  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/394781865/insightface
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   """;Computer Vision;https://github.com/xjnpark/ds
"""Machine translation is a natural language processing task that aims to translate natural languages using computers automatically. Recent several years have witnessed the rapid development of end-to-end neural machine translation  which has become the new mainstream method in practical MT systems.  THUMT is an open-source toolkit for neural machine translation developed by [the Natural Language Processing Group at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/site2/index.php?lang=en). The website of THUMT is: [http://thumt.thunlp.org/](http://thumt.thunlp.org/).   """;Natural Language Processing;https://github.com/wangmz15/Chinese-Error-Correction-with-THUMT
"""Mixup is a generic and straightforward data augmentation principle. In essence  mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so  mixup regularizes the neural network to favor simple linear behavior in-between training examples.  This repository contains the implementation used for the results in our paper (https://arxiv.org/abs/1710.09412).   """;Computer Vision;https://github.com/Ryoo72/dimension-wise_mixup
"""BART model [https://arxiv.org/pdf/1910.13461.pdf](https://arxiv.org/pdf/1910.13461.pdf)  Fairseq [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)  Fairseq tutorial on fine-tuning BART on Seq2Seq task [https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md](https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md)  Emerald dataset used for faceted summarization - https://github.com/hfthair/emerald_crawler    """;Sequential;https://github.com/khushsi/Finetuning_BART_for_FACET_Summarization
"""DeBERTa (Decoding-enhanced BERT with disentangled attention) improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism  where each word is represented using two vectors that encode its content and position  respectively  and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second  an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks.   """;General;https://github.com/huberemanuel/DeBERTa
"""DeBERTa (Decoding-enhanced BERT with disentangled attention) improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism  where each word is represented using two vectors that encode its content and position  respectively  and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second  an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks.   """;Natural Language Processing;https://github.com/huberemanuel/DeBERTa
"""FNet (https://arxiv.org/abs/2105.03824) is an attention-free model that is faster to train  and uses less memory. It does so by replaces self-attention with unparameterized Fourier Transform  but custom implementation is required to do causal (unidirectional) mask. In this project  we instead explore a hybrid approach. We use a encoder-decoder architecture  where the encoder is an attention-free  using Fourier Transform  and the decoder uses cross-attention and decoder self-attention.   """;Natural Language Processing;https://github.com/cccntu/ft5-demo-space
"""- This repository does not contain a [gail](https://arxiv.org/abs/1606.03476) / [infogail](https://arxiv.org/abs/1703.08840) / hgail implementation   - The reason ngsim_env does not contain the GAIL algorithm implementation is to enable the codebase to be more modular. This design decision enables ngsim_env to be used as an environment in which any imitation learning algorithm can be tested. Similarly  this design decision enables the GAIL algorithm to be a separate module that can be tested in any environment be that ngsim_env or otherwise. The installation process below gets the GAIL implementation from [sisl/hgail](https://github.com/sisl/hgail) - It also does not contain the human driver data you need for the environment to work. The installation process below gets the data from [sisl/NGSIM.jl](https://github.com/sisl/NGSIM.jl). - Figure below shows a diagram of the repositories ngsim_env depends on: ![dependecies](docs/ngsim_env:RepositoryDependencies.png)   """;General;https://github.com/sisl/ngsim_env
"""This Python project is a  from scratch  implementation of a Yolo object detection neural network model in Tensorflow. This implementation consists out of a functioning Yolo model  trainable using the Tensorflow ADAM optimizer on data like the Microsoft COCO dataset.    """;Computer Vision;https://github.com/RobbertBrand/Yolo-Tensorflow-Implementation
"""This project is an pytorch implement R-FCN and CoupleNet  large part code is reference from [jwyang/faster-rcnn.pytorch](https://github.com/jwyang/faster-rcnn.pytorch). The R-FCN structure is refer to [Caffe R-FCN](https://github.com/daijifeng001/R-FCN) and [Py-R-FCN](https://github.com/YuwenXiong/py-R-FCN)  - For R-FCN  mAP@0.5 reached 73.2 in VOC2007 trainval dataset - For CoupleNet  mAP@0.5 reached 75.2 in VOC2007 trainval dataset   """;Computer Vision;https://github.com/princewang1994/RFCN_CoupleNet.pytorch
"""Here is our pytorch implementation of the model described in the paper **EfficientDet: Scalable and Efficient Object Detection** [paper](https://arxiv.org/abs/1911.09070) (*Note*: We also provide pre-trained weights  which you could see at ./trained_models)  <p align=""center"">   <img src=""demo/video.gif""><br/>   <i>An example of our model's output.</i> </p>    """;Computer Vision;https://github.com/signatrix/efficientdet
"""Visualizing chromosomes is important for  many  for  medical  diagnostic  problems   but  chromosomes  often  overlap  and  it  is necessary to identify and distinguish between the overlapping chromosomes.  This fast and fully-automated segmentation solution allows to scale certain experiments to very large number of chromosomes.    """;Computer Vision;https://github.com/ArkaJU/SegNet---Chromosome
"""| Framework    | N |  """;General;https://github.com/koshian2/ResNet-MultipleFramework
"""| Framework    | N |  """;Computer Vision;https://github.com/koshian2/ResNet-MultipleFramework
"""The *Show and Tell* model is a deep neural network that learns how to describe the content of images. For example:  ![Example captions](g3doc/example_captions.jpg)   """;Computer Vision;https://github.com/brandontrabucco/im2txt_express
"""The demos in this folder are designed to give straightforward samples of using TensorFlow in mobile applications.  Inference is done using the [TensorFlow Android Inference Interface](../../../tensorflow/contrib/android)  which may be built separately if you want a standalone library to drop into your existing application. Object tracking and YUV -> RGB conversion is handled by libtensorflow_demo.so.  A device running Android 5.0 (API 21) or higher is required to run the demo due to the use of the camera2 API  although the native libraries themselves can run on API >= 14 devices.   """;General;https://github.com/leoliao2008/TensorflowMobileOfficialExample
"""The demos in this folder are designed to give straightforward samples of using TensorFlow in mobile applications.  Inference is done using the [TensorFlow Android Inference Interface](../../../tensorflow/contrib/android)  which may be built separately if you want a standalone library to drop into your existing application. Object tracking and YUV -> RGB conversion is handled by libtensorflow_demo.so.  A device running Android 5.0 (API 21) or higher is required to run the demo due to the use of the camera2 API  although the native libraries themselves can run on API >= 14 devices.   """;Computer Vision;https://github.com/leoliao2008/TensorflowMobileOfficialExample
"""That concludes this brief introduction into the wonderful world of Reinforcement Learning. Hopefully this provides you with a good foundation of understanding of the topic and can help springboard you into starting RL projects of your own. <br><br> Addtionally  it is important to note that RL is extremely expensive computationally. To train the Atari games  for example  even with a high end GPU  it could take multiple days of constant training. That being said  more simple environments can be trained is as little as 30 minutes to 1 hour.  <br> <br> Also important to note is that RL models are highly unstable and unpredictable. Even using the same seed for random number generation  the results of training may vary wildly from one training session to another. While it may have only taken 500 games to train the first time  the second time it might take 1000 games to reach the same performance. <br><br> There are also many hyperparameters to adjust  from the epsilon values to the layers of the neural network to the size of the batch in experience replay. Finding the right balance of these variables can be time-consuming  adding to the time sink that RL has the possibility to become. <br><br> That being said  it is extremely fulfilling when you see that your AI has actually started to learn and you see scores start climbing. <br><br> Beware  you may just find yourself become emotional involved in the success of your model.   You may have heard that the two types of learning are supervised and unsupervised - either you are training a model to correctly assign labels or training a model to group similar items together. There is  however  a third breed of learning: reinforcement learning. Reinforcement learning seeks to train a model make a sequence of decisions.   """;Reinforcement Learning;https://github.com/matthewsparr/Reinforcement-Learning-Lesson
"""Neuron is a swift package I developed to help learn how to make neural networks. It is far from perfect and I am still learning. There is A LOT to learn here and I've just scratched the surface. As of right now this package provides a way to get started in machine learning and neural networks.  <img width=""500"" src=""images/network.png"">   """;General;https://github.com/wvabrinskas/Neuron
"""The aim of this project is to distinguish gliomas which are the most difficult brain tumors to be detected with deep learning algorithms. Because  for a skilled radiologist  analysis of multimodal MRI scans can take up to 20 minutes and therefore  making this process automatic is obviously useful.   MRI can show different tissue contrasts through different pulse sequences  making it an adaptable and widely used imaging technique for visualizing regions of interest in the human brain.  Gliomas are the most commonly found tumors having irregular shape and ambiguous boundaries  making them one of the hardest tumors to detect. Detection of brain tumor using a segmentation approach is critical in cases  where survival of a subject depends on an accurate and timely clinical diagnosis.    We present a fully automatic deep learning approach for brain tumor segmentation in multi-contrast magnetic resonance image.    """;Computer Vision;https://github.com/mrvturan96/Brain-Tumor-Detection-and-Segmentation-using-Deep-Learning
"""We propose a deep learning based model and well-organized dataset for context aware paper citation recommendation.  Our model comprises a document encoder and a context encoder  which uses Graph Convolutional Networks ([GCN](https://arxiv.org/abs/1611.07308))  layer and Bidirectional Encoder Representations from Transformers ([BERT](https://arxiv.org/abs/1810.04805))  which is a pretrained model of textual data.  By modifying the related PeerRead AAN dataset  we propose a new dataset called FullTextPeerRead  FullTextANN containing  context sentences to cited references and paper metadata.   ![](https://i.imgur.com/FzmbImx.png)  The code is based on that([BERT](https://github.com/google-research/bert)  [GCN](https://github.com/tkipf/gae/)).   """;Graphs;https://github.com/TeamLab/bert-gcn-for-paper-citation
"""We propose a deep learning based model and well-organized dataset for context aware paper citation recommendation.  Our model comprises a document encoder and a context encoder  which uses Graph Convolutional Networks ([GCN](https://arxiv.org/abs/1611.07308))  layer and Bidirectional Encoder Representations from Transformers ([BERT](https://arxiv.org/abs/1810.04805))  which is a pretrained model of textual data.  By modifying the related PeerRead AAN dataset  we propose a new dataset called FullTextPeerRead  FullTextANN containing  context sentences to cited references and paper metadata.   ![](https://i.imgur.com/FzmbImx.png)  The code is based on that([BERT](https://github.com/google-research/bert)  [GCN](https://github.com/tkipf/gae/)).   """;Natural Language Processing;https://github.com/TeamLab/bert-gcn-for-paper-citation
"""SWA is a simple DNN training method that can be used as a drop-in replacement for SGD with improved generalization  faster convergence  and essentially no overhead. The key idea of SWA is to average multiple samples produced by SGD with a modified learning rate schedule. We use a constant or cyclical learning rate schedule that causes SGD to _explore_ the set of points in the weight space corresponding to high-performing networks. We observe that SWA converges more quickly than SGD  and to wider optima that provide higher test accuracy.   In this repo we implement the constant learning rate schedule that we found to be most practical on CIFAR datasets.  <p align=""center"">   <img src=""https://user-images.githubusercontent.com/14368801/37633888-89fdc05a-2bca-11e8-88aa-dd3661a44c3f.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633885-89d809a0-2bca-11e8-8d57-3bd78734cea3.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633887-89e93784-2bca-11e8-9d71-a385ea72ff7c.png"" width=250> </p>  Please cite our work if you find this approach useful in your research: ```latex @article{izmailov2018averaging    title={Averaging Weights Leads to Wider Optima and Better Generalization}    author={Izmailov  Pavel and Podoprikhin  Dmitrii and Garipov  Timur and Vetrov  Dmitry and Wilson  Andrew Gordon}    journal={arXiv preprint arXiv:1803.05407}    year={2018} } ```    """;General;https://github.com/izmailovpavel/contrib_swa_examples
"""This is the repository for my trained Deep Deterministic Policy Gradient based agent on the Unity Reacher Enviroment from the Deep Reinforcement Learning nanodegree program. To 'solve' the environment the agent must navigate the Envirnoment with an average score of greater than 30 over the last 100 episodes. This repository provides the code to achieve this in 110 episodes.      """;Reinforcement Learning;https://github.com/Remtasya/DDPG-Actor-Critic-Reinforcement-Learning-Reacher-Environment
"""This is the repository for my trained Deep Deterministic Policy Gradient based agent on the Unity Reacher Enviroment from the Deep Reinforcement Learning nanodegree program. To 'solve' the environment the agent must navigate the Envirnoment with an average score of greater than 30 over the last 100 episodes. This repository provides the code to achieve this in 110 episodes.      """;General;https://github.com/Remtasya/DDPG-Actor-Critic-Reinforcement-Learning-Reacher-Environment
"""The dataset creation process has several stages outlined below. We describe the process here at a high level. If you have questions about any individual steps  please contact Rebecca Roelofs (roelofs@cs.berkeley.edu) and Ludwig Schmidt (ludwig@berkeley.edu).    """;Computer Vision;https://github.com/modestyachts/ImageNetV2
"""En distribuant le travail en différentes partitions et sur différents noeuds  avec ce qu'on appelle le *Resilient Distributed Dataset (RDD)*  Spark est jusqu'à 30 fois plus rapide que Hadoop MapReduce pour exécuter un tri par exemple.  Spark fonctionne en 4 grandes étapes : - on crée un RDD à partir de notre jeu de données  - on applique différentes transformations pour en créer de nouveaux ; résultants de fonctions dites 'immutables' telles que `.map` ou `.filter`  - on décide quels RDDs garder en mémoire avec les fonctions `.persist` ou `.unpersist`  - et on peut ensuite appliquer des fonctions plus classiques à nos RDDs comme `.count` ou `.collect` qui modifie le RDD directement  sans en créer un nouveau.  Essayons de reproduire l'algorithme de MapReduce pour compter les mots.  ```python from pyspark import SparkContext  sc = pyspark.SparkContext() file = sc.textfile(""data/count.txt"")              #:split words on each line count = file.flatMap(lambda line: line.split("" ""))             #:add 1 for each occurence of a word             .map(lambda word: (word  1))             #:aggregate the number of occurences of each word             .reduceByKey(lambda a  b: a + b)              count.persist() count.saveAsTextFile(""data/count.txt"") ```    """;General;https://github.com/qmonmous/BigData-X-Python
"""This project contains the code (Note: The code is test in the environment with python=3.6  cuda=9.0  PyTorch-0.4.1  also support Pytorch-0.4.1+) for:  [**LEDNet: A Lightweight Encoder-Decoder Network for Real-time Semantic Segmentation**](https://arxiv.org/pdf/1905.02423.pdf)  by [Yu Wang](https://github.com/xiaoyufenfei).  <p align=""center""><img width=""100%"" src=""./images/LEDNet_overview.png"" /></p> The extensive computational burden limits the usage of CNNs in mobile devices for dense estimation tasks  a.k.a semantic segmentation. In this paper  we present a lightweight network to address this problem  namely **LEDNet**  which employs an asymmetric encoder-decoder architecture for the task of real-time semantic segmentation.More specifically  the encoder adopts a ResNet as backbone network  where two new operations  channel split and shuffle  are utilized in each residual block to greatly reduce computation cost while maintaining higher segmentation accuracy. On the other hand  an attention pyramid network (APN) is employed in the decoder to further lighten the entire network complexity. Our model has less than 1M parameters  and is able to run at over 71 FPS on a single GTX 1080Ti GPU card. The comprehensive experiments demonstrate that our approach achieves state-of-the-art results in terms of speed and accuracy trade-off on Cityscapes dataset. and becomes an effective method for real-time semantic segmentation tasks.   """;Computer Vision;https://github.com/xiaoyufenfei/LEDNet
"""This project contains the code (Note: The code is test in the environment with python=3.6  cuda=9.0  PyTorch-0.4.1  also support Pytorch-0.4.1+) for:  [**LEDNet: A Lightweight Encoder-Decoder Network for Real-time Semantic Segmentation**](https://arxiv.org/pdf/1905.02423.pdf)  by [Yu Wang](https://github.com/xiaoyufenfei).  <p align=""center""><img width=""100%"" src=""./images/LEDNet_overview.png"" /></p> The extensive computational burden limits the usage of CNNs in mobile devices for dense estimation tasks  a.k.a semantic segmentation. In this paper  we present a lightweight network to address this problem  namely **LEDNet**  which employs an asymmetric encoder-decoder architecture for the task of real-time semantic segmentation.More specifically  the encoder adopts a ResNet as backbone network  where two new operations  channel split and shuffle  are utilized in each residual block to greatly reduce computation cost while maintaining higher segmentation accuracy. On the other hand  an attention pyramid network (APN) is employed in the decoder to further lighten the entire network complexity. Our model has less than 1M parameters  and is able to run at over 71 FPS on a single GTX 1080Ti GPU card. The comprehensive experiments demonstrate that our approach achieves state-of-the-art results in terms of speed and accuracy trade-off on Cityscapes dataset. and becomes an effective method for real-time semantic segmentation tasks.   """;General;https://github.com/xiaoyufenfei/LEDNet
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/yeqiuyi/caffe-ssd
"""In this work  we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN  including only the most essential component in GCN—neighborhood aggregation—for collaborative filtering.   """;Graphs;https://github.com/kuandeng/LightGCN
"""|<img src=""images/2_gt.jpg"" width=""500px""> | <img src=""images/2_prop.jpg"" width=""500px""> | <img src=""images/2_det.jpg"" width=""500px""> |- | - | -|  RRPN is a Region Proposal Network (RPN) exploiting Radar detections to propose Regions of Interest (RoI) for object detection in autonomous vehicles. RRPN provides real-time RoIs for any two-stage object detection network while achieving precision and recall values higher than or on par with vision based RPNs. We evaluate RRPN using the [Fast R-CNN](https://arxiv.org/abs/1504.08083) network on the [NuScenes](https://www.nuscenes.org/) dataset and compare the results with the [Selective Search](https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf) algorithm.   > This project has been updated to work with the full nuScenes dataset (v1.0). > The results reported in the paper are based on the Teaser version of the  > nuScenes dataset (v0.1)  which is now deprecated.   """;Computer Vision;https://github.com/mrnabati/RRPN
"""This repository provides code for the analysis of Clinical Reading Comprehension task in the ACL2020 paper: [Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset](https://arxiv.org/abs/2005.00574)  ```bib @inproceedings{yue2020CliniRC   title={Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset}   author={Xiang Yue and Bernal Jimenez Gutierrez and Huan Sun}   booktitle={ACL}   year={2020} } ```   """;Natural Language Processing;https://github.com/xiangyue9607/CliniRC
""" DeepLab is a state-of-art deep learning system for semantic image segmentation built on top of [Caffe](http://caffe.berkeleyvision.org).  It combines densely-computed deep convolutional neural network (CNN) responses with densely connected conditional random fields (CRF).  This distribution provides a publicly available implementation for the key model ingredients first reported in an [arXiv paper](http://arxiv.org/abs/1412.7062)  accepted in revised form as conference publication to the ICLR-2015 conference.  It also contains implementations for methods supporting model learning using only weakly labeled examples  described in a second follow-up [arXiv paper](http://arxiv.org/abs/1502.02734). Please consult and consider citing the following papers:      @inproceedings{chen14semantic        title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        booktitle={ICLR}        url={http://arxiv.org/abs/1412.7062}        year={2015}     }      @article{papandreou15weak        title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation}        author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille}        journal={arxiv:1502.02734}        year={2015}     }  Note that if you use the densecrf implementation  please consult and cite the following paper:      @inproceedings{KrahenbuhlK11        title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}        author={Philipp Kr{\""{a}}henb{\""{u}}hl and Vladlen Koltun}        booktitle={NIPS}              year={2011}     }   """;Computer Vision;https://github.com/open-cv/deeplab-v1
""" GhostNet: More Features from Cheap Operations. CVPR 2020. [[arXiv]](https://arxiv.org/abs/1911.11907)  By Kai Han  Yunhe Wang  Qi Tian  Jianyuan Guo  Chunjing Xu  Chang Xu.  - **Approach**  <div align=""center"">    <img src=""./fig/ghost_module.png"" width=""720""> </div>  - **Performance**  GhostNet beats other SOTA lightweight CNNs such as **MobileNetV3** and **FBNet**.  <div align=""center"">    <img src=""./fig/flops_latency.png"" width=""720""> </div>    """;Computer Vision;https://github.com/iamhankai/ghostnet.pytorch
"""Named entity recognition (NER) is a central component in natural language processing tasks. Identifying named entities is a key part in systems e.g. for question answering or entity linking. Traditionally  NER systems are built using conditional random fields (CRFs). Recent systems are using neural network architectures like bidirectional LSTM with a CRF-layer ontop and pre-trained word embeddings ([Ma and Hovy  2016](http://aclweb.org/anthology/P16-1101); [Lample et al.  2016a](http://aclweb.org/anthology/N16-1030); [Reimers and Gurevych  2017](http://aclweb.org/anthology/D17-1035); [Lin et al.  2017](http://aclweb.org/anthology/W17-4421)).  Pre-trained word embeddings have been shown to be of great use for downstream NLP tasks ([Mikolov et al.  2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf); [Pennington et al.  2014](https://www.aclweb.org/anthology/D14-1162)). Many recently proposed approaches go beyond these pre-trained embeddings. Recent works have proposed methods that produce different representations for the same word depending on its contextual usage ([Peters et al.  2017](http://aclweb.org/anthology/P17-1161) [2018a](https://aclweb.org/anthology/N18-1202); [Akbik et al.  2018](https://www.aclweb.org/anthology/C18-1139); [Devlin et al.  2018](https://arxiv.org/abs/1810.04805)). These methods have shown to be very powerful in the fields of named entity recognition  coreference resolution  part-of-speech tagging and question answering  especially in combination with classic word embeddings.  Our paper is based on the work of [Riedl and Padó (2018)](http://aclweb.org/anthology/P18-2020). They showed how to build a model for German named entity recognition (NER) that performs at the state of the art for both contemporary and historical texts. Labeled historical texts for German named entity recognition are a low-resource domain. In order to achieve robust state-of-the-art results for historical texts they used transfer-learning with labeled data from other high-resource domains like CoNLL-2003 ([Tjong Kim Sang and De Meulder  2003](http://aclweb.org/anthology/W03-0419)) or GermEval ([Benikova et al.  2014](http://www.lrec-conf.org/proceedings/lrec2014/pdf/276_Paper.pdf)). They showed that using Bi-LSTM with a CRF as the top layer and word embeddings outperforms CRFs with hand-coded features in a big-data situation.  We build up upon their work and use the same low-resource datasets for Historic German. Furthermore  we show how to achieve new state-of-the-art results for Historic German named entity recognition by using only unlabeled data via pre-trained language models and word embeddings. We also introduce a novel language model pre-training objective  that uses only contemporary texts for training to achieve comparable state-of-the-art results on historical texts.   """;Natural Language Processing;https://github.com/dbmdz/historic-ner
"""The objective of this project is to learn more about conditional generative models. Having worked with GANs  it seems beneficial to study more about adding additional descriptive information with the input image to produce models that are able to distinctly represent specific subjects in the generated data. It seems to be a part of how users can select specific features or labels for the model to generate. As an early step of looking at this and taking into account the limitations of resources and time  this project will be experimenting with the vanilla variational autoencoder and a conditional variational autoencoder.    """;Computer Vision;https://github.com/jenyliu/DLA_interview
"""The objective of this project is to learn more about conditional generative models. Having worked with GANs  it seems beneficial to study more about adding additional descriptive information with the input image to produce models that are able to distinctly represent specific subjects in the generated data. It seems to be a part of how users can select specific features or labels for the model to generate. As an early step of looking at this and taking into account the limitations of resources and time  this project will be experimenting with the vanilla variational autoencoder and a conditional variational autoencoder.    """;General;https://github.com/jenyliu/DLA_interview
"""ROS에서 각 노드는 다음과 같이 연결되어 있습니다. Darknet_ros (YOLO) 노드는 ZED camera로 부터 left image를 subscribe하고  객체인식 결과로 Bounding Box들의 정보를 넘겨줍니다.  PSMnet 노드는 ZED camera로 부터 left and right (stereo) image 토픽을 subscribe하고  네트워크를 통해 disparity를  추정  깊이정보를 획득합니다. 최종적으로 객체인식 및 깊이추정 네트워크에서 획득한 정보를 결합하여 인식된 객체가 카매라로부터 몇 미터 떨어져 있는지 객체별로 정보를 표시합니다.  <img align=""center"" src=""https://user-images.githubusercontent.com/25498950/69921866-12429180-14da-11ea-8759-bb23bfb9151f.png"">        """;Computer Vision;https://github.com/ChelseaGH/sidewalk_prototype_AI_Hub
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/binhetech/bert-application
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/binhetech/bert-application
"""Recent advancement in deep learning has become one of the most powerful tools to solve the image classification and segmentation problem.Deep learning model learn the filter that helps in extraction and learning of the important feature form the images. These feature helps to find differences as well as similarities amongst the image. Deep learning models require large dataset to learn the complex data representation.In the paper[1] the authors have used DCNN model to find the green cover in the cities using Cityscapes dataset. Cityscapes dataset has 2975 images and mask of green cover of different cities around the world which was used as the training data and 500 image with masks were used as testing dataset. The images were google street view images. The DCNN model has an IOU of 61.2 percent. In this approach I used state of the art unet model and mobile net v2 model. Unet gave an IOU of 74.5 percent and mobile net v2 model gave an IOU of 64.3 percent which were better and lighter model than previously used DCNN model. The model were even tested on different machine type with different configuration to check their performance.   """;General;https://github.com/anant1203/Applying-Deep-Learning-for-Large-scale-Quantification-of-Urban-Tree-Cover
"""Recent advancement in deep learning has become one of the most powerful tools to solve the image classification and segmentation problem.Deep learning model learn the filter that helps in extraction and learning of the important feature form the images. These feature helps to find differences as well as similarities amongst the image. Deep learning models require large dataset to learn the complex data representation.In the paper[1] the authors have used DCNN model to find the green cover in the cities using Cityscapes dataset. Cityscapes dataset has 2975 images and mask of green cover of different cities around the world which was used as the training data and 500 image with masks were used as testing dataset. The images were google street view images. The DCNN model has an IOU of 61.2 percent. In this approach I used state of the art unet model and mobile net v2 model. Unet gave an IOU of 74.5 percent and mobile net v2 model gave an IOU of 64.3 percent which were better and lighter model than previously used DCNN model. The model were even tested on different machine type with different configuration to check their performance.   """;Computer Vision;https://github.com/anant1203/Applying-Deep-Learning-for-Large-scale-Quantification-of-Urban-Tree-Cover
"""DeepMind published its famous paper Playing Atari with Deep Reinforcement Learning  in which a new algorithm called DQN was implemented. It showed that an AI agent could learn to play games by simply watching the screen without any prior knowledge about the game. Also  I have added a few enhancement to the vanilla DQN from various papers and tested it on the MsPacman-v4 OpenAI's environment.   <img src=""./Figure_1.png"" align=""middle"">   """;Reinforcement Learning;https://github.com/cocolico14/N-step-Dueling-DDQN-PER-Pacman
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/jack16888/caffessd
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/cuber2460/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/cuber2460/bert
"""This repo contains **experimental and unofficial** implementation of image captioning frameworks including:  - Self-Critical Sequence Training (SCST) [[arxiv]](https://arxiv.org/abs/1612.00563)     - Sampling is done via beam search [[arxiv]](https://arxiv.org/abs/1707.07998) - Multi-Head Visual Attention     - Additive (with optional Layer Norm) [[arxiv]](https://arxiv.org/abs/1903.01072)     - Scaled Dot-Product [[arxiv]](https://arxiv.org/abs/1706.03762) - Graph-based Beam Search  Greedy Search and Sampling  The features might not be completely tested. For a more stable implementation  please refer to [this repo](https://github.com/jiahuei/COMIC-Compact-Image-Captioning-with-Attention).    """;General;https://github.com/jiahuei/Self-Critical-SCST-TensorFlow
"""This repo contains **experimental and unofficial** implementation of image captioning frameworks including:  - Self-Critical Sequence Training (SCST) [[arxiv]](https://arxiv.org/abs/1612.00563)     - Sampling is done via beam search [[arxiv]](https://arxiv.org/abs/1707.07998) - Multi-Head Visual Attention     - Additive (with optional Layer Norm) [[arxiv]](https://arxiv.org/abs/1903.01072)     - Scaled Dot-Product [[arxiv]](https://arxiv.org/abs/1706.03762) - Graph-based Beam Search  Greedy Search and Sampling  The features might not be completely tested. For a more stable implementation  please refer to [this repo](https://github.com/jiahuei/COMIC-Compact-Image-Captioning-with-Attention).    """;Natural Language Processing;https://github.com/jiahuei/Self-Critical-SCST-TensorFlow
"""This repository contains my own implementation of the MobileNet Convolutional Neural Network (CNN) developed in Python programming language with Keras and TensorFlow enabled for Graphic Processing Unit (GPU). The provided source-code contains two functions representing the implementations of the MobileNetV1 and MobileNetV2 architectures. A case study in a image set of two airplane models is also provided to evaluate the performance of the network.   """;General;https://github.com/danilojodas/MobileNet
"""This repository contains my own implementation of the MobileNet Convolutional Neural Network (CNN) developed in Python programming language with Keras and TensorFlow enabled for Graphic Processing Unit (GPU). The provided source-code contains two functions representing the implementations of the MobileNetV1 and MobileNetV2 architectures. A case study in a image set of two airplane models is also provided to evaluate the performance of the network.   """;Computer Vision;https://github.com/danilojodas/MobileNet
"""This repository is modified on [Cascade R-CNN](https://github.com/zhaoweicai/Detectron-Cascade-RCNN)  which is re-implemented by by Zhaowei Cai and Nuno Vasconcelos on the base of [Detectron](https://github.com/facebookresearch/Detectron).   Please follow [Detectron](https://github.com/facebookresearch/Detectron) on how to install the environment.    """;Computer Vision;https://github.com/Adrian398/CB-Net-EDD
"""Recent Ubuntu releases come with python3 installed. I use pip3 for installing dependencies so install that with `sudo apt install python3-pip`. Install git if you don't already have it with `sudo apt install git`.  Then clone this repo with `git clone https://github.com/harvitronix/reinforcement-learning-car.git`. It has some pretty big weights files saved in past commits  so to just get the latest the fastest  do `git clone https://github.com/harvitronix/reinforcement-learning-car.git --depth 1`.   """;Reinforcement Learning;https://github.com/borhanreo/Obstacle-Avoid-Car
"""This part shall give an overview about the different reimplemented algorithms. These can be divided into *model-free* and *model-based* approaches.   """;Reinforcement Learning;https://github.com/borea17/efficient_rl
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Caesarzhang/bert-zh
"""Semantic image segmentation is a basic street scene understanding task in autonomous driving  where each pixel in a high resolution image is categorized into a set of semantic labels. Unlike other scenarios  objects in autonomous driving scene exhibit very large scale changes  which poses great challenges for high-level feature representation in a sense that multi-scale information must be correctly encoded.  To remedy this problem  atrous convolution[2  3] was introduced to generate features with larger receptive fields without sacrificing spatial resolution. Built upon atrous convolution  Atrous Spatial Pyramid Pooling (ASPP)[3] was proposed to concatenate multiple atrous-convolved features using different dilation rates into a final feature representation. Although ASPP is able to generate multi-scale features  we argue the feature resolution in the scale-axis is not dense enough for the autonomous driving scenario. To this end  we propose Densely connected Atrous Spatial Pyramid Pooling (DenseASPP)  which connects a set of atrous convolutional layers in a dense way  such that it generates multi-scale features that not only cover a larger scale range  but also cover that scale range densely  without significantly increasing the model size. We evaluate DenseASPP on the street scene benchmark Cityscapes[4] and achieve state-of-the-art performance.   """;Computer Vision;https://github.com/DeepMotionAIResearch/DenseASPP
"""VISSL is a computer **VI**sion library for state-of-the-art **S**elf-**S**upervised **L**earning research with [PyTorch](https://pytorch.org). VISSL aims to accelerate research cycle in self-supervised learning: from designing a new self-supervised task to evaluating the learned representations. Key features include:  - **Reproducible implementation of SOTA in Self-Supervision**: All existing SOTA in Self-Supervision are implemented - [SwAV](https://arxiv.org/abs/2006.09882)  [SimCLR](https://arxiv.org/abs/2002.05709)  [MoCo(v2)](https://arxiv.org/abs/1911.05722)  [PIRL](https://arxiv.org/abs/1912.01991)  [NPID](https://arxiv.org/pdf/1805.01978.pdf)  [NPID++](https://arxiv.org/abs/1912.01991)  [DeepClusterV2](https://arxiv.org/abs/2006.09882)  [ClusterFit](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_ClusterFit_Improving_Generalization_of_Visual_Representations_CVPR_2020_paper.pdf)  [RotNet](https://arxiv.org/abs/1803.07728)  [Jigsaw](https://arxiv.org/abs/1603.09246). Also supports supervised trainings.  - **Benchmark suite**: Variety of benchmarks tasks including [linear image classification (places205  imagenet1k  voc07  food  CLEVR  dsprites  UCF101  stanford cars and many more)](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/linear_image_classification)  [full finetuning](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/fulltune)  [semi-supervised benchmark](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/semi_supervised)  [nearest neighbor benchmark](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/nearest_neighbor)  [object detection (Pascal VOC and COCO)](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/object_detection).  - **Ease of Usability**: easy to use using yaml configuration system based on [Hydra](https://github.com/facebookresearch/hydra).  - **Modular**: Easy to design new tasks and reuse the existing components from other tasks (objective functions  model trunk and heads  data transforms  etc.). The modular components are simple *drop-in replacements* in yaml config files.  - **Scalability**: Easy to train model on 1-gpu  multi-gpu and multi-node. Several components for large scale trainings provided as simple config file plugs: [Activation checkpointing](https://pytorch.org/docs/stable/checkpoint.html)  [ZeRO](https://arxiv.org/abs/1910.02054)  [FP16](https://nvidia.github.io/apex/amp.html#o1-mixed-precision-recommended-for-typical-use)  [LARC](https://arxiv.org/abs/1708.03888)  Stateful data sampler  data class to handle invalid images  large model backbones like [RegNets](https://arxiv.org/abs/2003.13678)  etc.  - **Model Zoo**: Over *60 pre-trained self-supervised model* weights.   """;General;https://github.com/facebookresearch/vissl
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/sherinaseri/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/keiyamashita111/aaproject
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/keiyamashita111/aaproject
"""Salmon life cycles follow a predictable pattern: hatch in freshwater  migrate to the ocean for the majority of their lives  and then migrate back to their original freshwater hatch sites before they spawn and then die. The time spent in freshwater and ocean salt water depends on the species.  Salmon populations in the waters of Puget Sound are estimated each year when a mature portion of the salmon migrate back from the ocean to freshwater to spawn. In many areas  this pathway is partially obstructed by boat locks (Seattle)  or hydroelectric dams (Bonneville) and the salmon travel through carefully built fish ladders on this upstream journey. As they pass through the ladders  viewing windows allow them to be seen by both tourists and biologists  and human viewers are still the primary way to count the fish.  |Fish ladder| Bonneville Dam public window| Bonneville counting window (non-public)| |---|---|---| |<img src=""./assets/fish_ladder.png"" alt=""fish ladder explanation at https: //www youtube com/watch?v=sabk7Khq0kQ"" width='300' />| <img src=""./assets/fish_143.jpg"" alt=""man viewing fish through underwater viewing window"" width=""200""/>|<img src=""http://www.oceanlight.com/stock-photo/bonneville-dam-salmon-count-photo-19368-548507.jpg"" alt=""fisheries biologist counting salmon""  height=""200"" />| |Watch at https: //www youtube com/watch?v=sabk7Khq0kQ||Photo: (c) Phillip Colla OceanLight.com|  Once tallied  the estimated population for each species determines sport fishing limits such as the number of fish per day and the length of the fishing season. This data is also used to make decisions in the operation of salmon fisheries  commercial fishing  restaurants  and tourism.  |Columbia River Chinook Season|Ballard Locks 2020 Sockeye Counts| |---|---| |<img src=""./assets/chinook_2020_Columbia.png"" height='200'/>|<img src=""./assets/ballard_locks_sockeye_counts_71220.jpg"" height='200'/>| |News source: https://wdfw.wa.gov/news/summer-chinook-salmon-fishing-open-july-much-columbia-river | Updated count: https://wdfw.wa.gov/fishing/reports/counts/lake-washington#sockeye|   The salmon counting task is easier when few are in the ladder; the task is more difficult when many are returning at once. Some locations estimate the full population by counting for a set period of time each day and comparing to historical data. In other locations  24/7 video recording enables biologists to review footage and tally the counts later; weekend tallies can take staff multiple days to catch up on counts. At some sites  interested individuals can sign up for daily notifications on the latest counts.    In the Pacific Northwest  salmon are vital to commerce and to the marine ecosystem  and fish population estimates are key factors in many policy decisions. Current methods require trained biologists to observe the fish passing a viewing window and manually record the fish count on a daily basis.  This project explored the possibility of using machine learning methods of object detection and classification to support these efforts  potentially enabling the collection of data in more locations and over longer time periods.  Custom trained models (e.g. YOLO v5) using images from fish ladders showed that accurate fish detection is promising but non-trivial  counting fish in a still image does not solve the problem of counting fish in video  and that classifying fish by species requires excellent viewing conditions.   """;Computer Vision;https://github.com/jshaffer94247/Counting-Fish
"""* The agent has to learn how to land a Lunar Lander to the moon surface safely  quickly and accurately. * If the agent just lets the lander fall freely  it is dangerous and thus get a very negative reward from the environment. * If the agent does not land quickly enough (after 20 seconds)  it fails its objective and receive a negative reward from the environment. * If the agent lands the lander safely but in wrong position  it is given either a small negative or small positive reward  depending on how far from the landing zone is the lander. * If the AI lands the lander to the landing zone quickly and safely  it is successful and is award very positive reward.   """;Reinforcement Learning;https://github.com/anh-nn01/Lunar-Lander-Double-Deep-Q-Networks
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/kangyiS/caffe_ky
"""PaddleDetection is an end-to-end object detection development kit based on PaddlePaddle  which implements varied mainstream object detection  instance segmentation  tracking and keypoint detection algorithms in modular designwhich with configurable modules such as network components  data augmentations and losses  and release many kinds SOTA industry practice models  integrates abilities of model compression and cross-platform high-performance deployment  aims to help developers in the whole end-to-end development in a faster and better way.   """;General;https://github.com/PaddlePaddle/PaddleDetection
"""Accurately ranking the vast number of candidate detections is crucial for dense object detectors to achieve high performance. In this work  we propose to learn IoU-aware classification scores (**IACS**) that simultaneously represent the object presence confidence and localization accuracy  to produce a more accurate ranking of detections in dense object detectors. In particular  we design a new loss function  named **Varifocal Loss (VFL)**  for training a dense object detector to predict the IACS  and a new efficient star-shaped bounding box feature representation (the features at nine yellow sampling points) for estimating the IACS and refining coarse bounding boxes. Combining these two new components and a bounding box refinement branch  we build a new IoU-aware dense object detector based on the FCOS+ATSS architecture  what we call **VarifocalNet** or **VFNet** for short. Extensive experiments on MS COCO benchmark show that our VFNet consistently surpasses the strong baseline by ~2.0 AP with different backbones. Our best model VFNet-X-1200 with Res2Net-101-DCN reaches a single-model single-scale AP of **55.1** on COCO `test-dev`  achieving the state-of-the-art performance among various object detectors.  <div align=""center"">   <img src=""VFNet.png"" width=""600px"" />   <p>Learning to Predict the IoU-aware Classification Score.</p> </div>   """;Computer Vision;https://github.com/hyz-xmaster/VarifocalNet
"""Accurately ranking the vast number of candidate detections is crucial for dense object detectors to achieve high performance. In this work  we propose to learn IoU-aware classification scores (**IACS**) that simultaneously represent the object presence confidence and localization accuracy  to produce a more accurate ranking of detections in dense object detectors. In particular  we design a new loss function  named **Varifocal Loss (VFL)**  for training a dense object detector to predict the IACS  and a new efficient star-shaped bounding box feature representation (the features at nine yellow sampling points) for estimating the IACS and refining coarse bounding boxes. Combining these two new components and a bounding box refinement branch  we build a new IoU-aware dense object detector based on the FCOS+ATSS architecture  what we call **VarifocalNet** or **VFNet** for short. Extensive experiments on MS COCO benchmark show that our VFNet consistently surpasses the strong baseline by ~2.0 AP with different backbones. Our best model VFNet-X-1200 with Res2Net-101-DCN reaches a single-model single-scale AP of **55.1** on COCO `test-dev`  achieving the state-of-the-art performance among various object detectors.  <div align=""center"">   <img src=""VFNet.png"" width=""600px"" />   <p>Learning to Predict the IoU-aware Classification Score.</p> </div>   """;General;https://github.com/hyz-xmaster/VarifocalNet
"""<p align=""center"">  <img src=""./figures/summary_table.png"" align=""center"" width=""500"" title=""summary_table"" > </p>    """;Computer Vision;https://github.com/clovaai/assembled-cnn
"""Code repo for reproducing [2017 CVPR](https://arxiv.org/abs/1611.08050) paper using keras.     """;General;https://github.com/anatolix/keras_Realtime_Multi-Person_Pose_Estimation
"""This is the Tensorflow code corresponding to [A Two-Stage Method for Text Line Detection in Historical Documents ](#a-two-stage-method-for-text-line-detection-in-historical-documents). This repo contains the neural pixel labeling part described in the paper. It contains the so-called ARU-Net (among others) which is basically an extended version of the well known U-Net [[2]](#u-net-convolutional-networks-for-biomedical-image-segmentation).  Besides the model and the basic workflow to train and test models  different data augmentation strategies are implemented to reduce the amound of training data needed. The repo's features are summarized below: + Inference Demo     + Trained and freezed tensorflow graph included     + Easy to reuse for own inference tests + Workflow      + Full training workflow to parametrize and train your own models     + Contains different models  data augmentation strategies  loss functions      + Training on specific GPU  this enables the training of several models on a multi GPU system in parallel     + Easy validation for trained model either using classical or ema-shadow weights  Please cite [[1]](#a-two-stage-method-for-text-line-detection-in-historical-documents) if you find this repo useful and/or use this software for own work.    """;Computer Vision;https://github.com/imagine5am/ARU-Net
"""**Robustness problem in neural networks:** Neural networks deployed in the real-world will encounter data with naturally occurring distortions  e.g. motion blur  brightness changes  etc. Such changes make up shifts from the training data distribution. While neural networks are able to learn complex functions in-distribution  their predictions are deemed unreliable under such shifts  i.e. they are not robust. This presents a core challenge that needs to be solved for these models to be useful in the real-world.  **Why do we need robust predictions?** Suppose we want to learn a mapping from an input domain  e.g. RGB images  to a target domain  e.g. surface normals (see above figure). A common approach is to learn this mapping with a `direct` path  i.e. `RGB → surface normals`. Since this path directly operates on the input domain  it is prone to being affected by any slight alterations in the RGB image  e.g. brightness changes.   **How do we obtain robust predictions?** An alternative can be to go through a **middle domain** that is invariant to that change. For example  the surface normals predicted via the `RGB → 2D edges → surface normals` path will be resilient to brightness distortions in the input as the 2D edges domain abstracts that away. However  the distortions that a model may encounter are broad and unknown ahead of time  and some middle domains can be too lossy for certain downstream predictions. These issues can be mitigated by employing an **ensemble** of predictions made via a **diverse set of middle domains** and merging their (relatively weaker) predictions into one (stronger) output on-the-fly.     """;Computer Vision;https://github.com/EPFL-VILAB/XDEnsembles
""" This report has two results from the lab. The first result is about sentiment analysis with the BERT model on Drug review datasets.1 We built a model to predict the sentiment of the drug review. The second result is about survival analysis with RSF and ANN on the breast cancer datasets from the University of Wisconsin.2 We built a model to predict time till the events  breast cancer occurs in our case  with other features.   """;Natural Language Processing;https://github.com/JeheonPark596/sentimentsurvival
""" This report has two results from the lab. The first result is about sentiment analysis with the BERT model on Drug review datasets.1 We built a model to predict the sentiment of the drug review. The second result is about survival analysis with RSF and ANN on the breast cancer datasets from the University of Wisconsin.2 We built a model to predict time till the events  breast cancer occurs in our case  with other features.   """;General;https://github.com/JeheonPark596/sentimentsurvival
"""We explore several different adaptive finetuning strategies in this repository. One thing that is common to all the strategies is the use of a policy network to determine which parts of the model to finetune/drop based on the input images-text pair. The chosen policy network is relatively very small when compared to the original VLBERT/LXMERT network. The policy network is optimized using Gumbel Softmax which relieves the argmax constraints to softmax while backpropagation.   """;Natural Language Processing;https://github.com/itsShnik/adaptively-finetuning-transformers
"""Image inpainting is the art of reconstructing damaged/missing parts of an image and can be extended to videos easily. Producing images where the missing parts have been filled with bothvisually and semantically plausible appeal  is the main objective of an artificial image inpainter.<br>  Before Deep learning   computer vision is used for that purpose. It’s worth noting that these techniques are good at inpainting backgrounds in an image but fail to generalize.<br> In modern approach  we train a neural network to predict missing parts of an image such that the predictions are both visually and semantically consistent.   """;General;https://github.com/akshay-gupta123/Context-Encoder
"""This is an official pytorch implementation of [*Deep High-Resolution Representation Learning for Human Pose Estimation*](https://arxiv.org/abs/1902.09212).  In this work  we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods **recover high-resolution representations from low-resolution representations** produced by a high-to-low resolution network. Instead  our proposed network **maintains high-resolution representations** through the whole process. We start from a high-resolution subnetwork as the first stage  gradually add high-to-low resolution subnetworks one by one to form more stages  and connect the mutli-resolution subnetworks **in parallel**. We conduct **repeated multi-scale fusions** such that each of the high-to-low resolution representations receives information from other parallel representations over and over  leading to rich high-resolution representations. As a result  the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. </br>  ![Illustrating the architecture of the proposed HRNet](/figures/hrnet.png)  """;Computer Vision;https://github.com/d-shivam/Pose-estimation-based-action-recognition-for-help-Situation-Identification
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/jack16888/caffe-ssd
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Caesarzhang/bert-zh
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/Lmonster/darknet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/sherinaseri/bert
"""In this competition  you’ll detect wheat heads from outdoor images of wheat plants  including wheat datasets from around the globe. Using worldwide data  you will focus on a generalized solution to estimate the number and size of wheat heads. To better gauge the performance for unseen genotypes  environments  and observational conditions  the training dataset covers multiple regions. You will use more than 3 000 images from Europe (France  UK  Switzerland) and North America (Canada). The test data includes about 1 000 images from Australia  Japan  and China.   """;Computer Vision;https://github.com/leduckhai/Global-Wheat-Detection-Kaggle
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/tvinith/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/tvinith/bert
"""Faster R-CNN has two networks: region proposal network (RPN) for generating region proposals and a network using these proposals to detect objects. The main different here with Fast R-CNN is that the later uses selective search to generate region proposals. The time cost of generating region proposals is much smaller in RPN than selective search  when RPN shares the most computation with the object detection network. Briefly  RPN ranks region boxes (called anchors) and proposes the ones most likely containing objects.   """;Computer Vision;https://github.com/KushaalShroff/Signature-and-Annotation-detection
"""R-CNN is a state-of-the-art visual object detection system that combines bottom-up region proposals with rich features computed by a convolutional neural network. At the time of its release  R-CNN improved the previous best detection performance on PASCAL VOC 2012 by 30% relative  going from 40.9% to 53.3% mean average precision. Unlike the previous best results  R-CNN achieves this performance without using contextual rescoring or an ensemble of feature types.  R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1311.2524) and will appear in a forthcoming CVPR 2014 paper.   """;Computer Vision;https://github.com/Coldmooon/RCNN-for-Latest-Caffe
"""The vanilla GAN (https://arxiv.org/abs/1406.2661) tries to find the Nash Equilibrium between Generator and Discriminator  and it minimizes the Jessen - Shannon Divergence at the optimal point. It is the generative model without the likelihood. However  there were some issues - GAN is very hard to train  and it is precarious. There were many proposed solutions to these problems  as mentioned earlier.  One of the breakthroughs was WGAN paper (https://arxiv.org/abs/1701.07875). Rather than finding the equilibrium between two neural networks  WGAN paper tries to minimize the 1-Wasserstein Distance(WD) between two networks. Intuitively  WD is the cost function of moving one distribution to the another. As the neural network is a powerful function approximator  WGAN finds the optimal transport from the sample to the real distribution.  However  the functions we derived from the WGAN need to meet the 1-Lipschitz condition. WGAN-GP came up with one solution to impose the gradient penalty(GP) as the gradient we obtained from the point between the real data and the samples deviates from 1. This approach works quite well.    """;Computer Vision;https://github.com/wayne1123/mnist_wgan_gp
"""The vanilla GAN (https://arxiv.org/abs/1406.2661) tries to find the Nash Equilibrium between Generator and Discriminator  and it minimizes the Jessen - Shannon Divergence at the optimal point. It is the generative model without the likelihood. However  there were some issues - GAN is very hard to train  and it is precarious. There were many proposed solutions to these problems  as mentioned earlier.  One of the breakthroughs was WGAN paper (https://arxiv.org/abs/1701.07875). Rather than finding the equilibrium between two neural networks  WGAN paper tries to minimize the 1-Wasserstein Distance(WD) between two networks. Intuitively  WD is the cost function of moving one distribution to the another. As the neural network is a powerful function approximator  WGAN finds the optimal transport from the sample to the real distribution.  However  the functions we derived from the WGAN need to meet the 1-Lipschitz condition. WGAN-GP came up with one solution to impose the gradient penalty(GP) as the gradient we obtained from the point between the real data and the samples deviates from 1. This approach works quite well.    """;General;https://github.com/wayne1123/mnist_wgan_gp
"""![Figure 1 from paper](examples/figure1.png)  Pytorch implementation of paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929).  We provide the pretrained pytorch weights which are converted from pretrained jax/flax models. We also provide fine-tune and evaluation script.  Similar results as in [original implementation](https://github.com/google-research/vision_transformer) are achieved.    """;Computer Vision;https://github.com/asyml/vision-transformer-pytorch
"""1. [requirements.txt](https://github.com/iDataist/Tennis-With-Multi-Agent-Reinforcement/blob/main/requirements.txt) - Includes all the required libraries for the Conda Environment. 2. [model.py](https://github.com/iDataist/Tennis-With-Multi-Agent-Reinforcement/blob/main/model.py) - Defines the actor and critic networks. 3. [agent.py](https://github.com/iDataist/Tennis-With-Multi-Agent-Reinforcement/blob/main/agent.py) - Defines the Agent that uses MADDPG to determine the best action to take and maximizes the overall or total reward. 4. [Tennis.ipynb](https://github.com/iDataist/Tennis-With-Multi-Agent-Reinforcement/blob/main/Tennis.ipynb) - The main file that trains the agents. This file can be run in the Conda environment.   """;Reinforcement Learning;https://github.com/iDataist/Tennis-With-Multi-Agent-Reinforcement
"""1) Data  When you run training script  it automatically downloads and saves the data if it is not downloaded already.   2) Installation of required packages: ``` pip install pipenv          #: To install pipenv if you don't have it already pipenv shell                #: To activate virtual env pipenv install --skip-lock  #: To install required packages.  ```  3) Training and evaluation of the models: ```   I) python 0_train.py                              #: Train autoencoder using default dataset STL10  II) python 1_eval.py -d ""CIFAR10"" -img 32          #: Evaluations using CIFAR10 dataset ``` If you want to train on another dataset  you can simply define it: ``` python 0_train.py -d ""dataset_name"" -img image_size ```  If you want to use Python 3.7  please follow the steps described in [Important Note](#important-note).    """;General;https://github.com/talipucar/PyFlow_SimCLR
"""<div align=center><img width=""100%"" src=""figs/FFB6D_overview.png""/></div>  [FFB6D](https://arxiv.org/abs/2103.02242v1) is a general framework for representation learning from a single RGBD image  and we applied it to the 6D pose estimation task by cascading downstream prediction headers for instance semantic segmentation and 3D keypoint voting prediction from PVN3D([Arxiv](https://arxiv.org/abs/1911.04231)  [Code](https://github.com/ethnhe/PVN3D)  [Video](https://www.bilibili.com/video/av89408773/)).  At the representation learning stage of FFB6D  we build **bidirectional** fusion modules in the **full flow** of the two networks  where fusion is applied to each encoding and decoding layer. In this way  the two networks can leverage local and global complementary information from the other one to obtain better representations. Moreover  at the output representation stage  we designed a simple but effective 3D keypoints selection algorithm considering the texture and geometry information of objects  which simplifies keypoint localization for precise pose estimation.  Please cite [FFB6D](https://arxiv.org/abs/2103.02242v1) & [PVN3D](https://arxiv.org/abs/1911.04231) if you use this repository in your publications:  ``` @InProceedings{He_2021_CVPR  author = {He  Yisheng and Huang  Haibin and Fan  Haoqiang and Chen  Qifeng and Sun  Jian}  title = {FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation}  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}  month = {June}  year = {2021} }  @InProceedings{He_2020_CVPR  author = {He  Yisheng and Sun  Wei and Huang  Haibin and Liu  Jianran and Fan  Haoqiang and Sun  Jian}  title = {PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation}  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}  month = {June}  year = {2020} } ```   """;Computer Vision;https://github.com/ethnhe/FFB6D
"""VISSL is a computer **VI**sion library for state-of-the-art **S**elf-**S**upervised **L**earning research with [PyTorch](https://pytorch.org). VISSL aims to accelerate research cycle in self-supervised learning: from designing a new self-supervised task to evaluating the learned representations. Key features include:  - **Reproducible implementation of SOTA in Self-Supervision**: All existing SOTA in Self-Supervision are implemented - [SwAV](https://arxiv.org/abs/2006.09882)  [SimCLR](https://arxiv.org/abs/2002.05709)  [MoCo(v2)](https://arxiv.org/abs/1911.05722)  [PIRL](https://arxiv.org/abs/1912.01991)  [NPID](https://arxiv.org/pdf/1805.01978.pdf)  [NPID++](https://arxiv.org/abs/1912.01991)  [DeepClusterV2](https://arxiv.org/abs/2006.09882)  [ClusterFit](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_ClusterFit_Improving_Generalization_of_Visual_Representations_CVPR_2020_paper.pdf)  [RotNet](https://arxiv.org/abs/1803.07728)  [Jigsaw](https://arxiv.org/abs/1603.09246). Also supports supervised trainings.  - **Benchmark suite**: Variety of benchmarks tasks including [linear image classification (places205  imagenet1k  voc07  food  CLEVR  dsprites  UCF101  stanford cars and many more)](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/linear_image_classification)  [full finetuning](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/fulltune)  [semi-supervised benchmark](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/semi_supervised)  [nearest neighbor benchmark](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/nearest_neighbor)  [object detection (Pascal VOC and COCO)](https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/object_detection).  - **Ease of Usability**: easy to use using yaml configuration system based on [Hydra](https://github.com/facebookresearch/hydra).  - **Modular**: Easy to design new tasks and reuse the existing components from other tasks (objective functions  model trunk and heads  data transforms  etc.). The modular components are simple *drop-in replacements* in yaml config files.  - **Scalability**: Easy to train model on 1-gpu  multi-gpu and multi-node. Several components for large scale trainings provided as simple config file plugs: [Activation checkpointing](https://pytorch.org/docs/stable/checkpoint.html)  [ZeRO](https://arxiv.org/abs/1910.02054)  [FP16](https://nvidia.github.io/apex/amp.html#o1-mixed-precision-recommended-for-typical-use)  [LARC](https://arxiv.org/abs/1708.03888)  Stateful data sampler  data class to handle invalid images  large model backbones like [RegNets](https://arxiv.org/abs/2003.13678)  etc.  - **Model Zoo**: Over *60 pre-trained self-supervised model* weights.   """;Computer Vision;https://github.com/facebookresearch/vissl
"""This is the official code of [high-resolution representations for Semantic Segmentation](https://arxiv.org/abs/1904.04514).  We augment the HRNet with a very simple segmentation head shown in the figure below. We aggregate the output representations at four different resolutions  and then use a 1x1 convolutions to fuse these representations. The output representations is fed into the classifier. We evaluate our methods on three datasets  Cityscapes  PASCAL-Context and LIP.  <!-- ![](figures/seg-hrnet.png) --> <figure>   <text-align: center;>   <img src=""./figures/seg-hrnet.png"" alt=""hrnet"" title="""" width=""900"" height=""150"" /> </figcaption> </figure>  Besides  we further combine HRNet with [Object Contextual Representation](https://arxiv.org/pdf/1909.11065.pdf) and achieve higher performance on the three datasets. The code of HRNet+OCR is contained in this branch. We illustrate the overall framework of OCR in the Figure and the equivalent Transformer pipelines:  <figure>   <text-align: center;>   <img src=""./figures/OCR.PNG"" alt=""OCR"" title="""" width=""900"" height=""200"" /> </figure>     <figure>   <text-align: center;>   <img src=""./figures/SegmentationTransformerOCR.png"" alt=""Segmentation Transformer"" title="""" width=""600"" /> </figure>   """;Computer Vision;https://github.com/HRNet/HRNet-Semantic-Segmentation
"""Lung Iris is a web application that can automatically detect pneumonia patients from their chest X-ray images. With more than 2 billions procedures for pneumonia diagnosis performed manually by radiologists worldwide  Lung Iris serves as the optimal solution for saving time and cost for the healthcare system. In addition  in view of the COVID-19 pandemic  Lung Iris can help identify critical COVID-19 patients who have developed pneumonia in their lungs  making the diagnosis time much faster for them so that they can receive prompt care. This is particularly effective in situations where the healcare system is overloaded and there is a lack of personnel.    """;Computer Vision;https://github.com/trangiabach/Lung-Iris
""" - MLPMixer  <img width=""500"" alt=""스크린샷 2021-05-10 오후 10 13 36"" src=""https://user-images.githubusercontent.com/22078438/117664703-0c77d200-b1dd-11eb-9dcd-498c829520a7.png"">  - ResMLP  <img width=""500"" alt=""스크린샷 2021-05-10 오후 10 13 51"" src=""https://user-images.githubusercontent.com/22078438/117664706-0da8ff00-b1dd-11eb-9541-308e76680810.png"">  - gMLP  <img width=""500"" alt=""스크린샷 2021-05-10 오후 10 13 51"" src=""https://user-images.githubusercontent.com/22078438/120160982-d48b0a00-c231-11eb-8f13-39c4f3de3cf2.png"">    """;Computer Vision;https://github.com/leaderj1001/Bag-of-MLP
""" - MLPMixer  <img width=""500"" alt=""스크린샷 2021-05-10 오후 10 13 36"" src=""https://user-images.githubusercontent.com/22078438/117664703-0c77d200-b1dd-11eb-9dcd-498c829520a7.png"">  - ResMLP  <img width=""500"" alt=""스크린샷 2021-05-10 오후 10 13 51"" src=""https://user-images.githubusercontent.com/22078438/117664706-0da8ff00-b1dd-11eb-9541-308e76680810.png"">  - gMLP  <img width=""500"" alt=""스크린샷 2021-05-10 오후 10 13 51"" src=""https://user-images.githubusercontent.com/22078438/120160982-d48b0a00-c231-11eb-8f13-39c4f3de3cf2.png"">    """;General;https://github.com/leaderj1001/Bag-of-MLP
"""The implementation is based on two papers:  - Simple Online and Realtime Tracking with a Deep Association Metric https://arxiv.org/abs/1703.07402 - YOLOv4: Optimal Speed and Accuracy of Object Detection https://arxiv.org/pdf/2004.10934.pdf   This repository contains a moded version of PyTorch YOLOv5 (https://github.com/ultralytics/yolov5). It filters out every detection that is not a person. The detections of persons are then passed to a Deep Sort algorithm (https://github.com/ZQPei/deep_sort_pytorch) which tracks the persons. The reason behind the fact that it just tracks persons is that the deep association metric is trained on a person ONLY datatset.   """;Computer Vision;https://github.com/ElipLam/IceBlock_Detection
"""Residual Attention Layer Transformer  shortened as RealFormer  is a transformer variant that incorporatess residual skip connections to allow previous attention scores to pass through the entire network. It outperforms canonical transformers on a variety of tasks and datasets  including masked language modeling (MLM)  [GLUE](https://gluebenchmark.com)  and [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/).   """;Natural Language Processing;https://github.com/jaketae/realformer
"""GEORGES is an Evolutionary Reinforcement Learning Framework inspired by emerging Evolutionary solutions **[1  2]** which are leveraging from a Population of individuals  allowing parameter and reward auto-tuning.   GEORGES is a combination of Population Based Training  Further Genetic operations (Mutation  Crossover...) and Tournament Simulation (Pool and Bracket model).  We train a population of individuals  or players  where each player mains a character and tries to maximize their Elo score. An individual with a score too low will eventually be replaced by a mutated version of a higher ranked individual.  The winning team of a tournament will generate an offspring through crossover  which will replace the worst player Elo-wise in the population.  The V-Trace algorithm **[3]** (an Actor Critic algorithm variant with off-policy correction) is used to train individuals  continuously  on top of the GA operators  from generated experience of games simulated on [Dolphin](https://github.com/dolphin-emu/dolphin).   """;General;https://github.com/villinvic/Georges
"""GEORGES is an Evolutionary Reinforcement Learning Framework inspired by emerging Evolutionary solutions **[1  2]** which are leveraging from a Population of individuals  allowing parameter and reward auto-tuning.   GEORGES is a combination of Population Based Training  Further Genetic operations (Mutation  Crossover...) and Tournament Simulation (Pool and Bracket model).  We train a population of individuals  or players  where each player mains a character and tries to maximize their Elo score. An individual with a score too low will eventually be replaced by a mutated version of a higher ranked individual.  The winning team of a tournament will generate an offspring through crossover  which will replace the worst player Elo-wise in the population.  The V-Trace algorithm **[3]** (an Actor Critic algorithm variant with off-policy correction) is used to train individuals  continuously  on top of the GA operators  from generated experience of games simulated on [Dolphin](https://github.com/dolphin-emu/dolphin).   """;Reinforcement Learning;https://github.com/villinvic/Georges
"""**XLNet** is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally  XLNet employs [Transformer-XL](https://arxiv.org/abs/1901.02860) as the backbone model  exhibiting excellent performance for language tasks involving long context. Overall  XLNet achieves state-of-the-art (SOTA) results on various downstream language tasks including question answering  natural language inference  sentiment analysis  and document ranking.  For a detailed description of technical details and experimental results  please refer to our paper:  ​        [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)  ​        Zhilin Yang\*  Zihang Dai\*  Yiming Yang  Jaime Carbonell  Ruslan Salakhutdinov  Quoc V. Le   ​        (*: equal contribution)   ​        Preprint 2019      """;Natural Language Processing;https://github.com/zihangdai/xlnet
"""Agents in various operational sites have to do a vast variety of tasks and have to multitask using the tools they have.  These agents are the face of the company to a lot of customers and the customer experience provided by these agents is very important in driving customer's perception of the product and the company.  In global companies  some agents also have to service accounts of accountholders who speak other languages. So it is important that agents have multiple tools at their fingertips that enable them to do their jobs better.  In addition  better tools can drive more efficiency which can help to reduce staffing demand and drive lower costs for organizations.  This project was to develop a suite of tools (called AA for Agent Assistant) to help agents in their efforts in servicing customers.  There were 3 ML based tools that have been developed here to assist the agents in their effort:  #1 **English to Spanish translator**:  The intent of this tool is to enable agents to communicate with Spanish-speaking customers. This was built using English and Spanish datasets of multiple sentences.  It was built using natural language processing tools in [scikit-learn](https://scikit-learn.org/stable/) and [NLTK](https://www.nltk.org/).  A sequence-to-sequence LSTM model was built using the underlying data to develop a model to enable translation.  The model's accuracy was around 80% for the test and train population.  #2 **Sentence Completion**: The intent of this tool is to enable an agent to complete their sentences after they had typed the first few words making them faster at their job.  This would be akin to how gmail completes sentences after you have typed a few words.  The same English sentences that were used for translation were used.  However  the input feature was the first few words from the English lines and the last few words from the same English line was the target for the model.  Similar to the translation algorithm  another sequence-to-sequence LSTM model was built to enable completion.  The model's completion accuracy is displayed to be ~80% for the test population  however  the proposed sentence completers are not as clear.   #3 **Chatbot to help agents**:  The intent of this tool is to enable an agent to type in a question about some credit card feature and it would then answer the question.  Using the FAQ from a bank's webpage and randomizing permutations  a chat corpus was created.  This is then used to used to train the model where a question is mapped to the intent behind the question.  Intent can range from how to apply  how to earn rewards  what is reported to the credit bureaus etc.  Each intent has a unique answer and the chatbot would respond with that answer.  The model accuracy here was over 95% primarily because the chat corpus was built around the FAQ words.  For the Translator and Completer  the execution of training and inferencing the LSTM model had to be developed separately. Given the long run-times  additional functions were developed to continue with the epoch training after a few epoch runs.  Also a set of decoding functions had to be developed to read in the text and load that into a certain form for the model's encoder and decoder to use and translate/complete.  A streamlit app was developed for all the 3 models so that AA can be demonstrated to agents and interested organizations.  Some challenges that were faced (especially for the Translator and Completer)  was that not all lines could be used due to system & memory limitations in the training process.  This limited the training of the model and the translations/completions it can come up with.  Also the model wouldn't run on a laptop and had to be run on Google Cloud Platform in order to run over a few hours.  Both Translator and Completer needed over 50 epochs which took over 5 hours to run on Google Cloud Platform.  Looking forward  the opportunity is to improve the algorithms and use more data in conjunction to improve the quality of the translation and completions.  For the chat model  the opportunity is to train the model on actual chat conversations so that it can be trained better on unseen data and develop better response.     """;Natural Language Processing;https://github.com/sraj-whatdoesthedatasay/Capstone
"""notebooks/ contains an example of data augmentation (DataAugmentation)  the training of the model (ModelTraining) and the testing of the trained model (ModelTesting)  app/ contains all the files to run the application: the trained model with the weights  the Flask application  the Dockerfile and the requirements file  test_images/ contains few images from the test subset that can be used to test the app   Computer vision is used for surface defects inspection in multiple fields  like manufacturing and civil engineering. In this project  the problem of detecting cracks in a concrete surface has been tackled using a deep learning model.   """;Computer Vision;https://github.com/simo-bat/Crack_detection
"""notebooks/ contains an example of data augmentation (DataAugmentation)  the training of the model (ModelTraining) and the testing of the trained model (ModelTesting)  app/ contains all the files to run the application: the trained model with the weights  the Flask application  the Dockerfile and the requirements file  test_images/ contains few images from the test subset that can be used to test the app   Computer vision is used for surface defects inspection in multiple fields  like manufacturing and civil engineering. In this project  the problem of detecting cracks in a concrete surface has been tackled using a deep learning model.   """;General;https://github.com/simo-bat/Crack_detection
"""Since our Shapes study makes use of multiple data sources and a specific augmentation process  we created a separate script called `prepare_Shapes_data.py` for this preprocessing stage. It can be invoked as follows: ```python -m code.ml.preprocessing.prepare_Shapes_data path/to/folds_file.csv path/to/output_directory/ factor``` Here  `folds_file.csv` is a csv file that contains the columns `path` (giving the relative path of the image file from the project's root directory) and `fold` (the fold to which this image belongs). For classification data  a column `class` indicates the image class  while for data with psychological similarity ratings  the column `id` gives the stimulus ID used in the similarity space. The script will read all images listed in the `path` column of the `folds_file.csv`  create `factor` augmented copies of each image (by scaling it to a random size between 168 and 224 and by randomly translating it afterwards). The resulting augmented images will be stored as individual png files in the given `output_directory` and an additional pickle file containing a (shuffled) list of paths and classes/ids is created in the same file.  The script takes the following optional arguments: - `-p` or `--pickle_output_folder`: If a pickle output similar to the one provided by `data_augmentation.py` is desired  you can define the output folder for the augmented images here. - `-n` or `--noise_prob`: A list of floats specifying the different noise levels of salt and pepper noise to be added in the pickle versions. - `-s` or `--seed`: Specify a seed for the random number generator in order to make the results deterministic. If no seed is given  then the random number generator is not seeded. - `-o` or `--output_size`: Size of the output image  defaults to 224. - `-m` or `--minimum_size`: Minimal size of the object  defaults to 168.   """;Computer Vision;https://github.com/lbechberger/LearningPsychologicalSpaces
"""This is an example of running Lambda on GreenGrass with MXNet pretrained model Inception v3 for image classification  Details on Inception v3 can be found in https://arxiv.org/abs/1512.00567   """;Computer Vision;https://github.com/xavierraffin/aws-greengrass-inception
"""This project is to study the use of Convolutional Neural Network and in particular the ResNet architecture. The show case is segmentation of Magnetic Resonance Images (MRI) of human brain into anatomical regions[2]. We will extend the ResNet topology into the processing of 3-dimensional voxels.    """;General;https://github.com/ashishpatel26/BrainMRI-Segmentation-Keras
"""This project is to study the use of Convolutional Neural Network and in particular the ResNet architecture. The show case is segmentation of Magnetic Resonance Images (MRI) of human brain into anatomical regions[2]. We will extend the ResNet topology into the processing of 3-dimensional voxels.    """;Computer Vision;https://github.com/ashishpatel26/BrainMRI-Segmentation-Keras
"""This repo contains PyTorch implementation for paper [HAQ: Hardware-Aware Automated Quantization with Mixed Precision](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_HAQ_Hardware-Aware_Automated_Quantization_With_Mixed_Precision_CVPR_2019_paper.pdf) (CVPR2019  oral)  ![overview](https://hanlab.mit.edu/projects/haq/images/overview.png)  ``` @inproceedings{haq  author = {Wang  Kuan and Liu  Zhijian and Lin  Yujun and Lin  Ji and Han  Song}  title = {HAQ: Hardware-Aware Automated Quantization With Mixed Precision}  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}  year = {2019} } ```  Other papers related to automated model design: - AMC: AutoML for Model Compression and Acceleration on Mobile Devices ([ECCV 2018](https://arxiv.org/abs/1802.03494))  - ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware ([ICLR 2019](https://arxiv.org/abs/1812.00332))   """;Computer Vision;https://github.com/mit-han-lab/haq
"""This repo contains PyTorch implementation for paper [HAQ: Hardware-Aware Automated Quantization with Mixed Precision](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_HAQ_Hardware-Aware_Automated_Quantization_With_Mixed_Precision_CVPR_2019_paper.pdf) (CVPR2019  oral)  ![overview](https://hanlab.mit.edu/projects/haq/images/overview.png)  ``` @inproceedings{haq  author = {Wang  Kuan and Liu  Zhijian and Lin  Yujun and Lin  Ji and Han  Song}  title = {HAQ: Hardware-Aware Automated Quantization With Mixed Precision}  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}  year = {2019} } ```  Other papers related to automated model design: - AMC: AutoML for Model Compression and Acceleration on Mobile Devices ([ECCV 2018](https://arxiv.org/abs/1802.03494))  - ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware ([ICLR 2019](https://arxiv.org/abs/1812.00332))   """;General;https://github.com/mit-han-lab/haq
"""Integration of (https://arxiv.org/abs/1506.02640) and Redmon and Farhadi  2016 (https://arxiv.org/abs/1612.08242).   YOLO stands for You Only Look Once. It's an object detector that uses features learned by a deep convolutional neural network to detect an object. Before we get out hands dirty with code  we must understand how YOLO works.     """;Computer Vision;https://github.com/kirilcvetkov92/Vehicle-Detection
"""Architecture of our complementary segmentation network  the optimal CompNet. The dense blocks (DB)  corresponding to the gray bars  are used in each encoder and decoder. The triple (x y z) in each dense block indicates that it has x convolutional layers with a kernel size 3×3; each layer has y filters  except for the last one that has z filters. SO: segmentation output for the brain mask; CO: complementary segmentation output for the non-brain mask; RO: reconstruction output for the input image. These three outputs produced by the Sigmoid function are the final predictions; while all other Sigmoids produce intermediate outputs  except for the green one that is the concatenation of the summation from each intermediate layers. Best viewed in color.  *ROI and CO branches -  We take the downsampling branch of a U-Net as it is  however we split the upsampling branch into two halves  one to obtain the Region of Interest and the other for Complementary aka non region of interest. Losses here are negative dice for ROI and positive dice for Non-ROI region.*  *Reconstruction Branch -  Next we merge these two ROI and non ROI outputs using ""Summation"" operation and then pass it into another U-Net  This U-Net is the reconstruction branch. The input is the summed image from previous step and the output is the ""original"" image that we start with. The loss of reconstruction branch is MSE.*  ``` The code in this repository provides only the stand alone code for this architecture. You may implement it as is  or convert it into modular structure if you so wish. The dataset of OASIS can obtained from the link above and the preprocessiong steps involved are mentioned in the paper.  You have to provide the inputs. ```   email me - rd31879@uga.edu for any questions !! Am happy to discuss    """;Computer Vision;https://github.com/raun1/MICCAI2018---Complementary_Segmentation_Network-Raw-Code
"""Our Bregman learning framework aims at training sparse neural networks in an inverse scale space manner  starting with very few parameters and gradually adding only relevant parameters during training. We train a neural network <img src=""https://latex.codecogs.com/svg.latex?f_\theta:\mathcal{X}\rightarrow\mathcal{Y}"" title=""net""/> parametrized by weights <img src=""https://latex.codecogs.com/svg.latex?\theta"" title=""weights""/> using the simple baseline algorithm <p align=""center"">       <img src=""https://latex.codecogs.com/svg.latex?\begin{cases}v\gets\ v-\tau\hat{\nabla}\mathcal{L}(\theta) \\\theta\gets\mathrm{prox}_{\delta\ J}(\delta\ v) \end{cases}"" title=""Update"" /> </p>  where  * <img src=""https://latex.codecogs.com/svg.latex?\mathcal{L}"" title=""loss""/> denotes a loss function with stochastic gradient <img src=""https://latex.codecogs.com/svg.latex?\hat{\nabla}\mathcal{L}"" title=""stochgrad""/>  * <img src=""https://latex.codecogs.com/svg.latex?J"" title=""J""/> is a sparsity-enforcing functional  e.g.  the <img src=""https://latex.codecogs.com/svg.latex?\ell_1"" title=""ell1""/>-norm  * <img src=""https://latex.codecogs.com/svg.latex?\mathrm{prox}_{\delta\ J}"" title=""prox""/> is the proximal operator of <img src=""https://latex.codecogs.com/svg.latex?J"" title=""J""/>.  Our algorithm is based on linearized Bregman iterations [[2]](#2) and is a simple extension of stochastic gradient descent which is recovered choosing <img src=""https://latex.codecogs.com/svg.latex?J=0"" title=""Jzero""/>. We also provide accelerations of our baseline algorithm using momentum and Adam [[3]](#3).   The variable <img src=""https://latex.codecogs.com/svg.latex?v"" title=""v""/> is a subgradient of <img src=""https://latex.codecogs.com/svg.latex?\theta"" title=""weights""/> with respect to the *elastic net* functional   <p align=""center"">       <img src=""https://latex.codecogs.com/svg.latex?J_\delta(\theta)=J(\theta)+\frac1\delta\|\theta\|^2"" title=""el-net""/> </p>  and stores the information which parameters are non-zero.   """;General;https://github.com/TimRoith/BregmanLearning
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/jessychen1016/yolov3Onpytorch
"""Code repo for winning 2016 MSCOCO Keypoints Challenge  2016 ECCV Best Demo Award  and 2017 CVPR Oral paper.    Watch our video result in [YouTube](https://www.youtube.com/watch?v=pW6nZXeWlGM&t=77s) or [our website](http://posefs1.perception.cs.cmu.edu/Users/ZheCao/humanpose.mp4).   We present a bottom-up approach for realtime multi-person pose estimation  without using any person detector. For more details  refer to our [CVPR'17 paper](https://arxiv.org/abs/1611.08050)  our [oral presentation video recording](https://www.youtube.com/watch?v=OgQLDEAjAZ8&list=PLvsYSxrlO0Cl4J_fgMhj2ElVmGR5UWKpB) at CVPR 2017 or our [presentation slides](http://image-net.org/challenges/talks/2016/Multi-person%20pose%20estimation-CMU.pdf) at ILSVRC and COCO workshop 2016.  <p align=""left""> <img src=""https://github.com/ZheC/Multi-Person-Pose-Estimation/blob/master/readme/dance.gif""  width=""720""> </p>  <p align=""left""> <img src=""https://github.com/ZheC/Multi-Person-Pose-Estimation/blob/master/readme/shake.gif""  width=""720""> </p>  This project is licensed under the terms of the [license](LICENSE).   """;General;https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/ModelBunker/PointNet-TensorFlow
"""[_CrazyAra_](https://crazyara.org/) is an open-source neural network chess variant engine  initially developed in pure python by [Johannes Czech](https://github.com/QueensGambit)  [Moritz Willig](https://github.com/MoritzWillig) and Alena Beyer in 2018. It started as a semester project at the [TU Darmstadt](https://www.tu-darmstadt.de/index.en.jsp) with the goal to train a neural network to play the chess variant [crazyhouse](https://en.wikipedia.org/wiki/Crazyhouse) via supervised learning on human data. The project was part of the course [_""Deep Learning: Architectures & Methods""_](https://piazza.com/tu-darmstadt.de/summer2019/20001034iv/home) held by [Kristian Kersting](https://ml-research.github.io/people/kkersting/index.html)  [Johannes Fürnkranz](http://www.ke.tu-darmstadt.de/staff/juffi) et al. in summer 2018.  The development was continued and the engine ported to C++ by [Johannes Czech](https://github.com/QueensGambit). In the course of a master thesis supervised by [Karl Stelzner](https://ml-research.github.io/people/kstelzner/) and [Kristian Kersting](https://ml-research.github.io/people/kkersting/index.html)  the engine learned crazyhouse in a reinforcement learning setting and was trained on other chess variants including chess960  King of the Hill and Three-Check.  The project is mainly inspired by the techniques described in the [Alpha-(Go)-Zero papers](https://arxiv.org/abs/1712.01815) by [David Silver](https://arxiv.org/search/cs?searchtype=author&query=Silver%2C+D)  [Thomas Hubert](https://arxiv.org/search/cs?searchtype=author&query=Hubert%2C+T)  [Julian Schrittwieser](https://arxiv.org/search/cs?searchtype=author&query=Schrittwieser%2C+J)  [Ioannis Antonoglou](https://arxiv.org/search/cs?searchtype=author&query=Antonoglou%2C+I)  [Matthew Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+M)  [Arthur Guez](https://arxiv.org/search/cs?searchtype=author&query=Guez%2C+A)  [Marc Lanctot](https://arxiv.org/search/cs?searchtype=author&query=Lanctot%2C+M)  [Laurent Sifre](https://arxiv.org/search/cs?searchtype=author&query=Sifre%2C+L)  [Dharshan Kumaran](https://arxiv.org/search/cs?searchtype=author&query=Kumaran%2C+D)  [Thore Graepel](https://arxiv.org/search/cs?searchtype=author&query=Graepel%2C+T)  [Timothy Lillicrap](https://arxiv.org/search/cs?searchtype=author&query=Lillicrap%2C+T)  [Karen Simonyan](https://arxiv.org/search/cs?searchtype=author&query=Simonyan%2C+K)  [Demis Hassabis](https://arxiv.org/search/cs?searchtype=author&query=Hassabis%2C+D).  The training scripts  preprocessing and neural network definition source files are written in python and located at [DeepCrazyhouse/src](https://github.com/QueensGambit/CrazyAra/tree/master/DeepCrazyhouse/src). There are two version of the search engine available: The initial version is written in python and located at [DeepCrazyhouse/src/domain/agent](https://github.com/QueensGambit/CrazyAra/tree/master/DeepCrazyhouse/src/domain/agent). The newer version is written in C++ and located at [engine/src](https://github.com/QueensGambit/CrazyAra/tree/master/engine/src).  _CrazyAra_ is an UCI chess engine and requires a GUI (e.g. [Cute Chess](https://github.com/cutechess/cutechess)  [XBoard](https://www.gnu.org/software/xboard/)  [WinBoard](http://hgm.nubati.net/)) for convinient usage.   """;Reinforcement Learning;https://github.com/QueensGambit/CrazyAra
"""[image1]: https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif ""Trained Agent""  ![Trained Agent][image1]  The RL agent is allowed to traverse across a two dimensional grid with blue and yellow bananas placed across it. The agent is expected to collect the yellow bananas while avoiding the blue ones. The agent receives a positive reward for every yellow banana it collects and a negative reward for every blue banana collected. The size of the state space is 37. The agent is able to move forwards and backwards as well as turn left and right  thus the size of the action space is 4. The minimal expected performance of the agent after training is a score of +13 over 100 consecutive episodes.   """;Reinforcement Learning;https://github.com/prajwalgatti/DRL-Navigation
"""For this project  we will work with the [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment.  In this environment  a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus  the goal of your agent is to maintain its position at the target location for as many time steps as possible.  The observation space consists of 33 variables corresponding to position  rotation  velocity  and angular velocities of the arm. Each action is a vector with four numbers  corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.  The task is episodic  and in order to solve the environment   the agent must get an average score of +30 over 100 consecutive episodes.   """;Reinforcement Learning;https://github.com/biemann/Continuous-Control
"""This lab is based on the XDF 2018  Machine learning for Embedded Workshop. It has been modified to run on the Ultra96 board.  During this session you will gain hands-on experience with the Xlinx DNNDK  and learn how to quantize  compile and deploy pre-trained network models to Xilinx embedded SoC platforms.    """;Computer Vision;https://github.com/jimheaton/Ultra96_ML_Embedded_Workshop
"""This lab is based on the XDF 2018  Machine learning for Embedded Workshop. It has been modified to run on the Ultra96 board.  During this session you will gain hands-on experience with the Xlinx DNNDK  and learn how to quantize  compile and deploy pre-trained network models to Xilinx embedded SoC platforms.    """;General;https://github.com/jimheaton/Ultra96_ML_Embedded_Workshop
"""U-Net is a deep  fully convolutional neural network architecture proposed for biomedical image segmentation. A visual representation of the network  as shown in the original publication [1]  can be found below.  ![Image of U-Net Architecture](images/U-Net.png)   """;Computer Vision;https://github.com/pricebenjamin/unet-estimator
"""A Keras implementation of YOLOv3 (Tensorflow backend) inspired by [allanzelener/YAD2K](https://github.com/allanzelener/YAD2K).   ---   """;Computer Vision;https://github.com/qqwweee/keras-yolo3
"""Quick  Draw! game was originally an experimental game released by Google to teach the general population about artificial intelligence. The game asks the user to draw a picture of an object or idea and uses a neural network artificial intelligence to guess what the sketch represents. Moreover  the network continues to guess as the user draws  even though the sketch is incomplete. The network uses such drawings to learn and increase its accuracy in the future.   """;Computer Vision;https://github.com/abel-leulseged/Quick-Draw
"""For this project  you will work with the [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment.  ![Trained Agent][image1]  In this environment  a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus  the goal of your agent is to maintain its position at the target location for as many time steps as possible.  The observation space consists of 33 variables corresponding to position  rotation  velocity  and angular velocities of the arm. Each action is a vector with four numbers  corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.   """;General;https://github.com/Zartris/TD3_continuous_control
"""For this project  you will work with the [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment.  ![Trained Agent][image1]  In this environment  a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus  the goal of your agent is to maintain its position at the target location for as many time steps as possible.  The observation space consists of 33 variables corresponding to position  rotation  velocity  and angular velocities of the arm. Each action is a vector with four numbers  corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.   """;Reinforcement Learning;https://github.com/Zartris/TD3_continuous_control
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   """;Computer Vision;https://github.com/qilei123/fpn_crop_v1_5d
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO** (https://pjreddie.com/darknet/yolo/) and to **Erik Lindernoren for the PyTorch implementation** this work is based on (https://github.com/eriklindernoren/PyTorch-YOLOv3).   This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information on Ultralytics projects please visit: https://www.ultralytics.com.   """;Computer Vision;https://github.com/thecryboy/DECONV
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/MichaelZhouwang/LMlexsub
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/abdullahozer11/Segmentation-and-Classification-of-Objects-in-Point-Clouds
"""This project is dedicated to unsupervised methods of text data transformation. The representation of text in such transformed form is inherent part of Natural Language Processing (NLP) and allows to consume unstructured data by various deep learning algorithms.  If NLP is a new term for you and sounds a bit mysterious  Yoav Goldberg in his [book](https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984) describes it as:  >_Natural language processing (NLP) is a collective term referring to automatic computational processing of human languages. This includes both algorithms that take human-produced text as input  and algorithms that produce natural looking text as outputs._  NLP embraces wide range of tasks e.g. named entity recognition  sentiment analysis  automatic summarization  natural language generation and many more. Each one is a subject for separate project and could be thoroughly studied.    Two tested methods in this project are **Unsupervised Word Segmentation into Subword Units** and **GloVe embeddings trained on our own corpus**.  I intentionally choose Polish language text to be analyzed  what I elaborate below  but be aware that the methods used are applicable to any language and text data.    """;Natural Language Processing;https://github.com/ksulima/Unsupervised-method-to-NPL-Polish-language
"""DeepLab is a state-of-art deep learning system for semantic image segmentation built on top of [Caffe](http://caffe.berkeleyvision.org).  It combines (1) *atrous convolution* to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks  (2) *atrous spatial pyramid pooling* to robustly segment objects at multiple scales with filters at multiple sampling rates and effective fields-of-views  and (3) densely connected conditional random fields (CRF) as post processing.  This distribution provides a publicly available implementation for the key model ingredients reported in our latest [arXiv paper](http://arxiv.org/abs/1606.00915). This version also supports the experiments (DeepLab v1) in our ICLR'15. You only need to modify the old prototxt files. For example  our proposed atrous convolution is called dilated convolution in CAFFE framework  and you need to change the convolution parameter ""hole"" to ""dilation"" (the usage is exactly the same). For the experiments in ICCV'15  there are some differences between our argmax and softmax_loss layers and Caffe's. Please refer to [DeepLabv1](https://bitbucket.org/deeplab/deeplab-public/) for details.  Please consult and consider citing the following papers:      @article{CP2016Deeplab        title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets  Atrous Convolution  and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        journal={arXiv:1606.00915}        year={2016}     }      @inproceedings{CY2016Attention        title={Attention to Scale: Scale-aware Semantic Image Segmentation}        author={Liang-Chieh Chen and Yi Yang and Jiang Wang and Wei Xu and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{CB2016Semantic        title={Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform}        author={Liang-Chieh Chen and Jonathan T Barron and George Papandreou and Kevin Murphy and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{PC2015Weak        title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation}        author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille}        booktitle={ICCV}        year={2015}     }      @inproceedings{CP2015Semantic        title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        booktitle={ICLR}        year={2015}     }   Note that if you use the densecrf implementation  please consult and cite the following paper:      @inproceedings{KrahenbuhlK11        title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}        author={Philipp Kr{\""{a}}henb{\""{u}}hl and Vladlen Koltun}        booktitle={NIPS}        year={2011}     }   """;Computer Vision;https://github.com/z01nl1o02/deeplab-v2
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/rikichou/face_recognition_insight_face
"""This project is to study the use of Convolutional Neural Network and in particular the ResNet architecture. The show case is segmentation of Magnetic Resonance Images (MRI) of human brain into anatomical regions[2]. We will extend the ResNet topology into the processing of 3-dimensional voxels.    """;General;https://github.com/bclwan/MRI_Brain_Segmentation
"""This project is to study the use of Convolutional Neural Network and in particular the ResNet architecture. The show case is segmentation of Magnetic Resonance Images (MRI) of human brain into anatomical regions[2]. We will extend the ResNet topology into the processing of 3-dimensional voxels.    """;Computer Vision;https://github.com/bclwan/MRI_Brain_Segmentation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/chrisseiler96/bert-client-server-tests
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/chrisseiler96/bert-client-server-tests
"""This repo contains text detection based on [Receptive Field Blocks](https://arxiv.org/abs/1711.07767). The text detection model provides a dense receptive field  for predicting text boxes in dense natural scene images like documents  articles etc.  The model is also inspired from [EAST: An Efficient and Accurate Scene Text Detector](https://arxiv.org/abs/1704.03155v2)  where the **RRBOX** part and loss function is taken from.  The features of the model are summarized below: + Keras implementation for lucid and clean code. + Backbone: **Resnet50** + Inference time for 720p images:      **GPU VERSION**     + Graphics Card: MX130     + Inference Time: 700ms     + Batch Size: 1         **CPU VERSION**     + CPU: Intel i7-8550U CPU @ 1.80GHz     + Inference Time: 1750ms     + Batch Size: 1  + The pre-trained model provided achieves **47.09**(Single Crop  Resize Only) F1-score on ICDAR 2015  but was not trained on ICDAR 2015. To improve accuracy fine-tune on ICDAR 2015  and predict with multiple crops.  + The model is tuned for predicting text boxes for natural scene documents  like bank statements  forms  recipts  etc  and evidently do OCR on these text boxes.   Please cite these [paper](https://arxiv.org/abs/1711.07767)  [paper](https://arxiv.org/abs/1704.03155v2) if you find this useful.   """;Computer Vision;https://github.com/Chris10M/RFB-Text-Detection
"""**train_RL.py**   Contains class *FishEnv*  which defines the environment of the game.   Contains class *Actor*  *ActorTarget*  *Critic*  *CriticTarget*  which defines the Actor and Critic in the model.   Contains method *train*  which is the main method of training.    **replay_buffer.py**   A buffer used to store state  action  reward at each timestep during the simulation  being sampled in the later stage to train the neural network in reinforcement learning.    **neural_network_share_weight.py**   Contains neural network definition.    **RL_Training_Asocial.py**   Contains definition of reward function.  """;Reinforcement Learning;https://github.com/fhbzc/FishAgentSimulation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/nicholasbao/nlp_job
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/nicholasbao/nlp_job
"""vedastr is an open source scene text recognition toolbox based on PyTorch. It is designed to be flexible in order to support rapid implementation and evaluation for scene text recognition task.     """;Computer Vision;https://github.com/Media-Smart/vedastr
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/faizansuhail89/bert
"""In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   """;General;https://github.com/yitu-opensource/ConvBert
"""In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   """;Natural Language Processing;https://github.com/yitu-opensource/ConvBert
"""In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   """;Sequential;https://github.com/yitu-opensource/ConvBert
"""Models can be trained with RandAugment for the dataset of interest with no need for a separate proxy task. By only tuning two hyperparameters(N  M)  you can achieve competitive performances as AutoAugments.   """;Computer Vision;https://github.com/ildoonet/pytorch-randaugment
"""0. [Requirements](#requirements) 0. [Usage](#usage) 0. [Example](#example) 0. [Customization](#customization) 0. [Citation](#citation)  <!---  """;General;https://github.com/denru01/netadapt
"""This toolkit is for developing the Natural Language Processing (NLP) pipelines  including model design and development  model deployment and distributed computing.   """;Natural Language Processing;https://github.com/Impavidity/relogic
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Maz101/Bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/pingheng001/Cnn-Bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/pingheng001/Cnn-Bert
"""DeepLab is a state-of-art deep learning system for semantic image segmentation built on top of [Caffe](http://caffe.berkeleyvision.org).  It combines (1) *atrous convolution* to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks  (2) *atrous spatial pyramid pooling* to robustly segment objects at multiple scales with filters at multiple sampling rates and effective fields-of-views  and (3) densely connected conditional random fields (CRF) as post processing.  This distribution provides a publicly available implementation for the key model ingredients reported in our latest [arXiv paper](http://arxiv.org/abs/1606.00915). This version also supports the experiments (DeepLab v1) in our ICLR'15. You only need to modify the old prototxt files. For example  our proposed atrous convolution is called dilated convolution in CAFFE framework  and you need to change the convolution parameter ""hole"" to ""dilation"" (the usage is exactly the same). For the experiments in ICCV'15  there are some differences between our argmax and softmax_loss layers and Caffe's. Please refer to [DeepLabv1](https://bitbucket.org/deeplab/deeplab-public/) for details.  Please consult and consider citing the following papers:      @article{CP2016Deeplab        title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets  Atrous Convolution  and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        journal={arXiv:1606.00915}        year={2016}     }      @inproceedings{CY2016Attention        title={Attention to Scale: Scale-aware Semantic Image Segmentation}        author={Liang-Chieh Chen and Yi Yang and Jiang Wang and Wei Xu and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{CB2016Semantic        title={Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform}        author={Liang-Chieh Chen and Jonathan T Barron and George Papandreou and Kevin Murphy and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{PC2015Weak        title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation}        author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille}        booktitle={ICCV}        year={2015}     }      @inproceedings{CP2015Semantic        title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        booktitle={ICLR}        year={2015}     }   Note that if you use the densecrf implementation  please consult and cite the following paper:      @inproceedings{KrahenbuhlK11        title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}        author={Philipp Kr{\""{a}}henb{\""{u}}hl and Vladlen Koltun}        booktitle={NIPS}        year={2011}     }   """;Computer Vision;https://github.com/cdmh/deeplab-public-ver2
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/Mersive-Technologies/yolov3
"""This project contains the code of HRNet-FCOS  i.e.  using [High-resolution Networks (HRNets)](https://arxiv.org/pdf/1904.04514.pdf) as the backbones for the [Fully Convolutional One-Stage Object Detection (FCOS)](https://arxiv.org/abs/1904.01355) algorithm  which achieves much better object detection performance compared with the ResNet-FCOS counterparts while keeping a similar computation complexity. For more projects using HRNets  please go to our [website](https://github.com/HRNet).   """;Computer Vision;https://github.com/HRNet/HRNet-FCOS
"""  [MegaFace](http://megaface.cs.washington.edu/) dataset includes 1 027 060 faces  690 572 identities.   Challenge 1 is taken to test our model with 1 million distractors.   ![image](https://github.com/foamliu/InsightFace-v2/raw/master/images/megaface_stats.png)    MS-Celeb-1M dataset for training  3 804 846 faces over 85 164 identities.    """;General;https://github.com/foamliu/InsightFace-PyTorch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/appcoreopc/berty
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/appcoreopc/berty
"""This repository is for implementation of the paper [Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles](https://arxiv.org/abs/1612.01474). This algorithm quantifies predictive predictive uncertainty in non-Bayesian NN with `Deep Ensemble Model`.      **Contribution** of this paper is that it describes `simple` and `scalable` method for `estimating predictive uncertainty estimates` from NN.    This paper uses 3 things for training  - Proper Scoring Rules - Adversarial Training to smooth predictive distributions - Deep Ensembles  This repository implemented without Adversarial training    Implementation and Results are as follows.     """;General;https://github.com/Kyushik/Predictive-Uncertainty-Estimation-using-Deep-Ensemble
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/faizansuhail89/bert
"""The implementation is based on two papers:  - Simple Online and Realtime Tracking with a Deep Association Metric https://arxiv.org/abs/1703.07402 - YOLOv4: Optimal Speed and Accuracy of Object Detection https://arxiv.org/pdf/2004.10934.pdf  ＃＃ 要求  Python 3.8 or later with all requirements.txt dependencies installed  including torch>=1.7. To install run:  `pip install -U -r requirements.txt`  All dependencies are included in the associated docker images. Docker requirements are:  - `nvidia-docker` - Nvidia Driver Version >= 440.44   This repository contains a moded version of PyTorch YOLOv5 (https://github.com/ultralytics/yolov5). It filters out every detection that is not a person. s. The reason behind the fact that it just tracks persons is that the deep association metric is trained on a person ONLY datatset.YOLO（https://github.com/ultralytics）   """;Computer Vision;https://github.com/oaqoe-DWQ/Yolov5_DeepSort_Pytorch
"""The authors of this paper presents a novel approach in producing pixel-wise categorical segmentations using the very common encoder-decoder architecture. The concept of the encoder-decoder architecture (a.k.a. autoencoder) is such that the encoder block breaks down the input data by sequentially and repeatedly converting it into a higher-dimensional representation from the previous layer while trading-off size. At the end of the encoder  the highest-dimensional representation is then fed into the decoder  which performs the same process  except in reverse. The high-dimensional  small-sized output of the encoder is sequentially and repeatedly reduced to lower-dimensions and upscaled to the original input size  with a desired semantic form of output.   <p align='center'>   <img width=""800"" alt=""segnet architecture"" src=""https://user-images.githubusercontent.com/19466657/120553062-0df47e80-c3c6-11eb-9355-cd0f5d449752.png"">   <br/>   Image taken from: https://arxiv.org/pdf/1511.00561.pdf. The autoencoder architecture of SegNet. </p>  In the case of SegNet  the input is images of road scenes in RGB format (3-channel)  and the output is a 32-channel one-hot encoded image of pixels (C  X  Y)  where C is the corresponding (1 of 32) predicted categories of the pixels  and X  Y are pixel coordinates. The novelty in their approach stems from the issue that spatial information is always lost in an image-autoencoder network during downsampling in the encoder (via maxpooling). To mitigate that  they propose keeping the indices (i.e. pixel-coordinates) where maxpooling is done at each layer  so that spatial information can be restored locally during upsampling in the decoder.    """;Computer Vision;https://github.com/vinceecws/SegNet_PyTorch
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/CV-deeplearning/caffe
"""In this work  we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN including only the most essential component in GCN—neighborhood aggregation—for collaborative filtering     """;Graphs;https://github.com/gusye1234/LightGCN-PyTorch
"""This project aims to detect emotions on faces. First it detects faces with haarcascade classifier or with MTCNN model  then through a convolutionnal network it classifies the emotion from seven categories {'angry'  'disgust'  'fear'  'happy'  'sad'  'surprise'  'neutral'}. To train the model we had two choices of dataset : FER2013 and CK+48. In this version  the model is trained with FER2013 dataset and it uses MTCNN model to detect faces.    """;Computer Vision;https://github.com/oellop/facial_expression
"""One-stage detector basically formulates object detection as dense classification and localization (i.e.  bounding box regression). The classification is usually optimized by Focal Loss and the box location is commonly learned under Dirac delta distribution. A recent trend for one-stage detectors is to introduce an \emph{individual} prediction branch to estimate the quality of localization  where the predicted quality facilitates the classification to improve detection performance. This paper delves into the \emph{representations} of the above three fundamental elements: quality estimation  classification and localization. Two problems are discovered in existing practices  including (1) the inconsistent usage of the quality estimation and classification between training and inference (i.e.  separately trained but compositely used in test) and (2) the inflexible Dirac delta distribution for localization when there is ambiguity and uncertainty which is often the case in complex scenes. To address the problems  we design new representations for these elements. Specifically  we merge the quality estimation into the class prediction vector to form a joint representation of localization quality and classification  and use a vector to represent arbitrary distribution of box locations. The improved representations eliminate the inconsistency risk and accurately depict the flexible distribution in real data  but contain \emph{continuous} labels  which is beyond the scope of Focal Loss. We then propose Generalized Focal Loss (GFL) that generalizes Focal Loss from its discrete form to the \emph{continuous} version for successful optimization. On COCO {\tt test-dev}  GFL achieves 45.0\% AP using ResNet-101 backbone  surpassing state-of-the-art SAPD (43.5\%) and ATSS (43.6\%) with higher or comparable inference speed  under the same backbone and training settings. Notably  our best model can achieve a single-model single-scale AP of 48.2\%  at 10 FPS on a single 2080Ti GPU.  <img src=""https://github.com/implus/GFocal/blob/master/gfocal.png"" width=""1000"" height=""300"" align=""middle""/>  For details see [GFocal](https://arxiv.org/pdf/2006.04388.pdf). The speed-accuracy trade-off is as follows:  <img src=""https://github.com/implus/GFocal/blob/master/sota_time_acc.jpg"" width=""541"" height=""365"" align=""middle""/>    """;General;https://github.com/implus/GFocal
"""Training GAN is hard. Models may never converge and mode collapses are common. <br>  When learning generative models  we assume the data we have comes from some unknown distribution <img src='./readme_images/pr.png' />. (The r stands for real) We want to learn a distribution <img src='./readme_images/ptheta.png' />​​ that approximates <img src='./readme_images/pr.png' />  where θ are the parameters of the distribution. <br> You can imagine two approaches for doing this. <br> - Directly learn the probability density function <img src='./readme_images/ptheta.png' />​​. We optimize <img src='./readme_images/ptheta.png' />​​ through maximum likelihood estimation.  - Learn a function that transforms an existing distribution Z into <img src='./readme_images/ptheta.png' />​​.   The first approach runs into problems. Given function <img src='./readme_images/ptheta.png' />​​​​  the MLE objective is <br> <img src='./readme_images/eqn13.png' />​​ <br> In the limit  this is equivalent to minimizing the KL-divergence. <br> <img src='./readme_images/eqn14.png' />​​ <br> <img src='./readme_images/eqn15.png' />​​ <br>  Variational Auto-Encoders (VAEs) and Generative Adversarial Networks (GANs) are well known examples of this approach.   """;Computer Vision;https://github.com/Mohammad-Rahmdel/WassersteinGAN-Tensorflow
"""This edition of yolov4 frame has been smoothly transferred from tf1+ and keras version. With tf2+ used extensively  many older functions have been optimized even won't be used any more. Keras has been embedded into tensorflow and won't be supported by official  except fixing bugs. For more readable  I've made a lot of changes while transferring  such as  layer aggregation  image augment and some other changes from tf1 to tf2.       【Now  IT ONLY SUPPORTS TF2+】     """;Computer Vision;https://github.com/robbebluecp/tf2-yolov4
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Katsumata420/bert_mlm
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/Katsumata420/bert_mlm
"""The authors present a novel approach to incorporate language information into extracting visual features by conditioning the Batch Normalization parameters on the language. They apply Conditional Batch Normalization (CBN) to a pre-trained ResNet and show that this significantly improves performance on visual question answering tasks. </br>   """;Computer Vision;https://github.com/ap229997/Conditional-Batch-Norm
"""The authors present a novel approach to incorporate language information into extracting visual features by conditioning the Batch Normalization parameters on the language. They apply Conditional Batch Normalization (CBN) to a pre-trained ResNet and show that this significantly improves performance on visual question answering tasks. </br>   """;General;https://github.com/ap229997/Conditional-Batch-Norm
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/leejang/two_stream_ssd_caffe
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/tolotrasamuel/real-time-object-detection-retraining-ncs-raspberry-pi
""" ![mask_visualization](./images/mask_visualization.gif)  Feature warping is a core technique in optical flow estimation; however  the ambiguity caused by occluded areas during warping is a major problem that remains unsolved. We propose an asymmetric occlusion-aware feature matching module  which can learn a rough occlusion mask that filters useless (occluded) areas immediately after feature warping without any explicit supervision. The proposed module can be easily integrated into end-to-end network architectures and enjoys performance gains while introducing negligible computational cost. The learned occlusion mask can be further fed into a subsequent network cascade with dual feature pyramids with which we achieve state-of-the-art performance. For more details  please refer to our [paper](https://arxiv.org/pdf/2003.10955.pdf).  This repository includes:  - Training and inferring scripts using Python and MXNet; and - Pretrained models of *MaskFlownet-S* and *MaskFlownet*.  Code has been tested with Python 3.6 and MXNet 1.5.   """;General;https://github.com/microsoft/MaskFlownet
"""Recently  channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neuralnetworks (CNNs). However  most existing methods dedicate to developing more sophisticated attention modules for achieving better performance which inevitably increase model complexity. To overcome the paradox of performance and complexity trade-off  this paper proposes an EfficientChannel Attention (ECA) module  which only involves a handful of parameters while bringing clear performance gain. By dissecting the channelattention module in SENet  we empirically show avoiding dimensionality reduction is important for learning channel attention  and appropriatecross-channel interaction can preserve performance while significantly decreasing model complexity. Therefore  we propose a localcross-channel interaction strategy without dimensionality reduction  which can be efficiently implemented via `1D` convolution. Furthermore we develop a method to adaptively select kernel size of `1D` convolution  determining coverage of local cross-channel interaction. Theproposed ECA module is efficient yet effective  e.g.  the parameters and computations of our modules against backbone of ResNet50 are 80 vs.24.37M and 4.7e-4 GFLOPs vs. 3.86 GFLOPs  respectively  and the performance boost is more than 2\% in terms of Top-1 accuracy. We extensivelyevaluate our ECA module on image classification  object detection and instance segmentation with backbones of ResNets and MobileNetV2. Theexperimental results show our module is more efficient while performing favorably against its counterparts.   """;General;https://github.com/BangguWu/ECANet
"""Recently  channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neuralnetworks (CNNs). However  most existing methods dedicate to developing more sophisticated attention modules for achieving better performance which inevitably increase model complexity. To overcome the paradox of performance and complexity trade-off  this paper proposes an EfficientChannel Attention (ECA) module  which only involves a handful of parameters while bringing clear performance gain. By dissecting the channelattention module in SENet  we empirically show avoiding dimensionality reduction is important for learning channel attention  and appropriatecross-channel interaction can preserve performance while significantly decreasing model complexity. Therefore  we propose a localcross-channel interaction strategy without dimensionality reduction  which can be efficiently implemented via `1D` convolution. Furthermore we develop a method to adaptively select kernel size of `1D` convolution  determining coverage of local cross-channel interaction. Theproposed ECA module is efficient yet effective  e.g.  the parameters and computations of our modules against backbone of ResNet50 are 80 vs.24.37M and 4.7e-4 GFLOPs vs. 3.86 GFLOPs  respectively  and the performance boost is more than 2\% in terms of Top-1 accuracy. We extensivelyevaluate our ECA module on image classification  object detection and instance segmentation with backbones of ResNets and MobileNetV2. Theexperimental results show our module is more efficient while performing favorably against its counterparts.   """;Computer Vision;https://github.com/BangguWu/ECANet
"""This repository is modified from mikel-brostrom/Yolov5_DeepSort_Pytorch (https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch). I fixed some bugs and extend it to multi-class version.It contains YOLOv5 (https://github.com/ultralytics/yolov5) and Deep Sort (https://github.com/ZQPei/deep_sort_pytorch). The deep sort model in this repository was only trained by pedestrians.   """;Computer Vision;https://github.com/WuPedin/Multi-class_Yolov5_DeepSort_Pytorch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/Maz101/Bert
"""This repository contains the code for fine-tuning a CLIP model [[Arxiv paper](https://arxiv.org/abs/2103.00020)][[OpenAI Github Repo](https://github.com/openai/CLIP)] on the [ROCO dataset](https://github.com/razorx89/roco-dataset)  a dataset made of radiology images and a caption. This work is done as a part of the [**Flax/Jax community week**](https://github.com/huggingface/transformers/blob/master/examples/research_projects/jax-projects/README.md#quickstart-flax-and-jax-in-transformers) organized by Hugging Face and Google.  [[🤗 Model card]](https://huggingface.co/flax-community/medclip-roco) [[Streamlit demo]](https://huggingface.co/spaces/kaushalya/medclip-roco)   """;Computer Vision;https://github.com/Kaushalya/medclip
"""---  Here I have taken a flower classification dataset from http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html. After some preprocessing of images I took InceptionNet as my model and fine tuned it's last 50 layers and freezed all the first 50 but batch normalization layers.   """;Computer Vision;https://github.com/bhuyanamit986/FlowerClassification
"""---  Here I have taken a flower classification dataset from http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html. After some preprocessing of images I took InceptionNet as my model and fine tuned it's last 50 layers and freezed all the first 50 but batch normalization layers.   """;General;https://github.com/bhuyanamit986/FlowerClassification
"""This project implements the Spectral Clustering algorithm presented in the following paper: https://arxiv.org/abs/0711.0189. Refer to 18.pdf for detailed information and analysis.  ![6 clusters](output/validation/6_c.png)       <a name=""Installation""></a>  """;General;https://github.com/devpouya/FastSpectralClustering
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;Natural Language Processing;https://github.com/chalothon/BERT_Practice
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/chalothon/BERT_Practice
"""This application is an implementation of People tracking using Tensorflow implementation of MobileNet V3 + SSD[1] and DeepSort[2]. This could very well be extended to any other objects for tracking like cars or animals.   """;Computer Vision;https://github.com/kshitij1489/object-tracking
"""This application is an implementation of People tracking using Tensorflow implementation of MobileNet V3 + SSD[1] and DeepSort[2]. This could very well be extended to any other objects for tracking like cars or animals.   """;General;https://github.com/kshitij1489/object-tracking
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/KiranAkadas/My_Pointnet_v2
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   """;Computer Vision;https://github.com/WLK12580/12
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   """;Computer Vision;https://github.com/wrappr/wrappr-core
""" SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   """;Computer Vision;https://github.com/lswzjuer/caffe-adas
"""SWA is a simple DNN training method that can be used as a drop-in replacement for SGD with improved generalization  faster convergence  and essentially no overhead. The key idea of SWA is to average multiple samples produced by SGD with a modified learning rate schedule. We use a constant or cyclical learning rate schedule that causes SGD to _explore_ the set of points in the weight space corresponding to high-performing networks. We observe that SWA converges more quickly than SGD  and to wider optima that provide higher test accuracy.   In this repo we implement the constant learning rate schedule that we found to be most practical on CIFAR datasets.  <p align=""center"">   <img src=""https://user-images.githubusercontent.com/14368801/37633888-89fdc05a-2bca-11e8-88aa-dd3661a44c3f.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633885-89d809a0-2bca-11e8-8d57-3bd78734cea3.png"" width=250>   <img src=""https://user-images.githubusercontent.com/14368801/37633887-89e93784-2bca-11e8-9d71-a385ea72ff7c.png"" width=250> </p>  Please cite our work if you find this approach useful in your research: ```latex @article{izmailov2018averaging    title={Averaging Weights Leads to Wider Optima and Better Generalization}    author={Izmailov  Pavel and Podoprikhin  Dmitrii and Garipov  Timur and Vetrov  Dmitry and Wilson  Andrew Gordon}    journal={arXiv preprint arXiv:1803.05407}    year={2018} } ```    """;General;https://github.com/julianfaraone/SWA
"""`Multi-digit MNIST` generator creates datasets consisting of handwritten digit images from [MNIST](http://yann.lecun.com/exdb/mnist/) for few-shot image classification and meta-learning. It simply samples images from MNIST dataset and put digits together to create images with multiple digits. It also creates training/validation/testing splits (64/20/16 classes for DoubleMNIST and 640/200/160 for TripleMNIST).   You can generate customized by following the cammands provided in [Usage](https://github.com/shaohua0116/MultiDigitMNIST#usage) to change the number of images in each class  the image size  etc. You can also download generated datasets from [Datasets](https://github.com/shaohua0116/MultiDigitMNIST#datasets).  This repository benchmarks the performance of **MAML** ([Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400)) using datasets created via the generation script in a variety of settings.  Some examples of images from the datasets are as follows.   - Double MNIST Datasets (100 classes: `00` to `99`)  | **Class** | 10 | 48 | 59 | 62 | 73 | | :---: | :---: | :---: | :---: | :---: | :---: | | **Image** | ![](./asset/examples/10/0_10.png) ![](./asset/examples/10/1_10.png) ![](./asset/examples/10/2_10.png) ![](./asset/examples/10/3_10.png) | ![](./asset/examples/48/0_48.png) ![](./asset/examples/48/1_48.png) ![](./asset/examples/48/2_48.png) ![](./asset/examples/48/3_48.png)| ![](./asset/examples/59/0_59.png) ![](./asset/examples/59/1_59.png) ![](./asset/examples/59/2_59.png) ![](./asset/examples/59/3_59.png) | ![](./asset/examples/62/0_62.png) ![](./asset/examples/62/1_62.png) ![](./asset/examples/62/2_62.png) ![](./asset/examples/62/3_62.png) | ![](./asset/examples/73/0_73.png) ![](./asset/examples/73/1_73.png) ![](./asset/examples/73/2_73.png) ![](./asset/examples/73/3_73.png) |   - Triple MNIST Datasets (1000 classes: `000` to `999`)  | **Class** | 039 | 146 | 258 | 512 | 874 | | :---: | :---: | :---: | :---: | :---: | :---: | | **Image** | ![](./asset/examples/039/0_039.png) ![](./asset/examples/039/1_039.png) ![](./asset/examples/039/2_039.png) ![](./asset/examples/039/3_039.png) | ![](./asset/examples/146/0_146.png) ![](./asset/examples/146/1_146.png) ![](./asset/examples/146/2_146.png) ![](./asset/examples/146/3_146.png)| ![](./asset/examples/258/0_258.png) ![](./asset/examples/258/1_258.png) ![](./asset/examples/258/2_258.png) ![](./asset/examples/258/3_258.png) | ![](./asset/examples/512/0_512.png) ![](./asset/examples/512/1_512.png) ![](./asset/examples/512/2_512.png) ![](./asset/examples/512/3_512.png) | ![](./asset/examples/874/0_874.png) ![](./asset/examples/874/1_874.png) ![](./asset/examples/874/2_874.png) ![](./asset/examples/874/3_874.png) |    """;General;https://github.com/shaohua0116/MultiDigitMNIST
"""Adversarial Model Perturbation (AMP) effectively improves the generalization performance of deep models by minimizing an ""AMP loss"" that can find flat local minima via applying a ""worst"" norm-bounded perturbation on the model parameter.  ![method](assets/method.jpg)   """;General;https://github.com/hiyouga/AMP-Regularizer
"""bert and multi_cased_L-12_H-768_A-12 should be extracted this directory.   ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ multi_cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip) |____ data		            #: train data (EnglishBERTdata3Labels  TurkishNERdata3Labels  TurkishNERdata7Labels) |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ run_ner.sh    		    #: run model and eval result  ```   """;Natural Language Processing;https://github.com/teghub/TurkishNER-BERT
"""* The Vehicle Routing Problem is a combinatorial optimization problem which asks ""What is the optimal set of routes for a fleet of vehicles to traverse in order to deliver to a given set of customers?“ * Capacitated Vehicle Routing Problem (CVRP) is a variant of the Vehicle Routing Problem in which the vehicles have a limited capacity of the goods. * This repository leverages Deep Reinforcement Learning to solve the CVRP problem.     """;Reinforcement Learning;https://github.com/theresearchai/vehicle_routing_rl_2
"""1. [requirements.txt](https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/blob/main/requirements.txt) - Includes all the required libraries for the Conda Environment. 2. [model.py](https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/blob/main/model.py) - Defines the actor and critic networks. 3. [ddpg_agent.py](https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/blob/main/ddpg_agent.py) -  Defines the Agent that uses DDPG to determine the best action to take and maximizes the overall or total reward. 4. [Continuous_Control.ipynb](https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/blob/main/Continuous_Control.ipynb) - The main file that trains the actor and critic networks. This file can be run in the Conda environment.   """;Reinforcement Learning;https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient
"""English | [简体中文](README_zh-CN.md)  MMDetection is an open source object detection toolbox based on PyTorch. It is a part of the [OpenMMLab](https://openmmlab.com/) project.  The master branch works with **PyTorch 1.3+**. The old v1.x branch works with PyTorch 1.1 to 1.4  but v2.0 is strongly recommended for faster speed  higher performance  better design and more friendly usage.  ![demo image](resources/coco_test_12510.jpg)   """;Computer Vision;https://github.com/shinya7y/UniverseNet
"""Convolutional Neural Networks are similar to ordinary Neural Networks (multi-layer perceptrons). Each neuron receive an input  perform a dot product with its weights and follow this with a non-linearity (here we only use ReLu). The whole network has a loss function that is here the Root Mean Square (RMS) error (details later). The network implements the 'rectangle method'. From the input sequence we invert for the start time  the end time and the average power of only one appliance   ![](./figures/convnet_architecture.png)   Convolutional neural networks have revolutionized computer vision. From an image the convolutional layer learns through its weights low level features. In the case of an image the features detectors (filters) would be: horizontal lines  blobs etc. These filters are built using a small receptive field and share weights across the entire input  which makes them translation invariant. Similarly  in the case of time series  the filters extract low level feature in the time series. By experimenting we found that only using 16 of these filters gives a good predictive power to the ConvNet. This convolutional layer is then flatten and we use 2 hidden layers of 1024 and 512 neurons with ReLu activation function before the output layer of 3 neurons (start time  end time and average power).     """;General;https://github.com/joseluis1061/neuralnilm
"""With recent advancements in machine learning techniques  researchers have demonstrated remarkable achievements in image synthesis (BigGAN  StyleGAN)  textual understanding (GPT-3)  and other areas of text and image manipulation. This hands-on workshop introduces state-of-the-art techniques for text-to-image translation  where textual prompts are used to guide the generation of visual imagery. Participants will gain experience with Open AI's CLIP network and Google's BigGAN  using free Google Colab notebooks which they can apply to their own work after the event. We will discuss other relationships between text and image in art and literature; consider the strengths and limitations of these new techniques; and relate these computational processes to human language  perception  and visual expression and imagination. __Please bring a text you would like to experiment with!__   """;Computer Vision;https://github.com/roberttwomey/machine-imagination-workshop
"""With recent advancements in machine learning techniques  researchers have demonstrated remarkable achievements in image synthesis (BigGAN  StyleGAN)  textual understanding (GPT-3)  and other areas of text and image manipulation. This hands-on workshop introduces state-of-the-art techniques for text-to-image translation  where textual prompts are used to guide the generation of visual imagery. Participants will gain experience with Open AI's CLIP network and Google's BigGAN  using free Google Colab notebooks which they can apply to their own work after the event. We will discuss other relationships between text and image in art and literature; consider the strengths and limitations of these new techniques; and relate these computational processes to human language  perception  and visual expression and imagination. __Please bring a text you would like to experiment with!__   """;Natural Language Processing;https://github.com/roberttwomey/machine-imagination-workshop
"""With recent advancements in machine learning techniques  researchers have demonstrated remarkable achievements in image synthesis (BigGAN  StyleGAN)  textual understanding (GPT-3)  and other areas of text and image manipulation. This hands-on workshop introduces state-of-the-art techniques for text-to-image translation  where textual prompts are used to guide the generation of visual imagery. Participants will gain experience with Open AI's CLIP network and Google's BigGAN  using free Google Colab notebooks which they can apply to their own work after the event. We will discuss other relationships between text and image in art and literature; consider the strengths and limitations of these new techniques; and relate these computational processes to human language  perception  and visual expression and imagination. __Please bring a text you would like to experiment with!__   """;General;https://github.com/roberttwomey/machine-imagination-workshop
"""**You can see trained agent in action [here](https://www.youtube.com/watch?v=kldATbEf1zE)**  A reward of +0.1 is provided for holding the ball at each step. The maximum reward is 40 points.    The state space has 33 dimensions and contains the position  rotation  velocity  and angular velocities of the arm.  The action has 4 dimension in range -1 to 1. And describe the tourge to each part of arm.  The task is episodic  and in order to solve the environment  your agent must get an average *score of +30 over 100* consecutive episodes.   This project  describes the reinforcement learning to resolve the continues control problem.  The problem is describe with continues state space and continues action space. The goal is to hold the ball with moving arm :)  I used TD3 algorithm which is extension of Deep Deterministic Gradient Policy method. I include my private extension of local exploration. more details in Report.pdf and https://arxiv.org/pdf/1802.09477.pdf  The enviroment comes from Unity  please read the Unity Environment  before making a copy and trying yourself!   """;General;https://github.com/pkasala/ContinuesControl
"""**You can see trained agent in action [here](https://www.youtube.com/watch?v=kldATbEf1zE)**  A reward of +0.1 is provided for holding the ball at each step. The maximum reward is 40 points.    The state space has 33 dimensions and contains the position  rotation  velocity  and angular velocities of the arm.  The action has 4 dimension in range -1 to 1. And describe the tourge to each part of arm.  The task is episodic  and in order to solve the environment  your agent must get an average *score of +30 over 100* consecutive episodes.   This project  describes the reinforcement learning to resolve the continues control problem.  The problem is describe with continues state space and continues action space. The goal is to hold the ball with moving arm :)  I used TD3 algorithm which is extension of Deep Deterministic Gradient Policy method. I include my private extension of local exploration. more details in Report.pdf and https://arxiv.org/pdf/1802.09477.pdf  The enviroment comes from Unity  please read the Unity Environment  before making a copy and trying yourself!   """;Reinforcement Learning;https://github.com/pkasala/ContinuesControl
"""Open-CyKG is a framework that is constructed using an attention-based neural Open Information Extraction (OIE) model to extract valuable cyber threat information from unstructured Advanced Persistent Threat (APT) reports. More specifically  we first identify relevant entities by developing a neural cybersecurity Named Entity Recognizer (NER) that aids in labeling relation triples generated by the OIE model. Afterwards  the extracted structured data is canonicalized to build the KG by employing fusion techniques using word embeddings.  <p align=""center"">     <img src=""https://github.com/IS5882/Open-CyKG/blob/main/Open%20CyKg%20images-Framework.png"" width=""550"" title=""Open-CyKG Framework"">   </p>     """;General;https://github.com/IS5882/Open-CyKG
"""YOLOX is an anchor-free version of YOLO  with a simpler design but better performance! It aims to bridge the gap between research and industrial communities. For more details  please refer to our [report on Arxiv](https://arxiv.org/abs/2107.08430).  This repo is an implementation of PyTorch version YOLOX  there is also a [MegEngine implementation](https://github.com/MegEngine/YOLOX).  <img src=""assets/git_fig.png"" width=""1000"" >   """;Computer Vision;https://github.com/StephenStorm/YOLOX
"""1. [Installation](#installation) 2. [Data preprocessing](#data-preprocessing) 3. [3 models](#create-separate-models-for-age-mask-and-gender) 4. [1 model](#create-a-single-model-for-all-age-mask-and-gender) 5. [Voting](#voting) 6. [Grayscale images training](#grayscale-images-training) 7. [Cropped images training](#cropped-images-training) 8. [References](#references)   """;Computer Vision;https://github.com/JIHOO97/Image-Classification-Competition
"""We use the MobileNet v1 convolutional neural network architecture [1]  which gives us a light-weight and efficient model with reasonable performance.   """;General;https://github.com/DCASE-REPO/dcase2019_task2_baseline
"""We use the MobileNet v1 convolutional neural network architecture [1]  which gives us a light-weight and efficient model with reasonable performance.   """;Computer Vision;https://github.com/DCASE-REPO/dcase2019_task2_baseline
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   """;General;https://github.com/MichaelZhouwang/LMlexsub
"""In multi-object tracking the leading paradigm is tracking-by-detection which is often a two step approach. Recent one-shot approaches have shown promising results that are able to run in real-time. One-shot models learn detections and appearance embeddings jointly. We built upon the one-shot method by changing the way the embeddings are trained. Softmax based features are trained by classifying the embedding feature map to the correspond track-IDs. We propose a pairwise loss that is able to learn embeddings without track-ID labels. On the MOT17 dataset we obtain competitive results with respect to the softmax based method.   """;Computer Vision;https://github.com/nadinenijssen/Github_5AUA0_Project_G12T1
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M and VGG2 datasets  which were already packed in the MxNet binary format. The network backbones include ResNet  InceptionResNet_v2  DenseNet  DPN and MobiletNet. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss. * loss-type=0:  Softmax * loss-type=1:  SphereFace * loss-type=2:  CosineFace * loss-type=4:  ArcFace * loss-type=5:  Combined Margin * loss-type=12: TripletLoss  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/zrui94/insight_mx
"""Stochastic gradient descent (SGD) is a popular method for training neural networks  using “minibatches” of data to update the network’s weights.  Improvements are often made upon SGD by either using acceleration/momentum or by altering the learning rate over time. Zhang et al. propose a novel improvement in [LookaheadOptimizer: k steps forward  1 step back](https://arxiv.org/abs/1907.08610). They showed that Lookahead consistantly outperformed other optimizers on popular language modelling  machine translantion and image classification tasks.   Lookahead uses a set of fast weights which lookahead k steps and a set of slow weights with learning rate alpha. From a high-level perspective  Lookahead chooses the search direction by calculating the fast weights of the inner optimizer. The approach facilitates the use of any inner optimizer such as Adam or SGD. This comes with the cost of a slighlty increased time complexity  however the original authors illustrate a significant increase in efficiency.   This project aims to test these findings by reimplementing the main CIFAR-10/100 and Penn Treebank experiments. See [our paper](https://github.com/COMP6248-Reproducability-Challenge/LookaheadOptimizer/blob/master/Reproducibility%20Challenge%20LA%20Optimizer.pdf) for more details and our findings.    """;General;https://github.com/COMP6248-Reproducability-Challenge/LookaheadOptimizer
"""**Deep Vision Transformer** is initially described in [arxiv](https://arxiv.org/abs/2103.11886)  which observes the attention collapese phenomenon when training deep vision transformers: In this paper  we show that  unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers  the performance of ViTs saturate fast when scaled to be deeper. More specifically  we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper  the attention maps gradually become similar and even much the same after certain layers. In other words  the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs  the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation  we propose a simple yet effective method  named Re-attention  to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably  when training a deep ViT model with 32 transformer blocks  the Top-1 classification accuracy can be improved by 1.6% on ImageNet.  <p align=""center""> <img src=""https://github.com/zhoudaquan/DeepViT_ICCV21/blob/master/figures/performance_comparison.png"" | width=500> </p>   """;General;https://github.com/zhoudaquan/dvit_repo
"""**Deep Vision Transformer** is initially described in [arxiv](https://arxiv.org/abs/2103.11886)  which observes the attention collapese phenomenon when training deep vision transformers: In this paper  we show that  unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers  the performance of ViTs saturate fast when scaled to be deeper. More specifically  we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper  the attention maps gradually become similar and even much the same after certain layers. In other words  the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs  the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation  we propose a simple yet effective method  named Re-attention  to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably  when training a deep ViT model with 32 transformer blocks  the Top-1 classification accuracy can be improved by 1.6% on ImageNet.  <p align=""center""> <img src=""https://github.com/zhoudaquan/DeepViT_ICCV21/blob/master/figures/performance_comparison.png"" | width=500> </p>   """;Computer Vision;https://github.com/zhoudaquan/dvit_repo
"""This project is based on our CVPR2019 paper. You can find the [arXiv](https://arxiv.org/abs/1811.07246) version here.  ``` @inproceedings{wu2019pointconv    title={Pointconv: Deep convolutional networks on 3d point clouds}    author={Wu  Wenxuan and Qi  Zhongang and Fuxin  Li}    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}    pages={9621--9630}    year={2019} } ```  Unlike images which are represented in regular dense grids  3D point clouds are irregular and unordered  hence applying convolution on them can be difficult. In this paper  we extend the dynamic filter to a new convolution operation  named PointConv. PointConv can be applied on point clouds to build deep convolutional networks. We treat convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight and density functions. With respect to a given point  the weight functions are learned with multi-layer perceptron networks and the density functions through kernel density estimation. A novel reformulation is proposed for efficiently computing the weight functions  which allowed us to dramatically scale up the network and significantly improve its performance. The learned convolution kernel can be used to compute translation-invariant and permutation-invariant convolution on any point set in the 3D space. Besides  PointConv can also be used as deconvolution operators to propagate features from a subsampled point cloud back to its original resolution. Experiments on ModelNet40  ShapeNet  and ScanNet show that deep convolutional neural networks built on PointConv are able to achieve state-of-the-art on challenging semantic segmentation benchmarks on 3D point clouds. Besides  our experiments converting CIFAR-10 into a point cloud showed that networks built on PointConv can match the performance of convolutional networks in 2D images of a similar structure.   """;Computer Vision;https://github.com/kenakai16/pointconv_pytorch
"""To date  most open access public smart meter datasets are still at 30-minute or hourly temporal resolution. While this level of granularity could be sufficient for billing or deriving aggregated generation or consumption patterns  it may not fully capture the weather transients or consumption spikes. One potential solution is to synthetically interpolate high resolution data from commonly accessible lower resolution data  for this work  the SRGAN model is used for this purpose.  """;General;https://github.com/tomtrac/SRGAN_power_data_generation
"""To date  most open access public smart meter datasets are still at 30-minute or hourly temporal resolution. While this level of granularity could be sufficient for billing or deriving aggregated generation or consumption patterns  it may not fully capture the weather transients or consumption spikes. One potential solution is to synthetically interpolate high resolution data from commonly accessible lower resolution data  for this work  the SRGAN model is used for this purpose.  """;Computer Vision;https://github.com/tomtrac/SRGAN_power_data_generation
"""We find that specialized language adapters might not be robust to unseen language variations  and that utilization of multiple existing pretrained language adapters alleviates this issue. We propose an algorithm named EMEA(Entropy Minimized Ensemble of Language Adapters)  which optimizes the ensemble weights of a group of related language adapters at test time for each test input.   """;General;https://github.com/cindyxinyiwang/emea
"""**Adaptive Notes Generator** *is a tool that helps us attend online classes effectively:star_struck:. Due to the Online class culture  taking notes in pen and paper is not a good idea  the only options left are to click screenshots or struggle to note down everything in your notebook:unamused:. Our application will make your life easier  once a meeting video:film_projector: is provided  we will create the notes that will save you time:stopwatch: of research and gathering resources. We will divide your meeting into useful segments and add additional data to make it easy to understand any concept.:bookmark_tabs::bookmark_tabs:*   """;Sequential;https://github.com/KushGrandhi/Polaroid
"""DeBERTa (Decoding-enhanced BERT with disentangled attention) improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism  where each word is represented using two vectors that encode its content and position  respectively  and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second  an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks.   """;General;https://github.com/microsoft/DeBERTa
"""DeBERTa (Decoding-enhanced BERT with disentangled attention) improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism  where each word is represented using two vectors that encode its content and position  respectively  and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second  an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks.   """;Natural Language Processing;https://github.com/microsoft/DeBERTa
"""Online Hard Example Mining (OHEM) is an online bootstrapping algorithm for training region-based ConvNet object detectors like [Fast R-CNN](https://github.com/rbgirshick/fast-rcnn). OHEM  - works nicely in the Stochastic Gradient Descent (SGD) paradigm  - simplifies training by removing some heuristics and hyperparameters  - leads to better convergence (lower training set loss)  - consistently gives significanlty higher mAP on PASCAL VOC and MS COCO.  OHEM was initially presented at CVPR 2016 as an Oral Presentation. For more details  see the [arXiv tech report](http://arxiv.org/abs/1604.03540).   """;General;https://github.com/abhi2610/ohem
"""Animation movie studios like Pixar uses a technique called Pathtracing which produces high-quality photorealistic images. Due to the computational complexity of this approach  it will take 8-16 hours to render depending on the composition of the scene. This time-consuming process makes Pathtracing unsuitable for interactive image synthesis. To achieve this increased visual quality in a real time application many approaches have been proposed in the recent past to approximate global illumination effects like ambient occlusion  reflections  indirect light  scattering  depth of field  motion blur and caustics. While these techniques improve the visual quality  the results are incomparable to the one produce by Pathtracing. We propose a novel technique where we make use of a deep generative model to generate high-quality photorealistic frames from a geometry buffer(G-buffer). The main idea here is to train a deep convolutional neural network to find a mapping from G-buffer to pathtraced image of the same scene. This trained network can then be used in a real time scene to get high-quality results.    """;Computer Vision;https://github.com/CreativeCodingLab/DeepIllumination
"""Animation movie studios like Pixar uses a technique called Pathtracing which produces high-quality photorealistic images. Due to the computational complexity of this approach  it will take 8-16 hours to render depending on the composition of the scene. This time-consuming process makes Pathtracing unsuitable for interactive image synthesis. To achieve this increased visual quality in a real time application many approaches have been proposed in the recent past to approximate global illumination effects like ambient occlusion  reflections  indirect light  scattering  depth of field  motion blur and caustics. While these techniques improve the visual quality  the results are incomparable to the one produce by Pathtracing. We propose a novel technique where we make use of a deep generative model to generate high-quality photorealistic frames from a geometry buffer(G-buffer). The main idea here is to train a deep convolutional neural network to find a mapping from G-buffer to pathtraced image of the same scene. This trained network can then be used in a real time scene to get high-quality results.    """;General;https://github.com/CreativeCodingLab/DeepIllumination
"""This package provides a Lasagne/Theano-based implementation of the deep Q-learning algorithm described in:  [Playing Atari with Deep Reinforcement Learning](http://arxiv.org/abs/1312.5602) Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Alex Graves  Ioannis Antonoglou  Daan Wierstra  Martin Riedmiller  and   Mnih  Volodymyr  et al. ""Human-level control through deep reinforcement learning."" Nature 518.7540 (2015): 529-533.  Here is a video showing a trained network playing breakout (using an earlier version of the code):   http://youtu.be/SZ88F82KLX4   """;Reinforcement Learning;https://github.com/spragunr/deep_q_rl
"""This is the Tensorflow code corresponding to [A Two-Stage Method for Text Line Detection in Historical Documents ](#a-two-stage-method-for-text-line-detection-in-historical-documents). This repo contains the neural pixel labeling part described in the paper. It contains the so-called ARU-Net (among others) which is basically an extended version of the well known U-Net [[2]](#u-net-convolutional-networks-for-biomedical-image-segmentation).  Besides the model and the basic workflow to train and test models  different data augmentation strategies are implemented to reduce the amound of training data needed. The repo's features are summarized below: + Inference Demo     + Trained and freezed tensorflow graph included     + Easy to reuse for own inference tests + Workflow      + Full training workflow to parametrize and train your own models     + Contains different models  data augmentation strategies  loss functions      + Training on specific GPU  this enables the training of several models on a multi GPU system in parallel     + Easy validation for trained model either using classical or ema-shadow weights  Please cite [[1]](#a-two-stage-method-for-text-line-detection-in-historical-documents) if you find this repo useful and/or use this software for own work.    """;Computer Vision;https://github.com/TobiasGruening/ARU-Net
"""**nmtpy** is a suite of Python tools  primarily based on the starter code provided in [dl4mt-tutorial](https://github.com/nyu-dl/dl4mt-tutorial) for training neural machine translation networks using Theano. The basic motivation behind forking **dl4mt-tutorial** was to create a framework where it would be easy to implement a new model by just copying and modifying an existing model class (or even inheriting from it and overriding some of its methods).  To achieve this purpose  **nmtpy** tries to completely isolate training loop  beam search  iteration and model definition:   - `nmt-train` script to start a training experiment   - `nmt-translate` to produce model-agnostic translations. You just pass a trained model's   checkpoint file and it does its job.   - `nmt-rescore` to rescore translation hypotheses using an nmtpy model.   - An abstract `BaseModel` class to derive from to define your NMT architecture.   - An abstract `Iterator` to derive from for your custom iterators.  A non-exhaustive list of differences between **nmtpy** and **dl4mt-tutorial** is as follows:    - No shell script  everything is in Python   - Overhaul object-oriented refactoring of the code: clear separation of API and scripts that interface with the API   - INI style configuration files to define everything regarding a training experiment   - Transparent cleanup mechanism to kill stale processes  remove temporary files   - Simultaneous logging of training details to stdout and log file   - Supports out-of-the-box BLEU  METEOR and COCO eval metrics   - Includes [subword-nmt](https://github.com/rsennrich/subword-nmt) utilities for training and applying BPE model (NOTE: This may change as the upstream subword-nmt moves forward as well.)   - Plugin-like text filters for hypothesis post-processing (Example: BPE  Compound  Char2Words for Char-NMT)   - Early-stopping and checkpointing based on perplexity  BLEU or METEOR (Ability to add new metrics easily)   - Single `.npz` file to store everything about a training experiment   - Automatic free GPU selection and reservation using `nvidia-smi`   - Shuffling support between epochs:     - Simple shuffle     - [Homogeneous batches of same-length samples](https://github.com/kelvinxu/arctic-captions) to improve training speed   - Improved parallel translation decoding on CPU   - Forced decoding i.e. rescoring using NMT   - Export decoding informations into `json` for further visualization of attention coefficients   - Improved numerical stability and reproducibility   - Glorot/Xavier  He  Orthogonal weight initializations   - Efficient SGD  Adadelta  RMSProp and ADAM: Single forward/backward theano function without intermediate variables   - Ability to stop updating a set of weights by recompiling optimizer   - Several recurrent blocks:     - GRU  Conditional GRU (CGRU) and LSTM     - Multimodal attentive CGRU variants   - [Layer Normalization](https://github.com/ryankiros/layer-norm) support for GRU   - 2-way or 3-way [tied target embeddings](https://arxiv.org/abs/1608.05859)   - Simple/Non-recurrent Dropout  L2 weight decay   - Training and validation loss normalization for comparable perplexities   - Initialization of a model with a pretrained NMT for further finetuning   """;General;https://github.com/lium-lst/nmtpy
"""The current code trains an RNN ([Gated Recurrent Units](https://arxiv.org/abs/1406.1078)) to predict  at each timestep (i.e. visit)  the diagnosis codes occurring in the next visit. This is denoted as *Sequential Diagnoses Prediction* in the paper.  In the future  we will relases another version for making a single prediction for the entire visit sequence. (e.g. Predict the onset of heart failure given the visit record)  Note that the current code uses [Multi-level Clinical Classification Software for ICD-9-CM](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp) as the domain knowledge. We will release the one that uses ICD9 Diagnosis Hierarchy in the future. 	  """;General;https://github.com/mp2893/gram
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   """;General;https://github.com/longchr123/insightface-analysis
