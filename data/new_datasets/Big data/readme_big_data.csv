Label;Repo;Text
Big data;https://github.com/benpickles/peity;"""# Peity    [![Build Status](https://travis-ci.org/benpickles/peity.svg?branch=master)](https://travis-ci.org/benpickles/peity)    Peity (sounds like deity) is a jQuery plugin that converts an element's content into a mini `<svg>` pie, donut, line or bar chart.    ## Basic Usage    ### HTML    ```html  <span class=""pie"">3/5</span>  <span class=""donut"">5,2,3</span>  <span class=""line"">3,5,1,6,2</span>  <span class=""bar"">2,5,3,6,2,1</span>  ```    ### JavaScript (jQuery)    ```js  $("".pie"").peity(""pie"");  $("".donut"").peity(""donut"");  $("".line"").peity(""line"");  $("".bar"").peity(""bar"");  ```    ## Docs    More detailed usage can be found at [benpickles.github.io/peity](http://benpickles.github.io/peity/).    ## Development    Run the automated visual regression tests with:        make test    Run a filtered set of tests with:        ARGS=""--grep bar"" make test    To manually view all test cases run:        make server    And hit <http://localhost:8080/>.    ## Release    Update the version string in `jquery.peity.js`, run `make release`, and follow the instructions.    ## Copyright    Copyright 2009-2020 [Ben Pickles](http://benpickles.com/). See [LICENCE](https://github.com/benpickles/peity/blob/master/LICENCE) for details. """
Big data;https://github.com/cswinter/LocustDB;"""# LocustDB    [![Build Status][bi]][bl] [![Crates.io][ci]][cl] [![Gitter][gi]][gl]    [bi]: https://github.com/cswinter/LocustDB/workflows/Test/badge.svg  [bl]: https://github.com/cswinter/LocustDB/actions    [ci]: https://img.shields.io/crates/v/locustdb.svg  [cl]: https://crates.io/crates/locustdb/    [gi]: https://badges.gitter.im/LocustDB/Lobby.svg  [gl]: https://gitter.im/LocustDB/Lobby    An experimental analytics database aiming to set a new standard for query performance and storage efficiency on commodity hardware.  See [How to Analyze Billions of Records per Second on a Single Desktop PC][blogpost] and [How to Read 100s of Millions of Records per Second from a Single Disk][blogpost-2] for an overview of current capabilities.    ## Usage    Download the [latest binary release][latest-release], which can be run from the command line on most x64 Linux systems, including Windows Subsystem for Linux. For example, to load the file `test_data/nyc-taxi.csv.gz` in this repository and start the repl run:    ```Bash  ./locustdb --load test_data/nyc-taxi.csv.gz --trips  ```    When loading `.csv` or `.csv.gz` files with `--load`, the first line of each file is assumed to be a header containing the names for all columns. The type of each column will be derived automatically, but this might break for columns that contain a mixture of numbers/strings/empty entries.    To persist data to disk in LocustDB's internal storage format (which allows fast queries from disk after the initial load), specify the storage location with `--db-path`  When creating/opening a persistent database, LocustDB will open a lot of files and might crash if the limit on the number of open files is too low.  On Linux, you can check the current limit with `ulimit -n` and set a new limit with e.g. `ulimit -n 4096`.    The `--trips` flag will configure the ingestion schema for loading the 1.46 billion taxi ride dataset which can be downloaded [here][nyc-taxi-trips].    For additional usage info, invoke with `--help`:    ```Bash  $ ./locustdb --help  LocustDB 0.2.1  Clemens Winter <clemenswinter1@gmail.com>  Massively parallel, high performance analytics database that will rapidly devour all of your data.    USAGE:      locustdb [FLAGS] [OPTIONS]    FLAGS:      -h, --help             Prints help information          --mem-lz4          Keep data cached in memory lz4 encoded. Decreases memory usage and query speeds.          --reduced-trips    Set ingestion schema for select set of columns from nyc taxi ride dataset          --seq-disk-read    Improves performance on HDD, can hurt performance on SSD.          --trips            Set ingestion schema for nyc taxi ride dataset      -V, --version          Prints version information    OPTIONS:          --db-path <PATH>           Path to data directory          --load <FILES>             Load .csv or .csv.gz files into the database          --mem-limit-tables <GB>    Limit for in-memory size of tables in GiB [default: 8]          --partition-size <ROWS>    Number of rows per partition when loading new data [default: 65536]          --readahead <MB>           How much data to load at a time when reading from disk during queries in MiB                                     [default: 256]          --schema <SCHEMA>          Comma separated list specifying the types and (optionally) names of all columns in                                     files specified by `--load` option.                                     Valid types: `s`, `string`, `i`, `integer`, `ns` (nullable string), `ni` (nullable                                     integer)                                     Example schema without column names: `int,string,string,string,int`                                     Example schema with column names: `name:s,age:i,country:s`          --table <NAME>             Name for the table populated with --load [default: default]          --threads <INTEGER>        Number of worker threads. [default: number of cores (12)]  ```    ## Goals  A vision for LocustDB.    ### Fast  Query performance for analytics workloads is best-in-class on commodity hardware, both for data cached in memory and for data read from disk.    ### Cost-efficient  LocustDB automatically achieves spectacular compression ratios, has minimal indexing overhead, and requires less machines to store the same amount of data than any other system. The trade-off between performance and storage efficiency is configurable.    ### Low latency  New data is available for queries within seconds.    ### Scalable  LocustDB scales seamlessly from a single machine to large clusters.    ### Flexible and easy to use  LocustDB should be usable with minimal configuration or schema-setup as:  - a highly available distributed analytics system continuously ingesting data and executing queries  - a commandline tool/repl for loading and analysing data from CSV files  - an embedded database/query engine included in other Rust programs via cargo      ## Non-goals  Until LocustDB is production ready these are distractions at best, if not wholly incompatible with the main goals.    ### Strong consistency and durability guarantees  - small amounts of data may be lost during ingestion  - when a node is unavailable, queries may return incomplete results  - results returned by queries may not represent a consistent snapshot    ### High QPS  LocustDB does not efficiently execute queries inserting or operating on small amounts of data.    ### Full SQL support  - All data is append only and can only be deleted/expired in bulk.  - LocustDB does not support queries that cannot be evaluated independently by each node (large joins, complex subqueries, precise set sizes, precise top n).    ### Support for cost-inefficient or specialised hardware  LocustDB does not run on GPUs.      ## Compiling from source    1. Install Rust: [rustup.rs][rustup]  2. Clone the repository    ```Bash  git clone https://github.com/cswinter/LocustDB.git  cd LocustDB  ```    3. Compile with `--release` for optimal performance:    ```Bash  cargo run --release --bin repl -- --load test_data/nyc-taxi.csv.gz --reduced-trips  ```    ### Running tests or benchmarks    `cargo test`    `cargo bench`    ### Storage backend  LocustDB has support for persisting data to disk and running queries on data stored on disk.  This feature is disabled by default, and has to be enabled explicitly by passing `--features ""enable_rocksdb""` to cargo during compilation.  The database backend uses RocksDB, which is a somewhat complex C++ dependency that has to be compiled from source and requires gcc and various libraries to be available.  You will have to manually install those on your system, instructions can be found [here][rocksdb-dependencies].  You may also have to install various other random tools until compilation succeeds.    ### LZ4    Compile with `--features ""enable_lz4""` to enable an additional lz4 compression pass which can significantly reduce data size both on disk and in-memory, at the cost of slightly slower in-memory queries.      [nyc-taxi-trips]: https://www.dropbox.com/sh/4xm5vf1stnf7a0h/AADRRVLsqqzUNWEPzcKnGN_Pa?dl=0  [blogpost]: https://clemenswinter.com/2018/07/09/how-to-analyze-billions-of-records-per-second-on-a-single-desktop-pc/  [blogpost-2]: https://clemenswinter.com/2018/08/13/how-read-100s-of-millions-of-records-per-second-from-a-single-disk/  [rustup]: https://rustup.rs/  [rocksdb-dependencies]: https://github.com/facebook/rocksdb/blob/master/INSTALL.md#dependencies  [latest-release]: https://github.com/cswinter/LocustDB/releases/download/v0.1.0-alpha/locustdb-0.1.0-alpha-x64-linux.0-alpha """
Big data;https://github.com/adobe-research/spindle;"""# Spindle    **Spindle is [Brandon Amos'](http://github.com/bamos)  2014 summer internship project with Adobe Research  and is not under active development.**    ---    ![](https://github.com/adobe-research/spindle/raw/master/images/architecture.png)    Analytics platforms such as [Adobe Analytics][adobe-analytics]  are growing to process petabytes of data in real-time.  Delivering responsive interfaces querying this amount of data is difficult,  and there are many distributed data processing technologies such  as [Hadoop MapReduce][mapreduce], [Apache Spark][spark],  [Apache Drill][drill], and [Cloudera Impala][impala]  to build low-latency query systems.    Spark is part of the [Apache Software Foundation][apache]  and claims speedups up to 100x faster than Hadoop for in-memory  processing.  Spark is shifting from a research project to a production-ready library,  and academic publications and presentations from  the [2014 Spark Summit][2014-spark-summit]  archives several use cases of Spark and related technology.  For example,  [NBC Universal][nbc] presents their use of Spark to query [HBase][hbase]  tables and analyze an international cable TV video distribution [here][nbc-pres].  Telefonica presents their use of  Spark with [Cassandra][cassandra]  for cyber security analytics [here][telefonica-pres].  [ADAM][adam] is an open source data storage format and processing  pipeline for genomics data built in Spark and [Parquet][parquet].    Even though people are publishing use cases of Spark,  few people have published  experiences of building and tuning production-ready Spark systems.  Thorough knowledge of Spark internals  and libraries that interoperate well with Spark is necessary  to achieve optimal performance from Spark applications.    **Spindle is a prototype Spark-based web analytics query engine designed  around the requirements of production workloads.**  Spindle exposes query requests through a multi-threaded  HTTP interface implemented with [Spray][spray].  Queries are processed by loading data from [Apache Parquet][parquet] columnar  storage format on the  [Hadoop distributed filesystem][hdfs].    This repo contains the Spindle implementation and benchmarking scripts  to observe Spindle's performance while exploring Spark's tuning options.  Spindle's goal is to process petabytes of data on thousands of nodes,  but the current implementation has not yet been tested at this scale.  Our current experimental results use six nodes,  each with 24 cores and 21g of Spark memory, to query 13.1GB of analytics data.  The trends show that further Spark tuning and optimizations should  be investigated before attempting larger scale deployments.    # Demo  We used Spindle to generate static webpages that are hosted  statically [here][demo].  Unfortunately, the demo is only for illustrative purposes and  is not running Spindle in real-time.    ![](https://github.com/adobe-research/spindle/raw/master/images/top-pages-by-browser.png)  ![](https://github.com/adobe-research/spindle/raw/master/images/adhoc.png)    [Grunt][grunt] is used to deploy `demo` to [Github pages][ghp]  in the [gh-pages][ghp] branch with the [grunt-build-control][gbc] plugin.  The [npm][npm] dependencies are managed in [package.json][pjson]  and can be installed with `npm install`.    # Loading Sample Data  The `load-sample-data` directory contains a Scala program  to load the following sample data into [HDFS][hdfs]  modeled after  [adobe-research/spark-parquet-thrift-example][spark-parquet-thrift-example].  See [adobe-research/spark-parquet-thrift-example][spark-parquet-thrift-example]  for more information on running this application  with [adobe-research/spark-cluster-deployment][spark-cluster-deployment].    ### hdfs://hdfs_server_address:8020/spindle-sample-data/2014-08-14  | post_pagename | user_agent | visit_referrer | post_visid_high | post_visid_low | visit_num | hit_time_gmt | post_purchaseid | post_product_list | first_hit_referrer |  |---|---|---|---|---|---|---|---|---|---|  | Page A | Chrome | http://facebook.com | 111 | 111 | 1 | 1408007374 | | | http://google.com  | Page B | Chrome | http://facebook.com | 111 | 111 | 1 | 1408007377 | | | http://google.com  | Page C | Chrome | http://facebook.com | 111 | 111 | 1 | 1408007380 | purchase1 | ;ProductID1;1;40;,;ProductID2;1;20; | http://google.com  | Page B | Chrome | http://google.com | 222 | 222 | 1 | 1408007379 | | | http://google.com  | Page C | Chrome | http://google.com | 222 | 222 | 1 | 1408007381 | | | http://google.com  | Page A | Firefox | http://google.com | 222 | 222 | 1 | 1408007382 | | | http://google.com  | Page A | Safari | http://google.com | 333 | 333 | 1 | 1408007383 | | | http://facebook.com  | Page B | Safari | http://google.com | 333 | 333 | 1 | 1408007386 | | | http://facebook.com    ### hdfs://hdfs_server_address:8020/spindle-sample-data/2014-08-15  | post_pagename | user_agent | visit_referrer | post_visid_high | post_visid_low | visit_num | hit_time_gmt | post_purchaseid | post_product_list | first_hit_referrer |  |---|---|---|---|---|---|---|---|---|---|  | Page A | Chrome | http://facebook.com | 111 | 111 | 1 | 1408097374 | | | http://google.com  | Page B | Chrome | http://facebook.com | 111 | 111 | 1 | 1408097377 | | | http://google.com  | Page C | Chrome | http://facebook.com | 111 | 111 | 1 | 1408097380 | purchase1 | ;ProductID1;1;60;,;ProductID2;1;100; | http://google.com  | Page B | Chrome | http://google.com | 222 | 222 | 1 | 1408097379 | | | http://google.com  | Page A | Safari | http://google.com | 333 | 333 | 1 | 1408097383 | | | http://facebook.com  | Page B | Safari | http://google.com | 333 | 333 | 1 | 1408097386 | | | http://facebook.com    ### hdfs://hdfs_server_address:8020/spindle-sample-data/2014-08-16  | post_pagename | user_agent | visit_referrer | post_visid_high | post_visid_low | visit_num | hit_time_gmt | post_purchaseid | post_product_list | first_hit_referrer |  |---|---|---|---|---|---|---|---|---|---|  | Page A | Chrome | http://facebook.com | 111 | 111 | 1 | 1408187380 | purchase1 | ;ProductID1;1;60;,;ProductID2;1;100; | http://google.com  | Page B | Chrome | http://facebook.com | 111 | 111 | 1 | 1408187380 | purchase1 | ;ProductID1;1;200; | http://google.com  | Page D | Chrome | http://google.com | 222 | 222 | 1 | 1408187379 | | | http://google.com  | Page A | Safari | http://google.com | 333 | 333 | 1 | 1408187383 | | | http://facebook.com  | Page B | Safari | http://google.com | 333 | 333 | 1 | 1408187386 | | | http://facebook.com  | Page C | Safari | http://google.com | 333 | 333 | 1 | 1408187388 | | | http://facebook.com    # Queries.  Spindle includes eight queries that are representative of  the data sets and computations of real queries the  Adobe Marketing Cloud processes.  All collect statements refer to the combined filter and map operation,  not the operation to gather an RDD as a local Scala object.    + *Q0* (**Pageviews**)    is a breakdown of the number of pages viewed    each day in the specified range.  + *Q1* (**Revenue**) is the overall revenue for each day in    the specified range.  + *Q2* (**RevenueFromTopReferringDomains**) obtains the top referring    domains for each visit and breaks down the revenue by day.    The `visit_referrer` field is preprocessed into each record in    the raw data.  + *Q3* (**RevenueFromTopReferringDomainsFirstVisitGoogle**) is    the same as RevenueFromTopReferringDomains, but with the    visitor's absolute first referrer from Google.    The `first_hit_referrer` field is preprocessed into each record in    the raw data.  + *Q4* (**TopPages**) is a breakdown of the top pages for the    entire date range, not per day.  + *Q5* (**TopPagesByBrowser**) is a breakdown of the browsers    used for TopPages.  + *Q6* (**TopPagesByPreviousTopPages**) breaks down the top previous    pages a visitor was at for TopPages.  + *Q7* (**TopReferringDomains**) is the top referring domains for    the entire date range, not per day.    The following table shows the columnar subset  each query utilizes.    ![](https://github.com/adobe-research/spindle/raw/master/images/columns-needed.png)    The following table shows the operations each query performs  and is intended as a summary rather than full description of  the implementations.  The bold text in indicate operations in which the target  partition size is specified, which is further described in the  ""Partitioning"" section below.    ![](https://github.com/adobe-research/spindle/raw/master/images/query-operations.png)    # Spindle Architecture  The query engine provides a request and response interface to  interact with the application layer, and Spindle's goal is to  benchmark a realistic low latency web analytics query engine.    Spindle provides query requests and reports over HTTP with the  [Spray][spray] library, which is multi-threaded and provides  REST/HTTP-based integration layer on Scala for queries and parameters,  as illustrated in the figure below.    ![](https://github.com/adobe-research/spindle/raw/master/images/architecture.png)    When a user request to execute a query over HTTP,  Spray allocates a thread to process the HTTP request and converts  it into a Spray request.  The Spray request follows a route defined in the `QueryService` Actor,  and queries are processed with the `QueryProcessor` singleton object.  The `QueryProcessor` interacts with a global Spark context,  which connects the Scala application to the Spark cluster.    The Spark context supports multi-threading and offers a  `FIFO` and `FAIR` scheduling options for concurrent queries.  Spindle uses Spark's `FAIR` scheduling option to minimize overall latency.    ## Future Work - Utilizing Spark job servers or resource managers.  Spindle's architecture can likely be improved on larger clusters by  utilizing a job server or resource manager to  maintain a pool of Spark contexts for query execution.  [Ooyala's spark-jobserver][spark-jobserver] provides  a RESTful interface for submitting Spark jobs that Spindle could  interface with instead of interfacing with Spark directly.  [YARN][yarn] can also be used to manage Spark's  resources on a cluster, as described in [this article][spark-yarn].    However, allocating resources on the cluster raises additional  questions and engineering work that Spindle can address in future work.  Spindle's current architecture coincides HDFS and Spark workers  on the same nodes, minimizing the network traffic required  to load data.  How much will the performance degrade if the resource manager  allocates some subset of Spark workers that don't  coincide with any of the HDFS data being accessed?    Furthermore, how would a production-ready caching policy  on a pool of Spark Contexts look?  What if many queries are being submitted and executed on  different Spark Contexts that use the same data?  Scheduling the queries on the same Spark Context and  caching the data between query executions would substantially  increase the performance, but how should the scheduler  be informed of this information?    ## Data Format  Adobe Analytics events data have at least 250 columns,  and sometimes significantly more than 250 columns.  Most queries use less than 7 columns, and loading all of the  columns into memory to only use 7 is inefficient.  Spindle stores event data in the [Parquet][parquet] columnar store  on the [Hadoop Distributed File System][hdfs] (HDFS) with  [Kryo][kryo] serialization enabled  to only load the subsets of columns each query requires.    [Cassandra][cassandra] is a NoSQL database that we considered  as an alternate to Parquet.  However, Spindle also utilizes [Spark SQL][spark-sql],  which supports Parquet, but not Cassandra.    Parquet can be used with [Avro][avro] or [Thrift][thrift] schemas.  [Matt Massie's article][spark-parquet-avro] provides an example of  using Parquet with Avro.  [adobe-research/spark-parquet-thrift-example][spark-parquet-thrift-example]  is a complete [Scala][scala]/[sbt][sbt] project  using Thrift for data serialization and shows how to only load the  specified columnar subset.  For a more detailed introduction to Thrift,  see [Thrift: The Missing Guide][thrift-guide].    The entire Adobe Analytics schema cannot be published.  The open source release of Spindle uses  [AnalyticsData.thrift][AnalyticsData.thrift],  which contains 10 non-proprietary fields for web analytics.    Columns postprocessed into the data after collection have the `post_`  prefix along with `visit_referrer` and `first_hit_referrer`.  Visitors are categorized by concatenating the strings  `post_visid_high` and `post_visid_low`.  A visitor has visits which are numbered by `visit_num`,  and a visit has hits that occur at `hit_time_gmt`.  If the hit is a webpage hit from a browser, the `post_pagename` and  `user_agent` fields are used, and the revenue from a hit,  is denoted in `post_purchaseid` and `post_product_list`.    ```Thrift  struct AnalyticsData {    1: string post_pagename;    2: string user_agent;    3: string visit_referrer;    4: string post_visid_high;    5: string post_visid_low;    6: string visit_num;    7: string hit_time_gmt;    8: string post_purchaseid;    9: string post_product_list;    10: string first_hit_referrer;  }  ```    This data is separated by day on disk of format `YYYY-MM-DD`.    ## Caching Data  Spindle provides a caching option that will cache the loaded  Spark data in memory between query requests to show the  maximum speedup caching provides.  Caching introduces a number of interesting questions when dealing  with sparse data.  For example, two queries could be submitted on the same date range  that request overlapping, but not identical, column subsets.  How should these data sets with partially overlapping values be  cached in the application?  What if one of the queries is called substantially more times than  the other? How should the caching policy ensure these columns are  not evicted?  We will explore these questions in future work.    ## Partitioning  Spark affords partitioning data across nodes for operations  such as `distinct`, `reduceByKey`, and `groupByKey` to specify the  minimum number of resulting partitions.    Counting the number of records in an RDD  expensive, and automatically knowing the optimal number of partitions  for operations depends highly on the data and operations.  For optimal partitioning, applications should estimate the  number of records to process and ensure the partitions contain  some minimum value of records.    Spindle puts a target number of records in each partition  by estimating the total number of records to be processed  from Parquet's metadata.  However, most queries filter records before doing operations that  impact the partitioning by approximately 50\% in our data.  For example, an empty `post_pagename` field indicates that the  analytics hit is from an event other than a user visiting a page,  and the first Spark operation in TopPages is to obtain only  the page visit hits by filtering out records with empty `post_pagename`  fields.    # Installing Spark and HDFS on a cluster.  | ![](https://github.com/adobe-research/spark-cluster-deployment/raw/master/images/initial-deployment-2.png) | ![](https://github.com/adobe-research/spark-cluster-deployment/raw/master/images/application-deployment-1.png) |  |---|---|    Spark 1.0.0 can be deployed to traditional cloud and job management services  such as [EC2][spark-ec2], [Mesos][spark-mesos], or  [Yarn][spark-yarn].  Further, Spark's [standalone cluster][spark-standalone] mode enables  Spark to run on other servers without installing other  job management services.    However, configuring and submitting applications to a Spark 1.0.0 standalone  cluster currently requires files to be synchronized across the entire cluster,  including the Spark installation directory.  These problems have motivated our  [adobe-research/spark-cluster-deployment][spark-cluster-deployment] project,  which utilizes [Fabric][fabric] and [Puppet][puppet] to further automate  the Spark standalone cluster.    # Building    Ensure you have the following software on the server.  Spindle has been developed on CentOS 6.5 with  sbt 0.13.5, Spark 1.0.0, Hadoop 2.0.0-cdh4.7.0,  and parquet-thrift 1.5.0.    | Command | Output |  |---|---|  | `cat /etc/centos-release` | CentOS release 6.5 (Final) |  | `sbt --version` | sbt launcher version 0.13.5 |  | `thrift --version` | Thrift version 0.9.1 |  | `hadoop version` | Hadoop 2.0.0-cdh4.7.0 |  | `cat /usr/lib/spark/RELEASE` | Spark 1.0.0 built for Hadoop 2.0.0-cdh4.7.0 |    Spindle uses [sbt][sbt] and the [sbt-assembly][sbt-assembly] plugin  to build Spark into a fat JAR to be deployed to the Spark cluster.  Using [adobe-research/spark-cluster-deployment][spark-cluster-deployment],  modify `config.yaml` to have your server configurations,  and build the application with `ss-a`, send the JAR to your cluster  with `ss-sy`, and start Spindle with `ss-st`.    # Experimental Results  All experiments leverage a homogeneous six node production cluster  of HP ProLiant DL360p Gen8 blades.  Each node has 32GB of DDR3 memory at 1333MHz,  (2) 6 core Intel Xeon 0 processors at 2.30GHz and 1066MHz FSB,  and (10) 15K SAS 146GB, RAID 5 hard disks.  Furthermore, each node has CentOS 6.5, Hadoop 2.0.0-cdh4.7.0,  Spark 1.0.0, sbt 0.13.5, and Thrift 0.9.1.  The Spark workers each utilizes 21g of memory.    These experiments benchmark Spindle's queries  on a week's worth of data consuming 13.1G as serialized Thrift objects  in Parquet.    The YAML formatted results, scripts, and resulting figures  are in the [benchmark-scripts][benchmark-scripts] directory.    ## Scaling HDFS and Spark workers.  Predicting the optimal resource allocation to minimize query latency for  distributed applications is difficult. No production software can accurately  predict the optimal number of Spark and HDFS nodes for a given application.  This experiment observes the execution time of queries as the number of Spark  and HDFS workers is increased. We manually scale and rebalance the HDFS data.    The following figure shows the time to load all columns the queries  use for the week of data as the Spark and HDFS workers are scaled. The data is  loaded by caching the Spark RDD and performing a null operation on them, such  as `rdd.cache.foreach{x =>{}}`. The downward trend of the data load times  indicate that using more Spark or HDFS workers will decrease the time to load  data.    ![](https://raw.githubusercontent.com/adobe-research/spindle/master/benchmark-scripts/scaling/dataLoad.png)    The following table and plot show the execution time of the queries  with cached data when scaling the HDFS and Spark workers.  The bold data indicates where adding a  Spark and HDFS worker hurts performance. The surprising results show that  adding a single Spark or HDFS worker commonly hurts query performance, and  interestingly, no query experiences minimal execution time when using all 6  workers. Our future work is to further experiment by tuning Spark to understand  the performance degradation, which might be caused by network traffic or  imbalanced workloads.    Q2 and Q3 are similar queries and consequently have similar performance as  scaling the Spark and HDFS workers, but has an anomaly when using 3 workers  where Q2 executes in 17.10s and Q3 executes in 55.15s. Q6â€™s execution time  increases by 10.67 seconds between three and six Spark and HDFS workers.    ![](https://github.com/adobe-research/spindle/raw/master/images/scaling-spark-hdfs.png)  ![](https://raw.githubusercontent.com/adobe-research/spindle/master/benchmark-scripts/scaling/scalingWorkers.png)    ## Intermediate data partitioning.  Spark cannot optimize the number of records in the partitions  because counting the number of records in the initial and  intermediate data sets is expensive, and the  Spark application has to provide the number of partitions  to use for certain computations.  This experiment fully utilizes all six nodes with Spark (144 cores)  and HDFS workers.    Averaging four execution times for each point between  10,000 and 1,500,000 target partition sizes for every query  results in similar performance to the TopPages query (Q4) shown below.    ![](https://github.com/adobe-research/spindle/raw/master/benchmark-scripts/partitions/png/TopPages.png)    Targeting 10,000 records per partition results in poor performance,  which we suspect is due to the Spark overhead of creating an execution  environment for the task, and the performance monotonically decreases  and levels off at a target partition size of 1,500,000.  This experiment fully utilizes all six nodes with Spark (144 cores)  and HDFS workers.    The table below summarizes the results from all queries  by showing the best average execution times for all partitions  and the execution time at a target partition size of 1,500,000.  Q2 and Q3 have nearly identical performance because Q3  only adds a filter to Q2.    | Query | Best Execution Time (s) | Final Execution Time (s) |  |---|---|---|  | TopPages | 3.31 | 3.37 |  | TopPagesByBrowser | 15.41 | 15.58 |  | TopPagesByPreviousTopPages | 34.70 | 36.89 |  | TopReferringDomains | 5.68 | 5.68 |  | RevenueFromTopReferringDomains | 16.66 | 16.661 |  | RevenueFromTopReferringDomainsFirstVisitGoogle | 16.89 | 16.89 |    The remaining experiments use a target partition size of 1,500,000,  and the performance is the best observed for the operations with partitioning.  We expect the support for specifying partitioning for  loading Parquet data from HDFS will yield further performance results.    ## Impact of caching on query execution time.  This experiment shows the ideal speedups from having  all the data in memory as RDD's.  Furthermore, the performances from caching in this experiment  are better than the performances from caching the raw data in memory because  the RDD is cached, and the time to load raw data  into a RDD is non-negligible.    The figure below shows the average execution times from four trials  of every query with and without caching.  Caching the data substantially improves performance, but  reveals that Spindle has further performance bottlenecks inhibiting  subsecond query execution time.  These bottlenecks can be partially overcome by preprocessing the data  and further analyzing Spark internals.  ![](https://github.com/adobe-research/spindle/raw/master/benchmark-scripts/caching/caching.png)    ## Query execution time for concurrent queries.  Spindle's can process concurrent queries with multi-threading, since  many users will use the analytics application concurrently.  Users will request different queries concurrently,  but for simplicity, this experiment shows the performance  degradation as the same query is called with an increasing  number of threads with in-memory caching.    This experiment will spawn a number of threads which continuously  execute the same query.  Each thread remains loaded and continues processing  queries until all threads have processed four queries,  and the average execution time of the first four queries  from every thread will be used as a metric to estimate the  slowdowns.    The performance of the TopPages query below  is indicative of the performance of most queries.  TopPages appears to underutilize the Spark system when  processing in serial, and the Spark schedule is able to process  two queries concurrently and return them as a factor of 1.32 of  the original execution time.    ![](https://github.com/adobe-research/spindle/raw/master/benchmark-scripts/concurrent/png/TopPages.png)    The slowdown factors from serial execution are shown in  the table below for two and eight concurrent queries.    | Query | Serial Time (ms) | 2 Concurrent Slowdown | 8 Concurrent Slowdown |  |---|---|---|---|  | Pageviews | 2.70 | 1.63 | 5.98 |  | TopPages | 3.37 | 1.32 | 5.66 |  | TopPagesByBrowser | 15.93 | 2.02 | 7.58 |  | TopPagesByPreviousTopPages | 37.49 | 1.24 | 4.15 |  | Revenue | 2.74 | 1.53 | 5.82 |  | TopReferringDomains | 5.75 | 1.19 | 4.45 |  | RevenueFromTopReferringDomains | 17.79 | 1.55 | 5.91 |  | RevenueFromTopReferringDomainsFirstVisitGoogle | 16.35 | 1.68 | 7.29 |    This experiment shows the ability of Spark's scheduler at the  small scale of six nodes.  The slowdowns for two concurrent queries indicate further query optimizations  could better balance the work between all Spark workers and  likely result in better query execution time.    # Contributing and Development Status  Spindle is not currently under active development by Adobe.  However, we are happy to review and respond to issues,  questions, and pull requests.    # License  Bundled applications are copyright their respective owners.  [Twitter Bootstrap][bootstrap] and  [dangrossman/bootstrap-daterangepicker][bootstrap-daterangepicker]  are Apache 2.0 licensed  and [rlamana/Terminus][terminus] is MIT licensed.  Diagrams are available in the public domain from  [bamos/beamer-snippets][beamer-snippets].    All other portions are copyright 2014 Adobe Systems Incorporated  under the Apache 2 license, and a copy is provided in `LICENSE`.    [adobe-analytics]: http://www.adobe.com/solutions/digital-analytics.html    [mapreduce]: http://wiki.apache.org/hadoop/MapReduce  [drill]: http://incubator.apache.org/drill/  [impala]: http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html  [spark]: http://spark.apache.org/  [spark-sql]: https://spark.apache.org/sql/  [spark-ec2]: http://spark.apache.org/docs/1.0.0/ec2-scripts.html  [spark-mesos]: http://spark.apache.org/docs/1.0.0/running-on-mesos.html  [spark-yarn]: http://spark.apache.org/docs/1.0.0/running-on-yarn.html  [spark-standalone]: http://spark.apache.org/docs/1.0.0/spark-standalone.html    [apache]: http://www.apache.org/  [hbase]: http://hbase.apache.org/  [cassandra]: http://cassandra.apache.org  [parquet]: http://parquet.io/  [hdfs]: http://hadoop.apache.org/  [thrift]: https://thrift.apache.org/  [thrift-guide]: http://diwakergupta.github.io/thrift-missing-guide/  [avro]: http://avro.apache.org/  [spark-parquet-avro]: http://zenfractal.com/2013/08/21/a-powerful-big-data-trio/  [spray]: http://spray.io  [kryo]: https://github.com/EsotericSoftware/kryo  [fabric]: http://www.fabfile.org/  [puppet]: http://puppetlabs.com/puppet/puppet-open-source    [2014-spark-summit]: http://spark-summit.org/2014  [nbc]: http://www.nbcuni.com/  [nbc-pres]: http://spark-summit.org/wp-content/uploads/2014/06/Using-Spark-to-Generate-Analytics-for-International-Cable-TV-Video-Distribution-Christopher-Burdorf.pdf  [telefonica-pres]: http://spark-summit.org/wp-content/uploads/2014/07/Spark-use-case-at-Telefonica-CBS-Fran-Gomez.pdf  [adam]: https://github.com/bigdatagenomics/adam    [grunt]: http://gruntjs.com/  [ghp]: https://pages.github.com/  [gbc]: https://github.com/robwierzbowski/grunt-build-control  [npm]: https://www.npmjs.org/    [scala]: http://scala-lang.org  [rdd]: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD    [sbt]: http://www.scala-sbt.org/  [sbt-thrift]: https://github.com/bigtoast/sbt-thrift  [sbt-assembly]: https://github.com/sbt/sbt-assembly    [pjson]: https://github.com/adobe-research/spindle/blob/master/package.json  [AnalyticsData.thrift]: https://github.com/adobe-research/spindle/blob/master/src/main/thrift/AnalyticsData.thrift  [benchmark-scipts]: https://github.com/adobe-research/spindle/tree/master/benchmark-scripts    [demo]: http://adobe-research.github.io/spindle/  [spark-parquet-thrift-example]: https://github.com/adobe-research/spark-parquet-thrift-example  [spark-cluster-deployment]: https://github.com/adobe-research/spark-cluster-deployment    [bootstrap]: http://getbootstrap.com/  [terminus]: https://github.com/rlamana/Terminus  [beamer-snippets]: https://github.com/bamos/beamer-snippets  [bootstrap-daterangepicker]: https://github.com/dangrossman/bootstrap-daterangepicker    [spark-jobserver]: https://github.com/ooyala/spark-jobserver  [yarn]: http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html  [spark-yarn]: http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/ """
Big data;https://github.com/Yelp/elastalert;"""Recent changes: As of Elastalert 0.2.0, you must use Python 3.6. Python 2 will not longer be supported.    [![Build Status](https://travis-ci.org/Yelp/elastalert.svg)](https://travis-ci.org/Yelp/elastalert)  [![Join the chat at https://gitter.im/Yelp/elastalert](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/Yelp/elastalert?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    ## ElastAlert - [Read the Docs](http://elastalert.readthedocs.org).  ### Easy & Flexible Alerting With Elasticsearch    ElastAlert is a simple framework for alerting on anomalies, spikes, or other patterns of interest from data in Elasticsearch.    ElastAlert works with all versions of Elasticsearch.    At Yelp, we use Elasticsearch, Logstash and Kibana for managing our ever increasing amount of data and logs.  Kibana is great for visualizing and querying data, but we quickly realized that it needed a companion tool for alerting  on inconsistencies in our data. Out of this need, ElastAlert was created.    If you have data being written into Elasticsearch in near real time and want to be alerted when that data matches certain patterns, ElastAlert is the tool for you. If you can see it in Kibana, ElastAlert can alert on it.    ## Overview    We designed ElastAlert to be reliable, highly modular, and easy to set up and configure.    It works by combining Elasticsearch with two types of components, rule types and alerts.  Elasticsearch is periodically queried and the data is passed to the rule type, which determines when  a match is found. When a match occurs, it is given to one or more alerts, which take action based on the match.    This is configured by a set of rules, each of which defines a query, a rule type, and a set of alerts.    Several rule types with common monitoring paradigms are included with ElastAlert:    - Match where there are at least X events in Y time"" (``frequency`` type)  - Match when the rate of events increases or decreases"" (``spike`` type)  - Match when there are less than X events in Y time"" (``flatline`` type)  - Match when a certain field matches a blacklist/whitelist"" (``blacklist`` and ``whitelist`` type)  - Match on any event matching a given filter"" (``any`` type)  - Match when a field has two different values within some time"" (``change`` type)  - Match when a never before seen term appears in a field"" (``new_term`` type)  - Match when the number of unique values for a field is above or below a threshold (``cardinality`` type)    Currently, we have built-in support for the following alert types:    - Email  - JIRA  - OpsGenie  - Commands  - HipChat  - MS Teams  - Slack  - Telegram  - GoogleChat  - AWS SNS  - VictorOps  - PagerDuty  - PagerTree  - Exotel  - Twilio  - Gitter  - Line Notify  - Zabbix    Additional rule types and alerts can be easily imported or written.    In addition to this basic usage, there are many other features that make alerts more useful:    - Alerts link to Kibana dashboards  - Aggregate counts for arbitrary fields  - Combine alerts into periodic reports  - Separate alerts by using a unique key field  - Intercept and enhance match data    To get started, check out `Running ElastAlert For The First Time` in the [documentation](http://elastalert.readthedocs.org).    ## Running ElastAlert  You can either install the latest released version of ElastAlert using pip:    ```pip install elastalert```    or you can clone the ElastAlert repository for the most recent changes:    ```git clone https://github.com/Yelp/elastalert.git```    Install the module:    ```pip install ""setuptools>=11.3""```    ```python setup.py install```    The following invocation can be used to run ElastAlert after installing    ``$ elastalert [--debug] [--verbose] [--start <timestamp>] [--end <timestamp>] [--rule <filename.yaml>] [--config <filename.yaml>]``    ``--debug`` will print additional information to the screen as well as suppresses alerts and instead prints the alert body. Not compatible with `--verbose`.    ``--verbose`` will print additional information without suppressing alerts. Not compatible with `--debug.`    ``--start`` will begin querying at the given timestamp. By default, ElastAlert will begin querying from the present.  Timestamp format is ``YYYY-MM-DDTHH-MM-SS[-/+HH:MM]`` (Note the T between date and hour).  Eg: ``--start 2014-09-26T12:00:00`` (UTC) or ``--start 2014-10-01T07:30:00-05:00``    ``--end`` will cause ElastAlert to stop querying at the given timestamp. By default, ElastAlert will continue  to query indefinitely.    ``--rule`` will allow you to run only one rule. It must still be in the rules folder.  Eg: ``--rule this_rule.yaml``    ``--config`` allows you to specify the location of the configuration. By default, it is will look for config.yaml in the current directory.    ## Third Party Tools And Extras  ### Kibana plugin  ![img](https://raw.githubusercontent.com/bitsensor/elastalert-kibana-plugin/master/showcase.gif)  Available at the [ElastAlert Kibana plugin repository](https://github.com/bitsensor/elastalert-kibana-plugin).    ### Docker  A [Dockerized version](https://github.com/bitsensor/elastalert) of ElastAlert including a REST api is build from `master` to `bitsensor/elastalert:latest`.    ```bash  git clone https://github.com/bitsensor/elastalert.git; cd elastalert  docker run -d -p 3030:3030 \      -v `pwd`/config/elastalert.yaml:/opt/elastalert/config.yaml \      -v `pwd`/config/config.json:/opt/elastalert-server/config/config.json \      -v `pwd`/rules:/opt/elastalert/rules \      -v `pwd`/rule_templates:/opt/elastalert/rule_templates \      --net=""host"" \      --name elastalert bitsensor/elastalert:latest  ```    ## Documentation    Read the documentation at [Read the Docs](http://elastalert.readthedocs.org).    To build a html version of the docs locally    ```  pip install sphinx_rtd_theme sphinx  cd docs  make html  ```    View in browser at build/html/index.html    ## Configuration    See config.yaml.example for details on configuration.    ## Example rules    Examples of different types of rules can be found in example_rules/.    - ``example_spike.yaml`` is an example of the ""spike"" rule type, which allows you to alert when the rate of events, averaged over a time period,  increases by a given factor. This example will send an email alert when there are 3 times more events matching a filter occurring within the  last 2 hours than the number of events in the previous 2 hours.    - ``example_frequency.yaml`` is an example of the ""frequency"" rule type, which will alert when there are a given number of events occuring  within a time period. This example will send an email when 50 documents matching a given filter occur within a 4 hour timeframe.    - ``example_change.yaml`` is an example of the ""change"" rule type, which will alert when a certain field in two documents changes. In this example,  the alert email is sent when two documents with the same 'username' field but a different value of the 'country_name' field occur within 24 hours  of each other.    - ``example_new_term.yaml`` is an example of the ""new term"" rule type, which alerts when a new value appears in a field or fields. In this example,  an email is sent when a new value of (""username"", ""computer"") is encountered in example login logs.    ## Frequently Asked Questions    ### My rule is not getting any hits?    So you've managed to set up ElastAlert, write a rule, and run it, but nothing happens, or it says ``0 query hits``. First of all, we recommend using the command ``elastalert-test-rule rule.yaml`` to debug. It will show you how many documents match your filters for the last 24 hours (or more, see ``--help``), and then shows you if any alerts would have fired. If you have a filter in your rule, remove it and try again. This will show you if the index is correct and that you have at least some documents. If you have a filter in Kibana and want to recreate it in ElastAlert, you probably want to use a query string. Your filter will look like    ```  filter:  - query:      query_string:        query: ""foo: bar AND baz: abc*""  ```  If you receive an error that Elasticsearch is unable to parse it, it's likely the YAML is not spaced correctly, and the filter is not in the right format. If you are using other types of filters, like ``term``, a common pitfall is not realizing that you may need to use the analyzed token. This is the default if you are using Logstash. For example,    ```  filter:  - term:      foo: ""Test Document""  ```    will not match even if the original value for ``foo`` was exactly ""Test Document"". Instead, you want to use ``foo.raw``. If you are still having trouble troubleshooting why your documents do not match, try running ElastAlert with ``--es_debug_trace /path/to/file.log``. This will log the queries made to Elasticsearch in full so that you can see exactly what is happening.    ### I got hits, why didn't I get an alert?    If you got logs that had ``X query hits, 0 matches, 0 alerts sent``, it depends on the ``type`` why you didn't get any alerts. If ``type: any``, a match will occur for every hit. If you are using ``type: frequency``, ``num_events`` must occur within ``timeframe`` of each other for a match to occur. Different rules apply for different rule types.    If you see ``X matches, 0 alerts sent``, this may occur for several reasons. If you set ``aggregation``, the alert will not be sent until after that time has elapsed. If you have gotten an alert for this same rule before, that rule may be silenced for a period of time. The default is one minute between alerts. If a rule is silenced, you will see ``Ignoring match for silenced rule`` in the logs.    If you see ``X alerts sent`` but didn't get any alert, it's probably related to the alert configuration. If you are using the ``--debug`` flag, you will not receive any alerts. Instead, the alert text will be written to the console. Use ``--verbose`` to achieve the same affects without preventing alerts. If you are using email alert, make sure you have it configured for an SMTP server. By default, it will connect to localhost on port 25. It will also use the word ""elastalert"" as the ""From:"" address. Some SMTP servers will reject this because it does not have a domain while others will add their own domain automatically. See the email section in the documentation for how to configure this.    ### Why did I only get one alert when I expected to get several?    There is a setting called ``realert`` which is the minimum time between two alerts for the same rule. Any alert that occurs within this time will simply be dropped. The default value for this is one minute. If you want to receive an alert for every single match, even if they occur right after each other, use    ```  realert:    minutes: 0  ```    You can of course set it higher as well.    ### How can I prevent duplicate alerts?    By setting ``realert``, you will prevent the same rule from alerting twice in an amount of time.    ```  realert:    days: 1  ```    You can also prevent duplicates based on a certain field by using ``query_key``. For example, to prevent multiple alerts for the same user, you might use    ```  realert:    hours: 8  query_key: user  ```    Note that this will also affect the way many rule types work. If you are using ``type: frequency`` for example, ``num_events`` for a single value of ``query_key`` must occur before an alert will be sent. You can also use a compound of multiple fields for this key. For example, if you only wanted to receieve an alert once for a specific error and hostname, you could use    ```  query_key: [error, hostname]  ```    Internally, this works by creating a new field for each document called ``field1,field2`` with a value of ``value1,value2`` and using that as the ``query_key``.    The data for when an alert will fire again is stored in Elasticsearch in the ``elastalert_status`` index, with a ``_type`` of ``silence`` and also cached in memory.    ### How can I change what's in the alert?    You can use the field ``alert_text`` to add custom text to an alert. By setting ``alert_text_type: alert_text_only``, it will be the entirety of the alert. You can also add different fields from the alert by using Python style string formatting and ``alert_text_args``. For example    ```  alert_text: ""Something happened with {0} at {1}""  alert_text_type: alert_text_only  alert_text_args: [""username"", ""@timestamp""]  ```    You can also limit the alert to only containing certain fields from the document by using ``include``.    ```  include: [""ip_address"", ""hostname"", ""status""]  ```    ### My alert only contains data for one event, how can I see more?    If you are using ``type: frequency``, you can set the option ``attach_related: true`` and every document will be included in the alert. An alternative, which works for every type, is ``top_count_keys``. This will show the top counts for each value for certain fields. For example, if you have    ```  top_count_keys: [""ip_address"", ""status""]  ```    and 10 documents matched your alert, it may contain something like    ```  ip_address:  127.0.0.1: 7  10.0.0.1: 2  192.168.0.1: 1    status:  200: 9  500: 1  ```    ### How can I make the alert come at a certain time?    The ``aggregation`` feature will take every alert that has occured over a period of time and send them together in one alert. You can use cron style syntax to send all alerts that have occured since the last once by using    ```  aggregation:    schedule: '2 4 * * mon,fri'  ```    ### I have lots of documents and it's really slow, how can I speed it up?    There are several ways to potentially speed up queries. If you are using ``index: logstash-*``, Elasticsearch will query all shards, even if they do not possibly contain data with the correct timestamp. Instead, you can use Python time format strings and set ``use_strftime_index``    ```  index: logstash-%Y.%m  use_strftime_index: true  ```    Another thing you could change is ``buffer_time``. By default, ElastAlert will query large overlapping windows in order to ensure that it does not miss any events, even if they are indexed in real time. In config.yaml, you can adjust ``buffer_time`` to a smaller number to only query the most recent few minutes.    ```  buffer_time:    minutes: 5  ```    By default, ElastAlert will download every document in full before processing them. Instead, you can have ElastAlert simply get a count of the number of documents that have occured in between each query. To do this, set ``use_count_query: true``. This cannot be used if you use ``query_key``, because ElastAlert will not know the contents of each documents, just the total number of them. This also reduces the precision of alerts, because all events that occur between each query will be rounded to a single timestamp.    If you are using ``query_key`` (a single key, not multiple keys) you can use ``use_terms_query``. This will make ElastAlert perform a terms aggregation to get the counts for each value of a certain field. Both ``use_terms_query`` and ``use_count_query`` also require ``doc_type`` to be set to the ``_type`` of the documents. They may not be compatible with all rule types.    ### Can I perform aggregations?    The only aggregation supported currently is a terms aggregation, by setting ``use_terms_query``.    ### I'm not using @timestamp, what do I do?    You can use ``timestamp_field`` to change which field ElastAlert will use as the timestamp. You can use ``timestamp_type`` to change it between ISO 8601 and unix timestamps. You must have some kind of timestamp for ElastAlert to work. If your events are not in real time, you can use ``query_delay`` and ``buffer_time`` to adjust when ElastAlert will look for documents.    ### I'm using flatline but I don't see any alerts    When using ``type: flatline``, ElastAlert must see at least one document before it will alert you that it has stopped seeing them.    ### How can I get a ""resolve"" event?    ElastAlert does not currently support stateful alerts or resolve events.    ### Can I set a warning threshold?    Currently, the only way to set a warning threshold is by creating a second rule with a lower threshold.    ## License    ElastAlert is licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0    ### Read the documentation at [Read the Docs](http://elastalert.readthedocs.org).    ### Questions? Drop by #elastalert on Freenode IRC. """
Big data;https://github.com/allegro/hermes;"""Hermes  ======    [![Build Status](https://github.com/allegro/hermes/actions/workflows/ci.yml/badge.svg?branch=master)](https://github.com/allegro/hermes/actions/workflows/ci.yml?query=branch%3Amaster)  [![Documentation Status](https://readthedocs.org/projects/hermes-pubsub/badge/?version=latest)](https://readthedocs.org/projects/hermes-pubsub/?badge=latest)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/pl.allegro.tech.hermes/hermes-client/badge.svg)](https://maven-badges.herokuapp.com/maven-central/pl.allegro.tech.hermes/hermes-client)  [![Join the chat](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/allegro/hermes?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    Hermes is an asynchronous message broker built on top of [Kafka](http://kafka.apache.org/).  We provide reliable, fault tolerant REST interface for message publishing and adaptive push  mechanisms for message sending.    See our 10-minute getting started guide with Vagrant: [Getting started](http://hermes-pubsub.readthedocs.org/en/latest/quickstart/)    Visit our page: [hermes.allegro.tech](http://hermes.allegro.tech)    See our full documentation: [http://hermes-pubsub.readthedocs.org/en/latest/](http://hermes-pubsub.readthedocs.org/en/latest/)    Questions? We are on [gitter](https://gitter.im/allegro/hermes).    ## License    **hermes** is published under [Apache License 2.0](http://www.apache.org/licenses/LICENSE-2.0). """
Big data;https://github.com/materializeinc/materialize;"""[![Build status](https://badge.buildkite.com/97d6604e015bf633d1c2a12d166bb46f3b43a927d3952c999a.svg?branch=main)](https://buildkite.com/materialize/tests)  [![Doc reference](https://img.shields.io/badge/doc-reference-orange)](https://materialize.com/docs)  [![Chat on Slack](https://img.shields.io/badge/chat-on%20slack-purple)](https://materialize.com/s/chat)    [<img src=""https://materialize.com/wp-content/uploads/2020/01/materialize_logo_primary.png"" width=60%>](https://materialize.com)    Materialize is a streaming database for real-time applications.    ## Get started    Check out [our getting started guide](https://materialize.com/docs/get-started/).    ## About    Materialize lets you ask questions of your live data, which it answers and then maintains for you as your data continue to change. The moment you need a refreshed answer, you can get it in milliseconds. Materialize is designed to help you interactively explore your streaming data, perform data warehousing analytics against live relational data, or just increase the freshness *and* reduce the load of your dashboard and monitoring tasks.    Materialize focuses on providing correct and consistent answers with minimal latency. It does not ask you to accept either approximate answers or eventual consistency. Whenever Materialize answers a query, that answer is the correct result on some specific (and recent) version of your data. Materialize does all of this by recasting your SQL92 queries as *dataflows*, which can react efficiently to changes in your data as they happen. Materialize is powered by [timely dataflow](https://github.com/TimelyDataflow/timely-dataflow), which connects the times at which your inputs change with the times of answers reported back to you.    We support a large fraction of  PostgreSQL, and are actively working on supporting more builtin PostgreSQL functions. Please file an issue if there's something that you expected to work that didn't!    ## Get data in    Materialize reads Avro, Protobuf, JSON, and newline-delimited text. Need something else? [Just ask](https://github.com/MaterializeInc/materialize/issues/new/choose).    Materialize can read data from Kafka topics, Kinesis streams (in preview), or tail local files.    ## Transform, manipulate, and read your data    Once you've got the data in, define views and perform reads via the PostgreSQL protocol. Use your favorite PostgreSQL CLI, including the `psql` you probably already have on your system.    Materialize supports a comprehensive variety of SQL features, all using the PostgreSQL dialect and protocol:    -   Joins, Joins, Joins! Materialize supports multi-column join conditions, multi-way joins, self-joins, cross-joins, inner joins, outer joins, etc.  -   Delta-joins avoid intermediate state blowup compared to systems that can only plan nested binary joins - tested on joins of up to 64 relations.  -   Support for subqueries. Materialize's SQL optimizer performs subquery decorrelation out-of-the-box, avoiding the need to manually rewrite subqueries into joins.  -   Materialize supports streams that contain CDC data (currently supporting the [Debezium](https://debezium.io/blog/2017/09/25/streaming-to-another-database/) format). Materialize can incrementally maintain views in the presence of arbitrary inserts, updates, and deletes. No asterisks.  -   All the aggregations. `GROUP BY` , `MIN`, `MAX`, `COUNT`, `SUM`, `STDDEV`, `HAVING`, etc.  -   `ORDER BY`  -   `LIMIT`  -   `DISTINCT`  -   JSON support in the PostgreSQL dialect including operators and functions like `->`, `->>`, `@>`, `?`, `jsonb_array_element`, `jsonb_each`. Materialize automatically plans lateral joins for efficient `jsonb_each` support.  -   Nest views on views on views!  -   Multiple views that have overlapping subplans can share underlying indices for space and compute efficiency, so just declaratively define _what you want_, and we'll worry about how to efficiently maintain them.    ### Just show us what it can do!    Here's an example join query that works fine in Materialize, `TPC-H` query 15:    ```sql  -- Views define commonly reused subqueries.  CREATE VIEW revenue (supplier_no, total_revenue) AS      SELECT          l_suppkey,          SUM(l_extendedprice * (1 - l_discount))      FROM          lineitem      WHERE          l_shipdate >= DATE '1996-01-01'          AND l_shipdate < DATE '1996-01-01' + INTERVAL '3' month      GROUP BY          l_suppkey;    -- Materialized views are maintained automatically, and can depend on non-materialized views.  CREATE MATERIALIZED VIEW tpch_q15 AS    SELECT      s_suppkey,      s_name,      s_address,      s_phone,      total_revenue  FROM      supplier,      revenue  WHERE      s_suppkey = supplier_no      AND total_revenue = (          SELECT              max(total_revenue)          FROM              revenue      )  ORDER BY      s_suppkey  ```    Stream inserts, updates, and deletes on the underlying tables (`lineitem` and `supplier`), and Materialize keeps the materialized view incrementally updated. You can type `SELECT * FROM tpch_q15` and expect to see the current results immediately!    ## Get data out    **Pull based**: Use any PostgreSQL-compatible driver in any language/environment to make `SELECT` queries against your views. Tell them they're talking to a PostgreSQL database, they don't ever need to know otherwise.    **Push based**: Or configure Materialize to stream results to a Kafka topic as soon as the views change.    If you want to use an ORM, [chat with us](https://github.com/MaterializeInc/materialize/issues/new/choose). They're surprisingly tricky.    ## Documentation    Check out [our documentation](https://materialize.com/docs/).    ## License    Materialize is source-available and [licensed](LICENSE) under the BSL 1.1, converting to the open-source Apache 2.0 license after 4 years. As stated in the BSL, Materialize is free forever on a single node.    Materialize is also available as [a paid cloud service](https://materialize.com/pricing/) with additional features such as high availability via multi-active replication.    ## How does it work?    Materialize is built on top of [differential dataflow](https://github.com/TimelyDataflow/differential-dataflow) and [timely dataflow](https://github.com/TimelyDataflow/timely-dataflow), and builds on a decade of cutting-edge stream processing research.    ## For developers    Materialize is written entirely in Rust.    Developers can find docs at [doc/developer](doc/developer), and Rust API documentation is hosted at <https://dev.materialize.com/api/rust/>. The Materialize development roadmap is divided up into roughly month-long milestones, and [managed in GitHub](https://github.com/MaterializeInc/materialize/milestones?direction=asc&sort=due_date&state=open).    Contributions are welcome. Prospective code contributors might find the [good first issue tag](https://github.com/MaterializeInc/materialize/issues?q=is%3Aopen+is%3Aissue+label%3A%22D-good+first+issue%22) useful. We value all contributions equally, but bug reports are more equal.    ## Credits    Materialize is lovingly crafted by [a team of developers](https://github.com/MaterializeInc/materialize/graphs/contributors) and one bot. [Join us](https://materialize.com/careers/). """
Big data;https://github.com/linkedin/cleo;"""What is Cleo?  =======================    Cleo is a flexible software library for enabling rapid development of partial, out-of-order and real-time typeahead search.  It is suitable for data sets of varying sizes and types. Cleo has been used extensively to power LinkedIn typeahead search  covering professional network connections, companies, groups, questions, skills and other site features.    ### Homepage:    Find out more about Cleo at: http://sna-projects.com/cleo    ### License:    Apache Public License (APL) 2.0    ### Artifacts:    1. cleo.jar <-- core library    ### Maven:    groupId: com.sna-projects.cleo    artifactId: cleo    version: 1.2.6    ### Code Examples:    https://github.com/linkedin/cleo/tree/master/src/examples    https://github.com/jingwei/cleo-primer    ### Eclipse:    Set up Eclipse for Cleo by executing the command below:    mvn eclipse:eclipse    Inside Eclipse, select Preferences > Java > Build Path > Classpath Variables. Define a new classpath variable M2_REPO and assign maven repository.    For more information, check out http://maven.apache.org/guides/mini/guide-ide-eclipse.html   """
Big data;https://github.com/NationalSecurityAgency/timely;"""![Timely](timely-readme-logo.png)    [![Apache License][li]][ll]    Timely is a time series database application that provides secure access to time series data. Timely is written in Java and designed to work with [Apache Accumulo](https://accumulo.apache.org/) and [Grafana](https://www.grafana.com). Documentation is located [here](https://nationalsecurityagency.github.io/timely/).    [li]: https://img.shields.io/badge/license-ASL-blue.svg  [ll]: https://www.apache.org/licenses/LICENSE-2.0 """
Big data;https://github.com/okfn/recline;"""Present a (Frictionless) dataset for viewing and exploration.    ## Install    Git clone then:    ```  yarn install  ```    ## Usage    In this directory:    ```bash  export PORTAL_DATASET_PATH=/path/to/my/dataset  yarn dev  ```    And you will get a nice dataset page at `http://localhost:3000`    ![](https://i.imgur.com/KSEtNF1.png) """
Big data;https://github.com/benedekrozemberczki/awesome-decision-tree-papers;"""# Awesome Decision Tree Research Papers  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)  [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)  [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/awesome-decision-tree-papers.svg)](https://github.com/benedekrozemberczki/awesome-decision-tree-papers/archive/master.zip)   ![License](https://img.shields.io/github/license/benedekrozemberczki/awesome-decision-tree-papers.svg?color=blue) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)  <p align=""center"">    <img width=""300"" src=""tree.png"">  </p>    A curated list of classification and regression tree research papers with implementations from the following conferences:    - Machine learning     * [NeurIPS](https://nips.cc/)      * [ICML](https://icml.cc/)      * [ICLR](https://iclr.cc/)  - Computer vision     * [CVPR](http://cvpr2019.thecvf.com/)     * [ICCV](http://iccv2019.thecvf.com/)     * [ECCV](https://eccv2018.org/)  - Natural language processing     * [ACL](http://www.acl2019.org/EN/index.xhtml)     * [NAACL](https://naacl2019.org/)     * [EMNLP](https://www.emnlp-ijcnlp2019.org/)   - Data     * [KDD](https://www.kdd.org/)     * [CIKM](http://www.cikmconference.org/)        * [ICDM](http://icdm2019.bigke.org/)     * [SDM](https://www.siam.org/Conferences/CM/Conference/sdm19)        * [PAKDD](http://pakdd2019.medmeeting.org)     * [PKDD/ECML](http://ecmlpkdd2019.org)     * [SIGIR](https://sigir.org/)     * [WWW](https://www2019.thewebconf.org/)     * [WSDM](www.wsdm-conference.org)   - Artificial intelligence     * [AAAI](https://www.aaai.org/)     * [AISTATS](https://www.aistats.org/)     * [ICANN](https://e-nns.org/icann2019/)        * [IJCAI](https://www.ijcai.org/)     * [UAI](http://www.auai.org/)    Similar collections about [graph classification](https://github.com/benedekrozemberczki/awesome-graph-classification), [gradient boosting](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers), [fraud detection](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers), [Monte Carlo tree search](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers), and [community detection](https://github.com/benedekrozemberczki/awesome-community-detection) papers with implementations.      ## 2021    - **Online Probabilistic Label Trees (AISTATS 2021)**    - Kalina Jasinska-Kobus, Marek Wydmuch, Devanathan Thiruvenkatachari, Krzysztof DembczyÅ„ski    - [[Paper]](https://arxiv.org/abs/2007.04451)    - [[Code]](https://github.com/mwydmuch/napkinXC)    - **Optimal Decision Trees for Nonlinear Metrics (AAAI 2021)**    - Emir Demirovic, Peter J. Stuckey    - [[Paper]](https://arxiv.org/abs/2009.06921)    - **SAT-based Decision Tree Learning for Large Data Sets (AAAI 2021)**    - AndrÃ© Schidler, Stefan Szeider    - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/16509)    - **Parameterized Complexity of Small Decision Tree Learning (AAAI 2021)**    - Sebastian Ordyniak, Stefan Szeider    - [[Paper]](https://www.ac.tuwien.ac.at/files/tr/ac-tr-21-002.pdf)    - **Counterfactual Explanations for Oblique Decision Trees: Exact - Efficient Algorithms (AAAI 2021)**    - Miguel Ã. Carreira-PerpiÃ±Ã¡n, Suryabhan Singh Hada    - [[Paper]](https://arxiv.org/abs/2103.01096)    - **Geometric Heuristics for Transfer Learning in Decision Trees (CIKM 2021)**    - Siddhesh Chaubal, Mateusz Rzepecki, Patrick K. Nicholson, Guangyuan Piao, Alessandra Sala    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3459637.3482259)    - **Fairness-Aware Training of Decision Trees by Abstract Interpretation (CIKM 2021)**    - Francesco Ranzato, Caterina Urban, Marco Zanella    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3459637.3482342)    - **Enabling Efficiency-Precision Trade-offs for Label Trees in Extreme Classification (CIKM 2021)**    - Tavor Z. Baharav, Daniel L. Jiang, Kedarnath Kolluri, Sujay Sanghavi, Inderjit S. Dhillon    - [[Paper]](https://arxiv.org/abs/2106.00730)    - **Are Neural Rankers still Outperformed by Gradient Boosted Decision Trees (ICLR 2021)**    - Zhen Qin, Le Yan, Honglei Zhuang, Yi Tay, Rama Kumar Pasumarthi, Xuanhui Wang, Michael Bendersky, Marc Najork    - [[Paper]](https://openreview.net/forum?id=Ut1vF_q_vC)    - **NBDT: Neural-Backed Decision Tree (ICLR 2021)**    - Alvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee, Suzanne Petryk, Sarah Adel Bargal, Joseph E. Gonzalez    - [[Paper]](https://arxiv.org/abs/2004.00221)    - **Versatile Verification of Tree Ensembles (ICML 2021)**    - Laurens Devos, Wannes Meert, Jesse Davis    - [[Paper]](https://arxiv.org/abs/2010.13880)    - **Connecting Interpretability and Robustness in Decision Trees through Separation (ICML 2021)**    - Michal Moshkovitz, Yao-Yuan Yang, Kamalika Chaudhuri    - [[Paper]](https://arxiv.org/abs/2102.07048)    - **Optimal Counterfactual Explanations in Tree Ensembles (ICML 2021)**    - Axel Parmentier, Thibaut Vidal    - [[Paper]](https://arxiv.org/abs/2106.06631)    - **Efficient Training of Robust Decision Trees Against Adversarial Examples (ICML 2021)**    - DaniÃ«l Vos, Sicco Verwer    - [[Paper]](https://arxiv.org/abs/2012.10438)    - **Learning Binary Decision Trees by Argmin Differentiation (ICML 2021)**    - Valentina Zantedeschi, Matt J. Kusner, Vlad Niculae    - [[Paper]](https://arxiv.org/pdf/2010.04627.pdf)     - **BLOCKSET (Block-Aligned Serialized Trees): Reducing Inference Latency for Tree ensemble Deployment (KDD 2021)**    - Meghana Madhyastha, Kunal Lillaney, James Browne, Joshua T. Vogelstein, Randal Burns    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3447548.3467368)    - **ControlBurn: Feature Selection by Sparse Forests (KDD 2021)**    - Brian Liu, Miaolan Xie, Madeleine Udell    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3447548.3467387?sid=SCITRUS)    - **Probabilistic Gradient Boosting Machines for Large-Scale Probabilistic Regression (KDD 2021)**    - Olivier Sprangers, Sebastian Schelter, Maarten de Rijke    - [[Paper]](https://dl.acm.org/doi/10.1145/3447548.3467278)    - **Verifying Tree Ensembles by Reasoning about Potential Instances (SDM 2021)**    - Laurens Devos, Wannes Meert, Jesse Davis    - [[Paper]](https://arxiv.org/abs/2001.11905)    ## 2020    - **DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim Verification (ACL 2020)**    - Lianwei Wu, Yuan Rao, Yongqiang Zhao, Hao Liang, Ambreen Nazir    - [[Paper]](https://arxiv.org/abs/2004.13455)    - **Privacy-Preserving Gradient Boosting Decision Trees (AAAI 2020)**    - Qinbin Li, Zhaomin Wu, Zeyi Wen, Bingsheng He    - [[Paper]](https://arxiv.org/abs/1911.04209)    - **Practical Federated Gradient Boosting Decision Trees (AAAI 2020)**    - Qinbin Li, Zeyi Wen, Bingsheng He    - [[Paper]](https://arxiv.org/abs/1911.04206)    - **Efficient Inference of Optimal Decision Trees (AAAI 2020)**    - Florent Avellaneda    - [[Paper]](http://florent.avellaneda.free.fr/dl/AAAI20.pdf)    - [[Code]](https://github.com/FlorentAvellaneda/InferDT)    - **Learning Optimal Decision Trees Using Caching Branch-and-Bound Search (AAAI 2020)**    - Gael Aglin, Siegfried Nijssen, Pierre Schaus    - [[Paper]](https://dial.uclouvain.be/pr/boreal/fr/object/boreal%3A223390/datastream/PDF_01/view)    - [[Code]](https://pypi.org/project/dl8.5/)    - **Abstract Interpretation of Decision Tree Ensemble Classifiers (AAAI 2020)**    - Francesco Ranzato, Marco Zanella    - [[Paper]](https://www.math.unipd.it/~ranzato/papers/aaai20.pdf)    - [[Code]](https://github.com/abstract-machine-learning/silva)    - **Scalable Feature Selection for (Multitask) Gradient Boosted Trees (AISTATS 2020)**    - Cuize Han, Nikhil Rao, Daria Sorokina, Karthik Subbian    - [[Paper]](http://proceedings.mlr.press/v108/han20a.html)    - **Optimization Methods for Interpretable Differentiable Decision Trees Applied to Reinforcement Learning (AISTATS 2020)**    - Andrew Silva, Matthew C. Gombolay, Taylor W. Killian, Ivan Dario Jimenez Jimenez, Sung-Hyun Son    - [[Paper]](https://arxiv.org/abs/1903.09338)    - **Exploiting Categorical Structure Using Tree-Based Methods (AISTATS 2020)**    - Brian Lucena    - [[Paper]](https://arxiv.org/abs/2004.07383)    - **LdSM: Logarithm-depth Streaming Multi-label Decision Trees (AISTATS 2020)**    - Maryam Majzoubi, Anna Choromanska    - [[Paper]](https://arxiv.org/abs/1905.10428)      - **Oblique Decision Trees from Derivatives of ReLU Networks (ICLR 2020)**    - Guang-He Lee, Tommi S. Jaakkola    - [[Paper]](https://openreview.net/pdf?id=Bke8UR4FPB)    - [[Code]](https://github.com/guanghelee/iclr20-lcn)      - **Provable Guarantees for Decision Tree Induction: the Agnostic Setting (ICML 2020)**    - Guy Blanc, Jane Lange, Li-Yang Tan    - [[Paper]](https://arxiv.org/abs/2006.00743v1)    - **Decision Trees for Decision-Making under the Predict-then-Optimize Framework (ICML 2020)**    - Adam N. Elmachtoub, Jason Cheuk Nam Liang, Ryan McNellis    - [[Paper]](https://arxiv.org/abs/2003.00360)    - **The Tree Ensemble Layer: Differentiability meets Conditional Computation (ICML 2020)**    - Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, Rahul Mazumder    - [[Paper]](https://arxiv.org/abs/2002.07772)    - [[Code]](https://github.com/google-research/google-research/tree/master/tf_trees)    - **Generalized and Scalable Optimal Sparse Decision Trees (ICML 2020)**    - Jimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, Margo I. Seltzer    - [[Paper]](https://arxiv.org/abs/2006.08690)    - [[Code]](https://github.com/xiyanghu/OSDT)    - **Born-Again Tree Ensembles (ICML 2020)**    - Thibaut Vidal, Maximilian Schiffer    - [[Paper]](https://arxiv.org/abs/2003.11132)    - [[Code]](https://github.com/vidalt/BA-Trees)    - **On Lp-norm Robustness of Ensemble Decision Stumps and Trees (ICML 2020)**    - Yihan Wang, Huan Zhang, Hongge Chen, Duane S. Boning, Cho-Jui Hsieh    - [[Paper]](https://arxiv.org/abs/2008.08755)    - **Smaller, More Accurate Regression Forests Using Tree Alternating Optimization (ICML 2020)**    - Arman Zharmagambetov, Miguel Ã. Carreira-Perpinan    - [[Paper]](http://proceedings.mlr.press/v119/zharmagambetov20a.html)      - **Learning Optimal Decision Trees with MaxSAT and its Integration in AdaBoost (IJCAI 2020)**    - Hao Hu, Mohamed Siala, Emmanuel Hebrard, Marie-JosÃ© Huguet    - [[Paper]](https://www.ijcai.org/Proceedings/2020/163)    - **Speeding up Very Fast Decision Tree with Low Computational Cost (IJCAI 2020)**    - Jian Sun, Hongyu Jia, Bo Hu, Xiao Huang, Hao Zhang, Hai Wan, Xibin Zhao    - [[Paper]](https://www.ijcai.org/Proceedings/2020/0177.pdf)    - **PyDL8.5: a Library for Learning Optimal Decision Trees (IJCAI 2020)**    - GaÃ«l Aglin, Siegfried Nijssen, Pierre Schaus    - [[Paper]](https://www.ijcai.org/Proceedings/2020/0750.pdf)    - [[Code]](https://github.com/aia-uclouvain/pydl8.5)    - **Geodesic Forests (KDD 2020)**    - Meghana Madhyastha, Gongkai Li, Veronika Strnadova-Neeley, James Browne, Joshua T. Vogelstein, Randal Burns    - [[Paper]](https://dl.acm.org/doi/pdf/10.1145/3394486.3403094)    - **A Scalable MIP-based Method for Learning Optimal Multivariate Decision Trees (NeurIPS 2020)**    - Haoran Zhu, Pavankumar Murali, Dzung T. Phan, Lam M. Nguyen, Jayant Kalagnanam    - [[Paper]](https://arxiv.org/abs/2011.03375)    - **Estimating Decision Tree Learnability with Polylogarithmic Sample Complexity (NeurIPS 2020)**    - Guy Blanc, Neha Gupta, Jane Lange, Li-Yang Tan    - [[Paper]](https://arxiv.org/abs/2011.01584)     - **Universal Guarantees for Decision Tree Induction via a Higher-Order Splitting Criterion (NeurIPS 2020)**    - Guy Blanc, Neha Gupta, Jane Lange, Li-Yang Tan    - [[Paper]](https://arxiv.org/abs/2010.08633)    - **Smooth And Consistent Probabilistic Regression Trees (NeurIPS 2020)**    - Sami Alkhoury, Emilie Devijver, Marianne Clausel, Myriam Tami, Ã‰ric Gaussier, Georges Oppenheim    - [[Paper]](https://papers.nips.cc/paper/2020/file/8289889263db4a40463e3f358bb7c7a1-Paper.pdf)    - **An Efficient Adversarial Attack for Tree Ensembles (NeurIPS 2020)**    - Chong Zhang, Huan Zhang, Cho-Jui Hsieh    - [[Paper]](https://arxiv.org/abs/2010.11598)    - [[Code]](https://github.com/chong-z/tree-ensemble-attack)    - **Decision Trees as Partitioning Machines to Characterize their Generalization Properties (NeurIPS 2020)**    - Jean-Samuel Leboeuf, FrÃ©dÃ©ric Leblanc, Mario Marchand    - [[Paper]](https://papers.nips.cc/paper/2020/file/d2a10b0bd670e442b1d3caa3fbf9e695-Paper.pdf)      - **Evidence Weighted Tree Ensembles for Text Classification (SIGIR 2020)**    - Md. Zahidul Islam, Jixue Liu, Jiuyong Li, Lin Liu, Wei Kang    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3397271.3401229)      ## 2019    - **Multi-Level Deep Cascade Trees for Conversion Rate Prediction in Recommendation System (AAAI 2019)**    - Hong Wen, Jing Zhang, Quan Lin, Keping Yang, Pipei Huang    - [[Paper]](https://arxiv.org/pdf/1805.09484.pdf)      - **Induction of Non-Monotonic Logic Programs to Explain Boosted Tree Models Using LIME (AAAI 2019)**    - Farhad Shakerin, Gopal Gupta    - [[Paper]](https://arxiv.org/abs/1808.00629)      - **Learning Optimal and Fair Decision Trees for Non-Discriminative Decision-Making (AAAI 2019)**    - Sina Aghaei, Mohammad Javad Azizi, Phebe Vayanos    - [[Paper]](https://arxiv.org/abs/1903.10598)    - **Desiderata for Interpretability: Explaining Decision Tree Predictions with Counterfactuals (AAAI 2019)**    - Kacper Sokol, Peter A. Flach    - [[Paper]](https://aaai.org/ojs/index.php/AAAI/article/view/5154)      - **Weighted Oblique Decision Trees (AAAI 2019)**    - Bin-Bin Yang, Song-Qing Shen, Wei Gao    - [[Paper]](https://aaai.org/ojs/index.php/AAAI/article/view/4505)    - **Learning Optimal Classification Trees Using a Binary Linear Program Formulation (AAAI 2019)**    - Sicco Verwer, Yingqian Zhang    - [[Paper]](https://yingqianzhang.net/wp-content/uploads/2018/12/VerwerZhangAAAI-final.pdf)    - **Optimization of Hierarchical Regression Model with Application to Optimizing Multi-Response Regression K-ary Trees (AAAI 2019)**    - Pooya Tavallali, Peyman Tavallali, Mukesh Singhal    - [[Paper]](https://aaai.org/ojs/index.php/AAAI/article/view/4447/4325)    - **XBART: Accelerated Bayesian Additive Regression Trees (AISTATS 2019)**    - Jingyu He, Saar Yalov, P. Richard Hahn    - [[Paper]](https://arxiv.org/abs/1810.02215)    - **Interaction Detection with Bayesian Decision Tree Ensembles (AISTATS 2019)**    - Junliang Du, Antonio R. Linero    - [[Paper]](https://arxiv.org/abs/1809.08524)      - **Adversarial Training of Gradient-Boosted Decision Trees (CIKM 2019)**    - Stefano Calzavara, Claudio Lucchese, Gabriele Tolomei    - [[Paper]](https://www.dais.unive.it/~calzavara/papers/cikm19.pdf)    - **Interpretable MTL from Heterogeneous Domains using Boosted Tree (CIKM 2019)**    - Ya-Lin Zhang, Longfei Li    - [[Paper]](https://dl.acm.org/citation.cfm?id=3357384.3358072)    - **Interpreting CNNs via Decision Trees (CVPR 2019)**    - Quanshi Zhang, Yu Yang, Haotian Ma, Ying Nian Wu    - [[Paper]](https://arxiv.org/abs/1802.00121)      - **EDiT: Interpreting Ensemble Models via Compact Soft Decision Trees (ICDM 2019)**    - Jaemin Yoo, Lee Sael    - [[Paper]](https://github.com/leesael/EDiT/blob/master/docs/YooS19.pdf)    - [[Code]](https://github.com/leesael/EDiT)    - **Fair Adversarial Gradient Tree Boosting (ICDM 2019)**    - Vincent Grari, Boris Ruf, Sylvain Lamprier, Marcin Detyniecki    - [[Paper]](https://arxiv.org/abs/1911.05369)    - **Functional Transparency for Structured Data: a Game-Theoretic Approach (ICML 2019)**    - Guang-He Lee, Wengong Jin, David Alvarez-Melis, Tommi S. Jaakkola    - [[Paper]](http://proceedings.mlr.press/v97/lee19b/lee19b.pdf)      - **Incorporating Grouping Information into Bayesian Decision Tree Ensembles (ICML 2019)**    - Junliang Du, Antonio R. Linero    - [[Paper]](http://proceedings.mlr.press/v97/du19d.html)    - **Adaptive Neural Trees (ICML 2019)**    - Ryutaro Tanno, Kai Arulkumaran, Daniel C. Alexander, Antonio Criminisi, Aditya V. Nori    - [[Paper]](https://arxiv.org/abs/1807.06699)    - [[Code]](https://github.com/rtanno21609/AdaptiveNeuralTrees)    - **Robust Decision Trees Against Adversarial Examples (ICML 2019)**    - Hongge Chen, Huan Zhang, Duane S. Boning, Cho-Jui Hsieh    - [[Paper]](https://arxiv.org/abs/1902.10660)    - [[Code]](https://github.com/chenhongge/RobustTrees)      - **Learn Smart with Less: Building Better Online Decision Trees with Fewer Training Examples (IJCAI 2019)**    - Ariyam Das, Jin Wang, Sahil M. Gandhi, Jae Lee, Wei Wang, Carlo Zaniolo    - [[Paper]](https://www.ijcai.org/proceedings/2019/0306.pdf)      - **FAHT: An Adaptive Fairness-aware Decision Tree Classifier (IJCAI 2019)**    - Wenbin Zhang, Eirini Ntoutsi    - [[Paper]](https://arxiv.org/abs/1907.07237)    - [[Code]](https://github.com/vanbanTruong/FAHT)    - **Inter-node Hellinger Distance based Decision Tree (IJCAI 2019)**    - Pritom Saha Akash, Md. Eusha Kadir, Amin Ahsan Ali, Mohammad Shoyaib    - [[Paper]](https://www.ijcai.org/proceedings/2019/0272.pdf)    - [[Matlab Code]](https://github.com/ZDanielsResearch/HellingerTreesMatlab)    - [[R Code]](https://github.com/kaustubhrpatil/HDDT)    - **Gradient Boosting with Piece-Wise Linear Regression Trees (IJCAI 2019)**    - Yu Shi, Jian Li, Zhize Li    - [[Paper]](https://arxiv.org/abs/1802.05640)    - [[Code]](https://github.com/GBDT-PL/GBDT-PL)    - **A Gradient-Based Split Criterion for Highly Accurate and Transparent Model Trees (IJCAI 2019)**    - Klaus Broelemann, Gjergji Kasneci    - [[Paper]](https://arxiv.org/abs/1809.09703)      - **Combining Decision Trees and Neural Networks for Learning-to-Rank in Personal Search (KDD 2019)**    - Pan Li, Zhen Qin, Xuanhui Wang, Donald Metzler    - [[Paper]](https://ai.google/research/pubs/pub48133/)      - **Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers (NeurIPS 2019)**    - Guang-He Lee, Yang Yuan, Shiyu Chang, Tommi S. Jaakkola    - [[Paper]](https://papers.nips.cc/paper/8737-tight-certificates-of-adversarial-robustness-for-randomly-smoothed-classifiers.pdf)    - [[Code]](https://github.com/guanghelee/Randomized_Smoothing)    - **Partitioning Structure Learning for Segmented Linear Regression Trees (NeurIPS 2019)**    - Xiangyu Zheng, Song Xi Chen    - [[Paper]](https://papers.nips.cc/paper/8494-partitioning-structure-learning-for-segmented-linear-regression-trees)      - **Provably Robust Boosted Decision Stumps and Trees against Adversarial Attacks (NeurIPS 2019)**    - Maksym Andriushchenko, Matthias Hein    - [[Paper]](https://arxiv.org/abs/1906.03526)    - [[Code]](https://github.com/max-andr/provably-robust-boosting)    - **Optimal Decision Tree with Noisy Outcomes (NeurIPS 2019)**    - Su Jia, Viswanath Nagarajan, Fatemeh Navidi, R. Ravi    - [[Paper]](https://papers.nips.cc/paper/8592-optimal-decision-tree-with-noisy-outcomes.pdf)    - [[Code]](https://github.com/sjia1/ODT-with-noisy-outcomes)    - **Regularized Gradient Boosting (NeurIPS 2019)**    - Corinna Cortes, Mehryar Mohri, Dmitry Storcheus    - [[Paper]](https://papers.nips.cc/paper/8784-regularized-gradient-boosting.pdf)    - **Optimal Sparse Decision Trees (NeurIPS 2019)**    - Xiyang Hu, Cynthia Rudin, Margo Seltzer    - [[Paper]](https://papers.nips.cc/paper/8947-optimal-sparse-decision-trees.pdf)    - [[Code]](https://github.com/xiyanghu/OSDT)      - **MonoForest framework for tree ensemble analysis (NeurIPS 2019)**    - Igor Kuralenok, Vasilii Ershov, Igor Labutin    - [[Paper]](https://papers.nips.cc/paper/9530-monoforest-framework-for-tree-ensemble-analysis)    - [[Code]](https://github.com/xiyanghu/OSDT)    - **Calibrating Probability Estimation Trees using Venn-Abers Predictors (SDM 2019)**    - Ulf Johansson, Tuwe LÃ¶fstrÃ¶m, Henrik BostrÃ¶m    - [[Paper]](https://epubs.siam.org/doi/pdf/10.1137/1.9781611975673.4)      - **Fast Training for Large-Scale One-versus-All Linear Classifiers using Tree-Structured Initialization (SDM 2019)**    - Huang Fang, Minhao Cheng, Cho-Jui Hsieh, Michael P. Friedlander    - [[Paper]](https://epubs.siam.org/doi/pdf/10.1137/1.9781611975673.32)    - **Forest Packing: Fast Parallel, Decision Forests (SDM 2019)**    - James Browne, Disa Mhembere, Tyler M. Tomita, Joshua T. Vogelstein, Randal Burns    - [[Paper]](https://epubs.siam.org/doi/abs/10.1137/1.9781611975673.6)      - **Block-distributed Gradient Boosted Trees (SIGIR 2019)**    - Theodore Vasiloudis, Hyunsu Cho, Henrik BostrÃ¶m    - [[Paper]](https://arxiv.org/abs/1904.10522)      - **Entity Personalized Talent Search Models with Tree Interaction Features (WWW 2019)**    - Cagri Ozcaglar, Sahin Cem Geyik, Brian Schmitz, Prakhar Sharma, Alex Shelkovnykov, Yiming Ma, Erik Buchanan    - [[Paper]](https://arxiv.org/abs/1902.09041)    ## 2018  - **Adapting to Concept Drift in Credit Card Transaction Data Streams Using Contextual Bandits and Decision Trees (AAAI 2018)**    - Dennis J. N. J. Soemers, Tim Brys, Kurt Driessens, Mark H. M. Winands, Ann NowÃ©    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16183/16394)    - **MERCS: Multi-Directional Ensembles of Regression and Classification Trees (AAAI 2018)**    - Elia Van Wolputte, Evgeniya Korneva, Hendrik Blockeel    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16875/16735)    - [[Code]](https://github.com/eliavw/mercs-v5)    - **Differential Performance Debugging With Discriminant Regression Trees (AAAI 2018)**    - Saeid Tizpaz-Niari, Pavol CernÃ½, Bor-Yuh Evan Chang, Ashutosh Trivedi    - [[Paper]](https://arxiv.org/abs/1711.04076)    - [[Code]](https://github.com/cuplv/DPDEBUGGER)    - **Estimating the Class Prior in Positive and Unlabeled Data Through Decision Tree Induction (AAAI 2018)**    - Jessa Bekker, Jesse Davis    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16776)    - **MDP-Based Cost Sensitive Classification Using Decision Trees (AAAI 2018)**    - Shlomi Maliah, Guy Shani    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17128)    - **Generative Adversarial Image Synthesis With Decision Tree Latent Controller (CVPR 2018)**    - Takuhiro Kaneko, Kaoru Hiramatsu, Kunio Kashino    - [[Paper]](https://arxiv.org/abs/1805.10603)    - [[Code]](https://github.com/LynnHo/DTLC-GAN-Tensorflow)    - **Enhancing Very Fast Decision Trees with Local Split-Time Predictions (ICDM 2018)**    - Viktor Losing, Heiko Wersing, Barbara Hammer    - [[Paper]](https://www.techfak.uni-bielefeld.de/~hwersing/LosingHammerWersing_ICDM2018.pdf)    - [[Code]](https://github.com/ICDM2018Submission/VFDT-split-time-prediction)      - **Realization of Random Forest for Real-Time Evaluation through Tree Framing (ICDM 2018)**    - Sebastian BuschjÃ¤ger, Kuan-Hsun Chen, Jian-Jia Chen, Katharina Morik    - [[Paper]](https://sfb876.tu-dortmund.de/PublicPublicationFiles/buschjaeger_2018a.pdf)    - **Finding Influential Training Samples for Gradient Boosted Decision Trees (ICML 2018)**    - Boris Sharchilev, Yury Ustinovskiy, Pavel Serdyukov, Maarten de Rijke    - [[Paper]](https://arxiv.org/abs/1802.06640)    - [[Code]](https://github.com/bsharchilev/influence_boosting)    - **Learning Optimal Decision Trees with SAT (IJCAI 2018)**    - Nina Narodytska, Alexey Ignatiev, Filipe Pereira, JoÃ£o Marques-Silva    - [[Paper]](https://www.ijcai.org/proceedings/2018/0189.pdf)    - **Extremely Fast Decision Tree (KDD 2018)**    - Chaitanya Manapragada, Geoffrey I. Webb, Mahsa Salehi    - [[Paper]](https://arxiv.org/abs/1802.08780)    - [[Code]](https://github.com/doubleplusplus/incremental_decision_tree-CART-Random_Forest_python)      - **RapidScorer: Fast Tree Ensemble Evaluation by Maximizing Compactness in Data Level Parallelization (KDD 2018)**    - Ting Ye, Hucheng Zhou, Will Y. Zou, Bin Gao, Ruofei Zhang    - [[Paper]](http://ai.stanford.edu/~wzou/kdd_rapidscorer.pdf)      - **CatBoost: Unbiased Boosting with Categorical Features (NIPS 2018)**    - Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, Andrey Gulin    - [[Paper]](https://papers.nips.cc/paper/7898-catboost-unbiased-boosting-with-categorical-features.pdf)    - [[Code]](https://catboost.ai/)      - **Active Learning for Non-Parametric Regression Using Purely Random Trees (NIPS 2018)**    - Jack Goetz, Ambuj Tewari, Paul Zimmerman    - [[Paper]](https://papers.nips.cc/paper/7520-active-learning-for-non-parametric-regression-using-purely-random-trees.pdf)    - **Alternating Optimization of Decision Trees with Application to Learning Sparse Oblique Trees (NIPS 2018)**    - Miguel Ã. Carreira-PerpiÃ±Ã¡n, Pooya Tavallali    - [[Paper]](https://papers.nips.cc/paper/7397-alternating-optimization-of-decision-trees-with-application-to-learning-sparse-oblique-trees)    - **Multi-Layered Gradient Boosting Decision Trees (NIPS 2018)**    - Ji Feng, Yang Yu, Zhi-Hua Zhou    - [[Paper]](https://papers.nips.cc/paper/7614-multi-layered-gradient-boosting-decision-trees.pdf)    - [[Code]](https://github.com/kingfengji/mGBDT)      - **Transparent Tree Ensembles (SIGIR 2018)**    - Alexander Moore, Vanessa Murdock, Yaxiong Cai, Kristine Jones    - [[Paper]](http://delivery.acm.org/10.1145/3220000/3210151/p1241-moore.pdf?ip=129.215.164.203&id=3210151&acc=ACTIVE%20SERVICE&key=C2D842D97AC95F7A%2EEB9E991028F4E1F1%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1559054892_a29816c683aa83a0ce0fbb777c68daba)    - **Privacy-aware Ranking with Tree Ensembles on the Cloud (SIGIR 2018)**    - Shiyu Ji, Jinjin Shao, Daniel Agun, Tao Yang    - [[Paper]](https://sites.cs.ucsb.edu/projects/ds/sigir18.pdf)    ## 2017  - **Strategic Sequences of Arguments for Persuasion Using Decision Trees (AAAI 2017)**    - Emmanuel Hadoux, Anthony Hunter    - [[Paper]](http://www0.cs.ucl.ac.uk/staff/a.hunter/papers/aaai17.pdf)    - **BoostVHT: Boosting Distributed Streaming Decision Trees (CIKM 2017)**    - Theodore Vasiloudis, Foteini Beligianni, Gianmarco De Francisci Morales    - [[Paper]](https://melmeric.files.wordpress.com/2010/05/boostvht-boosting-distributed-streaming-decision-trees.pdf)    - **Latency Reduction via Decision Tree Based Query Construction (CIKM 2017)**    - Aman Grover, Dhruv Arya, Ganesh Venkataraman    - [[Paper]](https://dl.acm.org/citation.cfm?id=3132865)    - **Enumerating Distinct Decision Trees (ICML 2017)**    - Salvatore Ruggieri    - [[Paper]](http://proceedings.mlr.press/v70/ruggieri17a/ruggieri17a.pdf)    - **Gradient Boosted Decision Trees for High Dimensional Sparse Output (ICML 2017)**    - Si Si, Huan Zhang, S. Sathiya Keerthi, Dhruv Mahajan, Inderjit S. Dhillon, Cho-Jui Hsieh    - [[Paper]](http://proceedings.mlr.press/v70/si17a.html)    - [[Code]](https://github.com/springdaisy/GBDT)    - **Consistent Feature Attribution for Tree Ensembles (ICML 2017)**    - Scott M. Lundberg, Su-In Lee    - [[Paper]](https://arxiv.org/abs/1706.06060)    - [[Code]](https://github.com/slundberg/shap)    - **Extremely Fast Decision Tree Mining for Evolving Data Streams (KDD 2017)**    - Albert Bifet, Jiajin Zhang, Wei Fan, Cheng He, Jianfeng Zhang, Jianfeng Qian, Geoff Holmes, Bernhard Pfahringer    - [[Paper]](https://core.ac.uk/download/pdf/151040580.pdf)      - **CatBoost: Gradient Boosting with Categorical Features Support (NIPS 2017)**    - Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin    - [[Paper]](https://arxiv.org/abs/1810.11363)    - [[Code]](https://catboost.ai/)    - **LightGBM: A Highly Efficient Gradient Boosting Decision Tree (NIPS 2017)**    - Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu    - [[Paper]](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)    - [[Code]](https://lightgbm.readthedocs.io/en/latest/)    - **Variable Importance Using Decision Trees (NIPS 2017)**    - Jalil Kazemitabar, Arash Amini, Adam Bloniarz, Ameet S. Talwalkar    - [[Paper]](https://papers.nips.cc/paper/6646-variable-importance-using-decision-trees)    - **A Unified Approach to Interpreting Model Predictions (NIPS 2017)**    - Scott M. Lundberg, Su-In Lee    - [[Paper]](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions)    - [[Code]](https://github.com/slundberg/shap)    - **Pruning Decision Trees via Max-Heap Projection (SDM 2017)**    - Zhi Nie, Binbin Lin, Shuai Huang, Naren Ramakrishnan, Wei Fan, Jieping Ye    - [[Paper]](https://www.researchgate.net/publication/317485748_Pruning_Decision_Trees_via_Max-Heap_Projection)    - **A Practical Method for Solving Contextual Bandit Problems Using Decision Trees (UAI 2017)**    - Adam N. Elmachtoub, Ryan McNellis, Sechan Oh, Marek Petrik    - [[Paper]](https://arxiv.org/abs/1706.04687)    - **Complexity of Solving Decision Trees with Skew-Symmetric Bilinear Utility (UAI 2017)**    - Hugo Gilbert, Olivier Spanjaard    - [[Paper]](http://auai.org/uai2017/proceedings/papers/64.pdf)      - **GB-CENT: Gradient Boosted Categorical Embedding and Numerical Trees (WWW 2017)**    - Qian Zhao, Yue Shi, Liangjie Hong    - [[Paper]](http://papers.www2017.com.au.s3-website-ap-southeast-2.amazonaws.com/proceedings/p1311.pdf)    ## 2016  - **Sparse Perceptron Decision Tree for Millions of Dimensions (AAAI 2016)**    - Weiwei Liu, Ivor W. Tsang    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12111)    - **Learning Online Smooth Predictors for Realtime Camera Planning Using Recurrent Decision Trees (CVPR 2016)**    - Jianhui Chen, Hoang Minh Le, Peter Carr, Yisong Yue, James J. Little    - [[Paper]](http://hoangle.info/papers/cvpr2016_online_smooth_long.pdf)    - **Online Learning with Bayesian Classification Trees (CVPR 2016)**    - Samuel Rota BulÃ², Peter Kontschieder    - [[Paper]](http://www.dsi.unive.it/~srotabul/files/publications/CVPR2016.pdf)    - **Accurate Robust and Efficient Error Estimation for Decision Trees (ICML 2016)**    - Lixin Fan    - [[Paper]](http://proceedings.mlr.press/v48/fan16.pdf)    - **Meta-Gradient Boosted Decision Tree Model for Weight and Target Learning (ICML 2016)**    - Yury Ustinovskiy, Valentina Fedorova, Gleb Gusev, Pavel Serdyukov    - [[Paper]](http://proceedings.mlr.press/v48/ustinovskiy16.html)    - **Boosted Decision Tree Regression Adjustment for Variance Reduction in Online Controlled Experiments (KDD 2016)**    - Alexey Poyarkov, Alexey Drutsa, Andrey Khalyavin, Gleb Gusev, Pavel Serdyukov    - [[Paper]](https://www.kdd.org/kdd2016/papers/files/adf0653-poyarkovA.pdf)      - **XGBoost: A Scalable Tree Boosting System (KDD 2016)**    - Tianqi Chen, Carlos Guestrin    - [[Paper]](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)    - [[Code]](https://xgboost.readthedocs.io/en/latest/)    - **Yggdrasil: An Optimized System for Training Deep Decision Trees at Scale (NIPS 2016)**    - Firas Abuzaid, Joseph K. Bradley, Feynman T. Liang, Andrew Feng, Lee Yang, Matei Zaharia, Ameet S. Talwalkar    - [[Paper]](https://papers.nips.cc/paper/6366-yggdrasil-an-optimized-system-for-training-deep-decision-trees-at-scale)    - **A Communication-Efficient Parallel Algorithm for Decision Tree (NIPS 2016)**    - Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhiming Ma, Tie-Yan Liu    - [[Paper]](https://arxiv.org/abs/1611.01276)    - [[Code]](https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst)    - **Exploiting CPU SIMD Extensions to Speed-up Document Scoring with Tree Ensembles (SIGIR 2016)**    - Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Nicola Tonellotto, Rossano Venturini    - [[Paper]](http://pages.di.unipi.it/rossano/wp-content/uploads/sites/7/2016/07/SIGIR16a.pdf)    - [[Code]](https://github.com/hpclab/vectorized-quickscorer)    - **Post-Learning Optimization of Tree Ensembles for Efficient Ranking (SIGIR 2016)**    - Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Fabrizio Silvestri, Salvatore Trani    - [[Paper]](https://www.researchgate.net/publication/305081572_Post-Learning_Optimization_of_Tree_Ensembles_for_Efficient_Ranking)    - [[Code]](https://github.com/hpclab/quickrank)    ## 2015  - **Particle Gibbs for Bayesian Additive Regression Trees (AISTATS 2015)**    - Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh    - [[Paper]](https://arxiv.org/abs/1502.04622)    - **DART: Dropouts Meet Multiple Additive Regression Trees (AISTATS 2015)**    - Korlakai Vinayak Rashmi, Ran Gilad-Bachrach    - [[Paper]](https://arxiv.org/abs/1505.01866)    - [[Code]](https://xgboost.readthedocs.io/en/latest/)    - **Single Target Tracking Using Adaptive Clustered Decision Trees and Dynamic Multi-level Appearance Models (CVPR 2015)**    - Jingjing Xiao, Rustam Stolkin, Ales Leonardis    - [[Paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/3B_058.pdf)    - **Face Alignment Using Cascade Gaussian Process Regression Trees (CVPR 2015)**    - Donghoon Lee, Hyunsin Park, Chang Dong Yoo    - [[Paper]](https://slsp.kaist.ac.kr/paperdata/Face_Alignment_Using.pdf)    - [[Code]](https://github.com/donghoonlee04/cGPRT)    - **Tracking-by-Segmentation with Online Gradient Boosting Decision Tree (ICCV 2015)**    - Jeany Son, Ilchae Jung, Kayoung Park, Bohyung Han    - [[Paper]](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Son_Tracking-by-Segmentation_With_Online_ICCV_2015_paper.pdf)    - **Entropy Evaluation Based on Confidence Intervals of Frequency Estimates : Application to the Learning of Decision Trees (ICML 2015)**    - Mathieu Serrurier, Henri Prade    - [[Paper]](http://proceedings.mlr.press/v37/serrurier15.pdf)      - **Large-scale Distributed Dependent Nonparametric Trees (ICML 2015)**    - Zhiting Hu, Qirong Ho, Avinava Dubey, Eric P. Xing    - [[Paper]](https://www.cs.cmu.edu/~zhitingh/data/icml15hu.pdf)    - **Optimal Action Extraction for Random Forests and Boosted Trees (KDD 2015)**    - Zhicheng Cui, Wenlin Chen, Yujie He, Yixin Chen    - [[Paper]](https://www.cse.wustl.edu/~ychen/public/OAE.pdf)    - **A Decision Tree Framework for Spatiotemporal Sequence Prediction (KDD 2015)**    - Taehwan Kim, Yisong Yue, Sarah L. Taylor, Iain A. Matthews    - [[Paper]](http://www.yisongyue.com/publications/kdd2015_ssw_dt.pdf)    - **Efficient Non-greedy Optimization of Decision Trees (NIPS 2015)**    - Mohammad Norouzi, Maxwell D. Collins, Matthew Johnson, David J. Fleet, Pushmeet Kohli    - [[Paper]](https://arxiv.org/abs/1511.04056)      - **QuickScorer: A Fast Algorithm to Rank Documents with Additive Ensembles of Regression Trees (SIGIR 2015)**    - Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Nicola Tonellotto, Rossano Venturini    - [[Paper]](http://pages.di.unipi.it/rossano/wp-content/uploads/sites/7/2015/11/sigir15.pdf)    - [[Code]](https://github.com/hpclab/quickrank)    ## 2014    - **A Mixtures-of-Trees Framework for Multi-Label Classification (CIKM 2014)**    - Charmgil Hong, Iyad Batal, Milos Hauskrecht    - [[Paper]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4410801/)      - **On Building Decision Trees from Large-scale Data in Applications of On-line Advertising (CIKM 2014)**    - Shivaram Kalyanakrishnan, Deepthi Singh, Ravi Kant    - [[Paper]](https://www.cse.iitb.ac.in/~shivaram/papers/ksk_cikm_2014.pdf)    - **Fast Supervised Hashing with Decision Trees for High-Dimensional Data (CVPR 2014)**    - Guosheng Lin, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, David Suter    - [[Paper]](https://arxiv.org/abs/1404.1561)    - **One Millisecond Face Alignment with an Ensemble of Regression Trees (CVPR 2014)**    - Vahid Kazemi, Josephine Sullivan    - [[Paper]](https://www.researchgate.net/publication/264419855_One_Millisecond_Face_Alignment_with_an_Ensemble_of_Regression_Trees)      - **The return of AdaBoost.MH: multi-class Hamming trees (ICLR 2014)**    - BalÃ¡zs KÃ©gl    - [[Paper]](https://arxiv.org/pdf/1312.6086.pdf)    - **Diagnosis Determination: Decision Trees Optimizing Simultaneously Worst and Expected Testing Cost (ICML 2014)**    - Ferdinando Cicalese, Eduardo Sany Laber, Aline Medeiros Saettler    - [[Paper]](https://pdfs.semanticscholar.org/47ae/852f83b76f95b27ab00308d04f6020bdf71f.pdf)      - **Learning Multiple-Question Decision Trees for Cold-Start Recommendation (WSDM 2013)**    - Mingxuan Sun, Fuxin Li, Joonseok Lee, Ke Zhou, Guy Lebanon, Hongyuan Zha    - [[Paper]](http://www.joonseok.net/papers/coldstart.pdf)    ## 2013  - **Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria (ICCV 2013)**    - Christoph N. Straehle, Ullrich KÃ¶the, Fred A. Hamprecht    - [[Paper]](https://ieeexplore.ieee.org/document/6751340)    - **Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees (ICCV 2013)**    - Oisin Mac Aodha, Gabriel J. Brostow    - [[Paper]](https://ieeexplore.ieee.org/document/6751133)    - **Conformal Prediction Using Decision Trees (ICDM 2013)**    - Ulf Johansson, Henrik BostrÃ¶m, Tuve LÃ¶fstrÃ¶m    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6729517)    - **Focal-Test-Based Spatial Decision Tree Learning: A Summary of Results (ICDM 2013)**    - Zhe Jiang, Shashi Shekhar, Xun Zhou, Joseph K. Knight, Jennifer Corcoran    - [[Paper]](https://pdfs.semanticscholar.org/f28e/df8d9eed76e4ce97cb6bd4182d590547be5e.pdf)    - **Top-down Particle Filtering for Bayesian Decision Trees (ICML 2013)**    - Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh    - [[Paper]](https://arxiv.org/abs/1303.0561)    - **Quickly Boosting Decision Trees - Pruning Underachieving Features Early (ICML 2013)**    - Ron Appel, Thomas J. Fuchs, Piotr DollÃ¡r, Pietro Perona    - [[Paper]](http://proceedings.mlr.press/v28/appel13.pdf)    - **Knowledge Compilation for Model Counting: Affine Decision Trees (IJCAI 2013)**    - FrÃ©dÃ©ric Koriche, Jean-Marie Lagniez, Pierre Marquis, Samuel Thomas    - [[Paper]](https://www.researchgate.net/publication/262398921_Knowledge_Compilation_for_Model_Counting_Affine_Decision_Trees)      - **Understanding Variable Importances in Forests of Randomized Trees (NIPS 2013)**    - Gilles Louppe, Louis Wehenkel, Antonio Sutera, Pierre Geurts    - [[Paper]](https://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-trees)    - **Regression-tree Tuning in a Streaming Setting (NIPS 2013)**    - Samory Kpotufe, Francesco Orabona    - [[Paper]](https://papers.nips.cc/paper/4898-regression-tree-tuning-in-a-streaming-setting)    - **Learning Max-Margin Tree Predictors (UAI 2013)**    - Ofer Meshi, Elad Eban, Gal Elidan, Amir Globerson    - [[Paper]](https://ttic.uchicago.edu/~meshi/papers/mtreen.pdf)    ## 2012  - **Regression Tree Fields - An Efficient, Non-parametric Approach to Image Labeling Problems (CVPR 2012)**    - Jeremy Jancsary, Sebastian Nowozin, Toby Sharp, Carsten Rother    - [[Paper]](http://www.nowozin.net/sebastian/papers/jancsary2012rtf.pdf)    - **ConfDTree: Improving Decision Trees Using Confidence Intervals (ICDM 2012)**    - Gilad Katz, Asaf Shabtai, Lior Rokach, Nir Ofek    - [[Paper]](https://ieeexplore.ieee.org/document/6413889)    - **Improved Information Gain Estimates for Decision Tree Induction (ICML 2012)**    - Sebastian Nowozin    - [[Paper]](https://arxiv.org/abs/1206.4620)    - **Learning Partially Observable Models Using Temporally Abstract Decision Trees (NIPS 2012)**    - Erik Talvitie    - [[Paper]](https://papers.nips.cc/paper/4662-learning-partially-observable-models-using-temporally-abstract-decision-trees)      - **Subtree Replacement in Decision Tree Simplification (SDM 2012)**    - Salvatore Ruggieri    - [[Paper]](http://pages.di.unipi.it/ruggieri/Papers/sdm2012.pdf)    ## 2011  - **Incorporating Boosted Regression Trees into Ecological Latent Variable Models (AAAI 2011)**    - Rebecca A. Hutchinson, Li-Ping Liu, Thomas G. Dietterich    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/viewFile/3711/4086)    - **Syntactic Decision Tree LMs: Random Selection or Intelligent Design (EMNLP 2011)**    - Denis Filimonov, Mary P. Harper    - [[Paper]](https://www.aclweb.org/anthology/D11-1064)      - **Decision Tree Fields (ICCV 2011)**    - Sebastian Nowozin, Carsten Rother, Shai Bagon, Toby Sharp, Bangpeng Yao, Pushmeet Kohli    - [[Paper]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/nrbsyk_iccv11.pdf)    - **Confidence in Predictions from Random Tree Ensembles (ICDM 2011)**    - Siddhartha Bhattacharyya    - [[Paper]](https://link.springer.com/article/10.1007/s10115-012-0600-z)    - **Speeding-Up Hoeffding-Based Regression Trees With Options (ICML 2011)**    - Elena Ikonomovska, JoÃ£o Gama, Bernard Zenko, Saso Dzeroski    - [[Paper]](https://icml.cc/Conferences/2011/papers/349_icmlpaper.pdf)      - **Density Estimation Trees (KDD 2011)**    - Parikshit Ram, Alexander G. Gray    - [[Paper]](https://mlpack.org/papers/det.pdf)      - **Bagging Gradient-Boosted Trees for High Precision, Low Variance Ranking Models (SIGIR 2011)**    - Yasser Ganjisaffar, Rich Caruana, Cristina Videira Lopes    - [[Paper]](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/materials/bagging_lmbamart_jforests.pdf)    - **On the Complexity of Decision Making in Possibilistic Decision Trees (UAI 2011)**    - HÃ©lÃ¨ne Fargier, Nahla Ben Amor, Wided Guezguez    - [[Paper]](https://dslpitt.org/uai/papers/11/p203-fargier.pdf)    - **Adaptive Bootstrapping of Recommender Systems Using Decision Trees (WSDM 2011)**    - Nadav Golbandi, Yehuda Koren, Ronny Lempel    - [[Paper]](https://dl.acm.org/citation.cfm?id=1935910)    - **Parallel Boosted Regression Trees for Web Search Ranking (WWW 2011)**    - Stephen Tyree, Kilian Q. Weinberger, Kunal Agrawal, Jennifer Paykin    - [[Paper]](http://www.cs.cornell.edu/~kilian/papers/fr819-tyreeA.pdf)    ## 2010  - **Discrimination Aware Decision Tree Learning (ICDM 2010)**    - Faisal Kamiran, Toon Calders, Mykola Pechenizkiy    - [[Paper]](https://www.win.tue.nl/~mpechen/publications/pubs/KamiranICDM2010.pdf)    - **Decision Trees for Uplift Modeling (ICDM 2010)**    - Piotr Rzepakowski, Szymon Jaroszewicz    - [[Paper]](https://core.ac.uk/download/pdf/81899141.pdf)    - **Learning Markov Network Structure with Decision Trees (ICDM 2010)**    - Daniel Lowd, Jesse Davis    - [[Paper]](https://ix.cs.uoregon.edu/~lowd/icdm10lowd.pdf)    - **Multivariate Dyadic Regression Trees for Sparse Learning Problems (NIPS 2010)**    - Han Liu, Xi Chen    - [[Paper]](https://papers.nips.cc/paper/4178-multivariate-dyadic-regression-trees-for-sparse-learning-problems.pdf)      - **Fast and Accurate Gene Prediction by Decision Tree Classification (SDM 2010)**    - Rong She, Jeffrey Shih-Chieh Chu, Ke Wang, Nansheng Chen    - [[Paper]](http://www.sfu.ca/~chenn/genBlastDT_sdm.pdf)    - **A Robust Decision Tree Algorithm for Imbalanced Data Sets (SDM 2010)**    - Wei Liu, Sanjay Chawla, David A. Cieslak, Nitesh V. Chawla    - [[Paper]](https://www3.nd.edu/~nchawla/papers/SDM10.pdf)    ## 2009  - **Stochastic Gradient Boosted Distributed Decision Trees (CIKM 2009)**    - Jerry Ye, Jyh-Herng Chow, Jiang Chen, Zhaohui Zheng    - [[Paper]](https://dl.acm.org/citation.cfm?id=1646301)      - **Feature Selection for Ranking Using Boosted Trees (CIKM 2009)**    - Feng Pan, Tim Converse, David Ahn, Franco Salvetti, Gianluca Donato    - [[Paper]](http://www.francosalvetti.com/cikm09_camera2.pdf)      - **Thai Word Segmentation with Hidden Markov Model and Decision Tree (PAKDD 2009)**    - Poramin Bheganan, Richi Nayak, Yue Xu    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-642-01307-2_10)    - **Parameter Estimdation in Semi-Random Decision Tree Ensembling on Streaming Data (PAKDD 2009)**    - Pei-Pei Li, Qianhui Liang, Xindong Wu, Xuegang Hu    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-642-01307-2_35)    - **DTU: A Decision Tree for Uncertain Data (PAKDD 2009)**    - Biao Qin, Yuni Xia, Fang Li    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-642-01307-2_4)    ## 2008  - **Predicting Future Decision Trees from Evolving Data (ICDM 2008)**    - Mirko BÃ¶ttcher, Martin Spott, Rudolf Kruse    - [[Paper]](https://ieeexplore.ieee.org/document/4781098)    - **Bayes Optimal Classification for Decision Trees (ICML 2008)**    - Siegfried Nijssen    - [[Paper]](http://icml2008.cs.helsinki.fi/papers/455.pdf)      - **A New Credit Scoring Method Based on Rough Sets and Decision Tree (PAKDD 2008)**    - XiYue Zhou, Defu Zhang, Yi Jiang    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-68125-0_117)    - **A Comparison of Different Off-Centered Entropies to Deal with Class Imbalance for Decision Trees (PAKDD 2008)**    - Philippe Lenca, StÃ©phane Lallich, Thanh-Nghi Do, Nguyen-Khang Pham    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-68125-0_59)    - **BOAI: Fast Alternating Decision Tree Induction Based on Bottom-Up Evaluation (PAKDD 2008)**    - Bishan Yang, Tengjiao Wang, Dongqing Yang, Lei Chang    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-68125-0_36)    - **A General Framework for Estimating Similarity of Datasets and Decision Trees: Exploring Semantic Similarity of Decision Trees (SDM 2008)**    - Irene Ntoutsi, Alexandros Kalousis, Yannis Theodoridis    - [[Paper]](https://www.researchgate.net/publication/220907047_A_general_framework_for_estimating_similarity_of_datasets_and_decision_trees_exploring_semantic_similarity_of_decision_trees)    - **ROC-tree: A Novel Decision Tree Induction Algorithm Based on Receiver Operating Characteristics to Classify Gene Expression Data (SDM 2008)**    - M. Maruf Hossain, Md. Rafiul Hassan, James Bailey    - [[Paper]](https://pdfs.semanticscholar.org/bd80/db2f0903169b7611d34b2cc85f60a736375d.pdf)    ## 2007    - **Tree-based Classifiers for Bilayer Video Segmentation (CVPR 2007)**    - Pei Yin, Antonio Criminisi, John M. Winn, Irfan A. Essa    - [[Paper]](https://ieeexplore.ieee.org/document/4270033)    - **Additive Groves of Regression Trees (ECML 2007)**    - Daria Sorokina, Rich Caruana, Mirek Riedewald    - [[Paper]](http://additivegroves.net/papers/groves.pdf)    - **Decision Tree Instability and Active Learning (ECML 2007)**    - Kenneth Dwyer, Robert Holte    - [[Paper]](https://webdocs.cs.ualberta.ca/~holte/Publications/ecml07.pdf)    - **Ensembles of Multi-Objective Decision Trees (ECML 2007)**    - Dragi Kocev, Celine Vens, Jan Struyf, Saso Dzeroski    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-74958-5_61)    - **Seeing the Forest Through the Trees: Learning a Comprehensible Model from an Ensemble (ECML 2007)**    - Anneleen Van Assche, Hendrik Blockeel    - [[Paper]](http://ftp.cs.wisc.edu/machine-learning/shavlik-group/ilp07wip/ilp07_assche.pdf)    - **Sample Compression Bounds for Decision Trees (ICML 2007)**    - Mohak Shah    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.331.9136&rep=rep1&type=pdf)    - **A Tighter Error Bound for Decision Tree Learning Using PAC Learnability (IJCAI 2007)**    - Chaithanya Pichuka, Raju S. Bapi, Chakravarthy Bhagvati, Arun K. Pujari, Bulusu Lakshmana Deekshatulu    - [[Paper]](https://www.ijcai.org/Proceedings/07/Papers/163.pdf)    - **Keep the Decision Tree and Estimate the Class Probabilities Using its Decision Boundary (IJCAI 2007)**    - Isabelle Alvarez, Stephan Bernard, Guillaume Deffuant    - [[Paper]](https://www.ijcai.org/Proceedings/07/Papers/104.pdf)    - **Real Boosting a la Carte with an Application to Boosting Oblique Decision Tree (IJCAI 2007)**    - Claudia Henry, Richard Nock, Frank Nielsen    - [[Paper]](https://www.ijcai.org/Proceedings/07/Papers/135.pdf)    - **Scalable Look-ahead Linear Regression Trees (KDD 2007)**    - David S. Vogel, Ognian Asparouhov, Tobias Scheffer    - [[Paper]](https://www.cs.uni-potsdam.de/ml/publications/kdd2007.pdf)    - **Mining Optimal Decision Trees from Itemset Lattices (KDD 2007)**    - Siegfried Nijssen, Ã‰lisa Fromont    - [[Paper]](https://hal.archives-ouvertes.fr/hal-00372011/document)      - **A Hybrid Multi-group Privacy-Preserving Approach for Building Decision Trees (PAKDD 2007)**    - Zhouxuan Teng, Wenliang Du    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-71701-0_30)    ## 2006  - **Decision Tree Methods for Finding Reusable MDP Homomorphisms (AAAI 2006)**    - Alicia P. Wolfe, Andrew G. Barto    - [[Paper]](https://www.aaai.org/Papers/AAAI/2006/AAAI06-085.pdf)    - **A Fast Decision Tree Learning Algorithm (AAAI 2006)**    - Jiang Su, Harry Zhang    - [[Paper]](http://www.cs.unb.ca/~hzhang/publications/AAAI06.pdf)    - **Anytime Induction of Decision Trees: An Iterative Improvement Approach (AAAI 2006)**    - Saher Esmeir, Shaul Markovitch    - [[Paper]](https://www.aaai.org/Papers/AAAI/2006/AAAI06-056.pdf)    - **When a Decision Tree Learner Has Plenty of Time (AAAI 2006)**    - Saher Esmeir, Shaul Markovitch    - [[Paper]](https://www.aaai.org/Papers/AAAI/2006/AAAI06-259.pdf)    - **Decision Trees for Functional Variables (ICDM 2006)**    - Suhrid Balakrishnan, David Madigan    - [[Paper]](http://archive.dimacs.rutgers.edu/Research/MMS/PAPERS/fdt17.pdf)      - **Cost-Sensitive Decision Tree Learning for Forensic Classification (ECML 2006)**    - Jason V. Davis, Jungwoo Ha, Christopher J. Rossbach, Hany E. Ramadan, Emmett Witchel    - [[Paper]](https://www.cs.utexas.edu/users/witchel/pubs/davis-ecml06.pdf)    - **Improving the Ranking Performance of Decision Trees (ECML 2006)**    - Bin Wang, Harry Zhang    - [[Paper]](https://link.springer.com/chapter/10.1007/11871842_44)    - **A General Framework for Accurate and Fast Regression by Data Summarization in Random Decision Trees (KDD 2006)**    - Wei Fan, Joe McCloskey, Philip S. Yu    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.442.2004&rep=rep1&type=pdf)      - **Constructing Decision Trees for Graph-Structured Data by Chunkingless Graph-Based Induction (PAKDD 2006)**    - Phu Chien Nguyen, Kouzou Ohara, Akira Mogi, Hiroshi Motoda, Takashi Washio    - [[Paper]](http://www.ar.sanken.osaka-u.ac.jp/~motoda/papers/pakdd06.pdf)    - **Variable Randomness in Decision Tree Ensembles (PAKDD 2006)**    - Fei Tony Liu, Kai Ming Ting    - [[Paper]](https://link.springer.com/chapter/10.1007/11731139_12)    - **Generalized Conditional Entropy and a Metric Splitting Criterion for Decision Trees (PAKDD 2006)**    - Dan A. Simovici, Szymon Jaroszewicz    - [[Paper]](https://www.researchgate.net/profile/Szymon_Jaroszewicz/publication/220895184_Generalized_Conditional_Entropy_and_a_Metric_Splitting_Criterion_for_Decision_Trees/links/0fcfd50b1267f7b868000000/Generalized-Conditional-Entropy-and-a-Metric-Splitting-Criterion-for-Decision-Trees.pdf)    - **Decision Trees for Hierarchical Multilabel Classification: A Case Study in Functional Genomics (PKDD 2006)**    - Hendrik Blockeel, Leander Schietgat, Jan Struyf, Saso Dzeroski, Amanda Clare    - [[Paper]](https://link.springer.com/chapter/10.1007/11871637_7)    - **k-Anonymous Decision Tree Induction (PKDD 2006)**    - Arik Friedman, Assaf Schuster, Ran Wolff    - [[Paper]](http://www.cs.technion.ac.il/~arikf/online-publications/kADET06.pdf)    ## 2005  - **Representing Conditional Independence Using Decision Trees (AAAI 2005)**    - Jiang Su, Harry Zhang    - [[Paper]](http://www.cs.unb.ca/~hzhang/publications/AAAI051SuJ.pdf)    - **Use of Expert Knowledge for Decision Tree Pruning (AAAI 2005)**    - Jingfeng Cai, John Durkin    - [[Paper]](http://www.aaai.org/Papers/AAAI/2005/SA05-009.pdf)      - **Model Selection in Omnivariate Decision Trees (ECML 2005)**    - Olcay Taner Yildiz, Ethem Alpaydin    - [[Paper]](https://www.cmpe.boun.edu.tr/~ethem/files/papers/yildiz_ecml05.pdf)    - **Combining Bias and Variance Reduction Techniques for Regression Trees (ECML 2005)**    - Yuk Lai Suen, Prem Melville, Raymond J. Mooney    - [[Paper]](http://www.cs.utexas.edu/users/ml/papers/bv-ecml-05.pdf)    - **Simple Test Strategies for Cost-Sensitive Decision Trees (ECML 2005)**    - Shengli Sheng, Charles X. Ling, Qiang Yang    - [[Paper]](https://www.researchgate.net/publication/3297582_Test_strategies_for_cost-sensitive_decision_trees)    - **Effective Estimation of Posterior Probabilities: Explaining the Accuracy of Randomized Decision Tree Approaches (ICDM 2005)**    - Wei Fan, Ed Greengrass, Joe McCloskey, Philip S. Yu, Kevin Drummey    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.9713&rep=rep1&type=pdf)    - **Exploiting Informative Priors for Bayesian Classification and Regression Trees (IJCAI 2005)**    - Nicos Angelopoulos, James Cussens    - [[Paper]](https://www.ijcai.org/Proceedings/05/Papers/1013.pdf)    - **Ranking Cases with Decision Trees: a Geometric Method that Preserves Intelligibility (IJCAI 2005)**    - Isabelle Alvarez, Stephan Bernard    - [[Paper]](https://www.ijcai.org/Proceedings/05/Papers/1502.pdf)      - **Maximizing Tree Diversity by Building Complete-Random Decision Trees (PAKDD 2005)**    - Fei Tony Liu, Kai Ming Ting, Wei Fan    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.7805&rep=rep1&type=pdf)    - **Hybrid Cost-Sensitive Decision Tree (PKDD 2005)**    - Shengli Sheng, Charles X. Ling    - [[Paper]](https://cling.csd.uwo.ca/papers/pkdd05a.pdf)    - **Tree2 - Decision Trees for Tree Structured Data (PKDD 2005)**    - BjÃ¶rn Bringmann, Albrecht Zimmermann    - [[Paper]](https://link.springer.com/chapter/10.1007/11564126_10)    - **Building Decision Trees on Records Linked through Key References (SDM 2005)**    - Ke Wang, Yabo Xu, Philip S. Yu, Rong She    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.215.7181&rep=rep1&type=pdf)    - **Decision Tree Induction in High Dimensional, Hierarchically Distributed Databases (SDM 2005)**    - Amir Bar-Or, Ran Wolff, Assaf Schuster, Daniel Keren    - [[Paper]](https://www.semanticscholar.org/paper/Decision-Tree-Induction-in-High-Dimensional%2C-Bar-Or-Wolff/90235fc35c27dae273681f7847c2b20ff37928a9)    - **Boosted Decision Trees for Word Recognition in Handwritten Document Retrieval (SIGIR 2005)**    - Nicholas R. Howe, Toni M. Rath, R. Manmatha    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.152.1551&rep=rep1&type=pdf)    ## 2004  - **On the Optimality of Probability Estimation by Random Decision Trees (AAAI 2004)**    - Wei Fan    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.447.2128&rep=rep1&type=pdf)    - **Occam's Razor and a Non-Syntactic Measure of Decision Tree Complexity (AAAI 2004)**    - Goutam Paul    - [[Paper]](https://www.aaai.org/Papers/AAAI/2004/AAAI04-130.pdf)    - **Using Emerging Patterns and Decision Trees in Rare-Class Classification (ICDM 2004)**    - Hamad Alhammady, Kotagiri Ramamohanarao    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/1410299)    - **Orthogonal Decision Trees (ICDM 2004)**    - Hillol Kargupta, Haimonti Dutta    - [[Paper]](https://www.csee.umbc.edu/~hillol/PUBS/odtree.pdf)    - **Improving the Reliability of Decision Tree and Naive Bayes Learners (ICDM 2004)**    - David George Lindsay, SiÃ¢n Cox    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.521.3127&rep=rep1&type=pdf)    - **Communication Efficient Construction of Decision Trees Over Heterogeneously Distributed Data (ICDM 2004)**    - Chris Giannella, Kun Liu, Todd Olsen, Hillol Kargupta    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.7119&rep=rep1&type=pdf)    - **Decision Tree Evolution Using Limited Number of Labeled Data Items from Drifting Data Streams (ICDM 2004)**    - Wei Fan, Yi-an Huang, Philip S. Yu    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.9450&rep=rep1&type=pdf)    - **Lookahead-based Algorithms for Anytime Induction of Decision Trees (ICML 2004)**    - Saher Esmeir, Shaul Markovitch    - [[Paper]](http://www.cs.technion.ac.il/~shaulm/papers/pdf/Esmeir-Markovitch-icml2004.pdf)      - **Decision Trees with Minimal Costs (ICML 2004)**    - Charles X. Ling, Qiang Yang, Jianning Wang, Shichao Zhang    - [[Paper]](https://icml.cc/Conferences/2004/proceedings/papers/136.pdf)    - **Training Conditional Random Fields via Gradient Tree Boosting (ICML 2004)**    - Thomas G. Dietterich, Adam Ashenfelter, Yaroslav Bulatov    - [[Paper]](http://web.engr.oregonstate.edu/~tgd/publications/ml2004-treecrf.pdf)    - **Detecting Structural Metadata with Decision Trees and Transformation-Based Learning (NAACL 2004)**    - Joungbum Kim, Sarah E. Schwarm, Mari Ostendorf    - [[Paper]](https://www.aclweb.org/anthology/N04-1018)    - **On the Adaptive Properties of Decision Trees (NIPS 2004)**    - Clayton D. Scott, Robert D. Nowak    - [[Paper]](https://papers.nips.cc/paper/2625-on-the-adaptive-properties-of-decision-trees.pdf)      - **A Metric Approach to Building Decision Trees Based on Goodman-Kruskal Association Index (PAKDD 2004)**    - Dan A. Simovici, Szymon Jaroszewicz    - [[Paper]](https://www.researchgate.net/publication/2906289_A_Metric_Approach_to_Building_Decision_Trees_Based_on_Goodman-Kruskal_Association_Index)    ## 2003  - **Rademacher Penalization over Decision Tree Prunings (ECML 2003)**    - Matti KÃ¤Ã¤riÃ¤inen, Tapio Elomaa    - [[Paper]](https://www.researchgate.net/publication/221112653_Rademacher_Penalization_over_Decision_Tree_Prunings)      - **Ensembles of Cascading Trees (ICDM 2003)**    - Jinyan Li, Huiqing Liu    - [[Paper]](https://www.researchgate.net/publication/4047523_Ensembles_of_cascading_trees)    - **Postprocessing Decision Trees to Extract Actionable Knowledge (ICDM 2003)**    - Qiang Yang, Jie Yin, Charles X. Ling, Tielin Chen    - [[Paper]](https://pdfs.semanticscholar.org/b2c6/ff54c7aeefc70820ff04a8fc8b804012c504.pdf)    - **K-D Decision Tree: An Accelerated and Memory Efficient Nearest Neighbor Classifier (ICDM 2003)**    - Tomoyuki Shibata, Takekazu Kato, Toshikazu Wada    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/1250997)    - **Identifying Markov Blankets with Decision Tree Induction (ICDM 2003)**    - Lewis J. Frey, Douglas H. Fisher, Ioannis Tsamardinos, Constantin F. Aliferis, Alexander R. Statnikov    - [[Paper]](https://www.semanticscholar.org/paper/Identifying-Markov-Blankets-with-Decision-Tree-Frey-Fisher/1aa0b0ede22f3963c923ea320a8bed91ac5aafbf)    - **Comparing Naive Bayes, Decision Trees, and SVM with AUC and Accuracy (ICDM 2003)**    - Jin Huang, Jingjing Lu, Charles X. Ling    - [[Paper]](https://pdfs.semanticscholar.org/8a73/74b98a9d94b8c01e996e72340f86a4327869.pdf)    - **Boosting Lazy Decision Trees (ICML 2003)**    - Xiaoli Zhang Fern, Carla E. Brodley    - [[Paper]](https://www.aaai.org/Papers/ICML/2003/ICML03-026.pdf)    - **Decision Tree with Better Ranking (ICML 2003)**    - Charles X. Ling, Robert J. Yan    - [[Paper]](https://www.aaai.org/Papers/ICML/2003/ICML03-064.pdf)    - **Skewing: An Efficient Alternative to Lookahead for Decision Tree Induction (IJCAI 2003)**    - David Page, Soumya Ray    - [[Paper]](http://pages.cs.wisc.edu/~dpage/ijcai3.pdf)    - **Efficient Decision Tree Construction on Streaming Data (KDD 2003)**    - Ruoming Jin, Gagan Agrawal    - [[Paper]](http://web.cse.ohio-state.edu/~agrawal.28/p/sigkdd03.pdf)    - **PaintingClass: Interactive Construction Visualization and Exploration of Decision Trees (KDD 2003)**    - Soon Tee Teoh, Kwan-Liu Ma    - [[Paper]](https://www.researchgate.net/publication/220272011_PaintingClass_interactive_construction_visualization_and_exploration_of_decision_trees)    - **Accurate Decision Trees for Mining High-Speed Data Streams (KDD 2003)**    - JoÃ£o Gama, Ricardo Rocha, Pedro Medas    - [[Paper]](http://staff.icar.cnr.it/manco/Teaching/2006/datamining/Esami2006/ArticoliSelezionatiDM/SEMINARI/Mining%20Data%20Streams/kdd03.pdf)    - **Near-Minimax Optimal Classification with Dyadic Classification Trees (NIPS 2003)**    - Clayton D. Scott, Robert D. Nowak    - [[Paper]](http://nowak.ece.wisc.edu/nips03.pdf)      - **Improving Performance of Decision Tree Algorithms with Multi-edited Nearest Neighbor Rule (PAKDD 2003)**    - Chenzhou Ye, Jie Yang, Lixiu Yao, Nian-yi Chen    - [[Paper]](https://www.researchgate.net/publication/220895462_Improving_Performance_of_Decision_Tree_Algorithms_with_Multi-edited_Nearest_Neighbor_Rule)    - **Arbogodai: a New Approach for Decision Trees (PKDD 2003)**    - Djamel A. Zighed, Gilbert Ritschard, Walid Erray, Vasile-Marian Scuturici    - [[Paper]](http://mephisto.unige.ch/pub/publications/gr/zig_rit_arbo_pkdd03.pdf)    - **Communication and Memory Efficient Parallel Decision Tree Construction (SDM 2003)**    - Ruoming Jin, Gagan Agrawal    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.3059&rep=rep1&type=pdf)    - **Decision Tree Classification of Spatial Data Patterns from Videokeratography using Zernicke Polynomials (SDM 2003)**    - Michael D. Twa, Srinivasan Parthasarathy, Thomas W. Raasch, Mark Bullimore    - [[Paper]](https://www.researchgate.net/publication/220907147_Decision_Tree_Classification_of_Spatial_Data_Patterns_From_Videokeratography_Using_Zernike_Polynomials)    ## 2002    - **Multiclass Alternating Decision Trees (ECML 2002)**    - Geoffrey Holmes, Bernhard Pfahringer, Richard Kirkby, Eibe Frank, Mark A. Hall    - [[Paper]](https://www.cs.waikato.ac.nz/~bernhard/papers/ecml2002.pdf)      - **Heterogeneous Forests of Decision Trees (ICANN 2002)**    - Krzysztof Grabczewski, Wlodzislaw Duch    - [[Paper]](https://fizyka.umk.pl/publications/kmk/02forest.pdf)    - **Solving the Fragmentation Problem of Decision Trees by Discovering Boundary Emerging Patterns (ICDM 2002)**    - Jinyan Li, Limsoon Wong    - [[Paper]](https://ieeexplore.ieee.org/document/1184021)    - **Solving the Fragmentation Problem of Decision Trees by Discovering Boundary Emerging Patterns (ICDM 2002)**    - Jinyan Li, Limsoon Wong    - [[Paper]](https://www.comp.nus.edu.sg/~wongls/psZ/decisionTreeandEP-2.ps)    - **Learning Decision Trees Using the Area Under the ROC Curve (ICML 2002)**    - CÃ©sar Ferri, Peter A. Flach, JosÃ© HernÃ¡ndez-Orallo    - [[Paper]](http://dmip.webs.upv.es/papers/ICML2002.pdf)    - **Finding an Optimal Gain-Ratio Subset-Split Test for a Set-Valued Attribute in Decision Tree Induction (ICML 2002)**    - Fumio Takechi, Einoshin Suzuki    - [[Paper]](https://www.researchgate.net/publication/221346121_Finding_an_Optimal_Gain-Ratio_Subset-Split_Test_for_a_Set-Valued_Attribute_in_Decision_Tree_Induction)    - **Efficiently Mining Frequent Trees in a Forest (KDD 2002)**    - Mohammed Javeed Zaki    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.160.8511&rep=rep1&type=pdf)    - **SECRET: a Scalable Linear Regression Tree Algorithm (KDD 2002)**    - Alin Dobra, Johannes Gehrke    - [[Paper]](http://www.cs.cornell.edu/people/dobra/papers/secret-extended.pdf)    - **Instability of Decision Tree Classification Algorithms (KDD 2002)**    - Ruey-Hsia Li, Geneva G. Belford    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.12.8094&rep=rep1&type=pdf)    - **Extracting Decision Trees From Trained Neural Networks (KDD 2002)**    - Olcay Boz    - [[Paper]](http://dspace.library.iitb.ac.in/jspui/bitstream/10054/1285/1/5664.pdf)    - **Dyadic Classification Trees via Structural Risk Minimization (NIPS 2002)**    - Clayton D. Scott, Robert D. Nowak    - [[Paper]](https://papers.nips.cc/paper/2198-dyadic-classification-trees-via-structural-risk-minimization.pdf)      - **Approximate Splitting for Ensembles of Trees using Histograms (SDM 2002)**    - Chandrika Kamath, Erick CantÃº-Paz, David Littau    - [[Paper]](https://pdfs.semanticscholar.org/0855/0a94993a268e4e3e99c41e7e0ee43eabd993.pdf)    ## 2001  - **Japanese Named Entity Recognition based on a Simple Rule Generator and Decision Tree Learning (ACL 2001)**    - Hideki Isozaki    - [[Paper]](https://www.aclweb.org/anthology/P01-1041)    - **Message Length as an Effective Ockham's Razor in Decision Tree Induction (AISTATS 2001)**    - Scott Needham, David L. Dowe    - [[Paper]](www.gatsby.ucl.ac.uk/aistats/aistats2001/files/needham122.ps)    - **SQL Database Primitives for Decision Tree Classifiers (CIKM 2001)**    - Kai-Uwe Sattler, Oliver Dunemann    - [[Paper]](http://fusion.cs.uni-magdeburg.de/pubs/classprim.pdf)      - **A Unified Framework for Evaluation Metrics in Classification Using Decision Trees (ECML 2001)**    - Ricardo Vilalta, Mark Brodie, Daniel Oblinger, Irina Rish    - [[Paper]](https://scholar.harvard.edu/files/nkc/files/2015_framework_for_benefit_risk_assessment_value_in_health.pdf)    - **Backpropagation in Decision Trees for Regression (ECML 2001)**    - Victor Medina-Chico, Alberto SuÃ¡rez, James F. Lutsko    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-44795-4_30)    - **Consensus Decision Trees: Using Consensus Hierarchical Clustering for Data Relabelling and Reduction (ECML 2001)**    - Branko Kavsek, Nada Lavrac, Anuska Ferligoj    - [[Paper]](https://link.springer.com/content/pdf/10.1007/3-540-44795-4_22.pdf)    - **Mining Decision Trees from Data Streams in a Mobile Environment (ICDM 2001)**    - Hillol Kargupta, Byung-Hoon Park    - [[Paper]](https://ieeexplore.ieee.org/document/989530)    - **Efficient Determination of Dynamic Split Points in a Decision Tree (ICDM 2001)**    - David Maxwell Chickering, Christopher Meek, Robert Rounthwaite    - [[Paper]](https://pdfs.semanticscholar.org/3587/a245c34ea415b205a903bde3220eb533d1a7.pdf)    - **A Comparison of Stacking with Meta Decision Trees to Bagging, Boosting, and Stacking with other Methods (ICDM 2001)**    - Bernard Zenko, Ljupco Todorovski, Saso Dzeroski    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.3118&rep=rep1&type=pdf)    - **Efficient Algorithms for Decision Tree Cross-Validation (ICML 2001)**    - Hendrik Blockeel, Jan Struyf    - [[Paper]](http://www.jmlr.org/papers/volume3/blockeel02a/blockeel02a.pdf)    - **Bias Correction in Classification Tree Construction (ICML 2001)**    - Alin Dobra, Johannes Gehrke    - [[Paper]](http://www.cs.cornell.edu/people/dobra/papers/icml2001-bias.pdf)    - **Breeding Decision Trees Using Evolutionary Techniques (ICML 2001)**    - Athanassios Papagelis, Dimitrios Kalles    - [[Paper]](http://www.gatree.com/data/BreedinDecisioTreeUsinEvo.pdf)    - **Obtaining Calibrated Probability Estimates from Decision Trees and Naive Bayesian Classifiers (ICML 2001)**    - Bianca Zadrozny, Charles Elkan    - [[Paper]](http://cseweb.ucsd.edu/~elkan/calibrated.pdf)    - **Temporal Decision Trees or the lazy ECU vindicated (IJCAI 2001)**    - Luca Console, Claudia Picardi, Daniele Theseider DuprÃ©    - [[Paper]](https://www.researchgate.net/publication/220815333_Temporal_Decision_Trees_or_the_lazy_ECU_vindicated)      - **Data Mining Criteria for Tree-based Regression and Classification (KDD 2001)**    - Andreas Buja, Yung-Seop Lee    - [[Paper]](https://repository.upenn.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=1406&context=statistics_papers)    - **A Decision Tree of Bigrams is an Accurate Predictor of Word Sense (NAACL 2001)**    - Ted Pedersen    - [[Paper]](https://www.aclweb.org/anthology/N01-1011)      - **Rule Reduction over Numerical Attributes in Decision Tree Using Multilayer Perceptron (PAKDD 2001)**    - DaeEun Kim, Jaeho Lee    - [[Paper]](https://dl.acm.org/citation.cfm?id=693490)    - **A Scalable Algorithm for Rule Post-pruning of Large Decision Trees (PAKDD 2001)**    - Trong Dung Nguyen, Tu Bao Ho, Hiroshi Shimodaira    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45357-1_49)    - **Optimizing the Induction of Alternating Decision Trees (PAKDD 2001)**    - Bernhard Pfahringer, Geoffrey Holmes, Richard Kirkby    - [[Paper]](https://www.researchgate.net/publication/33051701_Optimizing_the_Induction_of_Alternating_Decision_Trees)    - **Interactive Construction of Decision Trees (PAKDD 2001)**    - Jianchao Han, Nick Cercone    - [[Paper]](https://pure.tue.nl/ws/files/3522084/672434611234867.pdf)    - **Bloomy Decision Tree for Multi-objective Classification (PKDD 2001)**    - Einoshin Suzuki, Masafumi Gotoh, Yuta Choki    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-44794-6_36)      - **A Fourier Analysis Based Approach to Learning Decision Trees in a Distributed Environment (SDM 2001)**    - Byung-Hoon Park, Rajeev Ayyagari, Hillol Kargupta    - [[Paper]](https://archive.siam.org/meetings/sdm01/pdf/sdm01_19.pdf)      ## 2000    - **Intuitive Representation of Decision Trees Using General Rules and Exceptions (AAAI 2000)**    - Bing Liu, Minqing Hu, Wynne Hsu    - [[Paper]](https://pdfs.semanticscholar.org/e284/96551e595f1850a53f93affa98919147712f.pdf)    - **Tagging Unknown Proper Names Using Decision Trees (ACL 2000)**    - FrÃ©dÃ©ric BÃ©chet, Alexis Nasr, Franck Genet    - [[Paper]](https://www.aclweb.org/anthology/P00-1011)    - **Clustering Through Decision Tree Construction (CIKM 2000)**    - Bing Liu, Yiyuan Xia, Philip S. Yu    - [[Paper]](https://dl.acm.org/citation.cfm?id=354775)    - **Handling Continuous-Valued Attributes in Decision Tree with Neural Network Modelling (ECML 2000)**    - DaeEun Kim, Jaeho Lee    - [[Paper]](https://link.springer.com/content/pdf/10.1007/3-540-45164-1_22.pdf)    - **Investigation and Reduction of Discretization Variance in Decision Tree Induction (ECML 2000)**    - Pierre Geurts, Louis Wehenkel    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45164-1_17)    - **Nonparametric Regularization of Decision Trees (ECML 2000)**    - Tobias Scheffer    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45164-1_36)    - **Exploiting the Cost (In)sensitivity of Decision Tree Splitting Criteria (ICML 2000)**    - Chris Drummond, Robert C. Holte    - [[Paper]](https://pdfs.semanticscholar.org/160e/21c3acc925b60dc040cb1705e58bb166b045.pdf)    - **Multi-agent Q-learning and Regression Trees for Automated Pricing Decisions (ICML 2000)**    - Manu Sridharan, Gerald Tesauro    - [[Paper]](https://manu.sridharan.net/files/icml00.pdf)    - **Growing Decision Trees on Support-less Association Rules (KDD 2000)**    - Ke Wang, Senqiang Zhou, Yu He    - [[Paper]](https://www2.cs.sfu.ca/~wangk/pub/kdd002.pdf)    - **Efficient Algorithms for Constructing Decision Trees with Constraints (KDD 2000)**    - Minos N. Garofalakis, Dongjoon Hyun, Rajeev Rastogi, Kyuseok Shim    - [[Paper]](http://www.softnet.tuc.gr/~minos/Papers/kdd00-cam.pdf)    - **Interactive Visualization in Mining Large Decision Trees (PAKDD 2000)**    - Trong Dung Nguyen, Tu Bao Ho, Hiroshi Shimodaira    - [[Paper]](https://link.springer.com/content/pdf/10.1007/3-540-45571-X_40.pdf)    - **VQTree: Vector Quantization for Decision Tree Induction (PAKDD 2000)**    - Shlomo Geva, Lawrence Buckingham    - [[Paper]](https://link.springer.com/chapter/10.1007%2F3-540-45571-X_41)    - **Some Enhencements of Decision Tree Bagging (PKDD 2000)**    - Pierre Geurts    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45372-5_14)    - **Combining Multiple Models with Meta Decision Trees (PKDD 2000)**    - Ljupco Todorovski, Saso Dzeroski    - [[Paper]](http://kt.ijs.si/bernard/mdts/pub01.pdf)    - **Induction of Multivariate Decision Trees by Using Dipolar Criteria (PKDD 2000)**    - Leon Bobrowski, Marek Kretowski    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45372-5_33)    - **Decision Tree Toolkit: A Component-Based Library of Decision Tree Algorithms (PKDD 2000)**    - Nikos Drossos, Athanassios Papagelis, Dimitrios Kalles    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45372-5_40)    ## 1999  - **Modeling Decision Tree Performance with the Power Law (AISTATS 1999)**    - Lewis J. Frey, Douglas H. Fisher    - [[Paper]](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/ModelingTree.pdf)    - **Causal Mechanisms and Classification Trees for Predicting Chemical Carcinogens (AISTATS 1999)**    - Louis Anthony Cox Jr.    - [[Paper]](https://pdfs.semanticscholar.org/0d7b/1d55c5abfd024aacf645c66d0c90c283814e.pdf)    - **POS Tags and Decision Trees for Language Modeling (EMNLP 1999)**    - Peter A. Heeman    - [[Paper]](https://www.aclweb.org/anthology/W99-0617)    - **Lazy Bayesian Rules: A Lazy Semi-Naive Bayesian Learning Technique Competitive to Boosting Decision Trees (ICML 1999)**    - Zijian Zheng, Geoffrey I. Webb, Kai Ming Ting    - [[Paper]](https://pdfs.semanticscholar.org/067e/86836ddbcb5e2844e955c16e058366a18c77.pdf)    - **The Alternating Decision Tree Learning Algorithm (ICML 1999)**    - Yoav Freund, Llew Mason    - [[Paper]](https://cseweb.ucsd.edu/~yfreund/papers/atrees.pdf)    - [[Code]](https://github.com/rajanil/mkboost)    - **Boosting with Multi-Way Branching in Decision Trees (NIPS 1999)**    - Yishay Mansour, David A. McAllester    - [[Paper]](https://papers.nips.cc/paper/1659-boosting-with-multi-way-branching-in-decision-trees.pdf)    ## 1998  - **Learning Sorting and Decision Trees with POMDPs (ICML 1998)**    - Blai Bonet, Hector Geffner    - [[Paper]](https://bonetblai.github.io/reports/icml98-learning.pdf)    - **Using a Permutation Test for Attribute Selection in Decision Trees (ICML 1998)**    - Eibe Frank, Ian H. Witten    - [[Paper]](https://pdfs.semanticscholar.org/9aa9/21b0203e06e98b49bf726a33e124f4310ea3.pdf)    - **A Fast and Bottom-Up Decision Tree Pruning Algorithm with Near-Optimal Generalization (ICML 1998)**    - Michael J. Kearns, Yishay Mansour    - [[Paper]](https://www.cis.upenn.edu/~mkearns/papers/pruning.pdf)    ## 1997  - **Pessimistic Decision Tree Pruning Based Continuous-Time (ICML 1997)**    - Yishay Mansour    - [[Paper]](https://pdfs.semanticscholar.org/b6fc/e37612db10a9756b904b5e79e1144ca12574.pdf)    - **PAC Learning with Constant-Partition Classification Noise and Applications to Decision Tree Induction (ICML 1997)**    - Scott E. Decatur    - [[Paper]](https://www.semanticscholar.org/paper/PAC-Learning-with-Constant-Partition-Classification-Decatur/dd205073aeb512ecd1e823b35f556058fdeea5e0)    - **Option Decision Trees with Majority Votes (ICML 1997)**    - Ron Kohavi, Clayton Kunz    - [[Paper]](https://pdfs.semanticscholar.org/383b/381d1ac0bb41ec595e0d1603ed642809eb86.pdf)    - **Integrating Feature Construction with Multiple Classifiers in Decision Tree Induction (ICML 1997)**    - Ricardo Vilalta, Larry A. Rendell    - [[Paper]](https://pdfs.semanticscholar.org/1f73/d9d409a75d16871cfa1182ac72b37c839d86.pdf)    - **Functional Models for Regression Tree Leaves (ICML 1997)**    - LuÃ­s Torgo    - [[Paper]](https://pdfs.semanticscholar.org/48e4/b3187ca234308e97e1ac0cab84222c603bdd.pdf)    - **The Effects of Training Set Size on Decision Tree Complexity (ICML 1997)**    - Tim Oates, David D. Jensen    - [[Paper]](https://pdfs.semanticscholar.org/e003/9dbdec3bd4cfbb3273b623fbed2d6b2f0cc9.pdf)    - **Unsupervised On-line Learning of Decision Trees for Hierarchical Data Analysis (NIPS 1997)**    - Marcus Held, Joachim M. Buhmann    - [[Paper]](https://papers.nips.cc/paper/1479-unsupervised-on-line-learning-of-decision-trees-for-hierarchical-data-analysis.pdf)    - **Data-Dependent Structural Risk Minimization for Perceptron Decision Trees (NIPS 1997)**    - John Shawe-Taylor, Nello Cristianini    - [[Paper]](https://papers.nips.cc/paper/1359-data-dependent-structural-risk-minimization-for-perceptron-decision-trees)    - **Generalization in Decision Trees and DNF: Does Size Matter (NIPS 1997)**    - Mostefa Golea, Peter L. Bartlett, Wee Sun Lee, Llew Mason    - [[Paper]](https://papers.nips.cc/paper/1340-generalization-in-decision-trees-and-dnf-does-size-matter.pdf)    ## 1996  - **Second Tier for Decision Trees (ICML 1996)**    - Miroslav Kubat    - [[Paper]](https://pdfs.semanticscholar.org/b619/7c531b1c83dfaa52563449f9b8248cc68c5a.pdf)    - **Non-Linear Decision Trees - NDT (ICML 1996)**    - Andreas Ittner, Michael Schlosser    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.2133&rep=rep1&type=pdf)    - **Learning Relational Concepts with Decision Trees (ICML 1996)**    - Peter Geibel, Fritz Wysotzki    - [[Paper]](https://pdfs.semanticscholar.org/32f1/78d7266fee779257b87ac8f948951db57d1e.pdf)    ## 1995  - **A Hill-Climbing Approach for Optimizing Classification Trees (AISTATS 1995)**    - Xiaorong Sun, Steve Y. Chiu, Louis Anthony Cox Jr.    - [[Paper]](https://link.springer.com/chapter/10.1007%2F978-1-4612-2404-4_11)    - **An Exact Probability Metric for Decision Tree Splitting (AISTATS 1995)**    - J. Kent Martin    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.48.6378&rep=rep1&type=pdf)    - **On Pruning and Averaging Decision Trees (ICML 1995)**    - Jonathan J. Oliver, David J. Hand    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.6733&rep=rep1&type=pdf)    - **On Handling Tree-Structured Attributed in Decision Tree Learning (ICML 1995)**    - Hussein Almuallim, Yasuhiro Akiba, Shigeo Kaneda    - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603776500116)    - **Retrofitting Decision Tree Classifiers Using Kernel Density Estimation (ICML 1995)**    - Padhraic Smyth, Alexander G. Gray, Usama M. Fayyad    - [[Paper]](https://pdfs.semanticscholar.org/3a05/8ab505f096b23962591bb14e495a543aa2a1.pdf)    - **Increasing the Performance and Consistency of Classification Trees by Using the Accuracy Criterion at the Leaves (ICML 1995)**    - David J. Lubinsky    - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603776500530)    - **Efficient Algorithms for Finding Multi-way Splits for Decision Trees (ICML 1995)**    - Truxton Fulton, Simon Kasif, Steven Salzberg    - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603776500384)    - **Theory and Applications of Agnostic PAC-Learning with Small Decision Trees (ICML 1995)**    - Peter Auer, Robert C. Holte, Wolfgang Maass    - [[Paper]](https://igi-web.tugraz.at/PDF/77.pdf)    - **Boosting Decision Trees (NIPS 1995)**    - Harris Drucker, Corinna Cortes    - [[Paper]](http://papers.nips.cc/paper/1059-boosting-decision-trees.pdf)    - **Using Pairs of Data-Points to Define Splits for Decision Trees (NIPS 1995)**    - Geoffrey E. Hinton, Michael Revow    - [[Paper]](https://www.cs.toronto.edu/~hinton/absps/bcart.pdf)    - **A New Pruning Method for Solving Decision Trees and Game Trees (UAI 1995)**    - Prakash P. Shenoy    - [[Paper]](https://arxiv.org/abs/1302.4981)    ## 1994  - **A Statistical Approach to Decision Tree Modeling (ICML 1994)**    - Michael I. Jordan    - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603356500519)    - **In Defense of C4.5: Notes Learning One-Level Decision Trees (ICML 1994)**    - Tapio Elomaa    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.30.9386)    - **An Improved Algorithm for Incremental Induction of Decision Trees (ICML 1994)**    - Paul E. Utgoff    - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603356500465)      - **Decision Tree Parsing using a Hidden Derivation Model (NAACL 1994)**    - Frederick Jelinek, John D. Lafferty, David M. Magerman, Robert L. Mercer, Adwait Ratnaparkhi, Salim Roukos    - [[Paper]](http://acl-arc.comp.nus.edu.sg/archives/acl-arc-090501d3/data/pdf/anthology-PDF/H/H94/H94-1052.pdf)    ## 1993  - **Using Decision Trees to Improve Case-Based Learning (ICML 1993)**    - Claire Cardie    - [[Paper]](https://www.cs.cornell.edu/home/cardie/papers/ml-93.ps)      ## 1991  - **Context Dependent Modeling of Phones in Continuous Speech Using Decision Trees (NAACL 1991)**    - Lalit R. Bahl, Peter V. de Souza, P. S. Gopalakrishnan, David Nahamoo, Michael Picheny    - [[Paper]](https://www.aclweb.org/anthology/H91-1051.pdf)    ## 1989  - **Performance Comparisons Between Backpropagation Networks and Classification Trees on Three Real-World Applications (NIPS 1989)**    - Les E. Atlas, Ronald A. Cole, Jerome T. Connor, Mohamed A. El-Sharkawi, Robert J. Marks II, Yeshwant K. Muthusamy, Etienne Barnard    - [[Paper]](https://papers.nips.cc/paper/203-performance-comparisons-between-backpropagation-networks-and-classification-trees-on-three-real-world-applications)    ## 1988  - **Multiple Decision Trees (UAI 1988)**    - Suk Wah Kwok, Chris Carter    - [[Paper]](https://arxiv.org/abs/1304.2363)    ## 1987  - **Decision Tree Induction Systems: A Bayesian Analysis (UAI 1987)**    - Wray L. Buntine    - [[Paper]](https://arxiv.org/abs/1304.2732)      --------------------------------------------------------------------------------    **License**    - [CC0 Universal](https://github.com/benedekrozemberczki/awesome-decision-tree-papers/blob/master/LICENSE) """
Big data;https://github.com/griddb/griddb_nosql;"""<img src=""https://griddb.org/brand-resources/griddb-logo/png/color.png"" align=""center"" height=""240"" alt=""GridDB""/>    [![Visit Website](https://img.shields.io/badge/website-visit-orange.svg)](https://griddb.net)  ![GitHub All Releases](https://img.shields.io/github/downloads/griddb/griddb_nosql/total.svg)  ![GitHub release](https://img.shields.io/github/release/griddb/griddb_nosql.svg)  ## Overview    GridDB is Database for IoT with both NoSQL interface and SQL Interface.      Please refer to [GridDB Features Reference](https://github.com/griddb/docs-en/blob/master/manuals/GridDB_FeaturesReference.md) for functionality.      This repository includes server and Java client. And [jdbc repository](https://github.com/griddb/jdbc) includes JDBC Driver.    ## Quick start (Using source code)    We have confirmed the operation on CentOS 7.6 (gcc 4.8.5), Ubuntu 18.04 (gcc 4.8.5) and openSUSE Leap 15.1 (gcc 4.8.5).      Note: Please install tcl like ""yum install tcl.x86_64"" in advance.    ### Build a server and client(Java)      $ ./bootstrap.sh      $ ./configure      $ make      Note: When you use maven build for Java client, please run the following command. Then gridstore-X.X.X.jar file is created on target/.          $ cd java_client      $ ./make_source_for_mvn.sh      $ mvn clean      $ mvn install    ### Start a server      $ export GS_HOME=$PWD      $ export GS_LOG=$PWD/log      $ export PATH=${PATH}:$GS_HOME/bin        $ bin/gs_passwd admin        #input your_password      $ vi conf/gs_cluster.json        #    ""clusterName"":""your_clustername"" #<-- input your_clustername        $ bin/gs_startnode      $ bin/gs_joincluster -c your_clustername -u admin/your_password    ### Execute a sample program      $ export CLASSPATH=${CLASSPATH}:$GS_HOME/bin/gridstore.jar      $ mkdir gsSample      $ cp $GS_HOME/docs/sample/program/Sample1.java gsSample/.      $ javac gsSample/Sample1.java      $ java gsSample/Sample1 239.0.0.1 31999 your_clustername admin your_password        --> Person:  name=name02 status=false count=2 lob=[65, 66, 67, 68, 69, 70, 71, 72, 73, 74]    ### Stop a server      $ bin/gs_stopcluster -u admin/your_password      $ bin/gs_stopnode -u admin/your_password    ## Quick start (Using RPM or DEB)      We have confirmed the operation on CentOS 7.8/8.1, Ubuntu 18.04 and openSUSE Leap 15.1.    Note:  - When you install this package, a gsadm OS user are created in the OS.      Execute the operating command as the gsadm user.    - You don't need to set environment vatiable GS_HOME and GS_LOG.  - There is Java client library (gridstore.jar) on /usr/share/java and a sample on /usr/gridb-XXX/docs/sample/programs.  - The packages don't include trigger function.  - Please install Python2 in advance except CentOS7.    ### Install        (CentOS)      $ sudo rpm -ivh griddb-X.X.X-linux.x86_64.rpm        (Ubuntu)      $ sudo dpkg -i griddb_X.X.X_amd64.deb        (openSUSE)      $ sudo rpm -ivh griddb-X.X.X-opensuse.x86_64.rpm        Note: X.X.X is the GridDB version.    ### Start a server      [gsadm]$ gs_passwd admin        #input your_password      [gsadm]$ vi conf/gs_cluster.json        #    ""clusterName"":""your_clustername"" #<-- input your_clustername      [gsadm]$ gs_startnode      [gsadm]$ gs_joincluster -c your_clustername -u admin/your_password    ### Execute a sample program      $ export CLASSPATH=${CLASSPATH}:/usr/share/java/gridstore.jar      $ mkdir gsSample      $ cp /usr/griddb-X.X.X/docs/sample/program/Sample1.java gsSample/.      $ javac gsSample/Sample1.java      $ java gsSample/Sample1 239.0.0.1 31999 your_clustername admin your_password        --> Person:  name=name02 status=false count=2 lob=[65, 66, 67, 68, 69, 70, 71, 72, 73, 74]    ### Stop a server      [gsadm]$ gs_stopcluster -u admin/your_password      [gsadm]$ gs_stopnode -u admin/your_password    If necessary, please refer to [Installation Troubleshooting](docs/TroubleShootingTips.md).    ## Document    Refer to the file below for more detailed information.      - [Features Reference](https://github.com/griddb/docs-en/blob/master/manuals/GridDB_FeaturesReference.md)    - [Quick Start Guide](https://github.com/griddb/docs-en/blob/master/manuals/GridDB_QuickStartGuide.md)    - [Java API Reference](http://griddb.github.io/docs-en/manuals/GridDB_Java_API_Reference.html)    - [C API Reference](http://griddb.github.io/docs-en/manuals/GridDB_C_API_Reference.html)    - [TQL Reference](https://github.com/griddb/docs-en/blob/master/manuals/GridDB_TQL_Reference.md)    - [JDBC Driver UserGuide](https://github.com/griddb/docs-en/blob/master/manuals/GridDB_JDBC_Driver_UserGuide.md)    - [SQL Reference](https://github.com/griddb/docs-en/blob/master/manuals/GridDB_SQL_Reference.md)    - [V3.0 Release Notes](docs/GridDB-3.0.0-CE-RELEASE_NOTES.md)    - [V4.0 Release Notes](docs/GridDB-4.0-CE-RELEASE_NOTES.md)    - [V4.1 Release Notes](docs/GridDB-4.1-CE-RELEASE_NOTES.md)    - [V4.2 Release Notes](docs/GridDB-4.2-CE-RELEASE_NOTES.md)    - [V4.3 Release Notes](docs/GridDB-4.3-CE-RELEASE_NOTES.md)    - [V4.5 Release Notes](docs/GridDB-4.5-CE-RELEASE_NOTES.md)    - [V4.6 Release Notes](docs/GridDB-4.6-CE-RELEASE_NOTES.md)    ## Client and Connector    There are other clients and API for GridDB.        (NoSQL Interface)    * [GridDB C Client](https://github.com/griddb/c_client)    * [GridDB Python Client](https://github.com/griddb/python_client)    * [GridDB Ruby Client](https://github.com/griddb/ruby_client)    * [GridDB Go Client](https://github.com/griddb/go_client)    * [GridDB Node.JS Client (SWIG based)](https://github.com/griddb/nodejs_client)    * [GridDB Node API (node-addon-api based)](https://github.com/griddb/node-api)    * [GridDB PHP Client](https://github.com/griddb/php_client)    * [GridDB Perl Client](https://github.com/griddb/perl_client)        (SQL Interface)    * [GridDB JDBC Driver](https://github.com/griddb/jdbc)        (NoSQL & SQL Interface)    * [GridDB WebAPI](https://github.com/griddb/webapi)    * [GridDB CLI](https://github.com/griddb/cli)      There are some connectors for other OSS.    * [GridDB connector for Apache Hadoop MapReduce](https://github.com/griddb/griddb_hadoop_mapreduce)    * [GridDB connector for YCSB (https://github.com/brianfrankcooper/YCSB/tree/master/griddb)](https://github.com/brianfrankcooper/YCSB/tree/master/griddb)    * [GridDB connector for KairosDB](https://github.com/griddb/griddb_kairosdb)    * [GridDB connector for Apache Spark](https://github.com/griddb/griddb_spark)    * [GridDB Foreign Data Wrapper for PostgreSQL (https://github.com/pgspider/griddb_fdw)](https://github.com/pgspider/griddb_fdw)    * [GridDB Sample Application for Apache Kafka](https://github.com/griddb/griddb_kafka_sample_app)    * [GridDB Data Source for Grafana](https://github.com/griddb/griddb-datasource)    * [GridDB Plugin for Redash](https://github.com/griddb/griddb-redash)    * [GridDB Plugin for Fluentd](https://github.com/griddb/fluent-plugin-griddb)    * [GridDB Plugin for Tableau](https://github.com/griddb/tableau-plugin-griddb)    ## [Packages](docs/Packages.md)    ## Community    * Issues        Use the GitHub issue function if you have any requests, questions, or bug reports.    * PullRequest        Use the GitHub pull request function if you want to contribute code.      You'll need to agree GridDB Contributor License Agreement(CLA_rev1.1.pdf).      By using the GitHub pull request function, you shall be deemed to have agreed to GridDB Contributor License Agreement.    ## License    The server source license is GNU Affero General Public License (AGPL),    while the Java client library license and the operational commands is Apache License, version 2.0.    See 3rd_party/3rd_party.md for the source and license of the third party. """
Big data;https://github.com/Bobris/BTDB;"""# BTDB    Currently this project these parts:    -   Key Value Database  -   Wrapped Dynamic IL generation with debugging + extensions  -   IOC Container  -   Object Database with Relations  -   Snappy Compression  -   Event Storage    All code written in C# and licensed under very permissive [MIT license](http://www.opensource.org/licenses/mit-license.html). Targeting .Net 6.0, main code has just 1 dependency (Microsoft.Extensions.Primitives). Code is tested using xUnit Framework. Used in production on Windows and Linux, on OSX works as well.  Please is you find it useful or have questions, write me e-mail <boris.letocha@gmail.com> so I know that it is used.  It is available in Nuget <http://www.nuget.org/packages/BTDB>. Source code drops are Github releases.    ---    ## Key Value Database    ### Features:    -   This is Key Value store written in C# with 2 implementation old on managed heap and new on native heap (has also prefix compression).  -   It is easily embeddable.  -   One storage is just one directory.  -   It has [ACID] properties with [MVCC].  -   At one time there could be multiple read only transactions and one read/write transaction.  -   Export/Import to stream - could be used for compaction, snapshotting  -   Automatic compaction  -   Customizable compression  -   Relatively Fast DB Open due to key index file - though it still needs to load all keys to memory  -   Inspired by Bitcask [https://github.com/basho/bitcask/blob/develop/doc/bitcask-intro.pdf]    ### Design limits:    -   All keys data needs to fit in RAM  -   Maximum Key length is limited by 31bits (2GB).  -   Maximum value length is limited by 31bits (2GB).    ### Sample code:        using (var fileCollection = new InMemoryFileCollection())      using (IKeyValueDB db = new KeyValueDB(fileCollection))      {          using (var tr = db.StartTransaction())          {              tr.CreateOrUpdateKeyValue(new byte[] { 1 }, new byte[100000]);              tr.Commit();          }      }    ### Roadmap:    -   Everything is there just use it    ---    ## Wrapped Dynamic IL generation with debugging + extensions    This help you to write fluent code which generates IL code in runtime. It is used in Object Database part.    ### Sample code:        var method = ILBuilder.Instance.NewMethod<Func<Nested>>(""SampleCall"");      var il = method.Generator;      var local = il.DeclareLocal(typeof(Nested), ""n"");      il          .Newobj(() => new Nested())          .Dup()          .Stloc(local)          .Ldstr(""Test"")          .Call(() => ((Nested)null).Fun(""""))          .Ldloc(local)          .Ret();      var action = method.Create();    ### Roadmap:    -   Add support for all IL instructions as needed    ---    ## Object Database    ### Features:    -   Builds on top of Key Value Database and Reflection.Emit extensions.  -   It stores Plain .Net Objects and only their public properties with getters and setters.  -   All [ACID] and [MVCC] properties are preserved of course.  -   Automatic upgrading of model on read with dynamically generated optimal IL code.  -   Automatic versioning of model changes.  -   Enumeration of all objects  -   Each object type could store its ""singleton"" - very useful for root objects  -   Relations - Table with primary key and multiple secondary keys  -   By default objects are stored inline in parent object, use IIndirect for objects with Oid which will load lazily    Documentation: [https://github.com/Bobris/BTDB/blob/master/Doc/ODBDictionary.md]    Relations doc: [https://github.com/Bobris/BTDB/blob/master/Doc/Relations.md]    ### Sample code:        public class Person      {          public string Name { get; set; }          public uint Age { get; set; }      }        using (var tr = _db.StartTransaction())      {          tr.Store(new Person { Name = ""Bobris"", Age = 35 });          tr.Commit();      }      using (var tr = _db.StartTransaction())      {          var p = tr.Enumerate<Person>().First();          Assert.AreEqual(""Bobris"", p.Name);          Assert.AreEqual(35, p.Age);      }    ### Roadmap:    -   Support more types of properties  -   Free text search (far future if ever)    ---    ## Event storage    ### Features:    -   Optimal serialization with metadata  -   Deserialization also to dynamic  -   Storage is transactional  -   As storage could be used Azure Page Blobs  -   EventStorage2 is specialized to be used with Kafka, metadata are stored in separate topic    ---    ## Snappy Compression    ### Features:    -   Ported and inspired mainly by Go version of Snappy Compression [http://code.google.com/p/snappy/]  -   Fully compatible with original  -   Fully managed and safe implementation  -   Compression is aborted when target buffer size is not big enough    ### Roadmap:    -   Some speed optimizations around Spans would help    [acid]: http://en.wikipedia.org/wiki/ACID  [mvcc]: http://en.wikipedia.org/wiki/Multiversion_concurrency_control """
Big data;https://github.com/linkedin/kamikaze;"""What is Kamikaze  ===============    Kamikaze is a utility package wrapping set implementations on document lists.     It also implements the PForDelta compression algorithm for sorted integer segments to enable Inverted List compression for search engines like Lucene (http://lucene.apache.org/core/4_5_1/core/org/apache/lucene/util/PForDeltaDocIdSet.html).     Kamikaze is based on the PForDelta algorithm proposed in the following paper:  Inverted Index Compression and Query Processing with Optimized Document Ordering   Hao Yan, S.Ding and T.Suel.  The 18th International World Wide Web Conference (WWW'09), Madrid, Spain, April 2009    Kamikaze is open sourced by LinkedIn Corp : http://data.linkedin.com/opensource/kamikaze.    The principal committer of Kamikaze is Hao Yan. If you have any questions regarding Kamikaze, please email him at hyan@linkedin.com.    ------------------------------------    ### Wiki    Wiki is available [HERE](http://snaprojects.jira.com/wiki/display/KAMI/Home)    ### Issues    Issues are tracked [HERE](http://snaprojects.jira.com/browse/KAMI) """
Big data;https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers;"""# Awesome Monte Carlo Tree Search Papers.  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)[![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/awesome-monte-carlo-tree-search-papers.svg)](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers/archive/master.zip) ![License](https://img.shields.io/github/license/benedekrozemberczki/awesome-tree-search-papers.svg?color=blue) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)   <p align=""center"">    <img width=""600"" src=""tree.png"">  </p>    -----------------------------------------------    A curated list of Monte Carlo tree search papers with implementations from the following conferences/journals:    - Machine learning     * [NeurIPS](https://nips.cc/)     * [ICML](https://icml.cc/)  - Computer vision     * [CVPR](http://cvpr2019.thecvf.com/)     * [ICCV](http://iccv2019.thecvf.com/)  - Natural language processing     * [ACL](http://www.acl2019.org/EN/index.xhtml)  - Data     * [KDD](https://www.kdd.org/)  - Artificial intelligence     * [AAAI](https://www.aaai.org/)     * [AISTATS](https://www.aistats.org/)     * [IJCAI](https://www.ijcai.org/)     * [UAI](http://www.auai.org/)  - Robotics     * [RAS](https://www.journals.elsevier.com/robotics-and-autonomous-systems)  - Games     * [CIG](http://www.ieee-cig.org/)    Similar collections about [graph classification](https://github.com/benedekrozemberczki/awesome-graph-classification), [gradient boosting](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers), [classification/regression trees](https://github.com/benedekrozemberczki/awesome-decision-tree-papers), [fraud detection](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers), and [community detection](https://github.com/benedekrozemberczki/awesome-community-detection) papers with implementations.    ## 2021    - **Learning to Stop: Dynamic Simulation Monte-Carlo Tree Search (AAAI 2021)**    - Li-Cheng Lan, Ti-Rong Wu, I-Chen Wu, Cho-Jui Hsieh    - [[Paper]](https://arxiv.org/abs/2012.07910)    - **Dec-SGTS: Decentralized Sub-Goal Tree Search for Multi-Agent Coordination (AAAI 2021)**    - Minglong Li, Zhongxuan Cai, Wenjing Yang, Lixia Wu, Yinghui Xu, Ji Wang    - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/17345)    - **Improved POMDP Tree Search Planning with Prioritized Action Branching (AAAI 2021)**    - John Mern, Anil Yildiz, Lawrence Bush, Tapan Mukerji, Mykel J. Kochenderfer    - [[Paper]](https://arxiv.org/abs/2010.03599)    - **Dynamic Automaton-Guided Reward Shaping for Monte Carlo Tree Search (AAAI 2021)**    - Alvaro Velasquez, Brett Bissey, Lior Barak, Andre Beckus, Ismail Alkhouri, Daniel Melcer, George K. Atia    - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/17427)    - **Single Player Monte-Carlo Tree Search Based on the Plackett-Luce Model (AAAI 2021)**    - Felix Mohr, Viktor Bengs, Eyke HÃ¼llermeier    - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/17468)    - **Learning to Pack: A Data-Driven Tree Search Algorithm for Large-Scale 3D Bin Packing Problem (CIKM 2021)**    - Qianwen Zhu, Xihan Li, Zihan Zhang, Zhixing Luo, Xialiang Tong, Mingxuan Yuan, Jia Zeng    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3459637.3481933)    - **Practical Massively Parallel Monte-Carlo Tree Search Applied to Molecular Design (ICLR 2021)**    - Xiufeng Yang, Tanuj Kr Aasawat, Kazuki Yoshizoe    - [[Paper]](https://arxiv.org/abs/2006.10504)    - **Convex Regularization in Monte-Carlo Tree Search (ICML 2021)**    - Tuan Dam, Carlo D'Eramo, Jan Peters, Joni Pajarinen    - [[Paper]](https://arxiv.org/abs/2007.00391)    - **Combining Tree Search and Action Prediction for State-of-the-Art Performance in DouDiZhu (IJCAI 2021)**    - Yunsheng Zhang, Dong Yan, Bei Shi, Haobo Fu, Qiang Fu, Hang Su, Jun Zhu, Ning Chen    - [[Paper]](https://www.ijcai.org/proceedings/2021/470)    ## 2020    - **Monte Carlo Tree Search in Continuous Spaces Using Voronoi Optimistic Optimization with Regret Bounds (AAAI 2020)**    - Beomjoon Kim, Kyungjae Lee, Sungbin Lim, Leslie Pack Kaelbling, TomÃ¡s Lozano-PÃ©rez    - [[Paper]](https://www.aaai.org/Papers/AAAI/2020GB/AAAI-KimB.1282.pdf)    - **Neural Architecture Search Using Deep Neural Networks and Monte Carlo Tree Search (AAAI 2020)**    - Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca    - [[Paper]](https://arxiv.org/abs/1805.07440)    - [[Code]](https://github.com/linnanwang/AlphaX-NASBench101)    - **Monte-Carlo Tree Search in Continuous Action Spaces with Value Gradients (AAAI 2020)**    - Jongmin Lee, Wonseok Jeon, Geon-Hyeong Kim, Kee-Eung Kim    - [[Paper]](https://www.ijcai.org/Proceedings/16/Papers/104.pdf)    - [[Code]](https://github.com/leekwoon/KR-DL-UCT)      - **Approximate Inference in Discrete Distributions with Monte Carlo Tree Search and Value Functions (AISTATS 2020)**    - Lars Buesing, Nicolas Heess, Theophane Weber    - [[Paper]](https://arxiv.org/abs/1910.06862)    - **Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search (ICLR 2020)**    - Anji Liu, Jianshu Chen, Mingze Yu, Yu Zhai, Xuewen Zhou, Ji Liu    - [[Paper]](https://openreview.net/forum?id=BJlQtJSKDB)    - [[Code]](https://github.com/brilee/python_uct)      - **Information Particle Filter Tree: An Online Algorithm for POMDPs with Belief-Based Rewards on Continuous Domains (ICML 2020)**    - Johannes Fischer, Ã–mer Sahin Tas    - [[Paper]](http://proceedings.mlr.press/v119/fischer20a.html)    - [[Code]](https://github.com/johannes-fischer/icml2020_ipft)    - **Sub-Goal Trees a Framework for Goal-Based Reinforcement Learning (ICML 2020)**    - Tom Jurgenson, Or Avner, Edward Groshev, Aviv Tamar    - [[Paper]](https://arxiv.org/abs/2002.12361)      - **Monte-Carlo Tree Search for Scalable Coalition Formation (IJCAI 2020)**    - Feng Wu, Sarvapali D. Ramchurn    - [[Paper]](https://www.ijcai.org/Proceedings/2020/57)    - **Generalized Mean Estimation in Monte-Carlo Tree Search (IJCAI 2020)**    - Tuan Dam, Pascal Klink, Carlo D'Eramo, Jan Peters, Joni Pajarinen    - [[Paper]](https://arxiv.org/abs/1911.00384)    - **Sparse Tree Search Optimality Guarantees in POMDPs with Continuous Observation Spaces (IJCAI 2020)**    - Michael H. Lim, Claire Tomlin, Zachary N. Sunberg    - [[Paper]](https://arxiv.org/abs/1910.04332)      - **Mix and Match: An Optimistic Tree-Search Approach for Learning Models from Mixture Distributions (NeurIPS 2020)**    - Matthew Faw, Rajat Sen, Karthikeyan Shanmugam, Constantine Caramanis, Sanjay Shakkottai    - [[Paper]](https://arxiv.org/abs/1907.10154)    - **Extracting Knowledge from Web Text with Monte Carlo Tree Search (WWW 2020)**    - Guiliang Liu, Xu Li, Jiakang Wang, Mingming Sun, Ping Li    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3366423.3380010)    ## 2019  - **ACE: An Actor Ensemble Algorithm for Continuous Control with Tree Search (AAAI 2019)**    - Shangtong Zhang, Hengshuai Yao    - [[Paper]](https://arxiv.org/abs/1811.02696)    - [[Code]](https://github.com/ShangtongZhang/DeepRL)    - **A Monte Carlo Tree Search Player for Birds of a Feather Solitaire (AAAI 2019)**    - Christian Roberson, Katarina Sperduto    - [[Paper]](https://aaai.org/ojs/index.php/AAAI/article/view/5036)    - [[Code]](http://cs.gettysburg.edu/~tneller/puzzles/boaf/)    - **Vine Copula Structure Learning via Monte Carlo Tree Search (AISTATS 2019)**    - Bo Chang, Shenyi Pan, Harry Joe    - [[Paper]](http://proceedings.mlr.press/v89/chang19a/chang19a.pdf)    - [[Code]](https://github.com/changebo/Vine_MCTS)    - **Noisy Blackbox Optimization using Multi-fidelity Queries: A Tree Search Approach (AISTATS 2019)**    - Rajat Sen, Kirthevasan Kandasamy, Sanjay Shakkottai    - [[Paper]](https://arxiv.org/abs/1810.10482)    - [[Code]](https://github.com/rajatsen91/MFTREE_DET)    - **Reinforcement Learning Based Monte Carlo Tree Search for Temporal Path Discovery (ICDM 2019)**    - Pengfei Ding, Guanfeng Liu, Pengpeng Zhao, An Liu, Zhixu Li, Kai Zheng    - [[Paper]](https://zheng-kai.com/paper/icdm_2019_b.pdf)    - **Monte Carlo Tree Search for Policy Optimization (IJCAI 2019)**    - Xiaobai Ma, Katherine Rose Driggs-Campbell, Zongzhang Zhang, Mykel J. Kochenderfer    - [[Paper]](https://www.ijcai.org/proceedings/2019/0432.pdf)    - **Subgoal-Based Temporal Abstraction in Monte-Carlo Tree Search (IJCAI 2019)**    - Thomas Gabor, Jan Peter, Thomy Phan, Christian Meyer, Claudia Linnhoff-Popien    - [[Paper]](https://www.ijcai.org/proceedings/2019/0772.pdf)    - [[Code]](https://github.com/jnptr/subgoal-mcts)    - **Automated Machine Learning with Monte-Carlo Tree Search (IJCAI 2019)**    - Herilalaina Rakotoarison, Marc Schoenauer, MichÃ¨le Sebag    - [[Paper]](https://www.ijcai.org/proceedings/2019/0457.pdf)    - [[Code]](https://github.com/herilalaina/mosaic_ml)    - **Multiple Policy Value Monte Carlo Tree Search (IJCAI 2019)**    - Li-Cheng Lan, Wei Li, Ting-Han Wei, I-Chen Wu    - [[Paper]](https://www.ijcai.org/proceedings/2019/0653.pdf)    - **Learning Compositional Neural Programs with Recursive Tree Search and Planning (NeurIPS 2019)**    - Thomas Pierrot, Guillaume Ligner, Scott E. Reed, Olivier Sigaud, Nicolas Perrin, Alexandre Laterre, David Kas, Karim Beguir, Nando de Freitas    - [[Paper]](https://arxiv.org/abs/1905.12941)    ## 2018  - **Monte Carlo Methods for the Game Kingdomino (CIG 2018)**    - Magnus Gedda, Mikael Z. Lagerkvist, Martin Butler    - [[Paper]](https://arxiv.org/abs/1807.04458)    - [[Code]](https://github.com/mgedda/kdom-ai)    - [[Game Server]](https://github.com/mratin/kdom)    - **Reset-free Trial-and-Error Learning for Robot Damage Recovery (RAS 2018)**    - Konstantinos Chatzilygeroudis, Vassilis Vassiliades, Jean-Baptiste Mouret    - [[Paper]](https://arxiv.org/pdf/1610.04213.pdf)    - [[Code]](https://github.com/resibots/chatzilygeroudis_2018_rte)    - [[MCTS C++ Library]](https://github.com/resibots/mcts)    - **Memory-Augmented Monte Carlo Tree Search (AAAI 2018)**    - Chenjun Xiao, Jincheng Mei, Martin MÃ¼ller    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17139)    - **Feedback-Based Tree Search for Reinforcement Learning (ICML 2018)**    - Daniel R. Jiang, Emmanuel Ekwedike, Han Liu    - [[Paper]](https://arxiv.org/abs/1805.05935)    - **Extended Increasing Cost Tree Search for Non-Unit Cost Domains (IJCAI 2018)**    - Thayne T. Walker, Nathan R. Sturtevant, Ariel Felner    - [[Paper]](https://www.ijcai.org/proceedings/2018/74)    - **Three-Head Neural Network Architecture for Monte Carlo Tree Search (IJCAI 2018)**    - Chao Gao, Martin MÃ¼ller, Ryan Hayward    - [[Paper]](https://www.ijcai.org/proceedings/2018/523)    - **Bidding in Periodic Double Auctions Using Heuristics and Dynamic Monte Carlo Tree Search (IJCAI 2018)**    - Moinul Morshed Porag Chowdhury, Christopher Kiekintveld, Son Tran, William Yeoh    - [[Paper]](https://www.ijcai.org/proceedings/2018/23)    - **Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search (NIPS 2018)**    - Zhuwen Li, Qifeng Chen, Vladlen Koltun    - [[Paper]](https://arxiv.org/abs/1810.10659)    - **M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search (NIPS 2018)**    - Yelong Shen, Jianshu Chen, Po-Sen Huang, Yuqing Guo, Jianfeng Gao    - [[Paper]](https://arxiv.org/abs/1802.04394)    - **Single-Agent Policy Tree Search With Guarantees (NIPS 2018)**    - Laurent Orseau, Levi Lelis, Tor Lattimore, Theophane Weber    - [[Paper]](https://arxiv.org/abs/1811.10928)    - **Monte-Carlo Tree Search for Constrained POMDPs (NIPS 2018)**    - Jongmin Lee, Geon-hyeong Kim, Pascal Poupart, Kee-Eung Kim    - [[Paper]](https://cs.uwaterloo.ca/~ppoupart/publications/constrained-pomdps/mcts-constrained-pomdps-paper.pdf)    ## 2017  - **An Analysis of Monte Carlo Tree Search (AAAI 2017)**    - Steven James, George Dimitri Konidaris, Benjamin Rosman    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14886)    - **Beyond Monte Carlo Tree Search: Playing Go with Deep Alternative Neural Network and Long-Term Evaluation (AAAI 2017)**    - Jinzhuo Wang, Wenmin Wang, Ronggang Wang, Wen Gao    - [[Paper]](https://arxiv.org/abs/1706.04052)    - **Designing Better Playlists with Monte Carlo Tree Search (AAAI 2017)**    - Elad Liebman, Piyush Khandelwal, Maytal Saar-Tsechansky, Peter Stone    - [[Paper]](https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IAAI2017-eladlieb.pdf)    - **Learning in POMDPs with Monte Carlo Tree Search (ICML 2017)**    - Sammie Katt, Frans A. Oliehoek, Christopher Amato    - [[Paper]](https://arxiv.org/abs/1806.05631)    - **Learning to Run Heuristics in Tree Search (IJCAI 2017)**    - Elias B. Khalil, Bistra Dilkina, George L. Nemhauser, Shabbir Ahmed, Yufen Shao    - [[Paper]](https://www.ijcai.org/proceedings/2017/92)    - **Estimating the Size of Search Trees by Sampling with Domain Knowledge (IJCAI 2017)**    - Gleb Belov, Samuel Esler, Dylan Fernando, Pierre Le Bodic, George L. Nemhauser    - [[Paper]](https://www.ijcai.org/proceedings/2017/67)    - **A Monte Carlo Tree Search Approach to Active Malware Analysis (IJCAI 2017)**    - Riccardo Sartea, Alessandro Farinelli    - [[Paper]](https://www.ijcai.org/proceedings/2017/535)    - **Monte-Carlo Tree Search by Best Arm Identification (NIPS 2017)**    - Emilie Kaufmann, Wouter M. Koolen    - [[Paper]](https://arxiv.org/abs/1706.02986)    - **Thinking Fast and Slow with Deep Learning and Tree Search (NIPS 2017)**    - Thomas Anthony, Zheng Tian, David Barber    - [[Paper]](https://arxiv.org/abs/1705.08439)    - **Monte-Carlo Tree Search using Batch Value of Perfect Information (UAI 2017)**    - Shahaf S. Shperberg, Solomon Eyal Shimony, Ariel Felner    - [[Paper]](http://auai.org/uai2017/proceedings/papers/37.pdf)    ## 2016  - **Using Domain Knowledge to Improve Monte-Carlo Tree Search Performance in Parameterized Poker Squares (AAAI 2016)**    - Robert Arrington, Clay Langley, Steven Bogaerts    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11809)    - **Monte Carlo Tree Search for Multi-Robot Task Allocation (AAAI 2016)**    - Bilal Kartal, Ernesto Nunes, Julio Godoy, Maria L. Gini    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12154)    - **Large Scale Hard Sample Mining with Monte Carlo Tree Search (CVPR 2016)**    - Olivier CanÃ©vet, FranÃ§ois Fleuret    - [[Paper]](https://www.idiap.ch/~fleuret/papers/canevet-fleuret-cvpr2016.pdf)    - **On the Analysis of Complex Backup Strategies in Monte Carlo Tree Search (ICML 2016)**    - Piyush Khandelwal, Elad Liebman, Scott Niekum, Peter Stone    - [[Paper]](https://www.cs.utexas.edu/~eladlieb/ICML2016.pdf)    - **Deep Learning for Reward Design to Improve Monte Carlo Tree Search in ATARI Games (IJCAI 2016)**    - Xiaoxiao Guo, Satinder P. Singh, Richard L. Lewis, Honglak Lee    - [[Paper]](https://arxiv.org/abs/1604.07095)    - **Monte Carlo Tree Search in Continuous Action Spaces with Execution Uncertainty (IJCAI 2016)**    - Timothy Yee, Viliam LisÃ½, Michael H. Bowling    - [[Paper]](https://www.ijcai.org/Proceedings/16/Papers/104.pdf)    - **Learning Predictive State Representations via Monte-Carlo Tree Search (IJCAI 2016)**    - Yunlong Liu, Hexing Zhu, Yifeng Zeng, Zongxiong Dai    - [[Paper]](https://pdfs.semanticscholar.org/8056/df11094fc96d76826403f8b339dc14aa821f.pdf)    ## 2015  - **Efficient Globally Optimal Consensus Maximisation with Tree Search (CVPR 2015)**    - Tat-Jun Chin, Pulak Purkait, Anders P. Eriksson, David Suter    - [[Paper]](https://zpascal.net/cvpr2015/Chin_Efficient_Globally_Optimal_2015_CVPR_paper.pdf)    - **Interplanetary Trajectory Planning with Monte Carlo Tree Search (IJCAI 2015)**    - Daniel Hennes, Dario Izzo    - [[Paper]](https://pdfs.semanticscholar.org/ce42/53ca1c5b16e96cdbefae75649cd2588f42f3.pdf)    ## 2014  - **State Aggregation in Monte Carlo Tree Search (AAAI 2014)**    - Jesse Hostetler, Alan Fern, Tom Dietterich    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/download/8439/8712)    - **Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning (NIPS 2014)**    - Xiaoxiao Guo, Satinder P. Singh, Honglak Lee, Richard L. Lewis, Xiaoshi Wang    - [[Paper]](https://web.eecs.umich.edu/~baveja/Papers/UCTtoCNNsAtariGames-FinalVersion.pdf)    - **Learning Partial Policies to Speedup MDP Tree Search (UAI 2014)**    - Jervis Pinto, Alan Fern    - [[Paper]](http://www.jmlr.org/papers/volume18/15-251/15-251.pdf)    ## 2013  - **Monte Carlo Tree Search for Scheduling Activity Recognition (ICCV 2013)**    - Mohamed R. Amer, Sinisa Todorovic, Alan Fern, Song-Chun Zhu    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.405.5916&rep=rep1&type=pdf)    - **Convergence of Monte Carlo Tree Search in Simultaneous Move Games (NIPS 2013)**    - Viliam LisÃ½, Vojtech KovarÃ­k, Marc Lanctot, Branislav BosanskÃ½    - [[Paper]](https://papers.nips.cc/paper/5145-convergence-of-monte-carlo-tree-search-in-simultaneous-move-games)    - **Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search (NIPS 2013)**    - Aijun Bai, Feng Wu, Xiaoping Chen    - [[Paper]](https://papers.nips.cc/paper/5111-bayesian-mixture-modelling-and-inference-based-thompson-sampling-in-monte-carlo-tree-search)    ## 2012  - **Generalized Monte-Carlo Tree Search Extensions for General Game Playing (AAAI 2012)**    - Hilmar Finnsson    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewFile/4935/5300)    ## 2011  - **A Local Monte Carlo Tree Search Approach in Deterministic Planning (AAAI 2011)**    - Fan Xie, Hootan Nakhost, Martin MÃ¼ller    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.699.3833&rep=rep1&type=pdf)    - **Real-Time Solving of Quantified CSPs Based on Monte-Carlo Game Tree Search (IJCAI 2011)**    - Satomi Baba, Yongjoon Joe, Atsushi Iwasaki, Makoto Yokoo    - [[Paper]](https://www.ijcai.org/Proceedings/11/Papers/116.pdf)    - **Nested Rollout Policy Adaptation for Monte Carlo Tree Search (IJCAI 2011)**    - Christopher D. Rosin    - [[Paper]](https://www.ijcai.org/Proceedings/11/Papers/115.pdf)    - **Variance Reduction in Monte-Carlo Tree Search (NIPS 2011)**    - Joel Veness, Marc Lanctot, Michael H. Bowling    - [[Paper]](https://papers.nips.cc/paper/4288-variance-reduction-in-monte-carlo-tree-search)    - **Learning Is Planning: Near Bayes-Optimal Reinforcement Learning via Monte-Carlo Tree Search (UAI 2011)**    - John Asmuth, Michael L. Littman    - [[Paper]](https://arxiv.org/abs/1202.3699)    ## 2010  - **Understanding the Success of Perfect Information Monte Carlo Sampling in Game Tree Search (AAAI 2010)**    - Jeffrey Richard Long, Nathan R. Sturtevant, Michael Buro, Timothy Furtak    - [[Paper]](https://pdfs.semanticscholar.org/011e/2c79575721764c127e210c9d8105a6305e70.pdf)    - **Bayesian Inference in Monte-Carlo Tree Search (UAI 2010)**    - Gerald Tesauro, V. T. Rajan, Richard Segal    - [[Paper]](https://arxiv.org/abs/1203.3519)    ## 2009  - **Monte Carlo Tree Search Techniques in the Game of Kriegspiel (IJCAI 2009)**    - Paolo Ciancarini, Gian Piero Favini    - [[Paper]](https://www.aaai.org/ocs/index.php/IJCAI/IJCAI-09/paper/viewFile/396/693)    - **Bootstrapping from Game Tree Search (NIPS 2009)**    - Joel Veness, David Silver, William T. B. Uther, Alan Blair    - [[Paper]](https://papers.nips.cc/paper/3722-bootstrapping-from-game-tree-search)    ## 2008  - **Direct Mining of Discriminative and Essential Frequent Patterns via Model-Based Search Tree (KDD 2008)**    - Wei Fan, Kun Zhang, Hong Cheng, Jing Gao, Xifeng Yan, Jiawei Han, Philip S. Yu, Olivier Verscheure    - [[Paper]](http://www1.se.cuhk.edu.hk/~hcheng/paper/kdd08mbt.pdf)    ## 2007  - **Bandit Algorithms for Tree Search (UAI 2007)**    - Pierre-Arnaud Coquelin, RÃ©mi Munos    - [[Paper]](https://arxiv.org/pdf/1408.2028.pdf)    ## 2006  - **Properties of Forward Pruning in Game-Tree Search (AAAI 2006)**    - Yew Jin Lim, Wee Sun Lee    - [[Paper]](https://dl.acm.org/citation.cfm?id=1597351)    - **Graph Branch Algorithm: An Optimum Tree Search Method for Scored Dependency Graph with Arc Co-Occurrence Constraints (ACL 2006)**    - Hideki Hirakawa    - [[Paper]](https://www.aclweb.org/anthology/P06-2047/)    ## 2005  - **Game-Tree Search with Combinatorially Large Belief States (IJCAI 2005)**    - Austin Parker, Dana S. Nau, V. S. Subrahmanian    - [[Paper]](https://www.ijcai.org/Proceedings/05/Papers/0878.pdf)    ## 2003  - **Solving Finite Domain Constraint Hierarchies by Local Consistency and Tree Search (IJCAI 2003)**    - Stefano Bistarelli, Philippe Codognet, Kin Chuen Hui, Jimmy Ho-Man Lee    - [[Paper]](https://www.ijcai.org/Proceedings/03/Papers/200.pdf)    ## 2001  - **Incomplete Tree Search using Adaptive Probing (IJCAI 2001)**    - Wheeler Ruml    - [[Paper]](https://dash.harvard.edu/bitstream/handle/1/23017275/tr-02-01.pdf?sequence%3D1)    ## 1998  - **KnightCap: A Chess Programm That Learns by Combining TD with Game-Tree Search (ICML 1998)**    - Jonathan Baxter, Andrew Tridgell, Lex Weaver    - [[Paper]](https://arxiv.org/abs/cs/9901002)    ## 1988  - **A Tree Search Algorithm for Target Detection in Image Sequences (CVPR 1988)**    - Steven D. Blostein, Thomas S. Huang    - [[Paper]](https://ieeexplore.ieee.org/document/196309)    --------------------------------------------------------------------------------    **License**    - [CC0 Universal](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers/blob/master/LICENSE)    -------------------------------------------------------------------------------- """
Big data;https://github.com/nikolaypavlov/MLPNeuralNet;"""#MLPNeuralNet  [![Build Status](https://travis-ci.org/nikolaypavlov/MLPNeuralNet.svg?branch=master)](https://travis-ci.org/nikolaypavlov/MLPNeuralNet)  [![Join the chat at https://gitter.im/nikolaypavlov/MLPNeuralNet](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/nikolaypavlov/MLPNeuralNet?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    `MLPNeuralNet` is a fast [multilayer perceptron](http://en.wikipedia.org/wiki/Multilayer_perceptron) neural network library for iOS and Mac OS X. `MLPNeuralNet` predicts new examples through trained neural networks. It is built on top of Apple's [Accelerate Framework](https://developer.apple.com/library/ios/documentation/Accelerate/Reference/AccelerateFWRef/_index.html) using vectored operations and hardware acceleration (if available).    ![Neural Network](http://nikolaypavlov.github.io/MLPNeuralNet/images/500px-Artificial_neural_network.png)    ##Why would you use it?  Imagine that you have engineered a prediction model using Matlab (Python or R) and would like to use it in an iOS application. If that's the case, `MLPNeuralNet` is exactly what you need. `MLPNeuralNet` is designed to load and run models in [forward propagation](http://en.wikipedia.org/wiki/Backpropagation#Phase_1:_Propagation) mode only.    ###Features  - [Classification](http://en.wikipedia.org/wiki/Binary_classification), [Multi-class classification](http://en.wikipedia.org/wiki/Multiclass_classification) and regression output  - Vectorised implementation  - Works with double precision  - Multiple hidden layers or none (in that case it's same as logistic/linear regression)    ##Quick Example  Let's deploy a model for the AND function  ([conjunction](http://en.wikipedia.org/wiki/Logical_conjunction)) that works as follows: (of course, you do not need to use a neural network for this in the real world)    |X1 |X2 | Y |  |:-:|:-:|:-:|  | 0 | 0 | 0 |  | 1 | 0 | 0 |  | 0 | 1 | 0 |  | 1 | 1 | 1 |    Our model has the following weights and network configuration:    ![AND Model Example](http://nikolaypavlov.github.io/MLPNeuralNet/images/network-arch.png)    ```objective  // Use the designated initialiser to pass the network configuration and weights to the model.  // Note: You do not need to specify the biased units (+1 above) in the configuration.    NSArray *netConfig = @[@2, @1];  double wts[] = {-30, 20, 20};  NSData *weights = [NSData dataWithBytes:wts length:sizeof(wts)];    MLPNeuralNet *model = [[MLPNeuralNet alloc] initWithLayerConfig:netConfig                                                          weights:weights                                                       outputMode:MLPClassification];  // Predict output of the model for new sample  double sample[] = {0, 1};  NSData * vector = [NSData dataWithBytes:sample length:sizeof(sample)];  NSMutableData * prediction = [NSMutableData dataWithLength:sizeof(double)];  [model predictByFeatureVector:vector intoPredictionVector:prediction];    double * assessment = (double *)prediction.bytes;  NSLog(@""Model assessment is %f"", assessment[0]);  ```    ##Extended Example  Let's say you trained a net using pybrain or even your own home brewed implementation.    ![Extended Example](http://i.imgur.com/v2kMTUH.png)    ```objective  // Use the designated initialiser to pass the network configuration and weights to the model.  // Note: You do not need to specify the biased units (+1 above) in the configuration.    NSArray *netConfig = @[@3, @2, @1];  double wts[] = {b1, w1, w2, w3, b2, w4, w5, w6, b3, w7, w8};  NSData *weights = [NSData dataWithBytes:wts length:sizeof(wts)];    MLPNeuralNet *model = [[MLPNeuralNet alloc] initWithLayerConfig:netConfig                                                          weights:weights                                                       outputMode:MLPClassification];  model.hiddenActivationFunction = MLPSigmoid;  model.outputActivationFunction = MLPNone;    // Predict output of the model for new sample  double sample[] = {0, 1, 2};  NSData * vector = [NSData dataWithBytes:sample length:sizeof(sample)];  NSMutableData * prediction = [NSMutableData dataWithLength:sizeof(double)];  [model predictByFeatureVector:vector intoPredictionVector:prediction];    double * assessment = (double *)prediction.bytes;  NSLog(@""Model assessment is %f"", assessment[0]);  ```    ##Getting Started  The following instructions describe how to setup and install `MLPNeuralNet` using [CocoaPods](http://cocoapods.org/). It is written for Xcode 5 and the iOS 7.x(+) SDK. If you are not familiar with CocoaPods, just clone the repository and import `MLPNeuralNet` directly as a subproject.    ##Installing through CocoaPods  Please add the following line to your *Podfile*.    ```  pod 'MLPNeuralNet', '~> 1.0.0'  ```    ##Installing through Carthage  Please add the following line to your *Cartfile*.    ```  github ""nikolaypavlov/MLPNeuralNet"" ""master""  ```    ##Import `MLPNeuralNet.h`  Do not forget to add the following line to the top of your model:  ```objectivec  #import ""MLPNeuralNet.h""  ```    ##How many weights do I need to initialise network X->Y->Z?  Most of the popular libraries (including `MLPNeuralNet`) implicitly add biased units for each of the layers except the last one. Assuming these additional units, the total number of weights are `(X + 1) * Y + (Y + 1) * Z`.    ##Importing weights from other libs.  You can do this for *some* of the neural network packages available.    ###R nnet library:   ```r  #Assuming nnet_model is a trained neural network  nnet_model$wts  ```    ###Python NeuroLab    ```python  #Where net argument is an neurolab.core.Net object  import neurolab as nl  import numpy as np    def getweights(net):       vec = []       for layer in net.layers:           b = layer.np['b']           w = layer.np['w']           newvec = np.ravel(np.concatenate((b, np.ravel(w,order='F'))).reshape((layer.ci+1, layer.cn)), order = 'F')           [vec.append(nv) for nv in newvec]       return np.array(vec)    ```    ###Python neon  ```python  import numpy as np    def layer_names(params):      layer_names = params.keys()      layer_names.remove('epochs_complete')      # Sort layers by their appearance in the model architecture      # Since neon appands the index to the layer name we will use it to sort      layer_names.sort(key=lambda x: int(x.split(""_"")[-1]))      return layer_names    def getweights(file_name):      vec = []      # Load a stored model file from disk (should have extension prm)      params = pkl.load(open(file_name, 'r'))      layers = layer_names(params)            for layer in layers:          # Make sure our model has biases activated, otherwise add zeros here          b = params[layer]['biases']          w = params[layer]['weights']            newvec = np.ravel(np.hstack((b,w)))          [vec.append(nv) for nv in newvec]      return vec    # An example call  getweights(expanduser('~/data/workout-dl/workout-ep100.prm'))  ```    ###Python keras  ```python  import numpy as np    def get_weights_from_keras_model(model):      vec = np.array([])      for i in xrange(0, len(model.get_weights()), 2):          bias = model.get_weights()[i + 1]          weights_matrix = model.get_weights()[i]            newvec = np.ravel(np.concatenate((bias.reshape(-1, 1), weights_matrix.T), axis=1))          vec = np.append(vec, newvec)      return np.array(vec)    ```    ## Performance benchmarks  In this test, the neural network has grown layer by layer from a `1 -> 1` configuration to a `200 -> 200 -> 200 -> 1` configuration. At each step, the output is calculated and benchmarked using random input vectorisation and weights. Total number of weights grow from 2 to 80601 accordingly. I understand that the test is quite synthetic, but I hope it illustrates the performance. I will be happy if you can propose a better one! :)    ![MLPNeuralNet Performance Benchmark](http://nikolaypavlov.github.io/MLPNeuralNet/images/mlp-bench-regression-ios.png)    ##Unit Testing  `MLPNeuralNet` includes a diverse suite of unit tests in the `/MLPNeuralNetTests` subdirectory. You can execute them using the ``MLPNeuralNet`` scheme within Xcode.    ##Acknowledgements  `MLPNeuralNet` was inspired by:    - [Andrew Ng's course on Machine Learning](https://www.coursera.org/course/ml).  - [Jeff Leek course on Data Analysis](https://www.coursera.org/course/dataanalysis).    Credits:    - Neural Network image was taken from [Wikipedia Commons](http://en.wikipedia.org/wiki/File:Artificial_neural_network.svg).    ##Contact Me  Maintainer: [Mykola Pavlov](http://github.com/nikolaypavlov/) (me@nikolaypavlov.com)    **Please let me know on how you use `MLPNeuralNet` for some real world problems.**    ##Licensing  `MLPNeuralNet` is released under the BSD license. See the LICENSE file for more information. """
Big data;https://github.com/ml-tooling/ml-workspace;"""<h1 align=""center"">      <a href=""https://github.com/ml-tooling/ml-workspace"" title=""ML Workspace Home"">      <img width=50% alt="""" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/ml-workspace-logo.png""> </a>      <br>  </h1>    <p align=""center"">      <strong>All-in-one web-based development environment for machine learning</strong>  </p>    <p align=""center"">      <a href=""https://hub.docker.com/r/mltooling/ml-workspace"" title=""Docker Image Version""><img src=""https://img.shields.io/docker/v/mltooling/ml-workspace?color=blue&sort=semver""></a>      <a href=""https://hub.docker.com/r/mltooling/ml-workspace"" title=""Docker Pulls""><img src=""https://img.shields.io/docker/pulls/mltooling/ml-workspace.svg?color=blue""></a>      <a href=""https://hub.docker.com/r/mltooling/ml-workspace"" title=""Docker Image Size""><img src=""https://img.shields.io/docker/image-size/mltooling/ml-workspace?color=blue&sort=semver""></a>      <a href=""https://gitter.im/ml-tooling/ml-workspace"" title=""Chat on Gitter""><img src=""https://badges.gitter.im/ml-tooling/ml-workspace.svg""></a>      <a href=""https://mltooling.substack.com/subscribe"" title=""Subscribe to newsletter""><img src=""http://bit.ly/2Md9rxM""></a>      <a href=""https://twitter.com/mltooling"" title=""Follow on Twitter""><img src=""https://img.shields.io/twitter/follow/mltooling.svg?style=social&label=Follow""></a>  </p>    <p align=""center"">    <a href=""#getting-started"">Getting Started</a> â€¢    <a href=""#features"">Features & Screenshots</a> â€¢    <a href=""#support"">Support</a> â€¢    <a href=""https://github.com/ml-tooling/ml-workspace/issues/new?labels=bug&template=01_bug-report.md"">Report a Bug</a> â€¢    <a href=""#faq"">FAQ</a> â€¢    <a href=""#known-issues"">Known Issues</a> â€¢    <a href=""#contribution"">Contribution</a>  </p>    The ML workspace is an all-in-one web-based IDE specialized for machine learning and data science. It is simple to deploy and gets you started within minutes to productively built ML solutions on your own machines. This workspace is the ultimate tool for developers preloaded with a variety of popular data science libraries (e.g., Tensorflow, PyTorch, Keras, Sklearn) and dev tools (e.g., Jupyter, VS Code, Tensorboard) perfectly configured, optimized, and integrated.    ## Highlights    - ðŸ’«&nbsp; Jupyter, JupyterLab, and Visual Studio Code web-based IDEs.  - ðŸ—ƒ&nbsp; Pre-installed with many popular data science libraries & tools.  - ðŸ–¥&nbsp; Full Linux desktop GUI accessible via web browser.  - ðŸ”€&nbsp; Seamless Git integration optimized for notebooks.  - ðŸ“ˆ&nbsp; Integrated hardware & training monitoring via Tensorboard & Netdata.  - ðŸšª&nbsp; Access from anywhere via Web, SSH, or VNC under a single port.  - ðŸŽ›&nbsp; Usable as remote kernel (Jupyter) or remote machine (VS Code) via SSH.  - ðŸ³&nbsp; Easy to deploy on Mac, Linux, and Windows via Docker.    <br>    ## Getting Started    <p>  <a href=""https://labs.play-with-docker.com/?stack=https://raw.githubusercontent.com/ml-tooling/ml-workspace/main/deployment/play-with-docker/docker-compose.yml"" title=""Docker Image Metadata"" target=""_blank""><img src=""https://cdn.rawgit.com/play-with-docker/stacks/cff22438/assets/images/button.png"" alt=""Try in PWD"" width=""100px""></a>  </p>    ### Prerequisites    The workspace requires **Docker** to be installed on your machine ([ðŸ“– Installation Guide](https://docs.docker.com/install/#supported-platforms)).    ### Start single instance    Deploying a single workspace instance is as simple as:    ```bash  docker run -p 8080:8080 mltooling/ml-workspace:0.13.2  ```    VoilÃ , that was easy! Now, Docker will pull the latest workspace image to your machine. This may take a few minutes, depending on your internet speed. Once the workspace is started, you can access it via http://localhost:8080.    > _If started on another machine or with a different port, make sure to use the machine's IP/DNS and/or the exposed port._    To deploy a single instance for productive usage, we recommend to apply at least the following options:    ```bash  docker run -d \      -p 8080:8080 \      --name ""ml-workspace"" \      -v ""${PWD}:/workspace"" \      --env AUTHENTICATE_VIA_JUPYTER=""mytoken"" \      --shm-size 512m \      --restart always \      mltooling/ml-workspace:0.13.2  ```    This command runs the container in background (`-d`), mounts your current working directory into the `/workspace` folder (`-v`), secures the workspace via a provided token (`--env AUTHENTICATE_VIA_JUPYTER`), provides 512MB of shared memory (`--shm-size`) to prevent unexpected crashes (see [known issues section](#known-issues)), and keeps the container running even on system restarts (`--restart always`). You can find additional options for docker run [here](https://docs.docker.com/engine/reference/commandline/run/) and workspace configuration options in [the section below](#Configuration).    ### Configuration Options    The workspace provides a variety of configuration options that can be used by setting environment variables (via docker run option: `--env`).    <details>  <summary>Configuration options (click to expand...)</summary>    <table>      <tr>          <th>Variable</th>          <th>Description</th>          <th>Default</th>      </tr>      <tr>          <td>WORKSPACE_BASE_URL</td>          <td>The base URL under which Jupyter and all other tools will be reachable from.</td>          <td>/</td>      </tr>      <tr>          <td>WORKSPACE_SSL_ENABLED</td>          <td>Enable or disable SSL. When set to true, either certificate (cert.crt) must be mounted to <code>/resources/ssl</code> or, if not, the container generates self-signed certificate.</td>          <td>false</td>      </tr>      <tr>          <td>WORKSPACE_AUTH_USER</td>          <td>Basic auth user name. To enable basic auth, both the user and password need to be set. We recommend to use the <code>AUTHENTICATE_VIA_JUPYTER</code> for securing the workspace.</td>          <td></td>      </tr>      <tr>          <td>WORKSPACE_AUTH_PASSWORD</td>          <td>Basic auth user password. To enable basic auth, both the user and password need to be set. We recommend to use the <code>AUTHENTICATE_VIA_JUPYTER</code> for securing the workspace.</td>          <td></td>      </tr>      <tr>          <td>WORKSPACE_PORT</td>          <td>Configures the main container-internal port of the workspace proxy. For most scenarios, this configuration should not be changed, and the port configuration via Docker should be used instead of the workspace should be accessible from a different port.</td>          <td>8080</td>      </tr>      <tr>          <td>CONFIG_BACKUP_ENABLED</td>          <td>Automatically backup and restore user configuration to the persisted <code>/workspace</code> folder, such as the .ssh, .jupyter, or .gitconfig from the users home directory.</td>          <td>true</td>      </tr>      <tr>          <td>SHARED_LINKS_ENABLED</td>          <td>Enable or disable the capability to share resources via external links. This is used to enable file sharing, access to workspace-internal ports, and easy command-based SSH setup. All shared links are protected via a token. However, there are certain risks since the token cannot be easily invalidated after sharing and does not expire.</td>          <td>true</td>      </tr>      <tr>          <td>INCLUDE_TUTORIALS</td>          <td>If <code>true</code>, a selection of tutorial and introduction notebooks are added to the <code>/workspace</code> folder at container startup, but only if the folder is empty.</td>          <td>true</td>      </tr>      <tr>          <td>MAX_NUM_THREADS</td>          <td>The number of threads used for computations when using various common libraries (MKL, OPENBLAS, OMP, NUMBA, ...). You can also use <code>auto</code> to let the workspace dynamically determine the number of threads based on available CPU resources. This configuration can be overwritten by the user from within the workspace. Generally, it is good to set it at or below the number of CPUs available to the workspace.</td>          <td>auto</td>      </tr>      <tr>          <td colspan=""3""><b>Jupyter Configuration:</b></td>      </tr>      <tr>          <td>SHUTDOWN_INACTIVE_KERNELS</td>          <td>Automatically shutdown inactive kernels after a given timeout (to clean up memory or GPU resources). Value can be either a timeout in seconds or set to <code>true</code> with a default value of 48h.</td>          <td>false</td>      </tr>      <tr>          <td>AUTHENTICATE_VIA_JUPYTER</td>          <td>If <code>true</code>, all HTTP requests will be authenticated against the Jupyter server, meaning that the authentication method configured with Jupyter will be used for all other tools as well. This can be deactivated with <code>false</code>. Any other value will activate this authentication and are applied as token via NotebookApp.token configuration of Jupyter.</td>          <td>false</td>      </tr>      <tr>          <td>NOTEBOOK_ARGS</td>          <td>Add and overwrite Jupyter configuration options via command line args. Refer to <a href=""https://jupyter-notebook.readthedocs.io/en/stable/config.html"">this overview</a> for all options.</td>          <td></td>      </tr>  </table>    </details>    ### Persist Data    To persist the data, you need to mount a volume into `/workspace` (via docker run option: `-v`).    <details>  <summary>Details (click to expand...)</summary>    The default work directory within the container is `/workspace`, which is also the root directory of the Jupyter instance. The `/workspace` directory is intended to be used for all the important work artifacts. Data within other directories of the server (e.g., `/root`) might get lost at container restarts.  </details>    ### Enable Authentication    We strongly recommend enabling authentication via one of the following two options. For both options, the user will be required to authenticate for accessing any of the pre-installed tools.    > _The authentication only works for all tools accessed through the main workspace port (default: `8080`). This works for all preinstalled tools and the [Access Ports](#access-ports) feature. If you expose another port of the container, please make sure to secure it with authentication as well!_    <details>  <summary>Details (click to expand...)</summary>    #### Token-based Authentication via Jupyter (recommended)    Activate the token-based authentication based on the authentication implementation of Jupyter via the `AUTHENTICATE_VIA_JUPYTER` variable:    ```bash  docker run -p 8080:8080 --env AUTHENTICATE_VIA_JUPYTER=""mytoken"" mltooling/ml-workspace:0.13.2  ```    You can also use `<generated>` to let Jupyter generate a random token that is printed out on the container logs. A value of `true` will not set any token but activate that every request to any tool in the workspace will be checked with the Jupyter instance if the user is authenticated. This is used for tools like JupyterHub, which configures its own way of authentication.    #### Basic Authentication via Nginx    Activate the basic authentication via the `WORKSPACE_AUTH_USER` and `WORKSPACE_AUTH_PASSWORD` variable:    ```bash  docker run -p 8080:8080 --env WORKSPACE_AUTH_USER=""user"" --env WORKSPACE_AUTH_PASSWORD=""pwd"" mltooling/ml-workspace:0.13.2  ```    The basic authentication is configured via the nginx proxy and might be more performant compared to the other option since with `AUTHENTICATE_VIA_JUPYTER` every request to any tool in the workspace will check via the Jupyter instance if the user (based on the request cookies) is authenticated.    </details>    ### Enable SSL/HTTPS    We recommend enabling SSL so that the workspace is accessible via HTTPS (encrypted communication). SSL encryption can be activated via the `WORKSPACE_SSL_ENABLED` variable.     <details>  <summary>Details (click to expand...)</summary>    When set to `true`, either the `cert.crt` and `cert.key` file must be mounted to `/resources/ssl` or, if the certificate files do not exist, the container generates self-signed certificates. For example, if the `/path/with/certificate/files` on the local system contains a valid certificate for the host domain (`cert.crt` and `cert.key` file), it can be used from the workspace as shown below:    ```bash  docker run \      -p 8080:8080 \      --env WORKSPACE_SSL_ENABLED=""true"" \      -v /path/with/certificate/files:/resources/ssl:ro \      mltooling/ml-workspace:0.13.2  ```    If you want to host the workspace on a public domain, we recommend to use [Let's encrypt](https://letsencrypt.org/getting-started/) to get a trusted certificate for your domain.  To use the generated certificate (e.g., via [certbot](https://certbot.eff.org/) tool) for the workspace, the `privkey.pem` corresponds to the `cert.key` file and the `fullchain.pem` to the `cert.crt` file.    > _When you enable SSL support, you must access the workspace over `https://`, not over plain `http://`._    </details>    ### Limit Memory & CPU    By default, the workspace container has no resource constraints and can use as much of a given resource as the hostâ€™s kernel scheduler allows. Docker provides ways to control how much memory, or CPU a container can use, by setting runtime configuration flags of the docker run command.    > _The workspace requires atleast 2 CPUs and 500MB to run stable and be usable._    <details>  <summary>Details (click to expand...)</summary>    For example, the following command restricts the workspace to only use a maximum of 8 CPUs, 16 GB of memory, and 1 GB of shared memory (see [Known Issues](#known-issues)):    ```bash  docker run -p 8080:8080 --cpus=8 --memory=16g --shm-size=1G mltooling/ml-workspace:0.13.2  ```    > ðŸ“– _For more options and documentation on resource constraints, please refer to the [official docker guide](https://docs.docker.com/config/containers/resource_constraints/)._    </details>    ### Enable Proxy    If a proxy is required, you can pass the proxy configuration via the `HTTP_PROXY`, `HTTPS_PROXY`, and `NO_PROXY` environment variables.    ### Workspace Flavors    In addition to the main workspace image (`mltooling/ml-workspace`), we provide other image flavors that extend the features or minimize the image size to support a variety of use cases.    #### Minimal Flavor    <p>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace"" title=""Docker Image Version""><img src=""https://img.shields.io/docker/v/mltooling/ml-workspace?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-minimal"" title=""Docker Image Size""><img src=""https://img.shields.io/docker/image-size/mltooling/ml-workspace-minimal?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-minimal"" title=""Docker Pulls""><img src=""https://img.shields.io/docker/pulls/mltooling/ml-workspace-minimal.svg""></a>  </p>    <details>  <summary>Details (click to expand...)</summary>    The minimal flavor (`mltooling/ml-workspace-minimal`) is our smallest image that contains most of the tools and features described in the [features section](#features) without most of the python libraries that are pre-installed in our main image. Any Python library or excluded tool can be installed manually during runtime by the user.    ```bash  docker run -p 8080:8080 mltooling/ml-workspace-minimal:0.13.2  ```  </details>    #### R Flavor    <p>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-r"" title=""Docker Image Version""><img src=""https://img.shields.io/docker/v/mltooling/ml-workspace-r?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-r"" title=""Docker Image Size""><img src=""https://img.shields.io/docker/image-size/mltooling/ml-workspace-r?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-r"" title=""Docker Pulls""><img src=""https://img.shields.io/docker/pulls/mltooling/ml-workspace-r.svg""></a>  </p>    <details>  <summary>Details (click to expand...)</summary>    The R flavor (`mltooling/ml-workspace-r`) is based on our default workspace image and extends it with the R-interpreter, R-Jupyter kernel, RStudio server (access via `Open Tool -> RStudio`), and a variety of popular packages from the R ecosystem.    ```bash  docker run -p 8080:8080 mltooling/ml-workspace-r:0.12.1  ```  </details>    #### Spark Flavor    <p>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-spark"" title=""Docker Image Version""><img src=""https://img.shields.io/docker/v/mltooling/ml-workspace-spark?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-spark"" title=""Docker Image Size""><img src=""https://img.shields.io/docker/image-size/mltooling/ml-workspace-spark?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-spark"" title=""Docker Pulls""><img src=""https://img.shields.io/docker/pulls/mltooling/ml-workspace-spark.svg""></a>  </p>    <details>  <summary>Details (click to expand...)</summary>    The Spark flavor (`mltooling/ml-workspace-spark`) is based on our R-flavor workspace image and extends it with the Spark runtime, Spark-Jupyter kernel, Zeppelin Notebook (access via `Open Tool -> Zeppelin`), PySpark, Hadoop, Java Kernel, and a few additional libraries & Jupyter extensions.    ```bash  docker run -p 8080:8080 mltooling/ml-workspace-spark:0.12.1  ```    </details>    #### GPU Flavor    <p>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-gpu"" title=""Docker Image Version""><img src=""https://img.shields.io/docker/v/mltooling/ml-workspace-gpu?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-gpu"" ttitle=""Docker Image Size""><img src=""https://img.shields.io/docker/image-size/mltooling/ml-workspace-gpu?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-gpu"" title=""Docker Pulls""><img src=""https://img.shields.io/docker/pulls/mltooling/ml-workspace-gpu.svg""></a>  </p>    <details>  <summary>Details (click to expand...)</summary>    > _Currently, the GPU-flavor only supports CUDA 11.2. Support for other CUDA versions might be added in the future._    The GPU flavor (`mltooling/ml-workspace-gpu`) is based on our default workspace image and extends it with CUDA 10.1 and GPU-ready versions of various machine learning libraries (e.g., tensorflow, pytorch, cntk, jax). This GPU image has the following additional requirements for the system:    - Nvidia Drivers for the GPUs. Drivers need to be CUDA 11.2 compatible, version `>=460.32.03` ([ðŸ“– Instructions](https://github.com/NVIDIA/nvidia-docker/wiki/Frequently-Asked-Questions#how-do-i-install-the-nvidia-driver)).  - (Docker >= 19.03) Nvidia Container Toolkit ([ðŸ“– Instructions](https://github.com/NVIDIA/nvidia-docker/wiki/Installation-(Native-GPU-Support))).    ```bash  docker run -p 8080:8080 --gpus all mltooling/ml-workspace-gpu:0.13.2  ```    - (Docker < 19.03) Nvidia Docker 2.0 ([ðŸ“– Instructions](https://github.com/NVIDIA/nvidia-docker/wiki/Installation-(version-2.0))).    ```bash  docker run -p 8080:8080 --runtime nvidia --env NVIDIA_VISIBLE_DEVICES=""all"" mltooling/ml-workspace-gpu:0.13.2  ```    The GPU flavor also comes with a few additional configuration options, as explained below:    <table>      <tr>          <th>Variable</th>          <th>Description</th>          <th>Default</th>      </tr>      <tr>          <td>NVIDIA_VISIBLE_DEVICES</td>          <td>Controls which GPUs will be accessible inside the workspace. By default, all GPUs from the host are accessible within the workspace. You can either use <code>all</code>, <code>none</code>, or specify a comma-separated list of device IDs (e.g., <code>0,1</code>). You can find out the list of available device IDs by running <code>nvidia-smi</code> on the host machine.</td>          <td>all</td>      </tr>      <tr>          <td>CUDA_VISIBLE_DEVICES</td>          <td>Controls which GPUs CUDA applications running inside the workspace will see. By default, all GPUs that the workspace has access to will be visible. To restrict applications, provide a comma-separated list of internal device IDs (e.g., <code>0,2</code>) based on the available devices within the workspace (run <code>nvidia-smi</code>). In comparison to <code>NVIDIA_VISIBLE_DEVICES</code>, the workspace user will be still able to access other GPUs by overwriting this configuration from within the workspace.</td>          <td></td>      </tr>      <tr>          <td>TF_FORCE_GPU_ALLOW_GROWTH</td>          <td>By default, the majority of GPU memory will be allocated by the first execution of a TensorFlow graph. While this behavior can be desirable for production pipelines, it is less desirable for interactive use. Use <code>true</code> to enable dynamic GPU Memory allocation or <code>false</code> to instruct TensorFlow to allocate all memory at execution.</td>          <td>true</td>      </tr>  </table>  </details>    ### Multi-user setup    The workspace is designed as a single-user development environment. For a multi-user setup, we recommend deploying [ðŸ§° ML Hub](https://github.com/ml-tooling/ml-hub). ML Hub is based on JupyterHub with the task to spawn, manage, and proxy workspace instances for multiple users.    <details>  <summary>Deployment (click to expand...)</summary>    ML Hub makes it easy to set up a multi-user environment on a single server (via Docker) or a cluster (via Kubernetes) and supports a variety of usage scenarios & authentication providers. You can try out ML Hub via:    ```bash  docker run -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock mltooling/ml-hub:latest  ```    For more information and documentation about ML Hub, please take a look at the [Github Site](https://github.com/ml-tooling/ml-hub).    </details>    ---    <br>    ## Support    This project is maintained by [Benjamin RÃ¤thlein](https://twitter.com/raethlein), [Lukas Masuch](https://twitter.com/LukasMasuch), and [Jan Kalkan](https://www.linkedin.com/in/jan-kalkan-b5390284/). Please understand that we won't be able to provide individual support via email. We also believe that help is much more valuable if it's shared publicly so that more people can benefit from it.    | Type                     | Channel                                              |  | ------------------------ | ------------------------------------------------------ |  | ðŸš¨&nbsp; **Bug Reports**       | <a href=""https://github.com/ml-tooling/ml-workspace/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+label%3Abug+sort%3Areactions-%2B1-desc+"" title=""Open Bug Report""><img src=""https://img.shields.io/github/issues/ml-tooling/ml-workspace/bug.svg""></a>                                  |  | ðŸŽ&nbsp; **Feature Requests**  | <a href=""https://github.com/ml-tooling/ml-workspace/issues?q=is%3Aopen+is%3Aissue+label%3Afeature+sort%3Areactions-%2B1-desc"" title=""Open Feature Request""><img src=""https://img.shields.io/github/issues/ml-tooling/ml-workspace/feature.svg?label=feature%20request""></a>                                 |  | ðŸ‘©â€ðŸ’»&nbsp; **Usage Questions**   |  <a href=""https://github.com/ml-tooling/ml-workspace/issues?q=is%3Aopen+is%3Aissue+label%3Asupport+sort%3Areactions-%2B1-desc"" title=""Open Support Request""> <img src=""https://img.shields.io/github/issues/ml-tooling/ml-workspace/support.svg?label=support%20request""></a> <a href=""https://stackoverflow.com/questions/tagged/ml-tooling"" title=""Open Question on Stackoverflow""> <img src=""https://img.shields.io/badge/stackoverflow-ml--tooling-orange.svg""></a> <a href=""https://gitter.im/ml-tooling/ml-workspace"" title=""Chat on Gitter""><img src=""https://badges.gitter.im/ml-tooling/ml-workspace.svg""></a> |  | ðŸ“¢&nbsp; **Announcements** | <a href=""https://gitter.im/ml-tooling/ml-workspace"" title=""Chat on Gitter""><img src=""https://badges.gitter.im/ml-tooling/ml-workspace.svg""></a> <a href=""https://mltooling.substack.com/subscribe"" title=""Subscribe for updates""><img src=""http://bit.ly/2Md9rxM""></a> <a href=""https://twitter.com/mltooling"" title=""ML Tooling on Twitter""><img src=""https://img.shields.io/twitter/follow/mltooling.svg?style=social&label=Follow""> |  | â“&nbsp; **Other Requests** | <a href=""mailto:team@mltooling.org"" title=""Email ML Tooling Team""><img src=""https://img.shields.io/badge/email-ML Tooling-green?logo=mail.ru&logoColor=white""></a> |    ---    <br>    ## Features    <p align=""center"">    <a href=""#jupyter"">Jupyter</a> â€¢    <a href=""#desktop-gui"">Desktop GUI</a> â€¢    <a href=""#visual-studio-code"">VS Code</a> â€¢    <a href=""#jupyterlab"">JupyterLab</a> â€¢    <a href=""#git-integration"">Git Integration</a> â€¢    <a href=""#file-sharing"">File Sharing</a> â€¢    <a href=""#access-ports"">Access Ports</a> â€¢    <a href=""#tensorboard"">Tensorboard</a> â€¢    <a href=""#extensibility"">Extensibility</a> â€¢    <a href=""#hardware-monitoring"">Hardware Monitoring</a> â€¢    <a href=""#ssh-access"">SSH Access</a> â€¢    <a href=""#remote-development"">Remote Development</a> â€¢    <a href=""#run-as-a-job"">Job Execution</a>  </p>    The workspace is equipped with a selection of best-in-class open-source development tools to help with the machine learning workflow. Many of these tools can be started from the `Open Tool` menu from Jupyter (the main application of the workspace):    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/open-tools.png""/>    > _Within your workspace you have **full root & sudo privileges** to install any library or tool you need via terminal (e.g., `pip`, `apt-get`, `conda`, or `npm`). You can find more ways to extend the workspace within the [Extensibility](#extensibility) section_    ### Jupyter    [Jupyter Notebook](https://jupyter.org/) is a web-based interactive environment for writing and running code. The main building blocks of Jupyter are the file-browser, the notebook editor, and kernels. The file-browser provides an interactive file manager for all notebooks, files, and folders in the `/workspace` directory.    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/jupyter-tree.png""/>    A new notebook can be created by clicking on the `New` drop-down button at the top of the list and selecting the desired language kernel.    > _You can spawn interactive **terminal** instances as well by selecting `New -> Terminal` in the file-browser._    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/jupyter-notebook.png""/>    The notebook editor enables users to author documents that include live code, markdown text, shell commands, LaTeX equations, interactive widgets, plots, and images. These notebook documents provide a complete and self-contained record of a computation that can be converted to various formats and shared with others.    > _This workspace has a variety of **third-party Jupyter extensions** activated. You can configure these extensions in the nbextensions configurator: `nbextensions` tab on the file browser_    The Notebook allows code to be run in a range of different programming languages. For each notebook document that a user opens, the web application starts a **kernel** that runs the code for that notebook and returns output. This workspace has a Python 3 kernel pre-installed. Additional Kernels can be installed to get access to other languages (e.g., R, Scala, Go) or additional computing resources (e.g., GPUs, CPUs, Memory).    > _**Python 2** is deprected and we do not recommend to use it. However, you can still install a Python 2.7 kernel via this command: `/bin/bash /resources/tools/python-27.sh`_    ### Desktop GUI    This workspace provides an HTTP-based VNC access to the workspace via [noVNC](https://github.com/novnc/noVNC). Thereby, you can access and work within the workspace with a fully-featured desktop GUI. To access this desktop GUI, go to `Open Tool`, select `VNC`, and click the `Connect` button. In the case you are asked for a password, use `vncpassword`.    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/desktop-vnc.png""/>    Once you are connected, you will see a desktop GUI that allows you to install and use full-fledged web-browsers or any other tool that is available for Ubuntu. Within the `Tools` folder on the desktop, you will find a collection of install scripts that makes it straightforward to install some of the most commonly used development tools, such as Atom, PyCharm, R-Runtime, R-Studio, or Postman (just double-click on the script).    **Clipboard:** If you want to share the clipboard between your machine and the workspace, you can use the copy-paste functionality as described below:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/desktop-vnc-clipboard.png""/>    > ðŸ’¡ _**Long-running tasks:** Use the desktop GUI for long-running Jupyter executions. By running notebooks from the browser of your workspace desktop GUI, all output will be synchronized to the notebook even if you have disconnected your browser from the notebook._    ### Visual Studio Code    [Visual Studio Code](https://github.com/microsoft/vscode) (`Open Tool -> VS Code`) is an open-source lightweight but powerful code editor with built-in support for a variety of languages and a rich ecosystem of extensions. It combines the simplicity of a source code editor with powerful developer tooling, like IntelliSense code completion and debugging. The workspace integrates VS Code as a web-based application accessible through the browser-based on the awesome [code-server](https://github.com/cdr/code-server) project. It allows you to customize every feature to your liking and install any number of third-party extensions.    <p align=""center""><img src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/vs-code.png""/></p>    The workspace also provides a VS Code integration into Jupyter allowing you to open a VS Code instance for any selected folder, as shown below:    <p align=""center""><img src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/vs-code-open.png""/></p>    ### JupyterLab    [JupyterLab](https://github.com/jupyterlab/jupyterlab) (`Open Tool -> JupyterLab`) is the next-generation user interface for Project Jupyter. It offers all the familiar building blocks of the classic Jupyter Notebook (notebook, terminal, text editor, file browser, rich outputs, etc.) in a flexible and powerful user interface. This JupyterLab instance comes pre-installed with a few helpful extensions such as a the [jupyterlab-toc](https://github.com/jupyterlab/jupyterlab-toc), [jupyterlab-git](https://github.com/jupyterlab/jupyterlab-git), and [juptyterlab-tensorboard](https://github.com/chaoleili/jupyterlab_tensorboard).    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/jupyterlab.png""/>    ### Git Integration    Version control is a crucial aspect of productive collaboration. To make this process as smooth as possible, we have integrated a custom-made Jupyter extension specialized on pushing single notebooks, a full-fledged web-based Git client ([ungit](https://github.com/FredrikNoren/ungit)), a tool to open and edit plain text documents (e.g., `.py`, `.md`) as notebooks ([jupytext](https://github.com/mwouts/jupytext)), as well as a notebook merging tool ([nbdime](https://github.com/jupyter/nbdime)). Additionally, JupyterLab and VS Code also provide GUI-based Git clients.    #### Clone Repository    For cloning repositories via `https`, we recommend to navigate to the desired root folder and to click on the `git` button as shown below:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/git-open.png""/>    This might ask for some required settings and, subsequently, opens [ungit](https://github.com/FredrikNoren/ungit), a web-based Git client with a clean and intuitive UI that makes it convenient to sync your code artifacts. Within ungit, you can clone any repository. If authentication is required, you will get asked for your credentials.    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/git-ungit-credentials.png""/>    #### Push, Pull, Merge, and Other Git Actions    To commit and push a single notebook to a remote Git repository, we recommend to use the Git plugin integrated into Jupyter, as shown below:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/git-push-notebook.png""/>    For more advanced Git operations, we recommend to use [ungit](https://github.com/FredrikNoren/ungit). With ungit, you can do most of the common git actions such as push, pull, merge, branch, tag, checkout, and many more.    #### Diffing and Merging Notebooks    Jupyter notebooks are great, but they often are huge files, with a very specific JSON file format. To enable seamless diffing and merging via Git this workspace is pre-installed with [nbdime](https://github.com/jupyter/nbdime). Nbdime understands the structure of notebook documents and, therefore, automatically makes intelligent decisions when diffing and merging notebooks. In the case you have merge conflicts, nbdime will make sure that the notebook is still readable by Jupyter, as shown below:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/git-nbdime-merging.png""/>    Furthermore, the workspace comes pre-installed with [jupytext](https://github.com/mwouts/jupytext), a Jupyter plugin that reads and writes notebooks as plain text files. This allows you to open, edit, and run scripts or markdown files (e.g., `.py`, `.md`) as notebooks within Jupyter. In the following screenshot, we have opened a markdown file via Jupyter:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/git-jupytext.png""/>    In combination with Git, jupytext enables a clear diff history and easy merging of version conflicts. With both of those tools, collaborating on Jupyter notebooks with Git becomes straightforward.    ### File Sharing    The workspace has a feature to share any file or folder with anyone via a token-protected link. To share data via a link, select any file or folder from the Jupyter directory tree and click on the share button as shown in the following screenshot:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/file-sharing-open.png""/>    This will generate a unique link protected via a token that gives anyone with the link access to view and download the selected data via the [Filebrowser](https://github.com/filebrowser/filebrowser) UI:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/file-sharing-filebrowser.png""/>    To deactivate or manage (e.g., provide edit permissions) shared links, open the Filebrowser via `Open Tool -> Filebrowser` and select `Settings->User Management`.    ### Access Ports    It is possible to securely access any workspace internal port by selecting `Open Tool -> Access Port`. With this feature, you are able to access a REST API or web application running inside the workspace directly with your browser. The feature enables developers  to build, run, test, and debug REST APIs or web applications directly from the workspace.    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/access-port.png""/>    If you want to use an HTTP client or share access to a given port, you can select the `Get shareable link` option. This generates a token-secured link that anyone with access to the link can use to access the specified port.    > _The HTTP app requires to be resolved from a relative URL path or configure a base path (`/tools/PORT/`). Tools made accessible this way are secured by the workspace's authentication system! If you decide to publish any other port of the container yourself instead of using this feature to make a tool accessible, please make sure to secure it via an authentication mechanism!_    <details>    <summary>Example (click to expand...)</summary>    1. Start an HTTP server on port `1234` by running this command in a terminal within the workspace: `python -m http.server 1234`  2. Select `Open Tool -> Access Port`, input port `1234`, and select the `Get shareable link` option.  3. Click `Access`, and you will see the content provided by Python's `http.server`.  4. The opened link can also be shared to other people or called from external applications (e.g., try with Incognito Mode in Chrome).    </details>    ### SSH Access    SSH provides a powerful set of features that enables you to be more productive with your development tasks. You can easily set up a secure and passwordless SSH connection to a workspace by selecting `Open Tool -> SSH`. This will generate a secure setup command that can be run on any Linux or Mac machine to configure a passwordless & secure SSH connection to the workspace. Alternatively, you can also download the setup script and run it (instead of using the command).    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/ssh-access.png""/>    > _The setup script only runs on Mac and Linux. Windows is currently not supported._    Just run the setup command or script on the machine from where you want to setup a connection to the workspace and input a name for the connection (e.g., `my-workspace`). You might also get asked for some additional input during the process, e.g. to install a remote kernel if `remote_ikernel` is installed. Once the passwordless SSH connection is successfully setup and tested, you can securely connect to the workspace by simply executing `ssh my-workspace`.    Besides the ability to execute commands on a remote machine, SSH also provides a variety of other features that can improve your development workflow as described in the following sections.    <details>  <summary><b>Tunnel Ports</b> (click to expand...)</summary>    An SSH connection can be used for tunneling application ports from the remote machine to the local machine, or vice versa. For example, you can expose the workspace internal port `5901` (VNC Server) to the local machine on port `5000` by executing:    ```bash  ssh -nNT -L 5000:localhost:5901 my-workspace  ```    > _To expose an application port from your local machine to a workspace, use the `-R` option (instead of `-L`)._    After the tunnel is established, you can use your favorite VNC viewer on your local machine and connect to `vnc://localhost:5000` (default password: `vncpassword`). To make the tunnel connection more resistant and reliable, we recommend to use [autossh](https://www.harding.motd.ca/autossh/) to automatically restart SSH tunnels in the case that the connection dies:    ```bash  autossh -M 0 -f -nNT -L 5000:localhost:5901 my-workspace  ```    Port tunneling is quite useful when you have started any server-based tool within the workspace that you like to make accessible for another machine. In its default setting, the workspace has a variety of tools already running on different ports, such as:    - `8080`: Main workspace port with access to all integrated tools.  - `8090`: Jupyter server.  - `8054`: VS Code server.  - `5901`: VNC server.  - `22`: SSH server.    You can find port information on all the tools in the [supervisor configuration](https://github.com/ml-tooling/ml-workspace/blob/main/resources/supervisor/supervisord.conf).    > ðŸ“– _For more information about port tunneling/forwarding, we recommend [this guide](https://www.everythingcli.org/ssh-tunnelling-for-fun-and-profit-local-vs-remote/)._    </details>    <details>  <summary><b>Copy Data via SCP</b> (click to expand...)</summary>    [SCP](https://linux.die.net/man/1/scp) allows files and directories to be securely copied to, from, or between different machines via SSH connections. For example, to copy a local file (`./local-file.txt`) into the `/workspace` folder inside the workspace, execute:    ```bash  scp ./local-file.txt my-workspace:/workspace  ```    To copy the `/workspace` directory from `my-workspace` to the working directory of the local machine, execute:    ```bash  scp -r my-workspace:/workspace .  ```    > ðŸ“– _For more information about scp, we recommend [this guide](https://www.garron.me/en/articles/scp.html)._  </details>    <details>  <summary><b>Sync Data via Rsync</b> (click to expand...)</summary>    [Rsync](https://linux.die.net/man/1/rsync) is a utility for efficiently transferring and synchronizing files between different machines (e.g., via SSH connections) by comparing the modification times and sizes of files. The rsync command will determine which files need to be updated each time it is run, which is far more efficient and convenient than using something like scp or sftp. For example, to sync all content of a local folder (`./local-project-folder/`) into the `/workspace/remote-project-folder/` folder inside the workspace, execute:    ```bash  rsync -rlptzvP --delete --exclude="".git"" ""./local-project-folder/"" ""my-workspace:/workspace/remote-project-folder/""  ```    If you have some changes inside the folder on the workspace, you can sync those changes back to the local folder by changing the source and destination arguments:    ```bash  rsync -rlptzvP --delete --exclude="".git"" ""my-workspace:/workspace/remote-project-folder/"" ""./local-project-folder/""  ```    You can rerun these commands each time you want to synchronize the latest copy of your files. Rsync will make sure that only updates will be transferred.    > ðŸ“– _You can find more information about rsync on [this man page](https://linux.die.net/man/1/rsync)._  </details>    <details>  <summary><b>Mount Folders via SSHFS</b> (click to expand...)</summary>    Besides copying and syncing data, an SSH connection can also be used to mount directories from a remote machine into the local filesystem via [SSHFS](https://github.com/libfuse/sshfs).   For example, to mount the `/workspace` directory of `my-workspace` into a local path (e.g. `/local/folder/path`), execute:    ```bash  sshfs -o reconnect my-workspace:/workspace /local/folder/path  ```    Once the remote directory is mounted, you can interact with the remote file system the same way as with any local directory and file.    > ðŸ“– _For more information about sshfs, we recommend [this guide](https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh)._  </details>    ### Remote Development    The workspace can be integrated and used as a remote runtime (also known as remote kernel/machine/interpreter) for a variety of popular development tools and IDEs, such as Jupyter, VS Code, PyCharm, Colab, or Atom Hydrogen. Thereby, you can connect your favorite development tool running on your local machine to a remote machine for code execution. This enables a local-quality development experience with remote-hosted compute resources.    These integrations usually require a passwordless SSH connection from the local machine to the workspace. To set up an SSH connection, please follow the steps explained in the [SSH Access](#ssh-access) section.    <details>  <summary><b>Jupyter - Remote Kernel</b> (click to expand...)</summary>    The workspace can be added to a Jupyter instance as a remote kernel by using the [remote_ikernel](https://bitbucket.org/tdaff/remote_ikernel/) tool. If you have installed remote_ikernel (`pip install remote_ikernel`) on your local machine, the SSH setup script of the workspace will automatically offer you the option to setup a remote kernel connection.    > _When running kernels on remote machines, the notebooks themselves will be saved onto the local filesystem, but the kernel will only have access to the filesystem of the remote machine running the kernel. If you need to sync data, you can make use of rsync, scp, or sshfs as explained in the [SSH Access](#ssh-access) section._    In case you want to manually setup and manage remote kernels, use the [remote_ikernel](https://bitbucket.org/tdaff/remote_ikernel/src/default/README.rst) command-line tool, as shown below:    ```bash  # Change my-workspace with the name of a workspace SSH connection  remote_ikernel manage --add \      --interface=ssh \      --kernel_cmd=""ipython kernel -f {connection_file}"" \      --name=""ml-server (Python)"" \      --host=""my-workspace""  ```    You can use the remote_ikernel command line functionality to list (`remote_ikernel manage --show`) or delete (`remote_ikernel manage --delete <REMOTE_KERNEL_NAME>`) remote kernel connections.    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/remote-dev-jupyter-kernel.png""/>    </details>    <details>  <summary><b>VS Code - Remote Machine</b> (click to expand...)</summary>    TheÂ Visual Studio Code [Remote - SSH](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh)Â extension allows you to open a remote folder on any remote machine with SSH access and work with it just as you would if the folder were on your own machine. Once connected to a remote machine, you can interact with files and folders anywhere on the remote filesystem and take full advantage of VS Code's feature set (IntelliSense, debugging, and extension support). The discovers and works out-of-the-box with passwordless SSH connections as configured by the workspace SSH setup script. To enable your local VS Code application to connect to a workspace:    1. Install [Remote - SSH](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh)Â extension inside your local VS Code.  2. Run the SSH setup script of a selected workspace as explained in the [SSH Access](#ssh-access) section.  3. Open the Remote-SSH panel in your local VS Code. All configured SSH connections should be automatically discovered. Just select any configured workspace connection you like to connect to as shown below:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/remote-dev-vscode.gif""/>    > ðŸ“– _You can find additional features and information about the Remote SSH extension in [this guide](https://code.visualstudio.com/docs/remote/ssh)._    </details>    ### Tensorboard    [Tensorboard](https://www.tensorflow.org/tensorboard) provides a suite of visualization tools to make it easier to understand, debug, and optimize your experiment runs. It includes logging features for scalar, histogram, model structure, embeddings, and text & image visualization. The workspace comes pre-installed with [jupyter_tensorboard extension](https://github.com/lspvic/jupyter_tensorboard) that integrates Tensorboard into the Jupyter interface with functionalities to start, manage, and stop instances. You can open a new instance for a valid logs directory, as shown below:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/tensorboard-open.png"" />    If you have opened a Tensorboard instance in a valid log directory, you will see the visualizations of your logged data:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/tensorboard-dashboard.png"" />    > _Tensorboard can be used in combination with many other ML frameworks besides Tensorflow. By using the [tensorboardX](https://github.com/lanpa/tensorboardX) library you can log basically from any python based library. Also, PyTorch has a direct Tensorboard integration as described [here](https://pytorch.org/docs/stable/tensorboard.html)._    If you prefer to see the tensorboard directly within your notebook, you can make use of following **Jupyter magic**:    ```  %load_ext tensorboard  %tensorboard --logdir /workspace/path/to/logs  ```    ### Hardware Monitoring    The workspace provides two pre-installed web-based tools to help developers during model training and other experimentation tasks to get insights into everything happening on the system and figure out performance bottlenecks.    [Netdata](https://github.com/netdata/netdata) (`Open Tool -> Netdata`) is a real-time hardware and performance monitoring dashboard that visualize the processes and services on your Linux systems. It monitors metrics about CPU, GPU, memory, disks, networks, processes, and more.    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/hardware-monitoring-netdata.png"" />    [Glances](https://github.com/nicolargo/glances) (`Open Tool -> Glances`) is a web-based hardware monitoring dashboard as well and can be used as an alternative to Netdata.    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/hardware-monitoring-glances.png""/>    > _Netdata and Glances will show you the hardware statistics for the entire machine on which the workspace container is running._    ### Run as a job    > _A job is defined as any computational task that runs for a certain time to completion, such as a model training or a data pipeline._    The workspace image can also be used to execute arbitrary Python code without starting any of the pre-installed tools. This provides a seamless way to productize your ML projects since the code that has been developed interactively within the workspace will have the same environment and configuration when run as a job via the same workspace image.    <details>  <summary><b>Run Python code as a job via the workspace image</b> (click to expand...)</summary>    To run Python code as a job, you need to provide a path or URL to a code directory (or script) via `EXECUTE_CODE`. The code can be either already mounted into the workspace container or downloaded from a version control system (e.g., git or svn) as described in the following sections. The selected code path needs to be python executable. In case the selected code is a directory (e.g., whenever you download the code from a VCS) you need to put a `__main__.py` file at the root of this directory. The `__main__.py` needs to contain the code that starts your job.    #### Run code from version control system    You can execute code directly from Git, Mercurial, Subversion, or Bazaar by using the pip-vcs format as described in [this guide](https://pip.pypa.io/en/stable/reference/pip_install/#vcs-support). For example, to execute code from a [subdirectory](https://github.com/ml-tooling/ml-workspace/tree/main/resources/tests/ml-job) of a git repository, just run:    ```bash  docker run --env EXECUTE_CODE=""git+https://github.com/ml-tooling/ml-workspace.git#subdirectory=resources/tests/ml-job"" mltooling/ml-workspace:0.13.2  ```    > ðŸ“– _For additional information on how to specify branches, commits, or tags please refer to [this guide](https://pip.pypa.io/en/stable/reference/pip_install/#vcs-support)._    #### Run code mounted into the workspace    In the following example, we mount and execute the current working directory (expected to contain our code) into the `/workspace/ml-job/` directory of the workspace:    ```bash  docker run -v ""${PWD}:/workspace/ml-job/"" --env EXECUTE_CODE=""/workspace/ml-job/"" mltooling/ml-workspace:0.13.2  ```    #### Install Dependencies    In the case that the pre-installed workspace libraries are not compatible with your code, you can install or change dependencies by just adding one or multiple of the following files to your code directory:    - `requirements.txt`: [pip requirements format](https://pip.pypa.io/en/stable/user_guide/#requirements-files) for pip-installable dependencies.  - `environment.yml`: [conda environment file](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html?highlight=environment.yml#creating-an-environment-file-manually) to create a separate Python environment.  - `setup.sh`: A shell script executed via `/bin/bash`.    The execution order is 1. `environment.yml` -> 2. `setup.sh` -> 3. `requirements.txt`    #### Test job in interactive mode    You can test your job code within the workspace (started normally with interactive tools) by executing the following python script:    ```bash  python /resources/scripts/execute_code.py /path/to/your/job  ```    #### Build a custom job image    It is also possible to embed your code directly into a custom job image, as shown below:    ```dockerfile  FROM mltooling/ml-workspace:0.13.2    # Add job code to image  COPY ml-job /workspace/ml-job  ENV EXECUTE_CODE=/workspace/ml-job    # Install requirements only  RUN python /resources/scripts/execute_code.py --requirements-only    # Execute only the code at container startup  CMD [""python"", ""/resources/docker-entrypoint.py"", ""--code-only""]  ```    </details>    ### Pre-installed Libraries and Interpreters    The workspace is pre-installed with many popular interpreters, data science libraries, and ubuntu packages:    - **Interpreter:** Python 3.8 (Miniconda 3), NodeJS 14, Scala, Perl 5  - **Python libraries:** Tensorflow, Keras, Pytorch, Sklearn, XGBoost, MXNet, Theano, and [many more](https://github.com/ml-tooling/ml-workspace/tree/main/resources/libraries)  - **Package Manager:** `conda`, `pip`, `apt-get`, `npm`, `yarn`, `sdk`, `poetry`, `gdebi`...      The full list of installed tools can be found within the [Dockerfile](https://github.com/ml-tooling/ml-workspace/blob/main/Dockerfile).    > _For every minor version release, we run vulnerability, virus, and security checks within the workspace using [safety](https://pyup.io/safety/), [clamav](https://www.clamav.net/), [trivy](https://github.com/aquasecurity/trivy), and [snyk via docker scan](https://docs.docker.com/engine/scan/) to make sure that the workspace environment is as secure as possible. We are committed to fix and prevent all high- or critical-severity vulnerabilities. You can find some up-to-date reports [here](https://github.com/ml-tooling/ml-workspace/tree/main/resources/reports)._    ### Extensibility    The workspace provides a high degree of extensibility. Within the workspace, you have **full root & sudo privileges** to install any library or tool you need via terminal (e.g., `pip`, `apt-get`, `conda`, or `npm`). You can open a terminal by one of the following ways:    - **Jupyter:** `New -> Terminal`  - **Desktop VNC:** `Applications -> Terminal Emulator`  - **JupyterLab:** `File -> New -> Terminal`  - **VS Code:** `Terminal -> New Terminal`    Additionally, pre-installed tools such as Jupyter, JupyterLab, and Visual Studio Code each provide their own rich ecosystem of extensions. The workspace also contains a [collection of installer scripts](https://github.com/ml-tooling/ml-workspace/tree/main/resources/tools) for many commonly used development tools or libraries (e.g., `PyCharm`, `Zeppelin`, `RStudio`, `Starspace`). You can find and execute all tool installers via `Open Tool -> Install Tool`. Those scripts can be also executed from the Desktop VNC (double-click on the script within the `Tools` folder on the Desktop VNC).    <details>  <summary>Example (click to expand...)</summary>    For example, to install the [Apache Zeppelin](https://zeppelin.apache.org/) notebook server, simply execute:    ```bash  /resources/tools/zeppelin.sh --port=1234  ```    After installation, refresh the Jupyter website and the Zeppelin tool will be available under `Open Tool -> Zeppelin`. Other tools might only be available within the Desktop VNC (e.g., `atom` or `pycharm`) or do not provide any UI (e.g., `starspace`, `docker-client`).  </details>    As an alternative to extending the workspace at runtime, you can also customize the workspace Docker image to create your own flavor as explained in the [FAQ](#faq) section.    ---    <br>    ## FAQ    <details>  <summary><b>How to customize the workspace image (create your own flavor)?</b> (click to expand...)</summary>    The workspace can be extended in many ways at runtime, as explained [here](#extensibility). However, if you like to customize the workspace image with your own software or configuration, you can do that via a Dockerfile as shown below:    ```dockerfile  # Extend from any of the workspace versions/flavors  FROM mltooling/ml-workspace:0.13.2    # Run you customizations, e.g.  RUN \      # Install r-runtime, r-kernel, and r-studio web server from provided install scripts      /bin/bash $RESOURCES_PATH/tools/r-runtime.sh --install && \      /bin/bash $RESOURCES_PATH/tools/r-studio-server.sh --install && \      # Cleanup Layer - removes unneccessary cache files      clean-layer.sh  ```    Finally, use [docker build](https://docs.docker.com/engine/reference/commandline/build/) to build your customized Docker image.    > ðŸ“– _For a more comprehensive Dockerfile example, take a look at the [Dockerfile of the R-flavor](https://github.com/ml-tooling/ml-workspace/blob/main/r-flavor/Dockerfile)._    </details>    <details>  <summary><b>How to update a running workspace container?</b> (click to expand...)</summary>    To update a running workspace instance to a more recent version, the running Docker container needs to be replaced with a new container based on the updated workspace image.    All data within the workspace that is not persisted to a mounted volume will be lost during this update process. As mentioned in the [persist data](#Persist-Data) section, a volume is expected to be mounted into the `/workspace` folder. All tools within the workspace are configured to make use of the `/workspace` folder as the root directory for all source code and data artifacts. During an update, data within other directories will be removed, including installed/updated libraries or certain machine configurations. We have integrated a backup and restore feature (`CONFIG_BACKUP_ENABLED`) for various selected configuration files/folders, such as the user's Jupyter/VS-Code configuration, `~/.gitconfig`, and `~/.ssh`.    <details>    <summary>Update Example (click to expand...)</summary>    If the workspace is deployed via Docker (Kubernetes will have a different update process), you need to remove the existing container (via `docker rm`) and start a new one (via `docker run`) with the newer workspace image. Make sure to use the same configuration, volume, name, and port. For example, a workspace (image version `0.8.7`) was started with this command:  ```  docker run -d \      -p 8080:8080 \      --name ""ml-workspace"" \      -v ""/path/on/host:/workspace"" \      --env AUTHENTICATE_VIA_JUPYTER=""mytoken"" \      --restart always \      mltooling/ml-workspace:0.8.7  ```  and needs to be updated to version `0.9.1`, you need to:    1. Stop and remove the running workspace container: `docker stop ""ml-workspace"" && docker rm ""ml-workspace""`  2. Start a new workspace container with the newer image and same configuration: `docker run -d -p 8080:8080 --name ""ml-workspace"" -v ""/path/on/host:/workspace"" --env AUTHENTICATE_VIA_JUPYTER=""mytoken"" --restart always mltooling/ml-workspace:0.9.1`    </details>    </details>    <details>  <summary><b>How to configure the VNC server?</b> (click to expand...)</summary>    If you want to directly connect to the workspace via a VNC client (not using the [noVNC webapp](#desktop-gui)), you might be interested in changing certain VNC server configurations. To configure the VNC server, you can provide/overwrite the following environment variables at container start (via docker run option: `--env`):    <table>      <tr>          <th>Variable</th>          <th>Description</th>          <th>Default</th>      </tr>      <tr>          <td>VNC_PW</td>          <td>Password of VNC connection. This password only needs to be secure if the VNC server is directly exposed. If it is used via noVNC, it is already protected based on the configured authentication mechanism.</td>          <td>vncpassword</td>      </tr>      <tr>          <td>VNC_RESOLUTION</td>          <td>Default desktop resolution of VNC connection. When using noVNC, the resolution will be dynamically adapted to the window size.</td>          <td>1600x900</td>      </tr>      <tr>          <td>VNC_COL_DEPTH</td>          <td>Default color depth of VNC connection.</td>          <td>24</td>      </tr>  </table>    </details>    <details>  <summary><b>How to use a non-root user within the workspace?</b> (click to expand...)</summary>    Unfortunately, we currently do not support using a non-root user within the workspace. We plan to provide this capability and already started with some refactoring to allow this configuration. However, this still requires a lot more work, refactoring, and testing from our side.    Using root-user (or users with sudo permission) within containers is generally not recommended since, in case of system/kernel vulnerabilities, a user might be able to break out of the container and be able to access the host system. Since it is not very common to have such problematic kernel vulnerabilities, the risk of a severe attack is quite minimal. As explained in the [official Docker documentation](https://docs.docker.com/engine/security/security/#linux-kernel-capabilities), containers (even with root users) are generally quite secure in preventing a breakout to the host. And compared to many other container use-cases, we actually want to provide the flexibility to the user to have control and system-level installation permissions within the workspace container.    </details>    <details>  <summary><b>How to create and use a virtual environment?</b> (click to expand...)</summary>    The workspace comes preinstalled with various common tools to create isolated Python environments (virtual environments). The following sections provide a quick-intro on how to use these tools within the workspace. You can find information on when to use which tool [here](https://stackoverflow.com/a/41573588). Please refer to the documentation of the given tool for additional usage information.    **venv** (recommended):    To create a virtual environment via [venv](https://docs.python.org/3/tutorial/venv.html), execute the following commands:    ```bash  # Create environment in the working directory  python -m venv my-venv  # Activate environment in shell  source ./my-venv/bin/activate  # Optional: Create Jupyter kernel for this environment  pip install ipykernel  python -m ipykernel install --user --name=my-venv --display-name=""my-venv ($(python --version))""  # Optional: Close enviornment session  deactivate  ```    **pipenv** (recommended):    To create a virtual environment via [pipenv](https://pipenv.pypa.io/en/latest/), execute the following commands:    ```bash  # Create environment in the working directory  pipenv install  # Activate environment session in shell  pipenv shell  # Optional: Create Jupyter kernel for this environment  pipenv install ipykernel  python -m ipykernel install --user --name=my-pipenv --display-name=""my-pipenv ($(python --version))""  # Optional: Close environment session  exit  ```    **virtualenv**:    To create a virtual environment via [virtualenv](https://virtualenv.pypa.io/en/latest/), execute the following commands:    ```bash  # Create environment in the working directory  virtualenv my-virtualenv  # Activate environment session in shell  source ./my-virtualenv/bin/activate  # Optional: Create Jupyter kernel for this environment  pip install ipykernel  python -m ipykernel install --user --name=my-virtualenv --display-name=""my-virtualenv ($(python --version))""  # Optional: Close environment session  deactivate  ```    **conda**:    To create a virtual environment via [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html), execute the following commands:    ```bash  # Create environment (globally)  conda create -n my-conda-env  # Activate environment session in shell  conda activate my-conda-env  # Optional: Create Jupyter kernel for this environment  python -m ipykernel install --user --name=my-conda-env --display-name=""my-conda-env ($(python --version))""  # Optional: Close environment session  conda deactivate  ```    **Tip: Shell Commands in Jupyter Notebooks:**    If you install and use a virtual environment via a dedicated Jupyter Kernel and use shell commands within Jupyter (e.g. `!pip install matplotlib`), the wrong python/pip version will be used. To use the python/pip version of the selected kernel, do the following instead:    ```python  import sys  !{sys.executable} -m pip install matplotlib  ```    </details>    <details>  <summary><b>How to install a different Python version?</b> (click to expand...)</summary>    The workspace provides three easy options to install different Python versions alongside the main Python instance: [pyenv](https://github.com/pyenv/pyenv), [pipenv](https://pipenv.pypa.io/en/latest/cli/) (recommended), [conda](https://github.com/pyenv/pyenv).    **pipenv** (recommended):    To install a different python version (e.g. `3.7.8`) within the workspace via [pipenv](https://pipenv.pypa.io/en/latest/cli/), execute the following commands:    ```bash  # Install python vers  pipenv install --python=3.7.8  # Activate environment session in shell  pipenv shell  # Check python installation  python --version  # Optional: Create Jupyter kernel for this environment  pipenv install ipykernel  python -m ipykernel install --user --name=my-pipenv --display-name=""my-pipenv ($(python --version))""  # Optional: Close environment session  exit  ```    **pyenv**:    To install a different python version (e.g. `3.7.8`) within the workspace via [pyenv](https://github.com/pyenv/pyenv), execute the following commands:    ```bash  # Install python version  pyenv install 3.7.8  # Make globally accessible  pyenv global 3.7.8  # Activate python version in shell  pyenv shell 3.7.8  # Check python installation  python3.7 --version  # Optional: Create Jupyter kernel for this python version  python3.7 -m pip install ipykernel  python3.7 -m ipykernel install --user --name=my-pyenv-3.7.8 --display-name=""my-pyenv (Python 3.7.8)""  ```    **conda**:    To install a different python version (e.g. `3.7.8`) within the workspace via [conda](https://github.com/pyenv/pyenv), execute the following commands:    ```bash  # Create environment with python version  conda create -n my-conda-3.7 python=3.7.8  # Activate environment session in shell  conda activate my-conda-3.7  # Check python installation  python --version  # Optional: Create Jupyter kernel for this python version  pip install ipykernel  python -m ipykernel install --user --name=my-conda-3.7 --display-name=""my-conda ($(python --version))""  # Optional: Close environment session  conda deactivate  ```    **Tip: Shell Commands in Jupyter Notebooks:**    If you install and use another Python version via a dedicated Jupyter Kernel and use shell commands within Jupyter (e.g. `!pip install matplotlib`), the wrong python/pip version will be used. To use the python/pip version of the selected kernel, do the following instead:    ```python  import sys  !{sys.executable} -m pip install matplotlib  ```    </details>    <details>  <summary><b>Can I publish any other than the default port to access a tool inside the container?</b> (click to expand...)</summary>  You can do this, but please be aware that this port is <b>not</b> protected by the workspace's authentication mechanism then! For security reasons, we therefore highly recommend to use the <a href=""#access-ports"">Access Ports</a> functionality of the workspace.  </details>    <details>  <summary><b>System and Tool Translations</b> (click to expand...)</summary>  If you want to configure another language than English in your workspace and some tools are not translated properly, have a look <a href=""https://github.com/ml-tooling/ml-workspace/issues/70#issuecomment-841863145"">at this issue</a>. Try to comment out the 'exclude translations' line in `/etc/dpkg/dpkg.cfg.d/excludes` and re-install / configure the package.  </details>    ---    <br>    ## Known Issues    <details>    <summary><b>Too small shared memory might crash tools or scripts</b> (click to expand...)</summary>    Certain desktop tools (e.g., recent versions of [Firefox](https://github.com/jlesage/docker-firefox#increasing-shared-memory-size)) or libraries (e.g., Pytorch - see Issues: [1](https://github.com/pytorch/pytorch/issues/2244), [2](https://github.com/pytorch/pytorch/issues/1355)) might crash if the shared memory size (`/dev/shm`) is too small. The default shared memory size of Docker is 64MB, which might not be enough for a few tools. You can provide a higher shared memory size via the `shm-size` docker run option:    ```bash  docker run --shm-size=2G mltooling/ml-workspace:0.13.2  ```    </details>    <details>    <summary><b>Multiprocessing code is unexpectedly slow </b> (click to expand...)</summary>    In general, the performance of running code within Docker is [nearly identical](https://stackoverflow.com/questions/21889053/what-is-the-runtime-performance-cost-of-a-docker-container) compared to running it directly on the machine. However, in case you have limited the container's CPU quota (as explained in [this section](#limit-memory--cpu)), the container can still see the full count of CPU cores available on the machine and there is no technical way to prevent this. Many libraries and tools will use the full CPU count (e.g., via `os.cpu_count()`) to set the number of threads used for multiprocessing/-threading. This might cause the program to start more threads/processes than it can efficiently handle with the available CPU quota, which can tremendously slow down the overall performance. Therefore, it is important to set the available CPU count or the maximum number of threads explicitly to the configured CPU quota. The workspace provides capabilities to detect the number of available CPUs automatically, which are used to configure a variety of common libraries via environment variables such as `OMP_NUM_THREADS` or `MKL_NUM_THREADS`. It is also possible to explicitly set the number of available CPUs at container startup via the `MAX_NUM_THREADS` environment variable (see [configuration section](https://github.com/ml-tooling/ml-workspace#configuration-options)). The same environment variable can also be used to get the number of available CPUs at runtime.    Even though the automatic configuration capabilities of the workspace will fix a variety of inefficiencies, we still recommend configuring the number of available CPUs with all libraries explicitly. For example:    ```python  import os  MAX_NUM_THREADS = int(os.getenv(""MAX_NUM_THREADS""))    # Set in pytorch  import torch  torch.set_num_threads(MAX_NUM_THREADS)    # Set in tensorflow  import tensorflow as tf  config = tf.ConfigProto(      device_count={""CPU"": MAX_NUM_THREADS},      inter_op_parallelism_threads=MAX_NUM_THREADS,      intra_op_parallelism_threads=MAX_NUM_THREADS,  )  tf_session = tf.Session(config=config)    # Set session for keras  import keras.backend as K  K.set_session(tf_session)    # Set in sklearn estimator  from sklearn.linear_model import LogisticRegression  LogisticRegression(n_jobs=MAX_NUM_THREADS).fit(X, y)    # Set for multiprocessing pool  from multiprocessing import Pool    with Pool(MAX_NUM_THREADS) as pool:      results = pool.map(lst)  ```    </details>    <details>    <summary><b>Nginx terminates with SIGILL core dumped error</b> (click to expand...)</summary>    If you encounter the following error within the container logs when starting the workspace, it will most likely not be possible to run the workspace on your hardware:    ```  exited: nginx (terminated by SIGILL (core dumped); not expected)  ```    The OpenResty/Nginx binary package used within the workspace requires to run on a CPU with `SSE4.2` support (see [this issue](https://github.com/openresty/openresty/issues/267#issuecomment-309296900)). Unfortunately, some older CPUs do not have support for `SSE4.2` and, therefore, will not be able to run the workspace container. On Linux, you can check if your CPU supports `SSE4.2` when looking into the `cat /proc/cpuinfo` flags section. If you encounter this problem, feel free to notify us by commenting on the following issue: [#30](https://github.com/ml-tooling/ml-workspace/issues/30).    </details>    ---    <br>    ## Contribution    - Pull requests are encouraged and always welcome. Read our [contribution guidelines](https://github.com/ml-tooling/ml-workspace/tree/main/CONTRIBUTING.md) and check out [help-wanted](https://github.com/ml-tooling/ml-workspace/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+label%3A""help+wanted""+sort%3Areactions-%2B1-desc+) issues.  - Submit Github issues for any [feature request and enhancement](https://github.com/ml-tooling/ml-workspace/issues/new?assignees=&labels=feature&template=02_feature-request.md&title=), [bugs](https://github.com/ml-tooling/ml-workspace/issues/new?assignees=&labels=bug&template=01_bug-report.md&title=), or [documentation](https://github.com/ml-tooling/ml-workspace/issues/new?assignees=&labels=documentation&template=03_documentation.md&title=) problems.  - By participating in this project, you agree to abide by its [Code of Conduct](https://github.com/ml-tooling/ml-workspace/blob/main/.github/CODE_OF_CONDUCT.md).  - The [development section](#development) below contains information on how to build and test the project after you have implemented some changes.    ## Development    > _**Requirements**: [Docker](https://docs.docker.com/get-docker/) and [Act](https://github.com/nektos/act#installation) are required to be installed on your machine to execute the build process._    To simplify the process of building this project from scratch, we provide build-scripts - based on [universal-build](https://github.com/ml-tooling/universal-build) - that run all necessary steps (build, test, and release) within a containerized environment. To build and test your changes, execute the following command in the project root folder:    ```bash  act -b -j build  ```    Under the hood it uses the build.py files in this repo based on the [universal-build library](https://github.com/ml-tooling/universal-build). So, if you want to build it locally, you can also execute this command in the project root folder to build the docker container:    ```bash  python build.py --make  ```    For additional script options:    ```bash  python build.py --help  ```    Refer to our [contribution guides](https://github.com/ml-tooling/ml-workspace/blob/main/CONTRIBUTING.md#development-instructions) for more detailed information on our build scripts and development process.    ---    Licensed **Apache 2.0**. Created and maintained with â¤ï¸&nbsp; by developers from Berlin. """
Big data;https://github.com/WeBankFinTech/DataSphereStudio;"""![DSS](images/en_US/readme/DSS_logo.png)  ====    [![License](https://img.shields.io/badge/license-Apache%202-4EB1BA.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)    English | [ä¸­æ–‡](README-ZH.md)    ## Introduction     &nbsp; &nbsp; &nbsp; &nbsp;DataSphere Studio (DSS for short) is WeDataSphere, a one-stop data application development management portal developed by WeBank.     &nbsp; &nbsp; &nbsp; &nbsp;With the pluggable integrated framework design and the Linkis, a computing middleware, DSS can easily integrate  various upper-layer data application systems, making data development simple and easy to use.     &nbsp; &nbsp; &nbsp; &nbsp;DataSphere Studio is positioned as a data application development portal, and the closed loop covers the entire process of data application development. With a unified UI, the workflow-like graphical drag-and-drop development experience meets the entire lifecycle of data application development from data import, desensitization cleaning, data analysis, data mining, quality inspection, visualization, scheduling to data output applications, etc.     &nbsp; &nbsp; &nbsp; &nbsp;With the connection, reusability, and simplification capabilities of Linkis, DSS is born with financial-grade capabilities of high concurrency, high availability, multi-tenant isolation, and resource management.    ## UI preview     &nbsp; &nbsp; &nbsp; &nbsp;Please be patient, it will take some time to load gif.    ![DSS-V1.0 GIF](images/en_US/readme/DSS_gif.gif)    ## Core features    ### 1. One-stop, full-process application development management UI     &nbsp; &nbsp; &nbsp; &nbsp;DSS is highly integrated. Currently integrated components include(**DSS version compatibility for the above components, please visit: [Compatibility list of integrated components](README.md#4-integrated-data-application-components)**):     &nbsp; &nbsp; &nbsp; &nbsp;1. Data Development IDE Tool - [Scriptis](https://github.com/WeBankFinTech/Scriptis)     &nbsp; &nbsp; &nbsp; &nbsp;2. Data Visualization Tool - [Visualis](https://github.com/WeBankFinTech/Visualis) (Based on the open source project [Davinci](https://github.com/edp963/davinci ) contributed by CreditEase)     &nbsp; &nbsp; &nbsp; &nbsp;3. Data Quality Management Tool - [Qualitis](https://github.com/WeBankFinTech/Qualitis)     &nbsp; &nbsp; &nbsp; &nbsp;4. Workflow scheduling tool - [Schedulis](https://github.com/WeBankFinTech/Schedulis)     &nbsp; &nbsp; &nbsp; &nbsp;5. Data Exchange Tool - [Exchangis](https://github.com/WeBankFinTech/Exchangis) (**The upcoming Exchangis1.0 will be integrated with the DSS workflow**)     &nbsp; &nbsp; &nbsp; &nbsp;6. Data Api Service - [DataApiService](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Using_Document/DataApiService_Usage_Documentation.md)     &nbsp; &nbsp; &nbsp; &nbsp;7. Streaming Application Development Management Tool - [Streamis](https://github.com/WeBankFinTech/Streamis)     &nbsp; &nbsp; &nbsp; &nbsp;8. One-stop machine Learning Platform - [Prophecis](https://github.com/WeBankFinTech/Prophecis) (**Integrated version will be released soon**)     &nbsp; &nbsp; &nbsp; &nbsp;9. Workflow Task Scheduling Tool - DolphinScheduler (**In Code Merging**)      &nbsp; &nbsp; &nbsp; &nbsp;10. Help documentation and beginner's guide - UserGuide (**In Code Merging**)     &nbsp; &nbsp; &nbsp; &nbsp;11. Data Model Center - DataModelCenter (**In development**)      &nbsp; &nbsp; &nbsp; &nbsp;**DSS version compatibility for the above components, please visit: [Compatibility list of integrated components](README.md#4-integrated-data-application-components)**.     &nbsp; &nbsp; &nbsp; &nbsp;With a pluggable framework architecture, DSS is designed to allow users to quickly integrate new data application tools, or replace various tools that DSS has integrated. For example, replace Scriptis with Zeppelin, and replace Schedulis with DolphinScheduler...    ![DSS one-stop video](images/en_US/readme/onestop.gif)     ### 2. AppConn, based on Linkisï¼Œdefines a unique design concept     &nbsp; &nbsp; &nbsp; &nbsp;AppConn is the core concept that enables DSS to easily and quickly integrate various upper-layer web systems.     &nbsp; &nbsp; &nbsp; &nbsp;AppConn, an application connector, defines a set of unified front-end and back-end three-level integration protocols, allowing external data application systems to easily and quickly becoming a part of DSS data application development.      &nbsp; &nbsp; &nbsp; &nbsp;The three-level specifications of AppConn are: the first-level SSO specification, the second-level organizational structure specification, and the third-level development process specification.     &nbsp; &nbsp; &nbsp; &nbsp;DSS arranges multiple AppConns in series to form a workflow that supports real-time execution and scheduled execution. Users can complete the entire process development of data applications with simple drag and drop operations.     &nbsp; &nbsp; &nbsp; &nbsp;Since AppConn is integrated with Linkis, the external data application system shares the capabilities of resource management, concurrent limiting, and high performance. AppConn also allows sharable context across system level and thus makes external data application completely gets away from application silos.    ### 3. Workspace, as the management unit     &nbsp; &nbsp; &nbsp; &nbsp;With Workspace as the management unit, it organizes and manages business applications of various data application systems, defines a set of common standards for collaborative development of workspaces across data application systems, and provides user role management capabilities.    ### 4. Integrated data application components     &nbsp; &nbsp; &nbsp; &nbsp;DSS has integrated a variety of upper-layer data application systems by implementing multiple AppConns, which can basically meet the data development needs of users.     &nbsp; &nbsp; &nbsp; &nbsp;**If desired, new data application systems can also be easily integrated to replace or enrich DSS's data application development process.** [Click me to learn how to quickly integrate new application systems](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Development_Documentation/Third-party_System_Access_Development_Guide.md)    | Component | Description | DSS0.X Compatibility(recommend DSS0.9.1) | DSS1.0 Compatibility(recommend DSS1.0.1) |  | --------------- | -------------------------------------------------------------------- | --------- | ---------- |  | [**Linkis**](https://github.com/apache/incubator-linkis) | Apache Linkis, builds a layer of computation middleware, by using standard interfaces such as REST/WS/JDBC provided by Linkis, the upper applications can easily access the underlying engines such as MySQL/Spark/Hive/Presto/Flink, etc.  | recommend Linkis0.11.0 (**Released**) | recommend Linkis1.0.3 (**Released**)|  | [**DataApiService**](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Using_Document/DataApiService_Usage_Documentation.md) | (Third-party applications built into DSS) Data API service. The SQL script can be quickly published as a Restful interface, providing Rest access capability to the outside world. | Not supported | recommend DSS1.0.1 (**Released**) |  | [**Scriptis**](https://github.com/WeBankFinTech/DataSphereStudio) | (Third-party applications built into DSS) Support online script writing such as SQL, Pyspark, HiveQL, etc., submit to [Linkis](https://github.com/WeBankFinTech/Linkis ) to perform data analysis web tools. | recommend DSS0.9.1 (**Released**) | recommend DSS1.0.1 (**Released**) |  | [**Schedulis**](https://github.com/WeBankFinTech/Schedulis) | Workflow task scheduling system based on Azkaban secondary development, with financial-grade features such as high performance, high availability and multi-tenant resource isolation. | recommend Schedulis0.6.1 (**Released**) | >= Schedulis0.6.2 (**Developing**) |  | **EventCheck** | (Third-party applications built into DSS) Provides cross-business, cross-engineering, and cross-workflow signaling capabilities. | recommend DSS0.9.1 (**Released**) | recommend DSS1.0.1 (**Released**) |  | **SendEmail** | (Third-party applications built into DSS) Provides the ability to send data, all the result sets of other workflow nodes can be sent by email | recommend 0.9.1 (**Released**) | recommend 1.0.1 (**Released**) |  | [**Qualitis**](https://github.com/WeBankFinTech/Qualitis) | Data quality verification tool, providing data verification capabilities such as data integrity and correctness | recommend Qualitis0.8.0 (**Released**) | >= Qualitis0.9.0 (**Released**) |  | [**Streamis**](https://github.com/WeBankFinTech/Streamis) | Streaming application development management tool. It supports the release of Flink Jar and Flink SQL, and provides the development, debugging and production management capabilities of streaming applications, such as: start-stop, status monitoring, checkpoint, etc. | Not supported | >= Streamis0.1.0 (**Released**) |  | [**Exchangis**](https://github.com/WeBankFinTech/Exchangis) | A data exchange platform that supports data transmission between structured and unstructured heterogeneous data sources, the upcoming Exchangis1. 0, will be connected with DSS workflow | not supported | >= Exchangis1.0.0 (**Developing**) |  | [**Visualis**](https://github.com/WeBankFinTech/Visualis) | A data visualization BI tool based on the second development of Davinci, an open source project of CreditEase, provides users with financial-level data visualization capabilities in terms of data security. | recommend Visualis0.5.0 (**Released**) | >= Visualis1.0.0 (**Developing**) |  | [**Prophecis**](https://github.com/WeBankFinTech/Prophecis) | A one-stop machine learning platform that integrates multiple open source machine learning frameworks. Prophecis' MLFlow can be connected to DSS workflow through AppConn. | Not supported | >= Prophecis 0.3.0 (**Developing**) |  | **UserManager** | (Third-party applications built into DSS) Automatically initialize all user environments necessary for a new DSS user, including: creating Linux users, various user paths, directory authorization, etc. | recommend DSS0.9.1 (**Released**) | Planned in DSS1.0.2 (**Developing**) |  | [**DolphinScheduler**](https://github.com/apache/dolphinscheduler) | Apache DolphinScheduler, a distributed and scalable visual workflow task scheduling platform, supports one-click publishing of DSS workflows to DolphinScheduler. | Not supported | >= DolphinScheduler1.3.6, planned in DSS1.1.0 (**Developing**) |  | **UserGuide**     | (Third-party applications to be built into DSS) It mainly provides help documentation, beginner's guide, Dark mode skinning, etc.      | Not supported | Planning in DSS1.1.0 (**Developing**) |  | **DataModelCenter** | (Third-party applications to be built into DSS) It mainly provides the capabilities of data warehouse planning, data model development and data asset management. Data warehouse planning includes subject domains, data warehouse layers, modifiers, etc.; data model development includes indicators, dimensions, metrics, wizard-based table building, etc.; data assets are connected to Apache Atlas to provide data lineage capabilities. | Not supported | Planning in DSS1.2.0 (**Developing**) |  | [**Airflow**](https://github.com/apache/airflow) | Supports publishing DSS workflows to Apache Airflow for scheduling. | recommend DSS0.9.1, not yet merged | Not supported |      ## Demo Trial environment     &nbsp; &nbsp; &nbsp; &nbsp;The function of DataSphere Studio supporting script execution has high security risks, and the isolation of the WeDataSphere Demo environment has not been completed. Considering that many users are inquiring about the Demo environment, we decided to first issue invitation codes to the community and accept trial applications from enterprises and organizations.     &nbsp; &nbsp; &nbsp; &nbsp;If you want to try out the Demo environment, please join the DataSphere Studio community user group (**Please refer to the end of the document**), and contact **WeDataSphere Group Robot** to get an invitation code.     &nbsp; &nbsp; &nbsp; &nbsp;DataSphereStudio Demo environment user registration page: [click me to enter](https://dss-open.wedatasphere.com/#/register)     &nbsp; &nbsp; &nbsp; &nbsp;DataSphereStudio Demo environment login page: [click me to enter](https://dss-open.wedatasphere.com/#/login)    ##  Download     &nbsp; &nbsp; &nbsp; &nbsp;Please go to the [DSS Releases Page](https://github.com/WeBankFinTech/DataSphereStudio/releases) to download a compiled version or a source code package of DSS.    ## Compile and deploy     &nbsp; &nbsp; &nbsp; &nbsp;Please follow [Compile Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Development_Documentation/Compilation_Documentation.md) to compile DSS from source code.     &nbsp; &nbsp; &nbsp; &nbsp;Please refer to [Deployment Documents](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/DSS_Single-Server_Deployment_Documentation.md) to do the deployment.    ## Examples and Guidance     &nbsp; &nbsp; &nbsp; &nbsp;You can find examples and guidance for how to use DSS in [User Manual](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Using_Document/DSS_User_Manual.md).      ## Documents     &nbsp; &nbsp; &nbsp; &nbsp;For a complete list of documents for DSS1.0, see [DSS-Doc](https://github.com/WeBankFinTech/DataSphereStudio-Doc)     &nbsp; &nbsp; &nbsp; &nbsp;The following is the installation guide for DSS-related AppConn plugins:    - [Visualis AppConn Plugin Installation Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/VisualisAppConn_Plugin_Installation_Documentation.md)    - [Schedulis AppConn Plugin Installation Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/SchedulisAppConn_Plugin_Installation_Documentation.md)    - [Qualitis AppConn Plugin Installation Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/QualitisAppConn_Plugin_Installation_Documentation.md)    - [Exchangis AppConn Plugin Installation Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/ExchangisAppConn_Plugin_Installation_Documentation.md)    ## Architecture    ![DSS Architecture](images/en_US/readme/architecture.png)    ## Usage Scenarios     &nbsp; &nbsp;&nbsp; &nbsp;DataSphere Studio is suitable for the following scenarios:     &nbsp; &nbsp;&nbsp; &nbsp;1. Scenarios in which big data platform capability is being prepared or initialized but no data application tools are available.     &nbsp; &nbsp;&nbsp; &nbsp;2. Scenarios in which users already have big data foundation platform capabilities but with only a few data application tools.     &nbsp; &nbsp;&nbsp; &nbsp;3. Scenarios in which users have the ability of big data foundation platform and comprehensive data application tools, but suffers strong isolation and and high learning costs because those tools have not been integrated together.     &nbsp; &nbsp;&nbsp; &nbsp;4. Scenarios in which users have the capabilities of big data foundation platform and comprehensive data application tools. but lacks unified and standardized specifications, while a part of these tools have been integrated.    ## Contributing     &nbsp; &nbsp; &nbsp; &nbsp;Contributions are always welcomed, we need more contributors to build DSS together. either code, or doc, or other supports that could help the community.     &nbsp; &nbsp; &nbsp; &nbsp;For code and documentation contributions, please follow the contribution guide.    ## Communication     &nbsp; &nbsp; &nbsp; &nbsp;For any questions or suggestions, please kindly submit an issue.     &nbsp; &nbsp; &nbsp; &nbsp;You can scan the QR code below to join our WeChat and QQ group to get more immediate response.    ![communication](images/en_US/readme/communication.png)    ## Who is using DSS     &nbsp; &nbsp; &nbsp; &nbsp;We opened an issue for users to feedback and record who is using DSS.     &nbsp; &nbsp; &nbsp; &nbsp;Since the first release of DSS in 2019, it has accumulated more than 700 trial companies and 1000+ sandbox trial users, which involving diverse industries, from finance, banking, tele-communication, to manufactory, internet companies and so on.    ## License     &nbsp; &nbsp; &nbsp; &nbsp;DSS is under the Apache 2.0 license. See the [License](LICENSE) file for details. """
Big data;https://github.com/mesosphere/marathon;"""# Project Status    Support for DC/OS ends on October 31, 2021. We will continue to provide support for our current DC/OS customers per their contracts, of course. However, we will no longer be investing in new features or capabilities or maintaining the related repositories. If a customer wishes to continue use of the DC/OS Enterprise platform or other non-free DC/OS components, the customer can purchase an End-of-Life License or Perpetual Use License, however support is not included in these licenses and continued use of DC/OS will be at your own discretion and risk. We apologize for any inconvenience this may have caused.    We want to thank all of our loyal customers, particularly those DC/OS users who were fellow pioneers in the growth of the cloud native landscape from the beginning.    # [Marathon](https://mesosphere.github.io/marathon/) [![Build Status](https://jenkins.mesosphere.com/service/jenkins/buildStatus/icon?job=marathon-pipelines/master)](https://jenkins.mesosphere.com/service/jenkins/buildStatus/icon?job=marathon-pipelines/master) [![Issues](https://img.shields.io/badge/Issues-JIRA-ff69b4.svg?style=flat)](https://jira.mesosphere.com/projects/MARATHON/issues/)    Marathon is a production-proven [Apache Mesos][Mesos] framework for container orchestration. [DC/OS](https://dcos.io/get-started/#marathon) is the easiest way to start using Marathon. Issues are tracked in [JIRA](https://jira.mesosphere.com/projects/MARATHON/issues/).    Marathon provides a  [REST API](https://mesosphere.github.io/marathon/docs/rest-api.html) for  starting, stopping, and scaling applications. Marathon is written in Scala and  can run in highly-available mode by running multiple copies. The  state of running tasks gets stored in the Mesos state abstraction.    Marathon is also used as a *meta framework*: you can start other Mesos frameworks such as  Chronos or [Storm][Storm] with it to ensure they survive machine failures.  It can launch anything that can be launched in a standard shell. In fact, you  can even start other Marathon instances via Marathon.    ## Features    * *HA* - run any number of Marathon schedulers, but only one gets elected as      leader; if you access a non-leader, your request gets proxied to the      current leader  * *[Constraints](https://mesosphere.github.io/marathon/docs/constraints.html)* - e.g., only one instance of an application per rack, node, etc.  * *[Service Discovery &amp; Load Balancing](https://mesosphere.github.io/marathon/docs/service-discovery-load-balancing.html)* via HAProxy or the events API (see below).  * *[Health Checks](https://mesosphere.github.io/marathon/docs/health-checks.html)*: check your application's health via HTTP or TCP checks.  * *[Event Subscription](https://mesosphere.github.io/marathon/docs/rest-api.html#event-subscriptions)* lets you supply an HTTP endpoint to receive notifications, for example to integrate with an external load balancer.  * *[Marathon UI](https://mesosphere.github.io/marathon/docs/marathon-ui.html)*  * *[JSON/REST API](https://mesosphere.github.io/marathon/docs/rest-api.html)* for easy integration and scriptability  * *[*Basic Auth* and *SSL*](https://mesosphere.github.io/marathon/docs/ssl-basic-access-authentication.html)*  * *[Metrics](https://mesosphere.github.io/marathon/docs/metrics.html)*:    query them at `/metrics` in JSON format, push them to systems like Graphite, StatsD and DataDog, or scrape them using Prometheus.    ## Documentation    Marathon documentation is available on the [Marathon GitHub pages site](http://mesosphere.github.io/marathon/).    Documentation for installing and configuring the full Mesosphere stack including Mesos and Marathon is available on the [Mesosphere website](http://docs.mesosphere.com).    ## Issue Tracking    Marathon uses [JIRA](https://jira.mesosphere.com/projects/MARATHON) to track issues. You can [browse](https://jira.mesosphere.com/projects/MARATHON/issues/) existing issues or [file a new issue](https://jira.mesosphere.com/secure/CreateIssue!default.jspa?pid=10401) with your GitHub account.    Note for users of GitHub issues: All existing issues have been migrated and closed, and a reference to the related [JIRA](https://jira.mesosphere.com/projects/MARATHON) has been added as a comment.  We leave the GitHub issues available for reference. Going forward please use [JIRA](https://jira.mesosphere.com/projects/MARATHON) always.    ### Contributing    We heartily welcome external contributions to Marathon's documentation. Documentation should be committed to the `master` branch and published to our GitHub pages site using the instructions in [docs/README.md](https://github.com/mesosphere/marathon/tree/master/docs).    ## Setting Up And Running Marathon    ### Dependencies  Marathon has the following compile-time dependencies:  * sbt - A build tool for scala. You can find the instructions for installing sbt for Mac OS X and Linux over [here](http://www.scala-sbt.org/0.13/tutorial/Setup.html).  * JDK 1.8+    For run-time, Marathon has the following dependencies:  * libmesos - JNI bindings for talking to Apache Mesos master. Look at the *Install Mesos* section for instructions to get libmesos.  * Apache Zookeeper - You can have a separate Zookeeper installation specifically for Marathon, or you can use the same Zookeeper used by Mesos.    ### Installation    #### Getting started with [DC/OS](https://dcos.io/get-started/#marathon)  The by far easiest way to get Marathon running is to use [DC/OS](https://dcos.io/get-started/#marathon). Marathon is pre-bundled into [DC/OS](https://dcos.io/get-started/#marathon).    #### Install Mesos  Marathon requires libmesos, a shared object library, that contains JNI bindings for Marathon to talk to the Mesos master. *libmesos* comes as part of the Apache Mesos installation. There are three options for installing Apache Mesos.    ##### Installing Mesos from prepackaged releases  Instructions on how to install prepackaged releases of Mesos are available [in the Marathon docs](https://mesosphere.github.io/marathon/docs/).    ##### Building Mesos from source  **NOTE:** *Choose this option only if building Marathon from source, else there might be version incompatibility between pre-packaged releases of Marathon and Mesos built from source.*    You can find the instructions for compiling Mesos from source in the [Apache Mesos getting started docs](http://mesos.apache.org/getting-started/). If you want Mesos to install libraries and executables in a non-default location use the --prefix option during configuration as follows:    ```console  ./configure --prefix=<path to Mesos installation>  ```    The `make install` will install libmesos (libmesos.so on Linux and libmesos.dylib on Mac OS X) in the install directory.    ##### Using the Mesos Version Manager  **NOTE:** *Choose this option only if building Marathon from source, else there might be version incompatibility between pre-packaged releases of Marathon and Mesos built from source.*      The Mesos Version Manager (mvm) compiles, configures, and manages multiple versions of Apache Mesos.  It allows switching between versions quickly, making it easy to test Marathon against different versions of Mesos.    ###### Prerequisites    The Mesos Version Manager assumes that all dependencies of Apache Mesos are readily installed.    Please refer to the [Apache Mesos getting started docs](http://mesos.apache.org/gettingstarted/) for instructions on how to set up the build environment.    MVM compiles Mesos with SSL support by default, which requires openssl and libevent to be installed.    On macOS, the packages can be installed using brew: `brew install openssl libevent`    On CentOS, the packages can be installed using yum: `sudo yum install -y libevent-devel openssl-devel`    ###### Usage    The script can be run as follows:            cd marathon          cd tools          ./mvm.sh <VERSION> [SHELL]    The following command will launch a bash shell configured for Mesos 1.2.0: `./mvm.sh 1.2.0 bash`    You should consider placing the script into a folder in your shell's `PATH` if you are using it regularly.    The mvm script accepts three different formats as version name:    1. Version tags from the Mesos repository. Use `./mvm.sh --tags` in order to obtain a list of available tags.  2. Commit hashes from the Mesos repository.  3. The `--latest` flag, which automatically chooses the latest development version: `./mvm.sh --latest`.    MVM Will automatically download & compile Apache Mesos if necessary.  It will then spawn a new bash shell with the chosen version of Mesos activated.    For more information see `./mvm.sh --help`.    **Note**: You will have to re-run the script if you wish to use Mesos after closing the shell.  See `./mvm.sh --help` information on how to  permanently configure your shell for mvm to avoid this.    #### Install Marathon    Instructions on how to install prepackaged releases are available [in the Marathon docs](https://mesosphere.github.io/marathon/docs/). Alternatively, you can build Marathon from source.    ##### Building from Source    1.  To build Marathon from source, check out this repo and use sbt to build a universal:            git clone https://github.com/mesosphere/marathon.git          cd marathon          sbt 'run --master localhost:5050 --zk zk://localhost:2181/marathon'            **Troubleshooting**      1. Failure in retrieval of IP address of the local machine will result in an error and may look like this:                `Failed to obtain the IP address for '<local-machine>'; the DNS service may not be able to resolve it: nodename nor servname provided, or not known`                    Make sure that `LIBPROCESS_IP` environment variable is set.          ```          export LIBPROCESS_IP=""127.0.0.1""          ```      1. When the `MESOS_NATIVE_JAVA_LIBRARY` environment variable is not set, the following error may occur,                    `java.lang.UnsatisfiedLinkError: no mesos in java.library.path...`                    Make sure that `MESOS_NATIVE_JAVA_LIBRARY` environment variable is set.          ```          export MESOS_NATIVE_JAVA_LIBRARY=""/path/to/mesos/lib/libmesos.dylib""          ```       1.  Run `sbt universal:packageZipTarball` to package Marathon as an txz file      containing bin/marathon fully packaged.    1. Run `cd tools/packager; make tag-docker` for a local Marathon docker image.    ### Running in Development Mode    Mesos local mode allows you to run Marathon without launching a full Mesos  cluster. It is meant for experimentation and not recommended for production  use. Note that you still need to run ZooKeeper for storing state. The following  command launches Marathon on Mesos in *local mode*. Point your web browser to  `http://localhost:8080` to see the Marathon UI.            mesos-local          sbt 'run --master localhost:5050 --zk zk://localhost:2181/marathon'    For more information on how to run Marathon in production and configuration  options, see [the Marathon docs](https://mesosphere.github.io/marathon/docs/).    ## Developing Marathon    See [developing Marathon](./docs/docs/developing.md) in the docs.    ## Marathon Clients    * [marathonctl](https://github.com/shoenig/marathonctl) A handy CLI tool for controlling Marathon  * [Ruby gem and command line client](https://rubygems.org/gems/marathon-api)        Running Chronos with the Ruby Marathon Client:            marathon -M http://foo.bar:8080 start -i chronos -u https://s3.amazonaws.com/mesosphere-binaries-public/chronos/chronos.tgz \              -C ""./chronos/bin/demo ./chronos/config/nomail.yml \              ./chronos/target/chronos-1.0-SNAPSHOT.jar"" -c 1.0 -m 1024  * [Ruby gem marathon_deploy](https://github.com/eBayClassifiedsGroup/marathon_deploy) alternative command line tool to deploy using json or yaml files with ENV macros.  * [Scala client](https://github.com/guidewire/marathon-client), developed at Guidewire  * [Java client](https://github.com/mesosphere/marathon-client), developed at [Mesosphere](https://mesosphere.com)  * [Java client](https://github.com/mohitsoni/marathon-client) by Mohit Soni  * [Maven plugin](https://github.com/dcos-labs/dcos-maven-plugin), developed by [Johannes Unterstein](https://github.com/unterstein)  * [Maven plugin](https://github.com/holidaycheck/marathon-maven-plugin), developed at [HolidayCheck](http://www.holidaycheck.com/)  * [Python client](https://github.com/thefactory/marathon-python), developed at [The Factory](http://www.thefactory.com)  * [Python client](https://github.com/Wizcorp/marathon-client.py), developed at [Wizcorp](http://www.wizcorp.jp)  * [Go client](https://github.com/gambol99/go-marathon) by Rohith Jayawardene  * [Go client](https://github.com/jbdalido/gomarathon) by Jean-Baptiste Dalido  * [Node client](https://github.com/silas/node-mesos) by Silas Sewell  * [Clojure client](https://github.com/codemomentum/marathonclj) by Halit Olali  * [sbt plugin](https://github.com/Tapad/sbt-marathon), developed at [Tapad](https://tapad.com)    ## Companies using Marathon    Across all installations Marathon is managing applications on more than 100,000 nodes world-wide. These are some of the companies using it:    * [Adform](http://site.adform.com/)  * [Alauda](http://www.alauda.cn/)  * [Allegro](http://allegro.tech)  * [AllUnite](https://allunite.com/)  * [Argus Cyber Security](http://argus-sec.com/)  * [Artirix](http://www.artirix.com/)  * [Arukas](https://arukas.io/)  * [Avast](https://www.avast.com/)  * [AVENTER](https://www.aventer.biz/)  * [bol.com](https://www.bol.com/)  * [Brand24](https://brand24.com/)  * [Branding Brand](http://www.brandingbrand.com/)  * [China Mobile](http://www.chinamobileltd.com/en/global/home.php)  * [China Unicom](http://eng.chinaunicom.com/index/index.html)  * [Corvisa](https://www.corvisa.com/)  * [Criteo](http://www.criteo.com/)  * [Daemon](http://www.daemon.com.au/)  * [DataMan](http://www.shurenyun.com/)  * [Deutsche Telekom](https://www.telekom.com/)  * [DHL Parcel](https://www.dhlparcel.nl/)  * [Disqus](https://disqus.com/)  * [DueDil](https://www.duedil.com/)  * [eBay](http://www.ebay.com/)  * [Erasys](http://www.erasys.de/)  * [The Factory](https://github.com/thefactory/)  * [Football Radar](http://www.footballradar.com)  * [Guidewire](https://www.guidewire.com/)  * [Groupon](https://www.groupon.com/)  * [GSShop](http://www.gsshop.com/)  * [GrowingIO](https://www.growingio.com/)  * [HolidayCheck](http://www.holidaycheck.com/)  * [Human API](https://humanapi.co/)  * [Indix](http://www.indix.com/)  * [ING](http://www.ing.com/)  * [Intent HQ](https://www.intenthq.com/)  * [iQIYI](http://www.iqiyi.com/)  * [LaunchKey](https://launchkey.com/)  * [Mapillary](https://www.mapillary.com/)  * [Measurence](http://www.measurence.com/)  * [Motus](http://www.motus.com/)  * [Notonthehighstreet](http://www.notonthehighstreet.com/)  * [OpenTable](http://www.opentable.com/)  * [Opera](https://www.opera.com)  * [Orbitz](http://www.orbitz.com/)  * [Otto](https://www.otto.de/)  * [OVH](https://ovh.com/)  * [PayPal](https://www.paypal.com)  * [Qubit](http://www.qubit.com/)  * [RelateIQ](https://www.salesforceiq.com/)  * [Refinery29](https://www.refinery29.com)  * [Sailthru](http://www.sailthru.com/)  * [SAKURA Internet Inc](https://www.sakura.ad.jp/)  * [sloppy.io](http://sloppy.io/)  * [SmartProcure](https://smartprocure.us/)  * [Strava](https://www.strava.com)  * [Sveriges Television](http://www.svt.se)  * [Salesforce](http://www.salesforce.com)  * [T2 Systems](http://t2systems.com)  * [Tapad](https://tapad.com)  * [Teradata](http://www.teradata.com)  * [Toss](https://www.toss.im/)  * [trivago](http://www.trivago.com/)  * [tronc / L.A. Times](http://www.tronc.com/about-us/)  * [VANAD Enovation](http://www.vanadenovation.nl/)  * [Viadeo](http://www.viadeo.com)  * [Wikia](http://www.wikia.com/Wikia)  * [William Hill](https://www.williamhill.com)  * [WooRank](https://www.woorank.com/)  * [Yelp](http://www.yelp.com/)    Not in the list? Open a pull request and add yourself!    ## Help    Have you found an issue? Feel free to report it using our [JIRA Issues](https://jira.mesosphere.com/projects/MARATHON/summary) page.  In order to speed up response times, we ask you to provide as much  information on how to reproduce the problem as possible. If the issue is related   in any way to the web UI, we kindly ask you to use the `gui` label.    If you have questions, please post on the [Marathon Framework](https://groups.google.com/forum/?hl=en#!forum/marathon-framework) email list.     You can find Marathon support in the `#marathon` channel, and Mesos support in the `#mesos` channel, on [freenode][freenode] (IRC). Alternatively, check out the same channels on the [Mesos Slack](https://mesos.slack.com/) ([request an invitation here](https://mesos-slackin.herokuapp.com/)).     The team at [Mesosphere][Mesosphere] is also happy to answer any questions.    If you'd like to take part in design research and test new features in Marathon before they're released, please add your name to our [UX Research](http://uxresearch.mesosphere.com) list.    ## Authors    Marathon was created by [Tobias Knaup](https://github.com/guenter) and  [Florian Leibert](https://github.com/florianleibert) and continues to be  developed by the team at Mesosphere and by many contributors from  the community.    [Chronos]: https://github.com/mesos/chronos ""Airbnb's Chronos""  [Mesos]: https://mesos.apache.org/ ""Apache Mesos""  [Zookeeper]: https://zookeeper.apache.org/ ""Apache Zookeeper""  [Storm]: http://storm.apache.org ""distributed realtime computation""  [freenode]: https://freenode.net/ ""IRC channels""  [upstart]: http://upstart.ubuntu.com/ ""Ubuntu's event-based daemons""  [init]: https://en.wikipedia.org/wiki/Init ""init""  [Mesosphere]: https://mesosphere.com/ ""Mesosphere""    ## Acknowledgements    **YourKit, LLC**    ![YourKit, LLC](https://www.yourkit.com/images/yklogo.png)    YourKit supports open source projects with its full-featured Java  Profiler.  YourKit, LLC is the creator of <a  href=""https://www.yourkit.com/java/profiler/index.jsp"">YourKit Java  Profiler</a>  and <a href=""https://www.yourkit.com/.net/profiler/index.jsp"">YourKit  .NET Profiler</a>,  innovative and intelligent tools for profiling Java and .NET  applications. """
Big data;https://github.com/krotik/eliasdb;"""EliasDB  =======    <p align=""center"">    <img height=""300px"" style=""height:300px;"" src=""eliasdb_logo.png"">  </p>    EliasDB is a graph-based database which aims to provide a lightweight solution for projects which want to store their data as a graph.    [![Code coverage](https://void.devt.de/pub/eliasdb/test_result.svg)](https://void.devt.de/pub/eliasdb/coverage.txt)  [![Go Report Card](https://goreportcard.com/badge/devt.de/krotik/eliasdb?style=flat-square)](https://goreportcard.com/report/devt.de/krotik/eliasdb)  [![Go Reference](https://pkg.go.dev/badge/krotik/eliasdb.svg)](https://pkg.go.dev/devt.de/krotik/eliasdb)  [![Mentioned in Awesome Go](https://awesome.re/mentioned-badge-flat.svg)](https://github.com/avelino/awesome-go)    Features  --------  - Build on top of a custom key-value store which supports transactions and memory-only storage.  - Data is stored in nodes (key-value objects) which are connected via edges.  - Stored graphs can be separated via partitions.  - Stored graphs support cascading deletions - delete one node and all its ""children"".  - All stored data is indexed and can be quickly searched via a full text phrase search.  - EliasDB has a GraphQL interface which can be used to store and retrieve data.  - For more complex queries EliasDB has an own query language called EQL with an sql-like syntax.  - Includes a scripting interpreter to define alternative actions for database operations or writing backend logic.  - Written in Go from scratch. Only uses gorilla/websocket to support websockets for GraphQL subscriptions.  - The database can be embedded or used as a standalone application.  - When used as a standalone application it comes with an internal HTTPS webserver which provides user management, a REST API and a basic file server.  - When used as an embedded database it supports transactions with rollbacks, iteration of data and rule based consistency management.    Getting Started (standalone application)  ----------------------------------------  You can download a pre-compiled package for Windows (win64) or Linux (amd64) [here](https://void.devt.de/pub/eliasdb).    Extract it and execute the executable with:  ```  eliasdb server  ```  The executable should automatically create 3 subfolders and a configuration file. It should start an HTTPS server on port 9090. To see a terminal point your webbrowser to:  ```  https://localhost:9090/db/term.html  ```  After accepting the self-signed certificate from the server you should see a web terminal. EliasDB can be stopped with a simple CTRL+C or by overwriting the content in eliasdb.lck with a single character.    Getting Started (docker image)  ------------------------------  You can pull the latest docker image of EliasDB from [Dockerhub](https://hub.docker.com/r/krotik/eliasdb):  ```  docker pull krotik/eliasdb  ```    Create an empty directory, change into it and run the following to start the server:  ```  docker run --user $(id -u):$(id -g) -v $PWD:/data -p 9090:9090 krotik/eliasdb server  ```  This exposes port 9090 from the container on the local machine. All runtime related files are written to the current directory as the current user/group.    Connect to the running server with a console by running:  ```  docker run --rm --network=""host"" -it -v $PWD:/data --user $(id -u):$(id -g) -v $PWD:/data krotik/eliasdb console  ```    ### Tutorial:    To get an idea of what EliasDB is about have a look at the [tutorial](examples/tutorial/doc/tutorial.md). This tutorial will cover the basics of EQL and show how data is organized.    There is a separate [tutorial](examples/tutorial/doc/tutorial_graphql.md) on using ELiasDB with GraphQL.    ### REST API:    The terminal uses a REST API to communicate with the backend. The REST API can be browsed using a dynamically generated swagger.json definition (https://localhost:9090/db/swagger.json). You can browse the API of EliasDB's latest version [here](http://petstore.swagger.io/?url=https://devt.de/krotik/eliasdb/raw/master/swagger.json).    ### Scripting    EliasDB supports a scripting language called [ECAL](ecal.md) to define alternative actions for database operations such as store, update or delete. The actions can be taken before, instead (by calling `db.raiseGraphEventHandled()`) or after the normal database operation. The language is powerful enough to write backend logic for applications.    There is a [VSCode integration](https://devt.de/krotik/ecal/src/master/ecal-support/README.md) available which supports syntax highlighting and debugging via the debug server. More information can be found in the [code repository](https://devt.de/krotik/ecal) of the interpreter.    ### Clustering:    EliasDB supports to be run in a cluster by joining multiple instances of EliasDB together. You can read more about it [here](cluster.md).    ### Command line options  The main EliasDB executable has two main tools:  ```  Usage of ./eliasdb <tool>    EliasDB graph based database    Available commands:        console   EliasDB server console      server    Start EliasDB server  ```  The most important one is server which starts the database server. The server has several options:  ```  Usage of ./eliasdb server [options]      -export string      	Export the current database to a zip file    -help      	Show this help message    -import string      	Import a database from a zip file    -no-serv      	Do not start the server after initialization  ```  If the `EnableECALScripts` configuration option is set the following additional option is available:  ```  -ecal-console      Start an interactive interpreter console for ECAL  ```  The interactive console can be used to inspect and modify the runtime state of the ECAL interpreter.    Once the server is started the console tool can be used to interact with the server. The options of the console tool are:  ```  Usage of ./eliasdb console [options]      -exec string      	Execute a single line and exit    -file string      	Read commands from a file and exit    -help      	Show this help message    -host string      	Host of the EliasDB server (default ""localhost"")    -port string      	Port of the EliasDB server (default ""9090"")  ```  On the console type 'q' to exit and 'help' to get an overview of available commands:  ```  Command Description  export  Exports the last output.  find    Do a full-text search of the database.  help    Display descriptions for all available commands.  info    Returns general database information.  part    Displays or sets the current partition.  ver     Displays server version information.  ```  It is also possible to directly run EQL and GraphQL queries on the console. Use the arrow keys to cycle through the command history.    ### Configuration  EliasDB uses a single configuration file called eliasdb.config.json. After starting EliasDB for the first time it should create a default configuration file. Available configurations are:    | Configuration Option | Description |  | --- | --- |  | ClusterConfigFile | Cluster configuration file. |  | ClusterLogHistory | File which is used to store the console history. |  | ClusterStateInfoFile | File which is used to store the cluster state. |  | CookieMaxAgeSeconds | Lifetime for cookies used by EliasDB. |  | ECALDebugServerHost | Hostname the ECAL debug server should listen to. |  | ECALDebugServerPort | Port on which the debug server should listen on. |  | ECALEntryScript | Entry script for ECAL interpreter. |  | ECALLogFile | Logfile for ECAL interpreter. An empty string will cause the logger to write to the console. |  | ECALLogLevel | Log level for ECAL interpreter. Can be debug, info or error. |  | ECALScriptFolder | Directory for ECAL scripts. |  | ECALWorkerCount | Number of worker threads in the ECA engine's thread pool. |  | EnableAccessControl | Flag if access control for EliasDB should be enabled. This provides user authentication and authorization features. |  | EnableCluster | Flag if EliasDB clustering support should be enabled. EXPERIMENTAL! |  | EnableClusterTerminal | Flag if the cluster terminal file /web/db/cluster.html should be created. |  | EnableECALDebugServer | Flag if the ECAL debug server should be started. Note: This will slow ECAL performance significantly. |  | EnableECALScripts | Flag if ECAL scripts should be executed on startup. |  | EnableReadOnly | Flag if the datastore should be open read-only. |  | EnableWebFolder | Flag if the files in the webfolder /web should be served up by the webserver. If false only the REST API is accessible. |  | EnableWebTerminal | Flag if the web terminal file /web/db/term.html should be created. |  | HTTPSCertificate | Name of the webserver certificate which should be used. A new one is created if it does not exist. |  | HTTPSHost | Hostname the webserver should listen to. This host is also used in the dynamically generated swagger definition. |  | HTTPSKey | Name of the webserver private key which should be used. A new one is created if it does not exist. |  | HTTPSPort | Port on which the webserver should listen on. |  | LocationAccessDB | File which is used to store access control information. This file can be edited while the server is running and changes will be picked up immediately. |  | LocationDatastore | Directory for datastore files. |  | LocationHTTPS | Directory for the webserver's SSL related files. |  | LocationUserDB | File which is used to store (hashed) user passwords. |  | LocationWebFolder | Directory of the webserver's webfolder. |  | LockFile | Lockfile for the webserver which will be watched duing runtime. Replacing the content of this file with a single character will shutdown the webserver gracefully. |  | MemoryOnlyStorage | Flag if the datastore should only be kept in memory. |  | ResultCacheMaxAgeSeconds | EQL queries create result sets which are cached. The value describes the amount of time in seconds a result is kept in the cache. |  | ResultCacheMaxSize | EQL queries create result sets which are cached. The value describes the number of results which can be kept in the cache. |    Note: It is not (and will never be) possible to access the REST API via HTTP.    Enabling Access Control  -----------------------  It is possible to enforce access control by enabling the `EnableAccessControl` configuration option. When started with enabled access control EliasDB will only allow known users to connect. Users must authenticate with a password before connecting to the web interface or the REST API. On the first start with the flag enabled the following users are created by default:    |Username|Default Password|Groups|Description|  |---|---|---|---|  |elias|elias|admin/public|Default Admin|  |johndoe|doe|public|Default unprivileged user|    Users can be managed from the console. Please do either delete the default users or change their password after starting EliasDB.    Users are organized in groups and permissions are assigned to groups. Permissions are given to endpoints of the REST API. The following permissions are available:    |Type|Allowed HTTP Request Type|Description|  |---|---|---|  |Create|Post|Creating new data|  |Read|Get|Read data|  |Update|Put|Modify existing data|  |Delete|Delete|Delete data|    The default group permissions are:    |Group|Path|Permissions|  |---|---|---|  |admin|/db/*|`CRUD`|  |public|/|`-R--`|  ||/css/*|`-R--`|  ||/db/*|`-R--`|  ||/img/*|`-R--`|  ||/js/*|`-R--`|  ||/vendor/*|`-R--`|      Building EliasDB  ----------------  To build EliasDB from source you need to have Go installed (go >= 1.12):    - Create a directory, change into it and run:  ```  git clone https://github.com/krotik/eliasdb/ .  ```    - You can build EliasDB's executable with:  ```  go build cli/eliasdb.go  ```    Building EliasDB as Docker image  --------------------------------  EliasDB can be build as a secure and compact Docker image.    - Create a directory, change into it and run:  ```  git clone https://github.com/krotik/eliasdb/ .  ```    - You can now build the Docker image with:  ```  docker build --tag krotik/eliasdb .  ```    Example Applications  --------------------  - [Chat](examples/chat/doc/chat.md) - A simple chat application showing node modification via ECAL script, user management and subscriptions.  - [Data-mining](examples/data-mining/doc/data-mining.md) - A more complex application which uses the cluster feature of EliasDB and GraphQL for data queries.  - [Game](examples/game/doc/game.md) - A multiplayer game example using ECAL for simulating the game scene in the backend.    Further Reading  ---------------  - A design document which describes the different components of the graph database. [Link](eliasdb_design.md)  - A reference for EliasDB's custom query language EQL. [Link](eql.md)  - A reference for EliasDB's support for GraphQL. [Link](graphql.md)  - A quick overview of what you can do when you embed EliasDB in your own Go project. [Link](embedding.md)    License  -------  EliasDB source code is available under the [Mozilla Public License](/LICENSE). """
Big data;https://github.com/cockroachdb/cockroach;"""<p align=""center"">    <img src='docs/media/cockroach_db.png?raw=true' width='70%'>  </p>    ---    CockroachDB is a cloud-native distributed SQL database designed to build,   scale, and manage modern, data-intensive applications.    - [What is CockroachDB?](#what-is-cockroachdb)  - [Docs](#docs)  - [Starting with Cockroach Cloud](#starting-with-cockroachcloud)  - [Starting with CockroachDB](#starting-with-cockroachdb)  - [Client Drivers](#client-drivers)  - [Deployment](#deployment)  - [Need Help?](#need-help)  - [Contributing](#contributing)  - [Design](#design)  - [Comparison with Other Databases](#comparison-with-other-databases)  - [See Also](#see-also)    ## What is CockroachDB?    CockroachDB is a distributed SQL database built on a transactional and  strongly-consistent key-value store. It **scales** horizontally;  **survives** disk, machine, rack, and even datacenter failures with  minimal latency disruption and no manual intervention; supports  **strongly-consistent** ACID transactions; and provides a familiar  **SQL** API for structuring, manipulating, and querying data.    For more details, see our [FAQ](https://cockroachlabs.com/docs/stable/frequently-asked-questions.html) or [architecture document](  https://www.cockroachlabs.com/docs/stable/architecture/overview.html).    <p align=""center"">    <a href='https://www.youtube.com/watch?v=VgXiMcbGwzQ'> <img src='docs/media/explainer-video-preview.png' width='70%'> </a>  </p>    ## Docs    For guidance on installation, development, deployment, and administration, see our [User Documentation](https://cockroachlabs.com/docs/stable/).    ## Starting with CockroachCloud    We can run CockroachDB for you, so you don't have to run your own cluster.    See our online documentation: [Quickstart with CockroachCloud](https://www.cockroachlabs.com/docs/cockroachcloud/quickstart.html)    ## Starting with CockroachDB    1. Install CockroachDB:  [using a pre-built executable](https://www.cockroachlabs.com/docs/stable/install-cockroachdb.html) or [build it from source](https://www.cockroachlabs.com/docs/v21.1/install-cockroachdb-linux#build-from-source).  2. [Start a local cluster](https://www.cockroachlabs.com/docs/stable/start-a-local-cluster.html) and connect to it via the [built-in SQL client](https://www.cockroachlabs.com/docs/stable/use-the-built-in-sql-client.html).  3. [Learn more about CockroachDB SQL](https://www.cockroachlabs.com/docs/stable/learn-cockroachdb-sql.html).  4. Use a PostgreSQL-compatible driver or ORM to [build an app with CockroachDB](https://www.cockroachlabs.com/docs/stable/hello-world-example-apps.html).  5. [Explore core features](https://www.cockroachlabs.com/docs/stable/demo-data-replication.html), such as data replication, automatic rebalancing, and fault tolerance and recovery.    ## Client Drivers    CockroachDB supports the PostgreSQL wire protocol, so you can use any available PostgreSQL client drivers to connect from various languages.    - For recommended drivers that we've tested, see [Install Client Drivers](https://www.cockroachlabs.com/docs/stable/install-client-drivers.html).  - For tutorials using these drivers, as well as supported ORMs, see [Example Apps](https://www.cockroachlabs.com/docs/stable/example-apps.html).    ## Deployment    - [CockroachCloud](https://www.cockroachlabs.com/docs/cockroachcloud/quickstart) - Steps to create a [free CockroachCloud cluster](https://cockroachlabs.cloud/signup?referralId=githubquickstart) on your preferred Cloud platform.  - [Manual](https://www.cockroachlabs.com/docs/stable/manual-deployment.html) - Steps to deploy a CockroachDB cluster manually on multiple machines.  - [Cloud](https://www.cockroachlabs.com/docs/stable/cloud-deployment.html) - Guides for deploying CockroachDB on various cloud platforms.  - [Orchestration](https://www.cockroachlabs.com/docs/stable/orchestration.html) - Guides for running CockroachDB with popular open-source orchestration systems.    ## Need Help?    - [CockroachDB Community Slack](https://go.crdb.dev/p/slack) - Join our slack to connect with our engineers and other users running CockroachDB.  - [CockroachDB Forum](https://forum.cockroachlabs.com/) and [Stack Overflow](https://stackoverflow.com/questions/tagged/cockroachdb) - Ask questions, find answers, and help other users.  - [Troubleshooting documentation](https://www.cockroachlabs.com/docs/stable/troubleshooting-overview.html) - Learn how to troubleshoot common errors, cluster setup, and SQL query behavior.  - For filing bugs, suggesting improvements, or requesting new features, help us out by [opening an issue](https://github.com/cockroachdb/cockroach/issues/new).    ## Building from source    See [our wiki](https://wiki.crdb.io/wiki/spaces/CRDB/pages/181338446/Getting+and+building+from+source) for more details.    ## Contributing    We welcome your contributions! If you're looking for issues to work on, try looking at the [good first issue list](https://github.com/cockroachdb/cockroach/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22). We do our best to tag issues suitable for new external contributors with that label, so it's a great way to find something you can help with!    See [our wiki](https://wiki.crdb.io/wiki/spaces/CRDB/pages/73204033/Contributing+to+CockroachDB) for more details.    Engineering discussions take place on our public mailing list, [cockroach-db@googlegroups.com](https://groups.google.com/forum/#!forum/cockroach-db). Also please join our [Community Slack](https://go.crdb.dev/p/slack) (there's a dedicated #contributors channel!) to ask questions, discuss your ideas, and connect with other contributors.    ## Design    For an in-depth discussion of the CockroachDB architecture, see our  [Architecture  Guide](https://www.cockroachlabs.com/docs/stable/architecture/overview.html).  For the original design motivation, see our [design  doc](https://github.com/cockroachdb/cockroach/blob/master/docs/design.md).    ## Licensing    Current CockroachDB code is released under a combination of two licenses, the [Business Source License (BSL)](https://www.cockroachlabs.com/docs/stable/licensing-faqs.html#bsl) and the [Cockroach Community License (CCL)](https://www.cockroachlabs.com/docs/stable/licensing-faqs.html#ccl).    When contributing to a CockroachDB feature, you can find the relevant license in the comments at the top of each file.    For more information, see the [Licensing FAQs](https://www.cockroachlabs.com/docs/stable/licensing-faqs.html).    ## Comparison with Other Databases    To see how key features of CockroachDB stack up against other databases,  check out [CockroachDB in Comparison](https://www.cockroachlabs.com/docs/stable/cockroachdb-in-comparison.html).    ## See Also    - [Tech Talks](https://www.cockroachlabs.com/community/tech-talks/) (by CockroachDB founders, engineers, and customers!)  - [CockroachDB User Documentation](https://cockroachlabs.com/docs/stable/)  - [The CockroachDB Blog](https://www.cockroachlabs.com/blog/)  - Key design documents    - [Serializable, Lockless, Distributed: Isolation in CockroachDB](https://www.cockroachlabs.com/blog/serializable-lockless-distributed-isolation-cockroachdb/)    - [Consensus, Made Thrive](https://www.cockroachlabs.com/blog/consensus-made-thrive/)    - [Trust, But Verify: How CockroachDB Checks Replication](https://www.cockroachlabs.com/blog/trust-but-verify-cockroachdb-checks-replication/)    - [Living Without Atomic Clocks](https://www.cockroachlabs.com/blog/living-without-atomic-clocks/)    - [The CockroachDB Architecture Document](https://github.com/cockroachdb/cockroach/blob/master/docs/design.md) """
Big data;https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers;"""# Awesome Gradient Boosting Research Papers.  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) ![License](https://img.shields.io/github/license/benedekrozemberczki/awesome-gradient-boosting-papers.svg?color=blue) [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/awesome-gradient-boosting-papers.svg)](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers/archive/master.zip) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)  <p align=""center"">    <img width=""450"" src=""boosting.gif"">  </p>    ---------------------------------------------    A curated list of gradient and adaptive boosting papers with implementations from the following conferences:    - Machine learning     * [NeurIPS](https://nips.cc/)      * [ICML](https://icml.cc/)      * [ICLR](https://iclr.cc/)  - Computer vision     * [CVPR](http://cvpr2019.thecvf.com/)     * [ICCV](http://iccv2019.thecvf.com/)     * [ECCV](https://eccv2018.org/)  - Natural language processing     * [ACL](http://www.acl2019.org/EN/index.xhtml)     * [NAACL](https://naacl2019.org/)     * [EMNLP](https://www.emnlp-ijcnlp2019.org/)   - Data     * [KDD](https://www.kdd.org/)     * [CIKM](http://www.cikmconference.org/)        * [ICDM](http://icdm2019.bigke.org/)     * [SDM](https://www.siam.org/Conferences/CM/Conference/sdm19)        * [PAKDD](http://pakdd2019.medmeeting.org)     * [PKDD/ECML](http://ecmlpkdd2019.org)     * [RECSYS](https://recsys.acm.org/)     * [SIGIR](https://sigir.org/)     * [WWW](https://www2019.thewebconf.org/)     * [WSDM](www.wsdm-conference.org)   - Artificial intelligence     * [AAAI](https://www.aaai.org/)     * [AISTATS](https://www.aistats.org/)     * [ICANN](https://e-nns.org/icann2019/)        * [IJCAI](https://www.ijcai.org/)     * [UAI](http://www.auai.org/)    Similar collections about [graph classification](https://github.com/benedekrozemberczki/awesome-graph-classification), [classification/regression tree](https://github.com/benedekrozemberczki/awesome-decision-tree-papers), [fraud detection](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers), [Monte Carlo tree search](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers), and [community detection](https://github.com/benedekrozemberczki/awesome-community-detection) papers with implementations.    ## 2021    - **Precision-based Boosting (AAAI 2021)**    - Mohammad Hossein Nikravan, Marjan Movahedan, Sandra Zilles    - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/17105)    - **BNN: Boosting Neural Network Framework Utilizing Limited Amount of Data (CIKM 2021)**    - Amit Livne, Roy Dor, Bracha Shapira, Lior Rokach    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3459637.3482414)    - **Unsupervised Domain Adaptation for Static Malware Detection based on Gradient Boosting Trees (CIKM 2021)**    - Panpan Qi, Wei Wang, Lei Zhu, See-Kiong Ng    - [[Paper]](https://dl.acm.org/doi/pdf/10.1145/3459637.3482400)    - **Individually Fair Gradient Boosting (ICLR 2021)**    - Alexander Vargo, Fan Zhang, Mikhail Yurochkin, Yuekai Sun    - [[Paper]](https://arxiv.org/abs/2103.16785)    - **Are Neural Rankers still Outperformed by Gradient Boosted Decision Trees (ICLR 2021)**    - Zhen Qin, Le Yan, Honglei Zhuang, Yi Tay, Rama Kumar Pasumarthi, Xuanhui Wang, Michael Bendersky, Marc Najork    - [[Paper]](https://iclr.cc/virtual/2021/spotlight/3536)    - **AdaGCN: Adaboosting Graph Convolutional Networks into Deep Models (ICLR 2021)**    - Ke Sun, Zhanxing Zhu, Zhouchen Lin    - [[Paper]](https://arxiv.org/abs/1908.05081)    - [[Code]](https://github.com/datake/AdaGCN)    - **Uncertainty in Gradient Boosting via Ensembles (ICLR 2021)**    - Andrey Malinin, Liudmila Prokhorenkova, Aleksei Ustimenko    - [[Paper]](https://arxiv.org/abs/2006.10562)    -   - **Boost then Convolve: Gradient Boosting Meets Graph Neural Networks (ICLR 2021)**    - Sergei Ivanov, Liudmila Prokhorenkova    - [[Paper]](https://arxiv.org/abs/2101.08543)    - **GBHT: Gradient Boosting Histogram Transform for Density Estimation (ICML 2021)**    - Jingyi Cui, Hanyuan Hang, Yisen Wang, Zhouchen Lin    - [[Paper]](https://arxiv.org/abs/2106.05738)    - **Boosting for Online Convex Optimization (ICML 2021)**    - Elad Hazan, Karan Singh    - [[Paper]](https://arxiv.org/abs/2102.09305)    - **Accuracy, Interpretability, and Differential Privacy via Explainable Boosting (ICML 2021)**    - Harsha Nori, Rich Caruana, Zhiqi Bu, Judy Hanwen Shen, Janardhan Kulkarni    - [[Paper]](https://arxiv.org/abs/2106.09680)    - **SGLB: Stochastic Gradient Langevin Boosting (ICML 2021)**    - Aleksei Ustimenko, Liudmila Prokhorenkova    - [[Paper]](https://arxiv.org/abs/2001.07248)    - **Self-boosting for Feature Distillation (IJCAI 2021)**    - Yulong Pei, Yanyun Qu, Junping Zhang    - [[Paper]](https://www.ijcai.org/proceedings/2021/131)    - **Boosting Variational Inference With Locally Adaptive Step-Sizes (IJCAI 2021)**    - Gideon Dresdner, Saurav Shekhar, Fabian Pedregosa, Francesco Locatello, Gunnar RÃ¤tsch    - [[Paper]](https://arxiv.org/abs/2105.09240)    - **Probabilistic Gradient Boosting Machines for Large-Scale Probabilistic Regression (KDD 2021)**    - Olivier Sprangers, Sebastian Schelter, Maarten de Rijke    - [[Paper]](https://arxiv.org/abs/2106.01682)    - **Task-wise Split Gradient Boosting Trees for Multi-center Diabetes Prediction (KDD 2021)**    - Mingcheng Chen, Zhenghui Wang, Zhiyun Zhao, Weinan Zhang, Xiawei Guo, Jian Shen, Yanru Qu, Jieli Lu, Min Xu, Yu Xu, Tiange Wang, Mian Li, Weiwei Tu, Yong Yu, Yufang Bi, Weiqing Wang, Guang Ning    - [[Paper]](https://arxiv.org/abs/2108.07107)    - **Better Short than Greedy: Interpretable Models through Optimal Rule Boosting (SDM 2021)**    - Mario Boley, Simon Teshuva, Pierre Le Bodic, Geoffrey I. Webb    - [[Paper]](https://arxiv.org/abs/2101.08380)    ## 2020    - **A Unified Framework for Knowledge Intensive Gradient Boosting: Leveraging Human Experts for Noisy Sparse Domains (AAAI 2020)**    - Harsha Kokel, Phillip Odom, Shuo Yang, Sriraam Natarajan    - [[Paper]](https://personal.utdallas.edu/~sriraam.natarajan/Papers/Kokel_AAAI20.pdf)    - [[Code]](https://github.com/harshakokel/KiGB)    - **Practical Federated Gradient Boosting Decision Trees (AAAI 2020)**    - Qinbin Li, Zeyi Wen, Bingsheng He    - [[Paper]](https://arxiv.org/abs/1911.04206)    - **Privacy-Preserving Gradient Boosting Decision Trees (AAAI 2020)**    - Qinbin Li, Zhaomin Wu, Zeyi Wen, Bingsheng He    - [[Paper]](https://arxiv.org/abs/1911.04209)      - **Accelerating Gradient Boosting Machines (AISTATS 2020)**    - Haihao Lu, Sai Praneeth Karimireddy, Natalia Ponomareva, Vahab S. Mirrokni    - [[Paper]](https://arxiv.org/abs/1903.08708)    - **Scalable Feature Selection for Multitask Gradient Boosted Trees (AISTATS 2020)**    - Cuize Han, Nikhil Rao, Daria Sorokina, Karthik Subbian    - [[Paper]](http://proceedings.mlr.press/v108/han20a.html)    - **Functional Gradient Boosting for Learning Residual-like Networks with Statistical Guarantees (AISTATS 2020)**    - Atsushi Nitanda, Taiji Suzuki    - [[Paper]](http://proceedings.mlr.press/v108/nitanda20a.html)      - **Learning Optimal Decision Trees with MaxSAT and its Integration in AdaBoost (IJCAI 2020)**    - Hao Hu, Mohamed Siala, Emmanuel Hebrard, Marie-JosÃ© Huguet    - [[Paper]](https://www.ijcai.org/Proceedings/2020/163)    - **MixBoost: Synthetic Oversampling using Boosted Mixup for Handling Extreme Imbalance (ICDM 2020)**    - Anubha Kabra, Ayush Chopra, Nikaash Puri, Pinkesh Badjatiya, Sukriti Verma, Piyush Gupta, Balaji Krishnamurthy    - [[Paper]](https://arxiv.org/abs/2009.01571)    - **Boosting for Control of Dynamical Systems (ICML 2020)**    - Naman Agarwal, Nataly Brukhim, Elad Hazan, Zhou Lu    - [[Paper]](https://arxiv.org/abs/1906.08720)    - **Quantum Boosting (ICML 2020)**    - Srinivasan Arunachalam, Reevu Maity    - [[Paper]](https://arxiv.org/abs/2002.05056)    - **Boosted Histogram Transform for Regression (ICML 2020)**    - Yuchao Cai, Hanyuan Hang, Hanfang Yang, Zhouchen Lin    - [[Paper]](https://proceedings.icml.cc/static/paper_files/icml/2020/2360-Paper.pdf)    - **Boosting Frank-Wolfe by Chasing Gradients (ICML 2020)**    - Cyrille W. Combettes, Sebastian Pokutta    - [[Paper]](https://arxiv.org/abs/2003.06369)    - **NGBoost: Natural Gradient Boosting for Probabilistic Prediction (ICML 2020)**    - Tony Duan, Avati Anand, Daisy Yi Ding, Khanh K. Thai, Sanjay Basu, Andrew Y. Ng, Alejandro Schuler    - [[Paper]](https://arxiv.org/abs/1910.03225)    - [[Code]](https://github.com/stanfordmlgroup/ngboost)      - **Online Agnostic Boosting via Regret Minimization (NeurIPS 2020)**    - Nataly Brukhim, Xinyi Chen, Elad Hazan, Shay Moran    - [[Paper]](https://arxiv.org/abs/2003.01150)      - **Boosting First-Order Methods by Shifting Objective: New Schemes with Faster Worst Case Rates (NeurIPS 2020)**    - Kaiwen Zhou, Anthony Man-Cho So, James Cheng    - [[Paper]](https://arxiv.org/abs/2005.12061)    - **Optimization and Generalization Analysis of Transduction through Gradient Boosting and Application to Multi-scale Graph Neural Networks (NeurIPS 2020)**    - Kenta Oono, Taiji Suzuki    - [[Paper]](https://arxiv.org/abs/2006.08550)    - [[Code]](https://github.com/delta2323/GB-GNN)      - **Gradient Boosted Normalizing Flows (NeurIPS 2020)**    - Robert Giaquinto, Arindam Banerjee    - [[Paper]](https://arxiv.org/abs/2002.11896)    - [[Code]](https://github.com/robert-giaquinto/gradient-boosted-normalizing-flows)    - **HyperML: A Boosting Metric Learning Approach in Hyperbolic Space for Recommender Systems (WSDM 2020)**    - Lucas Vinh Tran, Yi Tay, Shuai Zhang, Gao Cong, Xiaoli Li    - [[Paper]](https://arxiv.org/abs/1809.01703)    ## 2019    - **Induction of Non-Monotonic Logic Programs to Explain Boosted Tree Models Using LIME (AAAI 2019)**    - Farhad Shakerin, Gopal Gupta    - [[Paper]](https://arxiv.org/abs/1808.00629)    - **Verifying Robustness of Gradient Boosted Models (AAAI 2019)**    - Gil Einziger, Maayan Goldstein, Yaniv Sa'ar, Itai Segall    - [[Paper]](https://arxiv.org/pdf/1906.10991.pdf)    - **Online Multiclass Boosting with Bandit Feedback (AISTATS 2019)**    - Daniel T. Zhang, Young Hun Jung, Ambuj Tewari    - [[Paper]](https://arxiv.org/abs/1810.05290)      - **AdaFair: Cumulative Fairness Adaptive Boosting (CIKM 2019)**    - Vasileios Iosifidis, Eirini Ntoutsi    - [[Paper]](https://arxiv.org/abs/1909.08982)    - **Interpretable MTL from Heterogeneous Domains using Boosted Tree (CIKM 2019)**    - Ya-Lin Zhang, Longfei Li    - [[Paper]](https://dl.acm.org/citation.cfm?id=3357384.3358072)    - **Adversarial Training of Gradient-Boosted Decision Trees (CIKM 2019)**    - Stefano Calzavara, Claudio Lucchese, Gabriele Tolomei    - [[Paper]](https://www.dais.unive.it/~calzavara/papers/cikm19.pdf)      - **Fair Adversarial Gradient Tree Boosting (ICDM 2019)**    - Vincent Grari, Boris Ruf, Sylvain Lamprier, Marcin Detyniecki    - [[Paper]](https://arxiv.org/abs/1911.05369)    - **Boosted Density Estimation Remastered (ICML 2019)**    - Zac Cranko, Richard Nock    - [[Paper]](https://arxiv.org/abs/1803.08178)    - **Lossless or Quantized Boosting with Integer Arithmetic (ICML 2019)**    - Richard Nock, Robert C. Williamson    - [[Paper]](http://proceedings.mlr.press/v97/nock19a.html)    - **Optimal Minimal Margin Maximization with Boosting (ICML 2019)**    - Alexander Mathiasen, Kasper Green Larsen, Allan GrÃ¸nlund    - [[Paper]](https://arxiv.org/abs/1901.10789)    - **Katalyst: Boosting Convex Katayusha for Non-Convex Problems with a Large Condition Number (ICML 2019)**    - Zaiyi Chen, Yi Xu, Haoyuan Hu, Tianbao Yang    - [[Paper]](https://arxiv.org/abs/1809.06754)      - **Boosting for Comparison-Based Learning (IJCAI 2019)**    - MichaÃ«l Perrot, Ulrike von Luxburg    - [[Paper]](https://arxiv.org/abs/1810.13333)    - **AugBoost: Gradient Boosting Enhanced with Step-Wise Feature Augmentation (IJCAI 2019)**    - Philip Tannor, Lior Rokach    - [[Paper]](https://www.ijcai.org/proceedings/2019/0493.pdf)    - **Gradient Boosting with Piece-Wise Linear Regression Trees (IJCAI 2019)**    - Yu Shi, Jian Li, Zhize Li    - [[Paper]](https://arxiv.org/abs/1802.05640)    - [[Code]](https://github.com/GBDT-PL/GBDT-PL)      - **SpiderBoost and Momentum: Faster Variance Reduction Algorithms (NeurIPS 2019)**    - Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, Vahid Tarokh    - [[Paper]](http://papers.nips.cc/paper/8511-spiderboost-and-momentum-faster-variance-reduction-algorithms)    - **Faster Boosting with Smaller Memory (NeurIPS 2019)**    - Julaiti Alafate, Yoav Freund    - [[Paper]](https://arxiv.org/abs/1901.09047)    - **Regularized Gradient Boosting (NeurIPS 2019)**    - Corinna Cortes, Mehryar Mohri, Dmitry Storcheus    - [[Paper]](https://papers.nips.cc/paper/8784-regularized-gradient-boosting)    - **Margin-Based Generalization Lower Bounds for Boosted Classifiers (NeurIPS 2019)**    - Allan GrÃ¸nlund, Lior Kamma, Kasper Green Larsen, Alexander Mathiasen, Jelani Nelson    - [[Paper]](https://arxiv.org/abs/1909.12518)    - **Minimal Variance Sampling in Stochastic Gradient Boosting (NeurIPS 2019)**    - Bulat Ibragimov, Gleb Gusev    - [[Paper]](https://papers.nips.cc/paper/9645-minimal-variance-sampling-in-stochastic-gradient-boosting)    - **Universal Boosting Variational Inference (NeurIPS 2019)**    - Trevor Campbell, Xinglong Li    - [[Paper]](https://arxiv.org/abs/1906.01235)      - **Provably Robust Boosted Decision Stumps and Trees against Adversarial Attacks (NeurIPS 2019)**    - Maksym Andriushchenko, Matthias Hein    - [[Paper]](https://arxiv.org/abs/1906.03526)    - [[Code]](https://github.com/max-andr/provably-robust-boosting)      - **Block-distributed Gradient Boosted Trees (SIGIR 2019)**    - Theodore Vasiloudis, Hyunsu Cho, Henrik BostrÃ¶m    - [[Paper]](https://arxiv.org/abs/1904.10522)      - **Learning to Rank in Theory and Practice: From Gradient Boosting to Neural Networks and Unbiased Learning (SIGIR 2019)**    - Claudio Lucchese, Franco Maria Nardini, Rama Kumar Pasumarthi, Sebastian Bruch, Michael Bendersky, Xuanhui Wang, Harrie Oosterhuis, Rolf Jagerman, Maarten de Rijke    - [[Paper]](https://www.researchgate.net/publication/334579610_Learning_to_Rank_in_Theory_and_Practice_From_Gradient_Boosting_to_Neural_Networks_and_Unbiased_Learning)    ## 2018  - **Boosted Generative Models (AAAI 2018)**    - Aditya Grover, Stefano Ermon    - [[Paper]](https://arxiv.org/pdf/1702.08484.pdf)    - [[Code]](https://github.com/ermongroup/bgm)    - **Boosting Variational Inference: an Optimization Perspective (AISTATS 2018)**    - Francesco Locatello, Rajiv Khanna, Joydeep Ghosh, Gunnar RÃ¤tsch    - [[Paper]](https://arxiv.org/abs/1708.01733)    - [[Code]](https://github.com/ratschlab/boosting-bbvi)    - **Online Boosting Algorithms for Multi-label Ranking (AISTATS 2018)**    - Young Hun Jung, Ambuj Tewari    - [[Paper]](https://arxiv.org/abs/1710.08079)    - [[Code]](https://github.com/yhjung88/OnlineMLRBoostingWithVFDT)    - **DualBoost: Handling Missing Values with Feature Weights and Weak Classifiers that Abstain (CIKM 2018)**    - Weihong Wang, Jie Xu, Yang Wang, Chen Cai, Fang Chen    - [[Paper]](http://delivery.acm.org/10.1145/3270000/3269319/p1543-wang.pdf?ip=129.215.164.203&id=3269319&acc=ACTIVE%20SERVICE&key=C2D842D97AC95F7A%2EEB9E991028F4E1F1%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1558633895_f01b39fd47b943fd01eade763a397e04)    - **Functional Gradient Boosting based on Residual Network Perception (ICML 2018)**    - Atsushi Nitanda, Taiji Suzuki    - [[Paper]](https://arxiv.org/abs/1802.09031)    - [[Code]](https://github.com/anitan0925/ResFGB)    - **Finding Influential Training Samples for Gradient Boosted Decision Trees (ICML 2018)**    - Boris Sharchilev, Yury Ustinovskiy, Pavel Serdyukov, Maarten de Rijke    - [[Paper]](https://arxiv.org/abs/1802.06640)    - **Learning Deep ResNet Blocks Sequentially using Boosting Theory (ICML 2018)**    - Furong Huang, Jordan T. Ash, John Langford, Robert E. Schapire    - [[Paper]](https://arxiv.org/abs/1706.04964)    - [[Code]](https://github.com/JordanAsh/boostresnet)    - **UCBoost: A Boosting Approach to Tame Complexity and Optimality for Stochastic Bandits (IJCAI 2018)**    - Fang Liu, Sinong Wang, Swapna Buccapatnam, Ness B. Shroff    - [[Paper]](https://www.ijcai.org/proceedings/2018/0338.pdf)    - [[Code]](https://smpybandits.github.io/docs/Policies.UCBoost.html)    - **Adaboost with Auto-Evaluation for Conversational Models (IJCAI 2018)**    - Juncen Li, Ping Luo, Ganbin Zhou, Fen Lin, Cheng Niu    - [[Paper]](https://www.ijcai.org/proceedings/2018/0580.pdf)    - **Ensemble Neural Relation Extraction with Adaptive Boosting (IJCAI 2018)**    - Dongdong Yang, Senzhang Wang, Zhoujun Li    - [[Paper]](https://www.ijcai.org/proceedings/2018/0630.pdf)    - **CatBoost: Unbiased Boosting with Categorical Features (NIPS 2018)**    - Liudmila Ostroumova Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, Andrey Gulin    - [[Paper]](https://papers.nips.cc/paper/7898-catboost-unbiased-boosting-with-categorical-features.pdf)    - [[Code]](https://github.com/catboost/catboost)    - **Multitask Boosting for Survival Analysis with Competing Risks (NIPS 2018)**    - Alexis Bellot, Mihaela van der Schaar    - [[Paper]](https://papers.nips.cc/paper/7413-multitask-boosting-for-survival-analysis-with-competing-risks)    - **Multi-Layered Gradient Boosting Decision Trees (NIPS 2018)**    - Ji Feng, Yang Yu, Zhi-Hua Zhou    - [[Paper]](https://papers.nips.cc/paper/7614-multi-layered-gradient-boosting-decision-trees.pdf)    - [[Code]](https://github.com/kingfengji/mGBDT)    - **Boosted Sparse and Low-Rank Tensor Regression (NIPS 2018)**    - Lifang He, Kun Chen, Wanwan Xu, Jiayu Zhou, Fei Wang    - [[Paper]](https://arxiv.org/abs/1811.01158)    - [[Code]](https://github.com/LifangHe/NeurIPS18_SURF)      - **Selective Gradient Boosting for Effective Learning to Rank (SIGIR 2018)**    - Claudio Lucchese, Franco Maria Nardini, Raffaele Perego, Salvatore Orlando, Salvatore Trani    - [[Paper]](http://quickrank.isti.cnr.it/selective-data/selective-SIGIR2018.pdf)    - [[Code]](https://github.com/hpclab/quickrank/blob/master/documentation/selective.md)    ## 2017  - **Boosting for Real-Time Multivariate Time Series Classification (AAAI 2017)**    - Haishuai Wang, Jun Wu    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14852/14241)    - **Cross-Domain Sentiment Classification via Topic-Related TrAdaBoost (AAAI 2017)**    - Xingchang Huang, Yanghui Rao, Haoran Xie, Tak-Lam Wong, Fu Lee Wang    - [[Paper]](https://pdfs.semanticscholar.org/826c/c83d98a5c4c7dcc02be1f4dd9c27e2b99670.pdf)    - [[Code]](https://github.com/xchhuang/cross-domain-sentiment-classification)    - **Extreme Gradient Boosting and Behavioral Biometrics (AAAI 2017)**    - Benjamin Manning    - [[Paper]](https://pdfs.semanticscholar.org/8c6e/6c887d6d47dda3f0c73297fd4da516fef1ee.pdf)    - **FeaBoost: Joint Feature and Label Refinement for Semantic Segmentation (AAAI 2017)**    - Yulei Niu, Zhiwu Lu, Songfang Huang, Xin Gao, Ji-Rong Wen    - [[Paper]](https://pdfs.semanticscholar.org/d566/73be998b3ed38ccbb53551e38758ae8cfc9d.pdf)    - **Boosting Complementary Hash Tables for Fast Nearest Neighbor Search (AAAI 2017)**    - Xianglong Liu, Cheng Deng, Yadong Mu, Zhujin Li    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14336)    - **Gradient Boosting on Stochastic Data Streams (AISTATS 2017)**    - Hanzhang Hu, Wen Sun, Arun Venkatraman, Martial Hebert, J. Andrew Bagnell    - [[Paper]](https://arxiv.org/abs/1703.00377)    - **BoostVHT: Boosting Distributed Streaming Decision Trees (CIKM 2017)**    - Theodore Vasiloudis, Foteini Beligianni, Gianmarco De Francisci Morales    - [[Paper]](https://melmeric.files.wordpress.com/2010/05/boostvht-boosting-distributed-streaming-decision-trees.pdf)    - **Fast Boosting Based Detection Using Scale Invariant Multimodal Multiresolution Filtered Features (CVPR 2017)**    - Arthur Daniel Costea, Robert Varga, Sergiu Nedevschi    - [[Paper]](http://openaccess.thecvf.com/content_cvpr_2017/papers/Costea_Fast_Boosting_Based_CVPR_2017_paper.pdf)    - **BIER - Boosting Independent Embeddings Robustly (ICCV 2017)**    - Michael Opitz, Georg Waltner, Horst Possegger, Horst Bischof    - [[Paper]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Opitz_BIER_-_Boosting_ICCV_2017_paper.pdf)    - [[Code]](https://github.com/mop/bier)    - **An Analysis of Boosted Linear Classifiers on Noisy Data with Applications to Multiple-Instance Learning (ICDM 2017)**    - Rui Liu, Soumya Ray    - [[Paper]](https://ieeexplore.ieee.org/document/8215501)    - **Variational Boosting: Iteratively Refining Posterior Approximations (ICML 2017)**    - Andrew C. Miller, Nicholas J. Foti, Ryan P. Adams    - [[Paper]](https://arxiv.org/abs/1611.06585)    - [[Code]](https://github.com/andymiller/vboost)    - **Boosted Fitted Q-Iteration (ICML 2017)**    - Samuele Tosatto, Matteo Pirotta, Carlo D'Eramo, Marcello Restelli    - [[Paper]](http://proceedings.mlr.press/v70/tosatto17a.html)    - **A Simple Multi-Class Boosting Framework with Theoretical Guarantees and Empirical Proficiency (ICML 2017)**    - Ron Appel, Pietro Perona    - [[Paper]](http://proceedings.mlr.press/v70/appel17a.html)    - [[Code]](https://github.com/GuillaumeCollin/A-Simple-Multi-Class-Boosting-Framework-with-Theoretical-Guarantees-and-Empirical-Proficiency)    - **Gradient Boosted Decision Trees for High Dimensional Sparse Output (ICML 2017)**    - Si Si, Huan Zhang, S. Sathiya Keerthi, Dhruv Mahajan, Inderjit S. Dhillon, Cho-Jui Hsieh    - [[Paper]](http://proceedings.mlr.press/v70/si17a.html)    - [[Code]](https://github.com/springdaisy/GBDT)    - **Local Topic Discovery via Boosted Ensemble of Nonnegative Matrix Factorization (IJCAI 2017)**    - Sangho Suh, Jaegul Choo, Joonseok Lee, Chandan K. Reddy    - [[Paper]](http://dmkd.cs.vt.edu/papers/IJCAI17.pdf)    - [[Code]](https://github.com/benedekrozemberczki/BoostedFactorization)    - **Boosted Zero-Shot Learning with Semantic Correlation Regularization (IJCAI 2017)**    - Te Pi, Xi Li, Zhongfei (Mark) Zhang    - [[Paper]](https://arxiv.org/abs/1707.08008)    - **BDT: Gradient Boosted Decision Tables for High Accuracy and Scoring Efficiency (KDD 2017)**    - Yin Lou, Mikhail Obukhov    - [[Paper]](https://yinlou.github.io/papers/lou-kdd17.pdf)      - **CatBoost: Gradient Boosting with Categorical Features Support (NIPS 2017)**    - Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin    - [[Paper]](https://arxiv.org/abs/1810.11363)    - [[Code]](https://catboost.ai/)    - **Cost Efficient Gradient Boosting (NIPS 2017)**    - Sven Peter, Ferran Diego, Fred A. Hamprecht, Boaz Nadler    - [[Paper]](https://papers.nips.cc/paper/6753-cost-efficient-gradient-boosting)    - [[Code]](https://github.com/svenpeter42/LightGBM-CEGB)    - **AdaGAN: Boosting Generative Models (NIPS 2017)**    - Ilya O. Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, Bernhard SchÃ¶lkopf    - [[Paper]](https://arxiv.org/abs/1701.02386)    - [[Code]](https://github.com/tolstikhin/adagan)    - **LightGBM: A Highly Efficient Gradient Boosting Decision Tree (NIPS 2017)**    - Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu    - [[Paper]](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)    - [[Code]](https://lightgbm.readthedocs.io/en/latest/)    - **Early Stopping for Kernel Boosting Algorithms: A General Analysis with Localized Complexities (NIPS 2017)**    - Yuting Wei, Fanny Yang, Martin J. Wainwright    - [[Paper]](https://arxiv.org/abs/1707.01543)    - [[Code]](https://github.com/fanny-yang/EarlyStoppingRKHS)    - **Online Multiclass Boosting (NIPS 2017)**    - Young Hun Jung, Jack Goetz, Ambuj Tewari    - [[Paper]](https://papers.nips.cc/paper/6693-online-multiclass-boosting.pdf)    - **Stacking Bagged and Boosted Forests for Effective Automated Classification (SIGIR 2017)**    - Raphael R. Campos, SÃ©rgio D. Canuto, Thiago Salles, Clebson C. A. de SÃ¡, Marcos AndrÃ© GonÃ§alves    - [[Paper]](https://homepages.dcc.ufmg.br/~rcampos/papers/sigir2017/appendix.pdf)    - [[Code]](https://github.com/raphaelcampos/stacking-bagged-boosted-forests)    - **GB-CENT: Gradient Boosted Categorical Embedding and Numerical Trees (WWW 2017)**    - Qian Zhao, Yue Shi, Liangjie Hong    - [[Paper]](http://papers.www2017.com.au.s3-website-ap-southeast-2.amazonaws.com/proceedings/p1311.pdf)    - [[Code]](https://github.com/grouplens/samantha)    ## 2016  - **Group Cost-Sensitive Boosting for Multi-Resolution Pedestrian Detection (AAAI 2016)**    - Chao Zhu, Yuxin Peng    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewFile/11898/12146)    - [[Code]](https://github.com/nnikolaou/Cost-sensitive-Boosting-Tutorial)    - **Communication Efficient Distributed Agnostic Boosting (AISTATS 2016)**    - Shang-Tse Chen, Maria-Florina Balcan, Duen Horng Chau    - [[Paper]](https://arxiv.org/abs/1506.06318)    - **Logistic Boosting Regression for Label Distribution Learning (CVPR 2016)**    - Chao Xing, Xin Geng, Hui Xue    - [[Paper]](https://zpascal.net/cvpr2016/Xing_Logistic_Boosting_Regression_CVPR_2016_paper.pdf)    - **Structured Regression Gradient Boosting (CVPR 2016)**    - Ferran Diego, Fred A. Hamprecht    - [[Paper]](https://hci.iwr.uni-heidelberg.de/sites/default/files/publications/files/1037872734/diego_16_structured.pdf)      - **L-EnsNMF: Boosted Local Topic Discovery via Ensemble of Nonnegative Matrix Factorization (ICDM 2016)**    - Sangho Suh, Jaegul Choo, Joonseok Lee, Chandan K. Reddy    - [[Paper]](https://ieeexplore.ieee.org/document/7837872)    - [[Code]](https://github.com/benedekrozemberczki/BoostedFactorization)    - **Meta-Gradient Boosted Decision Tree Model for Weight and Target Learning (ICML 2016)**    - Yury Ustinovskiy, Valentina Fedorova, Gleb Gusev, Pavel Serdyukov    - [[Paper]](http://proceedings.mlr.press/v48/ustinovskiy16.html)    - **Generalized Dictionary for Multitask Learning with Boosting (IJCAI 2016)**    - Boyu Wang, Joelle Pineau    - [[Paper]](https://www.ijcai.org/Proceedings/16/Papers/299.pdf)    - **Self-Paced Boost Learning for Classification (IJCAI 2016)**    - Te Pi, Xi Li, Zhongfei Zhang, Deyu Meng, Fei Wu, Jun Xiao, Yueting Zhuang    - [[Paper]](https://pdfs.semanticscholar.org/31b6/ab4a0771d5b7405cacdd12c398b1c832729d.pdf)    - **Interactive Martingale Boosting (IJCAI 2016)**    - Ashish Kulkarni, Pushpak Burange, Ganesh Ramakrishnan    - [[Paper]](https://www.ijcai.org/Proceedings/16/Papers/124.pdf)    - **Optimal and Adaptive Algorithms for Online Boosting (IJCAI 2016)**    - Alina Beygelzimer, Satyen Kale, Haipeng Luo    - [[Paper]](https://www.ijcai.org/Proceedings/16/Papers/614.pdf)    - [[Code]](https://github.com/VowpalWabbit/vowpal_wabbit/blob/master/vowpalwabbit/boosting.cc)    - **Rating-Boosted Latent Topics: Understanding Users and Items with Ratings and Reviews (IJCAI 2016)**    - Yunzhi Tan, Min Zhang, Yiqun Liu, Shaoping Ma    - [[Paper]](https://pdfs.semanticscholar.org/db63/89e0ca49ec0e4686e40604e7489cb4c0729d.pdf)    - **XGBoost: A Scalable Tree Boosting System (KDD 2016)**    - Tianqi Chen, Carlos Guestrin    - [[Paper]](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)    - [[Code]](https://github.com/dmlc/xgboost)    - **Boosted Decision Tree Regression Adjustment for Variance Reduction in Online Controlled Experiments (KDD 2016)**    - Alexey Poyarkov, Alexey Drutsa, Andrey Khalyavin, Gleb Gusev, Pavel Serdyukov    - [[Paper]](https://www.kdd.org/kdd2016/papers/files/adf0653-poyarkovA.pdf)    - **Boosting with Abstention (NIPS 2016)**    - Corinna Cortes, Giulia DeSalvo, Mehryar Mohri    - [[Paper]](https://papers.nips.cc/paper/6336-boosting-with-abstention)    - **SEBOOST - Boosting Stochastic Learning Using Subspace Optimization Techniques (NIPS 2016)**    - Elad Richardson, Rom Herskovitz, Boris Ginsburg, Michael Zibulevsky    - [[Paper]](https://papers.nips.cc/paper/6109-seboost-boosting-stochastic-learning-using-subspace-optimization-techniques.pdf)    - [[Code]](https://github.com/eladrich/seboost)    - **Incremental Boosting Convolutional Neural Network for Facial Action Unit Recognition (NIPS 2016)**    - Shizhong Han, Zibo Meng, Ahmed-Shehab Khan, Yan Tong    - [[Paper]](https://arxiv.org/abs/1707.05395)    - [[Code]](https://github.com/sjsingh91/IB-CNN)      - **Generalized BROOF-L2R: A General Framework for Learning to Rank Based on Boosting and Random Forests (SIGIR 2016)**    - Clebson C. A. de SÃ¡, Marcos AndrÃ© GonÃ§alves, Daniel Xavier de Sousa, Thiago Salles    - [[Paper]](https://dl.acm.org/citation.cfm?id=2911540)    ## 2015    - **Online Boosting Algorithms for Anytime Transfer and Multitask Learning (AAAI 2015)**    - Boyu Wang, Joelle Pineau    - [[Paper]](https://www.cs.mcgill.ca/~jpineau/files/bwang-aaai15.pdf)    - **A Boosted Multi-Task Model for Pedestrian Detection with Occlusion Handling (AAAI 2015)**    - Chao Zhu, Yuxin Peng    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/viewFile/9879/9825)    - **Efficient Second-Order Gradient Boosting for Conditional Random Fields (AISTATS 2015)**    - Tianqi Chen, Sameer Singh, Ben Taskar, Carlos Guestrin    - [[Paper]](http://proceedings.mlr.press/v38/chen15b.html)    - **Tumblr Blog Recommendation with Boosted Inductive Matrix Completion (CIKM 2015)**    - Donghyuk Shin, Suleyman Cetintas, Kuang-Chih Lee, Inderjit S. Dhillon    - [[Paper]](https://dl.acm.org/citation.cfm?id=2806578)    - **Basis mapping based boosting for object detection (CVPR 2015)**    - Haoyu Ren, Ze-Nian Li    - [[Paper]](https://ieeexplore.ieee.org/document/7298766)    - **Tracking-by-Segmentation with Online Gradient Boosting Decision Tree (ICCV 2015)**    - Jeany Son, Ilchae Jung, Kayoung Park, Bohyung Han    - [[Paper]](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Son_Tracking-by-Segmentation_With_Online_ICCV_2015_paper.pdf)    - [[Code]](http://cvlab.postech.ac.kr/research/ogbdt_track/)    - **Learning to Boost Filamentary Structure Segmentation (ICCV 2015)**    - Lin Gu, Li Cheng    - [[Paper]](https://isg.nist.gov/BII_2015/webPages/pages/2015_BII_program/PDFs/Day_3/Session_9/Abstract_Gu_Lin.pdf)    - **Optimal and Adaptive Algorithms for Online Boosting (ICML 2015)**    - Alina Beygelzimer, Satyen Kale, Haipeng Luo    - [[Paper]](http://proceedings.mlr.press/v37/beygelzimer15.pdf)    - [[Code]](https://github.com/VowpalWabbit/vowpal_wabbit/blob/master/vowpalwabbit/boosting.cc)    - **Rademacher Observations, Private Data, and Boosting (ICML 2015)**    - Richard Nock, Giorgio Patrini, Arik Friedman    - [[Paper]](https://arxiv.org/abs/1502.02322)    - **Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions (ICML 2015)**    - Taehoon Lee, Sungroh Yoon    - [[Paper]](https://pdfs.semanticscholar.org/d0ad/beef3053e98dd88ff74f42744417bc65a729.pdf)    - **A Direct Boosting Approach for Semi-supervised Classification (IJCAI 2015)**    - Shaodan Zhai, Tian Xia, Zhongliang Li, Shaojun Wang    - [[Paper]](https://www.ijcai.org/Proceedings/15/Papers/565.pdf)    - **A Boosting Algorithm for Item Recommendation with Implicit Feedback (IJCAI 2015)**    - Yong Liu, Peilin Zhao, Aixin Sun, Chunyan Miao    - [[Paper]](https://www.ijcai.org/Proceedings/15/Papers/255.pdf)    - [[Code]](https://github.com/microsoft/recommenders)    - **Training-Time Optimization of a Budgeted Booster (IJCAI 2015)**    - Yi Huang, Brian Powers, Lev Reyzin    - [[Paper]](https://www.ijcai.org/Proceedings/15/Papers/504.pdf)    - **Optimal Action Extraction for Random Forests and Boosted Trees (KDD 2015)**    - Zhicheng Cui, Wenlin Chen, Yujie He, Yixin Chen    - [[Paper]](https://www.cse.wustl.edu/~ychen/public/OAE.pdf)    - **Online Gradient Boosting (NIPS 2015)**    - Alina Beygelzimer, Elad Hazan, Satyen Kale, Haipeng Luo    - [[Paper]](https://arxiv.org/abs/1506.04820)    - [[Code]](https://github.com/crm416/online_boosting)      - **BROOF: Exploiting Out-of-Bag Errors Boosting and Random Forests for Effective Automated Classification (SIGIR 2015)**    - Thiago Salles, Marcos AndrÃ© GonÃ§alves, Victor Rodrigues, Leonardo C. da Rocha    - [[Paper]](https://homepages.dcc.ufmg.br/~tsalles/broof/appendix.pdf)    - **Boosting Search with Deep Understanding of Contents and Users (WSDM 2015)**    - Kaihua Zhu    - [[Paper]](https://www.researchgate.net/publication/282482189_Boosting_Search_with_Deep_Understanding_of_Contents_and_Users)    ## 2014  - **On Boosting Sparse Parities (AAAI 2014)**    - Lev Reyzin    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8587)    - **Joint Coupled-Feature Representation and Coupled Boosting for AD Diagnosis (CVPR 2014)**    - Yinghuan Shi, Heung-Il Suk, Yang Gao, Dinggang Shen    - [[Paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Shi_Joint_Coupled-Feature_Representation_2014_CVPR_paper.pdf)    - **From Categories to Individuals in Real Time - A Unified Boosting Approach (CVPR 2014)**    - David Hall, Pietro Perona    - [[Paper]](https://ieeexplore.ieee.org/document/6909424)    - [[Code]](http://www.vision.caltech.edu/~dhall/projects/CategoriesToIndividuals/)    - **Efficient Boosted Exemplar-Based Face Detection (CVPR 2014)**    - Haoxiang Li, Zhe Lin, Jonathan Brandt, Xiaohui Shen, Gang Hua    - [[Paper]](http://users.eecs.northwestern.edu/~xsh835/assets/cvpr14_exemplarfacedetection.pdf)    - **Facial Expression Recognition via a Boosted Deep Belief Network (CVPR 2014)**    - Ping Liu, Shizhong Han, Zibo Meng, Yan Tong    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6909629)    - **Confidence-Rated Multiple Instance Boosting for Object Detection (CVPR 2014)**    - Karim Ali, Kate Saenko    - [[Paper]](https://ieeexplore.ieee.org/document/6909708)    - **The Return of AdaBoost.MH: Multi-Class Hamming Trees (ICLR 2014)**    - BalÃ¡zs KÃ©gl    - [[Paper]](https://arxiv.org/pdf/1312.6086.pdf)    - [[Code]](https://github.com/aciditeam/acidano/blob/master/acidano/utils/cost.py)    - **Deep Boosting (ICML 2014)**    - Corinna Cortes, Mehryar Mohri, Umar Syed    - [[Paper]](http://proceedings.mlr.press/v32/cortesb14.pdf)    - [[Code]](https://github.com/google/deepboost)    - **A Convergence Rate Analysis for LogitBoost, MART and Their Variant (ICML 2014)**    - Peng Sun, Tong Zhang, Jie Zhou    - [[Paper]](http://proceedings.mlr.press/v32/sunc14.pdf)    - **Boosting with Online Binary Learners for the Multiclass Bandit Problem (ICML 2014)**    - Shang-Tse Chen, Hsuan-Tien Lin, Chi-Jen Lu    - [[Paper]](https://www.cc.gatech.edu/~schen351/paper/icml14boost.pdf)    - **Boosting Multi-Step Autoregressive Forecasts (ICML 2014)**    - Souhaib Ben Taieb, Rob J. Hyndman    - [[Paper]](http://proceedings.mlr.press/v32/taieb14.pdf)    - **Dynamic Programming Boosting for Discriminative Macro-Action Discovery (ICML 2014)**    - Leonidas Lefakis, FranÃ§ois Fleuret    - [[Paper]](http://proceedings.mlr.press/v32/lefakis14.html)    - **Guess-Averse Loss Functions For Cost-Sensitive Multiclass Boosting (ICML 2014)**    - Oscar Beijbom, Mohammad J. Saberian, David J. Kriegman, Nuno Vasconcelos    - [[Paper]](http://proceedings.mlr.press/v32/beijbom14.pdf)    - **A Multi-Class Boosting Method with Direct Optimization (KDD 2014)**    - Shaodan Zhai, Tian Xia, Shaojun Wang    - [[Paper]](https://dl.acm.org/citation.cfm?id=2623689)    - **Gradient Boosted Feature Selection (KDD 2014)**    - Zhixiang Eddie Xu, Gao Huang, Kilian Q. Weinberger, Alice X. Zheng    - [[Paper]](https://arxiv.org/abs/1901.04055)    - [[Code]](https://github.com/dmlc/xgboost)    - **Multi-Class Deep Boosting (NIPS 2014)**    - Vitaly Kuznetsov, Mehryar Mohri, Umar Syed    - [[Paper]](https://papers.nips.cc/paper/5514-multi-class-deep-boosting)    - **Deconvolution of High Dimensional Mixtures via Boosting with Application to Diffusion-Weighted MRI of Human Brain (NIPS 2014)**    - Charles Y. Zheng, Franco Pestilli, Ariel Rokem    - [[Paper]](https://papers.nips.cc/paper/5506-deconvolution-of-high-dimensional-mixtures-via-boosting-with-application-to-diffusion-weighted-mri-of-human-brain)    - **A Drifting-Games Analysis for Online Learning and Applications to Boosting (NIPS 2014)**    - Haipeng Luo, Robert E. Schapire    - [[Paper]](https://arxiv.org/abs/1406.1856)    - **A Boosting Framework on Grounds of Online Learning (NIPS 2014)**    - Tofigh Naghibi Mohamadpoor, Beat Pfister    - [[Paper]](https://papers.nips.cc/paper/5512-a-boosting-framework-on-grounds-of-online-learning.pdf)      - **Gradient Boosting Factorization Machines (RECSYS 2014)**    - Chen Cheng, Fen Xia, Tong Zhang, Irwin King, Michael R. Lyu    - [[Paper]](http://tongzhang-ml.org/papers/recsys14-fm.pdf)    ## 2013    - **Boosting Binary Keypoint Descriptors (CVPR 2013)**    - Tomasz Trzcinski, C. Mario Christoudias, Pascal Fua, Vincent Lepetit    - [[Paper]](https://cvlab.epfl.ch/research/page-90554-en-html/research-detect-binboost/)    - [[Code]](https://github.com/biotrump/cvlab-BINBOOST)    - **PerturBoost: Practical Confidential Classifier Learning in the Cloud (ICDM 2013)**    - Keke Chen, Shumin Guo    - [[Paper]](https://ieeexplore.ieee.org/document/6729587)    - **Multiclass Semi-Supervised Boosting Using Similarity Learning (ICDM 2013)**    - Jafar Tanha, Mohammad Javad Saberian, Maarten van Someren    - [[Paper]](https://www.cse.msu.edu/~rongjin/publications/MultiClass-08.pdf)    - **Saving Evaluation Time for the Decision Function in Boosting: Representation and Reordering Base Learner (ICML 2013)**    - Peng Sun, Jie Zhou    - [[Paper]](http://proceedings.mlr.press/v28/sun13.pdf)    - **General Functional Matrix Factorization Using Gradient Boosting (ICML 2013)**    - Tianqi Chen, Hang Li, Qiang Yang, Yong Yu    - [[Paper]](http://w.hangli-hl.com/uploads/3/1/6/8/3168008/icml_2013.pdf)    - **Margins, Shrinkage, and Boosting (ICML 2013)**    - Matus Telgarsky    - [[Paper]](https://arxiv.org/abs/1303.4172)    - **Quickly Boosting Decision Trees - Pruning Underachieving Features Early (ICML 2013)**    - Ron Appel, Thomas J. Fuchs, Piotr DollÃ¡r, Pietro Perona    - [[Paper]](http://proceedings.mlr.press/v28/appel13.pdf)    - [[Code]](https://github.com/pdollar/toolbox/blob/master/classify/adaBoostTrain.m)    - **Human Boosting (ICML 2013)**    - Harsh H. Pareek, Pradeep Ravikumar    - [[Paper]](https://www.cs.cmu.edu/~pradeepr/paperz/humanboosting.pdf)    - **Collaborative Boosting for Activity Classification in Microblogs (KDD 2013)**    - Yangqiu Song, Zhengdong Lu, Cane Wing-ki Leung, Qiang Yang    - [[Paper]](http://chbrown.github.io/kdd-2013-usb/kdd/p482.pdf)    - **Direct 0-1 Loss Minimization and Margin Maximization with Boosting (NIPS 2013)**    - Shaodan Zhai, Tian Xia, Ming Tan, Shaojun Wang    - [[Paper]](https://papers.nips.cc/paper/5214-direct-0-1-loss-minimization-and-margin-maximization-with-boosting)    - **Reservoir Boosting : Between Online and Offline Ensemble Learning (NIPS 2013)**    - Leonidas Lefakis, FranÃ§ois Fleuret    - [[Paper]](https://papers.nips.cc/paper/5215-reservoir-boosting-between-online-and-offline-ensemble-learning)    - **Non-Linear Domain Adaptation with Boosting (NIPS 2013)**    - Carlos J. Becker, C. Mario Christoudias, Pascal Fua    - [[Paper]](https://papers.nips.cc/paper/5200-non-linear-domain-adaptation-with-boosting)    - **Boosting in the Presence of Label Noise (UAI 2013)**    - Jakramate Bootkrajang, Ata KabÃ¡n    - [[Paper]](https://arxiv.org/abs/1309.6818)    ## 2012  - **Contextual Boost for Pedestrian Detection (CVPR 2012)**    - Yuanyuan Ding, Jing Xiao    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.308.5611&rep=rep1&type=pdf)    - **Shrink Boost for Selecting Multi-LBP Histogram Features in Object Detection (CVPR 2012)**    - Cher Keng Heng, Sumio Yokomitsu, Yuichi Matsumoto, Hajime Tamura    - [[Paper]](https://ieeexplore.ieee.org/document/6248061)    - **Boosting Bottom-Up and Top-Down Visual Features for Saliency Estimation (CVPR 2012)**    - Ali Borji    - [[Paper]](http://ilab.usc.edu/borji/papers/cvpr-2012-BUModel-v4.pdf)    - **Boosting Algorithms for Simultaneous Feature Extraction and Selection (CVPR 2012)**    - Mohammad J. Saberian, Nuno Vasconcelos    - [[Paper]](http://svcl.ucsd.edu/publications/conference/2012/cvpr/SOPBoost.pdf)    - **Sharing Features in Multi-class Boosting via Group Sparsity (CVPR 2012)**    - Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel    - [[Paper]](https://cs.adelaide.edu.au/~paulp/publications/pubs/sharing_cvpr2012.pdf)    - **Feature Weighting and Selection Using Hypothesis Margin of Boosting (ICDM 2012)**    - Malak Alshawabkeh, Javed A. Aslam, Jennifer G. Dy, David R. Kaeli    - [[Paper]](http://www.ece.neu.edu/fac-ece/jdy/papers/alshawabkeh-ICDM2012.pdf)    - **An AdaBoost Algorithm for Multiclass Semi-supervised Learning (ICDM 2012)**    - Jafar Tanha, Maarten van Someren, Hamideh Afsarmanesh    - [[Paper]]https://ieeexplore.ieee.org/document/6413799)      - **AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem (ICML 2012)**    - Peng Sun, Mark D. Reid, Jie Zhou    - [[Paper]](AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem)    - [[Code]](https://github.com/pengsun/AOSOLogitBoost)    - **An Online Boosting Algorithm with Theoretical Justifications (ICML 2012)**    - Shang-Tse Chen, Hsuan-Tien Lin, Chi-Jen Lu    - [[Paper]](https://arxiv.org/abs/1206.6422)    - **Learning Image Descriptors with the Boosting-Trick (NIPS 2012)**    - Tomasz Trzcinski, C. Mario Christoudias, Vincent Lepetit, Pascal Fua    - [[Paper]](https://papers.nips.cc/paper/4848-learning-image-descriptors-with-the-boosting-trick.pdf)    - [[Code]](https://github.com/biotrump/cvlab-BINBOOST)    - **Accelerated Training for Matrix-norm Regularization: A Boosting Approach (NIPS 2012)**    - Xinhua Zhang, Yaoliang Yu, Dale Schuurmans    - [[Paper]](https://papers.nips.cc/paper/4663-accelerated-training-for-matrix-norm-regularization-a-boosting-approach)      - **Learning from Heterogeneous Sources via Gradient Boosting Consensus (SDM 2012)**    - Xiaoxiao Shi, Jean-FranÃ§ois Paiement, David Grangier, Philip S. Yu    - [[Paper]](http://david.grangier.info/papers/2012/shi_sdm_2012.pdf)    - [[Code]](https://github.com/PriyeshV/GBDT-CC)    ## 2011  - **Selective Transfer Between Learning Tasks Using Task-Based Boosting (AAAI 2011)**    - Eric Eaton, Marie desJardins    - [[Paper]](http://www.cis.upenn.edu/~eeaton/papers/Eaton2011Selective.pdf)    - **Incorporating Boosted Regression Trees into Ecological Latent Variable Models (AAAI 2011)**    - Rebecca A. Hutchinson, Li-Ping Liu, Thomas G. Dietterich    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/viewFile/3711/4086)    - **FlowBoost - Appearance Learning from Sparsely Annotated Video (CVPR 2011)**    - Karim Ali, David Hasler, FranÃ§ois Fleuret    - [[Paper]](http://www.karimali.org/publications/AHF_CVPR11.pdf)    - **AdaBoost on Low-Rank PSD Matrices for Metric Learning (CVPR 2011)**    - Jinbo Bi, Dijia Wu, Le Lu, Meizhu Liu, Yimo Tao, Matthias Wolf    - [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5995363)    - **Boosted Local Structured HOG-LBP for Object Localization (CVPR 2011)**    - Junge Zhang, Kaiqi Huang, Yinan Yu, Tieniu Tan    - [[Paper]](http://www.cbsr.ia.ac.cn/users/ynyu/1682.pdf)    - **A Direct Formulation for Totally-Corrective Multi-Class Boosting (CVPR 2011)**    - Chunhua Shen, Zhihui Hao    - [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5995554)    - **Gated Classifiers: Boosting Under High Intra-class Variation (CVPR 2011)**    - Oscar M. Danielsson, Babak Rasolzadeh, Stefan Carlsson    - [[Paper]](http://www.nada.kth.se/cvap/cvg/papers/danielssonCVPR11.pdf)    - **TaylorBoost: First and Second-order Boosting Algorithms with Explicit Margin Control (CVPR 2011)**    - Mohammad J. Saberian, Hamed Masnadi-Shirazi, Nuno Vasconcelos    - [[Paper]](https://ieeexplore.ieee.org/document/5995605)    - [[Code]](https://pythonhosted.org/bob.learn.boosting/)    - **Robust and Efficient Regularized Boosting Using Total Bregman Divergence (CVPR 2011)**    - Meizhu Liu, Baba C. Vemuri    - [[Paper]](https://ieeexplore.ieee.org/document/5995686)    - **Treat Samples differently: Object Tracking with Semi-Supervised Online CovBoost (ICCV 2011)**    - Guorong Li, Lei Qin, Qingming Huang, Junbiao Pang, Shuqiang Jiang    - [[Paper]](https://ieeexplore.ieee.org/document/6126297)    - **LinkBoost: A Novel Cost-Sensitive Boosting Framework for Community-Level Network Link Prediction (ICDM 2011)**    - Prakash Mandayam Comar, Pang-Ning Tan, Anil K. Jain    - [[Paper]](http://www.cse.msu.edu/~ptan/papers/icdm2011.pdf)    - **Learning Markov Logic Networks via Functional Gradient Boosting (ICDM 2011)**    - Tushar Khot, Sriraam Natarajan, Kristian Kersting, Jude W. Shavlik    - [[Paper]](https://github.com/starling-lab/BoostSRL)    - [[Code]](https://ieeexplore.ieee.org/document/6137236)    - **Boosting on a Budget: Sampling for Feature-Efficient Prediction (ICML 2011)**    - Lev Reyzin    - [[Paper]](http://www.icml-2011.org/papers/348_icmlpaper.pdf)    - **Multiclass Boosting with Hinge Loss based on Output Coding (ICML 2011)**    - Tianshi Gao, Daphne Koller    - [[Paper]](http://ai.stanford.edu/~tianshig/papers/multiclassHingeBoost-ICML2011.pdf)    - [[Code]](https://github.com/memect/hao/blob/master/awesome/multiclass-boosting.md)    - **Generalized Boosting Algorithms for Convex Optimization (ICML 2011)**    - Alexander Grubb, Drew Bagnell    - [[Paper]](https://arxiv.org/pdf/1105.2054.pdf)    - **Imitation Learning in Relational Domains: A Functional-Gradient Boosting Approach (IJCAI 2011)**    - Sriraam Natarajan, Saket Joshi, Prasad Tadepalli, Kristian Kersting, Jude W. Shavlik    - [[Paper]](http://ftp.cs.wisc.edu/machine-learning/shavlik-group/natarajan.ijcai11.pdf)    - **Boosting with Maximum Adaptive Sampling (NIPS 2011)**    - Charles Dubout, FranÃ§ois Fleuret    - [[Paper]](https://papers.nips.cc/paper/4310-boosting-with-maximum-adaptive-sampling)    - **The Fast Convergence of Boosting (NIPS 2011)**    - Matus Telgarsky    - [[Paper]](https://papers.nips.cc/paper/4343-the-fast-convergence-of-boosting)    - **ShareBoost: Efficient Multiclass Learning with Feature Sharing (NIPS 2011)**    - Shai Shalev-Shwartz, Yonatan Wexler, Amnon Shashua    - [[Paper]](https://papers.nips.cc/paper/4213-shareboost-efficient-multiclass-learning-with-feature-sharing)    - **Multiclass Boosting: Theory and Algorithms (NIPS 2011)**    - Mohammad J. Saberian, Nuno Vasconcelos    - [[Paper]](https://papers.nips.cc/paper/4450-multiclass-boosting-theory-and-algorithms.pdf)    - **Variance Penalizing AdaBoost (NIPS 2011)**    - Pannagadatta K. Shivaswamy, Tony Jebara    - [[Paper]](https://papers.nips.cc/paper/4207-variance-penalizing-adaboost.pdf)    - **MKBoost: A Framework of Multiple Kernel Boosting (SDM 2011)**    - Hao Xia, Steven C. H. Hoi    - [[Paper]](https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=3280&context=sis_research)    - **A Boosting Approach to Improving Pseudo-Relevance Feedback (SIGIR 2011)**    - Yuanhua Lv, ChengXiang Zhai, Wan Chen    - [[Paper]](http://www.tyr.unlu.edu.ar/tallerIR/2012/papers/pseudorelevance.pdf)    - **Bagging Gradient-Boosted Trees for High Precision, Low Variance Ranking Models (SIGIR 2011)**    - Yasser Ganjisaffar, Rich Caruana, Cristina Videira Lopes    - [[Paper]](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/materials/bagging_lmbamart_jforests.pdf)    - **Boosting as a Product of Experts (UAI 2011)**    - Narayanan Unny Edakunni, Gary Brown, Tim Kovacs    - [[Paper]](https://arxiv.org/abs/1202.3716)    - **Parallel Boosted Regression Trees for Web Search Ranking (WWW 2011)**    - Stephen Tyree, Kilian Q. Weinberger, Kunal Agrawal, Jennifer Paykin    - [[Paper]](http://www.cs.cornell.edu/~kilian/papers/fr819-tyreeA.pdf)    - [[Code]](https://github.com/YS-L/pgbm)    ## 2010  - **The Boosting Effect of Exploratory Behaviors (AAAI 2010)**    - Jivko Sinapov, Alexander Stoytchev    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/download/1777/2265)    - **Boosting-Based System Combination for Machine Translation (ACL 2010)**    - Tong Xiao, Jingbo Zhu, Muhua Zhu, Huizhen Wang    - [[Paper]](https://www.aclweb.org/anthology/P10-1076)    - **BagBoo: A Scalable Hybrid Bagging-the-Boosting Model (CIKM 2010)**    - Dmitry Yurievich Pavlov, Alexey Gorodilov, Cliff A. Brunk    - [[Paper]](http://cache-default03h.cdn.yandex.net/download.yandex.ru/company/a_scalable_hybrid_bagging_the_boosting_model.pdf)    - [[Code]](https://github.com/arogozhnikov/infiniteboost)    - **Automatic Detection of Craters in Planetary Images: an Embedded Framework Using Feature Selection and Boosting (CIKM 2010)**    - Wei Ding, Tomasz F. Stepinski, LourenÃ§o P. C. Bandeira, Ricardo Vilalta, Youxi Wu, Zhenyu Lu, Tianyu Cao    - [[Paper]](https://www.uh.edu/~rvilalta/papers/2010/cikm10.pdf)    - **Facial Point Detection Using Boosted Regression and Graph Models (CVPR 2010)**    - Michel FranÃ§ois Valstar, Brais MartÃ­nez, Xavier Binefa, Maja Pantic    - [[Paper]](https://ibug.doc.ic.ac.uk/media/uploads/documents/CVPR-2010-ValstarEtAl-CAMERA.pdf)    - **Boosting for Transfer Learning with Multiple Sources (CVPR 2010)**    - Yi Yao, Gianfranco Doretto    - [[Paper]](https://ieeexplore.ieee.org/document/5539857)    - **Efficient Rotation Invariant Object Detection Using Boosted Random Ferns (CVPR 2010)**    - Michael Villamizar, Francesc Moreno-Noguer, Juan Andrade-Cetto, Alberto Sanfeliu    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.307.4002&rep=rep1&type=pdf)      - **Implicit Hierarchical Boosting for Multi-view Object Detection (CVPR 2010)**    - Xavier Perrotton, Marc Sturzel, Michel Roux    - [[Paper]](https://ieeexplore.ieee.org/document/5540115)    - **On-Line Semi-Supervised Multiple-Instance Boosting (CVPR 2010)**    - Bernhard Zeisl, Christian Leistner, Amir Saffari, Horst Bischof    - [[Paper]](https://ieeexplore.ieee.org/document/5539860)    - **Online Multi-Class LPBoost (CVPR 2010)**    - Amir Saffari, Martin Godec, Thomas Pock, Christian Leistner, Horst Bischof    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.8939&rep=rep1&type=pdf)    - [[Code]](https://github.com/amirsaffari/online-multiclass-lpboost)    - **Homotopy Regularization for Boosting (ICDM 2010)**    - Zheng Wang, Yangqiu Song, Changshui Zhang    - [[Paper]](https://ieeexplore.ieee.org/document/5694094)    - **Exploiting Local Data Uncertainty to Boost Global Outlier Detection (ICDM 2010)**    - Bo Liu, Jie Yin, Yanshan Xiao, Longbing Cao, Philip S. Yu    - [[Paper]](https://ieeexplore.ieee.org/document/5693984)    - **Boosting Classifiers with Tightened L0-Relaxation Penalties (ICML 2010)**    - Noam Goldberg, Jonathan Eckstein    - [[Paper]](https://pdfs.semanticscholar.org/11df/aed4ec2a2f72878789fa3a54d588d693bdda.pdf)    - **Boosting for Regression Transfer (ICML 2010)**    - David Pardoe, Peter Stone    - [[Paper]](https://www.cs.utexas.edu/~dpardoe/papers/ICML10.pdf)    - [[Code]](https://github.com/jay15summer/Two-stage-TrAdaboost.R2)    - **Boosted Backpropagation Learning for Training Deep Modular Networks (ICML 2010)**    - Alexander Grubb, J. Andrew Bagnell    - [[Paper]](https://icml.cc/Conferences/2010/papers/451.pdf)    - **Fast Boosting Using Adversarial Bandits (ICML 2010)**    - RÃ³bert Busa-Fekete, BalÃ¡zs KÃ©gl    - [[Paper]](https://www.lri.fr/~kegl/research/PDFs/BuKe10.pdf)    - **Boosting with Structure Information in the Functional Space: an Application to Graph Classification (KDD 2010)**    - Hongliang Fei, Jun Huan    - [[Paper]](https://dl.acm.org/citation.cfm?id=1835804.1835886)    - **Multi-task Learning for Boosting with Application to Web Search Ranking (KDD 2010)**    - Olivier Chapelle, Pannagadatta K. Shivaswamy, Srinivas Vadrevu, Kilian Q. Weinberger, Ya Zhang, Belle L. Tseng    - [[Paper]](https://dl.acm.org/citation.cfm?id=1835953)    - **A Theory of Multiclass Boosting (NIPS 2010)**    - Indraneel Mukherjee, Robert E. Schapire    - [[Paper]](http://rob.schapire.net/papers/multiboost-journal.pdf)    - **Boosting Classifier Cascades (NIPS 2010)**    - Mohammad J. Saberian, Nuno Vasconcelos    - [[Paper]](https://papers.nips.cc/paper/4033-boosting-classifier-cascades.pdf)    - **Joint Cascade Optimization Using A Product Of Boosted Classifiers (NIPS 2010)**    - Leonidas Lefakis, FranÃ§ois Fleuret    - [[Paper]](https://papers.nips.cc/paper/4148-joint-cascade-optimization-using-a-product-of-boosted-classifiers)    - **Robust LogitBoost and Adaptive Base Class (ABC) LogitBoost (UAI 2010)**    - Ping Li    - [[Paper]](https://arxiv.org/abs/1203.3491)    - [[Code]](https://github.com/pengsun/AOSOLogitBoost)    ## 2009    - **Feature Selection for Ranking Using Boosted Trees (CIKM 2009)**    - Feng Pan, Tim Converse, David Ahn, Franco Salvetti, Gianluca Donato    - [[Paper]](http://www.francosalvetti.com/cikm09_camera2.pdf)    - **Boosting KNN Text Classification Accuracy by Using Supervised Term Weighting Schemes (CIKM 2009)**    - Iyad Batal, Milos Hauskrecht    - [[Paper]](https://people.cs.pitt.edu/~milos/research/CIKM_2009_boosting_KNN.pdf)      - **Stochastic Gradient Boosted Distributed Decision Trees (CIKM 2009)**    - Jerry Ye, Jyh-Herng Chow, Jiang Chen, Zhaohui Zheng    - [[Paper]](http://cse.iitrpr.ac.in/ckn/courses/f2012/thomas.pdf)    - **A General Magnitude-Preserving Boosting Algorithm for Search Ranking (CIKM 2009)**    - Chenguang Zhu, Weizhu Chen, Zeyuan Allen Zhu, Gang Wang, Dong Wang, Zheng Chen    - [[Paper]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cikm2009-1.pdf)      - **Reducing Joint Boost-Based Multiclass Classification to Proximity Search (CVPR 2009)**    - Alexandra Stefan, Vassilis Athitsos, Quan Yuan, Stan Sclaroff    - [[Paper]](https://www.semanticscholar.org/paper/Reducing-JointBoost-based-multiclass-classification-Stefan-Athitsos/08ba1a7d91ce9b4ac26869bfe4bb7c955b0d1a24)    - **Imbalanced RankBoost for Efficiently Ranking Large-Scale Image-Video Collections (CVPR 2009)**    - Michele Merler, Rong Yan, John R. Smith    - [[Paper]](https://www.semanticscholar.org/paper/Imbalanced-RankBoost-for-efficiently-ranking-Merler-Yan/031ba6bf0d6df8bd3aa686ce85791b7d74f0b6d5)    - **Regularized Multi-Class Semi-Supervised Boosting (CVPR 2009)**    - Amir Saffari, Christian Leistner, Horst Bischof    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/5206715)    - **Learning to Associate: HybridBoosted Multi-Target Tracker for Crowded Scene (CVPR 2009)**    - Yuan Li, Chang Huang, Ram Nevatia    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.309.8335&rep=rep1&type=pdf)    - **Boosted Multi-task Learning for Face Verification with Applications to Web Image and Video Search (CVPR 2009)**    - Xiaogang Wang, Cha Zhang, Zhengyou Zhang    - [[Paper]](http://www.ee.cuhk.edu.hk/~xgwang/webface.pdf)    - **LidarBoost: Depth Superresolution for ToF 3D Shape Scanning (CVPR 2009)**    - Sebastian Schuon, Christian Theobalt, James E. Davis, Sebastian Thrun    - [[Paper]](http://ai.stanford.edu/~schuon/sr/cvpr09_poster_lidarboost.pdf)    - **Model Adaptation via Model Interpolation and Boosting for Web Search Ranking (EMNLP 2009)**    - Jianfeng Gao, Qiang Wu, Chris Burges, Krysta Marie Svore, Yi Su, Nazan Khan, Shalin Shah, Hongyan Zhou    - [[Paper]](https://pdfs.semanticscholar.org/7a82/66335d0b44596574588eabb090bfeae4ab35.pdf)    - **Finding Shareable Informative Patterns and Optimal Coding Matrix for Multiclass Boosting (ICCV 2009)**    - Bang Zhang, Getian Ye, Yang Wang, Jie Xu, Gunawan Herman    - [[Paper]](https://ieeexplore.ieee.org/document/5459146)    - **RankBoost with L1 Regularization for Facial Expression Recognition and Intensity Estimation (ICCV 2009)**    - Peng Yang, Qingshan Liu, Dimitris N. Metaxas    - [[Paper]](https://ieeexplore.ieee.org/document/5459371)    - **A Robust Boosting Tracker with Minimum Error Bound in a Co-Training Framework (ICCV 2009)**    - Rong Liu, Jian Cheng, Hanqing Lu    - [[Paper]](http://nlpr-web.ia.ac.cn/2009papers/gjhy/gh1.pdf)    - **Tutorial Summary: Survey of Boosting from an Optimization Perspective (ICML 2009)**    - Manfred K. Warmuth, S. V. N. Vishwanathan    - [[Paper]](http://www.stat.purdue.edu/~vishy/erlpboost/manfred.pdf)    - **Boosting Products of Base Classifiers (ICML 2009)**    - BalÃ¡zs KÃ©gl, RÃ³bert Busa-Fekete    - [[Paper]](https://users.lal.in2p3.fr/kegl/research/PDFs/keglBusafekete09.pdf)    - **ABC-boost: Adaptive Base Class Boost for Multi-Class Classification (ICML 2009)**    - Ping Li    - [[Paper]](https://icml.cc/Conferences/2009/papers/417.pdf)    - **Boosting with Structural Sparsity (ICML 2009)**    - John C. Duchi, Yoram Singer    - [[Paper]](https://web.stanford.edu/~jduchi/projects/DuchiSi09a.pdf)    - **Boosting Constrained Mutual Subspace Method for Robust Image-Set Based Object Recognition (IJCAI 2009)**    - Xi Li, Kazuhiro Fukui, Nanning Zheng    - [[Paper]](https://www.researchgate.net/publication/220812439_Boosting_Constrained_Mutual_Subspace_Method_for_Robust_Image-Set_Based_Object_Recognition)    - **Information Theoretic Regularization for Semi-supervised Boosting (KDD 2009)**    - Lei Zheng, Shaojun Wang, Yan Liu, Chi-Hoon Lee    - [[Paper]](https://pdfs.semanticscholar.org/5255/242d50851ce56354e10ae8fdcee6f47591c9.pdf)    - **Potential-Based Agnostic Boosting (NIPS 2009)**    - Adam Kalai, Varun Kanade    - [[Paper]](https://papers.nips.cc/paper/3676-potential-based-agnostic-boosting)    - **Positive Semidefinite Metric Learning with Boosting (NIPS 2009)**    - Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel    - [[Paper]](https://papers.nips.cc/paper/3658-positive-semidefinite-metric-learning-with-boosting)    - **Boosting with Spatial Regularization (NIPS 2009)**    - Zhen James Xiang, Yongxin Taylor Xi, Uri Hasson, Peter J. Ramadge    - [[Paper]](https://papers.nips.cc/paper/3696-boosting-with-spatial-regularization)      - **Effective Boosting of Na%C3%AFve Bayesian Classifiers by Local Accuracy Estimation (PAKDD 2009)**    - Zhipeng Xie    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-642-01307-2_88)    - **Multi-resolution Boosting for Classification and Regression Problems (PAKDD 2009)**    - Chandan K. Reddy, Jin Hyeong Park    - [[Paper]](http://dmkd.cs.vt.edu/papers/PAKDD09.pdf)    - **Efficient Active Learning with Boosting (SDM 2009)**    - Zheng Wang, Yangqiu Song, Changshui Zhang    - [[Paper]](https://pdfs.semanticscholar.org/c8be/b70c37e4b4c4ad77e46b39060c977779d201.pdf)    ## 2008  - **Group-Based Learning: A Boosting Approach (CIKM 2008)**    - Weijian Ni, Jun Xu, Hang Li, Yalou Huang    - [[Paper]](http://www.bigdatalab.ac.cn/~junxu/publications/CIKM2008_GroupLearn.pdf)    - **Semi-Supervised Boosting Using Visual Similarity Learning (CVPR 2008)**    - Christian Leistner, Helmut Grabner, Horst Bischof    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.7914&rep=rep1&type=pdf)    - **Mining Compositional Features for Boosting (CVPR 2008)**    - Junsong Yuan, Jiebo Luo, Ying Wu    - [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4587347)    - **Boosted Deformable Model for Human Body Alignment (CVPR 2008)**    - Xiaoming Liu, Ting Yu, Thomas Sebastian, Peter H. Tu    - [[Paper]](https://www.cse.msu.edu/~liuxm/publication/Liu_Yu_Sebastian_Tu_cvpr08.pdf)    - **Discriminative Modeling by Boosting on Multilevel Aggregates (CVPR 2008)**    - Jason J. Corso    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.409.3166&rep=rep1&type=pdf)    - **Face Alignment via Boosted Ranking Model (CVPR 2008)**    - Hao Wu, Xiaoming Liu, Gianfranco Doretto    - [[Paper]](https://ieeexplore.ieee.org/document/4587753)      - **Boosting Adaptive Linear Weak Classifiers for Online Learning and Tracking (CVPR 2008)**    - Toufiq Parag, Fatih Porikli, Ahmed M. Elgammal    - [[Paper]](https://www.merl.com/publications/docs/TR2008-065.pdf)    - **Detection with Multi-Exit Asymmetric Boosting (CVPR 2008)**    - Minh-Tri Pham, V-D. D. Hoang, Tat-Jen Cham    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.330.6364&rep=rep1&type=pdf)    - **Boosting Ordinal Features for Accurate and Fast Iris Recognition (CVPR 2008)**    - Zhaofeng He, Zhenan Sun, Tieniu Tan, Xianchao Qiu, Cheng Zhong, Wenbo Dong    - [[Paper]](https://www.researchgate.net/publication/224323296_Boosting_ordinal_features_for_accurate_and_fast_iris_recognition)    - **Adaptive and Compact Shape Descriptor by Progressive Feature Combination and Selection with Boosting (CVPR 2008)**    - Cheng Chen, Yueting Zhuang, Jun Xiao, Fei Wu    - [[Paper]](https://ieeexplore.ieee.org/document/4587613)      - **Boosting Relational Sequence Alignments (ICDM 2008)**    - Andreas Karwath, Kristian Kersting, Niels Landwehr    - [[Paper]](https://www.cs.uni-potsdam.de/~landwehr/ICDM08boosting.pdf)    - **Boosting with Incomplete Information (ICML 2008)**    - Gholamreza Haffari, Yang Wang, Shaojun Wang, Greg Mori, Feng Jiao    - [[Paper]](http://users.monash.edu.au/~gholamrh/publications/boosting_icml08_slides.pdf)      - **ManifoldBoost: Stagewise Function Approximation for Fully-, Semi- and Un-supervised Learning (ICML 2008)**    - Nicolas Loeff, David A. Forsyth, Deepak Ramachandran    - [[Paper]](http://reason.cs.uiuc.edu/deepak/manifoldboost.pdf)    - **Random Classification Noise Defeats All Convex Potential Boosters (ICML 2008)**    - Philip M. Long, Rocco A. Servedio    - [[Paper]](http://phillong.info/publications/LS09_potential.pdf)    - **Multi-class Cost-Sensitive Boosting with P-norm Loss Functions (KDD 2008)**    - Aurelie C. Lozano, Naoki Abe    - [[Paper]](https://dl.acm.org/citation.cfm?id=1401953)    - **MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features (NIPS 2008)**    - Tae-Kyun Kim, Roberto Cipolla    - [[Paper]](https://papers.nips.cc/paper/3483-mcboost-multiple-classifier-boosting-for-perceptual-co-clustering-of-images-and-visual-features)    - **PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning (NIPS 2008)**    - Chunhua Shen, Alan Welsh, Lei Wang    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.879.7750&rep=rep1&type=pdf)    - **On the Design of Loss Functions for Classification: Theory, Robustness to Outliers, and SavageBoost (NIPS 2008)**    - Hamed Masnadi-Shirazi, Nuno Vasconcelos    - [[Paper]](https://papers.nips.cc/paper/3591-on-the-design-of-loss-functions-for-classification-theory-robustness-to-outliers-and-savageboost)    - **Adaptive Martingale Boosting (NIPS 2008)**    - Philip M. Long, Rocco A. Servedio    - [[Paper]](http://phillong.info/publications/LS08_adaptive_martingale_boosting.pdf)      - **A Boosting Algorithm for Learning Bipartite Ranking Functions with Partially Labeled Data (SIGIR 2008)**    - Massih-Reza Amini, Tuong-Vinh Truong, Cyril Goutte    - [[Paper]](http://ama.liglab.fr/~amini/Publis/SemiSupRanking_sigir08.pdf)    ## 2007    - **Using Error-Correcting Output Codes with Model-Refinement to Boost Centroid Text Classifier (ACL 2007)**    - Songbo Tan    - [[Paper]](https://dl.acm.org/citation.cfm?id=1557794)    - **Fast Human Pose Estimation using Appearance and Motion via Multi-Dimensional Boosting Regression (CVPR 2007)**    - Alessandro Bissacco, Ming-Hsuan Yang, Stefano Soatto    - [[Paper]](http://vision.ucla.edu/papers/bissaccoYS07.pdf)    - **Generic Face Alignment using Boosted Appearance Model (CVPR 2007)**    - Xiaoming Liu    - [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4270290)    - **Eigenboosting: Combining Discriminative and Generative Information (CVPR 2007)**    - Helmut Grabner, Peter M. Roth, Horst Bischof    - [[Paper]](https://www.tugraz.at/fileadmin/user_upload/Institute/ICG/Documents/lrs/pubs/grabner_cvpr_07.pdf)    - **Online Learning Asymmetric Boosted Classifiers for Object Detection (CVPR 2007)**    - Minh-Tri Pham, Tat-Jen Cham    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/4270108)    - **Improving Part based Object Detection by Unsupervised Online Boosting (CVPR 2007)**    - Bo Wu, Ram Nevatia    - [[Paper]](https://ieeexplore.ieee.org/document/4270173)    - **A Specialized Processor Suitable for AdaBoost-Based Detection with Haar-like Features (CVPR 2007)**    - Masayuki Hiromoto, Kentaro Nakahara, Hiroki Sugano, Yukihiro Nakamura, Ryusuke Miyamoto    - [[Paper]](https://ieeexplore.ieee.org/document/4270413)    - **Simultaneous Object Detection and Segmentation by Boosting Local Shape Feature based Classifier (CVPR 2007)**    - Bo Wu, Ram Nevatia    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.309.9795&rep=rep1&type=pdf)    - **Compositional Boosting for Computing Hierarchical Image Structures (CVPR 2007)**    - Tianfu Wu, Gui-Song Xia, Song Chun Zhu    - [[Paper]](https://ieeexplore.ieee.org/document/4270059)    - **Boosting Coded Dynamic Features for Facial Action Units and Facial Expression Recognition (CVPR 2007)**    - Peng Yang, Qingshan Liu, Dimitris N. Metaxas    - [[Paper]](https://ieeexplore.ieee.org/document/4270084)    - **Object Classification in Visual Surveillance Using Adaboost (CVPR 2007)**    - John-Paul Renno, Dimitrios Makris, Graeme A. Jones    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/4270512)    - **A Boosting Regression Approach to Medical Anatomy Detection (CVPR 2007)**    - Shaohua Kevin Zhou, Jinghao Zhou, Dorin Comaniciu    - [[Paper]](http://ww.w.comaniciu.net/Papers/BoostingRegression_CVPR07.pdf)    - **Joint Real-time Object Detection and Pose Estimation Using Probabilistic Boosting Network (CVPR 2007)**    - Jingdan Zhang, Shaohua Kevin Zhou, Leonard McMillan, Dorin Comaniciu    - [[Paper]](http://csbio.unc.edu/mcmillan/pubs/CVPR07_Zhang.pdf)    - **Kernel Sharing With Joint Boosting For Multi-Class Concept Detection (CVPR 2007)**    - Wei Jiang, Shih-Fu Chang, Alexander C. Loui    - [[Paper]](http://www.ee.columbia.edu/~wjiang/references/jiangcvprws07.pdf)    - **Scale-Space Based Weak Regressors for Boosting (ECML 2007)**    - Jin Hyeong Park, Chandan K. Reddy    - [[Paper]](http://www.cs.wayne.edu/~reddy/Papers/ECML07.pdf)    - **Avoiding Boosting Overfitting by Removing Confusing Samples (ECML 2007)**    - Alexander Vezhnevets, Olga Barinova    - [[Paper]](http://groups.inf.ed.ac.uk/calvin/hp_avezhnev/Pubs/AvoidingBoostingOverfitting.pdf)    - **DynamicBoost: Boosting Time Series Generated by Dynamical Systems (ICCV 2007)**    - RenÃ© Vidal, Paolo Favaro    - [[Paper]](http://vision.jhu.edu/assets/VidalICCV07.pdf)    - **Incremental Learning of Boosted Face Detector (ICCV 2007)**    - Chang Huang, Haizhou Ai, Takayoshi Yamashita, Shihong Lao, Masato Kawade    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.126.9012&rep=rep1&type=pdf)    - **Gradient Feature Selection for Online Boosting (ICCV 2007)**    - Xiaoming Liu, Ting Yu    - [[Paper]](https://www.cse.msu.edu/~liuxm/publication/Liu_Yu_ICCV2007.pdf)    - **Fast Training and Selection of Haar Features Using Statistics in Boosting-based Face Detection (ICCV 2007)**    - Minh-Tri Pham, Tat-Jen Cham    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.212.6173&rep=rep1&type=pdf)    - **Cluster Boosted Tree Classifier for Multi-View - Multi-Pose Object Detection (ICCV 2007)**    - Bo Wu, Ramakant Nevatia    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.309.9885&rep=rep1&type=pdf)    - **Asymmetric Boosting (ICML 2007)**    - Hamed Masnadi-Shirazi, Nuno Vasconcelos    - [[Paper]](http://www.svcl.ucsd.edu/publications/conference/2007/icml07/AsymmetricBoosting.pdf)    - **Boosting for Transfer Learning (ICML 2007)**    - Wenyuan Dai, Qiang Yang, Gui-Rong Xue, Yong Yu    - [[Paper]](http://www.cs.ust.hk/~qyang/Docs/2007/tradaboost.pdf)      - **Gradient Boosting for Kernelized Output Spaces (ICML 2007)**    - Pierre Geurts, Louis Wehenkel, Florence d'AlchÃ©-Buc    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.435.3970&rep=rep1&type=pdf)    - **Boosting a Complete Technique to Find MSS and MUS Thanks to a Local Search Oracle (IJCAI 2007)**    - Ã‰ric GrÃ©goire, Bertrand Mazure, CÃ©dric Piette    - [[Paper]](http://www.cril.univ-artois.fr/~piette/IJCAI07_HYCAM.pdf)    - **Training Conditional Random Fields Using Virtual Evidence Boosting (IJCAI 2007)**    - Lin Liao, Tanzeem Choudhury, Dieter Fox, Henry A. Kautz    - [[Paper]](https://www.ijcai.org/Proceedings/07/Papers/407.pdf)    - **Simple Training of Dependency Parsers via Structured Boosting (IJCAI 2007)**    - Qin Iris Wang, Dekang Lin, Dale Schuurmans    - [[Paper]](https://www.ijcai.org/Proceedings/07/Papers/284.pdf)    - **Real Boosting a la Carte with an Application to Boosting Oblique Decision Tree (IJCAI 2007)**    - Claudia Henry, Richard Nock, Frank Nielsen    - [[Paper]](https://www.ijcai.org/Proceedings/07/Papers/135.pdf)    - **Managing Domain Knowledge and Multiple Models with Boosting (IJCAI 2007)**    - Peng Zang, Charles Lee Isbell Jr.    - [[Paper]](https://www.ijcai.org/Proceedings/07/Papers/185.pdf)    - **Model-Shared Subspace Boosting for Multi-label Classification (KDD 2007)**    - Rong Yan, Jelena Tesic, John R. Smith    - [[Paper]](http://rogerioferis.com/VisualRecognitionAndSearch2014/material/papers/IMARSKDD2007.pdf)    - **Regularized Boost for Semi-Supervised Learning (NIPS 2007)**    - Ke Chen, Shihai Wang    - [[Paper]](https://papers.nips.cc/paper/3167-regularized-boost-for-semi-supervised-learning.pdf)    - **Boosting Algorithms for Maximizing the Soft Margin (NIPS 2007)**    - Manfred K. Warmuth, Karen A. Glocer, Gunnar RÃ¤tsch    - [[Paper]](https://papers.nips.cc/paper/3374-boosting-algorithms-for-maximizing-the-soft-margin.pdf)    - **McRank: Learning to Rank Using Multiple Classification and Gradient Boosting (NIPS 2007)**    - Ping Li, Christopher J. C. Burges, Qiang Wu    - [[Paper]](https://papers.nips.cc/paper/3270-mcrank-learning-to-rank-using-multiple-classification-and-gradient-boosting.pdf)    - **One-Pass Boosting (NIPS 2007)**    - Zafer BarutÃ§uoglu, Philip M. Long, Rocco A. Servedio    - [[Paper]](http://phillong.info/publications/BLS07_one_pass.pdf)    - **Boosting the Area under the ROC Curve (NIPS 2007)**    - Philip M. Long, Rocco A. Servedio    - [[Paper]](https://papers.nips.cc/paper/3247-boosting-the-area-under-the-roc-curve.pdf)    - **FilterBoost: Regression and Classification on Large Datasets (NIPS 2007)**    - Joseph K. Bradley, Robert E. Schapire    - [[Paper]](http://rob.schapire.net/papers/FilterBoost_paper.pdf)    - **A General Boosting Method and its Application to Learning Ranking Functions for Web Search (NIPS 2007)**    - Zhaohui Zheng, Hongyuan Zha, Tong Zhang, Olivier Chapelle, Keke Chen, Gordon Sun    - [[Paper]](https://pdfs.semanticscholar.org/8f8d/874a3f0217289ba317b1f6175ac3b6f73d70.pdf)    - **Efficient Multiclass Boosting Classification with Active Learning (SDM 2007)**    - Jian Huang, Seyda Ertekin, Yang Song, Hongyuan Zha, C. Lee Giles    - [[Paper]](https://epubs.siam.org/doi/abs/10.1137/1.9781611972771.27)    - **AdaRank: a Boosting Algorithm for Information Retrieval (SIGIR 2007)**    - Jun Xu, Hang Li    - [[Paper]](http://www.bigdatalab.ac.cn/~junxu/publications/SIGIR2007_AdaRank.pdf)    ## 2006    - **Gradient Boosting for Sequence Alignment (AAAI 2006)**    - Charles Parker, Alan Fern, Prasad Tadepalli    - [[Paper]](http://web.engr.oregonstate.edu/~afern/papers/aaai06-align.pdf)    - **Boosting Kernel Models for Regression (ICDM 2006)**    - Ping Sun, Xin Yao    - [[Paper]](https://www.cs.bham.ac.uk/~xin/papers/icdm06SunYao.pdf)    - **Boosting for Learning Multiple Classes with Imbalanced Class Distribution (ICDM 2006)**    - Yanmin Sun, Mohamed S. Kamel, Yang Wang    - [[Paper]](http://people.ee.duke.edu/~lcarin/ImbalancedClassDistribution.pdf)    - **Boosting the Feature Space: Text Classification for Unstructured Data on the Web (ICDM 2006)**    - Yang Song, Ding Zhou, Jian Huang, Isaac G. Councill, Hongyuan Zha, C. Lee Giles    - [[Paper]](http://sonyis.me/paperpdf/icdm06_song.pdf)    - **Totally Corrective Boosting Algorithms that Maximize the Margin (ICML 2006)**    - Manfred K. Warmuth, Jun Liao, Gunnar RÃ¤tsch    - [[Paper]](https://users.soe.ucsc.edu/~manfred/pubs/C75.pdf)      - **How Boosting the Margin Can Also Boost Classifier Complexity (ICML 2006)**    - Lev Reyzin, Robert E. Schapire    - [[Paper]](http://rob.schapire.net/papers/boost_complexity.pdf)    - **Multiclass Boosting with Repartitioning (ICML 2006)**    - Ling Li    - [[Paper]](https://authors.library.caltech.edu/72259/1/p569-li.pdf)    - **AdaBoost is Consistent (NIPS 2006)**    - Peter L. Bartlett, Mikhail Traskin    - [[Paper]](http://jmlr.csail.mit.edu/papers/volume8/bartlett07b/bartlett07b.pdf)    - **Boosting Structured Prediction for Imitation Learning (NIPS 2006)**    - Nathan D. Ratliff, David M. Bradley, J. Andrew Bagnell, Joel E. Chestnutt    - [[Paper]](https://papers.nips.cc/paper/3154-boosting-structured-prediction-for-imitation-learning.pdf)    - **Chained Boosting (NIPS 2006)**    - Christian R. Shelton, Wesley Huie, Kin Fai Kan    - [[Paper]](https://papers.nips.cc/paper/2981-chained-boosting)      - **When Efficient Model Averaging Out-Performs Boosting and Bagging (PKDD 2006)**    - Ian Davidson, Wei Fan    - [[Paper]](https://link.springer.com/chapter/10.1007/11871637_46)    ## 2005  - **Semantic Place Classification of Indoor Environments with Mobile Robots Using Boosting (AAAI 2005)**    - Axel Rottmann, Ã“scar MartÃ­nez Mozos, Cyrill Stachniss, Wolfram Burgard    - [[Paper]](http://www2.informatik.uni-freiburg.de/~stachnis/pdf/rottmann05aaai.pdf)      - **Boosting-based Parse Reranking with Subtree Features (ACL 2005)**    - Taku Kudo, Jun Suzuki, Hideki Isozaki    - [[Paper]](http://chasen.org/~taku/publications/acl2005.pdf)    - **Using RankBoost to Compare Retrieval Systems (CIKM 2005)**    - Huyen-Trang Vu, Patrick Gallinari    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.9470&rep=rep1&type=pdf)    - **Classifier Fusion Using Shared Sampling Distribution for Boosting (ICDM 2005)**    - Costin Barbu, Raja Tanveer Iqbal, Jing Peng    - [[Paper]](https://ieeexplore.ieee.org/document/1565659)    - **Semi-Supervised Mixture of Kernels via LPBoost Methods (ICDM 2005)**    - Jinbo Bi, Glenn Fung, Murat Dundar, R. Bharat Rao    - [[Paper]](https://ieeexplore.ieee.org/document/1565728)    - **Efficient Discriminative Learning of Bayesian Network Classifier via Boosted Augmented Naive Bayes (ICML 2005)**    - Yushi Jing, Vladimir Pavlovic, James M. Rehg    - [[Paper]](http://mrl.isr.uc.pt/pub/bscw.cgi/d27355/Jing05Efficient.pdf)    - **Unifying the Error-Correcting and Output-Code AdaBoost within the Margin Framework (ICML 2005)**    - Yijun Sun, Sinisa Todorovic, Jian Li, Dapeng Wu    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.138.4246&rep=rep1&type=pdf)    - **A Smoothed Boosting Algorithm Using Probabilistic Output Codes (ICML 2005)**    - Rong Jin, Jian Zhang    - [[Paper]](http://www.stat.purdue.edu/~jianzhan/papers/icml05jin.pdf)    - **Robust Boosting and its Relation to Bagging (KDD 2005)**    - Saharon Rosset    - [[Paper]](https://www.tau.ac.il/~saharon/papers/bagboost.pdf)    - **Efficient Computations via Scalable Sparse Kernel Partial Least Squares and Boosted Latent Features (KDD 2005)**    - Michinari Momma    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.387.2078&rep=rep1&type=pdf)    - **Multiple Instance Boosting for Object Detection (NIPS 2005)**    - Paul A. Viola, John C. Platt, Cha Zhang    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.138.8312&rep=rep1&type=pdf)    - **Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations (NIPS 2005)**    - Aurelie C. Lozano, Sanjeev R. Kulkarni, Robert E. Schapire    - [[Paper]](https://www.cs.princeton.edu/~schapire/papers/betamix.pdf)      - **Boosted decision trees for word recognition in handwritten document retrieval (SIGIR 2005)**    - Nicholas R. Howe, Toni M. Rath, R. Manmatha    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.152.1551&rep=rep1&type=pdf)      - **Obtaining Calibrated Probabilities from Boosting (UAI 2005)**    - Alexandru Niculescu-Mizil, Rich Caruana    - [[Paper]](https://www.cs.cornell.edu/~caruana/niculescu.scldbst.crc.rev4.pdf)    ## 2004    - **Online Parallel Boosting (AAAI 2004)**    - Jesse A. Reichler, Harlan D. Harris, Michael A. Savchenko    - [[Paper]](https://www.aaai.org/Papers/AAAI/2004/AAAI04-059.pdf)    - **A Boosting Approach to Multiple Instance Learning (ECML 2004)**    - Peter Auer, Ronald Ortner    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-30115-8_9)    - **A Boosting Algorithm for Classification of Semi-Structured Text (EMNLP 2004)**    - Taku Kudo, Yuji Matsumoto    - [[Paper]](https://www.aclweb.org/anthology/W04-3239)    - **Text Classification by Boosting Weak Learners based on Terms and Concepts (ICDM 2004)**    - Stephan Bloehdorn, Andreas Hotho    - [[Paper]](https://ieeexplore.ieee.org/document/1410303)    - **Boosting Grammatical Inference with Confidence Oracles (ICML 2004)**    - Jean-Christophe Janodet, Richard Nock, Marc Sebban, Henri-Maxime Suchier    - [[Paper]](http://www1.univ-ag.fr/~rnock/Articles/Drafts/icml04-jnss.pdf)    - **Surrogate Maximization/Minimization Algorithms for AdaBoost and the Logistic Regression Model (ICML 2004)**    - Zhihua Zhang, James T. Kwok, Dit-Yan Yeung    - [[Paper]](https://icml.cc/Conferences/2004/proceedings/papers/77.pdf)    - **Training Conditional Random Fields via Gradient Tree Boosting (ICML 2004)**    - Thomas G. Dietterich, Adam Ashenfelter, Yaroslav Bulatov    - [[Paper]](http://web.engr.oregonstate.edu/~tgd/publications/ml2004-treecrf.pdf)    - **Boosting Margin Based Distance Functions for Clustering (ICML 2004)**    - Tomer Hertz, Aharon Bar-Hillel, Daphna Weinshall    - [[Paper]](http://www.cs.huji.ac.il/~daphna/papers/distboost-icml.pdf)    - **Column-Generation Boosting Methods for Mixture of Kernels (KDD 2004)**    - Jinbo Bi, Tong Zhang, Kristin P. Bennett    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.94.6359&rep=rep1&type=pdf)    - **Optimal Aggregation of Classifiers and Boosting Maps in Functional Magnetic Resonance Imaging (NIPS 2004)**    - Vladimir Koltchinskii, Manel MartÃ­nez-RamÃ³n, Stefan Posse    - [[Paper]](https://papers.nips.cc/paper/2699-optimal-aggregation-of-classifiers-and-boosting-maps-in-functional-magnetic-resonance-imaging.pdf)    - **Boosting on Manifolds: Adaptive Regularization of Base Classifiers (NIPS 2004)**    - BalÃ¡zs KÃ©gl, Ligen Wang    - [[Paper]](https://papers.nips.cc/paper/2613-boosting-on-manifolds-adaptive-regularization-of-base-classifiers)    - **Contextual Models for Object Detection Using Boosted Random Fields (NIPS 2004)**    - Antonio Torralba, Kevin P. Murphy, William T. Freeman    - [[Paper]](https://www.cs.ubc.ca/~murphyk/Papers/BRF-nips04-camera.pdf)    - **Generalization Error and Algorithmic Convergence of Median Boosting (NIPS 2004)**    - BalÃ¡zs KÃ©gl    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.8990&rep=rep1&type=pdf)    - **An Application of Boosting to Graph Classification (NIPS 2004)**    - Taku Kudo, Eisaku Maeda, Yuji Matsumoto    - [[Paper]](https://papers.nips.cc/paper/2739-an-application-of-boosting-to-graph-classification)    - **Logistic Regression and Boosting for Labeled Bags of Instances (PAKDD 2004)**    - Xin Xu, Eibe Frank    - [[Paper]](https://www.cs.waikato.ac.nz/~ml/publications/2004/xu-frank.pdf)    - **Fast and Light Boosting for Adaptive Mining of Data Streams (PAKDD 2004)**    - Fang Chu, Carlo Zaniolo    - [[Paper]](http://web.cs.ucla.edu/~zaniolo/papers/NBCAJMW77MW0J8CP.pdf)    ## 2003  - **On Boosting and the Exponential Loss (AISTATS 2003)**    - Abraham J. Wyner    - [[Paper]](http://www-stat.wharton.upenn.edu/~ajw/exploss.ps)    - **Boosting Support Vector Machines for Text Classification through Parameter-Free Threshold Relaxation (CIKM 2003)**    - James G. Shanahan, Norbert Roma    - [[Paper]](https://dl.acm.org/citation.cfm?id=956911)    - **Learning Cross-Document Structural Relationships Using Boosting (CIKM 2003)**    - Zhu Zhang, Jahna Otterbacher, Dragomir R. Radev    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.128.7712&rep=rep1&type=pdf)      - **On Boosting Improvement: Error Reduction and Convergence Speed-Up (ECML 2003)**    - Marc Sebban, Henri-Maxime Suchier    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-39857-8_32)    - **Boosting Lazy Decision Trees (ICML 2003)**    - Xiaoli Zhang Fern, Carla E. Brodley    - [[Paper]](https://www.aaai.org/Papers/ICML/2003/ICML03-026.pdf)    - **On the Convergence of Boosting Procedures (ICML 2003)**    - Tong Zhang, Bin Yu    - [[Paper]](https://pdfs.semanticscholar.org/dd3f/901b232280533fbdb9e57f144f44723617cf.pdf)    - **Linear Programming Boosting for Uneven Datasets (ICML 2003)**    - Jure Leskovec, John Shawe-Taylor    - [[Paper]](https://cs.stanford.edu/people/jure/pubs/textbooster-icml03.pdf)    - **Monte Carlo Theory as an Explanation of Bagging and Boosting (IJCAI 2003)**    - Roberto Esposito, Lorenza Saitta    - [[Paper]](https://dl.acm.org/citation.cfm?id=1630733)    - **On the Dynamics of Boosting (NIPS 2003)**    - Cynthia Rudin, Ingrid Daubechies, Robert E. Schapire    - [[Paper]](https://papers.nips.cc/paper/2535-on-the-dynamics-of-boosting)    - **Mutual Boosting for Contextual Inference (NIPS 2003)**    - Michael Fink, Pietro Perona    - [[Paper]](https://papers.nips.cc/paper/2520-mutual-boosting-for-contextual-inference)    - **Boosting Versus Covering (NIPS 2003)**    - Kohei Hatano, Manfred K. Warmuth    - [[Paper]](https://papers.nips.cc/paper/2532-boosting-versus-covering)    - **Multiple-Instance Learning via Disjunctive Programming Boosting (NIPS 2003)**    - Stuart Andrews, Thomas Hofmann    - [[Paper]](https://papers.nips.cc/paper/2478-multiple-instance-learning-via-disjunctive-programming-boosting)    - **Averaged Boosting: A Noise-Robust Ensemble Method (PAKDD 2003)**    - Yongdai Kim    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-36175-8_38)    - **SMOTEBoost: Improving Prediction of the Minority Class in Boosting (PKDD 2003)**    - Nitesh V. Chawla, Aleksandar Lazarevic, Lawrence O. Hall, Kevin W. Bowyer    - [[Paper]](https://www3.nd.edu/~nchawla/papers/ECML03.pdf)    ## 2002    - **Minimum Majority Classification and Boosting (AAAI 2002)**    - Philip M. Long    - [[Paper]](http://phillong.info/publications/minmaj.pdf)    - **Ranking Algorithms for Named Entity Extraction: Boosting and the Voted Perceptron (ACL 2002)**    - Michael Collins    - [[Paper]](https://www.aclweb.org/anthology/P02-1062)    - **Boosting to Correct Inductive Bias in Text Classification (CIKM 2002)**    - Yan Liu, Yiming Yang, Jaime G. Carbonell    - [[Paper]](https://dl.acm.org/citation.cfm?id=584792.584850)      - **How to Make AdaBoost.M1 Work for Weak Base Classifiers by Changing Only One Line of the Code (ECML 2002)**    - GÃ¼nther Eibl, Karl Peter Pfeiffer    - [[Paper]](https://dl.acm.org/citation.cfm?id=650068)    - **Scaling Boosting by Margin-Based Inclusionof Features and Relations (ECML 2002)**    - Susanne Hoche, Stefan Wrobel    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-36755-1_13)    - **A Robust Boosting Algorithm (ECML 2002)**    - Richard Nock, Patrice Lefaucheur    - [[Paper]](https://dl.acm.org/citation.cfm?id=650081)    - **iBoost: Boosting Using an instance-Based Exponential Weighting Scheme (ECML 2002)**    - Stephen Kwek, Chau Nguyen    - [[Paper]](https://www.researchgate.net/publication/220516082_iBoost_Boosting_using_an_instance-based_exponential_weighting_scheme)    - **Boosting Density Function Estimators (ECML 2002)**    - Franck Thollard, Marc Sebban, Philippe Ã‰zÃ©quel    - [[Paper]](https://link.springer.com/chapter/10.1007%2F3-540-36755-1_36)      - **Statistical Behavior and Consistency of Support Vector Machines, Boosting, and Beyond (ICML 2002)**    - Tong Zhang    - [[Paper]](https://www.researchgate.net/publication/221344927_Statistical_Behavior_and_Consistency_of_Support_Vector_Machines_Boosting_and_Beyond)    - **A Boosted Maximum Entropy Model for Learning Text Chunking (ICML 2002)**    - Seong-Bae Park, Byoung-Tak Zhang    - [[Paper]](https://www.researchgate.net/publication/221345636_A_Boosted_Maximum_Entropy_Model_for_Learning_Text_Chunking)    - **Towards Large Margin Speech Recognizers by Boosting and Discriminative Training (ICML 2002)**    - Carsten Meyer, Peter Beyerlein    - [[Paper]](https://www.semanticscholar.org/paper/Towards-Large-Margin-Speech-Recognizers-by-Boosting-Meyer-Beyerlein/8408479e36da812cdbf6bc15f7849c3e76a1016d)    - **Incorporating Prior Knowledge into Boosting (ICML 2002)**    - Robert E. Schapire, Marie Rochery, Mazin G. Rahim, Narendra K. Gupta    - [[Paper]](http://rob.schapire.net/papers/boostknowledge.pdf)    - **Modeling Auction Price Uncertainty Using Boosting-based Conditional Density Estimation (ICML 2002)**    - Robert E. Schapire, Peter Stone, David A. McAllester, Michael L. Littman, JÃ¡nos A. Csirik    - [[Paper]](http://www.cs.utexas.edu/~ai-lab/pubs/ICML02-tac.pdf)    - **MARK: A Boosting Algorithm for Heterogeneous Kernel Models (KDD 2002)**    - Kristin P. Bennett, Michinari Momma, Mark J. Embrechts    - [[Paper]](http://homepages.rpiscrews.us/~bennek/papers/kdd2.pdf)    - **Predicting rare classes: can boosting make any weak learner strong (KDD 2002)**    - Mahesh V. Joshi, Ramesh C. Agarwal, Vipin Kumar    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.1159&rep=rep1&type=pdf)    - **Kernel Design Using Boosting (NIPS 2002)**    - Koby Crammer, Joseph Keshet, Yoram Singer    - [[Paper]](https://pdfs.semanticscholar.org/ff79/344807e972fdd7e5e1c3ed5c539dd1aeecbe.pdf)    - **FloatBoost Learning for Classification (NIPS 2002)**    - Stan Z. Li, ZhenQiu Zhang, Heung-Yeung Shum, HongJiang Zhang    - [[Paper]](https://pdfs.semanticscholar.org/8ccc/5ef87eab96a4cae226750eba8322b30606ea.pdf)    - **Discriminative Learning for Label Sequences via Boosting (NIPS 2002)**    - Yasemin Altun, Thomas Hofmann, Mark Johnson    - [[Paper]](http://web.science.mq.edu.au/~mjohnson/papers/nips02.pdf)    - **Boosting Density Estimation (NIPS 2002)**    - Saharon Rosset, Eran Segal    - [[Paper]](https://papers.nips.cc/paper/2298-boosting-density-estimation.pdf)    - **Self Supervised Boosting (NIPS 2002)**    - Max Welling, Richard S. Zemel, Geoffrey E. Hinton    - [[Paper]](https://pdfs.semanticscholar.org/6a2a/f112a803e70c23b7055de2e73007cf42c301.pdf)    - **Boosted Dyadic Kernel Discriminants (NIPS 2002)**    - Baback Moghaddam, Gregory Shakhnarovich    - [[Paper]](http://www.merl.com/publications/docs/TR2002-55.pdf)      - **A Method to Boost Support Vector Machines (PAKDD 2002)**    - Lili Diao, Keyun Hu, Yuchang Lu, Chunyi Shi    - [[Paper]](https://elkingarcia.github.io/Papers/MLDM07.pdf)    - **A Method to Boost Naive Bayesian Classifiers (PAKDD 2002)**    - Lili Diao, Keyun Hu, Yuchang Lu, Chunyi Shi    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-47887-6_11)    - **Predicting Rare Classes: Comparing Two-Phase Rule Induction to Cost-Sensitive Boosting (PKDD 2002)**    - Mahesh V. Joshi, Ramesh C. Agarwal, Vipin Kumar    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45681-3_20)    - **Iterative Data Squashing for Boosting Based on a Distribution-Sensitive Distance (PKDD 2002)**    - Yuta Choki, Einoshin Suzuki    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45681-3_8)    - **Staged Mixture Modelling and Boosting (UAI 2002)**    - Christopher Meek, Bo Thiesson, David Heckerman    - [[Paper]](https://arxiv.org/abs/1301.0586)    - **Advances in Boosting (UAI 2002)**    - Robert E. Schapire    - [[Paper]](http://rob.schapire.net/papers/uai02.pdf)    ## 2001  - **Is Regularization Unnecessary for Boosting? (AISTATS 2001)**    - Wenxin Jiang    - [[Paper]](https://www.researchgate.net/publication/2439718_Is_Regularization_Unnecessary_for_Boosting)    - **Online Bagging and Boosting (AISTATS 2001)**    - Nikunj C. Oza, Stuart J. Russell    - [[Paper]](https://ti.arc.nasa.gov/m/profile/oza/files/ozru01a.pdf)      - **Text Categorization Using Transductive Boosting (ECML 2001)**    - Hirotoshi Taira, Masahiko Haruno    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-44795-4_39)    - **Improving Term Extraction by System Combination Using Boosting (ECML 2001)**    - Jordi Vivaldi, LluÃ­s MÃ rquez, Horacio RodrÃ­guez    - [[Paper]](https://dl.acm.org/citation.cfm?id=3108351)    - **Analysis of the Performance of AdaBoost.M2 for the Simulated Digit-Recognition-Example (ECML 2001)**    - GÃ¼nther Eibl, Karl Peter Pfeiffer    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-44795-4_10)    - **On the Practice of Branching Program Boosting (ECML 2001)**    - Tapio Elomaa, Matti KÃ¤Ã¤riÃ¤inen    - [[Paper]](https://www.researchgate.net/publication/221112522_On_the_Practice_of_Branching_Program_Boosting)    - **Boosting Mixture Models for Semi-supervised Learning (ICANN 2001)**    - Yves Grandvalet, Florence d'AlchÃ©-Buc, Christophe Ambroise    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-44668-0_7    - **A Comparison of Stacking with Meta Decision Trees to Bagging, Boosting, and Stacking with other Methods (ICDM 2001)**    - Bernard Zenko, Ljupco Todorovski, Saso Dzeroski    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.3118&rep=rep1&type=pdf)    - **Using Boosting to Simplify Classification Models (ICDM 2001)**    - Virginia Wheway    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/989565)    - **Evaluating Boosting Algorithms to Classify Rare Classes: Comparison and Improvements (ICDM 2001)**    - Mahesh V. Joshi, Vipin Kumar, Ramesh C. Agarwal    - [[Paper]](https://pdfs.semanticscholar.org/b829/fe743e4beeeed65d32d2d7931354df7a2f60.pdf)    - [[Code]]( )    - **Boosting Neighborhood-Based Classifiers (ICML 2001)**    - Marc Sebban, Richard Nock, StÃ©phane Lallich    - [[Paper]](https://www.semanticscholar.org/paper/Boosting-Neighborhood-Based-Classifiers-Sebban-Nock/ee88e3bbe8a7e81cae7ee53da2c824de7c82f882)    - **Boosting Noisy Data (ICML 2001)**    - Abba Krieger, Chuan Long, Abraham J. Wyner    - [[Paper]](https://www.researchgate.net/profile/Abba_Krieger/publication/221345435_Boosting_Noisy_Data/links/00463528a1ba641692000000.pdf)    - **Some Theoretical Aspects of Boosting in the Presence of Noisy Data (ICML 2001)**    - Wenxin Jiang    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=2494A2C06ACA22FA971AC1C29B53FF62?doi=10.1.1.27.7231&rep=rep1&type=pdf)    - **Filters, Wrappers and a Boosting-Based Hybrid for Feature Selection (ICML 2001)**    - Sanmay Das    - [[Paper]](https://pdfs.semanticscholar.org/93b6/25a0e35b59fa6a3e7dc1cbdb31268d62d69f.pdf)    - **The Distributed Boosting Algorithm (KDD 2001)**    - Aleksandar Lazarevic, Zoran Obradovic    - [[Paper]](https://www.researchgate.net/publication/2488971_The_Distributed_Boosting_Algorithm)    - **Experimental Comparisons of Online and Batch Versions of Bagging and Boosting (KDD 2001)**    - Nikunj C. Oza, Stuart J. Russell    - [[Paper]](https://people.eecs.berkeley.edu/~russell/papers/kdd01-online.pdf)    - **Semi-supervised MarginBoost (NIPS 2001)**    - Florence d'AlchÃ©-Buc, Yves Grandvalet, Christophe Ambroise    - [[Paper]](https://pdfs.semanticscholar.org/2197/f1c2d55827b6928cc80030922569acce2d6c.pdf)    - **Boosting and Maximum Likelihood for Exponential Models (NIPS 2001)**    - Guy Lebanon, John D. Lafferty    - [[Paper]](https://papers.nips.cc/paper/2042-boosting-and-maximum-likelihood-for-exponential-models.pdf)    - **Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade (NIPS 2001)**    - Paul A. Viola, Michael J. Jones    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.4306&rep=rep1&type=pdf)      - **Boosting Localized Classifiers in Heterogeneous Databases (SDM 2001)**    - Aleksandar Lazarevic, Zoran Obradovic    - [[Paper]](https://epubs.siam.org/doi/abs/10.1137/1.9781611972719.14)    ## 2000  - **Boosted Wrapper Induction (AAAI 2000)**    - Dayne Freitag, Nicholas Kushmerick    - [[Paper]](https://pdfs.semanticscholar.org/d009/a2bd48a9d1971fbc0d99f6df00539a62048a.pdf)    - **An Improved Boosting Algorithm and its Application to Text Categorization (CIKM 2000)**    - Fabrizio Sebastiani, Alessandro Sperduti, Nicola Valdambrini    - [[Paper]](http://nmis.isti.cnr.it/sebastiani/Publications/CIKM00.pdf)    - **Boosting for Document Routing (CIKM 2000)**    - Raj D. Iyer, David D. Lewis, Robert E. Schapire, Yoram Singer, Amit Singhal    - [[Paper]](http://singhal.info/cikm-2000.pdf)    - **On the Boosting Pruning Problem (ECML 2000)**    - Christino Tamon, Jie Xiang    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45164-1_41)    - **Boosting Applied to Word Sense Disambiguation (ECML 2000)**    - Gerard Escudero, LluÃ­s MÃ rquez, German Rigau    - [[Paper]](https://dl.acm.org/citation.cfm?id=649539)    - **An Empirical Study of MetaCost Using Boosting Algorithms (ECML 2000)**    - Kai Ming Ting    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.1624&rep=rep1&type=pdf)    - **FeatureBoost: A Meta-Learning Algorithm that Improves Model Robustness (ICML 2000)**    - Joseph O'Sullivan, John Langford, Rich Caruana, Avrim Blum    - [[Paper]](https://www.researchgate.net/publication/221345746_FeatureBoost_A_Meta-Learning_Algorithm_that_Improves_Model_Robustness)    - **Comparing the Minimum Description Length Principle and Boosting in the Automatic Analysis of Discourse (ICML 2000)**    - Tadashi Nomoto, Yuji Matsumoto    - [[Paper]](https://www.researchgate.net/publication/221344998_Comparing_the_Minimum_Description_Length_Principle_and_Boosting_in_the_Automatic_Analysis_of_Discourse)    - **A Boosting Approach to Topic Spotting on Subdialogues (ICML 2000)**    - Kary Myers, Michael J. Kearns, Satinder P. Singh, Marilyn A. Walker    - [[Paper]](https://www.cis.upenn.edu/~mkearns/papers/topicspot.pdf)    - **A Comparative Study of Cost-Sensitive Boosting Algorithms (ICML 2000)**    - Kai Ming Ting    - [[Paper]](https://dl.acm.org/citation.cfm?id=657944)    - **Boosting a Positive-Data-Only Learner (ICML 2000)**    - Andrew R. Mitchell    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.3669)    - **A Column Generation Algorithm For Boosting (ICML 2000)**    - Kristin P. Bennett, Ayhan Demiriz, John Shawe-Taylor    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=1828D5853F656BD6892E9C2C446ECC68?doi=10.1.1.16.9612&rep=rep1&type=pdf)    - **A Gradient-Based Boosting Algorithm for Regression Problems (NIPS 2000)**    - Richard S. Zemel, Toniann Pitassi    - [[Paper]](https://pdfs.semanticscholar.org/c41a/9417f5605b55bdd216d119e47669a92f5c50.pdf)    - **Weak Learners and Improved Rates of Convergence in Boosting (NIPS 2000)**    - Shie Mannor, Ron Meir    - [[Paper]](https://papers.nips.cc/paper/1906-weak-learners-and-improved-rates-of-convergence-in-boosting.pdf)    - **Adaptive Boosting for Spatial Functions with Unstable Driving Attributes (PAKDD 2000)**    - Aleksandar Lazarevic, Tim Fiez, Zoran Obradovic    - [[Paper]](http://www.dabi.temple.edu/~zoran/papers/lazarevic01j.pdf)    - **Scaling Up a Boosting-Based Learner via Adaptive Sampling (PAKDD 2000)**    - Carlos Domingo, Osamu Watanabe    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45571-X_37)    - **Learning First Order Logic Time Series Classifiers: Rules and Boosting (PKDD 2000)**    - Juan J. RodrÃ­guez Diez, Carlos Alonso GonzÃ¡lez, Henrik BostrÃ¶m    - [[Paper]](https://people.dsv.su.se/~henke/papers/rodriguez00b.pdf)    - **Bagging and Boosting with Dynamic Integration of Classifiers (PKDD 2000)**    - Alexey Tsymbal, Seppo Puuronen    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45372-5_12)    - **Text Filtering by Boosting Naive Bayes Classifiers (SIGIR 2000)**    - Yu-Hwan Kim, Shang-Yoon Hahn, Byoung-Tak Zhang    - [[Paper]](https://www.researchgate.net/publication/221299823_Text_filtering_by_boosting_Naive_Bayes_classifiers)    ## 1999  - **Boosting Methodology for Regression Problems (AISTATS 1999)**    - Greg Ridgeway, David Madigan, Thomas Richardson    - [[Paper]](https://pdfs.semanticscholar.org/5f19/6a8baa281b2190c4519305bec8f5c91c8e5a.pdf)    - **Boosting Applied to Tagging and PP Attachment (EMNLP 1999)**    - Steven Abney, Robert E. Schapire, Yoram Singer    - [[Paper]](https://www.aclweb.org/anthology/W99-0606)    - **Lazy Bayesian Rules: A Lazy Semi-Naive Bayesian Learning Technique Competitive to Boosting Decision Trees (ICML 1999)**    - Zijian Zheng, Geoffrey I. Webb, Kai Ming Ting    - [[Paper]](https://pdfs.semanticscholar.org/067e/86836ddbcb5e2844e955c16e058366a18c77.pdf)    - **AdaCost: Misclassification Cost-Sensitive Boosting (ICML 1999)**    - Wei Fan, Salvatore J. Stolfo, Junxin Zhang, Philip K. Chan    - [[Paper]](https://pdfs.semanticscholar.org/9ddf/bc2cc5c1b13b80a1a487b9caa57e80edd863.pdf)    - **Boosting a Strong Learner: Evidence Against the Minimum Margin (ICML 1999)**    - Michael Bonnell Harries    - [[Paper]](https://dl.acm.org/citation.cfm?id=657480)    - **Boosting Algorithms as Gradient Descent (NIPS 1999)**    - Llew Mason, Jonathan Baxter, Peter L. Bartlett, Marcus R. Frean    - [[Paper]](https://papers.nips.cc/paper/1766-boosting-algorithms-as-gradient-descent.pdf)    - **Boosting with Multi-Way Branching in Decision Trees (NIPS 1999)**    - Yishay Mansour, David A. McAllester    - [[Paper]](https://papers.nips.cc/paper/1659-boosting-with-multi-way-branching-in-decision-trees.pdf)    - **Potential Boosters (NIPS 1999)**    - Nigel Duffy, David P. Helmbold    - [[Paper]](https://pdfs.semanticscholar.org/4884/c765b6ceab7bdfb6703489810c8a386fd2a8.pdf)    ## 1998  - **An Efficient Boosting Algorithm for Combining Preferences (ICML 1998)**    - Yoav Freund, Raj D. Iyer, Robert E. Schapire, Yoram Singer    - [[Paper]](http://jmlr.csail.mit.edu/papers/volume4/freund03a/freund03a.pdf)    - **Query Learning Strategies Using Boosting and Bagging (ICML 1998)**    - Naoki Abe, Hiroshi Mamitsuka    - [[Paper]](https://www.bic.kyoto-u.ac.jp/pathway/mami/pubs/Files/icml98.pdf)    - **Regularizing AdaBoost (NIPS 1998)**    - Gunnar RÃ¤tsch, Takashi Onoda, Klaus-Robert MÃ¼ller    - [[Paper]](https://pdfs.semanticscholar.org/0afc/9de245547c675d40ad29240e2788c0416f91.pdf)    ## 1997  - **Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods (ICML 1997)**    - Robert E. Schapire, Yoav Freund, Peter Barlett, Wee Sun Lee    - [[Paper]](https://www.cc.gatech.edu/~isbell/tutorials/boostingmargins.pdf)    - **Using Output Codes to Boost Multiclass Learning Problems (ICML 1997)**    - Robert E. Schapire    - [[Paper]](http://rob.schapire.net/papers/Schapire97.pdf)    - **Improving Regressors Using Boosting Techniques (ICML 1997)**    - Harris Drucker    - [[Paper]](https://pdfs.semanticscholar.org/8d49/e2dedb817f2c3330e74b63c5fc86d2399ce3.pdf)    - **Pruning Adaptive Boosting (ICML 1997)**    - Dragos D. Margineantu, Thomas G. Dietterich    - [[Paper]](https://pdfs.semanticscholar.org/b25f/615fc139fbdeccc3bcf4462f908d7f8e37f9.pdf)    - **Training Methods for Adaptive Boosting of Neural Networks (NIPS 1997)**    - Holger Schwenk, Yoshua Bengio    - [[Paper]](https://papers.nips.cc/paper/1335-training-methods-for-adaptive-boosting-of-neural-networks.pdf)    ## 1996  - **Experiments with a New Boosting Algorithm (ICML 1996)**    - Yoav Freund, Robert E. Schapire    - [[Paper]](https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf)    ## 1995  - **Boosting Decision Trees (NIPS 1995)**    - Harris Drucker, Corinna Cortes    - [[Paper]](https://papers.nips.cc/paper/1059-boosting-decision-trees.pdf)    ## 1994  - **Boosting and Other Machine Learning Algorithms (ICML 1994)**    - Harris Drucker, Corinna Cortes, Lawrence D. Jackel, Yann LeCun, Vladimir Vapnik    - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603356500155)    --------------------------------------------------------------------------------    **License**    - [CC0 Universal](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers/blob/master/LICENSE) """
Big data;https://github.com/IBMStreams/streamsx.topology;"""# streamsx.topology  A project that supports building streaming topologies (applications)  for IBM Streams in different programming languages, such as Python, Java, and Scala.  http://ibmstreams.github.io/streamsx.topology/      ## Changes  [CHANGELOG.md](com.ibm.streamsx.topology/CHANGELOG.md)    ## Development  [DEVELOPMENT.md](DEVELOPMENT.md)    ## Testing  [TESTING.md](TESTING.md) """
Big data;https://github.com/linkedin/gobblin;"""# Apache Gobblin   [![Build Status](https://github.com/apache/gobblin/actions/workflows/build_and_test.yaml/badge.svg?branch=master)](https://travis-ci.org/apache/gobblin)  [![Documentation Status](https://readthedocs.org/projects/gobblin/badge/?version=latest)](https://gobblin.readthedocs.org/en/latest/?badge=latest)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.apache.gobblin/gobblin-api/badge.svg)](https://search.maven.org/search?q=g:org.apache.gobblin)  [![Stack Overflow](http://img.shields.io/:stack%20overflow-gobblin-brightgreen.svg)](http://stackoverflow.com/questions/tagged/gobblin)  [![Join us on Slack](https://img.shields.io/badge/slack-apache--gobblin-brightgreen.svg)]( https://join.slack.com/t/apache-gobblin/shared_invite/zt-vqgdztup-UUq8S6gGJqE6L5~9~JelNg)  [![codecov.io](https://codecov.io/github/apache/gobblin/branch/master/graph/badge.svg)](https://codecov.io/github/apache/gobblin)    Apache Gobblin is a highly scalable data management solution for structured and byte-oriented data in heterogeneous data ecosystems.     ### Capabilities  - Ingestion and export of data from a variety of sources and sinks into and out of the data lake. Gobblin is optimized and designed for ELT patterns with inline transformations on ingest (small t).  - Data Organization within the lake (e.g. compaction, partitioning, deduplication)  - Lifecycle Management of data within the lake (e.g. data retention)  - Compliance Management of data across the ecosystem (e.g. fine-grain data deletions)    ### Highlights  - Battle tested at scale: Runs in production at petabyte-scale at companies like LinkedIn, PayPal, Verizon etc.  - Feature rich: Supports task partitioning, state management for incremental processing, atomic data publishing, data quality checking, job scheduling, fault tolerance etc.  - Supports stream and batch execution modes   - Control Plane (Gobblin-as-a-service) supports programmatic triggering and orchestration of data plane operations.     ### Common Patterns used in production  - Stream / Batch ingestion of Kafka to Data Lake (HDFS, S3, ADLS)  - Bulk-loading serving stores from the Data Lake (e.g. HDFS -> Couchbase)  - Support for data sync across Federated Data Lake (HDFS <-> HDFS, HDFS <-> S3, S3 <-> ADLS)  - Integrate external vendor API-s (e.g. Salesforce, Dynamics etc.) with data store (HDFS, Couchbase etc)  - Enforcing Data retention policies and GDPR deletion on HDFS / ADLS      ### Apache Gobblin is NOT  - A general purpose data transformation engine like Spark or Flink. Gobblin can delegate complex-data processing tasks to Spark, Hive etc.   - A data storage system like Apache Kafka or HDFS. Gobblin integrates with these systems as sources or sinks.   - A general-purpose workflow execution system like Airflow, Azkaban, Dagster, Luigi.       # Requirements  * Java >= 1.8    If building the distribution with tests turned on:  * Maven version 3.5.3     # Instructions to run Apache RAT (Release Audit Tool)  1. Extract the archive file to your local directory.  2. Run `./gradlew rat`. Report will be generated under build/rat/rat-report.html    # Instructions to build the distribution  1. Extract the archive file to your local directory.  2. Skip tests and build the distribution:   Run `./gradlew build -x findbugsMain -x test -x rat -x checkstyleMain`   The distribution will be created in build/gobblin-distribution/distributions directory.  (or)  3. Run tests and build the distribution (requires Maven):   Run `./gradlew build`   The distribution will be created in build/gobblin-distribution/distributions directory.    # Quick Links      * [Gobblin documentation](https://gobblin.apache.org/docs/)      * [Running Gobblin on Docker from your laptop](https://github.com/apache/gobblin/blob/master/gobblin-docs/user-guide/Docker-Integration.md)      * [Getting started guide](https://gobblin.apache.org/docs/Getting-Started/)      * [Gobblin architecture](https://gobblin.apache.org/docs/Gobblin-Architecture/)    * Community Slack: [Get your invite]( https://join.slack.com/t/apache-gobblin/shared_invite/zt-vqgdztup-UUq8S6gGJqE6L5~9~JelNg)    * [List of companies known to use Gobblin](https://gobblin.apache.org/docs/Powered-By/)     * [Sample project](https://github.com/apache/gobblin/tree/master/gobblin-example)    * [How to build Gobblin from source code](https://gobblin.apache.org/docs/user-guide/Building-Gobblin/)    * [Issue tracker - Apache Jira](https://issues.apache.org/jira/projects/GOBBLIN/issues/) """
Big data;https://github.com/twitter/storehaus;"""## Storehaus [![Build Status](https://secure.travis-ci.org/twitter/storehaus.png)](http://travis-ci.org/twitter/storehaus)    Storehaus is a library that makes it easy to work with asynchronous key value stores. Storehaus is built on top of Twitter's [Future](https://github.com/twitter/util/blob/master/util-core/src/main/scala/com/twitter/util/Future.scala).    ### Storehaus-Core    Storehaus's core module defines three traits; a read-only `ReadableStore` a write-only `WritableStore` and a read-write `Store`. The traits themselves are tiny:    ```scala  package com.twitter.storehaus    import com.twitter.util.{ Closable, Future, Time }    trait ReadableStore[-K, +V] extends Closeable {    def get(k: K): Future[Option[V]]    def multiGet[K1 <: K](ks: Set[K1]): Map[K1, Future[Option[V]]]    override def close(time: Time) = Future.Unit  }    trait WritableStore[-K, -V] {    def put(kv: (K, V)): Future[Unit] = multiPut(Map(kv)).apply(kv._1)    def multiPut[K1 <: K](kvs: Map[K1, V]): Map[K1, Future[Unit]] =      kvs.map { kv => (kv._1, put(kv)) }    override def close(time: Time) = Future.Unit  }    trait Store[-K, V] extends ReadableStore[K, V] with WritableStore[K, Option[V]]  ```    The `ReadableStore` trait uses the `Future[Option[V]]` return type to communicate one of three states about each value. A value is either    * definitely present,  * definitely missing, or  * unknown due to some error (perhaps a timeout, or a downed host).    The [`ReadableStore`](http://twitter.github.com/storehaus/#com.twitter.storehaus.ReadableStore$) and [`Store`](http://twitter.github.com/storehaus/#com.twitter.storehaus.Store$) companion objects provide a bunch of ways to create new stores. See the linked API documentation for more information.    ### Combinators    Coding with Storehaus's interfaces gives you access to a number of powerful combinators. The easiest way to access these combinators is by wrapping your store in an [`EnrichedReadableStore`](http://twitter.github.com/storehaus/#com.twitter.storehaus.EnrichedReadableStore) or an [`EnrichedStore`](http://twitter.github.com/storehaus/#com.twitter.storehaus.EnrichedStore). Storehaus provides implicit conversions inside of the [`ReadableStore`](http://twitter.github.com/storehaus/#com.twitter.storehaus.ReadableStore$) and [`Store`](http://twitter.github.com/storehaus/#com.twitter.storehaus.Store$) objects.    Here's an example of the `mapValues` combinator, useful for transforming the type of an existing store.    ```scala  import com.twitter.storehaus.ReadableStore  import ReadableStore.enrich    // Create a ReadableStore from Int -> String:  val store = ReadableStore.fromMap(Map[Int, String](1 -> ""some value"", 2 -> ""other value""))    // ""get"" behaves as expected:  store.get(1).get  // res5: Option[String] = Some(some value)    // calling ""mapValues"" with a function from V => NewV returns a new ReadableStore[K, NewV]:  val countStore: ReadableStore[Int, Int] = store.mapValues { s => s.size }    // This new store applies the function to every value on the way out:  countStore.get(1).get  // res6: Option[Int] = Some(10)  ```    ### Storehaus-Algebra    `storehaus-algebra` module adds the `MergeableStore` trait. If you're using key-value stores for aggregations, you're going to love `MergeableStore`.    ```scala  package com.twitter.storehaus.algebra    trait MergeableStore[-K, V] extends Store[K, V] {    def monoid: Monoid[V]    def merge(kv: (K, V)): Future[Option[V]] = multiMerge(Map(kv)).apply(kv._1)    def multiMerge[K1 <: K](kvs: Map[K1, V]): Map[K1, Future[Option[V]]] = kvs.map { kv => (kv._1, merge(kv)) }  }  ```    `MergeableStore`'s `merge` and `multiMerge` are similar to `put` and `multiPut`; the difference is that values added with `merge` are added to the store's existing value and the previous value is returned.  Because the addition is handled with a `Semigroup[V]` or `Monoid[V]` from Twitter's [Algebird](https://github.com/twitter/algebird) project, it's easy to write stores that aggregate [Lists](http://twitter.github.com/algebird/#com.twitter.algebird.ListMonoid), [decayed values](http://twitter.github.com/algebird/#com.twitter.algebird.DecayedValue), even [HyperLogLog](http://twitter.github.com/algebird/#com.twitter.algebird.HyperLogLog$) instances.    The [`MergeableStore`](http://twitter.github.com/storehaus/#com.twitter.storehaus.algebra.MergeableStore$) object provides a number of combinators on these stores. For ease of use, Storehaus provides an implicit conversion to an enrichment on `MergeableStore`. Access this by importing `MergeableStore.enrich`.    ### Other Modules    Storehaus provides a number of modules wrapping existing key-value stores. Enriching these key-value stores with Storehaus's combinators has been hugely helpful to us here at Twitter. Writing your jobs in terms of Storehaus stores makes it easy to test your jobs; use an in-memory `JMapStore` in testing and a `MemcacheStore` in production.      * [Storehaus-memcache](http://twitter.github.com/storehaus/#com.twitter.storehaus.memcache.MemcacheStore) (wraps Twitter's [finagle-memcachedx](https://github.com/twitter/finagle/tree/master/finagle-memcachedx) library)    * [Storehaus-mysql](http://twitter.github.com/storehaus/#com.twitter.storehaus.mysql.MySqlStore) (wraps Twitter's [finagle-mysql](https://github.com/twitter/finagle/tree/master/finagle-mysql) library)    * [Storehaus-redis](http://twitter.github.com/storehaus/#com.twitter.storehaus.redis.RedisStore) (wraps Twitter's [finagle-redis](https://github.com/twitter/finagle/tree/master/finagle-redis) library)    * [Storehaus-hbase](http://twitter.github.com/storehaus/#com.twitter.storehaus.hbase.HBaseStore)    * [Storehaus-dynamodb](https://github.com/twitter/storehaus/tree/develop/storehaus-dynamodb)    * [Storehaus-leveldb](https://github.com/twitter/storehaus/tree/develop/storehaus-leveldb)    #### Planned Modules    Here's a list of modules we plan in implementing, with links to the github issues tracking progress on these modules:    * [storehaus-berkeleydb](https://github.com/twitter/storehaus/issues/52)    ## Community and Documentation    This, and all [github.com/twitter](https://github.com/twitter) projects, are under the [Twitter Open Source Code of Conduct](https://engineering.twitter.com/opensource/code-of-conduct). Additionally, see the [Typelevel Code of Conduct](http://typelevel.org/conduct) for specific examples of harassing behavior that are not tolerated.    To learn more and find links to tutorials and information around the web, check out the [Storehaus Wiki](https://github.com/twitter/storehaus/wiki).    The latest ScalaDocs are hosted on Storehaus's [Github Project Page](http://twitter.github.io/storehaus).    Discussion occurs primarily on the [Storehaus mailing list](https://groups.google.com/forum/#!forum/storehaus). Issues should be reported on the [GitHub issue tracker](https://github.com/twitter/storehaus/issues).    ## Maven    Storehaus modules are available on maven central. The current groupid and version for all modules is, respectively, `""com.twitter""` and  `0.12.0`.    Current published artifacts are    * `storehaus-core_2.11`  * `storehaus-core_2.10`  * `storehaus-algebra_2.11`  * `storehaus-algebra_2.10`  * `storehaus-memcache_2.11`  * `storehaus-memcache_2.10`  * `storehaus-mysql_2.11`  * `storehaus-mysql_2.10`  * `storehaus-hbase_2.11`  * `storehaus-hbase_2.10`  * `storehaus-redis_2.11`  * `storehaus-redis_2.10`  * `storehaus-dynamodb_2.11`  * `storehaus-dynamodb_2.10`  * `storehaus-kafka-08_2.11`  * `storehaus-kafka-08_2.10`  * `storehaus-mongodb_2.11`  * `storehaus-mongodb_2.10`  * `storehaus-elasticsearch_2.11`  * `storehaus-elasticsearch_2.10`  * `storehaus-leveldb_2.11`  * `storehaus-leveldb_2.10`  * `storehaus-http_2.11`  * `storehaus-http_2.10`  * `storehaus-cache_2.11`  * `storehaus-cache_2.10`  * `storehaus-testing_2.11`  * `storehaus-testing_2.10`    The suffix denotes the scala version.    ## Testing notes    We use travis-ci to set up any underlying stores (e.g. MySQL, Redis, Memcached) for the tests. In order for these tests to pass on your local machine, you may need additional setup.    ### MySQL tests    You will need MySQL installed on your local machine.  Once installed, run the `mysql` commands listed in [.travis.yml](https://github.com/twitter/storehaus/blob/develop/.travis.yml) file.    ### Redis tests    You will need [redis](http://redis.io/) installed on your local machine. Redis comes bundled with an executable for spinning up a server called `redis-server`. The Storehaus redis tests expect the factory defaults for connecting to one of these redis server instances, resolvable on `localhost` port `6379`.    ### Memcached    You will need [Memcached](http://memcached.org/) installed on your local machine and running on the default port `11211`.    ## Authors    * Oscar Boykin <https://twitter.com/posco>  * Sam Ritchie <https://twitter.com/sritchie>    ## Contributors    Here are a few that shine among the many:    * Ruban Monu <https://twitter.com/rubanm>, for `storehaus-mysql`  * Doug Tangren <https://twitter.com/softprops>, for `storehaus-redis`  * Ryan Weald <https://twitter.com/rweald>, for `storehaus-dynamodb`    ## License    Copyright 2013 Twitter, Inc.    Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 """
Big data;https://github.com/facebook/bistro;"""# Bistro: A fast, flexible toolkit for scheduling and running distributed tasks    [![Build Status](https://travis-ci.org/facebook/bistro.svg?branch=master)](https://travis-ci.org/facebook/bistro)    This README is a very abbreviated introduction to Bistro. Visit  http://facebook.github.io/bistro for a more structured introduction, and for the docs.    Bistro is a toolkit for making distributed computation systems. It can  schedule and run distributed tasks, including data-parallel jobs.  It  enforces resource constraints for worker hosts and data-access bottlenecks.  It supports remote worker pools, low-latency batch scheduling, dynamic  shards, and a variety of other possibilities.  It has command-line and web  UIs.    Some of the diverse problems that Bistro solved at Facebook:   - Safely run map-only ETL tasks against live production databases (MySQL,     HBase, Postgres).   - Provide a resource-aware job queue for batch CPU/GPU compute jobs.   - Replace Hadoop for a periodic online data compression task on HBase,     improving time-to-completion and reliability by over 10x.    You can run Bistro ""out of the box"" to suit a variety of different  applications, but even so, it is a tool for engineers.  You should be able  to get started just by reading the documentation, but when in doubt, look at  the code --- it was written to be read.    Some applications of Bistro may involve writing small plugins to make it fit  your needs.  The code is built to be extensible.  Ask for tips, and we'll do  our best to [help](https://www.facebook.com/groups/bistro.scheduler).  In  return, we hope that you will send a pull request to allow us to share your  work with the community.    ## Early release    Although Bistro has been in production at Facebook for over 3 years, the  present public release is partial, including just the server components.    ## Install the dependencies and build    Bistro needs a 64-bit Linux, Folly, FBThrift, Proxygen, boost, and  libsqlite3.  You need 2-3GB of RAM to build, as well as GCC 4.9 or above.    `build/README.md` documents the usage of Docker-based scripts that build  Bistro on Ubuntu 14.04, 16.04, and Debian 8.6.  You should be able to follow  very similar steps on most modern Linux distributions.    If you run into dependency problems, look at `bistro/cmake/setup.cmake` for  a full list of Bistro's external dependencies (direct and indirect).  We  gratefully accept patches that improve Bistro's builds, or add support for  various flavors of Linux and Mac OS.    The binaries will be in `bistro/cmake/{Debug,Release}`.  Available build  targets are explained here:     http://cmake.org/Wiki/CMake_Useful_Variables#Compilers_and_Tools  You can start Bistro's unit tests by running `ctest` in those directories.    ## Your first Bistro run    This is just one simple demo, but Bistro is a very flexible tool. Refer to  http://facebook.github.io/bistro/ for more in-depth information.    We are going to start a single Bistro scheduler talking to one 'remote'  worker.    Aside: The scheduler tracks jobs, and data shards on which to execute them.  It also makes sure only to start new tasks when the required resources are  available.  The remote worker is a module for executing centrally scheduled  work on many machines.  The UI can aggregate many schedulers at once, so  using remote workers is optional --- a share-nothing, many-scheduler system  is sometimes preferable.    Let's make a task to execute:    ```  cat <<EOF > ~/demo_bistro_task.sh  #!/bin/bash  echo ""I got these arguments: \$@""  echo ""stderr is also logged"" 1>&2  echo ""done"" > ""\$2""  # Report the task status to Bistro via a named pipe  EOF  chmod u+x ~/demo_bistro_task.sh  ```    Open two terminals, one for the scheduler, and one for the worker.    ```  # In both terminals  cd bistro/bistro  # Start the scheduler in one terminal  ./cmake/Debug/server/bistro_scheduler \    --server_port=6789 --http_server_port=6790 \    --config_file=scripts/test_configs/simple --clean_statuses \    --CAUTION_startup_wait_for_workers=1 --instance_node_name=scheduler  # Start the worker in another  mkdir /tmp/bistro_worker  ./cmake/Debug/worker/bistro_worker --server_port=27182 --scheduler_host=:: \    --scheduler_port=6789 --worker_command=""$HOME/demo_bistro_task.sh"" \    --data_dir=/tmp/bistro_worker  ```    You should be seeing some lively log activity on both terminals. In several  seconds, the worker-scheduler negotiation should complete, and you should  see messages like ""Task ...  quit with status"" and ""Got status"".    Since we passed `--clean_statuses`, the scheduler will not persist any task  completions that happened during this run.  The worker, on the other hand,  will keep a record of the task logs in `/tmp/bistro_worker/task_logs.sql3`.    If you want task completions to persist across runs, tell Bistro where to  put the SQLite database, via `--data_dir=/tmp/bistro_scheduler` and  `--status_table=task_statuses`    ```  mkdir /tmp/bistro_scheduler  ./cmake/Debug/server/bistro_scheduler \    --server_port=6789 --http_server_port=6790 \    --config_file=scripts/test_configs/simple \    --data_dir=/tmp/bistro_scheduler --status_table=task_statuses \    --CAUTION_startup_wait_for_workers=1 --instance_node_name=scheduler  ```    You can query the running scheduler via its REST API:    ```  curl -d '{""a"":{""handler"":""jobs""},""b"":{""handler"":""running_tasks""}}' :::6790  curl -d '{""my subquery"":{""handler"":""task_logs"",""log_type"":""stdout""}}' :::6790  ```    **Pro-tip:** For ease of reading, pipe the output through either `jq` or  `json_pp` (from a Perl package). For longer outputs, try `| jq -C . | less -R`.    You should also take a look at the scheduler configuration to see how its  jobs, nodes, and resources were specified.    ```  less scripts/test_configs/simple  ```    For debugging, we typically invoke the binaries like this:    ```  gdb cmake/Debug/worker/bistro_worker -ex ""r ..."" 2>&1 | tee WORKER.txt  ```    When configuring a real deployment, be sure to carefully review the `--help`  of the scheduler & worker binaries, as well as the documentation on  http://facebook.github.io/bistro.  And don't hesitate to ask for help in the group:  https://www.facebook.com/groups/bistro.scheduler    ## License    See [LICENSE](LICENSE). """
Big data;https://github.com/shunfei/indexr;"""# IndexR    ![IndexR Logo](images/indexr-logo-150x150.png)    **IndexR** is a super fast columnar data format on HDFS, which focus on fast analytic, both for massive static(historical) data and rapidly ingesting realtime data. IndexR is designed for OLAP. IndexR is greatly suitable for building data warehouse based on Hadoop ecosystem.    * Super fast, 2~4x read speed of Parquet.  * 3 levels indices supported. Say goodbye to full scan.  * Support realtime ingestion. No more wait, analyse anything right after they happen.  * Hardware efficiency, anyone can use.  * Features like realtime and offline pre-aggregation, online schema update, 100% accurate, etc.  * Deep integration with Hadoop ecosystem. Adapted with popular query engines like Apache Drill, Apache Hive, etc.    #### Getting started    * Installation    * First [Compile from source](https://github.com/shunfei/indexr/wiki/Compilation) or download a pre-compiled package directly from [release page](https://github.com/shunfei/indexr/releases).    * Then [Set up a cluster](https://github.com/shunfei/indexr/wiki/Deployment).  * User manual - Check [here](https://github.com/shunfei/indexr/wiki/User-Guide).  * Any problems? - Found an [issue](https://github.com/shunfei/indexr/issues).    #### Documentation    [https://github.com/shunfei/indexr/wiki](https://github.com/shunfei/indexr/wiki)    #### Useful Links    * [IndexR æŠ€æœ¯ç™½çš®ä¹¦](https://github.com/shunfei/sfmind/blob/master/indexr_white_paper/indexr_white_paper.md)  * [IndexR introduction](https://github.com/shunfei/sfmind/blob/master/indexr.about.en.md)    Please feel free to file any issues.    ## Contact    * WeChat: xilyflow  * QQ Group: 606666586 (IndexRè®¨è®ºç»„)    ## License        Copyright 2016 Sunteng Tech.        Licensed under the Apache License, Version 2.0 (the ""License""); you may not      use this file except in compliance with the License. You may obtain a copy of      the License at        http://www.apache.org/licenses/LICENSE-2.0        Unless required by applicable law or agreed to in writing, software      distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT      WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the      License for the specific language governing permissions and limitations under      the License. """
Big data;https://github.com/fchollet/keras;"""# Keras: Deep Learning for humans    ![Keras logo](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)    This repository hosts the development of the Keras library.  Read the documentation at [keras.io](https://keras.io/).    ## About Keras    Keras is a deep learning API written in Python,  running on top of the machine learning platform [TensorFlow](https://github.com/tensorflow/tensorflow).  It was developed with a focus on enabling fast experimentation.  *Being able to go from idea to result as fast as possible is key to doing good research.*    Keras is:    -   **Simple** -- but not simplistic. Keras reduces developer *cognitive load*      to free you to focus on the parts of the problem that really matter.  -   **Flexible** -- Keras adopts the principle of *progressive disclosure of      complexity*: simple workflows should be quick and easy, while arbitrarily      advanced workflows should be *possible* via a clear path that builds upon      what you've already learned.  -   **Powerful** -- Keras provides industry-strength performance and      scalability: it is used by organizations and companies including NASA,      YouTube, and Waymo.    ---    ## Keras & TensorFlow 2    [TensorFlow 2](https://www.tensorflow.org/) is an end-to-end, open-source machine learning platform.  You can think of it as an infrastructure layer for  [differentiable programming](https://en.wikipedia.org/wiki/Differentiable_programming).  It combines four key abilities:    - Efficiently executing low-level tensor operations on CPU, GPU, or TPU.  - Computing the gradient of arbitrary differentiable expressions.  - Scaling computation to many devices, such as clusters of hundreds of GPUs.  - Exporting programs (""graphs"") to external runtimes such as servers, browsers, mobile and embedded devices.    Keras is the high-level API of TensorFlow 2: an approachable, highly-productive interface  for solving machine learning problems,  with a focus on modern deep learning. It provides essential abstractions and building blocks for developing  and shipping machine learning solutions with high iteration velocity.    Keras empowers engineers and researchers to take full advantage of the scalability  and cross-platform capabilities of TensorFlow 2: you can run Keras on TPU or on large clusters of GPUs,  and you can export your Keras models to run in the browser or on a mobile device.    ---    ## First contact with Keras    The core data structures of Keras are __layers__ and __models__.  The simplest type of model is the [`Sequential` model](/guides/sequential_model/), a linear stack of layers.  For more complex architectures, you should use the [Keras functional API](/guides/functional_api/),  which allows to build arbitrary graphs of layers, or [write models entirely from scratch via subclasssing](/guides/making_new_layers_and_models_via_subclassing/).    Here is the `Sequential` model:    ```python  from tensorflow.keras.models import Sequential    model = Sequential()  ```    Stacking layers is as easy as `.add()`:    ```python  from tensorflow.keras.layers import Dense    model.add(Dense(units=64, activation='relu'))  model.add(Dense(units=10, activation='softmax'))  ```    Once your model looks good, configure its learning process with `.compile()`:    ```python  model.compile(loss='categorical_crossentropy',                optimizer='sgd',                metrics=['accuracy'])  ```    If you need to, you can further configure your optimizer. The Keras philosophy is to keep simple things simple,  while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code via subclassing).    ```python  model.compile(loss=tf.keras.losses.categorical_crossentropy,                optimizer=tf.keras.optimizers.SGD(                    learning_rate=0.01, momentum=0.9, nesterov=True))  ```    You can now iterate on your training data in batches:    ```python  # x_train and y_train are Numpy arrays.  model.fit(x_train, y_train, epochs=5, batch_size=32)  ```    Evaluate your test loss and metrics in one line:    ```python  loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)  ```    Or generate predictions on new data:    ```python  classes = model.predict(x_test, batch_size=128)  ```    What you just saw is the most elementary way to use Keras.    However, Keras is also a highly-flexible framework suitable to iterate on state-of-the-art research ideas.  Keras follows the principle of **progressive disclosure of complexity**: it makes it easy to get started,  yet it makes it possible to handle arbitrarily advanced use cases,  only requiring incremental learning at each step.    In much the same way that you were able to train & evaluate a simple neural network above in a few lines,  you can use Keras to quickly develop new training procedures or exotic model architectures.  Here's a low-level training loop example, combining Keras functionality with the TensorFlow `GradientTape`:    ```python  import tensorflow as tf    # Prepare an optimizer.  optimizer = tf.keras.optimizers.Adam()  # Prepare a loss function.  loss_fn = tf.keras.losses.kl_divergence    # Iterate over the batches of a dataset.  for inputs, targets in dataset:      # Open a GradientTape.      with tf.GradientTape() as tape:          # Forward pass.          predictions = model(inputs)          # Compute the loss value for this batch.          loss_value = loss_fn(targets, predictions)        # Get gradients of loss wrt the weights.      gradients = tape.gradient(loss_value, model.trainable_weights)      # Update the weights of the model.      optimizer.apply_gradients(zip(gradients, model.trainable_weights))  ```    For more in-depth tutorials about Keras, you can check out:    - [Introduction to Keras for engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/)  - [Introduction to Keras for researchers](https://keras.io/getting_started/intro_to_keras_for_researchers/)  - [Developer guides](https://keras.io/guides/)    ---    ## Installation    Keras comes packaged with TensorFlow 2 as `tensorflow.keras`.  To start using Keras, simply [install TensorFlow 2](https://www.tensorflow.org/install).    ---    ## Release and compatibility    Keras has **nightly releases** (`keras-nightly` on PyPI)  and **stable releases** (`keras` on PyPI).  The nightly Keras releases are usually compatible with the corresponding version  of the `tf-nightly` releases  (e.g. `keras-nightly==2.7.0.dev2021100607` should be  used with `tf-nightly==2.7.0.dev2021100607`).  We don't maintain backward compatibility for nightly releases.  For stable releases, each Keras  version maps to a specific stable version of TensorFlow.    The table below shows the compatibility version mapping  between TensorFlow versions and Keras versions.    All the release branches can be found on [Github](https://github.com/keras-team/keras/releases).    All the release binaries can be found on [Pypi](https://pypi.org/project/keras/#history).    | Keras release | Note      | Compatible Tensorflow version |  | -----------   | ----------- | -----------        |  | [2.4](https://github.com/keras-team/keras/releases/tag/2.4.0)  | Last stable release of multi-backend Keras | < 2.5  | 2.5-pre| Pre-release (not formal) for standalone Keras repo | >= 2.5 < 2.6  | [2.6](https://github.com/keras-team/keras/releases/tag/v2.6.0)    | First formal release of standalone Keras.  | >= 2.6 < 2.7  | [2.7](https://github.com/keras-team/keras/releases/tag/v2.7.0-rc0)    | (Upcoming release) | >= 2.7 < 2.8  | nightly|                                            | tf-nightly    ---  ## Support    You can ask questions and join the development discussion:    - In the [TensorFlow forum](https://discuss.tensorflow.org/).  - On the [Keras Google group](https://groups.google.com/forum/#!forum/keras-users).  - On the [Keras Slack channel](https://kerasteam.slack.com). Use [this link](https://keras-slack-autojoin.herokuapp.com/) to request an invitation to the channel.    ---    ## Opening an issue    You can also post **bug reports and feature requests** (only)  in [GitHub issues](https://github.com/keras-team/keras/issues).      ---    ## Opening a PR    We welcome contributions! Before opening a PR, please read  [our contributor guide](https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md),  and the [API design guideline](https://github.com/keras-team/governance/blob/master/keras_api_design_guidelines.md). """
Big data;https://github.com/tidwall/tile38;"""<p align=""center"">    <a href=""https://tile38.com""><img       src=""/.github/images/logo-light.svg""       width=""284"" border=""0"" alt=""Tile38""></a>  </p>  <p align=""center"">  <a href=""https://tile38.com/slack/""><img src=""https://img.shields.io/badge/slack-channel-orange.svg"" alt=""Slack Channel""></a>  <a href=""https://hub.docker.com/r/tile38/tile38""><img src=""https://img.shields.io/badge/docker-ready-blue.svg"" alt=""Docker Ready""></a>  </p>    Tile38 is an open source (MIT licensed), in-memory geolocation data store, spatial index, and realtime geofence. It supports a variety of object types including lat/lon points, bounding boxes, XYZ tiles, Geohashes, and GeoJSON.     <p align=""center"">  <i>This README is quick start document. You can find detailed documentation at <a href=""https://tile38.com"">https://tile38.com</a>.</i><br><br>  <a href=""#searching""><img src=""/.github/images/search-nearby.png"" alt=""Nearby"" border=""0"" width=""120"" height=""120""></a>  <a href=""#searching""><img src=""/.github/images/search-within.png"" alt=""Within"" border=""0"" width=""120"" height=""120""></a>  <a href=""#searching""><img src=""/.github/images/search-intersects.png"" alt=""Intersects"" border=""0"" width=""120"" height=""120""></a>  <a href=""https://tile38.com/topics/geofencing""><img src=""/.github/images/geofence.gif"" alt=""Geofencing"" border=""0"" width=""120"" height=""120""></a>  <a href=""https://tile38.com/topics/roaming-geofences""><img src=""/.github/images/roaming.gif"" alt=""Roaming Geofences"" border=""0"" width=""120"" height=""120""></a>  </p>    ## Features    - Spatial index with [search](#searching) methods such as Nearby, Within, and Intersects.  - Realtime [geofencing](#geofencing) through [webhooks](https://tile38.com/commands/sethook) or [pub/sub channels](#pubsub-channels).  - Object types of [lat/lon](#latlon-point), [bbox](#bounding-box), [Geohash](#geohash), [GeoJSON](#geojson), [QuadKey](#quadkey), and [XYZ tile](#xyz-tile).  - Support for lots of [Clients Libraries](#tile38-client-libraries) written in many different languages.  - Variety of protocols, including [http](#http) (curl), [websockets](#websockets), [telnet](#telnet), and the [Redis RESP](https://redis.io/topics/protocol).  - Server responses are [RESP](https://redis.io/topics/protocol) or [JSON](https://www.json.org).  - Full [command line interface](#cli).  - Leader / follower [replication](#replication).  - In-memory database that persists on disk.    ## Components  - `tile38-server    ` - The server  - `tile38-cli       ` - Command line interface tool  - `tile38-benchmark ` - Server benchmark tool    ## Getting Started    ### Getting Tile38    Perhaps the easiest way to get the latest Tile38 is to use one of the pre-built release binaries which are available for OSX, Linux, FreeBSD, and Windows. Instructions for using these binaries are on the GitHub [releases page](https://github.com/tidwall/tile38/releases).    ### Docker     To run the latest stable version of Tile38:    ```  docker pull tile38/tile38  docker run -p 9851:9851 tile38/tile38  ```    Visit the [Tile38 hub page](https://hub.docker.com/r/tile38/tile38/) for more information.    ### Homebrew (macOS)    Install Tile38 using [Homebrew](https://brew.sh/)    ```sh  brew install tile38  tile38-server  ```    ### Building Tile38     Tile38 can be compiled and used on Linux, OSX, Windows, FreeBSD, and probably others since the codebase is 100% Go. We support both 32 bit and 64 bit systems. [Go](https://golang.org/dl/) must be installed on the build machine.    To build everything simply:  ```  $ make  ```    To test:  ```  $ make test  ```    ### Running   For command line options invoke:  ```  $ ./tile38-server -h  ```    To run a single server:    ```  $ ./tile38-server    # The tile38 shell connects to localhost:9851  $ ./tile38-cli  > help  ```    #### Prometheus Metrics  Tile38 can natively export Prometheus metrics by setting the `--metrics-addr` command line flag (disabled by default). This example exposes the HTTP metrics server on port 4321:  ```  # start server and enable Prometheus metrics, listen on local interface only  ./tile38-server --metrics-addr=127.0.0.1:4321    # access metrics  curl http://127.0.0.1:4321/metrics  ```  If you need to access the `/metrics` endpoint from a different host you'll have to set the flag accordingly, e.g. set it to `0.0.0.0:<<port>>` to listen on all interfaces.    Use the [redis_exporter](https://github.com/oliver006/redis_exporter) for more advanced use cases like extracting key values or running a lua script.      ## <a name=""cli""></a>Playing with Tile38    Basic operations:  ```  $ ./tile38-cli    # add a couple of points named 'truck1' and 'truck2' to a collection named 'fleet'.  > set fleet truck1 point 33.5123 -112.2693   # on the Loop 101 in Phoenix  > set fleet truck2 point 33.4626 -112.1695   # on the I-10 in Phoenix    # search the 'fleet' collection.  > scan fleet                                 # returns both trucks in 'fleet'  > nearby fleet point 33.462 -112.268 6000    # search 6 kilometers around a point. returns one truck.    # key value operations  > get fleet truck1                           # returns 'truck1'  > del fleet truck2                           # deletes 'truck2'  > drop fleet                                 # removes all   ```    Tile38 has a ton of [great commands](https://tile38.com/commands).    ## Fields  Fields are extra data that belongs to an object. A field is always a double precision floating point. There is no limit to the number of fields that an object can have.     To set a field when setting an object:  ```  > set fleet truck1 field speed 90 point 33.5123 -112.2693               > set fleet truck1 field speed 90 field age 21 point 33.5123 -112.2693  ```    To set a field when an object already exists:  ```  > fset fleet truck1 speed 90  ```    ## Searching    Tile38 has support to search for objects and points that are within or intersects other objects. All object types can be searched including Polygons, MultiPolygons, GeometryCollections, etc.    <img src=""/.github/images/search-within.png"" width=""200"" height=""200"" border=""0"" alt=""Search Within"" align=""left"">    #### Within   WITHIN searches a collection for objects that are fully contained inside a specified bounding area.  <BR CLEAR=""ALL"">    <img src=""/.github/images/search-intersects.png"" width=""200"" height=""200"" border=""0"" alt=""Search Intersects"" align=""left"">    #### Intersects  INTERSECTS searches a collection for objects that intersect a specified bounding area.  <BR CLEAR=""ALL"">    <img src=""/.github/images/search-nearby.png"" width=""200"" height=""200"" border=""0"" alt=""Search Nearby"" align=""left"">    #### Nearby  NEARBY searches a collection for objects that intersect a specified radius.  <BR CLEAR=""ALL"">    ### Search options  **WHERE** - This option allows for filtering out results based on [field](#fields) values. For example<br>```nearby fleet where speed 70 +inf point 33.462 -112.268 6000``` will return only the objects in the 'fleet' collection that are within the 6 km radius **and** have a field named `speed` that is greater than `70`. <br><br>Multiple WHEREs are concatenated as **and** clauses. ```WHERE speed 70 +inf WHERE age -inf 24``` would be interpreted as *speed is over 70 <b>and</b> age is less than 24.*<br><br>The default value for a field is always `0`. Thus if you do a WHERE on the field `speed` and an object does not have that field set, the server will pretend that the object does and that the value is Zero.    **MATCH** - MATCH is similar to WHERE except that it works on the object id instead of fields.<br>```nearby fleet match truck* point 33.462 -112.268 6000``` will return only the objects in the 'fleet' collection that are within the 6 km radius **and** have an object id that starts with `truck`. There can be multiple MATCH options in a single search. The MATCH value is a simple [glob pattern](https://en.wikipedia.org/wiki/Glob_(programming)).    **CURSOR** - CURSOR is used to iterate though many objects from the search results. An iteration begins when the CURSOR is set to Zero or not included with the request, and completes when the cursor returned by the server is Zero.    **NOFIELDS** - NOFIELDS tells the server that you do not want field values returned with the search results.    **LIMIT** - LIMIT can be used to limit the number of objects returned for a single search request.      ## Geofencing    <img src=""/.github/images/geofence.gif"" width=""200"" height=""200"" border=""0"" alt=""Geofence animation"" align=""left"">  A <a href=""https://en.wikipedia.org/wiki/Geo-fence"">geofence</a> is a virtual boundary that can detect when an object enters or exits the area. This boundary can be a radius, bounding box, or a polygon. Tile38 can turn any standard search into a geofence monitor by adding the FENCE keyword to the search.     *Tile38 also allows for [Webhooks](https://tile38.com/commands/sethook) to be assigned to Geofences.*    <br clear=""all"">    A simple example:  ```  > nearby fleet fence point 33.462 -112.268 6000  ```  This command opens a geofence that monitors the 'fleet' collection. The server will respond with:  ```  {""ok"":true,""live"":true}  ```  And the connection will be kept open. If any object enters or exits the 6 km radius around `33.462,-112.268` the server will respond in realtime with a message such as:    ```  {""command"":""set"",""detect"":""enter"",""id"":""truck02"",""object"":{""type"":""Point"",""coordinates"":[-112.2695,33.4626]}}  ```    The server will notify the client if the `command` is `del | set | drop`.     - `del` notifies the client that an object has been deleted from the collection that is being fenced.  - `drop` notifies the client that the entire collection is dropped.  - `set` notifies the client that an object has been added or updated, and when it's position is detected by the fence.    The `detect` may be one of the following values.    - `inside` is when an object is inside the specified area.  - `outside` is when an object is outside the specified area.  - `enter` is when an object that **was not** previously in the fence has entered the area.  - `exit` is when an object that **was** previously in the fence has exited the area.  - `cross` is when an object that **was not** previously in the fence has entered **and** exited the area.    These can be used when establishing a geofence, to pre-filter responses. For instance, to limit responses to `enter` and `exit` detections:    ```  > nearby fleet fence detect enter,exit point 33.462 -112.268 6000  ```    ### Pub/sub channels    Tile38 supports delivering geofence notications over pub/sub channels.     To create a static geofence that sends notifications when a bus is within 200 meters of a point and sends to the `busstop` channel:    ```  > setchan busstop nearby buses fence point 33.5123 -112.2693 200  ```    Subscribe on the `busstop` channel:    ```  > subscribe busstop  ```    ## Object types    All object types except for XYZ Tiles and QuadKeys can be stored in a collection. XYZ Tiles and QuadKeys are reserved for the SEARCH keyword only.    #### Lat/lon point  The most basic object type is a point that is composed of a latitude and a longitude. There is an optional `z` member that may be used for auxiliary data such as elevation or a timestamp.  ```  set fleet truck1 point 33.5123 -112.2693     # plain lat/lon  set fleet truck1 point 33.5123 -112.2693 225 # lat/lon with z member  ```    #### Bounding box  A bounding box consists of two points. The first being the southwestern most point and the second is the northeastern most point.  ```  set fleet truck1 bounds 30 -110 40 -100  ```  #### Geohash  A [geohash](https://en.wikipedia.org/wiki/Geohash) is a string representation of a point. With the length of the string indicating the precision of the point.   ```  set fleet truck1 hash 9tbnthxzr # this would be equivalent to 'point 33.5123 -112.2693'  ```    #### GeoJSON  [GeoJSON](https://tools.ietf.org/html/rfc7946) is an industry standard format for representing a variety of object types including a point, multipoint, linestring, multilinestring, polygon, multipolygon, geometrycollection, feature, and featurecollection.    <i>* All ignored members will not persist.</i>    **Important to note that all coordinates are in Longitude, Latitude order.**    ```  set city tempe object {""type"":""Polygon"",""coordinates"":[[[0,0],[10,10],[10,0],[0,0]]]}  ```    #### XYZ Tile  An XYZ tile is rectangle bounding area on earth that is represented by an X, Y coordinate and a Z (zoom) level.  Check out [maptiler.org](http://www.maptiler.org/google-maps-coordinates-tile-bounds-projection/) for an interactive example.    #### QuadKey  A QuadKey used the same coordinate system as an XYZ tile except that the string representation is a string characters composed of 0, 1, 2, or 3. For a detailed explanation checkout [The Bing Maps Tile System](https://msdn.microsoft.com/en-us/library/bb259689.aspx).    ## Network protocols    It's recommended to use a [client library](#tile38-client-libraries) or the [Tile38 CLI](#running), but there are times when only HTTP is available or when you need to test from a remote terminal. In those cases we provide an HTTP and telnet options.    #### HTTP  One of the simplest ways to call a tile38 command is to use HTTP. From the command line you can use [curl](https://curl.haxx.se/). For example:    ```  # call with request in the body  curl --data ""set fleet truck3 point 33.4762 -112.10923"" localhost:9851    # call with request in the url path  curl localhost:9851/set+fleet+truck3+point+33.4762+-112.10923  ```    #### Websockets  Websockets can be used when you need to Geofence and keep the connection alive. It works just like the HTTP example above, with the exception that the connection stays alive and the data is sent from the server as text websocket messages.    #### Telnet  There is the option to use a plain telnet connection. The default output through telnet is [RESP](https://redis.io/topics/protocol).    ```  telnet localhost 9851  set fleet truck3 point 33.4762 -112.10923  +OK    ```    The server will respond in [JSON](https://json.org) or [RESP](https://redis.io/topics/protocol) depending on which protocol is used when initiating the first command.    - HTTP and Websockets use JSON.   - Telnet and RESP clients use RESP.    ## Tile38 Client Libraries    The following clients are built specifically for Tile38.    Clients that support most Tile38 features are marked with a â­ï¸.    - â­ï¸ Go: [xjem/t38c](https://github.com/xjem/t38c)  - â­ï¸ Node.js: [node-tile38](https://github.com/phulst/node-tile38) ([example code](https://github.com/tidwall/tile38/wiki/Node.js-example-(node-tile38)))  - â­ï¸ Python: [pyle38](https://github.com/iwpnd/pyle38)  - Go: [cjkreklow/t38c](https://github.com/cjkreklow/t38c)  - Python: [pytile38](https://github.com/mitghi/pytile38)  - Rust: [nazar](https://github.com/younisshah/nazar)  - Swift: [Talon](https://github.com/mikekinney/Talon)  - Java: [tile38-client-java](https://github.com/jamshidrostami/tile38-client-java)  - Java: [tile38-client](https://github.com/HkMoyun/tile38-client)    ## Redis Client Libraries    Tile38 uses the [Redis RESP](https://redis.io/topics/protocol) protocol natively.   Therefore most clients that support basic Redis commands will also support Tile38.    - C: [hiredis](https://github.com/redis/hiredis)  - C#: [StackExchange.Redis](https://github.com/StackExchange/StackExchange.Redis)  - C++: [redox](https://github.com/hmartiro/redox)  - Clojure: [carmine](https://github.com/ptaoussanis/carmine)  - Common Lisp: [CL-Redis](https://github.com/vseloved/cl-redis)  - Erlang: [Eredis](https://github.com/wooga/eredis)  - Go: [go-redis](https://github.com/go-redis/redis) ([example code](https://github.com/tidwall/tile38/wiki/Go-example-(go-redis)))  - Go: [redigo](https://github.com/gomodule/redigo) ([example code](https://github.com/tidwall/tile38/wiki/Go-example-(redigo)))  - Haskell: [hedis](https://github.com/informatikr/hedis)  - Java: [lettuce](https://github.com/mp911de/lettuce) ([example code](https://github.com/tidwall/tile38/wiki/Java-example-(lettuce)))  - Node.js: [node_redis](https://github.com/NodeRedis/node_redis) ([example code](https://github.com/tidwall/tile38/wiki/Node.js-example-(node-redis)))  - Perl: [perl-redis](https://github.com/PerlRedis/perl-redis)  - PHP: [tinyredisclient](https://github.com/ptrofimov/tinyredisclient) ([example code](https://github.com/tidwall/tile38/wiki/PHP-example-(tinyredisclient)))  - PHP: [phpredis](https://github.com/phpredis/phpredis)  - Python: [redis-py](https://github.com/andymccurdy/redis-py) ([example code](https://github.com/tidwall/tile38/wiki/Python-example))  - Ruby: [redic](https://github.com/amakawa/redic) ([example code](https://github.com/tidwall/tile38/wiki/Ruby-example-(redic)))  - Ruby: [redis-rb](https://github.com/redis/redis-rb) ([example code](https://github.com/tidwall/tile38/wiki/Ruby-example-(redis-rb)))  - Rust: [redis-rs](https://github.com/mitsuhiko/redis-rs)  - Scala: [scala-redis](https://github.com/debasishg/scala-redis)  - Swift: [Redbird](https://github.com/czechboy0/Redbird)    ## Contact    Josh Baker [@tidwall](https://twitter.com/tidwall)    ## License    Tile38 source code is available under the MIT [License](/LICENSE). """
Big data;https://github.com/twitter/twemcache;"""# Twemcache: Twitter Memcached     [![status: retired](https://opensource.twitter.dev/status/retired.svg)](https://opensource.twitter.dev/status/#retired)  [![Build Status](https://secure.travis-ci.org/twitter/twemcache.png)](http://travis-ci.org/twitter/twemcache)    Twemcache is no longer actively maintained.  See [twitter/pelikan](https://github.com/twitter/pelikan) for our latest caching work.    Twemcache (pronounced ""tw-em-cache"") is the Twitter Memcached. Twemcache is based on a fork of [Memcached](http://memcached.org/) v.1.4.4 that has been heavily modified to make to suitable for the large scale production environment at Twitter.    ## Build    To build twemcache from distribution tarball:        $ ./configure      $ make      $ sudo make install    To build twemcache from distribution tarball with a non-standard path to [libevent](http://libevent.org) install:        $ ./configure --with-libevent=<path>      $ make      $ sudo make install    To build twemcache from distribution tarball with a statically linked libevent:        $ ./configure --enable-static=libevent      $ make      $ sudo make install    To build twemcache from distribution tarball in _debug mode_ with _assertion panics enabled_:        $ CFLAGS=""-ggdb3 -O0"" ./configure --enable-debug=full      $ make      $ sudo make install    To build twemcache from source with _debug logs enabled_ and _assertions disabled_:        $ git clone git@github.com:twitter/twemcache.git      $ cd twemcache      $ autoreconf -fvi      $ ./configure --enable-debug=log      $ make V=1      $ src/twemcache -h    ## Help        Usage: twemcache [-?hVCELdkrDS] [-o output file] [-v verbosity level]                 [-A stats aggr interval]                 [-t threads] [-P pid file] [-u user]                 [-x command logging entry] [-X command logging file]                 [-R max requests] [-c max conns] [-b backlog] [-p port] [-U udp port]                 [-l interface] [-s unix path] [-a access mask] [-M eviction strategy]                 [-f factor] [-m max memory] [-n min item chunk size] [-I slab size]                 [-z slab profile]        Options:        -h, --help                  : this help        -V, --version               : show version and exit        -E, --prealloc              : preallocate memory for all slabs        -L, --use-large-pages       : use large pages if available        -k, --lock-pages            : lock all pages and preallocate slab memory        -d, --daemonize             : run as a daemon        -r, --maximize-core-limit   : maximize core file limit        -C, --disable-cas           : disable use of cas        -D, --describe-stats        : print stats description and exit        -S, --show-sizes            : print slab and item struct sizes and exit        -o, --output=S              : set the logging file (default: stderr)        -v, --verbosity=N           : set the logging level (default: 5, min: 0, max: 11)        -A, --stats-aggr-interval=N : set the stats aggregation interval in usec (default: 100000 usec)        -t, --threads=N             : set number of threads to use (default: 4)        -P, --pidfile=S             : set the pid file (default: off)        -u, --user=S                : set user identity when run as root (default: off)        -x, --klog-entry=N          : set the command logging entry number per thread (default: 512)        -X, --klog-file=S           : set the command logging file (default: off)        -R, --max-requests=N        : set the maximum number of requests per event (default: 20)        -c, --max-conns=N           : set the maximum simultaneous connections (default: 1024)        -b, --backlog=N             : set the backlog queue limit (default 1024)        -p, --port=N                : set the tcp port to listen on (default: 11211)        -U, --udp-port=N            : set the udp port to listen on (default: 11211)        -l, --interface=S           : set the interface to listen on (default: all)        -s, --unix-path=S           : set the unix socket path to listen on (default: off)        -a, --access-mask=O         : set the access mask for unix socket in octal (default: 0700)        -M, --eviction-strategy=N   : set the eviction strategy on OOM (default: 2, random)        -f, --factor=D              : set the growth factor of slab item sizes (default: 1.25)        -m, --max-memory=N          : set the maximum memory to use for all items in MB (default: 64 MB)        -n, --min-item-chunk-size=N : set the minimum item chunk size in bytes (default: 72 bytes)        -I, --slab-size=N           : set slab size in bytes (default: 1048576 bytes)        -z, --slab-profile=S        : set the profile of slab item chunk sizes (default: off)    ## Features    * Supports the complete memcached ASCII protocol.  * Supports tcp, udp and unix domain sockets.  * Observability through lock-less stats collection and klogger.  * Pluggable eviction strategies.  * Easy debuggability through assertions and logging.    ## Slabs and Items    Memory in twemcache is organized into fixed sized slabs whose size is configured using the -I or --slab-size=N command-line argument. Every slab is carved into a collection of contiguous, equal size items. All slabs that are carved into items of a given size belong to a given slabclass. The number of slabclasses and the size of items they serve can be configured either from a geometric sequence with the inital item size set using -n or --min-item-chunk-size=N argument and growth ratio set using -f or --factor=D argument, or from a profile string set using -z or --slab-profile=S argument.    ## Eviction    Eviction is triggered when a cache reaches full memory capacity. This happens when all cached items are unexpired and there is no space available to store newer items. Twemcache supports the following eviction strategies, configured using the -M or --eviction-strategy=N command-line argument:    * No eviction (0) - don't evict, respond with server error reply.  * Item LRU eviction (1) - evict only existing items in the same slab class, least recently updated first; essentially a per-slabclass LRU eviction.  * Random eviction (2) - evict all items from a randomly chosen slab.  * Slab LRA eviction (4) - choose the least recently accessed slab, and evict all items from it to reuse the slab.  * Slab LRC eviction (8) - choose the least recently created slab, and evict all items from it to reuse the slab. Eviction ignores freeq & lruq to make sure the eviction follows the timestamp closely. Recommended if cache is updated on the write path.    Eviction strategies can be *stacked*, in the order of higher to lower bit. For example, `-M 5` means that if slab LRA eviciton fails, Twemcache will try item LRU eviction.    ## Observability    ### Stats    Stats are the primary form of observability in twemcache. Stats collection in twemcache is lock-less in a sense that each worker thread only updates its thread-local metrics, and a background aggregator thread collects metrics from all threads periodically, holding only one thread-local lock at a time. Once aggregated, stats polling comes for free. There is a slight trade-off between how up-to-date stats are and how much burden stats collection puts on the system, which can be controlled by the aggregation interval -A or --stats-aggr-interval=N command-line argument. By default, the aggregation interval is set to 100 msec. You can set the aggregation interval at run time using `config aggregate <num>\r\n` command. Stats collection can be disabled at run time by passing a negative aggregation interval or at build time through the --disable-stats configure option.    Metrics exposed by twemcache are of three types - timestamp, counter and gauge and are collected both at the global level and per slab level. You can read about the description of all stats exposed by twemcache using the -D or --describe-stats command-line argument.    The following commands can be used to query stats from a running twemcache  * `stats\r\n`  * `stats settings\r\n`  * `stats slabs\r\n`  * `stats sizes\r\n`  * `stats cachedump <id> <limit>\r\n`    ### Klogger (Command Logger)    Command logger allows users to capture the details of every incoming request. Each line of the command log gives precise information on the client, the time when a request was received, the command header including the command, key, flags and data length, a return code, and reply message length. Few example klog lines look as follows:        172.25.135.205:55438 - [09/Jul/2012:18:15:45 -0700] ""set foo 0 0 3"" 1 6      172.25.135.205:55438 - [09/Jul/2012:18:15:46 -0700] ""get foo"" 0 14      172.25.135.205:55438 - [09/Jul/2012:18:15:57 -0700] ""incr num 1"" 3 9      172.25.135.205:55438 - [09/Jul/2012:18:16:05 -0700] ""set num 0 0 1"" 1 6      172.25.135.205:55438 - [09/Jul/2012:18:16:09 -0700] ""incr num 1"" 0 1      172.25.135.205:55438 - [09/Jul/2012:18:16:13 -0700] ""get num"" 0 12    The command logger supports lockless read/write into ring buffers, whose size can be configured with -x or --klog-entry=N command-line argument. Each worker thread logs to a thread-local buffer as they process incoming queries, and a background thread asynchronously dumps buffer contents to a file configured with -X or --klog-file=S command-line argument.    Since this feature has the capability of generating hundreds of MBs of data per minute, the use must be planned carefully. An enabled klog moduled can be started or stopped by sending `config klog run start\r\n` and `config klog run stop\r\n` respectively. To control the speed of log generation, the command logger also supports sampling. Sample rate can be set over with `config klog sampling <num>\r\n` command, which samples one of num commands.    ### Logging    Logging in twemcache is only available when it is built with logging enabled (--enable-debug=[full|yes|log]). By default logs are written to stderr. Twemcache can also be configured to write logs to a specific file through the -o or --output=S command-line argument.    On a running twemcache, we can turn log levels up and down by sending it SIGTTIN and SIGTTOU signals respectively and reopen log files by sending it SIGHUP signal. Logging levels can be set to a specific value using the `verbosity <num>\r\n` command.    ## Issues and Support    Have a bug? Please create an issue here on GitHub!    https://github.com/twitter/twemcache/issues    ## Versioning    For transparency and insight into our release cycle, releases are be numbered with the [semantic versioning](http://semver.org/) format: `<major>.<minor>.<patch>` and constructed with the following guidelines:    * Breaking backwards compatibility bumps the major  * New additions without breaking backwards compatibility bumps the minor  * Bug fixes and misc changes bump the patch    ## Other Work    * [twemproxy](https://github.com/twitter/twemproxy) - a fast, light-weight proxy for memcached.  * [twemperf](https://github.com/twitter/twemperf) - a tool for measuring memcached server performance.  * [twctop.rb](https://github.com/twitter/twemcache/blob/master/scripts/twctop.rb) - a tool like top for monitoring a cluster of twemcache servers.    ## Contributors    * Manju Rajashekhar ([@manju](https://twitter.com/manju))  * Yao Yue ([@thinkingfish](https://twitter.com/thinkingfish))    ## License    Copyright 2003, Danga Interactive, Inc.    Copyright 2012 Twitter, Inc.    Licensed under the New BSD License, see the LICENSE file. """
Big data;https://github.com/Freeboard/freeboard;"""freeboard  ==========    **freeÂ·board** (noun) *\ËˆfrÄ“-ËŒbÈ¯rd\*    1. the distance between the waterline and the main deck or weather deck of a ship or between the level of the water and the upper edge of the side of a small boat.  2. the act of freeing data from below the ""waterline"" and exposing it to the world.  3. a damn-sexy, open source real-time dashboard builder/viewer for IOT and other web mashups.    ### Demo  http://freeboard.github.io/freeboard    https://freeboard.io    ### Screenshots  ![Weather](https://raw.github.com/Freeboard/branding/master/screenshots/freeboard-screenshot-1.jpg)    ### What is It?    Freeboard is a turn-key HTML-based ""engine"" for dashboards. Besides a nice looking layout engine, it provides a plugin architecture for creating datasources (which fetch data) and widgets (which display data)â€” freeboard then does all the work to connect the two together. Another feature of freeboard is its ability to run entirely in the browser as a single-page static web app without the need for a server. The feature makes it extremely attractive as a front-end for embedded devices which may have limited ability to serve complex and dynamic web pages.    The code here is the client-side portion of what you see when you visit a freeboard at http://freeboard.io. It does not include any of the server-side code for user management, saving to a database or public/private functionalityâ€” this is left up to you to implement should you want to use freeboard as an online service.    ### How to Use    Freeboard can be run entirely from a local hard drive. Simply download/clone the repository and open index.html. When using Chrome, you may run into issues with CORS when accessing JSON based APIs if you load from your local hard-driveâ€” in this case you can switch to using JSONP or load index.html and run from a local or remote web server.    1. git clone https://github.com/Freeboard/freeboard.git  2. cd freeboard  3. npm install  4. grunt    Then run a index.html or index-dev.html through a webserver.    ### API    While freeboard runs as a stand-alone app out of the box, you can augment and control it from javascript with a simple API. All API calls are made on the `freeboard` singleton object.    -------    **freeboard.initialize(allowEdit, [callback])**    Must be called first to initialize freeboard.    > **allowEdit** (boolean) - Sets the initial state of freeboard to allow or disallow editing.    > **callback** (function) - Function that will be called back when freeboard has finished initializing.    -------    **freeboard.newDashboard()**    Clear the contents of the freeboard and initialize a new dashboard.    -------    **freeboard.serialize()**    Serializes the current dashboard and returns a javascript object.    -------    **freeboard.loadDashboard(configuration, [callback])**    Load the dashboard from a serialized dashboard object.    > **configuration** (object) - A javascript object containing the configuration of a dashboard. Normally this will be an object that has been created and saved via the `freeboard.serialize()` function.    > **callback** (function) - Function that will be called back when the dashboard has finished loading.    -------    **freeboard.setEditing(editing, animate)**    Programatically control the editing state of the of dashboard.    > **editing** (bool) - Set to true or false to modify the view-only or editing state of the board.    > **animate** (function) - Set to true or false to animate the modification of the editing state. This animates the top-tab dropdown (the part where you can edit datasources and such).    -------    **freeboard.isEditing()**    Returns boolean depending on whether the dashboard is in in the view-only or edit state.    -------    **freeboard.loadDatasourcePlugin(plugin)**    Register a datasource plugin. See http://freeboard.github.io/freeboard/docs/plugin_example.html for information on creating plugins.    > **plugin** (object) - A plugin definition object as defined at http://freeboard.github.io/freeboard/docs/plugin_example.html    -------    **freeboard.loadWidgetPlugin(plugin)**    Register a widget plugin. See http://freeboard.github.io/freeboard/docs/plugin_example.html for information on creating plugins.    > **plugin** (object) - A plugin definition object as defined at http://freeboard.github.io/freeboard/docs/plugin_example.html    -------    **freeboard.showLoadingIndicator(show)**    Show/hide the loading indicator. The loading indicator will display an indicator over the entire board that can be useful when you have some code that takes a while and you want to give a visual indication and to prevent the user from modifying the board.    > **show** (boolean) - Set to true or false to show or hide the loading indicator.    -------    **freeboard.showDialog(contentElement, title, okButtonTitle, cancelButtonTitle, okCallback)**    Show a styled dialog box with custom content.    > **contentElement** (DOM or jquery element) - The DOM or jquery element to display within the content of the dialog box.    > **title** (string) - The title of the dialog box displayed on the top left.    > **okButtonTitle** (string) - The string to display in the button that will be used as the OK button. A null or undefined value will result in no button being displayed.    > **cancelButtonTitle** (string) - The string to display in the button that will be used as the Cancel button. A null or undefined value will result in no button being displayed.    > **okCallback** (function) - A function that will be called if the user presses the OK button.    -------    **freeboard.getDatasourceSettings(datasourceName)**    Returns an object with the current settings for a datasource or null if no datasource with the given name is found.    > **datasourceName** (string) - The name of a datasource in the dashboard.    -------    **freeboard.setDatasourceSettings(datasourceName, settings)**    Updates settings on a datasource.    > **datasourceName** (string) - The name of a datasource in the dashboard.    > **settings** (object) - An object of key-value pairs for the settings of the datasource. The values specified here will be combined with the current settings, so you do not need specify every setting if you only want to update one. To get a list of possible settings for a datasource, consult the datasource documentation or code, or call the freeboard.getDatasourceSettings function.    -------    **freeboard.on(eventName, callback)**    Attach to a global freeboard event.    > **eventName** (string) - The name of a global event. The following events are supported:    > **""dashboard_loaded""** - Occurs after a dashboard has been loaded.    > **""initialized""** - Occurs after freeboard has first been initialized.    > **callback** (function) - The callback function to be called when the event occurs.    -------    ### Building Plugins    See http://freeboard.github.io/freeboard/docs/plugin_example.html for info on how to build plugins for freeboard.    ### Testing Plugins    Just edit index.html and add a link to your javascript file near the end of the head.js script loader, like:    ```javascript  ...  ""path/to/my/plugin/file.js"",  $(function()  { //DOM Ready      freeboard.initialize(true);  });  ```    ### Copyright     Copyright Â© 2013 Jim Heising (https://github.com/jheising)<br/>Copyright Â© 2013 Bug Labs, Inc. (http://buglabs.net)<br/>Licensed under the **MIT** license.    --- """
Big data;https://github.com/benedekrozemberczki/awesome-community-detection;"""# Awesome Community Detection Research Papers  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)  [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)  [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/awesome-community-detection.svg)](https://github.com/benedekrozemberczki/awesome-community-detection/archive/master.zip)  ![License](https://img.shields.io/github/license/benedekrozemberczki/awesome-community-detection.svg?color=blue) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)      A collection of community detection papers.    Similar collections about [graph classification](https://github.com/benedekrozemberczki/awesome-graph-classification), [classification/regression tree](https://github.com/benedekrozemberczki/awesome-decision-tree-papers), [fraud detection](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers), and [gradient boosting](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers) papers with implementations.    <p align=""center"">    <img width=""460"" src=""coms.png"">  </p>    ## Table of Contents      1. [Matrix Factorization](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/factorization.md)    2. [Deep Learning](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/deep_learning.md)   3. [Label Propagation, Percolation and Random Walks](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/label_propagation.md)   4. [Tensor Decomposition](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/tensor_decomposition.md)  5. [Spectral Methods](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/spectral.md)   6. [Temporal Methods](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/temporal.md)   7. [Cyclic Patterns](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/cyclic.md)  8. [Centrality and Cuts](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/centrality.md)   9. [Physics Inspired](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/physics.md)  10. [Block Models](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/blockmodels.md)  11. [Hypergraphs](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/hyper_graphs.md)   12. [Others](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/others.md)   13. [Libraries](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/libraries.md)    --------------    **License**    - [CC0 Universal](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/LICENSE) """
Big data;https://github.com/benedekrozemberczki/littleballoffur;"""![Version](https://badge.fury.io/py/littleballoffur.svg?style=plastic) [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/littleballoffur.svg)](https://github.com/benedekrozemberczki/littleballoffur/archive/master.zip) [![Arxiv](https://img.shields.io/badge/ArXiv-2006.04311-orange.svg)](https://arxiv.org/abs/2006.04311) [![build badge](https://github.com/benedekrozemberczki/littleballoffur/workflows/CI/badge.svg)](https://github.com/benedekrozemberczki/littleballoffur/actions?query=workflow%3ACI) [![coverage badge](https://codecov.io/gh/benedekrozemberczki/littleballoffur/branch/master/graph/badge.svg)](https://codecov.io/github/benedekrozemberczki/littleballoffur?branch=master) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)    <p align=""center"">    <img width=""90%"" src=""https://github.com/benedekrozemberczki/littleballoffur/blob/master/littleballoffurlogo.jpg?sanitize=true"" />  </p>    ------------------------------------------------------------------------------    **Little Ball of Fur** is a graph sampling extension library for Python.    Please look at the **[Documentation](https://little-ball-of-fur.readthedocs.io/)**, relevant **[Paper](https://arxiv.org/abs/2006.04311)**, **[Promo video](https://youtu.be/5OpjBqlPWME)** and **[External Resources](https://little-ball-of-fur.readthedocs.io/en/latest/notes/resources.html)**.    ------------------------------------------------------------------------------    **Little Ball of Fur** consists of methods that can sample from graph structured data. To put it simply it is a Swiss Army knife for graph sampling tasks. First, it includes a large variety of vertex, edge, and exploration sampling techniques. Second, it provides a unified application public interface which makes the application of sampling algorithms trivial for end-users. Implemented methods cover a wide range of networking ([Networking](https://link.springer.com/conference/networking), [INFOCOM](https://infocom2020.ieee-infocom.org/), [SIGCOMM](http://www.sigcomm.org/)) and data mining ([KDD](https://www.kdd.org/kdd2020/), [TKDD](https://dl.acm.org/journal/tkdd), [ICDE](http://www.wikicfp.com/cfp/program?id=1331&s=ICDE&f=International%20Conference%20on%20Data%20Engineering)) conferences, workshops, and pieces from prominent journals.     ------------------------------------------------------------------------------    **Citing**    If you find **Little Ball of Fur** useful in your research, please consider citing the following paper:    ```bibtex  @inproceedings{littleballoffur,                 title={{Little Ball of Fur: A Python Library for Graph Sampling}},                 author={Benedek Rozemberczki and Oliver Kiss and Rik Sarkar},                 year={2020},                 pages = {3133â€“3140},                 booktitle={Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)},                 organization={ACM},  }  ```  ------------------------------------------------------------------------------    **A simple example**    **Little Ball of Fur** makes using modern graph subsampling techniques quite easy (see [here](https://little-ball-of-fur.readthedocs.io/en/latest/notes/introduction.html) for the accompanying tutorial).  For example, this is all it takes to use [Diffusion Sampling](https://arxiv.org/abs/2001.07463) on a Watts-Strogatz graph:    ```python  import networkx as nx  from littleballoffur import DiffusionSampler    graph = nx.newman_watts_strogatz_graph(1000, 20, 0.05)    sampler = DiffusionSampler()    new_graph = sampler.sample(graph)  ```    --------------------------------------------------------------------------------    **Methods included**    In detail, the following sampling methods were implemented.    **Node Sampling**      * **[Degree Based Node Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/node_sampling.html#littleballoffur.node_sampling.degreebasedsampler.DegreeBasedSampler)** from Adamic *et al.*: [Search In Power-Law Networks](https://arxiv.org/abs/cs/0103016) (Physical Review E 2001)    * **[Random Node Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/node_sampling.html#littleballoffur.node_sampling.randomnodesampler.RandomNodeSampler)** from Stumpf *et al.*: [SubNets of Scale-Free Networks Are Not Scale-Free: Sampling Properties of Networks](https://www.pnas.org/content/102/12/4221) (PNAS 2005)    * **[PageRank Based Node Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/node_sampling.html#littleballoffur.node_sampling.pagerankbasedsampler.PageRankBasedSampler)** from Leskovec *et al.*: [Sampling From Large Graphs](https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf) (KDD 2006)    **Edge Sampling**    * **[Random Edge Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.randomedgesampler.RandomEdgeSampler)** from Krishnamurthy *et al.*: [Reducing Large Internet Topologies for Faster Simulations](http://www.cs.ucr.edu/~michalis/PAPERS/sampling-networking-05.pdf) (Networking 2005)    * **[Random Node-Edge Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.randomnodeedgesampler.RandomNodeEdgeSampler)** from Krishnamurthy *et al.*: [Reducing Large Internet Topologies for Faster Simulations](http://www.cs.ucr.edu/~michalis/PAPERS/sampling-networking-05.pdf) (Networking 2005)    * **[Hybrid Node-Edge Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.hybridnodeedgesampler.HybridNodeEdgeSampler)** from Krishnamurthy *et al.*: [Reducing Large Internet Topologies for Faster Simulations](http://www.cs.ucr.edu/~michalis/PAPERS/sampling-networking-05.pdf) (Networking 2005)    * **[Random Edge Sampler with Induction](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.randomedgesamplerwithinduction.RandomEdgeSamplerWithInduction)** from Ahmed *et al.*: [Network Sampling: From Static to Streaming Graphs](https://dl.acm.org/doi/10.1145/2601438) (TKDD 2013)    * **[Random Edge Sampler with Partial Induction](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.randomedgesamplerwithpartialinduction.RandomEdgeSamplerWithPartialInduction)** from Ahmed *et al.*: [Network Sampling: From Static to Streaming Graphs](https://dl.acm.org/doi/10.1145/2601438) (TKDD 2013)    **Exploration Based Sampling**    * **[Snowball Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.snowballsampler.SnowBallSampler)** from Goodman: [Snowball Sampling](https://projecteuclid.org/euclid.aoms/1177705148) (The Annals of Mathematical Statistics 1961)    * **[Loop-Erased Random Walk Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.looperasedrandomwalksampler.LoopErasedRandomWalkSampler)** from Wilson: [Generating Random Spanning Trees More Quickly Than the Cover Time](https://link.springer.com/chapter/10.1007/978-1-4612-2168-5_12) (STOC 1996)    * **[Forest Fire Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.forestfiresampler.ForestFireSampler)** from Leskovec *et al.*: [Graphs over Time: Densification Laws, Shrinking Diameters and Possible Explanations](https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf) (KDD 2005)    <details>  <summary><b>Expand to see all exploration samplers...</b></summary>      * **[Random Node-Neighbor Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.randomnodeneighborsampler.RandomNodeNeighborSampler)** from Leskovec *et al.*: [Sampling From Large Graphs](https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf) (KDD 2006)    * **[Random Walk With Restart Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.randomwalkwithrestartsampler.RandomWalkWithRestartSampler)** from Leskovec *et al.*: [Sampling From Large Graphs](https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf) (KDD 2006)    * **[Metropolis Hastings Random Walk Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.metropolishastingsrandomwalksampler.MetropolisHastingsRandomWalkSampler)** from Hubler *et al.*: [Metropolis Algorithms for Representative Subgraph Sampling](http://mlcb.is.tuebingen.mpg.de/Veroeffentlichungen/papers/HueBorKriGha08.pdf) (ICDM 2008)    * **[Random Walk Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.randomwalksampler.RandomWalkSampler)** from Gjoka *et al.*: [Walking in Facebook: A Case Study of Unbiased Sampling of OSNs](https://ieeexplore.ieee.org/document/5462078) (INFOCOM 2010)    * **[Random Walk With Jump Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.randomwalkwithjumpsampler.RandomWalkWithJumpSampler)** from Ribeiro *et al.*: [Estimating and Sampling Graphs with Multidimensional Random Walks](https://arxiv.org/abs/1002.1751) (SIGCOMM 2010)    * **[Frontier Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.frontiersampler.FrontierSampler)** from Ribeiro *et al.*: [Estimating and Sampling Graphs with Multidimensional Random Walks](https://arxiv.org/abs/1002.1751) (SIGCOMM 2010)    * **[Community Structure Expansion Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.communitystructureexpansionsampler.CommunityStructureExpansionSampler)** from Maiya *et al.*: [Sampling Community Structure](http://arun.maiya.net/papers/maiya_etal-sampcomm.pdf) (WWW 2010)    * **[Non-Backtracking Random Walk Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.nonbacktrackingrandomwalksampler.NonBackTrackingRandomWalkSampler)** from Lee *et al.*: [Beyond Random Walk and Metropolis-Hastings Samplers: Why You Should Not Backtrack for Unbiased Graph Sampling](https://dl.acm.org/doi/10.1145/2318857.2254795) (SIGMETRICS 2012)    * **[Randomized Depth First Search Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.depthfirstsearchsampler.DepthFirstSearchSampler)** from Doerr *et al.*: [Metric Convergence in Social Network Sampling](https://dl.acm.org/doi/10.1145/2491159.2491168) (HotPlanet 2013)    * **[Randomized Breadth First Search Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.breadthfirstsearchsampler.BreadthFirstSearchSampler)** from Doerr *et al.*: [Metric Convergence in Social Network Sampling](https://dl.acm.org/doi/10.1145/2491159.2491168) (HotPlanet 2013)    * **[Rejection Constrained Metropolis Hastings Random Walk Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.metropolishastingsrandomwalksampler.MetropolisHastingsRandomWalkSampler)** from Li *et al.*: [On Random Walk Based Graph Sampling](https://ieeexplore.ieee.org/document/7113345) (ICDE 2015)    * **[Circulated Neighbors Random Walk Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.circulatedneighborsrandomwalksampler.CirculatedNeighborsRandomWalkSampler)** from Zhou *et al.*: [Leveraging History for Faster Sampling of Online Social Networks](https://dl.acm.org/doi/10.5555/2794367.2794373) (VLDB 2015)    * **[Shortest Path Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.shortestpathsampler.ShortestPathSampler)** from Rezvanian *et al.*: [Sampling Social Networks Using Shortest Paths](https://www.sciencedirect.com/science/article/pii/S0378437115000321) (Physica A 2015)    * **[Diffusion Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.diffusionsampler.DiffusionSampler)** from Rozemberczki *et al.*: [Fast Sequence-Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (Complex Networks 2018)    * **[Diffusion Tree Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.diffusiontreesampler.DiffusionTreeSampler)** from Rozemberczki *et al.*: [Fast Sequence-Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (Complex Networks 2018)    * **[Common Neighbor Aware Random Walk Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.commonneighborawarerandomwalksampler.CommonNeighborAwareRandomWalkSampler)** from Li *et al.*: [Walking with Perception: Efficient Random Walk Sampling via Common Neighbor Awareness](https://ieeexplore.ieee.org/document/8731555) (ICDE 2019)    * **[Spiky Ball Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.spikyballsampler.SpikyBallSampler)** from Ricaud *et al.*: [Spikyball Sampling: Exploring Large Networks via an Inhomogeneous Filtered Diffusion](https://www.mdpi.com/1999-4893/13/11/275) (Algorithms 2020)      </details>    Head over to our [documentation](https://little-ball-of-fur.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.  For a quick start, check out our [examples](https://github.com/benedekrozemberczki/littleballoffur/tree/master/examples.py).    If you notice anything unexpected, please open an [issue](https://github.com/benedekrozemberczki/littleballoffur/issues) and let us know.  If you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/littleballoffur/issues).  We are motivated to constantly make **Little Ball of Fur** even better.      --------------------------------------------------------------------------------    **Installation**    **Little Ball of Fur** can be installed with the following pip command.    ```sh  $ pip install littleballoffur  ```    As we create new releases frequently, upgrading the package casually might be beneficial.    ```sh  $ pip install littleballoffur --upgrade  ```    --------------------------------------------------------------------------    **Running examples**    As part of the documentation we provide a number of use cases to show how to use various sampling techniques. These can accessed [here](https://little-ball-of-fur.readthedocs.io/en/latest/notes/introduction.html) with detailed explanations.      Besides the case studies we provide synthetic examples for each model. These can be tried out by running the scripts in the examples folder. You can try out the random walk sampling example by running:    ```sh  $ cd examples  $ python ./exploration_sampling/randomwalk_sampler.py  ```    -------------------------------------------------------------------------      **Running tests**    ```sh  $ python setup.py test  ```    -------------------------------------------------------------------------      **License**    - [GNU General Public License v3.0](https://github.com/benedekrozemberczki/littleballoffur/blob/master/LICENSE) """
Big data;https://github.com/biokoda/actordb;"""### ActorDB is a distributed SQL database...    with the scalability of a KV store, while keeping the query capabilities of a relational database.    ActorDB is ideal as a server side database for [apps](http://www.actordb.com/docs-examples.html#example_filesync). Think of running a large mail service, dropbox, evernote, etc. They all require server side storage for user data, but the vast majority of queries is within a specific user. With many users, the server side database can get very large. Using ActorDB you can keep a full relational database for every user and not be forced into painful scaling strategies that require you to throw away everything that makes relational databases good.    ActorDB is a database that does not hide sharding from you. It makes it explicit, so you can keep fully relational chunks (i.e. actors) for the 99% of your database queries.    Even if your data model is not easily partitioned, ActorDB has a powerful KV data type that you can use instead. An [ActorDB KV](http://www.actordb.com/docs-kvstore.html#about_kv_store) type is an sql table that is partitioned across all servers. That table can have sub tables linked to it using foreign keys.    You can run queries or transactions on a single actor or across any number of actors. ActorDB can run on a single server or many servers. Writing to one actor is completely independent of writes to another actor, unless they are participating in the same transaction.    Servers can be added and schema can be updated at any time while the database is running.    Homepage: http://www.actordb.com/    For any questions you can use: https://gitter.im/actordb/    ActorDB is:    *   A distributed relational SQL database.  *   Consistent (not eventually consistent).  *   Distributed.  *   Redundant.  *   Massively concurrent.  *   No single point of failure.  *   ACID.  *   Connectable over MySQL protocol and [Thrift](https://github.com/biokoda/actordb/blob/master/adbt.thrift).  *   Replicated safely using the Raft distributed consensus algorithm.    Advantages    *   Complete horizontal scalability. All nodes are equivalent and you can have as many nodes as you need.  *   Full featured ACID database.  *   Suitable for very large datasets over many actors and servers.  *   No special drivers needed. Use the mysql driver of your language of choice.  *   Easy to configure and administer.  *   No global locks. Only the actors (one or many) involved in a transaction are locked during a write. All other actors are unaffected.  *   Uses stable reliable SQL and storage engines: SQLite on top of LMDB.  *   Inherits SQLite features like JSON support and common table expressions.    ### Would you like to contribute?    What we would most like to see is more client libraries on top of Thrift. Thrift generated code can be a bit verbose. Generally it is much nicer to implement an interface to it that hides some boilerplate code and uses nicer types.    Also if you have any ideas, thoughts on possible improvements or bugs to report, contact us using github issues.    So if you're interested in contributing. Use your language of choice. Generate a thrift interface using our [adbt.thrift](https://github.com/biokoda/actordb/blob/master/adbt.thrift), then write a clean interface to it.    We will list any outside contributions here.    ### Learn more    Documentation: http://www.actordb.com/docs-about.html    Story: http://blog.biokoda.com/post/112206754025/why-we-built-actordb    How SQLite runs on top of LMDB: http://blog.biokoda.com/post/133121776825/actordb-how-and-why-we-run-sqlite-on-top-of-lmdb    How to configure and run: http://www.actordb.com/docs-configuration.html    Change log: https://github.com/biokoda/actordb/blob/master/CHANGES.md    ### Client libs    Erlang: https://github.com/biokoda/actordb_client    .NET 2.0: https://github.com/hq-io/actordb-net    ### Builds    **ubuntu/debian package (64bit)**    https://dzbscw1ubdtyw.cloudfront.net/actordb_0.10.29-1_amd64.deb    **osx package (64bit):**    https://dzbscw1ubdtyw.cloudfront.net/actordb-0.10.29-OSX-x86_64.tar.gz    **red hat/centos package (64bit):**    Centos 7: https://dzbscw1ubdtyw.cloudfront.net/actordb-0.10.29-1.el7.x86_64.rpm    **general linux build (64bit)**    https://dzbscw1ubdtyw.cloudfront.net/actordb-0.10.29-linux.tar.gz    **windows package (64bit):**    https://dzbscw1ubdtyw.cloudfront.net/actordb-0.10.25-win-x86_64.zip """
Big data;https://github.com/twitter/scalding;"""# Scalding    Scalding is a Scala library that makes it easy to specify Hadoop MapReduce jobs. Scalding is built on top of [Cascading](http://www.cascading.org/), a Java library that abstracts away low-level Hadoop details. Scalding is comparable to [Pig](http://pig.apache.org/), but offers tight integration with Scala, bringing advantages of Scala to your MapReduce jobs.    ![Scalding Logo](https://raw.github.com/twitter/scalding/develop/logo/scalding.png)    Current version: `0.15.0`    ## Word Count    Hadoop is a distributed system for counting words. Here is how it's done in Scalding.    ```scala  package com.twitter.scalding.examples    import com.twitter.scalding._    class WordCountJob(args: Args) extends Job(args) {    TypedPipe.from(TextLine(args(""input"")))      .flatMap { line => tokenize(line) }      .groupBy { word => word } // use each word for a key      .size // in each group, get the size      .write(TypedTsv[(String, Long)](args(""output"")))      // Split a piece of text into individual words.    def tokenize(text: String): Array[String] = {      // Lowercase each word and remove punctuation.      text.toLowerCase.replaceAll(""[^a-zA-Z0-9\\s]"", """").split(""\\s+"")    }  }  ```    Notice that the `tokenize` function, which is standard Scala, integrates naturally with the rest of the MapReduce job. This is a very powerful feature of Scalding. (Compare it to the use of UDFs in Pig.)    You can find more example code under [examples/](https://github.com/twitter/scalding/tree/master/scalding-core/src/main/scala/com/twitter/scalding/examples). If you're interested in comparing Scalding to other languages, see our [Rosetta Code page](https://github.com/twitter/scalding/wiki/Rosetta-Code), which has several MapReduce tasks in Scalding and other frameworks (e.g., Pig and Hadoop Streaming).    ## Documentation and Getting Started    * [**Getting Started**](https://github.com/twitter/scalding/wiki/Getting-Started) page on the [Scalding Wiki](https://github.com/twitter/scalding/wiki)  * [Scalding Scaladocs](http://twitter.github.com/scalding) provide details beyond the API References. Prefer using this as it's always up to date.  * [**REPL in Wonderland**](https://gist.github.com/johnynek/a47699caa62f4f38a3e2) a hands-on tour of the    scalding REPL requiring only git and java installed.  * [**Runnable tutorials**](https://github.com/twitter/scalding/tree/master/tutorial) in the source.  * The API Reference, including many example Scalding snippets:    * [Type-safe API Reference](https://github.com/twitter/scalding/wiki/Type-safe-api-reference)    * [Fields-based API Reference](https://github.com/twitter/scalding/wiki/Fields-based-API-Reference)  * The Matrix Library provides a way of working with key-attribute-value scalding pipes:    * The [Introduction to Matrix Library](https://github.com/twitter/scalding/wiki/Introduction-to-Matrix-Library) contains an overview and a ""getting started"" example    * The [Matrix API Reference](https://github.com/twitter/scalding/wiki/Matrix-API-Reference) contains the Matrix Library API reference with examples  * [**Introduction to Scalding Execution**](https://github.com/twitter/scalding/wiki/Calling-Scalding-from-inside-your-application) contains general rules and examples of calling Scalding from inside another application.    Please feel free to use the beautiful [Scalding logo](https://drive.google.com/folderview?id=0B3i3pDi3yVgNbm9pMUdDcHFKVEk&usp=sharing) artwork anywhere.    ## Code of Conduct  This, and all github.com/twitter projects, are under the [Twitter Open Source Code of Conduct](https://engineering.twitter.com/opensource/code-of-conduct). Additionally, see the [Typelevel Code of Conduct](http://typelevel.org/conduct) for specific examples of harassing behavior that are not tolerated.    ## Building  There is a script (called sbt) in the root that loads the correct sbt version to build:    1. ```./sbt update``` (takes 2 minutes or more)  2. ```./sbt test```  3. ```./sbt assembly``` (needed to make the jar used by the scald.rb script)    The test suite takes a while to run. When you're in sbt, here's a shortcut to run just one test:    ```> test-only com.twitter.scalding.FileSourceTest```    Please refer to [FAQ page](https://github.com/twitter/scalding/wiki/Frequently-asked-questions#issues-with-sbt) if you encounter problems when using sbt.    We use [Travis CI](http://travis-ci.org/) to verify the build:  [![Build Status](https://travis-ci.org/twitter/scalding.svg?branch=develop)](http://travis-ci.org/twitter/scalding)    We use [Coveralls](https://coveralls.io/r/twitter/scalding) for code coverage results:  [![Coverage Status](https://coveralls.io/repos/twitter/scalding/badge.png?branch=develop)](https://coveralls.io/r/twitter/scalding?branch=develop)    Scalding modules are available from maven central.    The current groupid and version for all modules is, respectively, `""com.twitter""` and  `0.16.0-RC1`.    Current published artifacts are    * `scalding-core_2.10`  * `scalding-args_2.10`  * `scalding-date_2.10`  * `scalding-commons_2.10`  * `scalding-avro_2.10`  * `scalding-parquet_2.10`  * `scalding-repl_2.10`      The suffix denotes the scala version.    ## Adopters    * Ebay  * Etsy  * Sharethrough  * Snowplow Analytics  * Soundcloud  * Twitter    To see a full list of users or to add yourself, see the [wiki](https://github.com/twitter/scalding/wiki/Powered-By)    ## Contact  For user questions or scalding development (internals, extending, release planning):  <https://groups.google.com/forum/#!forum/scalding-dev> (Google search also works as a first step)    In the remote possibility that there exist bugs in this code, please report them to:  <https://github.com/twitter/scalding/issues>    Follow [@Scalding](http://twitter.com/scalding) on Twitter for updates.    Chat: [![Gitter](https://badges.gitter.im/twitter/scalding.svg)](https://gitter.im/twitter/scalding?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)    ## Authors:  * Avi Bryant <http://twitter.com/avibryant>  * Oscar Boykin <http://twitter.com/posco>  * Argyris Zymnis <http://twitter.com/argyris>    Thanks for assistance and contributions:    * Sam Ritchie <http://twitter.com/sritchie>  * Aaron Siegel: <http://twitter.com/asiegel>  * Ian O'Connell <http://twitter.com/0x138>  * Alex Levenson <http://twitter.com/THISWILLWORK>  * Jonathan Coveney <http://twitter.com/jco>  * Kevin Lin <http://twitter.com/reconditesea>  * Brad Greenlee: <http://twitter.com/bgreenlee>  * Edwin Chen <http://twitter.com/edchedch>  * Arkajit Dey: <http://twitter.com/arkajit>  * Krishnan Raman: <http://twitter.com/dxbydt_jasq>  * Flavian Vasile <http://twitter.com/flavianv>  * Chris Wensel <http://twitter.com/cwensel>  * Ning Liang <http://twitter.com/ningliang>  * Dmitriy Ryaboy <http://twitter.com/squarecog>  * Dong Wang <http://twitter.com/dongwang218>  * Josh Attenberg <http://twitter.com/jattenberg>  * Juliet Hougland <https://twitter.com/j_houg>  * Eddie Xie <https://twitter.com/eddiex>    A full list of [contributors](https://github.com/twitter/scalding/graphs/contributors) can be found on GitHub.    ## License  Copyright 2013 Twitter, Inc.    Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 """
Big data;https://github.com/benedekrozemberczki/awesome-graph-classification;"""# Awesome Graph Classification  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)  [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)  ![License](https://img.shields.io/github/license/benedekrozemberczki/awesome-graph-embedding.svg?color=blue)  [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/awesome-graph-classification.svg)](https://github.com/benedekrozemberczki/awesome-graph-classification/archive/master.zip) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)     A collection of graph classification methods, covering embedding, deep learning, graph kernel and factorization papers with reference implementations.    Relevant graph classification benchmark datasets are available [[here]](https://github.com/shiruipan/graph_datasets).    Similar collections about [community detection](https://github.com/benedekrozemberczki/awesome-community-detection), [classification/regression tree](https://github.com/benedekrozemberczki/awesome-decision-tree-papers), [fraud detection](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers), [Monte Carlo tree search](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers), and [gradient boosting](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers) papers with implementations.    <p align=""center"">    <img width=""400"" src=""atlas.png"">  </p>    -----------------------------------------------------    ## Contents      1. [Matrix Factorization](https://github.com/benedekrozemberczki/awesome-graph-classification/blob/master/chapters/matrix_factorization.md)    2. [Spectral and Statistical Fingerprints](https://github.com/benedekrozemberczki/awesome-graph-classification/blob/master/chapters/fingerprints.md)  3. [Deep Learning](https://github.com/benedekrozemberczki/awesome-graph-classification/blob/master/chapters/deep_learning.md)    4. [Graph Kernels](https://github.com/benedekrozemberczki/awesome-graph-classification/blob/master/chapters/kernels.md)    -------------------------------------------------------    **License**    - [CC0 Universal](https://github.com/benedekrozemberczki/awesome-graph-classification/blob/master/LICENSE) """
Big data;https://github.com/Sdogruyol/awesome-ruby;"""# Awesome Ruby    A curated list of awesome Ruby frameworks, libraries and resources. Inspired by [awesome-php](https://github.com/ziadoz/awesome-php) & [awesome-python](https://github.com/vinta/awesome-python).    ## Contribution    Your Pull requests are welcome! Let's make this the awesomest resource for Ruby :purple_heart:    - [Awesome Ruby](#awesome-ruby)    - [Admin Panels](#admin-panels)    - [Anti-spam](#anti-spam)    - [Asset Management](#asset-management)    - [Audio](#audio)    - [Auditing](#auditing)    - [Authentication and OAuth](#authentication-and-oauth)    - [Build Tools](#build-tools)    - [Caching](#caching)    - [Cloud Services](#cloud-services)    - [CMS](#cms)    - [Code Analysis and Linter](#code-analysis-and-linter)    - [Command-line Tools](#command-line-tools)    - [Configuration](#configuration)    - [CSS and Styling](#css-and-styling)    - [Data Validation](#data-validation)    - [Data Visualization](#data-visualization)    - [Database Drivers](#database-drivers)    - [Date and Time](#date-and-time)    - [Debugging Tools](#debugging-tools)    - [DevOps Tools](#devops-tools)    - [Distribution](#distribution)    - [Documentation](#documentation)    - [Downloader](#downloader)    - [E-Commerce & Online Paying](#e-commerce--online-paying)    - [E-Mail](#e-mail)    - [Environment Management](#environment-management)    - [File Uploading](#file-uploading)    - [Feature Flipping](#feature-flipping)    - [Foreign Function Interface](#foreign-function-interface)    - [Forms](#forms)    - [Game Development](#game-development)    - [Geolocation](#geolocation)    - [GUI](#gui)    - [High Performance](#high-performance)    - [HTML/XML/CSS Manipulation](#htmlxmlcss-manipulation)    - [HTTP](#http)    - [Imagery](#imagery)    - [Internationalization](#internationalization)    - [Logging](#logging)    - [Machine Learning](#machine-learning)    - [MapReduce](#mapreduce)    - [Multi-tenancy](#multi-tenancy)    - [Natural Language Processing](#natural-language-processing)    - [Networking](#networking)    - [ORM](#orm)    - [Package Management](#package-management)    - [Presentation Tools](#presentation-tools)    - [Processes and Threads](#processes-and-threads)    - [Push Notification](#push-notification)    - [Queue](#queue)    - [Serverless](#serverless)    - [Spreadsheets](#spreadsheets)    - [RESTful API](#restful-api)    - [Science and Data Analysis](#science-and-data-analysis)    - [Search](#search)    - [Site Monitoring](#site-monitoring)    - [Starter Apps](#starter-apps)    - [Tagging](#tagging)    - [Template Engine](#template-engine)    - [Testing](#testing)    - [Text Processing](#text-processing)    - [Third-party APIs](#third-party-apis)    - [URL Manipulation](#url-manipulation)    - [Video](#video)    - [Web Content Extracting](#web-content-extracting)    - [Web Crawling](#web-crawling)    - [Web Frameworks](#web-frameworks)    - [Web Servers](#web-servers)    - [WebSocket](#websocket)  - [Miscellaneous](#miscellaneous)    - [Editor Plugins](#editor-plugins)  - [Resources](#resources)    - [People to Follow](#people-to-follow)  - [Other Awesome Lists](#other-awesome-lists)  - [Contributing](#contributing)      ## Admin Panels    *Libraries for administrative interfaces.*      * [active_admin](https://github.com/activeadmin/activeadmin) The administration framework for Ruby on Rails applications    * [rails_admin](https://github.com/sferik/rails_admin) A Rails engine that provides an easy-to-use interface for managing your data    * [administrate](https://github.com/thoughtbot/administrate) A framework for creating flexible, powerful admin dashboards in Rails.    ## Anti-spam    *Libraries for fighting spam.*      * [RubySpamAssassin](https://github.com/noeticpenguin/RubySpamAssassin) Kills Spam Dead. Perhaps before it's sent!    ## Asset Management    *Tools for managing, compressing and minifying website assets.*      * [sprockets](https://github.com/sstephenson/sprockets) Rack-based asset packaging system    * [rails-assets](https://github.com/rails-assets/rails-assets/) is the frictionless proxy between Bundler and Bower    ## Audio      * [seal](https://github.com/zhangsu/seal) A C library (with Ruby binding) for 3D audio rendering    ## Auditing      *Tools for logging changes to ActiveRecord models*      * [Audited](https://github.com/collectiveidea/audited) - Audited (formerly acts_as_audited) is an ORM extension that logs all changes to your Rails models.    * [Logidze](https://github.com/palkan/logidze) - Logs model changes via database triggers (PL/SQL functions). Fastest model diffs. PostgreSQL 9.5+ only.    ## Authentication and OAuth    *Libraries for implementing authentications schemes.*      * [Devise](https://github.com/plataformatec/devise) - Devise is a flexible authentication solution for Rails based on Warden    * [Omniauth](https://github.com/intridea/omniauth) - OmniAuth is a flexible authentication system utilizing Rack middleware    * [Warden](https://github.com/hassox/warden) - General Rack Authentication Framework    * [AuthLogic](https://github.com/binarylogic/authlogic) - A simple ruby authentication solution    * [Sorcery](https://github.com/NoamB/sorcery) - Magical authentication for Rails 3 & 4    * [CanCanCan](https://github.com/CanCanCommunity/cancancan) Authorization gem for Rails (continued version of CanCan from ryanb)    * [pundit](https://github.com/elabs/pundit) - Minimal authorization using object oriented design.    * [authority](https://github.com/nathanl/authority) - ORM neutral authorization.    * [doorkeeper](https://github.com/doorkeeper-gem/doorkeeper) An OAuth 2 provider for Rails    * [tiddle](https://github.com/adamniedzielski/tiddle/) - Devise strategy for token authentication in API-only Ruby on Rails applications    ## Build Tools    *Compile software from source code.*      * [teapot](https://github.com/ioquatix/teapot) A decentralised build tool for managing complex cross-platform projects    ## Caching    *Libraries for caching data.*      * [rack-cache](https://github.com/rtomayko/rack-cache) HTTP Caching for Ruby Web Apps    * [Dalli](https://github.com/mperham/dalli) - a high performance pure Ruby client for accessing memcached servers.    ## Cloud Services      * [fog](https://github.com/fog/fog) The Ruby cloud services library    * [aws-sdk-ruby](https://github.com/aws/aws-sdk-ruby) The official AWS SDK for Ruby    ## CMS    *Content management systems*      * [Refinery CMS](http://www.refinerycms.com) An extendable Ruby on Rails CMS that supports Rails 3.2 and 4.2    * [Comfortable Mexican Sofa](https://github.com/comfy/comfortable-mexican-sofa) A powerful Rails 4/5 CMS Engine    * [Browser](http://www.browsercms.org/) Humane Content Management for Rails    * [Locomotive](http://www.locomotivecms.com/) a brand new CMS system with super sexy UI and cool features    * [Radiant](http://radiantcms.org/) A no-fluff, open source content management system    * [Nesta](http://nestacms.com/) A lightweight CMS, implemented in Sinatra    * [alchemy_cms](https://github.com/AlchemyCMS/alchemy_cms) the most powerful, user friendly and flexible Rails CMS    * [weby](https://github.com/cercomp/weby) Newbie CMS in Ruby on Rails    ## Code Analysis and Linter    *Libraries and tools for analyzing, parsing and manipulation codebases.*    * [sonarlint-intellij](https://github.com/SonarSource/sonarlint-intellij) - An IDE extension that helps you detect and fix quality issues as you write code.    * [Rubocop](https://github.com/bbatsov/rubocop) - A Ruby static code analyzer, based on the community Ruby style guide.    * [ruby-lint](https://github.com/YorickPeterse/ruby-lint) - ruby-lint is a static code analysis tool for Ruby    * [brakeman](https://github.com/presidentbeef/brakeman) - Static analysis tool which checks Ruby on Rails applications for security vulnerabilities    * [reek](https://github.com/troessner/reek) - Code smell detector for Ruby    * [Breezer](https://github.com/lambda2/breezer) - Lock your Gemfile dependencies to safe versions.    ## Command-line Tools    *Libraries for building command-line application.*      * [Commander](http://visionmedia.github.io/commander/) - The complete solution for Ruby command-line executables    * [Thor](https://github.com/erikhuda/thor) - Thor is a toolkit for building powerful command-line interfaces    ## Configuration    *Libraries for storing configuration options.*    ## CSS and Styling      * [sass](https://github.com/sass/sass) A CSS preproccessor      * [sass-rails](https://github.com/rails/sass-rails) Rails stylesheet engine for Sass    * [less-rails](https://github.com/metaskills/less-rails) The dynamic stylesheet language for the Rails    * [compass](https://github.com/Compass/compass) A a Stylesheet Authoring Environment    * [bootstrap-sass](https://github.com/twbs/bootstrap-sass) Official Sass port of Bootstrap    * [foundation-rails](https://github.com/zurb/foundation-rails) Foundation for Rails    * [bootswatch-rails](https://github.com/maxim/bootswatch-rails) Bootswatches converted to SCSS ready to use in Rails    * [bourbon](https://github.com/thoughtbot/bourbon) A lightweight mixin library for Sass    ## Data Validation    *Libraries for validating data. Used for forms in many cases.*      * [kangal](https://github.com/lab2023/kangal) - Extended validation gem for email, subdomain, credit card, tax number etc    * [bin_checker](https://github.com/lab2023/bin_checker) - BIN validator for Turkish Banks    ## Data Visualization    *Libraries for visualizing data.*      * [prosperity](https://github.com/smathieu/prosperity) The easiest way to graph data from your Rails models    ## Database Drivers    *Libraries for connecting and operating databases.*      * Relational Databases      * [ruby-pg](https://bitbucket.org/ged/ruby-pg) Ruby interface to the PostgreSQL >= 8.4      * [mysql2](https://github.com/brianmario/mysql2) A modern, simple and very fast Mysql library for Ruby      * [sqlite3-ruby](https://github.com/sparklemotion/sqlite3-ruby) Ruby bindings for the SQLite3 embedded database      * NoSQL Databases    ## Date and Time    *Libraries for working with dates and times.*      * [stamp](https://github.com/jeremyw/stamp) Date and time formatting for humans    * [chronic](https://github.com/mojombo/chronic) Natural language date/time parser      ## Debugging Tools    *Libraries for debugging and developing.*      * [byebug](https://github.com/deivid-rodriguez/byebug) - Debugging in Ruby 2    * [debugger](https://github.com/cldwalker/debugger) - port of ruby-debug that works on 1.9.2 and 1.9.3    * [puts_debuggerer](https://github.com/AndyObtiva/puts_debuggerer) - Debugger-less debugging FTW    ## DevOps Tools    *Software and libraries for DevOps.*      * [Puppet](https://github.com/puppetlabs/puppet) - Server automation framework and application    * [Chef](https://github.com/chef/chef) - A systems integration framework, built to bring the benefits of configuration management to your entire infrastructure.    * [Vagrant](https://www.vagrantup.com/) - Vagrant is a tool for building and distributing development environments.    * [Capistrano](http://capistranorb.com/) - Remote multi-server automation tool    * [Mina](https://github.com/mina-deploy/mina) Really fast deployer and server automation tool    * [Nanobox](https://github.com/nanobox-io/nanobox) - A micro-PaaS (Î¼PaaS) for creating consistent, isolated, Ruby environments deployable anywhere https://nanobox.io.    ## Distribution    *Libraries to create packaged executables for release distribution.*      * [fpm](https://github.com/jordansissel/fpm)  Building packages for multiple platforms (deb, rpm, etc) with great ease and sanity.    ## Documentation    *Libraries for generating project documentation.*      * [Dictum](https://github.com/Wolox/dictum) - A tool that let's you create automatic documentation of your Rails API endpoints through your tests.    * [rdoc](https://github.com/rdoc/rdoc) HTML and online documentation for Ruby projects    * [yard](https://github.com/lsegal/yard) A Ruby Documentation tool    ## Downloader    *Libraries for downloading.*      * [GitHub Starred Repos Downloader](https://github.com/LeonardoCardoso/gsrd) -  gsrd downloads your public starred repos. Just in case you want to keep a backup of them from time to time.    ## E-Commerce & Online Paying      * [Active Merchant](https://github.com/activemerchant/active_merchant) - A simple payment abstraction library extracted from Shopify.    * [Spree](https://github.com/spree/spree) - A complete open source e-commerce solution for Ruby on Rails.    * [Square SDK](https://github.com/square/square-ruby-sdk) - Use this gem to integrate Square payments into your app and grow your business with Square APIs including Catalog, Customers, Employees, Inventory, Labor, Locations, and Orders.    * [PayPal Merchant SDK](https://github.com/paypal/merchant-sdk-ruby) - Provides Ruby APIs for processing payments, recurring payments, subscriptions and transactions using PayPal's Merchant APIs.    ## E-Mail    *Libraries for sending and parsing email.*      * [mail](https://github.com/mikel/mail) A Really Ruby Mail Library    * [mailman](https://github.com/mailman/mailman) An incoming mail processing microframework in Ruby    ## Environment Management    *Libraries for Ruby version and environment management.*      * [chruby](https://github.com/postmodern/chruby) - Changes the current Ruby    * [chgems](https://github.com/postmodern/chgems) - Chroot for RubyGems    * [rvm](https://rvm.io/) - Ruby Version Manager    * [rbenv](http://rbenv.org/) - Groom your appâ€™s Ruby environment    * [ruby-install](https://github.com/postmodern/ruby-install) - Installs Ruby, JRuby, Rubinius, MagLev or MRuby    * [ruby-build](https://github.com/sstephenson/ruby-build) - Compile and install Ruby    * [Nanobox](https://github.com/nanobox-io/nanobox) - A tool for creating isolated Ruby environments for consistency across teams and application stages (dev, staging, production, etc.) https://nanobox.io.    ## Error Handling    *Libraries for exception and error handling.*      * [Exception Notification](https://github.com/smartinez87/exception_notification) - A set of notifiers for sending notifications when errors occur in a Rack/Rails application    * [Errbit](http://errbit.github.io/errbit) - The open source, self-hosted error catcher    * [Airbrake](https://github.com/airbrake/airbrake) - The official Airbrake library for Ruby on Rails (and other Rack based frameworks)    * [Better Errors](https://github.com/charliesome/better_errors) - Better error page for Rack apps    ## File Uploading    *Libraries for handling file uploads.*      * [paperclip](https://github.com/thoughtbot/paperclip) Easy file attachment management for ActiveRecord    * [dragonfly](https://github.com/markevans/dragonfly) On-the-fly processing - suitable for image uploading in Rails, Sinatra and much more    * [carrierwave](https://github.com/carrierwaveuploader/carrierwave) Classier solution for file uploads for Rails, Sinatra and other Ruby web frameworks    * [attache](https://github.com/choonkeat/attache) - Yet another approach to file upload https://attache-demo.herokuapp.com    ## Feature flipping    *Libraries for flipping features*      * [abstract_feature_branch](https://github.com/AndyObtiva/abstract_feature_branch) a gem that enables developers to easily branch by abstraction    * [helioth](https://github.com/gmontard/helioth) Manage feature flipping and rollout    * [flipper](https://github.com/jnunemaker/flipper) feature flipping for ANYTHING    * [flip](https://github.com/pda/flip) Flip lets you declare and manage feature flags, backed by cookies (private testing) and database (site-wide)    * [rollout](https://github.com/FetLife/rollout) Feature flippers.    ## Foreign Function Interface    *Libraries for providing foreign function interface.*    ## Forms    *Libraries for working with forms.*      * [simple_form](https://github.com/plataformatec/simple_form) Forms made easy for Rails    * [formtastic](https://github.com/justinfrench/formtastic) A Rails form builder plugin with semantically rich and accessible markup    * [bootstrap_form](https://github.com/bootstrap-ruby/bootstrap_form) A Rails form builder that makes it super easy to integrate Bootstrap v4-style forms into your Rails application    ## Game Development    *Awesome game development libraries.*      * [Gosu](https://www.libgosu.org/) - A 2D game development library for the Ruby and C++ programming languages      ## Geolocation    *Libraries for geocoding addresses and working with latitudes and longitudes.*      * [geocoder](https://github.com/alexreisner/geocoder) Complete Ruby geocoding solution    * [Geokit](https://github.com/geokit/geokit) - Geokit gem provides geocoding and distance/heading calculations.    ## Git Tools    *Libraries for working with Git VCS*      * [katip](https://github.com/lab2023/katip) - Change logger for Git initialized projects    ## GUI    *Libraries for working with graphical user interface applications.*      * [glimmer](https://github.com/AndyObtiva/glimmer) Ruby Desktop Development GUI Library    * [shoes](https://github.com/shoes/shoes) A tiny graphical app kit for ruby    * [shoes4](https://github.com/shoes/shoes4) the next version of Shoes    ## High Performance    *Libraries for making Ruby faster.*      * [EventMachine](https://github.com/eventmachine/eventmachine) - EventMachine: fast, simple event-processing library for Ruby programs    * [Celluloid](https://celluloid.io/) - Actor-based concurrent object framework for Ruby. It has its own [awesomeness](https://github.com/sashaegorov/awesome-celluloid).    ## HTML/XML/CSS Manipulation    *Libraries for working with HTML, XML & CSS.*      * [Nokogiri](http://www.nokogiri.org/)    * [loofah](https://github.com/flavorjones/loofah) A general library for manipulating and transforming HTML/XML documents and fragments    ## HTTP    *Libraries for working with HTTP.*      * [httparty](https://github.com/jnunemaker/httparty) Makes http fun again!    * [faraday](https://github.com/lostisland/faraday) Simple, but flexible HTTP client library, with support for multiple backends.    * [http](https://github.com/httprb/http) A simple Ruby DSL for making HTTP requests    * [excon](https://github.com/excon/excon) Usable, fast, simple HTTP(S) 1.1 for Ruby    * [nestful](https://github.com/maccman/nestful) Simple Ruby HTTP/REST client with a sane API    * [response_code](https://github.com/torokmark/response_code) Response Code in readable way    ## Imagery    *Libraries for manipulating images.*      * [rmagick](https://github.com/rmagick/rmagick) An interface to the ImageMagick and GraphicsMagick image processing libraries      *  [minimagick](https://github.com/minimagick/minimagick) Minified version of rmagick    * [chunky_png](https://github.com/wvanbergen/chunky_png) Read/write access to PNG images in pure Ruby    * [image_optim](https://github.com/toy/image_optim) Optimize images using multiple utilities    * [magickly](https://github.com/afeld/magickly) image manipulation as a (plugin-able) service    ## Internationalization    *Libraries for woking with i18n.*      * [i18n](https://github.com/svenfuchs/i18n) - Basic internationalization(i18n) library for Ruby    * [globalize](https://github.com/globalize/globalize) Rails I18n de-facto standard library for ActiveRecord model/data translation    * [i18n-tasks](https://github.com/glebm/i18n-tasks) Manage translations in ruby applications with the awesome power of static analysis    ## Logging    *Libraries for generating and working with log files.*      * [Logstash](https://github.com/elastic/logstash) Logstash is a tool for managing events and logs.    ## Machine Learning    *Libraries for Machine Learning.*      * [PredictionIO Ruby SDK](https://github.com/PredictionIO/PredictionIO-Ruby-SDK) - The PredictionIO Ruby SDK provides a convenient API to quickly record your users' behavior and retrieve personalized predictions for them    * [m2cgen](https://github.com/BayesWitnesses/m2cgen) - A CLI tool to transpile trained classic ML models into a native Ruby code with zero dependencies.    ## MapReduce    *Frameworks and libraries for MapReduce.*    ## Multi-tenancy    *Libraries for managing multi-tenant apps.*    * [Apartment](https://github.com/influitive/apartment) - Database multi-tenancy for Rack (and Rails) applications    ## Natural Language Processing    *Libraries for working with human languages.*    * [Treat](https://github.com/louismullie/treat) - Treat is a toolkit for natural language processing and computational linguistics in Ruby    ## Networking    *Libraries for network programming.*    ## ORM    *Libraries that implement Object-Relational Mapping or data mapping techniques.*    * Relational Databases      * [ActiveRecord](https://www.ruby-toolbox.com/projects/activerecord) - Databases on Rails. Build a persistent domain model by mapping database tables to Ruby classes    * [DataMapper](http://datamapper.org/) - DataMapper is an Object Relational Mapper written in Ruby. The goal is to create an ORM which is fast, thread-safe and feature rich.    * [Sequel](http://sequel.jeremyevans.net/) - The Database Toolkit for Ruby    * NoSQL Databases      * [Mongoid](http://mongoid.org) - Mongoid (pronounced mann-goyd) is an Object-Document-Mapper (ODM) for MongoDB written in Ruby.    * [Ohm](https://github.com/soveran/ohm) - Object-Hash Mapping for Redis      ## Package Management    *Libraries for package and dependency management.*      * [RubyGems](https://rubygems.org/) - RubyGems is a package manager for the Ruby programming language that provides a standard format for distributing Ruby programs and libraries    * [Bundler](http://bundler.io) - Bundler provides a consistent environment for Ruby projects by tracking and installing the exact gems and versions that are needed    * [Homebrew](http://brew.sh) - Homebrew installs the stuff you need that Apple didnâ€™t    * [Homebrew Cask](http://caskroom.io/) - Cask provides a friendly homebrew-style CLI workflow for the administration of Mac applications distributed as binaries    ## Pagination      * [kaminari](https://github.com/amatsuda/kaminari) A Scope & Engine based, clean, powerful, customizable and sophisticated paginator    * [will_paginate](https://github.com/mislav/will_paginate) Pagination library for Rails 3, Sinatra, Merb, DataMapper, and more    * [order_query](https://github.com/glebm/order_query) Keyset pagination to find the next or previous record(s) relative to the current one efficiently, e.g. for infinite scroll.    ## PDF Processing      * [DocRaptor](https://github.com/DocRaptor/docraptor-ruby) Wrapper library for [DocRaptor's](https://docraptor.com) Ruby-based HTML-to-PDF API    * [wicked_pdf](https://github.com/mileszs/wicked_pdf) PDF generator (from HTML) plugin for Ruby on Rails    * [pdfkit](https://github.com/pdfkit/pdfkit) HTML+CSS to PDF using wkhtmltopdf    * [prawn](https://github.com/prawnpdf/prawn) Fast, Nimble PDF Writer for Ruby    * [InvoicePrinter](https://github.com/strzibny/invoice_printer) - Super simple PDF invoicing in Ruby (built on top of Prawn).    ## Presentation Tools      * [rabbit](https://github.com/rabbit-shocker/rabbit) A programable presentaton tool by Ruby    * [reveal-ck](https://github.com/jedcn/reveal-ck) Reveal.js presentations with a Ruby toolset    ## Processes and Threads    *Libraries for woking with processes or threads*      * [Parallel](https://github.com/grosser/parallel) - Ruby parallel processing made simple and fast    ## Profiling      * [bullet](https://github.com/flyerhzm/bullet) - help to kill N+1 queries and unused eager loading    ## Push Notification      * [Rpush](https://github.com/rpush/rpush) - The push notification service for Ruby.    * [apn_sender](https://github.com/arthurnn/apn_sender) - Background worker to send Apple Push Notifications over a persistent TCP socket.    * [Houston](https://github.com/nomad/houston) - A simple gem for sending Apple Push Notifications.    * [webpush](https://github.com/zaru/webpush) - Encryption Utilities for Web Push protocol    ## Queue    *Libraries for working with event and task queues.*      * [Resque](https://github.com/resque/resque) A Redis-backed Ruby library for creating background jobs, placing them on multiple queues.    * [Delayed::Job](https://github.com/tobi/delayed_job) â€” Database backed asynchronous priority queue.    * [Qu](https://github.com/bkeepers/qu) A Ruby library for queuing and processing background jobs.    * [Sidekiq](https://github.com/mperham/sidekiq) Simple, efficient background processing for Ruby    ## RESTful API    *Libraries for developing RESTful APIs.*      * [Grape](http://intridea.github.io/grape/) - An opinionated micro-framework for creating REST-like APIs in Ruby.    * [Rails::API](https://github.com/rails-api/rails-api) - Rails for API only applications    * [jbuilder](https://github.com/rails/jbuilder) - Create JSON structures via a Builder-style DSL    * [rabl](https://github.com/nesquena/rabl) - General Ruby templating with json, bson, xml, plist and msgpack support    * [active_model_serializers](https://github.com/rails-api/active_model_serializers) - ActiveModel::Serializer implementation and Rails hooks    * [oat](https://github.com/ismasan/oat) - Adapters-based API serializers with Hypermedia support for Ruby apps (HAL, Siren, JSONAPI).    * [APIcasso](https://github.com/ErvalhouS/APIcasso) - An abstract API design as a Rails-based mountable engine. RESTfullize your legacy code.    ## Serverless  * [FaaStRuby](https://faastruby.io) - Serverless Software Development Platform for Ruby and Crystal developers.    ## Spreadsheets    *Libraries for manipulating Excel, Google Spreadsheets, Numbers, OpenOffice and LibreOffice files*      * [spreadsheet](https://github.com/zdavatz/spreadsheet) - The Spreadsheet Library is designed to read and write Spreadsheet Documents.    * [caxlsx](https://github.com/caxlsx/caxlsx) - Caxlsx excels at helping you generate beautiful Office Open XML Spreadsheet documents.    * [caxlsx_rails](https://github.com/caxlsx/caxlsx_rails) - Axlsx-Rails provides an Axlsx renderer so you can move all your spreadsheet code from your controller into view files.    * [roo](https://github.com/roo-rb/roo) - Roo implements read access for all spreadsheet types and read/write access for Google spreadsheets.    * [google-spreadsheet-ruby](https://github.com/gimite/google-spreadsheet-ruby) - This is a library to read/write Google Spreadsheet.    * [rubyXL](https://github.com/weshatheleopard/rubyXL) - rubyXL is a gem which allows the parsing, creation, and manipulation of Microsoft Excel (.xlsx/.xlsm) Documents    * [Odf-report](https://github.com/sandrods/odf-report) - Generates ODF files, given a template (.odt) and data, replacing tags    * [simple_xlsx_writer](https://github.com/harvesthq/simple_xlsx_writer) - Just as the name says, simple writter for Office 2007+ Excel files    * [remote_table](https://github.com/seamusabshere/remote_table) - Open local or remote XLSX, XLS, ODS, CSV (comma separated), TSV (tab separated), other delimited, fixed-width files, and Google Docs.    * [acts_as_caxlsx](https://github.com/caxlsx/acts_as_caxlsx) - acts_as_caxlsx lets you turn any ActiveRecord::Base inheriting class into an excel spreadsheet.    * [activeadmin-caxlsx](https://github.com/caxlsx/activeadmin-caxlsx) - This gem uses caxlsx to provide excel/xlsx downloads for resources in Active Admin.    * [to_spreadsheet](https://github.com/glebm/to_spreadsheet) - Render XLSX from Rails using existing views    * [write_xlsx](https://github.com/cxn03651/write_xlsx) - write_xlsx is a gem to create a new file in the Excel 2007+ XLSX format.    * [excel_rails](https://github.com/asanghi/excel_rails) - Allows you to program spreadsheets using .rxls views    * [sheets](https://github.com/bspaulding/Sheets) - Work with spreadsheets easily in a native ruby format.    * [workbook](https://github.com/murb/workbook) - Workbook contains workbooks, as in a table, contains rows, contains cells, reads/writes excel, ods and csv and tab separated files...    * [Spreadsheet report](https://github.com/gnoso/spreadsheet_report) - Simple tool for running queries against ActiveRecord and putting them into a Google Spreadsheet.    * [oxcelix](https://github.com/gbiczo/oxcelix) - A fast Excel 2007/2010 (.xlsx) file parser that returns a collection of Matrix objects    * [wrap_excel](https://github.com/tomiacannondale/wrap_excel) - WrapExcel is to wrap the win32ole, and easy to use Excel operations with ruby. Detailed description please see the README.    * [write_xlsx_rails](https://github.com/maxd/write_xlsx_rails) - xlsx renderer for Rails base on write_xlsx gem    * [Fastsheet](https://github.com/dkkoval/fastsheet) - Fast spreadsheet reader using Rust native extensions.    ## Scheduling      * [whenever](https://github.com/javan/whenever) Cron jobs in Ruby    * [resque-scheduler](https://github.com/resque/resque-scheduler) A light-weight job scheduling system built on top of resque    * [rufus-scheduler](https://github.com/jmettraux/rufus-scheduler) Scheduler for Ruby    * [Clockwork](https://github.com/tomykaira/clockwork) Clockwork is a cron replacement. It runs as a lightweight, long-running Ruby process which sits alongside your web processes (Mongrel/Thin) and your worker processes (DJ/Resque/Minion/Stalker) to schedule recurring work at particular times or dates.    ## Science and Data Analysis    *Libraries for scientific computing and data analyzing.*    ## Search    *Libraries and software for indexing and performing search queries on data.*      * [Thinking Sphinx](https://github.com/pat/thinking-sphinx) - Sphinx plugin for ActiveRecord/Rails    * [elasticsearch-ruby](https://github.com/elastic/elasticsearch-ruby) - Ruby integrations for Elasticsearch    * [Searchkick](https://github.com/ankane/searchkick) - Intelligent search made easy    * [Algoliasearch Rails](https://github.com/algolia/algoliasearch-rails/) - AlgoliaSearch integration to your favorite ORM    * [PgSearch](https://github.com/Casecommons/pg_search) - PostgreSQL's full text search    * [Rroonga](https://github.com/ranguba/rroonga) - The Ruby bindings of Groonga    * [Sunspot](https://github.com/sunspot/sunspot) - Solr-powered search for Ruby objects    ## Site Monitoring    *Libs for analytics, monitoring*      * [rack-google-analytics](https://github.com/kangguru/rack-google-analytics) Simple Rack middleware for implementing google analytics tracking    * [DataDog](https://github.com/DataDog/dogapi-rb) A monitoring service for IT, operations and development teams    * [Instrumental](https://github.com/Instrumental/instrumental_agent-ruby) High-scale, non-blocking agent for [Instrumental](https://instrumentalapp.com) application monitoring    * [Keen IO](https://github.com/keenlabs/keen-gem) Build analytics features directly into your Ruby apps    ## Static Page Generation      * [jekyll](https://github.com/jekyll/jekyll) A blog-aware, static site generator in Ruby    * [middleman](https://github.com/middleman/middleman)    ## Starter Apps    *App templates for creating apps quickly*      * [suspenders](https://github.com/thoughtbot/suspenders) A Rails template with our standard defaults, ready to deploy to Heroku    * [ruby2-rails4-bootstrap-heroku](https://github.com/diowa/ruby2-rails4-bootstrap-heroku) A starter application based on Ruby 2, Rails 4 and Bootstrap for Sass, deployable on Heroku    * [rails-bootstrap](https://github.com/RailsApps/rails-bootstrap) Rails 4.1 starter app with the Bootstrap front-end framework    * [rails4-starterkit](https://github.com/starterkits/rails4-starterkit) Rails 4.1 starter app with production ready performance, security, and authentication    * [cybele](https://github.com/lab2023/cybele) - Rails 4.x template with responder, simple form, haml, exception notification, etc ...    ## Text Processing    *Libraries for parsing and manipulating texts.*      * General      * Specific Formats      * Parser        * [Yomu](https://github.com/Erol) - Read text and metadata from files and documents (.doc, .docx, .pages, .odt, .rtf, .pdf)    ## Tagging    *Libraries for tagging items.*      * [acts-as-taggable-on](https://github.com/mbleigh/acts-as-taggable-on) - A tagging plugin for Rails applications that allows for custom tagging along dynamic contexts.    ## Template Engine    *Libraries and tools for templating and lexing.*      * [Slim](https://github.com/slim-template/slim) A templating lang that reduce the syntax to the essential parts without becoming cryptic.      * [slim-rails](https://github.com/slim-template/slim-rails) Rails port of Slim lang    * [Haml](https://github.com/haml/haml) HTML Abstraction Markup Language - A Markup Haiku      * [haml-rails](https://github.com/indirect/haml-rails) Rails port of Haml lang    * [Tilt](https://github.com/rtomayko/tilt)    * [Liquid](https://github.com/Shopify/liquid)    ## Testing    *Libraries for testing codebases and generating test data.*      * Testing Frameworks      * [RSpec](http://rspec.info/) - BDD for Ruby      * [MiniTest](https://github.com/seattlerb/minitest) - minitest provides a complete suite of testing facilities supporting TDD, BDD, mocking, and benchmarking      * [Cucumber]         * [Cucumber Github](https://github.com/cucumber/cucumber/wiki) - Cucumber is a tool that executes plain-text functional descriptions as automated tests         * [Cucumber Site](https://cucumber.io/) - Behaviour Driven Development with elegacy and joy      * [Spinach](https://github.com/codegram/spinach) - Spinach is a high-level BDD framework that leverages the expressive Gherkin language (used by Cucumber) to help you define executable specifications of your application or library's acceptance criteria.      * [Rubytest](http://rubyworks.github.io/rubytest) - Rubytest is a testing meta-framework useful for creating highly customize test suites or building whole new test frameworks.         * [BRASS](http://rubyworks.github.io/brass) - Bare-metal Ruby assertion system standard used by Rubytest.         * [Lemon](http://rubyworks.github.io/lemon) - Strict unit test system built on top of Rubytest.      * [shoulda-matchers](https://github.com/thoughtbot/shoulda-matchers) - Collection of testing matchers extracted from Shoulda      * [capybara](https://github.com/jnicklas/capybara) - Acceptance test framework for web applications    * Mock      * [RSpec-mocks](https://github.com/rspec/rspec-mocks) - RSpec's 'test double' framework, with support for stubbing and mocking      * [Mocha](http://gofreerange.com/mocha/docs/) - Mocking and stubbing library with JMock/SchMock syntax, which allows mocking and stubbing of methods on real (non-mock) classes.      * [FlexMock](https://github.com/jimweirich/flexmock) - Flexible mocking for Ruby testing    * Fake Data      * [Faker](https://github.com/stympy/faker) - A library for generating fake data such as names, addresses, and phone numbers      * [ffaker](https://github.com/ffaker/ffaker) - Faker Refactored.      * [Forgery](https://github.com/sevenwire/forgery) - Easy and customizable generation of forged data.    * Code Coverage      * [simplecov](https://github.com/colszowka/simplecov) Code coverage for Ruby 1.9+ with a powerful configuration library and automatic merging of coverage    * Load Testing      * Error Handler    ## Third-party APIs    *Libraries for accessing third party APIs.*      * [koala](https://github.com/arsduo/koala) A lightweight, flexible library for Facebook    * [fb_graph](https://github.com/nov/fb_graph) A full-stack Facebook Graph API wrapper    * [twitter](https://github.com/sferik/twitter) A Ruby interface to the Twitter API    * [tweetstream](https://github.com/tweetstream/tweetstream) A simple library for consuming Twitter's Streaming API    * [gitlab](https://github.com/NARKOZ/gitlab) Ruby wrapper and CLI for the GitLab API    * [octokit.rb](https://github.com/octokit/octokit.rb) Ruby toolkit for the GitHub API    * [instagram](https://github.com/Instagram/instagram-ruby-gem) The official gem for the Instagram API    * [linkedin](https://github.com/hexgnu/linkedin) Ruby wrapper for the LinkedIn API    * [twilio-ruby](https://github.com/twilio/twilio-ruby) A Ruby gem for communicating with the Twilio API and generating TwiML    * [viewpoint-spws](https://github.com/zenchild/viewpoint-spws) A Microsoft Sharepoint Web Services library for Ruby.    * [youtube_it](https://github.com/kylejginavan/youtube_it) An object-oriented Ruby wrapper for the YouTube GData API    * [flickraw](https://github.com/hanklords/flickraw) Flickraw is a library to access flickr api    * [f00px](https://github.com/500px/f00px) Official 500px api ruby gem    * [rspotify](https://github.com/guilhermesad/rspotify) Ruby wrapper for the Spotify Web API    ## URL Manipulation    *Libraries for parsing URLs.*    ## Video    *Libraries for manipulating video and GIFs.*      * [streamio-ffmpeg](https://github.com/streamio/streamio-ffmpeg) Simple yet powerful ruby FFmpeg wrapper for reading metadata and transcoding movies    ## Web Content Extracting    *Libraries for extracting web contents.*    ## Web Crawling    *Libraries for scraping websites.*      * [upton](https://github.com/propublica/upton) A batteries-included framework for easy web-scraping    * [metainspector](https://github.com/jaimeiniesta/metainspector)    ## Web Frameworks    *Web development frameworks.*      * [Ruby On Rails](http://rubyonrails.org/) - Ruby on Rails is a full-stack web framework optimized for programmer happiness and sustainable productivity    * [Sinatra](http://www.sinatrarb.com/) - Sinatra is a DSL for quickly creating web applications in Ruby with minimal effort.    * [Padrino](http://www.padrinorb.com/) - The Godfather of Sinatra provides a full-stack agnostic framework on top of Sinatra    * [Cramp](http://cramp.in/) - Cramp is a fully asynchronous real-time web application framework in Ruby    * [Lotus](http://lotusrb.org/) - A newborn complete Ruby web framework that is simple, fast and lightweight.    * [Cuba](http://cuba.is/) - Cuba is a microframework for web development originally inspired by Rum, a tiny but powerful mapper for Rack applications.    * [Pakyow](https://pakyow.com/) - Pakyow is an open-source framework for the modern web. Build working software faster with a development process that remains friendly to both designers and developers. It's built for getting along.    * [Hyperstack](https://hyperstack.org/) - A complete Isomorphic Ruby Framework using React, Opal and Rails    ## Web Servers    *App server interface*      * [puma](https://github.com/puma/puma) A simple, fast, threaded, and highly concurrent HTTP 1.1 server for Ruby/Rack applications.    * [thin](https://github.com/macournoyer/thin) A thin and fast web server    * [trinidad](https://github.com/trinidad/trinidad) Run Rails or Rack applications within an embedded Apache Tomcat container.    * [unicorn](https://github.com/defunkt/unicorn) An HTTP server for Rack applications designed to only serve fast clients.    * [passenger](https://github.com/phusion/passenger) A modern web server and application server for Ruby, Python, and Node.js.    * [pow](https://github.com/basecamp/pow) Pow treats files and directories as ruby objects giving you more power and flexibility.    * [goliath](https://github.com/postrank-labs/goliath) is a non-blocking Ruby web server framework.    ## WebSocket    *Libraries for woking with WebSocket.*      * [Faye](http://faye.jcoglan.com/ruby.html) - Simple pub/sub messaging for the web    * [websocket-rails](https://github.com/websocket-rails/websocket-rails) - Plug and play websocket support for ruby on rails.    # Miscellaneous    *Useful libraries or tools that don't fit in the categories above.*      * [packetfu](https://github.com/packetfu/packetfu) A mid-level packet manipulation library for Ruby.    * [chatterbot](https://github.com/muffinista/chatterbot) A straightforward ruby-based Twitter Bot Framework, using OAuth to authenticate    * [sneakers](https://github.com/jondot/sneakers) A fast background processing framework for Ruby and RabbitMQ    * [ransack](https://github.com/activerecord-hackery/ransack) Object-based searching.    * [cinch](https://github.com/cinchrb/cinch) The IRC Bot Building Framework    * [pry](https://github.com/pry/pry) An IRB alternative and runtime developer console    * [rib](http://rib.godfat.org/) A lightweight and extensible IRB replacement    * [jazz_hands](https://github.com/nixme/jazz_hands/) Pry-based enhancements for the default Rails 3 and 4 consoles    * [awesome_print](https://github.com/awesome-print/awesome_print) A Ruby library that pretty prints Ruby objects in full color exposing their internal structure with proper indentation.    * [friendly_id](https://github.com/norman/friendly_id) Slugging and permalink plugins for ActiveRecord    * [backup](https://github.com/backup/backup) An elegant DSL in Ruby for performing backups on UNIX-like systems    * [kss](https://github.com/kneath/kss) Documenting CSS and generating styleguides    * [AASM](https://github.com/aasm/aasm) - A library for adding finite state machines to Ruby classes    * [JsonCompare](https://github.com/a2design-inc/json-compare) - Returns the difference between two JSON files    * [blankable](https://github.com/lab2023/blankable) - Adds blank slates to index view in Rails    * [tcmb_currency](https://github.com/lab2023/tcmb_currency) - T.C.M.B. currencies for Money Gem    * [enumerize](https://github.com/brainspec/enumerize) - Enumerated attributes with I18n and ActiveRecord/Mongoid support    * [lol_dba](https://github.com/plentz/lol_dba) - lol_dba is a small package of rake tasks that scan your application models and displays a list of columns that probably should be indexed.    * [annotate-models](https://github.com/ctran/annotate_models) - Annotate ActiveRecord models    * [fast_attributes](https://github.com/applift/fast_attributes) - FastAttributes adds attributes with their types to the class    * [Github Changelog Generator](https://github.com/skywinder/Github-Changelog-Generator) â€” automatically generate change log from your tags, issues, labels and pull requests on GitHub.    * [Letter Opener](https://github.com/ryanb/letter_opener) â€” Preview email in the default browser instead of sending it.    * [Auto HTML](https://github.com/dejan/auto_html) â€” Transforming URLs to appropriate resource (image, link, YouTube, Vimeo video,...).    * [OctoLinker](https://github.com/OctoLinker/browser-extension) - Navigate through projects on GitHub.com efficiently with the OctoLinker browser extension.    * [BetterDocs](https://github/khusnetdinov/betterdocs) - Documentation with collection practices in ruby. Good for new ruby developers and beginners.      ## Editor Plugins    *Plugins for various editors.*      * [vim-ruby](https://github.com/vim-ruby/vim-ruby) Vim/Ruby Configuration Files    * [vim-rails](https://github.com/tpope/vim-rails) rails.vim: Ruby on Rails power tools    # Resources    *Where to discover things (libraries, news e.g) about Ruby.*      * [The Ruby Toolbox](https://www.ruby-toolbox.com/)    * [RubyGems](https://rubygems.org/)    * [RubyDaily](http://rubydaily.org) - Community driven news    * [Ruby Weekly](http://rubyweekly.com/) - A free, onceâ€“weekly e-mail round-up of Ruby news and articles.    * [Ruby5](https://ruby5.codeschool.com/) - The latest news in the Ruby and Rails community    * [RubyFlow](http://www.rubyflow.com) - Ruby Programming Community Link Blog    * [Ruby Curated Resources](https://hackr.io/tutorials/learn-ruby)    ## People to Follow    *People in Ruby World*      * [Yukihiro ""Matz"" Matsumoto](https://twitter.com/yukihiro_matz) - Creator of Ruby lang    * [David Heinemeier Hansson](https://twitter.com/dhh) - Creator of Rails framework    * [Koichi Sasada](https://github.com/ko1) - Ruby core committer and the developer of YARV    * [Aaron Patterson](http://tenderlovemaking.com/) - Committer to Nokogiri, Ruby, and Ruby on Rails    * [Avdi Grimm](http://devblog.avdi.org/) - Host of Ruby Tapas webcasts    * [Aman Gupta](http://tmm1.net/)- Ruby core committer      # Other Awesome Lists    Other amazingly awesome lists can be found in the [awesome-awesomeness](https://github.com/bayandin/awesome-awesomeness) list. """
Big data;https://github.com/pinterest/secor;"""# Pinterest Secor    [![Build Status](https://travis-ci.org/pinterest/secor.svg)](https://travis-ci.org/pinterest/secor)    Secor is a service persisting [Kafka] logs to [Amazon S3], [Google Cloud Storage], [Microsoft Azure Blob Storage] and [Openstack Swift].    ## Key features ##    - **strong consistency**: as long as [Kafka] is not dropping messages (e.g., due to aggressive cleanup policy) before Secor is able to read them, it is guaranteed that each message will be saved in exactly one [S3] file. This property is not compromised by the notorious temporal inconsistency of [S3] caused by the [eventual consistency] model,    - **fault tolerance**: any component of Secor is allowed to crash at any given point without compromising data integrity,    - **load distribution**: Secor may be distributed across multiple machines,    - **horizontal scalability**: scaling the system out to handle more load is as easy as starting extra Secor processes. Reducing the resource footprint can be achieved by killing any of the running Secor processes. Neither ramping up nor down has any impact on data consistency,    - **output partitioning**: Secor parses incoming messages and puts them under partitioned s3 paths to enable direct import into systems like [Hive]. day,hour,minute level partitions are supported by secor    - **configurable upload policies**: commit points controlling when data is persisted in S3 are configured through size-based and time-based policies (e.g., upload data when local buffer reaches size of 100MB and at least once per hour),    - **monitoring**: metrics tracking various performance properties are exposed through [Ostrich], [Micrometer] and optionally exported to [OpenTSDB] / [statsD],    - **customizability**: external log message parser may be loaded by updating the configuration,    - **event transformation**: external message level transformation can be done by using customized class.    - **Qubole interface**: Secor connects to [Qubole] to add finalized output partitions to Hive tables.    ## Release Notes    Release Notes for past versions can be found in [RELEASE.md](RELEASE.md).    ## Setup/Configuration Guide    Setup/Configuration instruction is available in [README.setup.md](README.setup.md).    ### Secor configuration for Kubernetes/GKE environment    Extra Setup instruction for Kubernetes/GKE environment is available in [README.kubernetes.md](README.kubernetes.md).    ## Detailed design    Design details are available in [DESIGN.md](DESIGN.md).    ## License    Secor is distributed under [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0.html).    ## Maintainers    * [Pawel Garbacki](https://github.com/pgarbacki)    * [Henry Cai](https://github.com/HenryCaiHaiying)    ## Contributors    * [Andy Kramolisch](https://github.com/andykram)    * [Brenden Matthews](https://github.com/brndnmtthws)    * [Lucas Zago](https://github.com/zago)    * [James Green](https://github.com/jfgreen)    * [Praveen Murugesan](https://github.com/lefthandmagic)    * [Zack Dever](https://github.com/zackdever)    * [Leo Woessner](https://github.com/estezz)    * [Jerome Gagnon](https://github.com/jgagnon1)    * [Taichi Nakashima](https://github.com/tcnksm)    * [Lovenish Goyal](https://github.com/lovenishgoyal)    * [Ahsan Nabi Dar](https://github.com/ahsandar)    * [Ashish Kumar](https://github.com/ashubhumca)    * [Ashwin Sinha](https://github.com/tygrash)    * [Avi Chad-Friedman](https://github.com/achad4)      ## Companies who use Secor      * [Airbnb](https://www.airbnb.com)    * [Appsflyer](https://www.appsflyer.com)    * [Branch](http://branch.io)    * [Coupang](https://www.coupang.com)    * [Credit Karma](https://www.creditkarma.com)    * [GO-JEK](http://gojekengineering.com/)    * [Nextperf](http://www.nextperf.com)    * [PayTM](https://www.paytm.com)    * [Pinterest](https://www.pinterest.com)    * [Rakuten](http://techblog.rakuten.co.jp/)    * [Robinhood](http://www.robinhood.com/)    * [Simplaex](https://www.simplaex.com/)    * [Skyscanner](http://www.skyscanner.net)    * [Strava](https://www.strava.com)    * [TiVo](https://www.tivo.com)    * [VarageSale](http://www.varagesale.com)    * [Viacom](http://www.viacom.com)    * [Wego](https://www.wego.com)    * [Yelp](http://www.yelp.com)    * [Zalando](http://www.zalando.com)    * [Zapier](https://www.zapier.com)    ## Help    If you have any questions or comments, you can reach us at [secor-users@googlegroups.com](https://groups.google.com/forum/#!forum/secor-users)    [Kafka]:http://kafka.apache.org/  [Amazon S3]:http://aws.amazon.com/s3/  [Microsoft Azure Blob Storage]:https://azure.microsoft.com/en-us/services/storage/blobs/  [S3]:http://aws.amazon.com/s3/  [Google Cloud Storage]:https://cloud.google.com/storage/  [eventual consistency]:http://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyMode  [Hive]:http://hive.apache.org/  [Ostrich]: https://github.com/twitter/ostrich  [Micrometer]: https://micrometer.io  [OpenTSDB]: http://opentsdb.net/  [Qubole]: http://www.qubole.com/  [statsD]: https://github.com/etsy/statsd/  [Openstack Swift]: http://swift.openstack.org  [Protocol Buffers]: https://developers.google.com/protocol-buffers/  [Parquet]: https://parquet.apache.org/ """
Big data;https://github.com/Netflix/PigPen;"""![](logo.png)    PigPen is map-reduce for Clojure, or distributed Clojure. It compiles to [Apache Pig](http://pig.apache.org/) or [Cascading](http://www.cascading.org/) but you don't need to know much about either of them to use it.    # Getting Started, Tutorials & Documentation    Getting started with Clojure and PigPen is really easy.      * The [wiki](https://github.com/Netflix/PigPen/wiki) explains what PigPen does and why we made it    * The [tutorial](https://github.com/Netflix/PigPen/wiki/Tutorial) is the best way to get Clojure and PigPen installed and start writing queries    * The [full API](http://netflix.github.io/PigPen/pigpen.core.html) lists all of the operators with example usage    * [PigPen for Clojure users](https://github.com/Netflix/PigPen/wiki/Getting_Started_for_Clojure_Users) is great for Clojure users new to map-reduce    * [PigPen for Pig users](https://github.com/Netflix/PigPen/wiki/Getting_Started_for_Pig_Users) is great for Pig users new to Clojure    * [PigPen for Cascading users](https://github.com/Netflix/PigPen/wiki/Getting_Started_for_Cascading_Users) is great for Cascading users new to Clojure    _Note: It is strongly recommended to familiarize yourself with Clojure before using PigPen._    _Note: PigPen is **not** a Clojure wrapper for writing Pig scripts you can hand edit. While entirely possible, the resulting scripts are not intended for human consumption._    # Questions & Complaints      * pigpen-support@googlegroups.com    * Group discussion archives can be accessed [here](https://groups.google.com/forum/#!forum/pigpen-support)    # Artifacts    `pigpen` is available from Maven:    With Leiningen:    ``` clj  ;; core library  [com.netflix.pigpen/pigpen ""0.3.3""]    ;; pig support  [com.netflix.pigpen/pigpen-pig ""0.3.3""]    ;; cascading support  [com.netflix.pigpen/pigpen-cascading ""0.3.3""]    ;; rx support  [com.netflix.pigpen/pigpen-rx ""0.3.3""]  ```    The platform libraries all reference the core library, so you only need to reference the platform specific one that you require and the core library should be included transitively.    _Note: PigPen requires Clojure 1.5.1 or greater_    ## Parquet    To use the parquet loader, add this to your dependencies:    ``` clj  [com.netflix.pigpen/pigpen-parquet-pig ""0.3.3""]  ```    Here an example of how to write parquet data.    ``` clj  (require '[pigpen.core :as pig])  (require '[pigpen.parquet :as pqt])    ;;  ;; assuming that `data` is in tuples  ;;  ;; [[""John"" ""Smith"" 28]  ;;  [""Jane"" ""Doe""   21]]    (defn save-to-parquet    [output-file data]    (->> data         ;; turning tuples into a map         (pig/map (partial zipmap [:firstname :lastname :age]))         ;; then storing to Parquet files         (pqt/store-parquet          output-file          (pqt/message ""test-schema""                       ;; the field names here MUST match the map's keys                       (pqt/binary ""firstname"")                       (pqt/binary ""lastname"")                       (pqt/int64  ""age"")))))  ```      And how to load the records back:    ``` clj  (defn load-from-parquet    [input-file]    ;; the output will be a sequence of maps    (pqt/load-parquet     input-file     (pqt/message ""test-schema""                  (pqt/binary ""firstname"")                  (pqt/binary ""lastname"")                  (pqt/int64  ""age""))))  ```      And check out the [`pigpen.parquet`](http://netflix.github.io/PigPen/pigpen.parquet.html) namespace for usage.    _Note: Parquet is currently only supported by Pig_    ## Avro    To use the avro loader (alpha), add this to your dependencies:    ``` clj  [com.netflix.pigpen/pigpen-avro-pig ""0.3.3""]  ```    And check out the [`pigpen.avro`](http://netflix.github.io/PigPen/pigpen.avro.html) namespace for usage.    _Note: Avro is currently only supported by Pig_    # Release Notes      * 0.3.3 - 5/19/16      * Explicitly disable `*print-length*` and `*print-level*` when generating scripts      * Add a better error message for storage types that expect a map with keywords    * 0.3.2 - 1/12/16      * Allow more types in generated pig scripts    * 0.3.1 - 10/19/15      * Update cascading version to 2.7.0      * Report correct pigpen version to concurrent      * Update nippy to 2.10.0 & tune performance    * 0.3.0 - 5/18/15      * No changes    * 0.3.0-rc-7 - 4/29/15      * Fixed bug in local mode where nils weren't handled consistently    * 0.3.0-rc.6 - 4/14/15      * Add local mode code eval memoization to avoid thrashing permgen      * Added [`pigpen.pig/set-options`](http://netflix.github.io/PigPen/pigpen.pig.html#var-set-options) command to explicitly set pig options in a script. This was previously available (though undocumented) by setting `{:pig-options {...}}` in any options block, but is now official.    * 0.3.0-rc.5 - 4/9/15      * Update core.async version    * 0.3.0-rc.4 - 4/8/15      * Memoize code evaluation when run in the cluster    * 0.3.0-rc.3 - 4/2/15      * Bugfixes    * 0.3.0-rc.2 - 3/30/15      * Parquet refactor. Local parquet loading no longer depends on Pig. Parquet schemas are now defined using Parquet classes.    * 0.3.0-rc.1 - 3/23/15      * Added Cascading support        * [`pigpen.cascading/generate-flow`](http://netflix.github.io/PigPen/pigpen.cascading.html#var-generate-flow) - Generate a cascading flow from a pigpen query        * [`pigpen.cascading/load-tap`](http://netflix.github.io/PigPen/pigpen.cascading.html#var-load-tap) - Load data from an existing cascading tap        * [`pigpen.cascading/store-tap`](http://netflix.github.io/PigPen/pigpen.cascading.html#var-store-tap) - Store data using an existing cascading tap      * Added [`pigpen.core/keys-fn`](http://netflix.github.io/PigPen/pigpen.core.html#var-keys-fn), a new convenience macro to support named anonymous functions. Like keys destructuring, but less verbose.      * New function based operators to build more dynamic scripts. These are function versions of all the core pigpen macros, but you have to handle quoting user code manually. These were previously available, but not officially supported. Now they're alpha, but supported and documented. See [`pigpen.core.fn`](http://netflix.github.io/PigPen/pigpen.core.fn.html)      * New lower-level operators to build custom storage and commands. These were previously available, but not officially supported. Now they're alpha, but supported and documented. See [`pigpen.core.op`](http://netflix.github.io/PigPen/pigpen.core.op.html)      * __*** Breaking Changes ***__        * `pigpen.core/script` is now [`pigpen.core/store-many`](http://netflix.github.io/PigPen/pigpen.core.html#var-store-many)        * `pigpen.core/generate-script` is now [`pigpen.pig/generate-script`](http://netflix.github.io/PigPen/pigpen.pig.html#var-generate-script)        * `pigpen.core/write-script` is now [`pigpen.pig/write-script`](http://netflix.github.io/PigPen/pigpen.pig.html#var-write-script)        * `pigpen.core/show` is now [`pigpen.viz/show`](http://netflix.github.io/PigPen/pigpen.viz.html#var-show) (requires dependency `[com.netflix.pigpen/pigpen-viz ""...""]`)        * `pig/dump` has changed. The old version was based on rx-java, and still exists as [`pigpen.rx/dump`](http://netflix.github.io/PigPen/pigpen.rx.html#var-dump). The replacement for [`pigpen.core/dump`](http://netflix.github.io/PigPen/pigpen.core.html#var-dump) is now entirely Clojure based. The Clojure version is better for unit tests and small data. All stages are evaluated eagerly, so the stack traces are simpler to read. The rx version is lazy, including the load-* commands. This means that you can load a large file, take a few rows, and process them without loading the entire file into memory. The downside is confusing stack traces and extra dependencies. See [here](https://github.com/Netflix/PigPen/wiki/Local_Evaluation) for more details.        * The interface for building custom loaders and storage has changed. See [here](https://github.com/Netflix/PigPen/wiki/Custom_Loaders) for more details. Please email pigpen-support@googlegroups.com with any questions.    * 0.2.15 - 2/20/15      * Include sources in jars    * 0.2.14 - 2/18/15      * Avro updates    * 0.2.13 - 1/19/15      * Added `load-avro` in the pigpen-avro project: http://avro.apache.org/      * Fixed the nRepl configuration; use `gradlew nRepl` to start an nRepl      * Exclude nested relations from closures    * 0.2.12 - 12/16/14      * Added `load-csv`, which allows for quoting per RFC 4180    * 0.2.11 - 10/24/14      * Fixed a bug (feature?) introduced by new rx version. Also upgraded to rc7. This would have only affected local mode where the data being read was faster than the code consuming it.    * 0.2.10 - 9/21/14      * Removed load-pig and store-pig. The pig data format is very bad and should not be used. If you used these and want them back, email pigpen-support@googlegroups.com and we'll put it into a separate jar. The jars required for this feature were causing conflicts elsewhere.      * Upgraded the following dependencies:        * org.clojure/clojure 1.5.1 -> 1.6.0 - this was also changed to a provided dependency, so you should be able to use any version greater than 1.5.1        * org.clojure/data.json 0.2.2 -> 0.2.5        * com.taoensso/nippy 2.6.0-RC1 -> 2.6.3        * clj-time 0.5.0 - no longer needed        * joda-time 2.2 -> 2.4 - pig needs this to run locally        * instaparse 1.2.14 - no longer needed        * io.reactivex/rxjava 0.9.2 -> 1.0.0-rc.1      * Fixed the rx limit bug. `pigpen.local/*max-load-records*` is no longer required.    * 0.2.9 - 9/16/14      * Fix a local-mode bug in `pigpen.fold/avg` where some collections would produce a NPE.      * Change fake pig delimiter to \n instead of \0. Allows for \0 to exist in input data.      * Remove 1000 record limit for local-mode. This was originally introduced to mitigate an rx bug. Until #61 is fixed, bind `pigpen.local/*max-load-records*` to the maximum number of records you want to read locally when reading large files. This now defaults to `nil` (no limit).      * Fix a local dispatch bug that would prevent loading folders locally    * 0.2.8 - 7/31/14      * Fix a bug in `load-tsv` and `load-lazy`    * 0.2.7 - 7/31/14 *** Don't use ***      * Fix `load-lazy` and speed up both `load-tsv` and `load-lazy`      * Convert to multi-project build      * Added pigpen-parquet with initial support for loading the Parquet format: https://github.com/apache/incubator-parquet-mr    * 0.2.6 - 6/17/14      * Minor optimization for local mode. The creation of a UDF was occurring for every value processed, causing it to run out of perm-gen space when processing large collections locally.      * Fix `(pig/return [])`      * Fix `(pig/dump (pig/reduce + (pig/return [])))`      * Fix `Long`s in scripts that are larger than an Integer      * Memoize local UDF instances per use of `pig/dump`      * The jar location in the generated script is now configurable. Use the `:pigpen-jar-location` option with `pig/generate-script` or `pig/write-script`.    * 0.2.5 - 4/9/14      * Remove `dump&show` and `dump&show+` in favor of `pigpen.oven/bake`. Call `bake` once and pass to as many outputs as you want. This is a breaking change, but I didn't increment the version because `dump&show` was just a tool to be used in the REPL. No scripts should break because of this change.      * Remove `dymp-async`. It appeared to be broken and was a bad idea from the start.      * Fix self-joins. This was a rare issue as a self join (with the same key) just duplicates data in a very expensive way.      * Clean up functional tests      * Fix `pigpen.oven/clean`. When it was pruning the graph, it was also removing REGISTER commands.    * 0.2.4 - 4/2/14      * Fix arity checking bug (affected varargs fns)      * Fix cases where an Algebraic fold function was falling back to the Accumulator interface, which was not supported. This affected using `cogroup` with `fold` over multiple relations.      * Fix debug mode (broken in 0.1.5)      * Change UDF initialization to not rely on memoization (caused stale data in REPL)      * Enable AOT. Improves cluster perf      * Add `:partition-by` option to `distinct`    * 0.2.3 - 3/27/14      * Added `load-json`, `store-json`, `load-string`, `store-string`      * Added `filter-by`, and `remove-by`    * 0.2.2 - 3/25/14      * Fixed bug in `pigpen.fold/vec`. This would also cause `fold/map` and `fold/filter` to not work when run in the cluster.    * 0.2.1 - 3/24/14      * Fixed bug when using `for` to generate scripts      * Fixed local mode bug with `map` followed by `reduce` or `fold`    * 0.2.0 - 3/3/14      * Added pigpen.fold - Note: this includes a breaking change in the join and cogroup syntax as follows:        ``` clj      ; before      (pig/join (foo on :f)                (bar on :b optional)                (fn [f b] ...))        ; after      (pig/join [(foo :on :f)                 (bar :on :b :type :optional)]                (fn [f b] ...))      ```        Each of the select clauses must now be wrapped in a vector - there is no longer a varargs overload to either of these forms. Within each of the select clauses, :on is now a keyword instead of a symbol, but a symbol will still work if used. If `optional` or `required` were used, they must be updated to `:type :optional` and `:type :required`, respectively.      * 0.1.5 - 2/17/14      * Performance improvements        * Implemented Pig's Accumulator interface        * Tuned nippy        * Reduced number of times data is serialized    * 0.1.4 - 1/31/14      * Fix sort bug in local mode    * 0.1.3 - 1/30/14      * Change Pig & Hadoop to be transitive dependencies      * Add support for consuming user code via closure    * 0.1.2 - 1/3/14      * Upgrade instaparse to 1.2.14    * 0.1.1 - 1/3/14      * Initial Release """
Big data;https://github.com/twitter/twemproxy;"""# twemproxy (nutcracker) [![Build Status](https://github.com/twitter/twemproxy/actions/workflows/main.yml/badge.svg?branch=master)](https://github.com/twitter/twemproxy/actions/workflows/main.yml?query=branch%3Amaster)    **twemproxy** (pronounced ""two-em-proxy""), aka **nutcracker** is a fast and lightweight proxy for [memcached](http://www.memcached.org/) and [redis](http://redis.io/) protocol. It was built primarily to reduce the number of connections to the caching servers on the backend. This, together with protocol pipelining and sharding enables you to horizontally scale your distributed caching architecture.    ## Build    To build twemproxy 0.5.0+ from [distribution tarball](https://github.com/twitter/twemproxy/releases):        $ ./configure      $ make      $ sudo make install    To build twemproxy 0.5.0+ from [distribution tarball](https://github.com/twitter/twemproxy/releases) in _debug mode_:        $ CFLAGS=""-ggdb3 -O0"" ./configure --enable-debug=full      $ make      $ sudo make install    To build twemproxy from source with _debug logs enabled_ and _assertions enabled_:        $ git clone git@github.com:twitter/twemproxy.git      $ cd twemproxy      $ autoreconf -fvi      $ ./configure --enable-debug=full      $ make      $ src/nutcracker -h    A quick checklist:    + Use newer version of gcc (older version of gcc has problems)  + Use CFLAGS=""-O1"" ./configure && make  + Use CFLAGS=""-O3 -fno-strict-aliasing"" ./configure && make  + `autoreconf -fvi && ./configure` needs `automake` and `libtool` to be installed    `make check` will run unit tests.    ### Older Releases    Distribution tarballs for older twemproxy releases (<= 0.4.1) can be found on [Google Drive](https://drive.google.com/open?id=0B6pVMMV5F5dfMUdJV25abllhUWM&authuser=0).  The build steps are the same (`./configure; make; sudo make install`).    ## Features    + Fast.  + Lightweight.  + Maintains persistent server connections.  + Keeps connection count on the backend caching servers low.  + Enables pipelining of requests and responses.  + Supports proxying to multiple servers.  + Supports multiple server pools simultaneously.  + Shard data automatically across multiple servers.  + Implements the complete [memcached ascii](notes/memcache.md) and [redis](notes/redis.md) protocol.  + Easy configuration of server pools through a YAML file.  + Supports multiple hashing modes including consistent hashing and distribution.  + Can be configured to disable nodes on failures.  + Observability via stats exposed on the stats monitoring port.  + Works with Linux, *BSD, OS X and SmartOS (Solaris)    ## Help        Usage: nutcracker [-?hVdDt] [-v verbosity level] [-o output file]                        [-c conf file] [-s stats port] [-a stats addr]                        [-i stats interval] [-p pid file] [-m mbuf size]        Options:        -h, --help             : this help        -V, --version          : show version and exit        -t, --test-conf        : test configuration for syntax errors and exit        -d, --daemonize        : run as a daemon        -D, --describe-stats   : print stats description and exit        -v, --verbose=N        : set logging level (default: 5, min: 0, max: 11)        -o, --output=S         : set logging file (default: stderr)        -c, --conf-file=S      : set configuration file (default: conf/nutcracker.yml)        -s, --stats-port=N     : set stats monitoring port (default: 22222)        -a, --stats-addr=S     : set stats monitoring ip (default: 0.0.0.0)        -i, --stats-interval=N : set stats aggregation interval in msec (default: 30000 msec)        -p, --pid-file=S       : set pid file (default: off)        -m, --mbuf-size=N      : set size of mbuf chunk in bytes (default: 16384 bytes)    ## Zero Copy    In twemproxy, all the memory for incoming requests and outgoing responses is allocated in mbuf. Mbuf enables zero-copy because the same buffer on which a request was received from the client is used for forwarding it to the server. Similarly the same mbuf on which a response was received from the server is used for forwarding it to the client.    Furthermore, memory for mbufs is managed using a reuse pool. This means that once mbuf is allocated, it is not deallocated, but just put back into the reuse pool. By default each mbuf chunk is set to 16K bytes in size. There is a trade-off between the mbuf size and number of concurrent connections twemproxy can support. A large mbuf size reduces the number of read syscalls made by twemproxy when reading requests or responses. However, with a large mbuf size, every active connection would use up 16K bytes of buffer which might be an issue when twemproxy is handling large number of concurrent connections from clients. When twemproxy is meant to handle a large number of concurrent client connections, you should set chunk size to a small value like 512 bytes using the -m or --mbuf-size=N argument.    ## Configuration    Twemproxy can be configured through a YAML file specified by the -c or --conf-file command-line argument on process start. The configuration file is used to specify the server pools and the servers within each pool that twemproxy manages. The configuration files parses and understands the following keys:    + **listen**: The listening address and port (name:port or ip:port) or an absolute path to sock file (e.g. /var/run/nutcracker.sock) for this server pool.  + **client_connections**: The maximum number of connections allowed from redis clients. Unlimited by default, though OS-imposed limitations will still apply.  + **hash**: The name of the hash function. Possible values are:    + one_at_a_time    + md5    + crc16    + crc32 (crc32 implementation compatible with [libmemcached](http://libmemcached.org/))    + crc32a (correct crc32 implementation as per the spec)    + fnv1_64    + fnv1a_64 (default)    + fnv1_32    + fnv1a_32    + hsieh    + murmur    + jenkins  + **hash_tag**: A two character string that specifies the part of the key used for hashing. Eg ""{}"" or ""$$"". [Hash tag](notes/recommendation.md#hash-tags) enable mapping different keys to the same server as long as the part of the key within the tag is the same.  + **distribution**: The key distribution mode for choosing backend servers based on the computed hash value. Possible values are:    + ketama (default, recommended. An implementation of https://en.wikipedia.org/wiki/Consistent_hashing)    + modula (use hash modulo number of servers to choose the backend)    + random (choose a random backend for each key of each request)  + **timeout**: The timeout value in msec that we wait for to establish a connection to the server or receive a response from a server. By default, we wait indefinitely.  + **backlog**: The TCP backlog argument. Defaults to 512.  + **tcpkeepalive**: A boolean value that controls if tcp keepalive is enabled for connections to servers. Defaults to false.  + **preconnect**: A boolean value that controls if twemproxy should preconnect to all the servers in this pool on process start. Defaults to false.  + **redis**: A boolean value that controls if a server pool speaks redis or memcached protocol. Defaults to false.  + **redis_auth**: Authenticate to the Redis server on connect.  + **redis_db**: The DB number to use on the pool servers. Defaults to 0. Note: Twemproxy will always present itself to clients as DB 0.  + **server_connections**: The maximum number of connections that can be opened to each server. By default, we open at most 1 server connection.  + **auto_eject_hosts**: A boolean value that controls if server should be ejected temporarily when it fails consecutively server_failure_limit times. See [liveness recommendations](notes/recommendation.md#liveness) for information. Defaults to false.  + **server_retry_timeout**: The timeout value in msec to wait for before retrying on a temporarily ejected server, when auto_eject_hosts is set to true. Defaults to 30000 msec.  + **server_failure_limit**: The number of consecutive failures on a server that would lead to it being temporarily ejected when auto_eject_hosts is set to true. Defaults to 2.  + **servers**: A list of server address, port and weight (name:port:weight or ip:port:weight) for this server pool.      For example, the configuration file in [conf/nutcracker.yml](conf/nutcracker.yml), also shown below, configures 5 server pools with names - _alpha_, _beta_, _gamma_, _delta_ and omega. Clients that intend to send requests to one of the 10 servers in pool delta connect to port 22124 on 127.0.0.1. Clients that intend to send request to one of 2 servers in pool omega connect to unix path /tmp/gamma. Requests sent to pool alpha and omega have no timeout and might require timeout functionality to be implemented on the client side. On the other hand, requests sent to pool beta, gamma and delta timeout after 400 msec, 400 msec and 100 msec respectively when no response is received from the server. Of the 5 server pools, only pools alpha, gamma and delta are configured to use server ejection and hence are resilient to server failures. All the 5 server pools use ketama consistent hashing for key distribution with the key hasher for pools alpha, beta, gamma and delta set to fnv1a_64 while that for pool omega set to hsieh. Also only pool beta uses [nodes names](notes/recommendation.md#node-names-for-consistent-hashing) for consistent hashing, while pool alpha, gamma, delta and omega use 'host:port:weight' for consistent hashing. Finally, only pool alpha and beta can speak the redis protocol, while pool gamma, delta and omega speak memcached protocol.        alpha:        listen: 127.0.0.1:22121        hash: fnv1a_64        distribution: ketama        auto_eject_hosts: true        redis: true        server_retry_timeout: 2000        server_failure_limit: 1        servers:         - 127.0.0.1:6379:1        beta:        listen: 127.0.0.1:22122        hash: fnv1a_64        hash_tag: ""{}""        distribution: ketama        auto_eject_hosts: false        timeout: 400        redis: true        servers:         - 127.0.0.1:6380:1 server1         - 127.0.0.1:6381:1 server2         - 127.0.0.1:6382:1 server3         - 127.0.0.1:6383:1 server4        gamma:        listen: 127.0.0.1:22123        hash: fnv1a_64        distribution: ketama        timeout: 400        backlog: 1024        preconnect: true        auto_eject_hosts: true        server_retry_timeout: 2000        server_failure_limit: 3        servers:         - 127.0.0.1:11212:1         - 127.0.0.1:11213:1        delta:        listen: 127.0.0.1:22124        hash: fnv1a_64        distribution: ketama        timeout: 100        auto_eject_hosts: true        server_retry_timeout: 2000        server_failure_limit: 1        servers:         - 127.0.0.1:11214:1         - 127.0.0.1:11215:1         - 127.0.0.1:11216:1         - 127.0.0.1:11217:1         - 127.0.0.1:11218:1         - 127.0.0.1:11219:1         - 127.0.0.1:11220:1         - 127.0.0.1:11221:1         - 127.0.0.1:11222:1         - 127.0.0.1:11223:1        omega:        listen: /tmp/gamma 0666        hash: hsieh        distribution: ketama        auto_eject_hosts: false        servers:         - 127.0.0.1:11214:100000         - 127.0.0.1:11215:1    Finally, to make writing a syntactically correct configuration file easier, twemproxy provides a command-line argument `-t` or `--test-conf` that can be used to test the YAML configuration file for any syntax error.    ## Observability    Observability in twemproxy is through logs and stats.    Twemproxy exposes stats at the granularity of server pool and servers per pool through the stats monitoring port by responding with the raw data over TCP. The stats are essentially JSON formatted key-value pairs, with the keys corresponding to counter names. By default stats are exposed on port 22222 and aggregated every 30 seconds. Both these values can be configured on program start using the `-c` or `--conf-file` and `-i` or `--stats-interval` command-line arguments respectively. You can print the description of all stats exported by  using the `-D` or `--describe-stats` command-line argument.        $ nutcracker --describe-stats        pool stats:        client_eof          ""# eof on client connections""        client_err          ""# errors on client connections""        client_connections  ""# active client connections""        server_ejects       ""# times backend server was ejected""        forward_error       ""# times we encountered a forwarding error""        fragments           ""# fragments created from a multi-vector request""        server stats:        server_eof          ""# eof on server connections""        server_err          ""# errors on server connections""        server_timedout     ""# timeouts on server connections""        server_connections  ""# active server connections""        requests            ""# requests""        request_bytes       ""total request bytes""        responses           ""# responses""        response_bytes      ""total response bytes""        in_queue            ""# requests in incoming queue""        in_queue_bytes      ""current request bytes in incoming queue""        out_queue           ""# requests in outgoing queue""        out_queue_bytes     ""current request bytes in outgoing queue""    See [`notes/debug.txt`](notes/debug.txt) for examples of how to read the stats from the stats port.    Logging in twemproxy is only available when twemproxy is built with logging enabled. By default logs are written to stderr. Twemproxy can also be configured to write logs to a specific file through the `-o` or `--output` command-line argument. On a running twemproxy, we can turn log levels up and down by sending it SIGTTIN and SIGTTOU signals respectively and reopen log files by sending it SIGHUP signal.    ## Pipelining    Twemproxy enables proxying multiple client connections onto one or few server connections. This architectural setup makes it ideal for pipelining requests and responses and hence saving on the round trip time.    For example, if twemproxy is proxying three client connections onto a single server and we get requests - `get key\r\n`, `set key 0 0 3\r\nval\r\n` and `delete key\r\n` on these three connections respectively, twemproxy would try to batch these requests and send them as a single message onto the server connection as `get key\r\nset key 0 0 3\r\nval\r\ndelete key\r\n`.    Pipelining is the reason why twemproxy ends up doing better in terms of throughput even though it introduces an extra hop between the client and server.    ## Deployment    If you are deploying twemproxy in production, you might consider reading through the [recommendation document](notes/recommendation.md) to understand the parameters you could tune in twemproxy to run it efficiently in the production environment.    ## Utils  + [collectd-plugin](https://github.com/bewie/collectd-twemproxy)  + [munin-plugin](https://github.com/eveiga/contrib/tree/nutcracker/plugins/nutcracker)  + [twemproxy-ganglia-module](https://github.com/ganglia/gmond_python_modules/tree/master/twemproxy)  + [nagios checks](https://github.com/wanelo/nagios-checks/blob/master/check_twemproxy)  + [circonus](https://github.com/wanelo-chef/nad-checks/blob/master/recipes/twemproxy.rb)  + [puppet module](https://github.com/wuakitv/puppet-twemproxy)  + [nutcracker-web](https://github.com/kontera-technologies/nutcracker-web)  + [redis-twemproxy agent](https://github.com/Stono/redis-twemproxy-agent)  + [sensu-metrics](https://github.com/sensu-plugins/sensu-plugins-twemproxy/blob/master/bin/metrics-twemproxy.rb)  + [redis-mgr](https://github.com/idning/redis-mgr)  + [smitty for twemproxy failover](https://github.com/areina/smitty)  + [Beholder, a Python agent for twemproxy failover](https://github.com/Serekh/beholder)  + [chef cookbook](https://supermarket.getchef.com/cookbooks/twemproxy)  + [twemsentinel](https://github.com/yak0/twemsentinel)    ## Companies using Twemproxy in Production  + [Twitter](https://twitter.com/)  + [Wikimedia](https://www.wikimedia.org/)  + [Pinterest](http://pinterest.com/)  + [Snapchat](http://www.snapchat.com/)  + [Flickr](https://www.flickr.com)  + [Yahoo!](https://www.yahoo.com)  + [Tumblr](https://www.tumblr.com/)  + [Vine](http://vine.co/)  + [Wayfair](http://www.wayfair.com/)  + [Kiip](http://www.kiip.me/)  + [Wuaki.tv](https://wuaki.tv/)  + [Wanelo](http://wanelo.com/)  + [Kontera](http://kontera.com/)  + [Bright](http://www.bright.com/)  + [56.com](http://www.56.com/)  + [Digg](http://digg.com/)  + [Gawkermedia](http://advertising.gawker.com/)  + [3scale.net](http://3scale.net)  + [Ooyala](http://www.ooyala.com)  + [Twitch](http://twitch.tv)  + [Socrata](http://www.socrata.com/)  + [Hootsuite](http://hootsuite.com/)  + [Trivago](http://www.trivago.com/)  + [Machinezone](http://www.machinezone.com)  + [Path](https://path.com)  + [AOL](http://engineering.aol.com/)  + [Soysuper](https://soysuper.com/)  + [Vinted](http://vinted.com/)  + [Poshmark](https://poshmark.com/)  + [FanDuel](https://www.fanduel.com/)  + [Bloomreach](http://bloomreach.com/)  + [Hootsuite](https://hootsuite.com)  + [Tradesy](https://www.tradesy.com/)  + [Uber](http://uber.com) ([details](http://highscalability.com/blog/2015/9/14/how-uber-scales-their-real-time-market-platform.html))  + [Greta](https://greta.io/)    ## Issues and Support    Have a bug or a question? Please create an issue here on GitHub!    https://github.com/twitter/twemproxy/issues    ## Committers    * Manju Rajashekhar ([@manju](https://twitter.com/manju))  * Lin Yang ([@idning](https://github.com/idning))  * Tyson Andre ([@TysonAndre](https://github.com/TysonAndre))    Thank you to all of our [contributors](https://github.com/twitter/twemproxy/graphs/contributors)!    ## License    Copyright 2012 Twitter, Inc.    Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 """
Big data;https://github.com/cayleygraph/cayley;"""<div align=""center"">    <a href=""https://github.com/cayleygraph/cayley"">      <img width=""200"" src=""https://github.com/cayleygraph/branding/raw/master/cayley_bottom.png"" alt=""Cayley"">    </a>  </div>    [![Build Status](https://travis-ci.com/cayleygraph/cayley.svg?branch=master)](https://travis-ci.com/cayleygraph/cayley)  [![Container Repository](https://img.shields.io/docker/cloud/build/cayleygraph/cayley ""Container Repository"")](https://hub.docker.com/r/cayleygraph/cayley)    Cayley is an open-source database for [Linked Data](https://www.w3.org/standards/semanticweb/data). It is inspired by the graph database behind Google's [Knowledge Graph](https://en.wikipedia.org/wiki/Knowledge_Graph) (formerly [Freebase](https://en.wikipedia.org/wiki/Freebase_(database))).    [![Get it from the Snap Store](https://snapcraft.io/static/images/badges/en/snap-store-white.svg)](https://snapcraft.io/cayley)    ## [Documentation](https://cayley.gitbook.io/cayley/)    ## Features    - Built-in query editor, visualizer and REPL  - Multiple query languages:    - [Gizmo](./docs/gizmoapi.md): query language inspired by [Gremlin](http://gremlindocs.com/)    - [GraphQL](./docs/graphql.md)-inspired query language    - [MQL](./docs/mql.md): simplified version for [Freebase](https://en.wikipedia.org/wiki/Freebase_(database)) fans  - Modular: easy to connect to your favorite programming languages and back-end stores  - Production ready: well tested and used by various companies for their production workloads  - Fast: optimized specifically for usage in applications    ### Performance    Rough performance testing shows that, on 2014 consumer hardware and an average disk, 134m quads in LevelDB is no problem and a multi-hop intersection query -- films starring X and Y -- takes ~150ms.    ## Community    - Website: [cayley.io](https://cayley.io)  - Slack: [cayleygraph.slack.com](https://cayleygraph.slack.com) -- Invite [here](https://cayley-slackin.herokuapp.com/)  - Discourse list: [discourse.cayley.io](https://discourse.cayley.io) (Also acts as mailing list, enable mailing list mode)  - Twitter: [@cayleygraph](https://twitter.com/cayleygraph)  - Involvement: [Contribute](./docs/contributing.md) """
Big data;https://github.com/tarantool/tarantool;"""# Tarantool    [![Actions Status][actions-badge]][actions-url]  [![Code Coverage][coverage-badge]][coverage-url]  [![Telegram][telegram-badge]][telegram-url]  [![GitHub Discussions][discussions-badge]][discussions-url]  [![Stack Overflow][stackoverflow-badge]][stackoverflow-url]    [Tarantool][tarantool-url] is an in-memory computing platform consisting of a  database and an application server.    It is distributed under [BSD 2-Clause][license] terms.    Key features of the application server:    * Heavily optimized Lua interpreter with incredibly fast tracing JIT compiler,    based on LuaJIT 2.1.  * Cooperative multitasking, non-blocking IO.  * [Persistent queues][queue].  * [Sharding][vshard].  * [Cluster and application management framework][cartridge].  * Access to external databases such as [MySQL][mysql] and [PostgreSQL][pg].  * A rich set of built-in and standalone [modules][modules].    Key features of the database:    * MessagePack data format and MessagePack based client-server protocol.  * Two data engines: 100% in-memory with complete WAL-based persistence and an    own implementation of LSM-tree, to use with large data sets.  * Multiple index types: HASH, TREE, RTREE, BITSET.  * Document oriented JSON path indexes.  * Asynchronous master-master replication.  * Synchronous quorum-based replication.  * RAFT-based automatic leader election for the single-leader configuration.  * Authentication and access control.  * ANSI SQL, including views, joins, referential and check constraints.  * [Connectors][connectors] for many programming languages.  * The database is a C extension of the application server and can be turned    off.    Supported platforms are Linux (x86_64, aarch64), Mac OS X (x86_64, M1), FreeBSD  (x86_64).    Tarantool is ideal for data-enriched components of scalable Web architecture:  queue servers, caches, stateful Web applications.    To download and install Tarantool as a binary package for your OS or using  Docker, please see the [download instructions][download].    To build Tarantool from source, see detailed [instructions][building] in the  Tarantool documentation.    To find modules, connectors and tools for Tarantool, check out our [Awesome  Tarantool][awesome-list] list.    Please report bugs to our [issue tracker][issue-tracker]. We also warmly  welcome your feedback on the [discussions][discussions-url] page and questions  on [Stack Overflow][stackoverflow-url].    We accept contributions via pull requests. Check out our [How to get  involved][get-involved] guide.    Thank you for your interest in Tarantool!    [actions-badge]: https://github.com/tarantool/tarantool/workflows/release/badge.svg  [actions-url]: https://github.com/tarantool/tarantool/actions  [coverage-badge]: https://coveralls.io/repos/github/tarantool/tarantool/badge.svg?branch=master  [coverage-url]: https://coveralls.io/github/tarantool/tarantool?branch=master  [telegram-badge]: https://img.shields.io/badge/Telegram-join%20chat-blue.svg  [telegram-url]: http://telegram.me/tarantool  [discussions-badge]: https://img.shields.io/github/discussions/tarantool/tarantool  [discussions-url]: https://github.com/tarantool/tarantool/discussions  [stackoverflow-badge]: https://img.shields.io/badge/stackoverflow-tarantool-orange.svg  [stackoverflow-url]: https://stackoverflow.com/questions/tagged/tarantool  [tarantool-url]: https://www.tarantool.io/en/  [license]: LICENSE  [modules]: https://www.tarantool.io/en/download/rocks  [queue]: https://github.com/tarantool/queue  [vshard]: https://github.com/tarantool/vshard  [cartridge]: https://github.com/tarantool/cartridge  [mysql]: https://github.com/tarantool/mysql  [pg]: https://github.com/tarantool/pg  [connectors]: https://www.tarantool.io/en/download/connectors  [download]: https://www.tarantool.io/en/download/  [building]: https://www.tarantool.io/en/doc/latest/dev_guide/building_from_source/  [issue-tracker]: https://github.com/tarantool/tarantool/issues  [get-involved]: https://www.tarantool.io/en/doc/latest/contributing/contributing/  [awesome-list]: https://github.com/tarantool/awesome-tarantool/ """
Big data;https://github.com/jnv/lists;"""# Lists    List of useful, silly and [awesome](#awesome-) lists curated on GitHub. Contributions welcome!    âœ¨ Now also available [in CSV](https://github.com/jnv/lists/blob/gh-pages/lists.csv)! âœ¨    - [Lists](#lists)    - [Non-technical](#non-technical)    - [Technical](#technical)      - [awesome-*](#awesome-)    - [Lists of lists](#lists-of-lists)      - [Lists of lists of lists](#lists-of-lists-of-lists)        - [Lists of lists of lists of lists](#lists-of-lists-of-lists-of-lists)          - [Lists of lists of lists of lists of lists](#lists-of-lists-of-lists-of-lists-of-lists)            - [Lists of lists of lists of lists of lists of lists](#lists-of-lists-of-lists-of-lists-of-lists-of-lists)            - [Lists of lists of lists of lists of lists of lists of lists](#lists-of-lists-of-lists-of-lists-of-lists-of-lists-of-lists)    - [License](#license)    <!-- lists-start -->    ## Non-technical    * [aksh](https://github.com/svaksha/aksh) â€“ Bibliography of STEM (Science, Technology, Engineering & Mathematics) resources and grey literature.  * [amas](https://github.com/sindresorhus/amas) â€“ Awesome & Marvelous Amas (Ask Me Anything) on GitHub  * [Annual-Reading-List](https://github.com/davidskeck/Annual-Reading-List) â€“ Things to read every year.  * [awesomebandnames](https://github.com/jnv/awesomebandnames) â€“ The open-source list of awesome band names.  * [awesome-belarus-online](https://github.com/Friz-zy/awesome-belarus-online) â€“ Useful belarusian online resources.  * [awesome-biology](https://github.com/raivivek/awesome-biology) â€“ Learning resources, research papers, tools and other resources related to Biology.  * [awesome-bitclout](https://github.com/barrymode/awesome-bitclout) â€“ BitClout is social media on a blockchain. Everyone gets their own coin.  * [awesome-board-games](https://github.com/edm00se/awesome-board-games) â€“ Awesome and exceptional board games    - https://awesomeboard.games  * [awesome-ethics](https://github.com/HussainAther/awesome-ethics)  * [awesome-fantasy](https://github.com/RichardLitt/awesome-fantasy) â€“ Fantasy literature worth reading.  * [awesome-gif](https://github.com/Kikobeats/awesome-gif) â€“ GIF /dÊ’/ links and resources.  * [awesome-glasgow](https://github.com/allyjweir/awesome-glasgow) â€“ Some highlights around Glasgow, Scotland.  * [awesome-hacking-locations](https://github.com/daviddias/awesome-hacking-locations) â€“ Hacking places, organised by Country and City, listing if it features power and wifi.  * [awesome-health](https://github.com/prabhic/awesome-health) â€“ Useful health resources.  * [awesome-images](https://github.com/heyalexej/awesome-images) â€“ Free (stock) photo resources for your projects.  * [awesome-kimchi](https://github.com/jeyraof/awesome-kimchi) â€“ Kimchi of the people, by the people, for the people.  * [awesome-lego](https://github.com/ad-si/awesome-lego)  * [awesome-lockpicking](https://github.com/fabacab/awesome-lockpicking) â€“ Guides, tools, and other resources related to the security and compromise of locks, safes, and keys.  * [awesome-maps](https://github.com/simsieg/awesome-maps) â€“ Various Online Maps  * [awesome-mental-health](https://github.com/dreamingechoes/awesome-mental-health) â€“ Articles, websites and resources about mental health in the software industry.    - https://dreamingechoes.github.io/awesome-mental-health  * [awesome-parasite](https://github.com/ecohealthalliance/awesome-parasite) â€“ Parasites and host-pathogen interactions.  * [awesome-philosophy](https://github.com/HussainAther/awesome-philosophy) â€“ Philosophy  * [awesome-reddit-channels](https://github.com/MadhuNimmo/awesome-reddit-channels) â€“ Reddit Channels every programmer must follow.  * [awesome-scifi](https://github.com/sindresorhus/awesome-scifi) â€“ Sci-Fi worth consuming.  * [awesome-speaking](https://github.com/matteofigus/awesome-speaking) â€“ Resources about public speaking  * [awesome-stock-resources](https://github.com/neutraltone/awesome-stock-resources) â€“ Stock photography, video and illustration websites.  * [awesome-theravada](https://github.com/johnjago/awesome-theravada) â€“ Theravada Buddhist teachings  * [awesome-uncopyright](https://github.com/johnjago/awesome-uncopyright) â€“ All things public domain  * [awesome-webcomics](https://github.com/dhamaniasad/awesome-webcomics)  * [baby-sleep](https://github.com/simple10/baby-sleep) â€“ Baby sleep guides curated from the best of the Internet.  * [bailfunds.github.io](https://github.com/bailfunds/bailfunds.github.io) â€“ Bail Funds for Protestors across the USA.    - https://bailfunds.github.io/  * [boardgames](https://gitlab.com/gamearians/boardgames) â€“ Boardgames and boardgame-related projects that can be found on GitHub.  * [chinese-poetry](https://github.com/chinese-poetry/chinese-poetry) _In Chinese_ â€“ The most comprehensive database of Chinese poetry    - http://shici.store  * [cocktails](https://github.com/balevine/cocktails) â€“ Cocktail Recipes  * [corporate-logos](https://github.com/marketreef/corporate-logos) â€“ Curated repo of publicly listed co. logos, identified by ticker. *Almost 1500 logos*  * [creative-commons-media](https://github.com/shime/creative-commons-media) â€“ Audio, graphics and other resources that provide media licensed under Creative Commons licenses.  * [discord-listings](https://github.com/angrymouse/discord-listings) â€“ Places where to promote Discord servers.  * [dissertation-tips](https://github.com/katychuang/dissertation-tips) â€“ Resources to help PhD students complete their dissertation successfully.  * [diversity-index](https://github.com/svaksha/diversity-index) â€“ Grants, scholarships and FA that encourages diversity in STEM fields aimed at half the world's population, Women!    - http://svaksha.github.io/diversity-index  * [diversity-twitter](https://github.com/gregorycoleman/diversity-twitter) â€“ Twitter feeds of interesting people to follow for Diversity & Inclusion  * [food](https://notabug.org/themusicgod1/food)  * [food-recipes](https://github.com/obfuscurity/food-recipes) â€“ Honest-to-goodness ""real food"" recipes  * [frequent-transit-maps](https://github.com/wwcline/list-of-frequent-transit-maps) â€“ Transit maps highlighting frequent all-day service  * [global-reports](https://github.com/andressoop/global-reports) â€“ Major global reports published by international organisations  * [guitarspecs](https://github.com/gitfrage/guitarspecs) â€“ Electric guitar's parts specs    - https://gitfrage.github.io/guitarspecs/  * [isaacs/reading-list](https://github.com/isaacs/reading-list) â€“ [isaac](https://github.com/isaacs)'s reading list.  * [lawrence-veggie](https://github.com/codysoyland/lawrence-veggie) â€“ Vegetarian/vegan restaurants in Lawrence, KS.  * [lawyersongithub](https://github.com/dpp/lawyersongithub) â€“ A club full of lawyers who also have GitHub accounts.  * [low-resource-languages](https://github.com/RichardLitt/low-resource-languages) â€“ Conservation, development, and documentation of endangered, minority, and low or under-resourced human languages.  * [Mind-Expanding-Books](https://github.com/hackerkid/Mind-Expanding-Books) â€“ :books: Books that will blow your mind    - http://books.vishnuks.com  * [mining-resources](https://github.com/Mining-Resources/mining-resources) â€“ Natural resources mining.  * [no-free-basics](https://github.com/net-neutrality/no-free-basics) â€“ Those who have spoken up against Facebook's â€œFree Basicsâ€    - https://net-neutrality.github.io/no-free-basics/  * [open-sustainable-technology](https://github.com/protontypes/open-sustainable-technology) â€“ Worldwide open technology projects preserving a stable climate, energy supply and vital natural resources.  * [plastic-free](https://github.com/IrosTheBeggar/plastic-free) â€“ Plastic-free products.  * [ProjectSoundtracks](https://github.com/sarthology/ProjectSoundtracks) â€“ Soundtracks to boost your Productivity and Focus.  * [PublicMedia](https://github.com/melodykramer/PublicMedia) â€“ Everything about public (broadcast) media.    - Also [an introduction to working with GitHub](https://melodykramer.github.io/2015/04/06/learning-github-without-one-line-of-code) for non-programmers.  * [recipes](https://github.com/bzimmerman/recipes) by @bzimmerman â€“ This repository contains tasty open-source recipes.  * [recipes](https://github.com/csswizardry/recipes) by @csswizardy â€“ Collection of things I like cooking  * [recipes](https://github.com/LarryMad/recipes) by @LarryMad  * [recipes](https://github.com/nofunsir/recipes) by @nofunsir  * [recipes](https://github.com/schacon/recipes) by @schacon  * [recipes](https://github.com/silizuo/recipes) _In Chinese and English_ by @silizuo  * [sf-vegetarian-restaurants](https://github.com/mojombo/sf-vegetarian-restaurants) â€“ Awesome vegetarian-friendly restaurants in SF  * [shelfies](https://github.com/kyro/shelfies) â€“ Bookshelves of awesome people, community-transcribed.  * [SiliconValleyThingsToDo](https://github.com/cjbarber/SiliconValleyThingsToDo) â€“ Things to do and activities within Silicon Valley.  * [stayinghomeclub](https://github.com/phildini/stayinghomeclub) â€“ All the companies working from home or events changed because of covid-19.    - https://stayinghome.club  * [Sustainable-Earth](https://github.com/bizz84/Sustainable-Earth) â€“ All things sustainable  * [tacofancy](https://github.com/sinker/tacofancy) â€“ community-driven taco repo. stars stars stars.  * [teesites](https://github.com/elder-cb/teesites) â€“ Great sites to buy awesome t-shirts and other cool stuff.      ## Technical    * [101](https://github.com/ojas/101) â€“ Resources on running a software biz.  * [10PL](https://github.com/nuprl/10PL) â€“ 10 papers that all PhD students in programming languages ought to know, for some value of 10.  * [1on1-questions](https://github.com/VGraupera/1on1-questions) â€“ 1 on 1 meeting questions.  * [30-seconds-of-code](https://github.com/30-seconds/30-seconds-of-code) â€“ JavaScript snippets you can understand in 30 seconds or less.    - https://30secondsofcode.org/  * [30-seconds-of-interviews](https://github.com/30-seconds/30-seconds-of-interviews) â€“ Common interview questions to help you prepare for your next interview.  * [a11yproject.com](https://github.com/a11yproject/a11yproject.com) â€“ A communityâ€“driven effort to make web accessibility easier.    - https://a11yproject.com  * [addinslist](https://github.com/daattali/addinslist) â€“ Useful [RStudio](https://www.rstudio.com/) addins  * [admesh-projects](https://github.com/admesh/admesh-projects) â€“ Projects using [ADMesh](https://github.com/admesh/admesh) (a triangulated solid meshes processor).  * [AI-reading-list](https://github.com/m0nologuer/AI-reading-list) â€“ Papers about Artificial Intelligence.  * [alexandria](https://github.com/alxgcrz/alexandria) _In English and Spanish_ â€“ Various resources by [@alxgcrz](https://github.com/alxgcrz)  * [algovis](https://github.com/enjalot/algovis) â€“ Algorithm Visualization.  * [alternative-front-ends](https://github.com/mendel5/alternative-front-ends) â€“ Alternative open source front-ends for popular internet platforms (e.g. YouTube, Twitter, etc.).  * [alternative-internet](https://github.com/redecentralize/alternative-internet) â€“ A collection of interesting new networks and tech aiming at decentralisation (in some form).  * [amazing-deployment](https://github.com/delirehberi/amazing-deployment)  * [android-awesome-libraries](https://github.com/kaiinui/android-awesome-libraries) â€“ Useful Android development libraries with usage examples.  * [android-dev-readme](https://github.com/anirudh24seven/android-dev-readme) â€“ Links for every Android developer.  * [AndroidDevTools](https://github.com/inferjay/AndroidDevTools) _In Chinese_ â€“ SDK, development tools, libraries, and resources.    - http://www.androiddevtools.cn/  * [android-jobs](https://github.com/android-cn/android-jobs) _In Chinese_ â€“ Android positions in China.  * [Android-Learning-Resources](https://github.com/zhujun2730/Android-Learning-Resources) _In Chinese_ â€“ Learning resources for Android.  * [android-open-project](https://github.com/Trinea/android-open-project) _In Chinese_ â€“ Collect and classify android open source projects.  * [android-security-awesome](https://github.com/ashishb/android-security-awesome) â€“ â€œA lot of work is happening in academia and industry on tools to perform dynamic analysis, static analysis and reverse engineering of android apps.â€  * [android-tech-frontier](https://github.com/hehonghui/android-tech-frontier) _In Chinese_ â€“ Translation of articles about Android development.  * [angular-education](https://github.com/timjacobi/angular-education) â€“ Helpful material to develop using Angular  * [AngularJS-Learning](https://github.com/jmcunningham/AngularJS-Learning)  * [ansible-gentoo-roles](https://github.com/jirutka/ansible-gentoo-roles) â€“ Ansible roles for Gentoo Linux.  * [apis-list](https://github.com/apis-list/apis-list) â€“ Community maintained, human and machine readable list of Public APIs  * [app-ideas](https://github.com/florinpop17/app-ideas) â€“ Application ideas which can be used to improve your coding skills.  * [app-launch-guide](https://github.com/adamwulf/app-launch-guide) â€“ Indie dev's definitive guide to building and launching your app, including pre-launch, marketing, building, QA, buzz building, and launch.  * [applied-ml](https://github.com/eugeneyan/applied-ml) â€“ Data science & machine learning in production.  * [APTnotes](https://github.com/kbandla/APTnotes) â€“ Various public documents, whitepapers and articles about APT [Advanced persistent threat] campaigns.  * [architect-awesome](https://github.com/xingshaocheng/architect-awesome) _In Chinese_ â€“ åŽç«¯æž¶æž„å¸ˆæŠ€æœ¯å›¾è°±  * [asynchronous-php](https://github.com/elazar/asynchronous-php) â€“ Asynchronous programming in PHP.  * [Automated-SPA-Testing](https://github.com/webpro/Automated-SPA-Testing) â€“ Automated unit & functional testing for web applications [JavaScript et al.].  * [awful-ai](https://github.com/daviddao/awful-ai) â€“ Current scary usages of AI, hoping to raise awareness to its misuses in society.  * [awmy](https://github.com/potch/awmy) â€“ Are We Meta Yet?    - http://arewemetayet.com/  * [b1fipl](https://github.com/marcpaq/b1fipl) â€“ A Bestiary of Single-File Implementations of Programming Languages.  * [Backpack](https://github.com/sevab/Backpack) â€“ Various learning resources, organized by technology/topic.  * [badass-dev-resources](https://github.com/sodevious/badass-dev-resources) â€“ #bada55 front-end developer resources.  * [Badges4-README.md-Profile](https://github.com/alexandresanlim/Badges4-README.md-Profile) â€“ Badges for GitHub profiles.  * [bangalore-startups](https://github.com/hemanth/bangalore-startups) â€“ Startups in Bangalore.  * [beautiful-docs](https://github.com/PharkMillups/beautiful-docs) â€“ Pointers to useful, well-written, and otherwise beautiful documentation.  * [BEM-resources](https://github.com/sturobson/BEM-resources)  * [Best-App](https://github.com/hzlzh/Best-App) _In Chinese_ â€“ Recommendations for best desktop and mobile apps.  * [best-of-awesomeness-and-usefulness-for-webdev](https://github.com/Pestov/best-of-awesomeness-and-usefulness-for-webdev) â€“ Digest of the most useful tools and resources for the last year.    - [Russian version](https://github.com/Pestov/best-of-awesomeness-and-usefulness-for-webdev/tree/master/ru)  * [best-practices-checklist](https://github.com/palash25/best-practices-checklist) â€“ Language-specific resources to look up the best practices followed by that particular language's community.  * [Best-websites-a-programmer-should-visit](https://github.com/sdmg15/Best-websites-a-programmer-should-visit) â€“ Some useful websites for programmers.  * [Best-websites-a-programmer-should-visit-zh](https://github.com/tuteng/Best-websites-a-programmer-should-visit-zh) _In Chinese_ â€“ ç¨‹åºå‘˜åº”è¯¥è®¿é—®çš„æœ€ä½³ç½‘ç«™ä¸­æ–‡ç‰ˆ  * [bigdata-ecosystem](https://github.com/zenkay/bigdata-ecosystem) â€“ Big-data related projects packed into a JSON dataset.    - http://bigdata.andreamostosi.name/  * [Big-List-of-ActivityPub](https://github.com/shleeable/Big-List-of-ActivityPub) â€“ ActivityPub Projects  * [big-list-of-naughty-strings](https://github.com/minimaxir/big-list-of-naughty-strings) â€“ Strings which have a high probability of causing issues when used as user-input data.  * [bioinformatics-compbio-tools](https://github.com/lancelafontaine/bioinformatics-compbio-tools) â€“ Bioinformatics and computational biology tools.  * [bitcoin-reading-list](https://github.com/jashmenn/bitcoin-reading-list) â€“ Learn to program Bitcoin transactions.  * [BNN-ANN-papers](https://github.com/takyamamoto/BNN-ANN-papers) â€“ Papers about Biological and Artificial Neural Networks related to (Computational) Neuroscience  * [bookmarklets](https://github.com/RadLikeWhoa/bookmarklets) â€“ Bookmarklets that are useful on the web    - https://sacha.me/bookmarklets/  * [bookshelf](https://github.com/OpenTechSchool/bookshelf) â€“ Reading lists for learners.  * [bots](https://github.com/hackerkid/bots) â€“ Tools for building bots  * [breakfast-repo](https://github.com/ashleygwilliams/breakfast-repo) â€“ Videos, recordings, and podcasts to accompany our morning coffee.  * [browser-resources](https://github.com/azu/browser-resources) â€“ Latest JavaScript information by browser.  * [build-your-own-x](https://github.com/danistefanovic/build-your-own-x) â€“ Build your own (insert technology here)  * [channels](https://github.com/andrew--r/channels) _In Russian_ â€“ YouTube channels for web developers.  * [citizen-science](https://github.com/dylanrees/citizen-science) â€“ Scientific tools to empower communities and/or practice various forms of non-institutional science  * [classics](https://github.com/eyy/classics) â€“ Classical studies (Latin and Ancient Greek) resources: software, code and raw data.  * [Clone-Wars](https://github.com/GorvGoyl/Clone-Wars) â€“ Open-source clones of popular sites.  * [cloud-conferences](https://github.com/stefan-kolb/cloud-conferences) â€“ A collection of scientific and industry conferences focused on cloud computing.    - http://stefan-kolb.github.io/cloud-conferences/  * [code-canon](https://github.com/darius/code-canon) â€“ Code worth reading.  * [codeface](https://github.com/chrissimpkins/codeface) â€“ Typefaces for source code / text editors.  * [Colorful](https://github.com/Siddharth11/Colorful) â€“ Choose your next color scheme  * [CompilerJobs](https://github.com/mgaudet/CompilerJobs) â€“ Compiler, language, and runtime teams for people looking for jobs in this area.  * [compilers-targeting-c](https://github.com/dbohdan/compilers-targeting-c) â€“ Compilers that can generate C code.  * [computer-science](https://github.com/ossu/computer-science) â€“ Path to a free self-taught graduation in Computer Science.  * [content-management-systems](https://github.com/ahadb/content-management-systems) â€“ Open source & proprietary content management systems.  * [critical-path-css-tools](https://github.com/addyosmani/critical-path-css-tools) â€“ Tools to help prioritize above-the-fold CSS.  * [CryptoList](https://github.com/coinpride/CryptoList) â€“ Blockchain & cryptocurrency resources.  * [crypto-might-not-suck](https://github.com/sweis/crypto-might-not-suck) â€“ Crypto Projects that Might not Suck.  * [cscs](https://github.com/SalGnt/cscs) â€“ Coding Style Conventions and Standards.  * [css-in-js](https://github.com/MicheleBertoli/css-in-js) â€“ CSS in JS techniques comparison for React et al.  * [css-protips](https://github.com/AllThingsSmitty/css-protips) â€“ Take your CSS skills pro  * [cto](https://github.com/92bondstreet/cto) â€“ Chief Technology Officers resources.  * [curated-list-espresso-sugar-plugins](https://github.com/GioSensation/curated-list-espresso-sugar-plugins) â€“ Sugar plugins for Espresso, the code editor by MacRabbit.  * [curated-programming-resources](https://github.com/Michael0x2a/curated-programming-resources) â€“ Resources for learning programming and computer science.  * [curatedseotools](https://github.com/sneg55/curatedseotools) â€“ Best SEO Tools Stash    - https://curatedseotools.com  * [cycle-ecosystem](https://github.com/Widdershin/cycle-ecosystem) â€“ What are the most popular and trending libraries for [Cycle.js](http://cycle.js.org/)?  * [dad-jokes](https://github.com/wesbos/dad-jokes) â€“ Dad style programming jokes.  * [datajournalists-toolbox](https://github.com/basilesimon/datajournalists-toolbox) â€“ Tools for datajournalists, with examples and gists.  * [datascience](https://github.com/r0f1/datascience) â€“ Python resources for data science.  * [data-science-blogs](https://github.com/rushter/data-science-blogs)  * [data-science-must-watch](https://github.com/kmonsoor/data-must-watch)  * [datasciencemasters](https://github.com/datasciencemasters/go) â€“ The Curriculum for learning Data Science, Open Source and at your fingertips.    - http://datasciencemasters.org/  * [datascience-pizza](https://github.com/PizzaDeDados/datascience-pizza) _In Portugese_ â€“ Materiais de estudo em anÃ¡lise de dados e Ã¡reas afins, empresas que trabalham com dados e dicionÃ¡rio de conceitos.  * [DataSciencePython](https://github.com/ujjwalkarn/DataSciencePython) â€“ Python tutorials for Data Science, NLP and Machine Learning  * [debugging-stories](https://github.com/danluu/debugging-stories) â€“ Collection of links to various debugging stories.  * [Deep-NLP-Resources](https://github.com/pawangeek/Deep-NLP-Resources) â€“ Deep Natural Language Processing  * [degoogle](https://github.com/tycrek/degoogle) â€“ Alternatives to Google's products.  * [Developer-Conferences](https://github.com/MurtzaM/Developer-Conferences) â€“ Upcoming developer conferences.  * [dev-movies](https://github.com/aryaminus/dev-movies) â€“ Recommended movies for people working in the Software and IT Industry.  * [devopsbookmarks.com](https://github.com/devopsbookmarks/devopsbookmarks.com) â€“ To discover tools in the devops landscape.    - http://www.devopsbookmarks.com/  * [devops_resources](https://github.com/dustinmm80/devops_resources)  * [DevopsWiki](https://github.com/Leo-G/DevopsWiki) â€“ Devops Tools, Tutorials and Scripts.  * [dev-resource](https://github.com/Ibrahim-Islam/dev-resource) â€“ Resources for devs online and offline.  * [digital-gardeners](https://github.com/MaggieAppleton/digital-gardeners) â€“ Resources for gardeners tending their digital notes on the public interwebs.  * [discord-resources](https://github.com/DTinker/discord-resources) â€“ Discord modding resources.  * [discount-for-student-dev](https://github.com/AchoArnold/discount-for-student-dev) â€“ Discounts on software (SaaS, PaaS, IaaS, etc.) and other offerings for developers who are students  * [dive-into-machine-learning](https://github.com/hangtwenty/dive-into-machine-learning) â€“ Dive into Machine Learning with Python Jupyter notebook and scikit-learn    - http://hangtwenty.github.io/dive-into-machine-learning/  * [django-must-watch](https://gitlab.com/rosarior/django-must-watch) â€“ Must-watch videos bout Django web framework + Python.  * [DL4NLP](https://github.com/andrewt3000/DL4NLP) â€“ Deep Learning for Natural Language Processing resources.  * [dumb-password-rules](https://github.com/dumb-password-rules/dumb-password-rules) â€“ Shaming sites with dumb password rules.  * [easy-application](https://github.com/j-delaney/easy-application) â€“ Software engineering companies that are easy to apply to.  * [effects-bibliography](https://github.com/yallop/effects-bibliography) â€“ A collaborative bibliography of work related to the theory and practice of computational effects  * [ElixirBooks](https://github.com/sger/ElixirBooks) â€“ Elixir programming language books  * [elm-companies](https://github.com/jah2488/elm-companies) â€“ Companies using Elm  * [embedded-scripting-languages](https://github.com/dbohdan/embedded-scripting-languages)  * [ember-links/list](https://github.com/ember-links/list) â€“ Ember.js web framework  * [empathy-in-engineering](https://github.com/KimberlyMunoz/empathy-in-engineering) â€“ Building and promoting more compassionate engineering cultures  * [engineering-blogs](https://github.com/kilimchoi/engineering-blogs)  * [engine.so](https://github.com/pmwkaa/engine.so) â€“ Tracking, Benchmarking and Sharing Information about an open source embedded data storage engines, internals, architectures, data storage and transaction processing.  * [erlang-bookmarks](https://github.com/0xAX/erlang-bookmarks) â€“ All about erlang programming language.  * [erlang-watchlist](https://github.com/gabrielelana/erlang-watchlist) â€“ Where to find good code to master Erlang idioms  * [ES6-Learning](https://github.com/ericdouglas/ES6-Learning) â€“ Resources to learn ECMAScript 6!  * [es6-tools](https://github.com/addyosmani/es6-tools) â€“ An aggregation of tooling for ES6  * [Essential-JavaScript-Links](https://github.com/starandtina/Essential-JavaScript-Links)    - http://starandtina.github.io/Essential-JavaScript-Links/  * [every-programmer-should-know](https://github.com/mtdvio/every-programmer-should-know) â€“ (Mostly) technical things every software developer should know.  * [Facets](https://github.com/O-I/Facets) â€“ One-liners in Ruby  * [fks](https://github.com/JacksonTian/fks) _In Chinese_ â€“ Frontend Knowledge Structure.  * [flat-file-cms](https://github.com/ahadb/flat-file-cms) â€“ Stictly flat-file cms systems.  * [FOSS-for-Dev](https://github.com/tvvocold/FOSS-for-Dev) â€“ Free and open-source software for developers  * [freeCodeCamp](https://github.com/freeCodeCamp/freeCodeCamp) â€“ Open Source, Free Full Stack Training with hours of coding challenges, projects, and certifications.    - https://www.freecodecamp.org/  * [free-for-dev](https://github.com/ripienaar/free-for-dev) â€“ Software, SaaS, PaaS etc offerings that have free tiers for devs.    - https://free-for.dev/  * [free-programming-books](https://github.com/EbookFoundation/free-programming-books)    - http://resrc.io/list/10/list-of-free-programming-books/  * [free-programming-books-zh_CN](https://github.com/justjavac/free-programming-books-zh_CN) _In Chinese_  * [frontdesk](https://github.com/miripiruni/frontdesk) â€“ Useful things for Front End Developers  * [Front-end-Developer-Interview-Questions](https://github.com/h5bp/Front-end-Developer-Interview-Questions) â€“ Helpful front-end related questions you can use to interview potential candidates, test yourself or completely ignore.    - Available in [various translations](https://github.com/darcyclarke/Front-end-Developer-Interview-Questions/tree/master/Translations)  * [Front-end-Web-Development-Interview-Question](https://github.com/paddingme/Front-end-Web-Development-Interview-Question) _In Chinese_  * [Front-End-Web-Development-Resources](https://github.com/RitikPatni/Front-End-Web-Development-Resources)    - https://resources.ritikpatni.me/  * [frontend-challenges](https://github.com/felipefialho/frontend-challenges) â€“ Playful challenges for job applicants to test your knowledge.  * [frontend-dev-bookmarks](https://github.com/dypsilon/frontend-dev-bookmarks) â€“ Frontend development resources I collected over time.  * [frontend-dev-resources](https://github.com/dmytroyarmak/frontend-dev-resources) â€“ Frontend resources [conferences].  * [frontend-developer-resources](https://github.com/mrcodedev/frontend-developer-resources) _In Spanish._ â€“ El camino del Frontend Developer.  * [frontend-development](https://github.com/mojpm/frontend-development)  * [frontend-resources](https://github.com/JonathanZWhite/frontend-resources) by @JonathanZWhite  * [frontend-resources](https://github.com/zedix/frontend-resources) by @zedix  * [frontend-stuff](https://github.com/moklick/frontend-stuff) â€“ Framework/libraries/tools to use when building things on the web. Mostly Javascript stuff.  * [frontend-tools](https://github.com/codylindley/frontend-tools) â€“ Tools for frontend (i.e. html, js, css) desktop/laptop (i.e. does not include tablet or phone yet) web development  * [fsharp-companies](https://github.com/Kavignon/fsharp-companies) â€“ Companies that use F#  * [game-datasets](https://github.com/leomaurodesenv/game-datasets) â€“ Game datasets, tools for artificial intelligence in games  * [Game-Networking-Resources](https://github.com/MFatihMAR/Game-Networking-Resources) â€“ Game Network Programming  * [games](https://github.com/leereilly/games) â€“ Popular/awesome videos games, add-on, maps, etc. hosted on GitHub.  * [generated-awesomeness](https://github.com/orsinium-labs/generated-awesomeness) â€“ Awesome list autogenerated from GitHub API.  * [git-cheat-sheet](https://github.com/arslanbilal/git-cheat-sheet) â€“ git and git flow cheat sheet    - http://bilalarslan.me/git-cheat-sheet/  * [github-cheat-sheet](https://github.com/tiimgreen/github-cheat-sheet) â€“ Cool features of Git and GitHub.  * [github-drama](https://github.com/nikolas/github-drama)  * [github-hall-of-fame](https://github.com/mehulkar/github-hall-of-fame) â€“ Hall of Fame for spectacular things on Github.  * [GoBooks](https://github.com/dariubs/GoBooks) â€“ Golang books.  * [go-is-not-good](https://github.com/ksimka/go-is-not-good) â€“ Articles that complain about Golang's imperfection.  * [go-must-watch](https://github.com/sauravtom/go-must-watch) â€“ Must-watch videos about Golang.  * [go-patterns](https://github.com/tmrts/go-patterns) â€“ Go design patterns, recipes and idioms    - http://tmrts.com/go-patterns  * [graph-adversarial-learning-literature](https://github.com/YingtongDou/graph-adversarial-learning-literature) â€“ Adversarial learning papers on graph-structured data.  * [graphics-resources](https://github.com/mattdesl/graphics-resources) â€“ Game development and realtime graphics programming.  * [guides](https://github.com/NARKOZ/guides) by @NARKOZ â€“ Design and development guides  * [guides](https://github.com/taniarascia/guides) by @taniarascia â€“ Web Development Guides, Tutorials and Snippets.  * [Hackathon-Resources](https://github.com/xasos/Hackathon-Resources) by @xasos â€“ Hackathon Resources for organizers.  * [hack-chat/3rd-party-software-list](https://github.com/hack-chat/3rd-party-software-list) â€“ Bots, clients, and other software people have made for [hack.chat](https://hack.chat).  * [hacker-laws](https://github.com/dwmkerr/hacker-laws) â€“ Laws, Theories, Principles and Patterns that developers will find useful.  * [hacktoberfest-swag](https://github.com/benbarth/hacktoberfest-swag) â€“ Looking for [Hacktoberfest](https://hacktoberfest.digitalocean.com/) swag? You've come to the right place.  * [hacktoberfest-swag-list](https://github.com/crweiner/hacktoberfest-swag-list) â€“ Companies giving out swag for participation in [Hacktoberfest](https://hacktoberfest.digitalocean.com/).    - https://hacktoberfestswaglist.com  * [HarmonyOS](https://github.com/Awesome-HarmonyOS/HarmonyOS) â€“ [HarmonyOS](https://www.harmonyos.com/en/) by Huawei  * [haskell-companies](https://github.com/erkmos/haskell-companies) â€“ Companies using Haskel  * [haskell-must-watch](https://github.com/hzlmn/haskell-must-watch)  * [HeadlessBrowsers](https://github.com/dhamaniasad/HeadlessBrowsers)  * [hipchat-alternatives](https://github.com/cjbarber/hipchat-alternatives)  * [hiring-without-whiteboards](https://github.com/poteto/hiring-without-whiteboards) â€“ Companies that don't have a broken hiring process.  * [htaccess](https://github.com/phanan/htaccess) â€“ Useful .htaccess snippets.  * [hyperawesome](https://github.com/jorgebucaran/hyperawesome) â€“ Hyperapp JavaScript framework  * [idaplugins-list](https://github.com/onethawt/idaplugins-list) â€“ Plugins for [IDA disassembler](https://www.hex-rays.com/products/ida/).  * [ideas](https://github.com/samsquire/ideas) â€“ One Hundred Ideas for Computing  * [InfoSec-Black-Friday](https://github.com/0x90n/InfoSec-Black-Friday) â€“ Deals for InfoSec related software/tools this Black Friday  * [Inspire](https://github.com/NoahBuscher/Inspire) â€“ Links to assist you in web design and development  * [interviews](https://github.com/kdn251/interviews) â€“ Your personal guide to Software Engineering technical interviews.  * [InterviewThis](https://github.com/Twipped/InterviewThis) â€“ Developer questions to ask prospective employers  * [ios-awesome-libraries](https://github.com/kaiinui/ios-awesome-libraries) â€“ Useful iOS development libraries with usage examples.  * [iOS-Developer-and-Designer-Interview-Questions](https://github.com/9magnets/iOS-Developer-and-Designer-Interview-Questions)  * [iOSDevResource](https://github.com/objcc/iOSDevResource)  * [iptv](https://github.com/iptv-org/iptv) â€“ 5000+ publicly available IPTV channels from all over the world.  * [javacard-curated-list](https://github.com/EnigmaBridge/javacard-curated-list) â€“ Java Card applets and related applications for cryptographic smartcards.  * [javascript-dev-bookmarks](https://github.com/didicodes/javascript-dev-bookmarks) â€“ Articles that will help you get better at JavaScript.  * [javascript-patterns](https://github.com/shichuan/javascript-patterns) â€“ JavaScript Patterns    - http://shichuan.github.io/javascript-patterns/  * [javascript-resources](https://github.com/ztsu/javascript-resources)  * [javascript-sdk-design](https://github.com/hueitan/javascript-sdk-design)  * [jquery-tips-everyone-should-know](https://github.com/AllThingsSmitty/jquery-tips-everyone-should-know)  * [jsemu](https://github.com/fcambus/jsemu) â€“ Emulators written in JavaScript.  * [jshomes/learning-resources](https://github.com/jshomes/learning-resources) â€“ Web Platform/SaaS Learning Resources.  * [jslibs](https://github.com/esamattis/jslibs) â€“ My picks of promising/useful Javascript libraries.    - *See also [JSwiki](http://jswiki.org/)*  * [js-must-watch](https://github.com/bolshchikov/js-must-watch) â€“ Must-watch videos about javascript.  * [jsonauts](https://github.com/jsonauts/jsonauts.github.com) â€“ The ultimate reference for JSON tooling and specs.    - http://jsonauts.github.io/  * [jstips](https://github.com/loverajoel/jstips) â€“ JavaScript tips    - http://jstips.co  * [jstools](https://github.com/codefellows/jstools) â€“ Foundational JavaScript Tools  * [js-type-master](https://github.com/yumyo/js-type-master) â€“ JavaScript resources about web typography.    - https://www.codefellows.org/blog/a-list-of-foundational-javascript-tools  * [Julia.jl](https://github.com/svaksha/Julia.jl) â€“ Curated decibans of Julia language.    - https://github.com/svaksha/Julia.jl  * [killer-talks](https://github.com/PharkMillups/killer-talks) â€“ Talks that are worth watching.  * [kubernetes-failure-stories](https://github.com/hjacobs/kubernetes-failure-stories) â€“ Public failure/horror stories related to Kubernetes    - https://k8s.af  * [langs-in-rust](https://github.com/alilleybrinker/langs-in-rust) â€“ Programming languages implemented in Rust.  * [language-list](https://github.com/thomasfoster96/language-list) â€“ Programming languages being developed on GitHub.  * [Laravel-Resources](https://github.com/abhimanyu003/Laravel-Resources) â€“ Laravel Framework Resources and Blogs.  * [learn-drupal](https://github.com/rocketeerbkw/learn-drupal) â€“ Stuff to help you learn Drupal.  * [learn-for-free](https://github.com/aviaryan/learn-for-free) â€“ Free learning resources for all topics you can think of.  * [learnhaskell](https://github.com/bitemyapp/learnhaskell) â€“ A curated guide for learning Haskell.  * [learning-code-through-github-repos](https://github.com/muchirijane/learning-code-through-github-repos) â€“ Github repositories that you can use in your coding journey.  * [learn-python](https://github.com/adrianmoisey/learn-python) by @adrianmoisey â€“ Links that teach Python.  * [learn-python](https://github.com/trekhleb/learn-python) by @trekhleb â€“ Python scripts that are split by topics and contain code examples with explanations.  * [learn-to-program](https://github.com/karlhorky/learn-to-program) â€“ Foundation in Web Development.  * [learn-tt](https://github.com/jozefg/learn-tt) â€“ Resources for learning type theory.  * [learnxinyminutes-docs](https://github.com/adambard/learnxinyminutes-docs) â€“ Code documentation written as code!    - https://learnxinyminutes.com/  * [libertr](https://github.com/gaapt/libertr) â€“ Resources for liberty seekers.  * [lifeofjs](https://github.com/abhijeetkpawar/lifeofjs) â€“ Curated source for all types of awesome resources available for JavaScript.  * [Linux_websites](https://github.com/hduffddybz/Linux_websites) _In Chinese_ â€“ Websites related to Linux kernel development.  * [lua-languages](https://github.com/hengestone/lua-languages) â€“ Languages that compile to Lua.  * [machine-learning-algorithms](https://github.com/Sahith02/machine-learning-algorithms) â€“ Conceptual understanding of all machine learning algorithms.  * [Machine-Learning-Tutorials](https://github.com/ujjwalkarn/Machine-Learning-Tutorials) â€“ Machine Learning and Deep Learning Tutorials  * [machine-learning-with-ruby](https://github.com/arbox/machine-learning-with-ruby) â€“ Machine learning in Ruby  * [macos-apps](https://github.com/learn-anything/macos-apps)  * [magictools](https://github.com/ellisonleao/magictools) â€“ Game Development resources to make magic happen.  * [maintenance-modules](https://github.com/maxogden/maintenance-modules) â€“ NPM / Node.js modules useful for maintaining or developing modules  * [manong](https://github.com/nemoTyrant/manong) _In Chinese_ â€“ Weekly digest of technology  * [markdown-resources](https://github.com/rhythmus/markdown-resources) â€“ Markdown resources: apps, dialects, parsers, people, â€¦  * [Marketing-for-Engineers](https://github.com/goabstract/Marketing-for-Engineers) â€“ Marketing articles & tools to grow your product.  * [mind-bicycles](https://github.com/pel-daniel/mind-bicycles) â€“ Future of programming projects  * [motion-ui-design](https://github.com/fliptheweb/motion-ui-design) â€“ Motion UI design, animations and transitions.  * [movies-for-hackers](https://github.com/k4m4/movies-for-hackers)    - https://hackermovie.club/  * [must-watch-css](https://github.com/AllThingsSmitty/must-watch-css) â€“ Must-watch videos about CSS.  * [must-watch-javascript](https://github.com/AllThingsSmitty/must-watch-javascript) â€“ Must-watch videos about JavaScript.  * [my_tech_resources](https://github.com/JamesLavin/my_tech_resources) by @JamesLavin  * [nashville-lispers/resources](https://github.com/nashville-lispers/resources) â€“ Lisp Resources: exercises, great books, videos, etc.  * [net-libraries-that-make-your-life-easier](https://github.com/tallesl/net-libraries-that-make-your-life-easier) â€“ Open Source .NET libraries that make your life easier.  * [neural-network-papers](https://github.com/robertsdionne/neural-network-papers)  * [nginx-resources](https://github.com/fcambus/nginx-resources) â€“ Nginx web server (+ Lua), OpenResty and Tengine.  * [nlp_thai_resources](https://github.com/kobkrit/nlp_thai_resources) â€“ Natural Language Processing for Thai  * [nlp-with-ruby](https://github.com/arbox/nlp-with-ruby) â€“ Practical Natural Language Processing done in Ruby    - http://rubynlp.org  * [node-daily](https://github.com/dailyNode/node-daily) _In Chinese_ â€“ Daily article about Node.js.  * [node-frameworks](https://github.com/pillarjs/node-frameworks) â€“ Comparison of server-side Node frameworks.  * [nodejs-conference-cfps](https://github.com/rosskukulinski/nodejs-conference-cfps) â€“ NodeJS and Javascript Conference Call for Presentations.  * [NodeJS-Learning](https://github.com/sergtitov/NodeJS-Learning) â€“ Resources to help you learn Node.js and keep up to date.  * [NotesIndex](https://github.com/Wilbeibi/NotesIndex)  * [not-yet-awesome-rust](https://github.com/not-yet-awesome-rust/not-yet-awesome-rust) â€“ Rust code and resources that do NOT exist yet, but would be beneficial to the Rust community.  * [offline-first](https://github.com/pazguille/offline-first) â€“ Everything you need to know to create offline-first web apps.  * [open-source-android-apps](https://github.com/pcqpcq/open-source-android-apps) â€“ Collection of Android Apps which are open source.  * [open-source-ios-apps](https://github.com/dkhamsing/open-source-ios-apps) â€“ Open-source iOS apps.  * [open-source-mac-os-apps](https://github.com/serhii-londar/open-source-mac-os-apps) â€“ macOS open source applications.  * [open-source-meetup-alternatives](https://github.com/coderbyheart/open-source-meetup-alternatives)  * [opensource-discordbots](https://github.com/gillesheinesch/opensource-discordbots) â€“ Open-source bots for Discord.  * [ops-books](https://github.com/stack72/ops-books) â€“ Book recommendations related to Continuous Delivery, DevOps, Operations and Systems Thinking.  * [osx-and-ios-security-awesome](https://github.com/ashishb/osx-and-ios-security-awesome) â€“ OSX and iOS related security tools  * [papers](https://github.com/NicolasT/papers) â€“ A collection of papers found across the web.  * [papers-we-love](https://github.com/papers-we-love/papers-we-love) â€“ Papers from the computer science community to read and discuss. (Contains actual papers)  * [ParseAlternatives](https://github.com/relatedcode/ParseAlternatives) â€“ Alternative backend service providers ala [Parse](http://parse.com/).  * [pattern_classification](https://github.com/rasbt/pattern_classification) â€“ A collection of tutorials and examples for solving and understanding machine learning and pattern classification tasks.  * [PayloadsAllTheThings](https://github.com/swisskyrepo/PayloadsAllTheThings) â€“ Useful payloads and bypasses for Web Application Security and Pentest/CTF  * [personal-security-checklist](https://github.com/Lissy93/personal-security-checklist) â€“ 100+ tips for protecting digital security and privacy  * [php-must-watch](https://github.com/phptodayorg/php-must-watch) â€“ Must-watch videos about PHP.  * [phpvietnam/bookmarks](https://github.com/phpvietnam/bookmarks) â€“ PHP resources for Vietnamese.  * [PlacesToPostYourStartup](https://github.com/mmccaff/PlacesToPostYourStartup) â€“ â€œWhere can I post my startup to get beta users?â€  * [planetruby/calendar](https://github.com/planetruby/calendar) â€“ Ruby events (meetups, conferences, camps, etc.) from around the world.    - https://planetruby.github.io/calendar/  * [post-mortems](https://github.com/danluu/post-mortems)  * [programmers-proverbs](https://github.com/AntJanus/programmers-proverbs) â€“ Proverbs from the programmer  * [programming-talks](https://github.com/hellerve/programming-talks) â€“ Awesome & Interesting Talks concerning Programming  * [progressive-enhancement-resources](https://github.com/jbmoelker/progressive-enhancement-resources) â€“ (code) examples.  * [project-based-learning](https://github.com/tuvtran/project-based-learning) â€“ Programming tutorials to build an application from scratch.  * [Projects](https://github.com/karan/Projects) â€“ Practical projects that anyone can solve in any programming language.  * [public-apis](https://github.com/public-apis/public-apis) â€“ JSON APIs for use in web development.  * [purescript-companies](https://github.com/ajnsit/purescript-companies) â€“ Companies that use Purescript  * [pycrumbs](https://github.com/kirang89/pycrumbs) â€“ Bits and Bytes of Python from the Internet.  * [py-must-watch](https://github.com/s16h/py-must-watch) by @s16h â€“ Must-watch videos about Python.  * [python-github-projects](https://github.com/checkcheckzz/python-github-projects) â€“ Collect and classify python projects on Github.    - http://itgeekworkhard.com/python-github-projects/  * [pythonidae](https://github.com/svaksha/pythonidae) â€“ Curated decibans of Python scientific programming resources.    - http://svaksha.github.io/pythonidae/  * [python-must-watch](https://github.com/primalpop/python-must-watch) by @primalpop â€“ Must-watch videos about Python.  * [python_reference](https://github.com/rasbt/python_reference) â€“ Useful functions, tutorials, and other Python-related things.  * [Qix](https://github.com/ty4z2008/Qix) _In Chinese_ â€“ Node, Golang, Machine Learning, PostgreSQL.  * [queues.io](https://github.com/lukaszx0/queues.io) â€“ Job queues, message queues and other queues.    - http://queues.io/  * [quick-look-plugins](https://github.com/sindresorhus/quick-look-plugins) â€“ macOS Quick Look plugins for developers  * [rails-must-watch](https://github.com/gerricchaplin/rails-must-watch) â€“ Must-watch videos about Ruby on Rails.  * [rbooks](https://github.com/RomanTsegelskyi/rbooks) â€“ R programming language books  * [remote-in-japan](https://github.com/remote-jp/remote-in-japan) â€“ Tech companies in Japan that hire remote workers.  * [remote-jobs](https://github.com/remoteintech/remote-jobs) â€“ Semi to fully remote-friendly companies in tech.  * [remote-jobs-brazil](https://github.com/lerrua/remote-jobs-brazil) â€“ Remote-friendly Brazilian companies.  * [resource-list](https://github.com/kyasui/resource-list) â€“ Design & Development Resources.  * [resources](https://github.com/jbranchaud/resources) by @jbranchaud â€“ Free, online resources for various technologies, languages, and tools.  * [Resources](https://github.com/tevko/Resources) by @tevko â€“ Tools for front end devs.  * [Resources-for-Beginner-Bug-Bounty-Hunters](https://github.com/nahamsec/Resources-for-Beginner-Bug-Bounty-Hunters) â€“ Getting started with bug bounties.  * [Resources-for-Writing-Shaders-in-Unity](https://github.com/VoxelBoy/Resources-for-Writing-Shaders-in-Unity)  * [retter](https://github.com/MaciejCzyzewski/retter) â€“ Hash functions, ciphers, tools, libraries, and materials related to cryptography & security.  * [reverse-interview](https://github.com/viraptor/reverse-interview) â€“ Questions to ask the company during your interview  * [Rich-Hickey-fanclub](https://github.com/tallesl/Rich-Hickey-fanclub) â€“ Rich Hickey's works on the internet.  * [rss-readers-list](https://github.com/smithbr/rss-readers-list) â€“ Reader replacements megalist    - http://smithbr.github.io/rss-readers-list  * [rubybib.org](https://github.com/rubybib/rubybib.org) â€“ The Ruby Bibliography    - http://rubybib.org/  * [ruby-bookmarks](https://github.com/dreikanter/ruby-bookmarks) â€“ Ruby and Ruby on Rails bookmarks collection.  * [ruby-dev-bookmarks](https://github.com/saberma/ruby-dev-bookmarks) â€“ Ruby development resources I've collected.  * [ruby-nlp](https://github.com/diasks2/ruby-nlp) â€“ Ruby Natural Language Processing (NLP) libraries, tools and software.  * [rust-lang-resources](https://github.com/dschenkelman/rust-lang-resources) â€“ Links related to the Rust programming language.  * [rxjs-ecosystem](https://github.com/Widdershin/rxjs-ecosystem) â€“ What are the most popular libraries in the RxJS ecosystem?  * [rx-react-flux](https://github.com/christianramsey/rx-react-flux) â€“ RxJS + React/Flux implementations.  * [scalable-css-reading-list](https://github.com/davidtheclark/scalable-css-reading-list) â€“ Collected dispatches from The Quest for Scalable CSS.  * [search-engine-optimization](https://github.com/marcobiedermann/search-engine-optimization) â€“ Checklist / collection of Search Engine Optimization (SEO) tips and technics.  * [SecLists](https://github.com/danielmiessler/SecLists) â€“ Lists used during security assessments: usernames, passwords, URLs, sensitive data patterns, fuzzing payloads, web shells, etc.  * [secure-email](https://github.com/OpenTechFund/secure-email) â€“ Overview of projects working on next-generation secure email.  * [Security_list](https://github.com/zbetcheckin/Security_list)  * [selfhosted-music-overview](https://github.com/basings/selfhosted-music-overview) â€“ Software network services which can be hosted on your own servers.  * [services-engineering](https://github.com/mmcgrana/services-engineering) â€“ A reading list for services engineering, with a focus on cloud infrastructure services.  * [shareable-links](https://github.com/vinkla/shareable-links) â€“ URLs for sharing on social media.  * [shellshocker-pocs](https://github.com/mubix/shellshocker-pocs) â€“ Proof of concepts and potential targets for Shellshock.  * [slack-groups](https://github.com/learn-anything/slack-groups) â€“ Public Slack communities.  * [spark-joy](https://github.com/sw-yx/spark-joy) â€“ Add design flair, user delight, and whimsy to your product.  * [spawnedshelter](https://github.com/unbalancedparentheses/spawnedshelter) â€“ Erlang Spawned Shelter â€“ the best articles, videos and presentations related to Erlang.  * [speech-language-processing](https://github.com/edobashira/speech-language-processing)  * [stack-on-a-budget](https://github.com/255kb/stack-on-a-budget) â€“ Services with great free tiers for developers on a budget  * [startup-must-watch](https://github.com/gerricchaplin/startup-must-watch) â€“ Must-watch videos devoted to Entrepreneurship and Startups.  * [startupreadings](https://github.com/dennybritz/startupreadings) â€“ Reading list for all things startup-related.  * [startup-resources](https://github.com/JonathanZWhite/startup-resources)  * [state-machines](https://github.com/achou11/state-machines)  * [static-analysis](https://github.com/analysis-tools-dev/static-analysis) â€“ Static analysis tools, linters and code quality checkers  * [Static-Site-Generators](https://github.com/pinceladasdaweb/Static-Site-Generators)  * [staticsitegenerators-list](https://github.com/bevry/staticsitegenerators-list)    - https://staticsitegenerators.net/  * [streaming-papers](https://github.com/sorenmacbeth/streaming-papers) â€“ Papers on streaming algorithms.  * [structured-text-tools](https://github.com/dbohdan/structured-text-tools) â€“ Command line tools for manipulating structured text data  * [styleguide-generators](https://github.com/davidhund/styleguide-generators) â€“ Automatic living styleguide generators.  * [sublime](https://github.com/JaredCubilla/sublime) â€“ Some of the best Sublime Text packages, themes, and goodies.  * [sublime-bookmarks](https://github.com/dreikanter/sublime-bookmarks) â€“ Sublime Text essential plugins and resources.  * [svelte/integrations](https://github.com/sveltejs/integrations) â€“ Ways to incorporate [Svelte](https://svelte.dev/) framework into your stack  * [SwiftInFlux](https://github.com/ksm/SwiftInFlux) â€“ An attempt to gather all that is in flux in Swift.  * [tech-weekly](https://github.com/adrianmoisey/tech-weekly) â€“ Weekly technical newsletters.  * [terminals-are-sexy](https://github.com/k4m4/terminals-are-sexy) â€“ Terminal frameworks, plugins & resources for CLI lovers.    - https://terminalsare.sexy/  * [the-book-of-secret-knowledge](https://github.com/trimstray/the-book-of-secret-knowledge) â€“ Inspiring lists, manuals, cheatsheets, blogs, hacks, one-liners, cli/web tools and more.  * [The-Documentation-Compendium](https://github.com/kylelobo/The-Documentation-Compendium) â€“ Templates & tips on writing high-quality documentation  * [think-awesome](https://github.com/thinkjs/think-awesome) â€“ [ThinkJS](https://thinkjs.org/) Node.js framework  * [til](https://github.com/jbranchaud/til) â€“ Today I Learned.  * [tips](https://github.com/git-tips/tips) â€“ Most commonly used git tips and tricks.    - http://git.io/git-tips  * [Toolbox](https://github.com/Dillion/Toolbox) â€“ Open source iOS stuff.  * [tool_lists](https://github.com/johnyf/tool_lists) â€“ Links to tools by theme. *Verification, synthesis, and static analysis.*  * [tools](https://github.com/lvwzhen/tools) â€“ Tools for web.  * [toolsforactivism](https://github.com/drewrwilson/toolsforactivism) â€“ Digital tools for activism  * [tools-list](https://github.com/everestpipkin/tools-list) â€“ Open source, experimental, and tiny tools for building game/website/interactive project.    - https://tinytools.directory/  * [ToolsOfTheTrade](https://github.com/cjbarber/ToolsOfTheTrade) â€“ Tools of The Trade, from Hacker News.  * [top-starred-devs-and-repos-to-follow](https://github.com/StijnMiroslav/top-starred-devs-and-repos-to-follow) â€“ Top-Starred Python GitHub Devs, Orgs, and Repos to Follow (All-Time and Trending).  * [trending-repositories](https://github.com/Semigradsky/trending-repositories) â€“ Repositories that were trending for a day.  * [trip-to-iOS](https://github.com/Aufree/trip-to-iOS) _In Chinese_ â€“ Delightful iOS resources.  * [twofactorauth](https://github.com/2factorauth/twofactorauth) â€“ Sites with two factor auth support which includes SMS, email, phone calls, hardware, and software.    - https://twofactorauth.org/  * [type-findings](https://github.com/charliewilco/type-findings) â€“ Posts about web typography.  * [typography](https://github.com/deanhume/typography) â€“ Web typography    - https://deanhume.github.io/typography/  * [ui-styleguides](https://github.com/kevinwuhoo/ui-styleguides)    - http://kevinformatics.com/ui-styleguides/  * [universities-on-github](https://github.com/filler/universities-on-github) â€“ Universities which have a public organization on GitHub.  * [upcoming-conferences](https://github.com/svenanders/upcoming-conferences) â€“ Upcoming web developer conferences.  * [vertx-awesome](https://github.com/vert-x3/vertx-awesome) â€“ [Vert.x](http://vertx.io/) toolkit  * [vim-galore](https://github.com/mhinz/vim-galore) â€“ All things Vim!  * [visual-programming-codex](https://github.com/ivanreese/visual-programming-codex) â€“ Resources and references for the past and future of visual programming.  * [we-are-twtxt](https://github.com/mdom/we-are-twtxt) â€“ [twtxt](https://twtxt.readthedocs.io/) users and bots  * [web-audio-resources](https://github.com/alemangui/web-audio-resources) â€“ A list of curated resources related to the Web audio API.  * [WebComponents-Polymer-Resources](https://github.com/matthiasn/WebComponents-Polymer-Resources)  * [webcomponents-the-right-way](https://github.com/mateusortiz/webcomponents-the-right-way) â€“ Introduction to Web Components.  * [web-dev-resources](https://github.com/ericandrewlewis/web-dev-resources) â€“ A table of contents for web developer resources across the internet.  * [web-development-resources](https://github.com/MasonONeal/web-development-resources)  * [webdev-jokes](https://github.com/jerstew/webdev-jokes) â€“ Web development jokes.  * [webdevresourcecuration](https://github.com/lwakefield/webdevresourcecuration)  * [weekly](https://github.com/zenany/weekly) _In Chinese_ â€“ Weekly summary of articles and resources.  * [what-next](https://github.com/messa/what-next) _In Czech_ â€“ Co dÄ›lat, kdyÅ¾ se chci nauÄit programovat jeÅ¡tÄ› vÃ­c.  * [Women-Made-It](https://github.com/LisaDziuba/Women-Made-It) â€“ Design & development tools, books, podcasts, and blogs made by women.  * [Worth-Reading-the-Android-technical-articles](https://github.com/zmywly8866/Worth-Reading-the-Android-technical-articles) _In Chinese_  * [You-Dont-Need](https://github.com/you-dont-need/You-Dont-Need) â€“ People choose popular projects, often not because it applies to their problems.      ### awesome-*    * [awesome-2048-and-beyond](https://github.com/cstrap/awesome-2048-and-beyond) â€“ Waste and lose at least 8 hours of your lifeâ€¦ then **multiply** itâ€¦  * [awesome4girls](https://github.com/cristianoliveira/awesome4girls) â€“ Inclusive events/projects/initiatives for women in the tech area.  * [awesome-a11y](https://github.com/brunopulis/awesome-a11y) â€“ Accesibility tools, articles and resources.  * [awesome-accessibility](https://github.com/GonzagaAccess/awesome-accessibility) â€“ Utilities for accessibility-based web development  * [awesome-acf](https://github.com/navidkashani/awesome-acf) â€“ Add-ons for the Advanced Custom Field plugin for WordPress.  * [awesome-actions](https://github.com/sdras/awesome-actions) â€“ [GitHub Actions](https://github.com/features/actions)  * [awesome-actionscript3](https://github.com/robinrodricks/awesome-actionscript3) â€“ ActionScript 3 and Adobe AIR.  * [awesome-activeadmin](https://github.com/serradura/awesome-activeadmin) â€“ Active Admin resources, extensions, posts and utilities. *For Rails.*  * [awesome-activitypub](https://github.com/BasixKOR/awesome-activitypub) â€“ ActivityPub based projects  * [awesome-ad-free](https://github.com/johnjago/awesome-ad-free) â€“ Ad-free alternatives to popular services on the web  * [awesome-ada](https://github.com/ohenley/awesome-ada) â€“ Ada and SPARK programming language  * [awesome-adafruitio](https://github.com/adafruit/awesome-adafruitio) â€“ [Adafruit IO](https://io.adafruit.com/) Internet of Things platform  * [awesome-advent-of-code](https://github.com/Bogdanp/awesome-advent-of-code) â€“ [Advent of Code](https://adventofcode.com/)  * [awesome-agile](https://github.com/lorabv/awesome-agile) â€“ Agile Software Development.    - https://lorabv.github.io/awesome-agile  * [awesome-agriculture](https://github.com/brycejohnston/awesome-agriculture) â€“ Open source technology for agriculture, farming, and gardening  * [awesome-alfred-workflows](https://github.com/alfred-workflows/awesome-alfred-workflows) â€“ [Alfred](https://www.alfredapp.com/) macOS app workflows  * [awesome-algolia](https://github.com/algolia/awesome-algolia) â€“ [Algolia](https://www.algolia.com/) web search service  * [awesome-algorithms](https://github.com/tayllan/awesome-algorithms) â€“ Places to learn and/or practice algorithms.  * [awesome-algorithms-education](https://github.com/gaerae/awesome-algorithms-education) â€“ Learning and practicing algorithms    - https://gaerae.com/awesome-algorithms  * [awesome-alternatives](https://gitlab.com/linuxcafefederation/awesome-alternatives) â€“ Mostly free and open source alternatives to proprietary software and services.  * [awesome-ama-answers](https://github.com/stoeffel/awesome-ama-answers) â€“ @stoeffel's AMA answers  * [awesome-amazon-alexa](https://github.com/miguelmota/awesome-amazon-alexa) â€“ Resources for the Amazon Alexa platform.  * [awesome-amazon-seller](https://github.com/ScaleLeap/awesome-amazon-seller) â€“ Tools and resources for Amazon sellers.  * [awesome-analytics](https://github.com/onurakpolat/awesome-analytics) â€“ Analytics services, frameworks, software and other tools.  * [awesome-android](https://github.com/Jackgris/awesome-android) _In Spanish._ by @Jackgris  * [awesome-android](https://github.com/JStumpp/awesome-android) by @JStumpp  * [awesome-android](https://github.com/snowdream/awesome-android) _Partially in Chinese_ by @snowdream  * [awesome-android-awesomeness](https://github.com/yongjhih/awesome-android-awesomeness)  * [awesome-android-kotlin-apps](https://github.com/androiddevnotes/awesome-android-kotlin-apps) â€“ Open-source Android apps written in Kotlin with particular tech stack and libraries.  * [awesome-android-learner](https://github.com/MakinGiants/awesome-android-learner) â€“ A â€œstudy guideâ€ for mobile development.  * [awesome-android-learning-resources](https://github.com/androiddevnotes/awesome-android-learning-resources)  * [awesome-android-libraries](https://github.com/wasabeef/awesome-android-libraries) â€“ General Android libraries.  * [awesome-android-performance](https://github.com/Juude/awesome-android-performance) â€“ Performance optimization on Android.  * [awesome-android-release-notes](https://github.com/pedronveloso/awesome-android-release-notes) â€“ Keep up-to-date with all the things related with Android software development.  * [awesome-android-tips](https://github.com/jiang111/awesome-android-tips) _In Chinese_  * [awesome-android-ui](https://github.com/wasabeef/awesome-android-ui) â€“ UI/UX libraries for Android.  * [awesome-androidstudio-plugins](https://github.com/jiang111/awesome-androidstudio-plugins) _In Chinese_  * [awesome-angular](https://github.com/hugoleodev/awesome-angular) by @hugoleodev  * [awesome-angular](https://github.com/PatrickJS/awesome-angular) by @PatrickJS  * [awesome-angularjs](https://github.com/gianarb/awesome-angularjs) by @gianarb  * [awesome-animation](https://github.com/Animatious/awesome-animation) â€“ Open-source UI animations by Animatious Group.  * [awesome-ansible](https://github.com/jdauphant/awesome-ansible) â€“ [Ansible](https://www.ansible.com/) configuration management  * [awesome-answers](https://github.com/cyberglot/awesome-answers) â€“ Inspiring and thoughtful answers given at stackoverflow, quora, etc.  * [awesome-ant-design](https://github.com/websemantics/awesome-ant-design) â€“ [Ant Design](https://ant.design/) system  * [awesome-api](https://github.com/Kikobeats/awesome-api) â€“ Design and implement RESTful API's  * [awesome-app-ideas](https://github.com/tastejs/awesome-app-ideas) â€“ Ideas for apps to demonstrate how framework or library approach specific problems.  * [awesome-appium](https://github.com/SrinivasanTarget/awesome-appium) â€“ [Appium](http://appium.io/) test automation frmework  * [awesome-apple](https://github.com/joeljfischer/awesome-apple) â€“ 3rd party libraries and tools for Apple platforms development.  * [awesome-appsec](https://github.com/paragonie/awesome-appsec) â€“ Resources for developers to learn application security.  * [awesome-arabic](https://github.com/OthmanAba/awesome-arabic) â€“ Arabic supporting tools, fonts, and development resources.  * [Awesome-arduino](https://github.com/Lembed/Awesome-arduino) â€“ Arduino hardwares, libraries and softwares with update script  * [awesome-arm-exploitation](https://github.com/HenryHoggard/awesome-arm-exploitation) â€“ ARM processors security and exploitation.  * [awesome-artificial-intelligence](https://github.com/owainlewis/awesome-artificial-intelligence)  * [awesome-asciidoc](https://github.com/bodiam/awesome-asciidoc) â€“ Collection of AsciiDoc tools, guides, tutorials and examples of usage.  * [awesome-asciidoctor](https://github.com/dongwq/awesome-asciidoctor) â€“ Collection of asciidoctorâ€™s intros, examples and usages.  * [awesome-ast](https://github.com/chadbrewbaker/awesome-ast) by @chadbrewbaker â€“ Tools for Abstract Syntax Tree processing.  * [awesome-ast](https://github.com/cowchimp/awesome-ast) by @cowchimp â€“ Abstract Syntax Trees.  * [awesome-asyncio](https://github.com/timofurrer/awesome-asyncio) â€“ [asyncio](https://docs.python.org/3/library/asyncio.html) Python library  * [awesome-asyncio-cn](https://github.com/chenjiandongx/awesome-asyncio-cn) _In Chinese_ â€“ [asyncio](https://docs.python.org/3/library/asyncio.html) Python library    - https://awesome-asyncio-cn.chenjiandongx.com/  * [awesome-atom](https://github.com/mehcode/awesome-atom) â€“ [Atom](https://atom.io/) text editor  * [awesome-audio-visualization](https://github.com/willianjusten/awesome-audio-visualization)  * [awesome-aurelia](https://github.com/aurelia-contrib/awesome-aurelia) â€“ [Aurelia](https://aurelia.io/) JavaScript framework  * [awesome-authentication](https://github.com/gitcommitshow/awesome-authentication)  * [awesome-AutoHotkey](https://github.com/ahkscript/awesome-AutoHotkey) â€“ AutoHotkey libraries, library distributions, scripts, tools and resources.  * [awesome-AutoIt](https://github.com/J2TeaM/awesome-AutoIt) â€“ UDFs, example scripts, tools and useful resources for AutoIt.    - https://j2team.github.io/awesome-AutoIt/  * [awesome-automotive](https://github.com/Marcin214/awesome-automotive) â€“ Automotive engineering.  * [awesome-ava](https://github.com/avajs/awesome-ava) â€“ [AVA](https://github.com/avajs/ava) JavaScript test runner.  * [awesome-avr](https://github.com/fffaraz/awesome-avr)  * [awesome-aws](https://github.com/donnemartin/awesome-aws) â€“ Amazon Web Services (AWS)  * [awesome-backbone](https://github.com/sadcitizen/awesome-backbone) â€“ Resources for [Backbone.js](http://backbonejs.org/)  * [awesome-bash](https://github.com/awesome-lists/awesome-bash)  * [awesome-bci](https://github.com/NeuroTechX/awesome-bci) â€“ Brain-Computer Interface.  * [awesome-beacon](https://github.com/rabschi/awesome-beacon) â€“ Bluetooth beacon (iBeacon, Eddystone)  * [awesome-beancount](https://github.com/wzyboy/awesome-beancount) â€“ [Beancount](http://furius.ca/beancount/), a double-entry bookkeeping with text files.  * [awesome-bem](https://github.com/getbem/awesome-bem) â€“ Tools, sites, articles about BEM (frontend development method).  * [awesome-big-o](https://github.com/okulbilisim/awesome-big-o) â€“ Big O notation  * [awesome-bigdata](https://github.com/onurakpolat/awesome-bigdata) â€“ Big data frameworks, resources and other awesomeness.  * [Awesome-Bioinformatics](https://github.com/danielecook/Awesome-Bioinformatics) â€“ Open-source bioinformatics software and libraries.  * [awesome-bitcoin](https://github.com/igorbarinov/awesome-bitcoin) â€“ Bitcoin services and tools for software developers.  * [awesome-bitcoin-payment-processors](https://github.com/alexk111/awesome-bitcoin-payment-processors) â€“ Bitcoin payment processors and stories from merchants using them.  * [awesome-blazor](https://github.com/AdrienTorris/awesome-blazor) â€“ [Blazor](https://blazor.net/), a .NET web framework using C#/Razor and HTML that runs in the browser with WebAssembly.  * [awesome-blender](https://github.com/agmmnn/awesome-blender) â€“ [Blender](https://www.blender.org/) add-ons, tools, tutorials and 3D resources.  * [awesome-blockchain](https://github.com/0xtokens/awesome-blockchain) by @0xtokens â€“ Blockchain and Crytocurrency Resources  * [awesome-blockchain](https://github.com/coderplex-org/awesome-blockchain) by @coderplex-org â€“ Blockchain, Bitcoin and Ethereum related resources  * [awesome-blockchain](https://github.com/cyberFund/awesome-blockchain) _In Russian_ by @cyberFund â€“ Digest of knowledge about crypto networks (including cryptocurrencies).  * [awesome-blockchain](https://github.com/hitripod/awesome-blockchain) by @hitripod  * [awesome-blockchain](https://github.com/igorbarinov/awesome-blockchain) by @igorbarinov â€“ The bitcoin blockchain services  * [awesome-blockchain](https://github.com/imbaniac/awesome-blockchain) by @imbaniac â€“ Blockchain services and exchanges  * [awesome-blockchain](https://github.com/iNiKe/awesome-blockchain) by @iNiKe â€“ Blockchain, ICO, â‚¿itcoin, Cryptocurrencies  * [awesome-blockchain](https://github.com/oiwn/awesome-blockchain) by @oiwn â€“ Projects and services based on blockchain technology  * [awesome-blockchain-ai](https://github.com/steven2358/awesome-blockchain-ai) â€“ Blockchain projects for Artificial Intelligence and Machine Learning  * [awesome-blockchains](https://github.com/openblockchains/awesome-blockchains) â€“ Blockchains - open distributed databases w/ crypto hashes incl. git  * [awesome-blockstack](https://github.com/jackzampolin/awesome-blockstack) â€“ [Blockstack](https://blockstack.org/) decentralized computing platform  * [awesome-book-authoring](https://github.com/TalAter/awesome-book-authoring) â€“ Resources for technical book authors  * [awesome-bootstrap](https://github.com/therebelrobot/awesome-bootstrap) â€“ Free Bootstrap themes I think are cool.  * [awesome-bpm](https://github.com/ungerts/awesome-bpm) â€“ Business Process Management (BPM) awesomeness.  * [awesome-broadcasting](https://github.com/ebu/awesome-broadcasting) â€“ Open source resources related to broadcast technologies    - http://ebu.io/opensource  * [awesome-browser-extensions-for-github](https://github.com/stefanbuck/awesome-browser-extensions-for-github) â€“ Browser extensions for GitHub.  * [awesome-browserify](https://github.com/browserify/awesome-browserify) â€“ [Browserify](http://browserify.org/) bundler  * [awesome-btcdev](https://github.com/btcbrdev/awesome-btcdev) â€“ Bitcoin development  * [awesome-bugs](https://github.com/criswell/awesome-bugs) â€“ Funny and interesting bugs  * [awesome-building-blocks-for-web-apps](https://github.com/componently-com/awesome-building-blocks-for-web-apps) â€“ Standalone features (services, components, libraries) to be integrated into web applications.    - https://www.componently.com/  * [awesome-c](https://github.com/aleksandar-todorovic/awesome-c) by @aleksandar-todorovic â€“ Continuing the development of awesome-c on GitHub  * [awesome-c](https://github.com/kozross/awesome-c) by @kozross â€“ C frameworks, libraries, resources etc.    - [mirror](https://notabug.org/koz.ross/awesome-c)  * [awesome-cakephp](https://github.com/FriendsOfCake/awesome-cakephp) â€“ [CakePHP](https://cakephp.org/) web framework  * [awesome-calculators](https://github.com/xxczaki/awesome-calculators)  * [awesome-canvas](https://github.com/raphamorim/awesome-canvas) â€“ HTML5 Canvas  * [awesome-captcha](https://github.com/ZYSzys/awesome-captcha) â€“ Captcha libraries and crack tools.    - http://zyszys.github.io/awesome-captcha/  * [awesome-cassandra](https://github.com/yikebocai/awesome-cassandra)  * [awesome-ccxt](https://github.com/suenot/awesome-ccxt) â€“ [CryptoCurrency eXchange Trading Library](https://github.com/ccxt/ccxt)  * [awesome_challenge_list](https://github.com/AwesomeRubyist/awesome_challenge_list) â€“ Sites with challenges to improve your programming skills.  * [awesome-challenges](https://github.com/mauriciovieira/awesome-challenges) â€“ Algorithmic challenges  * [awesome-charting](https://github.com/zingchart/awesome-charting) â€“ Charts and dataviz.  * [awesome-chatops](https://github.com/exAspArk/awesome-chatops) â€“ ChatOps â€“ managing operations through a chat  * [awesome-chef](https://github.com/obazoud/awesome-chef) â€“ Cookbooks, handlers, add-ons and other resources for Chef, a configuration management tool.  * [awesome-cheminformatics](https://github.com/hsiaoyi0504/awesome-cheminformatics) â€“ Chemical informatics  * [awesome-chess](https://github.com/hkirat/awesome-chess) â€“ Chess software, libraries, and resources  * [awesome-choo](https://github.com/choojs/awesome-choo) â€“ [choo](https://choo.io/) web framework  * [awesome-chrome-devtools](https://github.com/ChromeDevTools/awesome-chrome-devtools) â€“ Chrome DevTools ecosystem tooling and resources.  * [awesome-ci](https://github.com/ligurio/awesome-ci) by @ligurio â€“ Comparison of cloud based CI services.  * [awesome-ci](https://github.com/pditommaso/awesome-ci) by @pditommaso â€“ Continuous integation services.  * [awesome-ciandcd](https://github.com/cicdops/awesome-ciandcd) â€“ Continuous Integration and Continuous Delivery    - http://www.ciandcd.com/  * [awesome-circuitpython](https://github.com/adafruit/awesome-circuitpython) â€“ [CircuitPython](https://circuitpython.org/) microcontrollers programming language  * [awesome-cl](https://github.com/CodyReichert/awesome-cl) â€“ Common Lisp  * [awesome-cl-software](https://github.com/azzamsa/awesome-cl-software) â€“ Applications built with Common Lisp  * [awesome-cli-apps](https://github.com/agarrharr/awesome-cli-apps) â€“ Command line apps  * [awesome-clojure](https://github.com/mbuczko/awesome-clojure) by @mbuczko â€“ Useful links for clojurians  * [awesome-clojure](https://github.com/razum2um/awesome-clojure) by @razum2um  * [awesome-clojurescript](https://github.com/hantuzun/awesome-clojurescript)  * [awesome-cloud](https://github.com/JStumpp/awesome-cloud) â€“ Delightful cloud services.  * [awesome-cloud-certifications](https://gitlab.com/edzob/awesome-cloud-certifications) â€“ Certifications for cloud platforms  * [awesome-cloudflare](https://github.com/irazasyed/awesome-cloudflare) â€“ [Cloudflare](https://www.cloudflare.com/) tools and recipes.  * [awesome-cmake](https://github.com/onqtam/awesome-cmake) â€“ CMake  * [awesome-cms](https://github.com/postlight/awesome-cms) â€“ Open and closed source Content Management Systems (CMS)  * [Awesome-CobaltStrike-Defence](https://github.com/MichaelKoczwara/Awesome-CobaltStrike-Defence) â€“ Defences against [Cobalt Strike](https://www.cobaltstrike.com/), Adversary Simulations and Red Team Operations software.  * [awesome-cobol](https://github.com/mickaelandrieu/awesome-cobol) â€“ COBOL programming language  * [awesome-cocoa](https://github.com/v-braun/awesome-cocoa) â€“ Cocoa controls for iOS, watchOS and macOS    - http://cocoa.rocks  * [awesome-code-formatters](https://github.com/rishirdua/awesome-code-formatters)  * [awesome-code-review](https://github.com/joho/awesome-code-review)  * [awesome-codepoints](https://github.com/Codepoints/awesome-codepoints) â€“ Interesting Unicode characters  * [awesome-coins](https://github.com/Zheaoli/awesome-coins) â€“ Guide to cryto-currencies and their algos.  * [awesome-cold-showers](https://github.com/hwayne/awesome-cold-showers) â€“ For when people get too hyped up about things.  * [awesome-coldfusion](https://github.com/seancoyne/awesome-coldfusion)  * [awesome-common-lisp-learning](https://github.com/GustavBertram/awesome-common-lisp-learning)  * [awesome-community](https://github.com/phpearth/awesome-community) â€“ development, support and discussion channels, groups and communities.  * [awesome-community-building](https://github.com/CrowdDevHQ/awesome-community-building) â€“ Building developer communities.  * [awesome-community-detection](https://github.com/benedekrozemberczki/awesome-community-detection) â€“ Community detection papers with implementations.  * [awesome-comparisons](https://github.com/dhamaniasad/awesome-comparisons) â€“ Framework and code comparison projects, like TodoMVC and Notejam.  * [awesome-competitive-programming](https://github.com/lnishan/awesome-competitive-programming) â€“ Competitive Programming, Algorithm and Data Structure resources    - http://codeforces.com/blog/entry/23054  * [awesome-composer](https://github.com/jakoch/awesome-composer) â€“ Composer, Packagist, Satis PHP ecosystem  * [awesome-computational-neuroscience](https://github.com/eselkin/awesome-computational-neuroscience) â€“ Schools and researchers in computational neuroscience  * [awesome-computer-history](https://github.com/watson/awesome-computer-history) â€“ Computer history videos, documentaries and related folklore.  * [awesome-computer-vision](https://github.com/AGV-IIT-KGP/awesome-computer-vision) by @AGV-IIT-KGP  * [awesome-computer-vision](https://github.com/jbhuang0604/awesome-computer-vision) by @jbhuang0604  * [awesome-computer-vision-models](https://github.com/nerox8664/awesome-computer-vision-models) â€“ Popular deep learning models related to classification and segmentation task  * [awesome-conference-playlists](https://github.com/chentsulin/awesome-conference-playlists) â€“ Video playlists for conferences.  * [awesome-conferences](https://github.com/RichardLitt/awesome-conferences)  * [awesome-connectivity-info](https://github.com/stevesong/awesome-connectivity-info) â€“ Connectivity indexes and reports to help you better under who has access to communication infrastructure and on what terms.  * [awesome-conservation-tech](https://github.com/anselmbradford/awesome-conservation-tech) â€“ Intersection of tech and environmental conservation.  * [awesome-console-services](https://github.com/chubin/awesome-console-services) â€“ Console services (reachable via HTTP, HTTPS and other network protocols).  * [awesome-construct](https://github.com/WebCreationClub/awesome-construct) â€“ [Construct](https://www.construct.net/) game development toolkit  * [awesome-container](https://github.com/tcnksm/awesome-container) â€“ Container technologies and services.  * [awesome-conversational](https://github.com/mortenjust/awesome-conversational) â€“ Conversational UI  * [awesome-cordova](https://github.com/busterc/awesome-cordova) _Apache Cordova / PhoneGap_  * [Awesome-CoreML-Models](https://github.com/likedan/Awesome-CoreML-Models) â€“ Models for Core ML (for iOS 11+)  * [awesome-coronavirus](https://github.com/soroushchehresa/awesome-coronavirus) â€“ Projects and resources related to SARS-CoV-2 and COVID-19.  * [awesome-couchdb](https://github.com/quangv/awesome-couchdb) â€“ CouchDB resource list.  * [awesome-courses](https://github.com/fffaraz/awesome-courses) by @fffaraz â€“ Online programming/CS courses.  * [awesome-courses](https://github.com/prakhar1989/awesome-courses) by @prakhar1989 â€“ University Computer Science courses across the web.  * [awesome-cpp](https://github.com/fffaraz/awesome-cpp) â€“ C/C++  * [awesome-crdt](https://github.com/alangibson/awesome-crdt) â€“ Conflict-free replicated data types  * [awesome-creative-coding](https://github.com/terkelg/awesome-creative-coding) â€“ Creative Coding: Generative Art, Data visualization, Interaction Design  * [awesome-critical-tech-reading-list](https://github.com/chobeat/awesome-critical-tech-reading-list) â€“ Reading list for the modern critical programmer.  * [Awesome-Cross-Platform-Apps](https://github.com/Juude/Awesome-Cross-Platform-Apps) â€“ Solutions for building cross-platform apps.  * [awesome-cross-platform-nodejs](https://github.com/bcoe/awesome-cross-platform-nodejs) â€“ Tools for writing cross-platform Node.js code.  * [awesome-crypto-papers](https://github.com/pFarb/awesome-crypto-papers) â€“ Cryptography papers, articles, tutorials and howtos.  * [awesome-cryptocurrencies](https://github.com/kasketis/awesome-cryptocurrencies)  * [awesome-cryptography](https://github.com/sobolevn/awesome-cryptography) â€“ Cryptography and encryption resources.  * [awesome-crystal](https://github.com/veelenga/awesome-crystal) â€“ Crystal Language  * [awesome-css](https://github.com/awesome-css-group/awesome-css) by @awesome-css-group  * [awesome-css](https://github.com/bring2dip/awesome-css) by @deepakbhattarai  * [awesome-css-frameworks](https://github.com/troxler/awesome-css-frameworks) â€“ CSS frameworks  * [awesome-css-learning](https://github.com/micromata/awesome-css-learning) â€“ A tiny list limited to the best CSS Learning Resources  * [awesomeCSV](https://github.com/secretGeek/awesomeCSV) â€“ CSV, Comma Separated Values format  * [awesome-ctf](https://github.com/apsdehal/awesome-ctf) â€“ [Capture the Flag](https://en.wikipedia.org/wiki/Capture_the_flag#Computer_security)    - https://apsdehal.in/awesome-ctf/  * [awesome-cto](https://github.com/kuchin/awesome-cto) â€“ Resources for Chief Technology Officers, with the emphasis on startups  * [awesome-cto-resources](https://github.com/mateusz-brainhub/awesome-cto-resources) â€“ Grow as a Chief Technology Officer.  * [awesome-cybersecurity-blueteam](https://github.com/fabacab/awesome-cybersecurity-blueteam) â€“ [Cybersecurity blue teams](https://en.wikipedia.org/wiki/Blue_team_(computer_security)) resources  * [awesome-cyclejs](https://github.com/cyclejs-community/awesome-cyclejs) â€“ Cycle.js framework  * [awesome-d](https://github.com/zhaopuming/awesome-d) â€“ D programming language.  * [awesome-d3](https://github.com/wbkd/awesome-d3) â€“ [D3js](http://d3js.org/) libraries, plugins and utilities.  * [awesome-dart](https://github.com/yissachar/awesome-dart)  * [awesome-dash](https://github.com/ucg8j/awesome-dash) â€“ [Dash (plotly)](https://plot.ly/dash/) framework for analytical web applications  * [awesome-dashboard](https://github.com/obazoud/awesome-dashboard) â€“ Dashboards/visualization resources.  * [awesome-data-engineering](https://github.com/igorbarinov/awesome-data-engineering) â€“ Data engineering tools for software developers.  * [awesome-datascience](https://github.com/academic/awesome-datascience) â€“ An open source DataScience repository to learn and apply for real world problems.  * [awesome-datasets](https://github.com/viisar/awesome-datasets) â€“ Datasets for papers/experiments/validation.  * [awesome-dataviz](https://github.com/fasouto/awesome-dataviz) â€“ Data visualizations frameworks, libraries and software.  * [awesome-db](https://github.com/numetriclabz/awesome-db) â€“ Database libraries and resources.  * [awesome-ddd](https://github.com/heynickc/awesome-ddd) by @heynickc â€“ Domain-Driven Design (DDD), Command Query Responsibility Segregation (CQRS), Event Sourcing, and Event Storming  * [awesome-ddd](https://github.com/wkjagt/awesome-ddd) by @wkjagt â€“ Domain-Driven Design  * [awesome-decentralized-web](https://github.com/gdamdam/awesome-decentralized-web) â€“ Decentralized services and technologies  * [awesome-decision-tree-papers](https://github.com/benedekrozemberczki/awesome-decision-tree-papers) â€“ Decision Tree Research Papers  * [awesome-deep-learning](https://github.com/ChristosChristofidis/awesome-deep-learning) â€“ Deep Learning tutorials, projects and communities.  * [awesome-deep-learning-papers](https://github.com/terryum/awesome-deep-learning-papers) â€“ The most cited deep learning papers  * [awesome-deep-learning-resources](https://github.com/guillaume-chevalier/awesome-deep-learning-resources) â€“ Rough list of resources about deep learning.  * [awesome-deep-rl](https://github.com/tigerneil/awesome-deep-rl) â€“ Deep Reinforcement Learning  * [awesome-deep-vision](https://github.com/kjw0612/awesome-deep-vision) â€“ Computer vision / deep learning.  * [awesome-deku](https://github.com/lambtron/awesome-deku) â€“ Resources for the Deku library.  * [awesome-delphi](https://github.com/Fr0sT-Brutal/awesome-delphi)  * [awesome-deno](https://github.com/denolib/awesome-deno) â€“ [Deno](https://deno.land/), a secure runtime for JavaScript and TypeScript.  * [awesome-derby](https://github.com/russll/awesome-derby) â€“ Components for DerbyJS.  * [awesome-design](https://github.com/troyericg/awesome-design) â€“ Resources for digital designers.  * [awesome-design-patterns](https://github.com/DovAmir/awesome-design-patterns) â€“ Resources on software design patterns.  * [awesome-design-principles](https://github.com/robinstickel/awesome-design-principles)  * [awesome-design-systems](https://github.com/alexpate/awesome-design-systems)  * [Awesome-Design-Tools](https://github.com/goabstract/Awesome-Design-Tools)    - https://flawlessapp.io/designtools  * [awesome-desktop-js](https://github.com/styfle/awesome-desktop-js) â€“ Implementing desktop apps with JavaScript  * [awesome-dev-discord](https://github.com/ljosberinn/awesome-dev-discord) â€“ Official, development-related Discord servers.    - https://dev-discords.now.sh/  * [awesome-dev-fun](https://github.com/mislavcimpersak/awesome-dev-fun) â€“ Fun libs/packages/languages that have no real purpose but to make a developer chuckle.  * [awesome-developer-blogs](https://github.com/endymion1818/awesome-developer-blogs)  * [awesome-developer-experience](https://github.com/prokopsimek/awesome-developer-experience)  * [awesome-devenv](https://github.com/jondot/awesome-devenv) â€“ Tools, resources and workflow tips making an awesome development environment.  * [awesome-devops](https://github.com/joubertredrat/awesome-devops)  * [awesome-devrel](https://github.com/devrelcollective/awesome-devrel) â€“ Developer Relations  * [awesome-devtools](https://github.com/moimikey/awesome-devtools) â€“ In-browser bookmarklets, tools, and resources for front-end devs.  * [awesome-digital-nomads](https://github.com/cbovis/awesome-digital-nomads) â€“ Resources for Digital Nomads.  * [awesome-digitalocean](https://github.com/jonleibowitz/awesome-digitalocean) â€“ DigitalOcean cloud infrastructure provider  * [awesome-discord](https://github.com/alfg/awesome-discord) by @alfg  * [awesome-discord](https://github.com/jacc/awesome-discord) by @jacc â€“ Discord chat and VoIP application.  * [awesome-discord-communities](https://github.com/mhxion/awesome-discord-communities) â€“ Discord communities for programmers.  * [awesome-diversity](https://github.com/folkswhocode/awesome-diversity) â€“ Diversity in technology.  * [awesome-django](https://github.com/wsvincent/awesome-django) â€“ [Django](https://www.djangoproject.com/) Python web framework  * [awesome-django-cms](https://github.com/mishbahr/awesome-django-cms) â€“ django CMS add-ons.  * [awesome-docker](https://github.com/veggiemonk/awesome-docker) by @veggiemonk  * [awesome-docsify](https://github.com/docsifyjs/awesome-docsify) â€“ [docsify](https://docsify.js.org/) documentation site generator.  * [awesome-doctrine](https://github.com/biberlabs/awesome-doctrine) â€“ Doctrine ORM libraries and resources.  * [awesome-document-understanding](https://github.com/tstanislawek/awesome-document-understanding) â€“ Automated data extraction from documents.  * [awesome-dojo](https://github.com/petk/awesome-dojo) â€“ Dojo JavaScript Toolkit resources and libraries.  * [awesome-dot-dev](https://github.com/orbit-love/awesome-dot-dev) â€“ Developer resources on the .dev TLD.  * [awesome-dotfiles](https://github.com/webpro/awesome-dotfiles)  * [awesome-dotnet](https://github.com/quozd/awesome-dotnet) â€“ .NET libraries, tools, frameworks and software.  * [awesome-dotnet-architecture](https://github.com/mehdihadeli/awesome-dotnet-architecture) â€“ Software architecture, patterns, and principles in .NET platform.  * [awesome-dotnet-async](https://github.com/mehdihadeli/awesome-dotnet-async) â€“ Async, threading, and channels in .NET platform,  * [awesome-dotnet-core](https://github.com/thangchung/awesome-dotnet-core) â€“ .NET core libraries, tools, frameworks and software  * [awesome-dotnet-core-education](https://github.com/mehdihadeli/awesome-dotnet-core-education) â€“ .NET Core education resources.  * [awesome-draft-js](https://github.com/nikgraf/awesome-draft-js) â€“ [Draft.js](https://draftjs.org/) text editor framework  * [awesome-dropwizard](https://github.com/stve/awesome-dropwizard) â€“ [Dropwizard](https://www.dropwizard.io/) Java web framework  * [awesome-drupal](https://github.com/emincansumer/awesome-drupal) by @emincansumer  * [awesome-drupal](https://github.com/mrsinguyen/awesome-drupal) by @mrsinguyen  * [awesome-drupal](https://github.com/nirgn975/awesome-drupal) by @nirgn975 â€“ Useful resources for Drupal CMS :droplet:  * [awesome-dtrace](https://github.com/xen0l/awesome-dtrace) â€“ DTrace books, articles, videos, tools and resources.    - https://awesome-dtrace.com  * [awesome-ebpf](https://github.com/zoidbergwill/awesome-ebpf) â€“ eBPF Linux packet filter  * [awesome-economics](https://github.com/antontarasenko/awesome-economics) â€“ Economics related projects, software, people  * [awesome-ecs](https://github.com/nathanpeck/awesome-ecs) â€“ AWS Elastic Container Service and Fargate.  * [awesome-edtech-tools](https://github.com/hkalant/awesome-edtech-tools) â€“ Tools and resources for educators and virtual teachers.  * [awesome-educate](https://github.com/mercer/awesome-educate) â€“ Education resources online.  * [awesome-educational-games](https://github.com/yrgo/awesome-educational-games) â€“ Educational games to learn editors, languages, programming  * [awesome-ejabberd](https://github.com/shantanu-deshmukh/awesome-ejabberd) â€“ All awesome stuff of the ejabberd ecosystem.    - https://ejabberd.shantanudeshmukh.com  * [awesome-electron](https://github.com/sindresorhus/awesome-electron) â€“ Resources for creating apps with [Electron](http://electron.atom.io/) (formerly atom-shell).  * [awesome-electronics](https://github.com/kitspace/awesome-electronics) â€“ Electronic engineering  * [awesome-elixir](https://github.com/h4cc/awesome-elixir)  * [awesome-elm](https://github.com/sporto/awesome-elm) â€“ [Elm](https://elm-lang.org/), a functional reactive language  * [awesome-emacs](https://github.com/emacs-tw/awesome-emacs) by @emacs-tw  * [awesome-emacs](https://github.com/sefakilic/awesome-emacs) by @sefakilic  * [awesome-emacs](https://github.com/tacticiankerala/awesome-emacs) by @tacticiankerala  * [awesome-emails](https://github.com/jonathandion/awesome-emails) â€“ Build better emails.  * [awesome-embedded-rust](https://github.com/rust-embedded/awesome-embedded-rust) â€“ Embedded and Low-level development in the Rust programming language  * [awesome-ember](https://github.com/ember-community-russia/awesome-ember) by @ember-community-russia â€“ [Ember.js](https://emberjs.com/) JavaScript framework  * [awesome-ember](https://github.com/nmec/awesome-ember) by @nmec â€“ Ember.js things.  * [awesome-endless-codeforall-list](https://github.com/RobTranquillo/awesome-endless-codeforall-list) â€“ Every tool that civic hackers worldwide use to work.  * [awesome-engineer-onboarding](https://github.com/posquit0/awesome-engineer-onboarding)  * [awesome-engineering-ladders](https://github.com/posquit0/awesome-engineering-ladders)  * [awesome-engineering-team-principles](https://github.com/posquit0/awesome-engineering-team-principles)  * [awesome-eosio](https://github.com/DanailMinchev/awesome-eosio) â€“ [EOS.IO](https://eos.io/) blockchain protocol  * [awesome-erlang](https://github.com/drobakowski/awesome-erlang)  * [awesome-eslint](https://github.com/dustinspecker/awesome-eslint) â€“ [ESLint](https://eslint.org/) JavaScript linter  * [awesome-esolangs](https://github.com/angrykoala/awesome-esolangs) â€“ Esoteric languages  * [awesome-eta](https://github.com/sfischer13/awesome-eta) â€“ [Eta](https://eta-lang.org/) programming language  * [awesome-ethereum](https://github.com/bekatom/awesome-ethereum) by @bekatom â€“ [Ethereum](https://ethereum.org/) decentralized software platform & Dapps.  * [Awesome-Ethereum](https://github.com/ttumiel/Awesome-Ethereum) by @ttumiel  * [awesome-ethereum](https://github.com/vinsgo/awesome-ethereum) by @vinsgo    - http://awesome-ethereum.com/  * [awesome-ethereum-virtual-machine](https://github.com/pirapira/awesome-ethereum-virtual-machine)  * [awesome-falsehood](https://github.com/kdeldycke/awesome-falsehood) â€“ Falsehoods programmers believe in.  * [awesome-fantasy](https://github.com/r7kamura/awesome-fantasy) â€“ FinalFantasy-ish metaphors in software.  * [awesome-fast-check](https://github.com/dubzzz/awesome-fast-check) â€“ [fast-check](https://github.com/dubzzz/fast-check/) property based testing framework for JavaScript/TypeScript  * [awesome-fastapi](https://github.com/mjhea0/awesome-fastapi) â€“ [FastAPI](https://fastapi.tiangolo.com/) Python web framework  * [awesome-feathersjs](https://github.com/feathersjs/awesome-feathersjs) â€“ [Feathers](https://feathersjs.com/) Node.js framework for real-time applications REST APIs.  * [awesome-fediverse](https://github.com/emilebosch/awesome-fediverse) â€“ [Fediverse](https://en.wikipedia.org/wiki/Fediverse) resources.  * [awesome-ffmpeg](https://github.com/transitive-bullshit/awesome-ffmpeg) â€“ FFmpeg resources.  * [awesome-firebase](https://github.com/jthegedus/awesome-firebase) â€“ Firebase mobile development platform  * [awesome.fish](https://github.com/jorgebucaran/awesome.fish) â€“ Fish shell    - https://git.io/awesome-fish  * [awesome-flask](https://github.com/humiaozuzu/awesome-flask) â€“ Flask Python web framework resources and plugins.  * [awesome-flexbox](https://github.com/afonsopacifer/awesome-flexbox) â€“ CSS Flexible Box Layout Module.  * [awesome-fluidapp](https://github.com/lborgav/awesome-fluidapp) â€“ Icons, Userstyles and Userscripts for Fluid Apps  * [awesome-flutter](https://github.com/Solido/awesome-flutter) â€“ An awesome list that curates the best Flutter libraries, tools, tutorials, articles and more.  * [awesome-fonts](https://github.com/brabadu/awesome-fonts) â€“ Fonts and everything  * [awesome-food](https://github.com/jzarca01/awesome-food) â€“ Food related software projects  * [awesome-for-beginners](https://github.com/MunGell/awesome-for-beginners) â€“ Beginner-friendly projects to start contributing.  * [awesome-fortran](https://github.com/rabbiabram/awesome-fortran)  * [awesome-foss-apps](https://github.com/DataDaoDe/awesome-foss-apps) â€“ Production grade free and open source software  * [awesome-fp-js](https://github.com/stoeffel/awesome-fp-js) â€“ Functional programming stuff in JavaScript.  * [awesome-framer](https://github.com/podo/awesome-framer) â€“ Framer prototyping tool  * [awesome-fraud-detection-papers](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers) â€“ Fraud detection research papers.  * [awesome-frc](https://github.com/andrewda/awesome-frc) â€“ First Robotics Competition  * [awesome-free-software](https://github.com/johnjago/awesome-free-software) â€“ Free as in freedom software  * [awesome-frege](https://github.com/sfischer13/awesome-frege) â€“ [Frege](https://github.com/Frege/frege) programming language  * [awesome-fsharp](https://github.com/fsprojects/awesome-fsharp) â€“ F# programming language  * [awesome-fsm](https://github.com/leonardomso/awesome-fsm) by @leonardomso â€“ Finite State Machines and Statecharts  * [awesome-fsm](https://github.com/soixantecircuits/awesome-fsm) by @soixantecircuits â€“ Finite State Machines  * [awesome-functional-programming](https://github.com/lucasviola/awesome-functional-programming) by @lucasviola  * [awesome-functional-programming](https://github.com/xgrommx/awesome-functional-programming) by @xgrommx  * [awesome-funny-markov](https://github.com/sublimino/awesome-funny-markov) â€“ Delightfully amusing and facetious Markov chain output.  * [awesome-fuse](https://github.com/fuse-compound/awesome-fuse) â€“ [Fuse](https://fuseopen.com/) mobile development framework  * [awesome-fuzzing](https://github.com/cpuu/awesome-fuzzing) â€“ Fuzzing (or Fuzz Testing) for software security  * [awesome-gametalks](https://github.com/hzoo/awesome-gametalks) â€“ Gaming talks (development, design, etc)  * [awesome-gbdev](https://github.com/gbdev/awesome-gbdev) â€“ Game Boy development resources such as tools, docs, emulators, related projects and open-source ROMs    - https://gbdev.github.io/list  * [awesome-geek-podcasts](https://github.com/ayr-ton/awesome-geek-podcasts) â€“ Podcasts we like to listen to.    - http://ayr-ton.github.io/awesome-geek-podcasts  * [awesome-gemini](https://github.com/kr1sp1n/awesome-gemini) â€“ [Gemini protocol](https://gemini.circumlunar.space/)  * [awesome-geojson](https://github.com/tmcw/awesome-geojson) â€“ GeoJSON  * [awesome-ggplot2](https://github.com/erikgahner/awesome-ggplot2) â€“ [ggplot2](https://ggplot2.tidyverse.org/) data visualization for R.  * [awesome-gideros](https://github.com/stetso/awesome-gideros) â€“ [Gideros](http://giderosmobile.com/) game development framework  * [awesome-gif](https://github.com/davisonio/awesome-gif) â€“ GIF software resources    - https://davison.io/awesome-gif  * [awesome-gists](https://github.com/vsouza/awesome-gists) â€“ Amazing gists  * [awesome-git](https://github.com/dictcp/awesome-git) â€“ Git tools, resources and shiny things.  * [awesome-git-addons](https://github.com/stevemao/awesome-git-addons) â€“ Add-ons that extend/enhance the git CLI.  * [awesome-git-hooks](https://github.com/CompSciLauren/awesome-git-hooks) â€“ Easy-to-use git hooks for automating tasks during git workflows.  * [awesome-github](https://github.com/AntBranch/awesome-github) _In Chinese_ by @AntBranch â€“ GitHub guides, articles, sites, tools, projects and resources.  æ”¶é›†è¿™ä¸ªåˆ—è¡¨ï¼Œåªæ˜¯ä¸ºäº†æ›´å¥½åœ°ä½¿ç”¨äº²çˆ±çš„GitHub,æ¬¢è¿Žæäº¤prå’Œissueã€‚    - https://github.com/AntBranch/awesome-github  * [awesome-github](https://github.com/fffaraz/awesome-github) by @fffaraz â€“ Git and GitHub references.  * [awesome-github](https://github.com/Kikobeats/awesome-github) by @Kikobeats â€“ GitHub secrets and goodies.  * [awesome-github](https://github.com/phillipadsmith/awesome-github) by @phillipadsmith â€“ GitHub's awesomeness  * [awesome-github-repo](https://github.com/flyhigher139/awesome-github-repo) â€“ GitHub repositories; various topics like study materials, Raspberry Pi etc.  * [awesome-gnome](https://github.com/Kazhnuz/awesome-gnome) â€“ Gnome Desktop Environment.  * [awesome-go](https://github.com/avelino/awesome-go) by @avelino â€“ Golang    - http://awesome-go.com/  * [awesome-go-books](https://github.com/heatroom/awesome-go-books) â€“ Online and free golang books.  * [awesome-godot](https://github.com/godotengine/awesome-godot) â€“ [Godot](https://godotengine.org/) game engine  * [awesome-gradient-boosting-papers](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers) â€“ Gradient boosting research papers with implementations.  * [awesome-grails](https://github.com/hitenpratap/awesome-grails)  * [awesome-graph-classification](https://github.com/benedekrozemberczki/awesome-graph-classification) â€“ Graph embedding papers with implementations.  * [awesome-graphql](https://github.com/chentsulin/awesome-graphql) â€“ GraphQL & Relay Resources.  * [awesome-groovy](https://github.com/kdabir/awesome-groovy)  * [awesome-growth-hacking](https://github.com/bekatom/awesome-growth-hacking)  * [awesome-gulp](https://github.com/alferov/awesome-gulp) â€“ [Gulp](http://gulpjs.com/) build system resources and plugins.  * [awesome-gyazo](https://github.com/gyazo/awesome-gyazo) â€“ Tools for [Gyazo](https://gyazo.com/) screen capture application.  * [awesome-h2o](https://github.com/h2oai/awesome-h2o) â€“ H2O Machine Learning  * [awesome-hacking](https://github.com/carpedm20/awesome-hacking)  * [awesome-hacktoberfest-2020](https://github.com/Piyushhbhutoria/awesome-hacktoberfest-2020) â€“ [Hacktoberfest](https://hacktoberfest.digitalocean.com/)-friendly repositories and resources.  * [awesome-hadoop](https://github.com/youngwookim/awesome-hadoop) â€“ Hadoop and Hadoop ecosystem resources.  * [awesome-haskell](https://github.com/krispo/awesome-haskell)  * [awesome-hasura](https://github.com/aaronhayes/awesome-hasura) â€“ [Hasura](https://hasura.io/) is an instant realtime GraphQL engine for PostgreSQL.  * [awesome-haxe-gamedev](https://github.com/dvergar/awesome-haxe-gamedev) â€“ Game development in [Haxe](https://haxe.org/) cross-platform programming language  * [awesome-hbase](https://github.com/rayokota/awesome-hbase) â€“ Apache HBase  * [awesome-hdl](https://github.com/drom/awesome-hdl) â€“ Hardware Description Languages  * [awesome-healthcare](https://github.com/kakoni/awesome-healthcare) â€“ Open source healthcare software, libraries, tools and resources.  * [awesome-heroku](https://github.com/ianstormtaylor/awesome-heroku) â€“ Heroku resources.  * [awesome_hierarchical_matrices](https://github.com/gchavez2/awesome_hierarchical_matrices) â€“ Hierarchical matrices frameworks, libraries, and software.  * [awesome-home-assistant](https://github.com/frenck/awesome-home-assistant) â€“ [Home Assistant](https://www.home-assistant.io/) home automation    - https://awesome-ha.com  * [awesome-homematic](https://github.com/homematic-community/awesome-homematic) â€“ [HomeMatic](https://www.homematic.com/) home automation  * [awesome-honeypots](https://github.com/paralax/awesome-honeypots) â€“ Honeypot resources  * [awesome-html5](https://github.com/diegocard/awesome-html5)  * [awesome-humane-tech](https://github.com/humanetech-community/awesome-humane-tech) â€“ Promoting Solutions that Improve Wellbeing, Freedom and Society  * [awesome-hyper](https://github.com/bnb/awesome-hyper) â€“ [Hyper](https://hyper.is/) terminal  * [awesome-ibmcloud](https://github.com/victorshinya/awesome-ibmcloud) â€“ IBM Cloud    - https://awesome-ibmcloud.mybluemix.net  * [awesome-icons](https://github.com/notlmn/awesome-icons) â€“ Downloadable SVG/PNG/Font icon projects  * [awesome-idris](https://github.com/joaomilho/awesome-idris) â€“ ð›Œ [Idris](https://www.idris-lang.org/), functional programming language with dependent types  * [awesome-incident-response](https://github.com/meirwah/awesome-incident-response) â€“ Resources useful for incident responders.  * [awesome-indie](https://github.com/mezod/awesome-indie) â€“ Resources for independent developers to make money  * [awesome-infinidash](https://github.com/joenash/awesome-infinidash)  * [awesome-influxdb](https://github.com/mark-rushakoff/awesome-influxdb) â€“ Resources for the time series database InfluxDB  * [awesome-information-retrieval](https://github.com/harpribot/awesome-information-retrieval) â€“ Information retrieval resources  * [awesome-inspectit](https://github.com/inspectit-labs/awesome-inspectit) â€“ InspectIT documentations and resources.  * [awesome-interview-questions](https://github.com/MaximAbramchuck/awesome-interview-questions) â€“ Interview questions.  * [awesome-ionic](https://github.com/candelibas/awesome-ionic) â€“ [Ionic](https://ionicframework.com/) mobile development framework  * [awesome-ios](https://github.com/vsouza/awesome-ios)  * [awesome-ios-cn](https://github.com/jobbole/awesome-ios-cn) _In Chinese_ â€“ iOS èµ„æºå¤§å…¨ä¸­æ–‡ç‰ˆï¼Œå†…å®¹åŒ…æ‹¬ï¼šæ¡†æž¶ã€ç»„ä»¶ã€æµ‹è¯•ã€Apple Storeã€SDKã€XCodeã€ç½‘ç«™ã€ä¹¦ç±ç­‰  * [awesome-ios-ui](https://github.com/cjwirth/awesome-ios-ui) â€“ UI/UX libraries for iOS.  * [awesome-IoT](https://github.com/dharmeshkakadia/awesome-IoT) by @dharmeshkakadia â€“ Internet of Things  * [awesome-iot](https://github.com/HQarroum/awesome-iot) by @HQarroum â€“ Internet of Things  * [awesome-IoT-hybrid](https://github.com/weblancaster/awesome-IoT-hybrid) â€“ Internet of Things and Hybrid Applications  * [awesome-ipfs](https://github.com/ipfs/awesome-ipfs) â€“ [IPFS](https://ipfs.io/) distributed web    - https://awesome.ipfs.io/  * [awesome-irc](https://github.com/davisonio/awesome-irc) â€“ Internet Relay Chat protocol.  * [awesome-it-quotes](https://github.com/victorlaerte/awesome-it-quotes) â€“ Collect all relevant quotes said over the history of IT  * [awesome-jamstack](https://github.com/automata/awesome-jamstack) â€“ [JAMstack](https://jamstack.org) (JavaScript, APIs, Markup)  * [awesome-java](https://github.com/akullpp/awesome-java)  * [awesome-javascript](https://github.com/sorrycc/awesome-javascript)  * [awesome-javascript-books](https://github.com/heatroom/awesome-javascript-books) â€“ Online and free JavaScript books.  * [awesome-javascript-learning](https://github.com/micromata/awesome-javascript-learning) â€“ Tiny list limited to the best JavaScript Learning Resources  * [awesome-jitsi](https://github.com/easyjitsi/awesome-jitsi) â€“ [Jitsi](https://jitsi.org/) open-source video conferencing.  * [awesome-jmeter](https://github.com/aliesbelik/awesome-jmeter) â€“ Apache JMeter load testing  * [awesome-job-boards](https://github.com/emredurukn/awesome-job-boards) by @emredurukn  * [awesome-job-boards](https://github.com/tramcar/awesome-job-boards) by @tramcar  * [awesome-jquery](https://github.com/petk/awesome-jquery)  * [awesome-js-drama](https://github.com/scottcorgan/awesome-js-drama) â€“ JavaScript topics the just might spark the next revolt!  * [awesome-json](https://github.com/burningtree/awesome-json)  * [awesome-jsonschema](https://github.com/jviotti/awesome-jsonschema) â€“ [JSON Schema](http://json-schema.org/).  * [awesome-json-datasets](https://github.com/jdorfman/awesome-json-datasets) â€“ JSON datasets that don't require authentication  * [awesome-json-next](https://github.com/json-next/awesome-json-next) â€“ What's Next for JSON for Structured (Meta) Data in Text.  * [awesome-julia](https://github.com/melvin0008/awesome-julia)  * [awesome-jupyter](https://github.com/markusschanta/awesome-jupyter) â€“ [Jupyter](https://jupyter.org/)  * [awesome-jvm](https://github.com/deephacks/awesome-jvm)  * [awesome-kafka](https://github.com/monksy/awesome-kafka) â€“ [Apache Kafka](http://kafka.apache.org/), distributed streaming platform  * [awesome-katas](https://github.com/gamontal/awesome-katas) â€“ Code katas  * [awesome-kde](https://github.com/francoism90/awesome-kde) â€“ KDE Desktop Environment.  * [awesome-knockout](https://github.com/dnbard/awesome-knockout) â€“ Plugins for Knockout MVVM framework.  * [awesome-koa](https://github.com/ellerbrock/awesome-koa) â€“ [Koa.js](https://koajs.com/) Web Framework    - https://ellerbrock.github.io/awesome-koa  * [awesome-koans](https://github.com/ahmdrefat/awesome-koans) â€“ Programming kÅans in various languages.  * [awesome-kotlin](https://github.com/KotlinBy/awesome-kotlin) â€“ [Kotlin](https://kotlinlang.org/) programming language    - https://kotlin.link/  * [awesome-kotlin-native](https://github.com/bipinvaylu/awesome-kotlin-native) â€“ Kotlin Multiplatform libraries & resources.  * [awesome-kr-foss](https://github.com/darjeeling/awesome-kr-foss) â€“ Korean open source projects.  * [awesome-kubernetes](https://github.com/ramitsurana/awesome-kubernetes)    - https://ramitsurana.github.io/awesome-kubernetes  * [awesome-landing-page](https://github.com/nordicgiant2/awesome-landing-page) â€“ Landing pages templates  * [awesome-languages](https://github.com/perfaram/awesome-languages) â€“ Open-source programming languages.  * [awesome-laravel](https://github.com/chiraggude/awesome-laravel) by @chiraggude  * [awesome-laravel](https://github.com/TimothyDJones/awesome-laravel) by @TimothyDJones  * [Awesome-Laravel-Education](https://github.com/fukuball/Awesome-Laravel-Education) _In English and Chinese_ â€“ Laravel PHP framework learning resources.  * [awesome-latam](https://github.com/gophers-latam/awesome-latam) _In Spanish_ â€“ Recursos en EspaÃ±ol para desarrolladores de Golang.    - https://gophers-latam.github.io/  * [awesome-LaTeX](https://github.com/egeerardyn/awesome-LaTeX)  * [awesome-ld-preload](https://github.com/gaul/awesome-ld-preload) â€“ LD_PRELOAD, a mechanism for changing application behavior at run-time.  * [awesome-leading-and-managing](https://github.com/LappleApple/awesome-leading-and-managing) â€“ Leading people and being a manager. Geared toward tech, but potentially useful to anyone.  * [awesome-learn-datascience](https://github.com/siboehm/awesome-learn-datascience) â€“ Resources to help you get started with Data Science  * [awesome-ledger](https://github.com/sfischer13/awesome-ledger) â€“ Ledger command-line accounting system  * [awesome-legacy-code](https://github.com/legacycoderocks/awesome-legacy-code) â€“ Legacy systems with publicly available source code  * [awesome-less](https://github.com/LucasBassetti/awesome-less) â€“ Less CSS preprocessor  * [awesome-lesscode](https://github.com/dream2023/awesome-lesscode) _In Chinese_ â€“ Low code / no code projects  * [awesome-libgdx](https://github.com/rafaskb/awesome-libgdx) â€“ [libGDX](https://libgdx.badlogicgames.com/) cross-platform games development framework  * [awesome-libgen](https://github.com/freereadorg/awesome-libgen) â€“ Library Genesis, the world's largest free library.  * [awesome-libra](https://github.com/learndapp/awesome-libra) by @learndapp â€“ [Libra](https://libra.org/) cryptocurrency by Facebook  * [awesome-libra](https://github.com/reed-hong/awesome-libra) by @reed-hong â€“ [Facebook Diem](https://www.diem.com/) (nÃ©e Libra) digital currency.  * [awesome-librehosters](https://github.com/libresh/awesome-librehosters) â€“ Nice hosting providers  * [awesome-linguistics](https://github.com/theimpossibleastronaut/awesome-linguistics) â€“ Tools, theory and platforms for linguistics.  * [awesome-links](https://github.com/rbk/awesome-links) â€“ Web Development Links by @richardbenjamin.  * [awesome-linters](https://github.com/caramelomartins/awesome-linters) â€“ Resources for a more literate programming.  * [awesome-linux](https://github.com/aleksandar-todorovic/awesome-linux) â€“ Linux software.  * [awesome-linux-containers](https://github.com/Friz-zy/awesome-linux-containers) â€“ Linux Containers frameworks, libraries and software  * [awesome-linux-resources](https://github.com/itech001/awesome-linux-resources)    - http://www.linux6.com  * [Awesome-Linux-Software](https://github.com/luong-komorebi/Awesome-Linux-Software) â€“ Linux applications for all users and developers.  * [awesome-linuxaudio](https://github.com/nodiscc/awesome-linuxaudio) â€“ Professional audio/video/live events production on Linux.  * [awesome-lit-html](https://github.com/web-padawan/awesome-lit-html) â€“ [lit-html](https://lit-html.polymer-project.org/) HTML templating library  * [awesome-livecoding](https://github.com/toplap/awesome-livecoding) â€“ All things Livecoding.  * [awesome-logging](https://github.com/roundrobin/awesome-logging)  * [awesome-loginless](https://github.com/fiatjaf/awesome-loginless) â€“ Internet services that don't require logins or registrations.  * [awesome-love2d](https://github.com/love2d-community/awesome-love2d) â€“ [LÃ–VE](http://love2d.org/) Lua game framework  * [awesome-lowcode](https://github.com/taowen/awesome-lowcode) _In Chinese_ â€“ Chinese low code platforms.  * [awesome-lua](https://github.com/forhappy/awesome-lua) by @forhappy  * [awesome-lua](https://github.com/LewisJEllis/awesome-lua) by @LewisJEllis  * [awesome-lumen](https://github.com/unicodeveloper/awesome-lumen) â€“ [Lumen](https://lumen.laravel.com/), PHP Microframework by Laravel  * [awesome-luvit](https://github.com/luvit/awesome-luvit) â€“ [Luvit](https://luvit.io/), asynchronous I/O for Lua  * [awesome-mac](https://github.com/jaywcjlove/awesome-mac) by @jaywcjlove â€“ Premium macOS software in various categories    - https://git.io/macx  * [awesome-mac](https://github.com/xyNNN/awesome-mac) by @xyNNN â€“ macOS tools, applications and games.  * [awesome-mac-apps](https://github.com/justin-j/awesome-mac-apps) â€“ macOS apps  * [awesome-machine-learning](https://github.com/josephmisiti/awesome-machine-learning)  * [awesome-macOS](https://github.com/iCHAIT/awesome-macOS) â€“ OS X applications, tools and communities.  * [awesome-macos-command-line](https://github.com/herrbischoff/awesome-macos-command-line) â€“ Shell commands and tools specific to OS X.  * [awesome-macos-screensavers](https://github.com/agarrharr/awesome-macos-screensavers) â€“ Screensavers for Mac OS X  * [awesome-mad-science](https://github.com/feross/awesome-mad-science) â€“ npm packages that make you say ""wow, didn't know that was possible!""  * [awesome-magento2](https://github.com/DavidLambauer/awesome-magento2) â€“ [Magento 2](https://magento.com/) PHP eCommerce platform    - https://davidlambauer.github.io/awesome-magento2/  * [awesome-maintainers](https://github.com/nayafia/awesome-maintainers) â€“ Talks, blog posts, and interviews about the experience of being an open source maintainer  * [awesome-malware-analysis](https://github.com/rshipp/awesome-malware-analysis)  * [awesome-manifestos](https://github.com/imsky/awesome-manifestos) â€“ Interesting software manifestos and principles  * [awesome-marionette](https://github.com/sadcitizen/awesome-marionette) â€“ [marionette.js](https://marionettejs.com/) framework  * [awesome-markdown](https://github.com/BubuAnabelas/awesome-markdown)  * [awesome-markdown-alternatives](https://github.com/mundimark/awesome-markdown-alternatives) â€“ Light-weight markup markdown alternatives.  * [awesome-masonite](https://github.com/vaibhavmule/awesome-masonite) â€“ [Masonite](https://docs.masoniteproject.com/) Python web framework  * [awesome-mastodon](https://github.com/tleb/awesome-mastodon) â€“ [Mastodon](https://joinmastodon.org/) decentralized microblogging network  * [awesome-material](https://github.com/sachin1092/awesome-material) â€“ Google's material design  * [Awesome-MaterialDesign](https://github.com/lightSky/Awesome-MaterialDesign) _In Chinese_ â€“ Resources and libraries for [Material Design](http://www.google.com/design/spec/material-design/introduction.html).  * [awesome-math](https://github.com/rossant/awesome-math) â€“ Mathematics  * [awesome-MATLAB](https://github.com/mikecroucher/awesome-MATLAB)  * [awesome-mechanical-keyboard](https://github.com/BenRoe/awesome-mechanical-keyboard) â€“ Mechanical Keyboards    - https://keebfol.io  * [awesome-mesos](https://github.com/dharmeshkakadia/awesome-mesos) by @dharmeshkakadia  * [awesome-mesos](https://github.com/parolkar/awesome-mesos) by @parolkar  * [awesome-meteor](https://github.com/Urigo/awesome-meteor)  * [awesome-meteor-developers](https://github.com/harryadel/awesome-meteor-developers) â€“ Ways to support Meteor developers and packages.  * [awesome-mews](https://github.com/MewsSystems/awesome-mews) â€“ Resources Mews developers like and aligns with their vision.  * [awesome-micro-npm-packages](https://github.com/parro-it/awesome-micro-npm-packages) â€“ Small, focused npm packages.  * [awesome-microbit](https://github.com/carlosperate/awesome-microbit) â€“ BBC micro:bit  * [awesome-microfrontends](https://github.com/ChristianUlbrich/awesome-microfrontends)  * [awesome-microservices](https://github.com/mfornos/awesome-microservices) â€“ Microservice Architecture related principles and technologies.  * [awesome-minecraft](https://github.com/bs-community/awesome-minecraft)  * [awesome-minimalist](https://github.com/neiesc/awesome-minimalist) â€“ Minimalist frameworks (simple and lightweight).  * [awesome-mobile](https://github.com/alec-c4/awesome-mobile) â€“ Instruments for mobile marketing and development  * [awesome-mobile-web-development](https://github.com/myshov/awesome-mobile-web-development) â€“ All that you need to create a great mobile web experience  * [awesome-mongodb](https://github.com/ramnes/awesome-mongodb)  * [awesome-monitoring](https://github.com/crazy-canux/awesome-monitoring) â€“ INFRASTRUCTUREã€OPERATION SYSTEM and APPLICATION monitoring tools for Operations.    - http://canuxcheng.com/awesome-monitoring/  * [awesome-monte-carlo-tree-search-papers](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers) â€“ Monte Carlo tree search, a heuristic search algorithm frequently used in games.  * [awesome-motion-design-web](https://github.com/lucasmaiaesilva/awesome-motion-design-web)  * [awesome-motion-planning](https://github.com/AGV-IIT-KGP/awesome-motion-planning) â€“ Papers, books and tools for motion planning.  * [awesome-mqtt](https://github.com/hobbyquaker/awesome-mqtt) â€“ MQTT related stuff.  * [awesome-msr](https://github.com/dspinellis/awesome-msr) â€“ Empirical Software Engineering: evidence-based, data-driven research on software systems  * [awesome-music](https://github.com/ciconia/awesome-music) â€“ Music, audio, MIDI  * [awesome-mysql](https://github.com/shlomi-noach/awesome-mysql) â€“ MySQL software, libraries, tools and resources  * [awesome-naming](https://github.com/gruhn/awesome-naming) â€“ When naming things is done right.  * [awesome-neo4j](https://github.com/neueda/awesome-neo4j) â€“ Neo4j graph database  * [awesome-netherlands-events](https://github.com/awkward/awesome-netherlands-events) â€“ Dutch (tech related) events  * [awesome-network-analysis](https://github.com/briatte/awesome-network-analysis)    - http://f.briatte.org/r/awesome-network-analysis-list  * [awesome-network-embedding](https://github.com/chihming/awesome-network-embedding) â€“ Papers on node embedding techniques.  * [awesome-network-js](https://github.com/Kikobeats/awesome-network-js) â€“ Network layer resources in pure JavaScript  * [Awesome-Networking](https://github.com/clowwindy/Awesome-Networking)  * [awesome-neuroscience](https://github.com/analyticalmonk/awesome-neuroscience) â€“ Neuroscience libraries, software and resources    - http://akashtandon.com/awesome-neuroscience/  * [awesome-newsletters](https://github.com/mpron/awesome-newsletters) by @mpron â€“ Developer newsletters  * [awesome-newsletters](https://github.com/webpro/awesome-newsletters) by @webpro â€“ The best (weekly) newsletters  * [awesome-newsletters](https://github.com/zudochkin/awesome-newsletters) by @zudochkin  * [awesome-nextjs](https://github.com/unicodeveloper/awesome-nextjs) â€“ [Next.js](https://nextjs.org/) React-based JavaScript framework  * [awesome-nim](https://github.com/VPashkov/awesome-nim) â€“ [Nim](https://nim-lang.org/) programming language  * [awesome-nlp](https://github.com/keon/awesome-nlp) â€“ Natural Language Processing.  * [Awesome-no-code-tools](https://github.com/ElijT/Awesome-no-code-tools)  * [awesome-no-login-web-apps](https://github.com/aviaryan/awesome-no-login-web-apps) â€“ Web apps that work without login  * [awesome-nocode](https://github.com/nslindtner/awesome-nocode)  * [awesome-node-esm](https://github.com/talentlessguy/awesome-node-esm) â€“ ES modules for Node.js  * [awesome-nodejs](https://github.com/sindresorhus/awesome-nodejs) by @sindresorhus  * [awesome-non-financial-blockchain](https://github.com/machinomy/awesome-non-financial-blockchain) â€“ Non-financial applications of blockchain  * [awesome-nosql-guides](https://github.com/erictleung/awesome-nosql-guides) â€“ NoSQL databases    - https://erictleung.com/awesome-nosql-guides/  * [awesome-npm](https://github.com/sindresorhus/awesome-npm)  * [awesome-npm-scripts](https://github.com/RyanZim/awesome-npm-scripts) â€“ using npm as a build tool  * [awesome-ntnu](https://github.com/michaelmcmillan/awesome-ntnu) â€“ Projects by NTNU students.  * [awesome-nuxt](https://github.com/nuxt-community/awesome-nuxt) â€“ Resources for [Nuxt.js](https://nuxtjs.org/), framework for universal Vue.js applications.  * [awesome-objc-frameworks](https://github.com/follyxing/awesome-objc-frameworks)  * [awesome-observables](https://github.com/sindresorhus/awesome-observables) â€“ An Observable is a collection that arrives over time.  * [awesome-obsidian](https://github.com/kmaasrud/awesome-obsidian) â€“ [Obsidian](https://obsidian.md/) knowledge base app.  * [awesome-ocaml](https://github.com/ocaml-community/awesome-ocaml)  * [awesome-ocap](https://github.com/dckc/awesome-ocap) â€“ Capability-based security enables the concise composition of powerful patterns of cooperation without vulnerability.  * [awesome-okr](https://github.com/domenicosolazzo/awesome-okr) â€“ Objective - Key Results, the best practice of setting and communicating company, team and employee objectives and measuring their progress based on achieved results  * [awesome-online-ide](https://github.com/styfle/awesome-online-ide) â€“ Online development environments    - https://ide.ceriously.com  * [awesome-online-machine-learning](https://github.com/MaxHalford/awesome-online-machine-learning) â€“ [Online machine learning](https://en.wikipedia.org/wiki/Online_machine_learning)  * [awesome-open-company](https://github.com/opencompany/awesome-open-company) â€“ Open companies: Share as much as possible, charge as little as possible.  * [awesome-open-science](https://github.com/silky/awesome-open-science)  * [awesome-open-source-supporters](https://github.com/zachflower/awesome-open-source-supporters) â€“ Companies that offer their services for free to Open Source projects  * [awesome-opengl](https://github.com/eug/awesome-opengl) â€“ OpenGL libraries, debuggers and resources.  * [awesome-opensource-documents](https://github.com/44bits/awesome-opensource-documents) â€“ Open source or open source licensed documents, guides, books.  * [awesome-OpenSourcePhotography](https://github.com/ibaaj/awesome-OpenSourcePhotography) â€“ Free open source software & libraries for photography. Also tools for video.  * [awesome-osc](https://github.com/amir-arad/awesome-osc) â€“ [Open Sound Control](http://opensoundcontrol.org/)  * [awesome-oss-alternatives](https://github.com/RunaCapital/awesome-oss-alternatives) â€“ Open-source alternatives to established SaaS products.  * [awesome-pascal](https://github.com/Fr0sT-Brutal/awesome-pascal) â€“ Delphi/FreePascal/(any)Pascal frameworks, libraries, resources, and shiny things.  * [awesome-pcaptools](https://github.com/caesar0301/awesome-pcaptools) â€“ Tools to process network traces.  * [awesome-pentest](https://github.com/enaqx/awesome-pentest) â€“ Penetration testing resources and tools.  * [awesome-pentest-cheat-sheets](https://github.com/coreb1t/awesome-pentest-cheat-sheets) â€“ Penetration testing  * [Awesome-People-in-Computer-Vision](https://github.com/solarlee/Awesome-People-in-Computer-Vision)  * [awesome-perfocards](https://github.com/Wolg/awesome-perfocards) _See [perfokaart](https://et.wikipedia.org/wiki/Perfokaart)._  * [awesome-perl](https://github.com/hachiojipm/awesome-perl)  * [awesome-persian](https://github.com/fffaraz/awesome-persian) â€“ Persian/Farsi supporting tools, fonts, and development resources.  * [awesome-phalcon](https://github.com/phalcon/awesome-phalcon) â€“ [Phalcon](https://phalconphp.com/en/) PHP framework libraries and resources.  * [awesome-pharo](https://github.com/pharo-open-documentation/awesome-pharo) â€“ [Pharo](https://pharo.org/) Smalltalk  * [awesome-pharo-ml](https://github.com/pharo-ai/awesome-pharo-ml) â€“ Machine learning, AI, data science in Pharo.  * [awesome-php](https://github.com/ziadoz/awesome-php)  * [awesome-PICO-8](https://github.com/pico-8/awesome-PICO-8) â€“ [PICO-8](https://www.lexaloffle.com/pico-8.php) fantasy console for making, sharing and playing tiny games    - https://pico-8.github.io/awesome-PICO-8/  * [awesome-pinned-gists](https://github.com/matchai/awesome-pinned-gists) â€“ Dynamic pinned gists for GitHub.  * [awesome-pipeline](https://github.com/pditommaso/awesome-pipeline) â€“ Pipeline toolkits.  * [awesome-piracy](https://github.com/Igglybuff/awesome-piracy) â€“ Warez and piracy links  * [awesome-pixel-art](https://github.com/Siilwyn/awesome-pixel-art)  * [awesome-play1](https://github.com/PerfectCarl/awesome-play1) â€“ Play Framework 1.x modules, tools, and resources.  * [awesome-plotters](https://github.com/beardicus/awesome-plotters) â€“ Computer-controlled drawing machines and other visual art robots.  * [awesome-podcasts](https://github.com/Ghosh/awesome-podcasts) by @Ghosh â€“ Podcasts for designers, developers, product managers, entrepreneurs and hustlers    - http://podcasts.surge.sh/  * [awesome-podcasts](https://github.com/rShetty/awesome-podcasts) by @rShetty â€“ Important Podcasts for software engineers.  * [awesome-pokemon](https://github.com/tobiasbueschel/awesome-pokemon) â€“ PokÃ©mon & PokÃ©mon Go  * [awesome-polymer](https://github.com/Granze/awesome-polymer) â€“ [Polymer Project](https://www.polymer-project.org/)  * [awesome-postcss](https://github.com/jdrgomes/awesome-postcss) â€“ [PostCSS](https://postcss.org/) CSS processor  * [awesome-postgres](https://github.com/dhamaniasad/awesome-postgres)  * [awesome-power-mode](https://github.com/codeinthedark/awesome-power-mode)  * [awesome-powershell](https://github.com/janikvonrotz/awesome-powershell)  * [awesome-preact](https://github.com/preactjs/awesome-preact) â€“ [Preact](https://github.com/preactjs/preact) JavaScript framework  * [awesome-prisma](https://github.com/catalinmiron/awesome-prisma) â€“ [Prisma](https://www.prisma.io/) GraphQL library  * [awesome-privacy](https://github.com/pluja/awesome-privacy) â€“ Services and alternatives that respect your privacy because PRIVACY MATTERS.  * [awesome-product-design](https://github.com/teoga/awesome-product-design) by @teoga â€“ Bookmarks, resources, articles for product designers.  * [awesome-product-design](https://github.com/ttt30ga/awesome-product-design) by @ttt30ga â€“ Resources for product designers.  * [awesome-product-management](https://github.com/dend/awesome-product-management) â€“ Resources for product/program managers to learn and grow.  * [awesome-productivity](https://github.com/jyguyomarch/awesome-productivity) â€“ Delightful productivity resources.  * [awesome-ProductManager](https://github.com/hugo53/awesome-ProductManager) â€“ Books and tools for Product Managers.  * [awesome-programming-for-kids](https://github.com/HollyAdele/awesome-programming-for-kids) â€“ Teaching kids programming  * [awesome-progressive-web-apps](https://github.com/TalAter/awesome-progressive-web-apps) â€“ Progressive Web Apps (PWA)  * [awesome-projects-boilerplates](https://github.com/melvin0008/awesome-projects-boilerplates)  * [awesome-prolog](https://github.com/klaussinani/awesome-prolog) â€“ Prolog logic programming language  * [awesome-prometheus](https://github.com/roaldnefs/awesome-prometheus) â€“ [Prometheus](https://prometheus.io/) monitoring system  * [awesome-prometheus-alerts](https://github.com/samber/awesome-prometheus-alerts) â€“ Prometheus alerting rules    - https://awesome-prometheus-alerts.grep.to  * [awesome-promises](https://github.com/wbinnssmith/awesome-promises) â€“ JavaScript Promises.  * [awesome-public-datasets](https://github.com/awesomedata/awesome-public-datasets) by @awesomedata â€“ (Large-scale) public datasets on the Internet.    - [source data](https://github.com/awesomedata/apd-core)  * [awesome-puppet](https://github.com/rnelson0/awesome-puppet)  * [awesome-pure-css-no-javascript](https://github.com/Zhangjd/awesome-pure-css-no-javascript) _In Chinese_  * [awesome-purescript](https://github.com/passy/awesome-purescript)  * [awesome-pyramid](https://github.com/uralbash/awesome-pyramid) â€“ Resources for Pyramid Python web framework.  * [awesome-python](https://github.com/kevmo/awesome-python) by @kevmo  * [awesome-python](https://github.com/vinta/awesome-python) by @vinta  * [awesome-python-cn](https://github.com/jobbole/awesome-python-cn) _In Chinese_  * [awesome-python-data-science](https://github.com/krzjoa/awesome-python-data-science)  * [awesome-python-in-education](https://github.com/quobit/awesome-python-in-education)  * [awesome-python-models](https://github.com/grundic/awesome-python-models) â€“ List of ORMs, models, schemas, serializers, etc. libraries  for python.  * [awesome-python-scientific-audio](https://github.com/faroit/awesome-python-scientific-audio) â€“ Python software and packages related to scientific research in audio  * [awesome-python-talks](https://github.com/jhermann/awesome-python-talks) â€“ Videos related to Python, with a focus on training and gaining hands-on experience.  * [awesome-python-typing](https://github.com/typeddjango/awesome-python-typing) â€“ Python types, stubs, plugins, and tools to work with them.  * [Awesome-pytorch-list](https://github.com/bharathgs/Awesome-pytorch-list) â€“ [PyTorch](https://pytorch.org/) Python machine learning framework.  * [awesome-qa](https://github.com/seriousran/awesome-qa) â€“ [Question Answering](https://en.wikipedia.org/wiki/Question_answering) systems automatically answer questions asked in a natural language  * [awesome-qsharp](https://github.com/ebraminio/awesome-qsharp) â€“ [Q#](https://docs.microsoft.com/en-us/quantum/) quantum programming language  * [awesome-qt](https://github.com/JesseTG/awesome-qt) by @JesseTG â€“ Qt framework  * [awesome-qt](https://github.com/skhaz/awesome-qt) by @skhaz â€“ Qt framework  * [awesome-quantified-self](https://github.com/woop/awesome-quantified-self) â€“ Devices, Wearables, Applications, and Platforms for Self Tracking  * [awesome-quantum-computing](https://github.com/desireevl/awesome-quantum-computing) â€“ Quantum computing learning and developing resources.  * [awesome-R](https://github.com/qinwf/awesome-R)  * [awesome-radio](https://github.com/kyleterry/awesome-radio) â€“ Radio and citizens band (CB) radio resources.  * [awesome-rails](https://github.com/dpaluy/awesome-rails) by @dpaluy  * [awesome-rails](https://github.com/gramantin/awesome-rails) by @gramantin â€“ Projects and sites made with Rails.  * [awesome-rails](https://github.com/ruby-vietnam/awesome-rails) by @ruby-vietnam â€“ Rails libraries/app examples/ebooks/tutorials/screencasts/magazines/news.  * [awesome-rails-gem](https://github.com/hothero/awesome-rails-gem) â€“ Ruby Gems for Rails development.  * [awesome-random-forest](https://github.com/kjw0612/awesome-random-forest) â€“ Decision forest, tree-based methods, including random forest, bagging, and boosting.  * [awesome-raspberry-pi](https://github.com/blackout314/awesome-raspberry-pi) by @blackout314    - http://blackout314.github.io/awesome-raspberry-pi/  * [awesome-raspberry-pi](https://github.com/thibmaek/awesome-raspberry-pi) by @thibmaek â€“ Raspberry Pi tools, projects, images and resources  * [awesome-react](https://github.com/enaqx/awesome-react) â€“ ReactJS tools, resources, videos.  * [awesome-react-components](https://github.com/brillout/awesome-react-components) â€“ React Components & Libraries.  * [awesome-react-hooks](https://github.com/glauberfc/awesome-react-hooks) â€“ React Hooks  * [awesome-react-native](https://github.com/jondot/awesome-react-native)    - http://www.awesome-react-native.com  * [awesome-react-state-management](https://github.com/olegrjumin/awesome-react-state-management)  * [awesome-react-state-management-tools](https://github.com/cs01/awesome-react-state-management-tools)  * [awesome-readme](https://github.com/matiassingers/awesome-readme) â€“ READMEs examples and best practices  * [awesome-reasonml](https://github.com/vramana/awesome-reasonml) â€“ [ReasonML](https://reasonml.github.io/), [BuckleScript](https://bucklescript.github.io/) and [OCaml](https://ocaml.org/) programming languages.  * [awesome-recommender-system](https://github.com/Geek4IT/awesome-recommender-system) â€“ Recommender System frameworks, libraries and software.  * [awesome-recursion-schemes](https://github.com/passy/awesome-recursion-schemes)  * [awesome-redux](https://github.com/brillout/awesome-redux) by @brillout â€“ Redux Libraries & Learning Material    - https://devarchy.com/redux  * [awesome-redux](https://github.com/xgrommx/awesome-redux) by @xgrommx â€“ [Redux](https://github.com/rackt/redux) web application state container  * [awesome-refinerycms](https://github.com/refinerycms-contrib/awesome-refinerycms) â€“ [Refinery](https://www.refinerycms.com/) Ruby on Rails CMS  * [awesome-regex](https://github.com/aloisdg/awesome-regex) â€“ Regular expressions  * [awesome-regression-testing](https://github.com/mojoaxel/awesome-regression-testing) â€“ Visual regression testing  * [awesome-relay](https://github.com/expede/awesome-relay) â€“ [Relay](https://relay.dev/) JavaScript framework for React and GraphQL  * [awesome-reMarkable](https://github.com/reHackable/awesome-reMarkable) â€“ [reMarkable](https://remarkable.com/) e-ink tablet.  * [awesome-remote-job](https://github.com/lukasz-madon/awesome-remote-job) â€“ Remote companies and other resources.  * [awesome-RemoteWork](https://github.com/hugo53/awesome-RemoteWork) â€“ Books and links about and for remote work.  * [awesome-research](https://github.com/emptymalei/awesome-research) â€“ Tools to help you with research/life    - http://openmetric.org/tool/  * [awesome-rest](https://github.com/marmelab/awesome-rest) â€“ Great resources about RESTful API architecture, development, test, and performance  * [awesome-rethinkdb](https://github.com/d3viant0ne/awesome-rethinkdb) â€“ [RethinkDB](https://rethinkdb.com/) realtime database  * [awesome-retrospectives](https://github.com/josephearl/awesome-retrospectives) â€“ Facilitating and learning about retrospectives.  * [awesome-ripple](https://github.com/vhpoet/awesome-ripple) â€“ [Ripple](https://ripple.com/) cryptocurrency  * [awesome-rl](https://github.com/aikorea/awesome-rl) â€“ Reinforcement Learning.  * [awesome-rl-for-cybersecurity](https://github.com/Limmen/awesome-rl-for-cybersecurity) â€“ Reinforcement learning applied to cyber security.  * [awesome-rnn](https://github.com/kjw0612/awesome-rnn) â€“ Recurrent Neural Networks.  * [awesome-roadmaps](https://github.com/liuchong/awesome-roadmaps) â€“ Skills roadmaps for software development  * [awesome-roam](https://github.com/roam-unofficial/awesome-roam) â€“ Roam Research networked note-taking  * [awesome-robotics](https://github.com/Kiloreux/awesome-robotics)  * [awesome-ros2](https://github.com/fkromer/awesome-ros2) â€“ [Robot Operating System](http://www.ros.org/)    - https://fkromer.github.io/awesome-ros2  * [awesome-roslyn](https://github.com/ironcev/awesome-roslyn) â€“ Roslyn .NET Compiler Platform  * [awesome-rshiny](https://github.com/grabear/awesome-rshiny) â€“ A curated list of resources for the R shiny package.    - https://grabear.github.io/awesome-rshiny/  * [awesome-ruby](https://github.com/markets/awesome-ruby) by @markets    - http://awesome-ruby.com/  * [awesome-ruby](https://github.com/Sdogruyol/awesome-ruby) by @Sdogruyol  * [awesome-ruby-ast](https://github.com/rajasegar/awesome-ruby-ast) â€“ Abstract Syntax Trees (AST) in Ruby  * [AwesomeRubyist/awesome_podcast_list](https://github.com/AwesomeRubyist/awesome_podcast_list) â€“ Podcasts about Ruby and development, also in Russian.  * [AwesomeRubyist/awesome_reading_list](https://github.com/AwesomeRubyist/awesome_reading_list) â€“ Books about Ruby and Rails.  * [AwesomeRubyist/awesome_resource_list](https://github.com/AwesomeRubyist/awesome_resource_list) â€“ Resources for Ruby and Rails.  * [awesome-rubymotion](https://github.com/motion-open-source/awesome-rubymotion) â€“ [RubyMotion](http://www.rubymotion.com/), cross-platform development in Ruby    - http://motion-open-source.github.io/awesome-rubymotion/  * [awesome-rust](https://github.com/rust-unofficial/awesome-rust)  * [awesome-rxjava](https://github.com/eleventigers/awesome-rxjava) â€“ RxJava, reactive programming library  * [awesome-salesforce](https://github.com/mailtoharshit/awesome-salesforce) â€“ Salesforce Platform Resources  * [awesome-saltstack](https://github.com/hbokh/awesome-saltstack) â€“ [SaltStack](https://www.saltstack.com/) configuration management  * [awesome-sarl](https://github.com/sarl/awesome-sarl) â€“ Resources for [SARL](http://www.sarl.io/) Agent-Oriented Programming Language.  * [awesome-SAS](https://github.com/huyingjie/awesome-SAS) â€“ [SAS](https://www.sas.com/) analysis system  * [awesome-sass](https://github.com/Famolus/awesome-sass) by @Famolus â€“ Sass and SCSS CSS preprocessor  * [awesome-sass](https://github.com/HugoGiraudel/awesome-sass) by @HugoGiraudel â€“ Sass and SCSS CSS preprocessor  * [awesome-satellite-imagery-datasets](https://github.com/chrieke/awesome-satellite-imagery-datasets) â€“ Satellite imagery datasets with annotations for computer vision and deep learning.  * [awesome-scala](https://github.com/lauris/awesome-scala) â€“ Scala programming language  * [awesome-scala-native](https://github.com/tindzk/awesome-scala-native) â€“ [Scala Native](http://www.scala-native.org) compiler  * [awesome-scalability](https://github.com/binhnguyennus/awesome-scalability) â€“ The Patterns of Scalable, Reliable, and Performant Large-Scale Systems  * [awesome-scientific-computing](https://github.com/nschloe/awesome-scientific-computing) â€“ Software for numerical analysis  * [awesome-scientific-writing](https://github.com/writing-resources/awesome-scientific-writing) â€“ Tools, demos and resources to go beyond LaTeX.  * [awesome-sdn](https://github.com/sdnds-tw/awesome-sdn) â€“ Software Defined Network (SDN)  * [awesome-sec-talks](https://github.com/PaulSec/awesome-sec-talks) â€“ Security talks.  * [awesome-security](https://github.com/sbilly/awesome-security) â€“ Software, libraries, documents, books, resources and cool stuff about security.  * [awesome-selenium](https://github.com/christian-bromann/awesome-selenium)  * [awesome-selfhosted](https://github.com/awesome-selfhosted/awesome-selfhosted) â€“ Network services and web applications which can be hosted locally.  * [awesome-semantic-web](https://github.com/semantalytics/awesome-semantic-web) â€“ Semantic web and linked data  * [awesome-seo](https://github.com/teles/awesome-seo) â€“ SEO (Search Engine Optimization) links.    - http://jotateles.com.br/awesome-seo/  * [awesome-serverless](https://github.com/anaibol/awesome-serverless) by @anaibol â€“ Services, solutions and resources for serverless / nobackend applications.  * [awesome-serverless](https://github.com/pmuens/awesome-serverless) by @pmuens â€“ Resources related to serverless computing and serverless architectures.  * [awesome-serverless-security](https://github.com/puresec/awesome-serverless-security) â€“ Serverless security resources  * [awesome-service-workers](https://github.com/TalAter/awesome-service-workers) â€“ Service Workers for Progressive Web Applications  * [awesome-servicefabric](https://github.com/lawrencegripper/awesome-servicefabric) â€“ Azure [Service Fabric](https://docs.microsoft.com/en-us/azure/service-fabric/) distributed services platform  * [awesome-services](https://github.com/indrasantosa/awesome-services) â€“ Services that make a painful programmer's life easier.  * [awesome-sharepoint](https://github.com/BSUG/awesome-sharepoint) by @BSUG  * [awesome-SharePoint](https://github.com/siaf/awesome-SharePoint) by @siaf  * [awesome-sheet-music](https://github.com/ad-si/awesome-sheet-music) â€“ Sheet music software, libraries and resources.  * [awesome-shell](https://github.com/alebcay/awesome-shell) â€“ Command-line frameworks, toolkits, guides and gizmos.  * [awesome-sites](https://github.com/Gherciu/awesome-sites) â€“ Various websites with resources for development, graphics, and learning  * [awesome-sketch](https://github.com/diessica/awesome-sketch) â€“ Guides, articles, videos about [Sketch 3](http://www.sketchapp.com/).  * [awesome-slack](https://github.com/filipelinhares/awesome-slack) by @filipelinhares â€“ Communities powered by Slack.  * [awesome-slack](https://github.com/matiassingers/awesome-slack) by @matiassingers  * [awesome-slack-communities](https://github.com/radermacher/awesome-slack-communities) â€“ Public Slack Communities.  * [awesome-smart-tv](https://github.com/vitalets/awesome-smart-tv) â€“ Smart TV apps  * [awesome-software-architecture](https://github.com/mehdihadeli/awesome-software-architecture) by @mehdihadeli â€“ Software architecture, patterns, and principles.  * [awesome-software-architecture](https://github.com/simskij/awesome-software-architecture) by @simskij â€“ Design, reason around and build software using architectural patterns and methods  * [awesome-software-craftsmanship](https://github.com/benas/awesome-software-craftsmanship) â€“ [Software craftsmanship](http://manifesto.softwarecraftsmanship.org/) resources to help learn the craft.  * [awesome-software-patreons](https://github.com/uraimo/awesome-software-patreons) â€“ Programmers and software-related Patreon accounts.  * [awesome-software-quality](https://github.com/ligurio/awesome-software-quality) â€“ Free software testing books.  * [awesome-solid](https://github.com/kustomzone/awesome-solid) â€“ [Solid](https://solidproject.org/) (social linked data) project.  * [awesome-sound](https://github.com/hwclass/awesome-sound) â€“ Sound & audio libraries and resources.  * [awesome-space](https://github.com/elburz/awesome-space) â€“ Outer Space  * [awesome-space-books](https://github.com/Hunter-Github/awesome-space-books) â€“ Space exploration related book  * [awesome-spanish-nlp](https://github.com/dav009/awesome-spanish-nlp) â€“ Linguistic Resources for doing NLP & CL on Spanish  * [awesome-spark](https://github.com/awesome-spark/awesome-spark) â€“ Apache Spark packages and resources.  * [awesome-speakers](https://github.com/karlhorky/awesome-speakers) â€“ Speakers in the programming and design communities  * [awesome-sphinxdoc](https://github.com/yoloseem/awesome-sphinxdoc) â€“ Tools for Sphinx Python Documentation Generator.  * [awesome-split-keyboards](https://github.com/diimdeep/awesome-split-keyboards) â€“ Ergonomic split keyboards.  * [awesome-sqlalchemy](https://github.com/dahlia/awesome-sqlalchemy) â€“ Extra libraries for SQLAlchemy, a Python ORM.  * [awesome-sre](https://github.com/dastergon/awesome-sre) â€“ Site Reliability and Production Engineering    - https://sre.xyz  * [awesome-ssh](https://github.com/moul/awesome-ssh)    - https://manfred.life/awesome-ssh  * [awesome-stacks](https://github.com/stackshareio/awesome-stacks) â€“ Tech stacks for building different applications & features    - https://awesomestacks.dev  * [awesome-standard](https://github.com/standard/awesome-standard) â€“ Documenting the explosion of packages in the [standard](http://standardjs.com/) (JavaScript code style) ecosystem.  * [awesome-stars](https://github.com/lichunqiang/awesome-stars) _In Chinese_ â€“ Useful libraries with personal remarks.  * [awesome-startup](https://github.com/KrishMunot/awesome-startup) â€“ Resources to build your own startup  * [awesome-static-generators](https://github.com/myles/awesome-static-generators) â€“ Static web site generators.  * [awesome-static-website-services](https://github.com/agarrharr/awesome-static-website-services)  * [awesome-steam](https://github.com/scholtzm/awesome-steam) â€“ Steam video games distribution platform development  * [awesome-storybook](https://github.com/lauthieb/awesome-storybook) â€“ [Storybook](https://storybook.js.org/) UI web development  * [awesome-streaming](https://github.com/manuzhang/awesome-streaming) â€“ Streaming frameworks, applications, etc  * [awesome-styleguides](https://github.com/RichardLitt/awesome-styleguides)  * [awesome-stylelint](https://github.com/stylelint/awesome-stylelint) â€“ [Stylelint](https://stylelint.io/) CSS linter.  * [awesome-sustainable-technology](https://github.com/protontypes/awesome-sustainable-technology) â€“ Open technology projects sustaining stable climate, energy supply and vital natural resources.    - https://opensustain.tech/  * [awesome-svelte](https://github.com/CalvinWalzel/awesome-svelte) â€“ [Svelte](https://svelte.dev/) framework  * [awesome-svelte-resources](https://github.com/ryanatkn/awesome-svelte-resources) â€“ [Svelte](https://svelte.dev/) framework  * [awesome-svg](https://github.com/willianjusten/awesome-svg)  * [awesome-swedish-opensource](https://github.com/gurre/awesome-swedish-opensource) â€“ Open-source projects from Swedes  * [awesome-swift](https://github.com/matteocrippa/awesome-swift) by @matteocrippa  * [awesome-swift](https://github.com/Wolg/awesome-swift) by @Wolg  * [awesome-swift-and-tutorial-resources](https://github.com/MaxChen/awesome-swift-and-tutorial-resources) â€“ Swift programming language  * [Awesome-Swift-Education](https://github.com/hsavit1/Awesome-Swift-Education) â€“ Learn some Swift  * [Awesome-Swift-Playgrounds](https://github.com/uraimo/Awesome-Swift-Playgrounds) â€“ Swift Playgrounds  * [awesome-symfony](https://github.com/sitepoint-editors/awesome-symfony) â€“ [Symfony PHP framework](http://symfony.com/) bundles, utilities and resources.  * [awesome-symfony-education](https://github.com/pehapkari/awesome-symfony-education) â€“ Symfony PHP framework learning resources  * [awesome-sysadmin](https://github.com/kahun/awesome-sysadmin) by @kahun â€“ Open source sysadmin resources.  * [awesome-sysadmin](https://github.com/n1trux/awesome-sysadmin) by @n1trux â€“ Open source sysadmin resources.  * [awesome-system-design](https://github.com/madd86/awesome-system-design) â€“ Distributed systems design  * [awesome-system-fonts](https://github.com/mrmrs/awesome-system-fonts) â€“ Websites that use system fonts.  * [awesome-taglines](https://github.com/miketheman/awesome-taglines) â€“ Software taglines  * [awesome-tailwindcss](https://github.com/aniftyco/awesome-tailwindcss) â€“ [Tailwind CSS](https://tailwindcss.com/)    - https://git.io/awesome-tailwindcss  * [awesome-talks](https://github.com/JanVanRyswyck/awesome-talks)  * [awesome-tap](https://github.com/sindresorhus/awesome-tap) â€“ Test Anything Protocol  * [awesome-tech-blogs](https://github.com/markodenic/awesome-tech-blogs) â€“ Technical blogs    - https://tech-blogs.dev/  * [awesome-tech-conferences](https://github.com/trstringer/awesome-tech-conferences) â€“ Upcoming technical conferences.  * [awesome-tech-videos](https://github.com/lucasviola/awesome-tech-videos) â€“ Tech conferences from youtube, vimeo, etc for us to get inspired  * [awesome-technical-writing](https://github.com/BolajiAyodeji/awesome-technical-writing)  * [awesome-telegram](https://github.com/ebertti/awesome-telegram) â€“ Telegram messaging service  * [awesome-tensorflow](https://github.com/jtoy/awesome-tensorflow) â€“ [TensorFlow](https://www.tensorflow.org/) machine intelligence library.  * [awesome-terraform](https://github.com/shuaibiyy/awesome-terraform) â€“ HashiCorp Terraform  * [awesome-test-automation](https://github.com/atinfo/awesome-test-automation)    - http://automated-testing.info  * [awesome-testing](https://github.com/TheJambo/awesome-testing) â€“ Testing resources    - https://git.io/v1hSm  * [awesome-text-editing](https://github.com/dok/awesome-text-editing) â€“ Text editing resources and libraries for the web  * [awesome-textpattern](https://github.com/drmonkeyninja/awesome-textpattern) â€“ Textpattern plugins and resources  * [awesome-themes](https://github.com/AdrienTorris/awesome-themes) â€“ Web themes and templates  * [awesome-tikz](https://github.com/xiaohanyu/awesome-tikz) â€“ [TikZ](https://pgf-tikz.github.io/) graph drawing package for TeX/LaTeX/ConTeXt  * [awesome-tinkerpop](https://github.com/mohataher/awesome-tinkerpop) â€“ [Apache TinkerPop](http://tinkerpop.apache.org/) graph computing framework  * [awesome-token-sale](https://github.com/holographicio/awesome-token-sale) â€“ Token sale / ICO resources  * [awesome-torch](https://github.com/carpedm20/awesome-torch) â€“ Tutorials, projects and communities for [Torch](http://torch.ch/), a scientific computing framework for LuaJIT.  * [awesome-transit](https://github.com/CUTR-at-USF/awesome-transit) â€“ Transit APIs, apps, datasets, research, and software  * [awesome-tunneling](https://github.com/anderspitman/awesome-tunneling) â€“ [Ngrok](https://ngrok.com/) alternatives and other ngrok-like tunneling software and services. Focus on self-hosting.  * [awesome-twilio](https://github.com/Twilio-org/awesome-twilio) â€“ Curated repository of useful and generally awesome Twilio tools and technologies  * [AwesomeTwitterAccounts](https://github.com/yask123/AwesomeTwitterAccounts) â€“ Twitter accounts, organised by programming communities.  * [awesome-typescript](https://github.com/dzharii/awesome-typescript) by @dzharii â€“ TypeScript programming language  * [awesome-typescript](https://github.com/ellerbrock/awesome-typescript) by @ellerbrock    - https://ellerbrock.github.io/awesome-typescript  * [awesome-typescript-projects](https://github.com/brookshi/awesome-typescript-projects) â€“ TypeScript open-source projects  * [awesome-typography](https://github.com/Jolg42/awesome-typography) â€“ Resources on OpenType & TrueType.  * [awesome-ui-component-library](https://github.com/anubhavsrivastava/awesome-ui-component-library) â€“ Framework component libraries for UI styles/toolkit    - https://anubhavsrivastava.github.io/awesome-ui-component-library/  * [awesome-umbraco](https://github.com/umbraco-community/awesome-umbraco) â€“ Resources for Umbraco 7, a .NET CMS.  * [Awesome-Unicode](https://github.com/Wisdom/Awesome-Unicode) â€“ Unicode tidbits, packages and resources.    - https://git.io/Awesome-Unicode  * [awesome-unity](https://github.com/RyanNielson/awesome-unity) â€“ Assets and resources for [Unity](http://unity3d.com/) game engine.  * [awesome-unix](https://github.com/sirredbeard/Awesome-UNIX)  * [awesome-userscripts](https://github.com/brunocvcunha/awesome-userscripts)  * [awesome-uses](https://github.com/wesbos/awesome-uses) â€“ `/uses` pages detailing developer setups, gear, software and configs.    - https://uses.tech  * [awesome-uxn](https://github.com/hundredrabbits/awesome-uxn) â€“ The [Uxn](https://100r.co/site/uxn.html) ecosystem is a personal computing playground, created to host small tools and games, programmable in its own unique assembly language.  * [awesome-v](https://github.com/vlang/awesome-v) â€“ [V](https://vlang.io/) programming language  * [awesome-vagrant](https://github.com/iJackUA/awesome-vagrant)  * [awesome-vanilla-js](https://github.com/davidhund/awesome-vanilla-js) â€“ Plainâ€”â€˜Vanillaâ€™â€”JavaScript  * [awesome-vapor](https://github.com/Cellane/awesome-vapor) â€“ [Vapor](https://vapor.codes/) Swift web framework  * [awesome-vector-tiles](https://github.com/mapbox/awesome-vector-tiles) â€“ Implementations of the [Mapbox Vector Tile](https://www.mapbox.com/developers/vector-tiles/) specification.  * [awesome-vehicle-security](https://github.com/jaredthecoder/awesome-vehicle-security) â€“ Vehicle security and car hacking  * [awesome-vhdl](https://github.com/VHDL/awesome-vhdl) â€“ VHDL hardware description language  * [awesome-vim](https://github.com/akrawchyk/awesome-vim) by @akrawchyk  * [awesome-vim](https://github.com/matteocrippa/awesome-vim) by @matteocrippa  * [awesome-vite](https://github.com/vitejs/awesome-vite) â€“ [Vite](https://vitejs.dev/) front-end build tooling.  * [awesome-vlc](https://github.com/mfkl/awesome-vlc) â€“ [VideoLAN VLC](https://www.videolan.org/) multimedia player and framework.  * [awesome-volt](https://github.com/heri/awesome-volt) â€“ [Volt](http://voltframework.com/) Ruby web framework.  * [awesome-vorpal](https://github.com/vorpaljs/awesome-vorpal) â€“ [Vorpal](http://vorpal.js.org/) Node.js interactive CLI framework  * [awesome-vscode](https://github.com/viatsko/awesome-vscode) â€“ Visual Studio Code    - https://viatsko.github.io/awesome-vscode/  * [awesome-vue](https://github.com/vuejs/awesome-vue) â€“ Resources for [Vue.js](http://vuejs.org/) JavaScript UI library.  * [awesome-vulkan](https://github.com/vinjn/awesome-vulkan) â€“ [3D graphics and compute API](https://www.khronos.org/vulkan/)  * [awesome-wagtail](https://github.com/springload/awesome-wagtail) â€“ [Wagtail](https://wagtail.io/) Python CMS  * [awesome-wasm](https://github.com/mbasso/awesome-wasm) â€“ WebAssembly  * [awesome-watchos](https://github.com/yenchenlin/awesome-watchos) â€“ Apple watchOS  * [awesome-web-animation](https://github.com/sergey-pimenov/awesome-web-animation) â€“ Web animation libraries, books, apps etc.    - https://awesome-web-animation.netlify.com  * [awesome-web-archiving](https://github.com/iipc/awesome-web-archiving) â€“ Getting started with web archiving  * [awesome-web-design](https://github.com/nicolesaidy/awesome-web-design) â€“ Resources for digital designers.  * [awesome-web-effect](https://github.com/lindelof/awesome-web-effect) â€“ Exquisite and compact web page effects.  * [awesome-web-scraping](https://github.com/lorien/awesome-web-scraping) â€“ tools and programming libraries related to web scraping and data processing  * [awesome-web-security](https://github.com/qazbnm456/awesome-web-security)    - https://awesomelists.top/#/repos/qazbnm456/awesome-web-security  * [awesome-webaudio](https://github.com/notthetup/awesome-webaudio) â€“ WebAudio packages and resources.  * [awesome-webauthn](https://github.com/herrjemand/awesome-webauthn) â€“ WebAuthn/FIDO2  * [awesome-webcomponents](https://github.com/obetomuniz/awesome-webcomponents)  * [Awesome-WebExtensions](https://github.com/fregante/Awesome-WebExtensions) â€“ WebExtensions development.  * [awesome-webgl](https://github.com/sjfricke/awesome-webgl) â€“ WebGL libraries, resources and much more  * [awesome-webpack](https://github.com/webpack-contrib/awesome-webpack) â€“ Webpack resources, libraries and tools  * [awesome-webpack-perf](https://github.com/iamakulov/awesome-webpack-perf) â€“ Webpack tools for web performance  * [awesome-webservice](https://github.com/wapmorgan/awesome-webservice) â€“ Web and cloud services, SaaS.  * [awesome-websockets](https://github.com/facundofarias/awesome-websockets) â€“ Websocket libraries and resources.  * [awesome-webvis](https://github.com/rajsite/awesome-webvis) â€“ [WebVI](http://www.webvi.io/) examples made using [LabVIEW](http://www.ni.com/en-us/support/software-technology-preview.html) systems engineering software.  * [awesome-weekly](https://github.com/jondot/awesome-weekly) â€“ Quality weekly subscription newsletters from the software world.  * [awesome-wicket](https://github.com/PhantomYdn/awesome-wicket) â€“ [Apache Wicket](http://wicket.apache.org/) Java web framework  * [awesome-wikipedia](https://github.com/emijrp/awesome-wikipedia) â€“ Wikipedia-related frameworks, libraries, software, datasets and references.  * [Awesome-Windows/Awesome](https://github.com/Awesome-Windows/Awesome) â€“ Applications and tools for Windows.  * [awesome-wordpress](https://github.com/dropndot/awesome-wordpress) by @dropndot  * [awesome-wordpress](https://github.com/endel/awesome-wordpress) by @endel  * [awesome-wordpress](https://github.com/miziomon/awesome-wordpress) by @miziomon  * [awesome-workflow-engines](https://github.com/meirwah/awesome-workflow-engines) â€“ Open source workflow engines  * [awesome-workshopper](https://github.com/therebelrobot/awesome-workshopper)  * [awesome-wpo](https://github.com/davidsonfellipe/awesome-wpo) â€“ Web Performance Optimization  * [awesome-xamarin](https://github.com/XamSome/awesome-xamarin) by @XamSome â€“ [Xamarin](https://visualstudio.microsoft.com/xamarin/) mobile application framework  * [awesome-xamarin](https://github.com/XamSome/awesome-xamarin) by @XamSome â€“ Interesting libraries/tools for Xamarin mobile projects  * [awesome-xcode-plugin](https://github.com/aashishtamsya/awesome-xcode-scripts) â€“ XCode IDE scripts  * [awesome-xmpp](https://github.com/bluszcz/awesome-xmpp) â€“ Curated list of awesome XMPP protocol resources.  * [awesome-yamada](https://github.com/supermomonga/awesome-yamada) â€“ Dancing yamada  * [awesome-yii](https://github.com/iJackUA/awesome-yii) â€“ Yii PHP framework extensions, tutorials and other nice things.  * [awesome-zig](https://github.com/nrdmn/awesome-zig) â€“ [Zig](https://ziglang.org/) programming language.  * [awesome-zsh-plugins](https://github.com/unixorn/awesome-zsh-plugins)  * [awesomo](https://github.com/lk-geimfari/awesomo) â€“ Open source projects in various languages.  * [craftcms/awesome](https://github.com/craftcms/awesome) â€“ [Craft CMS](https://craftcms.com/)  * [not-awesome-es6-classes](https://github.com/petsel/not-awesome-es6-classes) â€“ Why ES6 (aka ES2015) classes are NOT awesome    - https://matthias-endler.de/awesome-static-analysis/      ## Lists of lists    * [awesome](https://github.com/sindresorhus/awesome) â€“ A curated list of awesome lists.  * [awesome-all](https://github.com/bradoyler/awesome-all) â€“ A curated list of awesome lists of awesome frameworks, libraries and software  * [awesome-android-awesomeness](https://github.com/yongjhih/awesome-android-awesomeness#awesomeness)  * [awesome-awesome](https://github.com/aligoren/awesome-awesome) by @aligoren â€“ List of GitHub Lists  * [awesome-awesome](https://github.com/emijrp/awesome-awesome) by @emijrp â€“ A curated list of awesome curated lists of many topics.  * [awesome-awesome](https://github.com/erichs/awesome-awesome) by @erichs â€“ A curated list of awesome curated lists! Inspired by inspiration.  * [awesome-awesome](https://github.com/oyvindrobertsen/awesome-awesome) by @oyvindrobertsen â€“ A curated list of curated lists of libraries, resources and shiny things for various languages.  * [awesome-awesomeness](https://github.com/bayandin/awesome-awesomeness) â€“ A curated list of awesome awesomeness  * [awesome-awesomeness-zh_CN](https://github.com/justjavac/awesome-awesomeness-zh_CN) _In Chinese_ â€“ ä¸­æ–‡ç‰ˆawesome list ç³»åˆ—æ–‡ç«   * [awesome-awesomes](https://github.com/fleveque/awesome-awesomes) â€“ Awesome collection of awesome lists of libraries, tools, frameworks and software for any programming language  * [awesome-collection](https://github.com/flyhigher139/awesome-collection) â€“ A list of awesome repos.  * [Awesome-Hacking](https://github.com/Hack-with-Github/Awesome-Hacking) â€“ Lists for hackers, pentesters and security researchers.  * [awesome-lists](https://github.com/pshah123/awesome-lists) â€“ A curated list for your curated lists, including other curated lists of curated lists that may or may not contain other curated lists.  * [curated-lists](https://github.com/learn-anything/curated-lists)  * [delightful](https://codeberg.org/teaserbot-labs/delightful) â€“ Home of delightful curated lists of free software, open science and information sources.  * [getAwesomeness](https://github.com/panzhangwang/getAwesomeness) â€“ Explorer designed for curated awesome list hosted on Github    - https://getawesomeness.herokuapp.com/  * [list-of-lists](https://github.com/cyrusstoller/list-of-lists) â€“ A meta list of lists of useful open source projects and developer tools.  * [ListOfGithubLists](https://github.com/asciimoo/ListOfGithubLists) â€“ List of github lists  * [must-watch-list](https://github.com/adrianmoisey/must-watch-list) â€“ List of must-watch lists.  * [this one](https://github.com/jnv/lists)  * [wiki](https://github.com/huguangju/wiki) _In Chinese_ â€“ A curated list of awesome lists.      ### Lists of lists of lists    * [awesome-awesome-awesome](https://github.com/geekan/awesome-awesome-awesome) by @geekan â€“ An awesome-awesome list.  * [awesome-awesome-awesome](https://github.com/t3chnoboy/awesome-awesome-awesome) by @t3chnoboy â€“ A a curated list of curated lists of awesome lists.  * [awesomecubed](https://github.com/hunterboerner/awesomecubed) â€“ A curated list of awesome awesomeness awesomenesses.  * [lologl](https://github.com/yaph/lologl) â€“ List of Lists of Github Lists.  * [meta-awesome](https://github.com/PatrickMcDonald/meta-awesome)  * [the one above](#lists-of-lists)      #### Lists of lists of lists of lists    * [awesome-awesome-awesome-awesome](https://github.com/sindresorhus/awesome-awesome-awesome-awesome)  * [the one above](#lists-of-lists-of-lists)      ##### Lists of lists of lists of lists of lists    * [awesome-power-of-5](https://github.com/therebelbeta/awesome-power-of-5)  * [the one above](#lists-of-lists-of-lists-of-lists)      ###### Lists of lists of lists of lists of lists of lists    * [awesome-awesome-awesome-awesome-awesome-awesome](https://github.com/enedil/awesome-awesome-awesome-awesome-awesome-awesome)  * [the one above](#lists-of-lists-of-lists-of-lists-of-lists)      ###### Lists of lists of lists of lists of lists of lists of lists    * [awesome-awesome-awesome-awesome-awesome-awesome-awesome](https://github.com/sparanoid/awesome-awesome-awesome-awesome-awesome-awesome-awesome)  * [the one above](#lists-of-lists-of-lists-of-lists-of-lists-of-lists)    <!-- lists-end -->    ## License    [![CC0 Public Domain](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)    Social preview photo by [Eli Francis](https://unsplash.com/@elifrancis?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/books-clutter?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText). """
Big data;https://github.com/benedekrozemberczki/pytorch_geometric_temporal;"""[pypi-image]: https://badge.fury.io/py/torch-geometric-temporal.svg  [pypi-url]: https://pypi.python.org/pypi/torch-geometric-temporal  [size-image]: https://img.shields.io/github/repo-size/benedekrozemberczki/pytorch_geometric_temporal.svg  [size-url]: https://github.com/benedekrozemberczki/pytorch_geometric_temporal/archive/master.zip  [build-image]: https://github.com/benedekrozemberczki/pytorch_geometric_temporal/workflows/CI/badge.svg  [build-url]: https://github.com/benedekrozemberczki/pytorch_geometric_temporal/actions?query=workflow%3ACI  [docs-image]: https://readthedocs.org/projects/pytorch-geometric-temporal/badge/?version=latest  [docs-url]: https://pytorch-geometric-temporal.readthedocs.io/en/latest/?badge=latest  [coverage-image]: https://codecov.io/gh/benedekrozemberczki/pytorch_geometric_temporal/branch/master/graph/badge.svg  [coverage-url]: https://codecov.io/github/benedekrozemberczki/pytorch_geometric_temporal?branch=master        <p align=""center"">    <img width=""90%"" src=""https://raw.githubusercontent.com/benedekrozemberczki/pytorch_geometric_temporal/master/docs/source/_static/img/text_logo.jpg?sanitize=true"" />  </p>    -----------------------------------------------------    [![PyPI Version][pypi-image]][pypi-url]  [![Docs Status][docs-image]][docs-url]  [![Code Coverage][coverage-image]][coverage-url]  [![Build Status][build-image]][build-url]  [![Arxiv](https://img.shields.io/badge/ArXiv-2104.07788-orange.svg)](https://arxiv.org/abs/2104.07788)  [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)â €    **[Documentation](https://pytorch-geometric-temporal.readthedocs.io)** | **[External Resources](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/resources.html)** | **[Datasets](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#discrete-time-datasets)**    *PyTorch Geometric Temporal* is a temporal (dynamic) extension library for [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric).    <p align=""justify"">The library consists of various dynamic and temporal geometric deep learning, embedding, and spatio-temporal regression methods from a variety of published research papers. Moreover, it comes with an easy-to-use dataset loader, train-test splitter and temporal snaphot iterator for dynamic and temporal graphs. The framework naturally provides GPU support. It also comes with a number of benchmark datasets from the epidemological forecasting, sharing economy, energy production and web traffic management domains. Finally, you can also create your own datasets.</p>    The package interfaces well with [Pytorch Lightning](https://pytorch-lightning.readthedocs.io) which allows training on CPUs, single and multiple GPUs out-of-the-box. Take a look at this [introductory example](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/blob/master/examples/recurrent/lightning_example.py) of using PyTorch Geometric Temporal with Pytorch Lighning.    We also provide detailed examples for each of the [recurrent](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/tree/master/examples/recurrent) models and [notebooks](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/tree/master/notebooks) for the attention based ones.      --------------------------------------------------------------------------------    **Case Study Tutorials**    We provide in-depth case study tutorials in theÂ [Documentation](https://pytorch-geometric-temporal.readthedocs.io/en/latest/), each covers an aspect of PyTorch Geometric Temporalâ€™s functionality.    **Incremental Training**:Â [Epidemiological Forecasting Case Study](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#epidemiological-forecasting)    **Cumulative Training**:Â [Web Traffic Management Case Study](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#web-traffic-prediction)    --------------------------------------------------------------------------------    **Citing**      If you find *PyTorch Geometric Temporal* and the new datasets useful in your research, please consider adding the following citation:    ```bibtex  @inproceedings{rozemberczki2021pytorch,                 author = {Benedek Rozemberczki and Paul Scherer and Yixuan He and George Panagopoulos and Alexander Riedel and Maria Astefanoaei and Oliver Kiss and Ferenc Beres and Guzman Lopez and Nicolas Collignon and Rik Sarkar},                 title = {{PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models}},                 year = {2021},                 booktitle={Proceedings of the 30th ACM International Conference on Information and Knowledge Management},                 pages = {4564â€“4573},  }  ```    --------------------------------------------------------------------------------    **A simple example**    PyTorch Geometric Temporal makes implementing Dynamic and Temporal Graph Neural Networks quite easy - see the accompanying [tutorial](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#applications). For example, this is all it takes to implement a recurrent graph convolutional network with two consecutive [graph convolutional GRU](https://arxiv.org/abs/1612.07659) cells and a linear layer:    ```python  import torch  import torch.nn.functional as F  from torch_geometric_temporal.nn.recurrent import GConvGRU    class RecurrentGCN(torch.nn.Module):        def __init__(self, node_features, num_classes):          super(RecurrentGCN, self).__init__()          self.recurrent_1 = GConvGRU(node_features, 32, 5)          self.recurrent_2 = GConvGRU(32, 16, 5)          self.linear = torch.nn.Linear(16, num_classes)        def forward(self, x, edge_index, edge_weight):          x = self.recurrent_1(x, edge_index, edge_weight)          x = F.relu(x)          x = F.dropout(x, training=self.training)          x = self.recurrent_2(x, edge_index, edge_weight)          x = F.relu(x)          x = F.dropout(x, training=self.training)          x = self.linear(x)          return F.log_softmax(x, dim=1)  ```  --------------------------------------------------------------------------------    **Methods Included**    In detail, the following temporal graph neural networks were implemented.      **Recurrent Graph Convolutions**    * **[DCRNN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.dcrnn.DCRNN)** from Li *et al.*: [Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting](https://arxiv.org/abs/1707.01926) (ICLR 2018)    * **[GConvGRU](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.gconv_gru.GConvGRU)** from Seo *et al.*: [Structured Sequence Modeling with Graph  Convolutional Recurrent Networks](https://arxiv.org/abs/1612.07659) (ICONIP 2018)    * **[GConvLSTM](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.gconv_lstm.GConvLSTM)** from Seo *et al.*: [Structured Sequence Modeling with Graph  Convolutional Recurrent Networks](https://arxiv.org/abs/1612.07659) (ICONIP 2018)    * **[GC-LSTM](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.gc_lstm.GCLSTM)** from Chen *et al.*: [GC-LSTM: Graph Convolution Embedded LSTM for Dynamic Link Prediction](https://arxiv.org/abs/1812.04206) (CoRR 2018)    * **[LRGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.lrgcn.LRGCN)** from Li *et al.*: [Predicting Path Failure In Time-Evolving Graphs](https://arxiv.org/abs/1905.03994) (KDD 2019)    * **[DyGrEncoder](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.dygrae.DyGrEncoder)** from Taheri *et al.*: [Learning to Represent the Evolution of Dynamic Graphs with Recurrent Models](https://dl.acm.org/doi/10.1145/3308560.3316581)    * **[EvolveGCNH](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.evolvegcnh.EvolveGCNH)** from Pareja *et al.*: [EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs](https://arxiv.org/abs/1902.10191)    * **[EvolveGCNO](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.evolvegcno.EvolveGCNO)** from Pareja *et al.*: [EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs](https://arxiv.org/abs/1902.10191)    * **[T-GCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.temporalgcn.TGCN)** from Zhao *et al.*: [T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction](https://arxiv.org/abs/1811.05320)    * **[A3T-GCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.attentiontemporalgcn.A3TGCN)** from Zhu *et al.*: [A3T-GCN: Attention Temporal Graph Convolutional Network for Traffic Forecasting](https://arxiv.org/abs/2006.11583)     * **[AGCRN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.agcrn.AGCRN)** from Bai *et al.*: [Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting](https://arxiv.org/abs/2007.02842) (NeurIPS 2020)    * **[MPNN LSTM](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.mpnn_lstm.MPNNLSTM)** from Panagopoulos *et al.*: [Transfer Graph Neural Networks for Pandemic Forecasting](https://arxiv.org/abs/2009.08388) (AAAI 2021)      **Attention Aggregated Temporal Graph Convolutions**    * **[STGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.stgcn.STConv)** from Yu *et al.*: [Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting](https://arxiv.org/abs/1709.04875) (IJCAI 2018)    * **[ASTGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.astgcn.ASTGCN)** from Guo *et al.*: [Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting](https://ojs.aaai.org/index.php/AAAI/article/view/3881) (AAAI 2019)    * **[MSTGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.mstgcn.MSTGCN)** from Guo *et al.*: [Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting](https://ojs.aaai.org/index.php/AAAI/article/view/3881) (AAAI 2019)    * **[GMAN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.gman.GMAN)** from Zheng *et al.*: [GMAN: A Graph Multi-Attention Network for Traffic Prediction](https://arxiv.org/pdf/1911.08415.pdf) (AAAI 2020)    * **[MTGNN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.mtgnn.MTGNN)** from Wu *et al.*: [Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks](https://arxiv.org/abs/2005.11650) (KDD 2020)    * **[2S-AGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.tsagcn.AAGCN)** from Shi *et al.*: [Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition](https://arxiv.org/abs/1805.07694) (CVPR 2019)    * **[DNNTSP](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.dnntsp.DNNTSP)** from Yu *et al.*: [Predicting Temporal Sets with Deep Neural Networks](https://dl.acm.org/doi/abs/10.1145/3394486.3403152) (KDD 2020)    **Auxiliary Graph Convolutions**    * **[TemporalConv](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.stgcn.TemporalConv)** from Yu *et al.*: [Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting](https://arxiv.org/abs/1709.04875) (IJCAI 2018)    * **[DConv](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.dcrnn.DConv)** from Li *et al.*: [Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting](https://arxiv.org/abs/1707.01926) (ICLR 2018)    * **[ChebConvAttention](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.astgcn.ChebConvAttention)** from Guo *et al.*: [Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting](https://ojs.aaai.org/index.php/AAAI/article/view/3881) (AAAI 2019)    * **[AVWGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.agcrn.AVWGCN)** from Bai *et al.*: [Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting](https://arxiv.org/abs/2007.02842) (NeurIPS 2020)      --------------------------------------------------------------------------------      Head over to our [documentation](https://pytorch-geometric-temporal.readthedocs.io) to find out more about installation, creation of datasets and a full list of implemented methods and available datasets.  For a quick start, check out the [examples](https://pytorch-geometric-temporal.readthedocs.io) in the `examples/` directory.    If you notice anything unexpected, please open an [issue](https://benedekrozemberczki/pytorch_geometric_temporal/issues). If you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/issues).      --------------------------------------------------------------------------------    **Installation**    Binaries are provided for Python version <= 3.9.    **PyTorch 1.10.0**    To install the binaries for PyTorch 1.10.0, simply run    ```sh  pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+${CUDA}.html  pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+${CUDA}.html  pip install torch-geometric  pip install torch-geometric-temporal  ```    where `${CUDA}` should be replaced by either `cpu`, `cu102`, or `cu113` depending on your PyTorch installation.    |             | `cpu` | `cu102` | `cu113` |  |-------------|-------|---------|---------|  | **Linux**   | âœ…    | âœ…      | âœ…      |  | **Windows** | âœ…    | âœ…      | âœ…      |  | **macOS**   | âœ…    |         |         |    --------------------------------------------------------------------------------    **Running tests**    ```  $ python setup.py test  ```  --------------------------------------------------------------------------------    **License**    - [MIT License](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/blob/master/LICENSE) """
Big data;https://github.com/Codecademy/EventHub;"""# EventHub  EventHub enables companies to do cross device event tracking. Events are joined by their associated user on EventHub and can be visualized by the built-in dashboard to answer the following common business questions  * what is my funnel conversion rate?  * what is my cohorted KPI retention?  * which variant in my A/B test has a higher conversion rate?    Most important of all, EventHub is free and open source.    **Table of Contents**  - [Quick Start](#quick-start)  - [Server](#server)  - [Dashboard](#dashboard)  - [Javascript Library](#javascript-library)  - [Ruby Library](#ruby-library)    ## Quick Start  ### Playground  A [demo server](http://codecademy:codecademy@floating-mesa-9408.herokuapp.com/) is available on Heroku and the username/password to access the dashboard is `codecademy/codecademy`.    - [Example funnel query](http://codecademy:codecademy@54.193.159.140/?start_date=20130101&end_date=20130107&num_days_to_complete_funnel=7&funnel_steps%5B%5D=receive_email&funnel_steps%5B%5D=view_track_page&funnel_steps%5B%5D=finish_course&type=funnel)  - [Example cohort query](http://codecademy:codecademy@54.193.159.140/?start_date=20130101&end_date=20130107&row_event_type=receive_email&column_event_type=start_track&num_days_per_row=1&num_columns=11&type=cohort)    ### Screenshots  ![Funnel screenshot](https://raw.githubusercontent.com/Codecademy/EventHub/master/funnel-screenshot.png)  ![Cohort screenshot](https://raw.githubusercontent.com/Codecademy/EventHub/master/cohort-screenshot.png)    ### Deploy with Heroku  Developers who want to try EventHub can quickly set the server up on Heroku with the following commands. However, please be aware that Heroku's file system is ephemeral and your data will be wiped after the instance is closed.  ```bash  git clone https://github.com/Codecademy/EventHub.git    cd EventHub  heroku create  git push heroku master    heroku open  ```    ### Required dependencies  * [java sdk7](http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html)  * [maven](http://maven.apache.org)    ### Compile and run  ```bash  # set up proper JAVA_HOME for mac  export JAVA_HOME=$(/usr/libexec/java_home)    git clone https://github.com/Codecademy/EventHub.git  cd EventHub  export EVENT_HUB_DIR=`pwd`  mvn -am -pl web clean package  java -jar web/target/web-1.0-SNAPSHOT.jar  ```    ### How to run all the tests  #### Unit/Integration/Functional testing  ```bash  mvn -am -pl web clean test  ```    #### Manual testing with curl  Comprehensive examples can be found in `script.sh`.  ```bash  cd ${EVENT_HUB_DIR}; ./script.sh  ```    Test all event related endpoints  * Add new event      ```bash      curl -X POST http://localhost:8080/events/track --data ""event_type=signup&external_user_id=foobar&event_property_1=1""      ```    * Batch add new event      ```bash      curl -X POST http://localhost:8080/events/batch_track --data ""events=[{event_type: signup, external_user_id: foobar, date: 20130101, event_property_1: 1}]""      ```    * Show all event types      ```bash      curl http://localhost:8080/events/types      ```    * Show events for a given user      ```bash      curl http://localhost:8080/users/timeline\?external_user_id\=chengtao@codecademy.com\&offset\=0\&num_records\=1      ```    * Show all property keys for the given event type      ```bash      curl 'http://localhost:8080/events/keys?event_type=signup'      ```    * Show all property values for the given event type and property key      ```bash      curl 'http://localhost:8080/events/values?event_type=signup&event_key=treatment'      ```    * Show all property values for the given event type, property key and value prefix      ```bash      curl 'http://localhost:8080/events/values?event_type=signup&event_key=treatment&prefix=fa'      ```    * Show server stats      ```bash      curl http://localhost:8080/varz      ```    * Funnel query      ```bash      today=`date +'%Y%m%d'`      end_date=`(date -d '+7day' +'%Y%m%d' || date -v '+7d' +'%Y%m%d') 2> /dev/null`        curl -X POST ""http://localhost:8080/events/funnel"" --data ""start_date=${today}&end_date=${end_date}&funnel_steps[]=signup&funnel_steps[]=view_shopping_cart&funnel_steps[]=checkout&num_days_to_complete_funnel=7&eck=event_property_1&ecv=1""      ```    * Retention query      ```bash      today=`date +'%Y%m%d'`      end_date=`(date -d '+7day' +'%Y%m%d' || date -v '+7d' +'%Y%m%d') 2> /dev/null`        curl -X POST ""http://localhost:8080/events/cohort"" --data ""start_date=${today}&end_date=${end_date}&row_event_type=signup&column_event_type=view_shopping_cart&num_days_per_row=1&num_columns=2""      ```    Test all user related endpoints  * show paginated events for a given user      ```bash      curl http://localhost:8080/users/timeline\?external_user_id\=chengtao@codecademy.com\&offset\=0\&num_records\=5      ```    * show information of users who have matched property keys & values      ```bash      curl -X POST http://localhost:8080/users/find --data ""ufk[]=external_user_id&ufv[]=chengtao1@codecademy.com""      ```    * add or update user information      ```bash      curl -X POST http://localhost:8080/users/add_or_update --data ""external_user_id=chengtao@codecademy.com&foo=bar&hello=world""      ```    * Show all property keys for users      ```bash      curl 'http://localhost:8080/users/keys      ```    * Show all property values for users given property key and (optional) value prefix      ```bash      curl 'http://localhost:8080/users/values?user_key=hello&prefix=w'      ```    #### Load testing with Jmeter  We use [Apache Jmeter](http://jmeter.apache.org) for load testing, and the load testing script can be found in `${EVENT_HUB_DIR}/jmeter.jmx`.  ```bash  export JMETER_DIR=~/Downloads/apache-jmeter-2.11/  java -jar ${JMETER_DIR}/bin/ApacheJMeter.jar -JnumThreads=1 -n -t jmeter.jmx -p jmeter.properties  java -jar ${JMETER_DIR}/bin/ApacheJMeter.jar -JnumThreads=5 -n -t jmeter.jmx -p jmeter.properties  java -jar ${JMETER_DIR}/bin/ApacheJMeter.jar -JnumThreads=10 -n -t jmeter.jmx -p jmeter.properties    # generate graph (require matplotlib)  ./plot_jmeter_performance.py 1-jmeter-performance.csv 5-jmeter-performance.csv 10-jmeter-performance.csv    # open ""Track Event.png""  ```    ## Server    ### Key observations & design decisions  Our goal is to build something usable on a single machine with a reasonably large SSD drive. Let's say, hypothetically, the server receives 100M events monthly (might cost you few thousand dollars per month to use SAAS provider), and each event is 500 bytes without compression. In this situation, storing all the events likely only takes you few hundreds GB with compression, and chances are, only the data in recent months are of interest.    Also, to efficiently run basic funnel and cohort queries without filtering, only two forward indices are needed, event index sharded by event types and event index sharded by users. Therefore, our strategy is to make those two indices as small as possible to fit in memory, and if the client wants to do filtering for events, we build a bloomfilter to reject most of the non exact-match. Imagine we are running another hypothetical query while assuming both indices and the bloomfilters can be fit in memory. Say there are 1M events that cannot be rejected and need to hit the disk, assuming each SSD disk read is 16 microseconds, we are talking about sub-minute query time, while assuming none of the data are in memory. In practice, this situation is likely much better as we cache all the recently hit records, and most of the queries likely only care the most recent data.    To simplify the design of the server and store indices compactly so that they fit in memory, we made the following two assumptions.    1. Times are associated to events when the server receives the an event  2. Date is the finest level of granularity    With the above two assumptions, we can rely on the server generated monotonically increasing id to maintain the total order for the events. In addition, as long as we track the id of the first event in any given date, we do not need to store the time information in the indices (which greatly reduces the size of the indices). The direct implication for those assumptions are, first, if the client chose to cache some events locally and sent them later, the timing for those events will be recorded as the server receives them, not when the user made those actions; second, though the server maintains the total ordering of all events, it cannot answer questions like what is the conversion rate for the given funnel between 2pm and 3pm on a given date.    Lastly, for both indices, since they are sharded by event types or users, we can expect the size of the indices to reduce significantly with proper compression.    ### Architecture  At the highest level, `com.codecademy.evenhub.web.EventHubHandler` is the main entry point. It runs a [Jetty](http://www.eclipse.org/jetty) server, reflectively collects supported commands under `com.codecademy.evenhub.web.commands`, handles JSONP request transparently, handles requests to static resources like the dashboard, and most importantly, act as a proxy which translates http request and respones to and from method calls to `com.codecademy.evenhub.EventHub`.    `com.codecademy.evenhub.EventHub` can be thought of as a facade to the key components of `UserStorage`, `EventStorage`, `ShardedEventIndex`, `DatedEventIndex`, `UserEventIndex` and `PropertiesIndex`.    For `UserStorage` and `EventStorage`, at the lowest level, we implemented `Journal{User,Event}Storage` backed by [HawtJournal](https://github.com/fusesource/hawtjournal/) to store underlying records reliably. In addition, when clients are quering records which cannot be filtered by the supported indices, the server will loop through all the potential hits, look up the properties from the `Journal` and then filter accordingly. For better performance, there are also decorators for each storage like `Cached{User,Event}Storage` to support caching and `BloomFiltered{User,Event}Storage` to support fast rejection for filters like `ExactMatch`. Please also beware that each `Storage` maintains a monotonically increasing counter as the internal id generator for each event and user received.    To make the funnel and cohort queries fast, `EventHub` also maintains three indices, `ShardedEventIndex`, `UserEventIndex`, and `DatedEventIndex` behind the scene. `DatedEventIndex` simply tracks the mapping from a given date, the id of the first event received in that day. `ShardedEventIndex` can be thought of as sorted event ids sharded by event type. `UserEventIndex` can be thought of as sorted event ids sharded by users.    Lastly, `EventHub` maintains a `PropertiesIndex` backed by [LevelDB Jni](https://github.com/fusesource/leveldbjni) to track what properties keys are available for a given event type and what properties values are available for a given event type and a property key.    ### Horizontal scalabiltiy  While EventHub does not need any information from different users, with a broker in front of EventHub servers, EventHub can be easily sharded by users and scale horizontally.    ### Performance  In the following three experiments, the spec of the computer used can be found in the following table    | Component      | Spec                                    |  |----------------|-----------------------------------------|  | Computer Model | Mac Book Pro, Retina 15-inch, Late 2013 |  | Processor      | 2GHz Intel Core i7                      |  | Memory         | 8GB 1600 MHz DDR3                       |  | Software       | OS X 10.9.2                             |  | Jvm            | Oracle JDK 1.7                          |    #### Write performance  The following graph is generated as described in [Load testing with Jmeter](#load-testing-with-jmeter). The graph shows both the throughput and latency of adding the first one million events (without batching) with different number of threads (1, 5, 10, 15).  ![Throughput and latency by threads](http://i60.tinypic.com/16ad66b.png)    #### Query performance  While it is difficult to come up with a generic benchmark, we would rather show something rather than show nothing. After generating about one million events with the load testing script as described in [Load testing with Jmeter](#load-testing-with-jmeter), we ran the four types of queries twice, once after the server starts cleanly and another time while the cache is still warm.    | Query                   | 1st execution | 2nd execution | command |  |-------------------------|---------------|---------------|---------|  | Funnel without filters  | 1.15s         | 0.19s         | curl -X POST ""http://localhost:8080/events/funnel"" --data ""start_date=20130101&end_date=20130130&funnel_steps[]=receive_email&funnel_steps[]=view_track_page&funnel_steps[]=start_track&num_days_to_complete_funnel=30"" |  | Funnel with filters     | 1.31s         | 0.43s         | curl -X POST ""http://localhost:8080/events/funnel"" --data ""start_date=20130101&end_date=20130130&funnel_steps[]=receive_email&funnel_steps[]=view_track_page&funnel_steps[]=start_track&num_days_to_complete_funnel=30&efk0[]=event_property_1&efv0[]=1"" |  | Cohort without filters  | 0.63s         | 0.13s         | curl -X POST ""http://localhost:8080/events/cohort"" --data ""start_date=20130101&end_date=20130130&row_event_type=receive_email&column_event_type=start_track&num_days_per_row=1&num_columns=7"" |  | Cohort with filters     | 1.20s         | 0.32s         | curl -X POST ""http://localhost:8080/events/cohort"" --data ""start_date=20130101&end_date=20130130&row_event_type=receive_email&column_event_type=start_track&num_days_per_row=1&num_columns=7&refk[]=event_property_1&refv[]=1"" |    #### Memory footprint  In the experiment, the server was bootstrapped differently. Instead of using the load testing script, we used subset of data from Codecademy, which has around 53M events and 2.4M users. Please be aware that the current storage format on disk is fairly inefficient and has serious internal fragmentation. However, when the data are loaded to memory, it will be much more efficient as we would never load those ""hole"" pages into memory.    | Key Component             | Size in memory  | Note |  |---------------------------|-----------------|------|  | ShardedEventIndex         | 424Mb           | (data size) + (index size) <br>= (event id size * number of events) + negligible<br>= (8 * 53M) |  | UserEventIndex            | 722Mb           | (data size) + (index size) <br>= (event id size * number of events) + (index entry size * number of users)<br>= (8 * 53M) + ((numPointersPerIndexEntry * 2 + 1) * 8 + 4) * 2.4M)<br>= (8 * 53M) + (124 * 2.4M) |  | BloomFilteredEventStorage | 848Mb           | (bloomfilter size) * (number of events) <br>= 16 * 53M |    ## Dashboard  The server comes with a built-in dashboard which is simply some static resources stored in `/web/src/main/resources/frontend` and gets compiled into the server jar file. After running the server, the dashboard can be accessed at [http://localhost:8080](http://localhost:8080). Through the dashboard, you can access the server for your funnel and cohort analysis.    #### Password protection  The dashboard comes with insecure basic authentication which send unencrypted information without SSL. Please use it at your own discretion. The default username/password is codecademy/codecademy and you can change it by modifying your web.properties file or use the following command to start your server  ```bash  USERNAME=foo  PASSWORD=bar  java -Deventhubhandler.username=${USERNAME} -Deventhubhandler.password=${PASSWORD} -jar web/target/web-1.0-SNAPSHOT.jar  ```    ## Javascript Library  The project comes with a javascript library which can be integrated with your website as a way to send events to your EventHub server.     ### How to run JS tests  #### install [karma](http://karma-runner.github.io/0.12/index.html)  ```bash  cd ${EVENT_HUB_DIR}    npm install -g karma  npm install -g karma-jasmine@2_0  npm install -g karma-chrome-launcher    karma start karma.conf.js  ```    ### API  The javascript library is extremely simple and heavily inspired by mixpanel. There are only five methods that a developer needs to understand. Beware that behind the scenes, the library maintains a queue backed by localStorage, buffers the events in the queue, and has a timer reguarly clear the queue. If the browser doesn't support localStorage, a in-memory queue will be created as EventHub is created. Also, our implementation relies on the server to track the timestamp of each event. Therefore, in the case of a browser session disconnected before all the events are sent, the remaining events will be sent in the next browser session and thus have the timestamp recorded as the next session starts.    #### window.newEventHub()  The method will create an EventHub and start the timer which clears out the event queue in every second (default)  ```javascript  var name = ""EventHub"";  var options = {    url: 'http://example.com',    flushInterval: 10 /* in seconds */  };  var eventHub = window.newEventHub(name, options);  ```    #### eventHub.track()  This method enqueues the given event which will be cleared in batch at every flushInterval. Beware that if there is no identify method called before the track method is called, the library will automatically generate an user id which remain the same for the entire session (clears after the browser tab is closed), and send the generated user id along with the queued event. On the other hand, if `eventhub.identify()` is called before the track method is called, the user information passed along with the identify method call will be merged to the queued event.  ```javascript  eventHub.track(""signup"", {    property_1: 'value1',    property_2: 'value2'  });  ```    #### eventHub.alias()  This method links the given user to the automatically generated user. Typically, you only want to call this method once -- right after the user successfully signs up.  ```javascript  eventHub.alias('chengtao@codecademy.com');  ```    #### eventHub.identify()  This method tells the library instead of using the automatically generated user information, use the given information instead.  ```javascript  eventHub.identify('chengtao@codecademy.com', {    user_property_1: 'value1',    user_property_2: 'value2'  });  ```    #### eventHub.register()  This method allows the developer to add additional information to the generated user.  ```javascript  eventHub.register({    user_property_1: 'value1',    user_property_2: 'value2'  });  ```    ### Scenario and Receipes  #### Link the events sent before and after an user sign up  The following code  ```javascript  var eventHub = window.newEventHub('EventHub', { url: 'http://example.com' });  eventHub.track('pageview', { page: 'home' });  eventHub.register({    ip: '10.0.0.1'  });    // after user signup  eventHub.alias('chengtao@codecademy.com');  eventHub.identify('chengtao@codecademy.com', {    gender: 'male'  });  eventHub.track('pageview', { page: 'learn' });  ```   will result in a funnel like  ```javascript  {    user: 'something generated',    event: 'pageview',    page: 'home',    ip: '10.0.0.1'  }  link 'chengtao@codecademy.com' to 'something generated'  {    user: 'chengtao@codecademy.com',    event: 'pageview',    page: 'learn',    gender: 'male'  }  ```    #### A/B testing  The following code  ```javascript  var eventHub = window.newEventHub('EventHub', { url: 'http://example.com' });  eventHub.identify('chengtao@codecademy.com', {});  eventHub.track('pageview', {    page: 'javascript exercise 1',    experiment: 'fancy feature',    treatment: 'new'  });  eventHub.track('submit', {    page: 'javascript exercise 1'  });  ```  and  ```javascript  var eventHub = window.newEventHub('EventHub', { url: 'http://example.com' });  eventHub.identify('bob@codecademy.com', {});  eventHub.track('pageview', {    page: 'javascript exercise 1',    experiment: 'fancy feature',    treatment: 'control'  });  eventHub.track('skip', {    page: 'javascript exercise 1'  });  ```  will result in two funnels like  ```javascript  {    user: 'chengtao@codecademy.com',    event: 'pageview',    page: 'javascript exercise 1',    experiment: 'fancy feature',    treatment: 'new'  }  {    user: 'chengtao@codecademy.com',    event: 'submit',    page: 'javascript exercise 1'  }  ```  and  ```javascript  {    user: 'bob@codecademy.com',    event: 'pageview',    page: 'javascript exercise 1',    experiment: 'fancy feature',    treatment: 'control'  }  {    user: 'bob@codecademy.com',    event: 'skip',    page: 'javascript exercise 1'  }  ```    ## Ruby Library  Separate ruby gem is also available at [https://github.com/Codecademy/EventHubClient](https://github.com/Codecademy/EventHubClient)    ## License  MIT License.    Copyright (c) 2014 Ryzac, Inc.   """
Big data;https://github.com/boltdb/bolt;"""Bolt [![Coverage Status](https://coveralls.io/repos/boltdb/bolt/badge.svg?branch=master)](https://coveralls.io/r/boltdb/bolt?branch=master) [![GoDoc](https://godoc.org/github.com/boltdb/bolt?status.svg)](https://godoc.org/github.com/boltdb/bolt) ![Version](https://img.shields.io/badge/version-1.2.1-green.svg)  ====    Bolt is a pure Go key/value store inspired by [Howard Chu's][hyc_symas]  [LMDB project][lmdb]. The goal of the project is to provide a simple,  fast, and reliable database for projects that don't require a full database  server such as Postgres or MySQL.    Since Bolt is meant to be used as such a low-level piece of functionality,  simplicity is key. The API will be small and only focus on getting values  and setting values. That's it.    [hyc_symas]: https://twitter.com/hyc_symas  [lmdb]: http://symas.com/mdb/    ## Project Status    Bolt is stable, the API is fixed, and the file format is fixed. Full unit  test coverage and randomized black box testing are used to ensure database  consistency and thread safety. Bolt is currently used in high-load production  environments serving databases as large as 1TB. Many companies such as  Shopify and Heroku use Bolt-backed services every day.    ## A message from the author    > The original goal of Bolt was to provide a simple pure Go key/value store and to  > not bloat the code with extraneous features. To that end, the project has been  > a success. However, this limited scope also means that the project is complete.  >   > Maintaining an open source database requires an immense amount of time and energy.  > Changes to the code can have unintended and sometimes catastrophic effects so  > even simple changes require hours and hours of careful testing and validation.  >  > Unfortunately I no longer have the time or energy to continue this work. Bolt is  > in a stable state and has years of successful production use. As such, I feel that  > leaving it in its current state is the most prudent course of action.  >  > If you are interested in using a more featureful version of Bolt, I suggest that  > you look at the CoreOS fork called [bbolt](https://github.com/coreos/bbolt).    - Ben Johnson ([@benbjohnson](https://twitter.com/benbjohnson))    ## Table of Contents    - [Getting Started](#getting-started)    - [Installing](#installing)    - [Opening a database](#opening-a-database)    - [Transactions](#transactions)      - [Read-write transactions](#read-write-transactions)      - [Read-only transactions](#read-only-transactions)      - [Batch read-write transactions](#batch-read-write-transactions)      - [Managing transactions manually](#managing-transactions-manually)    - [Using buckets](#using-buckets)    - [Using key/value pairs](#using-keyvalue-pairs)    - [Autoincrementing integer for the bucket](#autoincrementing-integer-for-the-bucket)    - [Iterating over keys](#iterating-over-keys)      - [Prefix scans](#prefix-scans)      - [Range scans](#range-scans)      - [ForEach()](#foreach)    - [Nested buckets](#nested-buckets)    - [Database backups](#database-backups)    - [Statistics](#statistics)    - [Read-Only Mode](#read-only-mode)    - [Mobile Use (iOS/Android)](#mobile-use-iosandroid)  - [Resources](#resources)  - [Comparison with other databases](#comparison-with-other-databases)    - [Postgres, MySQL, & other relational databases](#postgres-mysql--other-relational-databases)    - [LevelDB, RocksDB](#leveldb-rocksdb)    - [LMDB](#lmdb)  - [Caveats & Limitations](#caveats--limitations)  - [Reading the Source](#reading-the-source)  - [Other Projects Using Bolt](#other-projects-using-bolt)    ## Getting Started    ### Installing    To start using Bolt, install Go and run `go get`:    ```sh  $ go get github.com/boltdb/bolt/...  ```    This will retrieve the library and install the `bolt` command line utility into  your `$GOBIN` path.      ### Opening a database    The top-level object in Bolt is a `DB`. It is represented as a single file on  your disk and represents a consistent snapshot of your data.    To open your database, simply use the `bolt.Open()` function:    ```go  package main    import (  	""log""    	""github.com/boltdb/bolt""  )    func main() {  	// Open the my.db data file in your current directory.  	// It will be created if it doesn't exist.  	db, err := bolt.Open(""my.db"", 0600, nil)  	if err != nil {  		log.Fatal(err)  	}  	defer db.Close()    	...  }  ```    Please note that Bolt obtains a file lock on the data file so multiple processes  cannot open the same database at the same time. Opening an already open Bolt  database will cause it to hang until the other process closes it. To prevent  an indefinite wait you can pass a timeout option to the `Open()` function:    ```go  db, err := bolt.Open(""my.db"", 0600, &bolt.Options{Timeout: 1 * time.Second})  ```      ### Transactions    Bolt allows only one read-write transaction at a time but allows as many  read-only transactions as you want at a time. Each transaction has a consistent  view of the data as it existed when the transaction started.    Individual transactions and all objects created from them (e.g. buckets, keys)  are not thread safe. To work with data in multiple goroutines you must start  a transaction for each one or use locking to ensure only one goroutine accesses  a transaction at a time. Creating transaction from the `DB` is thread safe.    Read-only transactions and read-write transactions should not depend on one  another and generally shouldn't be opened simultaneously in the same goroutine.  This can cause a deadlock as the read-write transaction needs to periodically  re-map the data file but it cannot do so while a read-only transaction is open.      #### Read-write transactions    To start a read-write transaction, you can use the `DB.Update()` function:    ```go  err := db.Update(func(tx *bolt.Tx) error {  	...  	return nil  })  ```    Inside the closure, you have a consistent view of the database. You commit the  transaction by returning `nil` at the end. You can also rollback the transaction  at any point by returning an error. All database operations are allowed inside  a read-write transaction.    Always check the return error as it will report any disk failures that can cause  your transaction to not complete. If you return an error within your closure  it will be passed through.      #### Read-only transactions    To start a read-only transaction, you can use the `DB.View()` function:    ```go  err := db.View(func(tx *bolt.Tx) error {  	...  	return nil  })  ```    You also get a consistent view of the database within this closure, however,  no mutating operations are allowed within a read-only transaction. You can only  retrieve buckets, retrieve values, and copy the database within a read-only  transaction.      #### Batch read-write transactions    Each `DB.Update()` waits for disk to commit the writes. This overhead  can be minimized by combining multiple updates with the `DB.Batch()`  function:    ```go  err := db.Batch(func(tx *bolt.Tx) error {  	...  	return nil  })  ```    Concurrent Batch calls are opportunistically combined into larger  transactions. Batch is only useful when there are multiple goroutines  calling it.    The trade-off is that `Batch` can call the given  function multiple times, if parts of the transaction fail. The  function must be idempotent and side effects must take effect only  after a successful return from `DB.Batch()`.    For example: don't display messages from inside the function, instead  set variables in the enclosing scope:    ```go  var id uint64  err := db.Batch(func(tx *bolt.Tx) error {  	// Find last key in bucket, decode as bigendian uint64, increment  	// by one, encode back to []byte, and add new key.  	...  	id = newValue  	return nil  })  if err != nil {  	return ...  }  fmt.Println(""Allocated ID %d"", id)  ```      #### Managing transactions manually    The `DB.View()` and `DB.Update()` functions are wrappers around the `DB.Begin()`  function. These helper functions will start the transaction, execute a function,  and then safely close your transaction if an error is returned. This is the  recommended way to use Bolt transactions.    However, sometimes you may want to manually start and end your transactions.  You can use the `DB.Begin()` function directly but **please** be sure to close  the transaction.    ```go  // Start a writable transaction.  tx, err := db.Begin(true)  if err != nil {      return err  }  defer tx.Rollback()    // Use the transaction...  _, err := tx.CreateBucket([]byte(""MyBucket""))  if err != nil {      return err  }    // Commit the transaction and check for error.  if err := tx.Commit(); err != nil {      return err  }  ```    The first argument to `DB.Begin()` is a boolean stating if the transaction  should be writable.      ### Using buckets    Buckets are collections of key/value pairs within the database. All keys in a  bucket must be unique. You can create a bucket using the `DB.CreateBucket()`  function:    ```go  db.Update(func(tx *bolt.Tx) error {  	b, err := tx.CreateBucket([]byte(""MyBucket""))  	if err != nil {  		return fmt.Errorf(""create bucket: %s"", err)  	}  	return nil  })  ```    You can also create a bucket only if it doesn't exist by using the  `Tx.CreateBucketIfNotExists()` function. It's a common pattern to call this  function for all your top-level buckets after you open your database so you can  guarantee that they exist for future transactions.    To delete a bucket, simply call the `Tx.DeleteBucket()` function.      ### Using key/value pairs    To save a key/value pair to a bucket, use the `Bucket.Put()` function:    ```go  db.Update(func(tx *bolt.Tx) error {  	b := tx.Bucket([]byte(""MyBucket""))  	err := b.Put([]byte(""answer""), []byte(""42""))  	return err  })  ```    This will set the value of the `""answer""` key to `""42""` in the `MyBucket`  bucket. To retrieve this value, we can use the `Bucket.Get()` function:    ```go  db.View(func(tx *bolt.Tx) error {  	b := tx.Bucket([]byte(""MyBucket""))  	v := b.Get([]byte(""answer""))  	fmt.Printf(""The answer is: %s\n"", v)  	return nil  })  ```    The `Get()` function does not return an error because its operation is  guaranteed to work (unless there is some kind of system failure). If the key  exists then it will return its byte slice value. If it doesn't exist then it  will return `nil`. It's important to note that you can have a zero-length value  set to a key which is different than the key not existing.    Use the `Bucket.Delete()` function to delete a key from the bucket.    Please note that values returned from `Get()` are only valid while the  transaction is open. If you need to use a value outside of the transaction  then you must use `copy()` to copy it to another byte slice.      ### Autoincrementing integer for the bucket  By using the `NextSequence()` function, you can let Bolt determine a sequence  which can be used as the unique identifier for your key/value pairs. See the  example below.    ```go  // CreateUser saves u to the store. The new user ID is set on u once the data is persisted.  func (s *Store) CreateUser(u *User) error {      return s.db.Update(func(tx *bolt.Tx) error {          // Retrieve the users bucket.          // This should be created when the DB is first opened.          b := tx.Bucket([]byte(""users""))            // Generate ID for the user.          // This returns an error only if the Tx is closed or not writeable.          // That can't happen in an Update() call so I ignore the error check.          id, _ := b.NextSequence()          u.ID = int(id)            // Marshal user data into bytes.          buf, err := json.Marshal(u)          if err != nil {              return err          }            // Persist bytes to users bucket.          return b.Put(itob(u.ID), buf)      })  }    // itob returns an 8-byte big endian representation of v.  func itob(v int) []byte {      b := make([]byte, 8)      binary.BigEndian.PutUint64(b, uint64(v))      return b  }    type User struct {      ID int      ...  }  ```    ### Iterating over keys    Bolt stores its keys in byte-sorted order within a bucket. This makes sequential  iteration over these keys extremely fast. To iterate over keys we'll use a  `Cursor`:    ```go  db.View(func(tx *bolt.Tx) error {  	// Assume bucket exists and has keys  	b := tx.Bucket([]byte(""MyBucket""))    	c := b.Cursor()    	for k, v := c.First(); k != nil; k, v = c.Next() {  		fmt.Printf(""key=%s, value=%s\n"", k, v)  	}    	return nil  })  ```    The cursor allows you to move to a specific point in the list of keys and move  forward or backward through the keys one at a time.    The following functions are available on the cursor:    ```  First()  Move to the first key.  Last()   Move to the last key.  Seek()   Move to a specific key.  Next()   Move to the next key.  Prev()   Move to the previous key.  ```    Each of those functions has a return signature of `(key []byte, value []byte)`.  When you have iterated to the end of the cursor then `Next()` will return a  `nil` key.  You must seek to a position using `First()`, `Last()`, or `Seek()`  before calling `Next()` or `Prev()`. If you do not seek to a position then  these functions will return a `nil` key.    During iteration, if the key is non-`nil` but the value is `nil`, that means  the key refers to a bucket rather than a value.  Use `Bucket.Bucket()` to  access the sub-bucket.      #### Prefix scans    To iterate over a key prefix, you can combine `Seek()` and `bytes.HasPrefix()`:    ```go  db.View(func(tx *bolt.Tx) error {  	// Assume bucket exists and has keys  	c := tx.Bucket([]byte(""MyBucket"")).Cursor()    	prefix := []byte(""1234"")  	for k, v := c.Seek(prefix); k != nil && bytes.HasPrefix(k, prefix); k, v = c.Next() {  		fmt.Printf(""key=%s, value=%s\n"", k, v)  	}    	return nil  })  ```    #### Range scans    Another common use case is scanning over a range such as a time range. If you  use a sortable time encoding such as RFC3339 then you can query a specific  date range like this:    ```go  db.View(func(tx *bolt.Tx) error {  	// Assume our events bucket exists and has RFC3339 encoded time keys.  	c := tx.Bucket([]byte(""Events"")).Cursor()    	// Our time range spans the 90's decade.  	min := []byte(""1990-01-01T00:00:00Z"")  	max := []byte(""2000-01-01T00:00:00Z"")    	// Iterate over the 90's.  	for k, v := c.Seek(min); k != nil && bytes.Compare(k, max) <= 0; k, v = c.Next() {  		fmt.Printf(""%s: %s\n"", k, v)  	}    	return nil  })  ```    Note that, while RFC3339 is sortable, the Golang implementation of RFC3339Nano does not use a fixed number of digits after the decimal point and is therefore not sortable.      #### ForEach()    You can also use the function `ForEach()` if you know you'll be iterating over  all the keys in a bucket:    ```go  db.View(func(tx *bolt.Tx) error {  	// Assume bucket exists and has keys  	b := tx.Bucket([]byte(""MyBucket""))    	b.ForEach(func(k, v []byte) error {  		fmt.Printf(""key=%s, value=%s\n"", k, v)  		return nil  	})  	return nil  })  ```    Please note that keys and values in `ForEach()` are only valid while  the transaction is open. If you need to use a key or value outside of  the transaction, you must use `copy()` to copy it to another byte  slice.    ### Nested buckets    You can also store a bucket in a key to create nested buckets. The API is the  same as the bucket management API on the `DB` object:    ```go  func (*Bucket) CreateBucket(key []byte) (*Bucket, error)  func (*Bucket) CreateBucketIfNotExists(key []byte) (*Bucket, error)  func (*Bucket) DeleteBucket(key []byte) error  ```    Say you had a multi-tenant application where the root level bucket was the account bucket. Inside of this bucket was a sequence of accounts which themselves are buckets. And inside the sequence bucket you could have many buckets pertaining to the Account itself (Users, Notes, etc) isolating the information into logical groupings.    ```go    // createUser creates a new user in the given account.  func createUser(accountID int, u *User) error {      // Start the transaction.      tx, err := db.Begin(true)      if err != nil {          return err      }      defer tx.Rollback()        // Retrieve the root bucket for the account.      // Assume this has already been created when the account was set up.      root := tx.Bucket([]byte(strconv.FormatUint(accountID, 10)))        // Setup the users bucket.      bkt, err := root.CreateBucketIfNotExists([]byte(""USERS""))      if err != nil {          return err      }        // Generate an ID for the new user.      userID, err := bkt.NextSequence()      if err != nil {          return err      }      u.ID = userID        // Marshal and save the encoded user.      if buf, err := json.Marshal(u); err != nil {          return err      } else if err := bkt.Put([]byte(strconv.FormatUint(u.ID, 10)), buf); err != nil {          return err      }        // Commit the transaction.      if err := tx.Commit(); err != nil {          return err      }        return nil  }    ```          ### Database backups    Bolt is a single file so it's easy to backup. You can use the `Tx.WriteTo()`  function to write a consistent view of the database to a writer. If you call  this from a read-only transaction, it will perform a hot backup and not block  your other database reads and writes.    By default, it will use a regular file handle which will utilize the operating  system's page cache. See the [`Tx`](https://godoc.org/github.com/boltdb/bolt#Tx)  documentation for information about optimizing for larger-than-RAM datasets.    One common use case is to backup over HTTP so you can use tools like `cURL` to  do database backups:    ```go  func BackupHandleFunc(w http.ResponseWriter, req *http.Request) {  	err := db.View(func(tx *bolt.Tx) error {  		w.Header().Set(""Content-Type"", ""application/octet-stream"")  		w.Header().Set(""Content-Disposition"", `attachment; filename=""my.db""`)  		w.Header().Set(""Content-Length"", strconv.Itoa(int(tx.Size())))  		_, err := tx.WriteTo(w)  		return err  	})  	if err != nil {  		http.Error(w, err.Error(), http.StatusInternalServerError)  	}  }  ```    Then you can backup using this command:    ```sh  $ curl http://localhost/backup > my.db  ```    Or you can open your browser to `http://localhost/backup` and it will download  automatically.    If you want to backup to another file you can use the `Tx.CopyFile()` helper  function.      ### Statistics    The database keeps a running count of many of the internal operations it  performs so you can better understand what's going on. By grabbing a snapshot  of these stats at two points in time we can see what operations were performed  in that time range.    For example, we could start a goroutine to log stats every 10 seconds:    ```go  go func() {  	// Grab the initial stats.  	prev := db.Stats()    	for {  		// Wait for 10s.  		time.Sleep(10 * time.Second)    		// Grab the current stats and diff them.  		stats := db.Stats()  		diff := stats.Sub(&prev)    		// Encode stats to JSON and print to STDERR.  		json.NewEncoder(os.Stderr).Encode(diff)    		// Save stats for the next loop.  		prev = stats  	}  }()  ```    It's also useful to pipe these stats to a service such as statsd for monitoring  or to provide an HTTP endpoint that will perform a fixed-length sample.      ### Read-Only Mode    Sometimes it is useful to create a shared, read-only Bolt database. To this,  set the `Options.ReadOnly` flag when opening your database. Read-only mode  uses a shared lock to allow multiple processes to read from the database but  it will block any processes from opening the database in read-write mode.    ```go  db, err := bolt.Open(""my.db"", 0666, &bolt.Options{ReadOnly: true})  if err != nil {  	log.Fatal(err)  }  ```    ### Mobile Use (iOS/Android)    Bolt is able to run on mobile devices by leveraging the binding feature of the  [gomobile](https://github.com/golang/mobile) tool. Create a struct that will  contain your database logic and a reference to a `*bolt.DB` with a initializing  constructor that takes in a filepath where the database file will be stored.  Neither Android nor iOS require extra permissions or cleanup from using this method.    ```go  func NewBoltDB(filepath string) *BoltDB {  	db, err := bolt.Open(filepath+""/demo.db"", 0600, nil)  	if err != nil {  		log.Fatal(err)  	}    	return &BoltDB{db}  }    type BoltDB struct {  	db *bolt.DB  	...  }    func (b *BoltDB) Path() string {  	return b.db.Path()  }    func (b *BoltDB) Close() {  	b.db.Close()  }  ```    Database logic should be defined as methods on this wrapper struct.    To initialize this struct from the native language (both platforms now sync  their local storage to the cloud. These snippets disable that functionality for the  database file):    #### Android    ```java  String path;  if (android.os.Build.VERSION.SDK_INT >=android.os.Build.VERSION_CODES.LOLLIPOP){      path = getNoBackupFilesDir().getAbsolutePath();  } else{      path = getFilesDir().getAbsolutePath();  }  Boltmobiledemo.BoltDB boltDB = Boltmobiledemo.NewBoltDB(path)  ```    #### iOS    ```objc  - (void)demo {      NSString* path = [NSSearchPathForDirectoriesInDomains(NSLibraryDirectory,                                                            NSUserDomainMask,                                                            YES) objectAtIndex:0];  	GoBoltmobiledemoBoltDB * demo = GoBoltmobiledemoNewBoltDB(path);  	[self addSkipBackupAttributeToItemAtPath:demo.path];  	//Some DB Logic would go here  	[demo close];  }    - (BOOL)addSkipBackupAttributeToItemAtPath:(NSString *) filePathString  {      NSURL* URL= [NSURL fileURLWithPath: filePathString];      assert([[NSFileManager defaultManager] fileExistsAtPath: [URL path]]);        NSError *error = nil;      BOOL success = [URL setResourceValue: [NSNumber numberWithBool: YES]                                    forKey: NSURLIsExcludedFromBackupKey error: &error];      if(!success){          NSLog(@""Error excluding %@ from backup %@"", [URL lastPathComponent], error);      }      return success;  }    ```    ## Resources    For more information on getting started with Bolt, check out the following articles:    * [Intro to BoltDB: Painless Performant Persistence](http://npf.io/2014/07/intro-to-boltdb-painless-performant-persistence/) by [Nate Finch](https://github.com/natefinch).  * [Bolt -- an embedded key/value database for Go](https://www.progville.com/go/bolt-embedded-db-golang/) by Progville      ## Comparison with other databases    ### Postgres, MySQL, & other relational databases    Relational databases structure data into rows and are only accessible through  the use of SQL. This approach provides flexibility in how you store and query  your data but also incurs overhead in parsing and planning SQL statements. Bolt  accesses all data by a byte slice key. This makes Bolt fast to read and write  data by key but provides no built-in support for joining values together.    Most relational databases (with the exception of SQLite) are standalone servers  that run separately from your application. This gives your systems  flexibility to connect multiple application servers to a single database  server but also adds overhead in serializing and transporting data over the  network. Bolt runs as a library included in your application so all data access  has to go through your application's process. This brings data closer to your  application but limits multi-process access to the data.      ### LevelDB, RocksDB    LevelDB and its derivatives (RocksDB, HyperLevelDB) are similar to Bolt in that  they are libraries bundled into the application, however, their underlying  structure is a log-structured merge-tree (LSM tree). An LSM tree optimizes  random writes by using a write ahead log and multi-tiered, sorted files called  SSTables. Bolt uses a B+tree internally and only a single file. Both approaches  have trade-offs.    If you require a high random write throughput (>10,000 w/sec) or you need to use  spinning disks then LevelDB could be a good choice. If your application is  read-heavy or does a lot of range scans then Bolt could be a good choice.    One other important consideration is that LevelDB does not have transactions.  It supports batch writing of key/values pairs and it supports read snapshots  but it will not give you the ability to do a compare-and-swap operation safely.  Bolt supports fully serializable ACID transactions.      ### LMDB    Bolt was originally a port of LMDB so it is architecturally similar. Both use  a B+tree, have ACID semantics with fully serializable transactions, and support  lock-free MVCC using a single writer and multiple readers.    The two projects have somewhat diverged. LMDB heavily focuses on raw performance  while Bolt has focused on simplicity and ease of use. For example, LMDB allows  several unsafe actions such as direct writes for the sake of performance. Bolt  opts to disallow actions which can leave the database in a corrupted state. The  only exception to this in Bolt is `DB.NoSync`.    There are also a few differences in API. LMDB requires a maximum mmap size when  opening an `mdb_env` whereas Bolt will handle incremental mmap resizing  automatically. LMDB overloads the getter and setter functions with multiple  flags whereas Bolt splits these specialized cases into their own functions.      ## Caveats & Limitations    It's important to pick the right tool for the job and Bolt is no exception.  Here are a few things to note when evaluating and using Bolt:    * Bolt is good for read intensive workloads. Sequential write performance is    also fast but random writes can be slow. You can use `DB.Batch()` or add a    write-ahead log to help mitigate this issue.    * Bolt uses a B+tree internally so there can be a lot of random page access.    SSDs provide a significant performance boost over spinning disks.    * Try to avoid long running read transactions. Bolt uses copy-on-write so    old pages cannot be reclaimed while an old transaction is using them.    * Byte slices returned from Bolt are only valid during a transaction. Once the    transaction has been committed or rolled back then the memory they point to    can be reused by a new page or can be unmapped from virtual memory and you'll    see an `unexpected fault address` panic when accessing it.    * Bolt uses an exclusive write lock on the database file so it cannot be    shared by multiple processes.    * Be careful when using `Bucket.FillPercent`. Setting a high fill percent for    buckets that have random inserts will cause your database to have very poor    page utilization.    * Use larger buckets in general. Smaller buckets causes poor page utilization    once they become larger than the page size (typically 4KB).    * Bulk loading a lot of random writes into a new bucket can be slow as the    page will not split until the transaction is committed. Randomly inserting    more than 100,000 key/value pairs into a single new bucket in a single    transaction is not advised.    * Bolt uses a memory-mapped file so the underlying operating system handles the    caching of the data. Typically, the OS will cache as much of the file as it    can in memory and will release memory as needed to other processes. This means    that Bolt can show very high memory usage when working with large databases.    However, this is expected and the OS will release memory as needed. Bolt can    handle databases much larger than the available physical RAM, provided its    memory-map fits in the process virtual address space. It may be problematic    on 32-bits systems.    * The data structures in the Bolt database are memory mapped so the data file    will be endian specific. This means that you cannot copy a Bolt file from a    little endian machine to a big endian machine and have it work. For most    users this is not a concern since most modern CPUs are little endian.    * Because of the way pages are laid out on disk, Bolt cannot truncate data files    and return free pages back to the disk. Instead, Bolt maintains a free list    of unused pages within its data file. These free pages can be reused by later    transactions. This works well for many use cases as databases generally tend    to grow. However, it's important to note that deleting large chunks of data    will not allow you to reclaim that space on disk.      For more information on page allocation, [see this comment][page-allocation].    [page-allocation]: https://github.com/boltdb/bolt/issues/308#issuecomment-74811638      ## Reading the Source    Bolt is a relatively small code base (<3KLOC) for an embedded, serializable,  transactional key/value database so it can be a good starting point for people  interested in how databases work.    The best places to start are the main entry points into Bolt:    - `Open()` - Initializes the reference to the database. It's responsible for    creating the database if it doesn't exist, obtaining an exclusive lock on the    file, reading the meta pages, & memory-mapping the file.    - `DB.Begin()` - Starts a read-only or read-write transaction depending on the    value of the `writable` argument. This requires briefly obtaining the ""meta""    lock to keep track of open transactions. Only one read-write transaction can    exist at a time so the ""rwlock"" is acquired during the life of a read-write    transaction.    - `Bucket.Put()` - Writes a key/value pair into a bucket. After validating the    arguments, a cursor is used to traverse the B+tree to the page and position    where they key & value will be written. Once the position is found, the bucket    materializes the underlying page and the page's parent pages into memory as    ""nodes"". These nodes are where mutations occur during read-write transactions.    These changes get flushed to disk during commit.    - `Bucket.Get()` - Retrieves a key/value pair from a bucket. This uses a cursor    to move to the page & position of a key/value pair. During a read-only    transaction, the key and value data is returned as a direct reference to the    underlying mmap file so there's no allocation overhead. For read-write    transactions, this data may reference the mmap file or one of the in-memory    node values.    - `Cursor` - This object is simply for traversing the B+tree of on-disk pages    or in-memory nodes. It can seek to a specific key, move to the first or last    value, or it can move forward or backward. The cursor handles the movement up    and down the B+tree transparently to the end user.    - `Tx.Commit()` - Converts the in-memory dirty nodes and the list of free pages    into pages to be written to disk. Writing to disk then occurs in two phases.    First, the dirty pages are written to disk and an `fsync()` occurs. Second, a    new meta page with an incremented transaction ID is written and another    `fsync()` occurs. This two phase write ensures that partially written data    pages are ignored in the event of a crash since the meta page pointing to them    is never written. Partially written meta pages are invalidated because they    are written with a checksum.    If you have additional notes that could be helpful for others, please submit  them via pull request.      ## Other Projects Using Bolt    Below is a list of public, open source projects that use Bolt:    * [BoltDbWeb](https://github.com/evnix/boltdbweb) - A web based GUI for BoltDB files.  * [Operation Go: A Routine Mission](http://gocode.io) - An online programming game for Golang using Bolt for user accounts and a leaderboard.  * [Bazil](https://bazil.org/) - A file system that lets your data reside where it is most convenient for it to reside.  * [DVID](https://github.com/janelia-flyem/dvid) - Added Bolt as optional storage engine and testing it against Basho-tuned leveldb.  * [Skybox Analytics](https://github.com/skybox/skybox) - A standalone funnel analysis tool for web analytics.  * [Scuttlebutt](https://github.com/benbjohnson/scuttlebutt) - Uses Bolt to store and process all Twitter mentions of GitHub projects.  * [Wiki](https://github.com/peterhellberg/wiki) - A tiny wiki using Goji, BoltDB and Blackfriday.  * [ChainStore](https://github.com/pressly/chainstore) - Simple key-value interface to a variety of storage engines organized as a chain of operations.  * [MetricBase](https://github.com/msiebuhr/MetricBase) - Single-binary version of Graphite.  * [Gitchain](https://github.com/gitchain/gitchain) - Decentralized, peer-to-peer Git repositories aka ""Git meets Bitcoin"".  * [event-shuttle](https://github.com/sclasen/event-shuttle) - A Unix system service to collect and reliably deliver messages to Kafka.  * [ipxed](https://github.com/kelseyhightower/ipxed) - Web interface and api for ipxed.  * [BoltStore](https://github.com/yosssi/boltstore) - Session store using Bolt.  * [photosite/session](https://godoc.org/bitbucket.org/kardianos/photosite/session) - Sessions for a photo viewing site.  * [LedisDB](https://github.com/siddontang/ledisdb) - A high performance NoSQL, using Bolt as optional storage.  * [ipLocator](https://github.com/AndreasBriese/ipLocator) - A fast ip-geo-location-server using bolt with bloom filters.  * [cayley](https://github.com/google/cayley) - Cayley is an open-source graph database using Bolt as optional backend.  * [bleve](http://www.blevesearch.com/) - A pure Go search engine similar to ElasticSearch that uses Bolt as the default storage backend.  * [tentacool](https://github.com/optiflows/tentacool) - REST api server to manage system stuff (IP, DNS, Gateway...) on a linux server.  * [Seaweed File System](https://github.com/chrislusf/seaweedfs) - Highly scalable distributed key~file system with O(1) disk read.  * [InfluxDB](https://influxdata.com) - Scalable datastore for metrics, events, and real-time analytics.  * [Freehold](http://tshannon.bitbucket.org/freehold/) - An open, secure, and lightweight platform for your files and data.  * [Prometheus Annotation Server](https://github.com/oliver006/prom_annotation_server) - Annotation server for PromDash & Prometheus service monitoring system.  * [Consul](https://github.com/hashicorp/consul) - Consul is service discovery and configuration made easy. Distributed, highly available, and datacenter-aware.  * [Kala](https://github.com/ajvb/kala) - Kala is a modern job scheduler optimized to run on a single node. It is persistent, JSON over HTTP API, ISO 8601 duration notation, and dependent jobs.  * [drive](https://github.com/odeke-em/drive) - drive is an unofficial Google Drive command line client for \*NIX operating systems.  * [stow](https://github.com/djherbis/stow) -  a persistence manager for objects    backed by boltdb.  * [buckets](https://github.com/joyrexus/buckets) - a bolt wrapper streamlining    simple tx and key scans.  * [mbuckets](https://github.com/abhigupta912/mbuckets) - A Bolt wrapper that allows easy operations on multi level (nested) buckets.  * [Request Baskets](https://github.com/darklynx/request-baskets) - A web service to collect arbitrary HTTP requests and inspect them via REST API or simple web UI, similar to [RequestBin](http://requestb.in/) service  * [Go Report Card](https://goreportcard.com/) - Go code quality report cards as a (free and open source) service.  * [Boltdb Boilerplate](https://github.com/bobintornado/boltdb-boilerplate) - Boilerplate wrapper around bolt aiming to make simple calls one-liners.  * [lru](https://github.com/crowdriff/lru) - Easy to use Bolt-backed Least-Recently-Used (LRU) read-through cache with chainable remote stores.  * [Storm](https://github.com/asdine/storm) - Simple and powerful ORM for BoltDB.  * [GoWebApp](https://github.com/josephspurrier/gowebapp) - A basic MVC web application in Go using BoltDB.  * [SimpleBolt](https://github.com/xyproto/simplebolt) - A simple way to use BoltDB. Deals mainly with strings.  * [Algernon](https://github.com/xyproto/algernon) - A HTTP/2 web server with built-in support for Lua. Uses BoltDB as the default database backend.  * [MuLiFS](https://github.com/dankomiocevic/mulifs) - Music Library Filesystem creates a filesystem to organise your music files.  * [GoShort](https://github.com/pankajkhairnar/goShort) - GoShort is a URL shortener written in Golang and BoltDB for persistent key/value storage and for routing it's using high performent HTTPRouter.  * [torrent](https://github.com/anacrolix/torrent) - Full-featured BitTorrent client package and utilities in Go. BoltDB is a storage backend in development.  * [gopherpit](https://github.com/gopherpit/gopherpit) - A web service to manage Go remote import paths with custom domains  * [bolter](https://github.com/hasit/bolter) - Command-line app for viewing BoltDB file in your terminal.  * [btcwallet](https://github.com/btcsuite/btcwallet) - A bitcoin wallet.  * [dcrwallet](https://github.com/decred/dcrwallet) - A wallet for the Decred cryptocurrency.  * [Ironsmith](https://github.com/timshannon/ironsmith) - A simple, script-driven continuous integration (build - > test -> release) tool, with no external dependencies  * [BoltHold](https://github.com/timshannon/bolthold) - An embeddable NoSQL store for Go types built on BoltDB  * [Ponzu CMS](https://ponzu-cms.org) - Headless CMS + automatic JSON API with auto-HTTPS, HTTP/2 Server Push, and flexible server framework.    If you are using Bolt in a project please send a pull request to add it to the list. """
Big data;https://github.com/uber/AthenaX;"""[![Build Status][ci-img]][ci] [![ReadTheDocs][doc-img]][doc]    # AthenaX: SQL-based streaming analytics platform at scale    AthenaX is a streaming analytics platform that enables users to run production-quality, large scale streaming analytics using Structured Query Language (SQL). AthenaX was released and open sourced by [Uber Technologies][ubeross]. It is capable of scaling across hundreds of machines and processing hundreds of billions of real-time events daily.    See also:      * AthenaX [documentation][doc] for getting started, operational details, and other information.    * Blog post [Introducing AthenaX, Uber Engineeringâ€™s Open Source Streaming Analytics Platform](https://eng.uber.com/athenax/).    ## License  [Apache 2.0 License](./LICENSE).    [doc-img]: https://readthedocs.org/projects/athenax/badge/?version=latest  [doc]: http://athenax.readthedocs.org/en/latest/  [ci-img]: https://travis-ci.org/jaegertracing/jaeger.svg?branch=master  [ci]: https://travis-ci.org/uber/AthenaX  [ubeross]: http://uber.github.io """
Big data;https://github.com/spring-projects/spring-xd;"""Spring XD  =========    *Spring XD* makes it easy to solve common big data problems such as data ingestion and export, real-time analytics, and batch workflow orchestration.  By building on mature, open source projects such as Spring Integration, Data and Batch, Spring XD will simplify the process of creating real-word big data solutions.  XD stands for 'eXtreme Data' or 'x' as in y=mx+b :)    While it is possible today to build such solutions using Spring (see the [Spring Data Book][] for details and examples), Spring XD will move well beyond the framework API level by providing an out-of-the-box executable server, a pluggable module system, a high level configuration DSL, a simple model for distributing data processing instances on or off the Hadoop cluster, and more.    You can fork the repository and/or monitor JIRA to see what is going on. As always, we consider the feedback from our broad and passionate community to be one of our greatest assets.    ## Documentation    Look for it on the [XD wiki](https://github.com/springsource/spring-xd/wiki). [API Documentation](http://static.springsource.org/spring-xd/docs/current-SNAPSHOT/api/) (JavaDoc) is available as well. Please also visit the SpringSource.org [project website](http://www.springsource.org/spring-xd) for more information.    ## How to build     Check the documentation on how to build Spring XD [here](http://docs.spring.io/spring-xd/docs/current-SNAPSHOT/reference/html/#building-spring-xd).    ## Getting Help    * Get involved with the community on StackOverflow using the tag spring-xd.    ## License    *Spring XD* is released under version 2.0 of the [Apache License][].    ## Contributing to Spring XD    Here are some ways for you to get involved     * Create [JIRA](https://jira.springsource.org/browse/XD) tickets for bugs and new features and comment and vote on the ones that you are interested in.  * Follow the flow of developing on the [work board](https://jira.springsource.org/secure/RapidBoard.jspa?rapidView=6).  * Github is for social coding: if you want to write code, we encourage contributions through pull requests from [forks of this repository](http://help.github.com/forking/).  If you want to contribute code this way, please familiarize yourself with the process outlined for contributing to Spring projects here: [Contributor Guidelines](https://github.com/SpringSource/spring-integration/wiki/Contributor-Guidelines).    Before we accept a non-trivial patch or pull request we will need you to sign the [contributor's agreement](https://support.springsource.com/spring_committer_signup).  Signing the contributor's agreement does not grant anyone commit rights to the main repository, but it does mean that we can accept your contributions, and you will get an author credit if we do.  Active contributors might be asked to join the core team, and given the ability to merge pull requests.    ## Issue Tracking    Report issues via the [Spring XD JIRA][].    ## Continuous Integration    * **Master**: https://build.spring.io/browse/XD-MASTER  * **Sonar**: https://build.spring.io/browse/XD-SONAR    ## Metrics    Source Metrics are available via Sonar at:    * https://sonar.springsource.org/dashboard/index/org.springframework.xd:spring-xd      [Spring XD JIRA]: https://jira.springsource.org/browse/XD  [Apache License]: http://www.apache.org/licenses/LICENSE-2.0  [Spring Data Book]: http://bit.ly/sd-book  """
Big data;https://github.com/metabase/metabase;"""# Metabase    [Metabase](https://www.metabase.com/) is the easy, open-source way for everyone in your company to ask questions and learn from data.    ![Metabase Product Screenshot](docs/metabase-product-screenshot.png)    [![Latest Release](https://img.shields.io/github/release/metabase/metabase.svg?label=latest%20release)](https://github.com/metabase/metabase/releases)  [![Circle CI](https://circleci.com/gh/metabase/metabase.svg?style=svg&circle-token=3ccf0aa841028af027f2ac9e8df17ce603e90ef9)](https://circleci.com/gh/metabase/metabase)  [![codecov](https://codecov.io/gh/metabase/metabase/branch/master/graph/badge.svg)](https://codecov.io/gh/metabase/metabase)    ## Features    - [Set up in five minutes](https://metabase.com/docs/latest/setting-up-metabase.html) (we're not kidding).  - Let anyone on your team [ask questions](https://metabase.com/docs/latest/users-guide/04-asking-questions.html) without knowing SQL.  - Use the [SQL editor](https://www.metabase.com/docs/latest/users-guide/writing-sql.html) for more complex queries.   - Build handsome, interactive [dashboards](https://metabase.com/docs/latest/users-guide/06-sharing-answers.html) with filters, auto-refresh, fullscreen, and custom click behavior.  - Create [models](https://www.metabase.com/learn/getting-started/models) that clean up, annotate, and/or combine raw tables.  - Define canonical [segments and metrics](https://metabase.com/docs/latest/administration-guide/07-segments-and-metrics.html) for your team to use.  - Send data to Slack or email on a schedule with [dashboard subscriptions](https://www.metabase.com/docs/latest/users-guide/dashboard-subscriptions.html).  - Set up [alerts](https://www.metabase.com/docs/latest/users-guide/15-alerts.html) to have Metabase notify you when your data changes.  - [Embed charts and dashboards](https://www.metabase.com/docs/latest/administration-guide/13-embedding.html) in your app, or even [your entire Metabase](https://www.metabase.com/docs/latest/enterprise-guide/full-app-embedding.html).    Take a [tour of Metabase](https://www.metabase.com/learn/getting-started/tour-of-metabase).    ## Supported databases    - [Officially supported databases](https://www.metabase.com/docs/latest/administration-guide/01-managing-databases.html#officially-supported-databases).  - [Community-supported drivers](https://www.metabase.com/docs/latest/developers-guide-drivers.html#how-to-use-a-community-built-driver).    ## Installation    Metabase can be run just about anywhere. Check out our [Installation Guides](https://www.metabase.com/docs/latest/operations-guide/installing-metabase.html).    ## Contributing    To get started with a development installation of the Metabase, check out our [Developers Guide](https://www.metabase.com/docs/latest/developers-guide/start).    ## Internationalization    We want Metabase to be available in as many languages as possible. See which translations are available and help contribute to internationalization using our project over at [POEditor](https://poeditor.com/join/project/ynjQmwSsGh). You can also check out our [policies on translations](https://www.metabase.com/docs/latest/administration-guide/localization.html).    ## Extending Metabase    Metabase also allows you to hit our Query API directly from Javascript to integrate the simple analytics we provide with your own application or third party services to do things like:    - Build moderation interfaces.  - Export subsets of your users to third party marketing automation software.  - Provide a specialized customer lookup application for the people in your company.    Check out our guide, [Working with the Metabase API](https://www.metabase.com/learn/administration/metabase-api).    ## Security Disclosure    See [SECURITY.md](./SECURITY.md) for details.    ## License    This repository contains the source code for both the Open Source edition of Metabase, released under the AGPL, as well as the [commercial editions of Metabase](https://www.metabase.com/pricing/), which are released under the Metabase Commercial Software License.     See [LICENSE.txt](./LICENSE.txt) for details.    Unless otherwise noted, all files Â© 2022 Metabase, Inc. """
Big data;https://github.com/probcomp/BayesDB;"""BayesDB  =======    This repository contained a previous implementation of BayesDB which  has been replaced by the new implementation Bayeslite. For the new  repository, visit https://github.com/probcomp/bayeslite. For  information about BayesDB in general, visit  http://probcomp.csail.mit.edu/bayesdb    BayesDB, a Bayesian database, lets users query the probable  implications of their data as easily as a SQL database lets them query  the data itself. Using the built-in Bayesian Query Language (BQL),  users with no statistics training can solve basic data science  problems, such as detecting predictive relationships between  variables, inferring missing values, simulating probable observations,  and identifying statistically similar database entries.    # License    [Apache License, Version 2.0](https://github.com/probcomp/bayesdb/blob/master/LICENSE) """
Big data;https://github.com/facebookincubator/beringei;"""** THIS REPO HAS BEEN ARCHIVED AND IS NO LONGER BEING ACTIVELY MAINTAINED **    # Beringei [![CircleCI](https://circleci.com/gh/facebookincubator/beringei/tree/master.svg?style=svg)](https://circleci.com/gh/facebookincubator/beringei/tree/master)  A high performance, in memory time series storage engine    <img src=""./beringei_logo_clear.png"" height=200 width=200>    In the fall of 2015, we published the [paper â€œGorilla: A Fast, Scalable, In-Memory Time Series Databaseâ€](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf) at VLDB 2015. Beringei is the open source representation of the ideas presented in this paper.    Beringei is a high performance time series storage engine. Time series are commonly used as a representation of statistics, gauges, and counters for monitoring performance and health of a system.     ## Features    Beringei has the following features:    * Support for very fast, in-memory storage, backed by disk for persistence. Queries to the storage engine are always served out of memory for extremely fast query performance, but backed to disk so the process can be restarted or migrated with very little down time and no data loss.  * Extremely efficient streaming compression algorithm. Our streaming compression algorithm is able to compress real world time series data by over 90%. The delta of delta compression algorithm used by Beringei is also fast - we see that a single machine is able to compress more than 1.5 million datapoints/second.  * Reference sharded service implementation, including a client implementation.  * Reference http service implementation that enables direct Grafana integration.    ## How can I use Beringei?    Beringei can be used in one of two ways.     1. We have created a simple, sharded service, and reference client implementation, that can store and serve  time series query requests.   1. You can use Beringei as an embedded library to handle the low-level details of efficiently storing time series data. Using Beringei in this way is similar to [RocksDB](https://rocksdb.org) - the Beringei library can be the high performance storage system underlying your performance monitoring solution.      ## Requirements    Beringei is tested and working on:    * Ubuntu 16.10    We also depend on these open source projects:    * [fbthrift](https://github.com/facebook/fbthrift)  * [folly](https://github.com/facebook/folly)  * [wangle](https://github.com/facebook/wangle)  * [proxygen](https://github.com/facebook/proxygen)  * [gtest](https://github.com/google/googletest)  * [gflags](https://github.com/gflags/gflags)    ## Building Beringei    Our instructions are for Ubuntu 16.10 - but you will probably be able to modify  the install scripts and directions to work with other linux distros.    - Run `sudo ./setup_ubuntu.sh`.    - Build beringei.    ```  mkdir build && cd build && cmake .. && make  ```    - Generate a beringei configuration file.    ```  ./beringei/tools/beringei_configuration_generator --host_names $(hostname) --file_path /tmp/beringei.json  ```    - Start beringei.    ```  ./beringei/service/beringei_main \      -beringei_configuration_path /tmp/beringei.json \      -create_directories \      -sleep_between_bucket_finalization_secs 60 \      -allowed_timestamp_behind 300 \      -bucket_size 600 \      -buckets $((86400/600)) \      -logtostderr \      -v=2  ```    - Send data.    ```  while [[ 1 ]]; do      ./beringei/tools/beringei_put \          -beringei_configuration_path /tmp/beringei.json \          testkey ${RANDOM} \          -logtostderr -v 3      sleep 30  done  ```    - Read the data back.    ```  ./beringei/tools/beringei_get \      -beringei_configuration_path /tmp/beringei.json \      testkey \      -logtostderr -v 3  ```    ## License    Beringei is BSD-licensed. We also provide an additional patent grant. """
Big data;https://github.com/tensorflow/tensorflow;"""<div align=""center"">    <img src=""https://www.tensorflow.org/images/tf_logo_horizontal.png"">  </div>    [![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic)](https://badge.fury.io/py/tensorflow)  [![PyPI](https://badge.fury.io/py/tensorflow.svg)](https://badge.fury.io/py/tensorflow)  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4724125.svg)](https://doi.org/10.5281/zenodo.4724125)    **`Documentation`** |  ------------------- |  [![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/) |    [TensorFlow](https://www.tensorflow.org/) is an end-to-end open source platform  for machine learning. It has a comprehensive, flexible ecosystem of  [tools](https://www.tensorflow.org/resources/tools),  [libraries](https://www.tensorflow.org/resources/libraries-extensions), and  [community](https://www.tensorflow.org/community) resources that lets  researchers push the state-of-the-art in ML and developers easily build and  deploy ML-powered applications.    TensorFlow was originally developed by researchers and engineers working on the  Google Brain team within Google's Machine Intelligence Research organization to  conduct machine learning and deep neural networks research. The system is  general enough to be applicable in a wide variety of other domains, as well.    TensorFlow provides stable [Python](https://www.tensorflow.org/api_docs/python)  and [C++](https://www.tensorflow.org/api_docs/cc) APIs, as well as  non-guaranteed backward compatible API for  [other languages](https://www.tensorflow.org/api_docs).    Keep up-to-date with release announcements and security updates by subscribing  to  [announce@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce).  See all the [mailing lists](https://www.tensorflow.org/community/forums).    ## Install    See the [TensorFlow install guide](https://www.tensorflow.org/install) for the  [pip package](https://www.tensorflow.org/install/pip), to  [enable GPU support](https://www.tensorflow.org/install/gpu), use a  [Docker container](https://www.tensorflow.org/install/docker), and  [build from source](https://www.tensorflow.org/install/source).    To install the current release, which includes support for  [CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and  Windows)*:    ```  $ pip install tensorflow  ```    A smaller CPU-only package is also available:    ```  $ pip install tensorflow-cpu  ```    To update TensorFlow to the latest version, add `--upgrade` flag to the above  commands.    *Nightly binaries are available for testing using the  [tf-nightly](https://pypi.python.org/pypi/tf-nightly) and  [tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPi.*    #### *Try your first TensorFlow program*    ```shell  $ python  ```    ```python  >>> import tensorflow as tf  >>> tf.add(1, 2).numpy()  3  >>> hello = tf.constant('Hello, TensorFlow!')  >>> hello.numpy()  b'Hello, TensorFlow!'  ```    For more examples, see the  [TensorFlow tutorials](https://www.tensorflow.org/tutorials/).    ## Contribution guidelines    **If you want to contribute to TensorFlow, be sure to review the  [contribution guidelines](CONTRIBUTING.md). This project adheres to TensorFlow's  [code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to  uphold this code.**    **We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for  tracking requests and bugs, please see  [TensorFlow Discuss](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss)  for general questions and discussion, and please direct specific questions to  [Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**    The TensorFlow project strives to abide by generally accepted best practices in  open-source software development:    [![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow)  [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)  [![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)    ## Continuous build status    You can find more community-supported platforms and configurations in the  [TensorFlow SIG Build community builds table](https://github.com/tensorflow/build#community-supported-tensorflow-builds).    ### Official Builds    Build Type                    | Status                                                                                                                                                                           | Artifacts  ----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------  **Linux CPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html)           | [PyPI](https://pypi.org/project/tf-nightly/)  **Linux GPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html) | [PyPI](https://pypi.org/project/tf-nightly-gpu/)  **Linux XLA**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html)         | TBA  **macOS**                     | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html)     | [PyPI](https://pypi.org/project/tf-nightly/)  **Windows CPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html)       | [PyPI](https://pypi.org/project/tf-nightly/)  **Windows GPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html)       | [PyPI](https://pypi.org/project/tf-nightly-gpu/)  **Android**                   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html)               | [Download](https://bintray.com/google/tensorflow/tensorflow/_latestVersion)  **Raspberry Pi 0 and 1**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html)           | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl)  **Raspberry Pi 2 and 3**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html)           | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl)  **Libtensorflow MacOS CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/macos/latest/macos_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)  **Libtensorflow Linux CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/cpu/ubuntu_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)  **Libtensorflow Linux GPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/gpu/ubuntu_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)  **Libtensorflow Windows CPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/cpu/windows_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)  **Libtensorflow Windows GPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/gpu/windows_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)    ## Resources    *   [TensorFlow.org](https://www.tensorflow.org)  *   [TensorFlow Tutorials](https://www.tensorflow.org/tutorials/)  *   [TensorFlow Official Models](https://github.com/tensorflow/models/tree/master/official)  *   [TensorFlow Examples](https://github.com/tensorflow/examples)  *   [DeepLearning.AI TensorFlow Developer Professional Certificate](https://www.coursera.org/specializations/tensorflow-in-practice)  *   [TensorFlow: Data and Deployment from Coursera](https://www.coursera.org/specializations/tensorflow-data-and-deployment)  *   [Getting Started with TensorFlow 2 from Coursera](https://www.coursera.org/learn/getting-started-with-tensor-flow2)  *   [TensorFlow: Advanced Techniques from Coursera](https://www.coursera.org/specializations/tensorflow-advanced-techniques)  *   [TensorFlow 2 for Deep Learning Specialization from Coursera](https://www.coursera.org/specializations/tensorflow2-deeplearning)  *   [Intro to TensorFlow for A.I, M.L, and D.L from Coursera](https://www.coursera.org/learn/introduction-tensorflow)  *   [Intro to TensorFlow for Deep Learning from Udacity](https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187)  *   [Introduction to TensorFlow Lite from Udacity](https://www.udacity.com/course/intro-to-tensorflow-lite--ud190)  *   [Machine Learning with TensorFlow on GCP](https://www.coursera.org/specializations/machine-learning-tensorflow-gcp)  *   [TensorFlow Codelabs](https://codelabs.developers.google.com/?cat=TensorFlow)  *   [TensorFlow Blog](https://blog.tensorflow.org)  *   [Learn ML with TensorFlow](https://www.tensorflow.org/resources/learn-ml)  *   [TensorFlow Twitter](https://twitter.com/tensorflow)  *   [TensorFlow YouTube](https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ)  *   [TensorFlow model optimization roadmap](https://www.tensorflow.org/model_optimization/guide/roadmap)  *   [TensorFlow White Papers](https://www.tensorflow.org/about/bib)  *   [TensorBoard Visualization Toolkit](https://github.com/tensorflow/tensorboard)    Learn more about the  [TensorFlow community](https://www.tensorflow.org/community) and how to  [contribute](https://www.tensorflow.org/community/contribute).    ## License    [Apache License 2.0](LICENSE) """
Big data;https://github.com/ziadoz/awesome-php;"""# Awesome PHP ![](https://github.com/ziadoz/awesome-php/workflows/Awesome%20Bot/badge.svg)    A curated list of amazingly awesome PHP libraries, resources and shiny things.    ## Contributing and Collaborating  Please see [CONTRIBUTING](https://github.com/ziadoz/awesome-php/blob/master/CONTRIBUTING.md), [CODE-OF-CONDUCT](https://github.com/ziadoz/awesome-php/blob/master/CODE-OF-CONDUCT.md) and [COLLABORATING](https://github.com/ziadoz/awesome-php/blob/master/COLLABORATING.md) for details.    ## Table of Contents  - [Awesome PHP](#awesome-php)      - [Composer Repositories](#composer-repositories)      - [Dependency Management](#dependency-management)      - [Dependency Management Extras](#dependency-management-extras)      - [Frameworks](#frameworks)      - [Framework Extras](#framework-extras)      - [Content Management Systems](#content-management-systems-cms)      - [Components](#components)      - [Micro Frameworks](#micro-frameworks)      - [Micro Framework Extras](#micro-framework-extras)      - [Routers](#routers)      - [Templating](#templating)      - [Static Site Generators](#static-site-generators)      - [HTTP](#http)      - [Scraping](#scraping)      - [Middlewares](#middlewares)      - [URL](#url)      - [Email](#email)      - [Files](#Files)      - [Streams](#streams)      - [Dependency Injection](#dependency-injection)      - [Imagery](#imagery)      - [Testing](#testing)      - [Continuous Integration](#continuous-integration)      - [Documentation](#documentation)      - [Security](#security)      - [Passwords](#passwords)      - [Code Analysis](#code-analysis)      - [Code Quality](#code-quality)      - [Static Analysis](#static-analysis)      - [Architectural](#architectural)      - [Debugging and Profiling](#debugging-and-profiling)      - [Build Tools](#build-tools)      - [Task Runners](#task-runners)      - [Navigation](#navigation)      - [Asset Management](#asset-management)      - [Geolocation](#geolocation)      - [Date and Time](#date-and-time)      - [Event](#event)      - [Logging](#logging)      - [E-commerce](#e-commerce)      - [PDF](#pdf)      - [Office](#office)      - [Database](#database)      - [Migrations](#migrations)      - [NoSQL](#nosql)      - [Queue](#queue)      - [Search](#search)      - [Command Line](#command-line)      - [Authentication and Authorization](#authentication-and-authorization)      - [Markup and CSS](#markup-and-css)      - [JSON](#json)      - [Strings](#strings)      - [Numbers](#numbers)      - [Filtering and Validation](#filtering-and-validation)      - [API](#api)      - [Caching and Locking](#caching-and-locking)      - [Data Structure and Storage](#data-structure-and-storage)      - [Notifications](#notifications)      - [Deployment](#deployment)      - [Internationalisation and Localisation](#internationalisation-and-localisation)      - [Serverless](#serverless)      - [Configuration](#configuration)      - [Third Party APIs](#third-party-apis)      - [Extensions](#extensions)      - [Miscellaneous](#miscellaneous)  - [Software](#software)      - [PHP Installation](#php-installation)      - [Development Environment](#development-environment)      - [Virtual Machines](#virtual-machines)      - [Text Editors and IDEs](#text-editors-and-ides)      - [Web Applications](#web-applications)      - [Infrastructure](#infrastructure)  - [Resources](#resources)      - [PHP Websites](#php-websites)      - [PHP Books](#php-books)      - [PHP Videos](#php-videos)      - [PHP Podcasts](#php-podcasts)      - [PHP Newsletters](#php-newsletters)      - [PHP Reading](#php-reading)      - [PHP Internals Reading](#php-internals-reading)    ### Composer Repositories  *Composer Repositories.*    * [Firegento](https://packages.firegento.com/) - Magento Module Composer Repository.  * [Packagist](https://packagist.org/) - The PHP Package Repository.  * [Private Packagist](https://packagist.com/) - Composer package archive as a service for PHP.  * [WordPress Packagist](https://wpackagist.org/) - Manage your plugins with Composer.    ### Dependency Management  *Libraries for dependency and package management.*    * [Composer Installers](https://github.com/composer/installers) - A  multi framework Composer library installer.  * [Composer](https://getcomposer.org/) - A package and dependency manager.  * [Phive](https://phar.io/) - A PHAR manager.  * [Pickle](https://github.com/FriendsOfPHP/pickle) - A PHP extension installer.    ### Dependency Management Extras  *Extras related to dependency management.*    * [Composed](https://github.com/joshdifabio/composed) - A library to parse your project's Composer environment at runtime.  * [Composer Merge Plugin](https://github.com/wikimedia/composer-merge-plugin) - A composer plugin to merge several `composer.json` files.  * [Composer Normalize](https://github.com/ergebnis/composer-normalize) - A plugin for normalising `composer.json` files.   * [Composer Patches](https://github.com/cweagans/composer-patches) - A plugin for Composer to apply patches.  * [Composer Require Checker](https://github.com/maglnet/ComposerRequireChecker) - CLI tool to analyze composer dependencies and verify that no unknown symbols are used in the sources of a package.  * [Composer Unused](https://github.com/composer-unused/composer-unused) - A CLI Tool to scan for unused composer packages.  * [Prestissimo](https://github.com/hirak/prestissimo) - A composer plugin which enables parallel install process.  * [Repman](https://repman.io) - A private PHP package repository manager and Packagist proxy.  * [Satis](https://github.com/composer/satis) - A static Composer repository generator.  * [Tooly](https://github.com/tommy-muehle/tooly-composer-script) - A library to manage PHAR files in project using Composer.  * [Toran Proxy](https://toranproxy.com) - A static Composer repository and proxy.    ### Frameworks  *Web development frameworks.*    * [CakePHP](https://cakephp.org/) - A rapid application development framework.  * [Laminas](https://getlaminas.org/) - A framework comprised of individual components (previously Zend Framework).  * [Laravel](https://laravel.com/) - A web application framework with expressive, elegant syntax.  * [Nette](https://nette.org) - A web framework comprised of mature components.  * [Phalcon](https://phalcon.io/en-us) - A framework implemented as a C extension.  * [Spiral](https://spiral.dev/) - A high performance PHP/Go framework.  * [Symfony](https://symfony.com/) - A set of reuseable components and a web framework.  * [Yii2](https://github.com/yiisoft/yii2/) - A fast, secure, and efficient web framework.    ### Framework Extras  *Extras related to web development frameworks.*    * [CakePHP CRUD](https://github.com/friendsofcake/crud) - A Rapid Application Development (RAD) plugin for CakePHP.  * [Knp RAD Components](https://rad.knplabs.com/) - A set of Rapid Application Development (RAD) components for Symfony.  * [LaravelS](https://github.com/hhxsv5/laravel-s) - Glue for using Swoole in Laravel or Lumen.  * [Symfony CMF](https://github.com/symfony-cmf/symfony-cmf) - A Content Management Framework to create custom CMS.    ### Content Management Systems (CMS)  *Tools for managing digital content.*    * [Backdrop](https://backdropcms.org) - A CMS targeting small-to-medium sized business and non-profits (a fork of Drupal).  * [Concrete5](https://www.concrete5.org/) - A CMS targeting users with a minimum of technical skills.  * [CraftCMS](https://github.com/craftcms/cms) - A flexible, user-friendly CMS for creating custom digital experiences on the web and beyond.  * [Drupal](https://www.drupal.org) - An enterprise level CMS.  * [Grav](https://github.com/getgrav/grav) - A modern flat-file CMS.  * [Joomla](https://www.joomla.org/) - Another leading CMS.  * [Kirby](https://getkirby.com/) - A flat-file CMS that adapts to any project.  * [Magento](https://magento.com/) - The most popular ecommerce platform.  * [Moodle](https://moodle.org/) - An open-source learning platform.  * [Pico CMS](http://picocms.org/) - A stupidly simple, blazing fast, flat file CMS.  * [Statamic](https://statamic.com/) - Build beautiful, easy to manage websites.  * [WordPress](https://wordpress.org/) - A blogging platform and CMS.    ### Components  *Standalone components from web development frameworks and development groups.*    * [Aura](http://auraphp.com/) - Independent components, fully decoupled from each other and from any framework.  * [CakePHP Plugins](https://plugins.cakephp.org/) - A directory of CakePHP plugins.  * [Hoa Project](https://hoa-project.net/En/) - Another package of PHP components.  * [Laravel Components](https://github.com/illuminate) - The Laravel Framework components.  * [League of Extraordinary Packages](https://thephpleague.com/) - A PHP package development group.  * [Spatie Open Source](https://spatie.be/open-source) - A collection of open source PHP and Laravel packages.  * [Symfony Components](https://symfony.com/components) - The components that make Symfony.  * [Laminas Components](https://docs.laminas.dev/components/) - The components that make the Laminas Framework.    ### Micro Frameworks  *Micro frameworks and routers.*    * [Laravel-Zero](https://laravel-zero.com) - A micro-framework for console applications.  * [Lumen](https://lumen.laravel.com) - A micro-framework by Laravel.  * [Mezzio](https://getexpressive.org/) - A micro-framework by Laminas.  * [Radar](https://github.com/radarphp/Radar.Adr) - An Action-Domain-Responder implementation for PHP.  * [Silly](https://github.com/mnapoli/silly) - A micro-framework for CLI applications.  * [Slim](https://www.slimframework.com/) - Another simple micro framework.    ### Micro Framework Extras  *Extras related to micro frameworks and routers.*    * [Slim Skeleton](https://github.com/slimphp/Slim-Skeleton) - A skeleton for Slim.  * [Slim Twig View](https://github.com/slimphp/Slim-Views) - Integrate Twig into Slim.  * [Slim PHP View](https://github.com/slimphp/PHP-View) - A simple PHP renderer for Slim.    ### Routers  *Libraries for handling application routing.*    * [Aura.Router](https://github.com/auraphp/Aura.Router) - A full-featured routing library.  * [Fast Route](https://github.com/nikic/FastRoute) - A fast routing library.  * [Klein](https://github.com/klein/klein.php) - A flexible router.  * [Pux](https://github.com/c9s/Pux) - Another fast routing library.  * [Route](https://github.com/thephpleague/route) - A routing library built on top of Fast Route.    ### Templating  *Libraries and tools for templating and lexing.*    * [MtHaml](https://github.com/arnaud-lb/MtHaml) - A PHP implementation of the HAML template language.  * [Mustache](https://github.com/bobthecow/mustache.php) - A PHP implementation of the Mustache template language.  * [PHPTAL](https://phptal.org/) - A PHP implementation of the [TAL](https://en.wikipedia.org/wiki/Template_Attribute_Language) templating language.  * [Plates](http://platesphp.com/) - A native PHP templating library.  * [Smarty](https://www.smarty.net/) - A template engine to complement PHP.  * [Twig](https://twig.symfony.com/) - A comprehensive templating language.    ### Static Site Generators  *Tools for pre-processing content to generate web pages.*    * [Couscous](http://couscous.io) - Couscous turns Markdown documentation into beautiful websites. It's GitHub Pages on steroids.  * [Jigsaw](http://jigsaw.tighten.co/) - Simple static sites with Laravel's Blade.  * [Sculpin](https://sculpin.io) - A tool that converts Markdown and Twig into static HTML.  * [Spress](http://spress.yosymfony.com) - An extensible tool that converts Markdown and Twig into HTML.    ### HTTP  *Libraries for working with HTTP.*    * [Buzz](https://github.com/kriswallsmith/Buzz) - Another HTTP client.  * [Guzzle]( https://github.com/guzzle/guzzle) - A comprehensive HTTP client.  * [HTTPlug](http://httplug.io) - An HTTP client abstraction without binding to a specific implementation.  * [Nyholm PSR-7](https://github.com/Nyholm/psr7) - A super lightweight PSR-7 implementation. Very strict and very fast.  * [PHP VCR](https://php-vcr.github.io/) - A library for recording and replaying HTTP requests.  * [Requests](https://github.com/rmccue/Requests) - A simple HTTP library.  * [Retrofit](https://github.com/tebru/retrofit-php) - A library to ease creation of REST API clients.  * [Symfony HTTP Client](https://github.com/symfony/http-client) - A component to fetch HTTP resources synchronously or asynchronously.  * [Laminas Diactoros](https://github.com/laminas/laminas-diactoros) - PSR-7 HTTP Message implementation.    ### Scraping  *Libraries for scraping websites.*    * [Chrome PHP](https://github.com/chrome-php/chrome) - Instrument headless Chrome/Chromium instances from PHP.   * [DiDOM](https://github.com/Imangazaliev/DiDOM) - A super fast HTML scrapper and parser.  * [Embed](https://github.com/oscarotero/Embed) - An information extractor from any web service or page.  * [Goutte](https://github.com/FriendsOfPHP/Goutte) - A simple web scraper.  * [Symfony Panther](https://github.com/symfony/panther) - A browser testing and web crawling library for PHP and Symfony.  * [PHP Spider](https://github.com/mvdbos/php-spider) - A configurable and extensible PHP web spider.    ### Middlewares  *Libraries for building application using middlewares.*    * [PSR-7 Middlewares](https://github.com/oscarotero/psr7-middlewares) - Inspiring collection of handy middlewares.  * [Relay](https://github.com/relayphp/Relay.Relay) - A PHP 5.5 PSR-7 middleware dispatcher.  * [Stack](https://github.com/stackphp) - A library of stackable middleware for Symfony.  * [Laminas Stratigility](https://github.com/laminas/laminas-stratigility) - Middleware for PHP built on top of PSR-7.    ### URL  *Libraries for parsing URLs.*    * [PHP Domain Parser](https://github.com/jeremykendall/php-domain-parser) - A domain suffix parser library.  * [Purl](https://github.com/jwage/purl) - A URL manipulation library.  * [sabre/uri](https://github.com/sabre-io/uri) - A functional URI manipulation library.  * [Uri](https://github.com/thephpleague/uri) - Another URL manipulation library.    ### Email  *Libraries for sending and parsing email.*    * [CssToInlineStyles](https://github.com/tijsverkoyen/CssToInlineStyles) - A library to inline CSS in email templates.  * [Email Reply Parser](https://github.com/willdurand/EmailReplyParser) - An email reply parser library.  * [Email Validator](https://github.com/nojacko/email-validator) - A small email address validation library.  * [Fetch](https://github.com/tedious/Fetch) - An IMAP library.  * [Mautic](https://github.com/mautic/mautic) - Email marketing automation  * [PHPMailer](https://github.com/PHPMailer/PHPMailer) - Another mailer solution.  * [PHP IMAP](https://github.com/barbushin/php-imap) - A library to access mailboxes via POP3, IMAP and NNTP.  * [Stampie](https://github.com/Stampie/Stampie) - A library for email services such as [SendGrid](https://sendgrid.com/), [PostMark](https://postmarkapp.com), [MailGun](https://www.mailgun.com/) and [Mandrill](https://mailchimp.com/features/transactional-email/).  * [SwiftMailer](https://swiftmailer.symfony.com) - A mailer solution.  * [Symfony Mailer](https://github.com/symfony/mailer) - A powerful library for creating and sending emails.    ### Files  *Libraries for file manipulation and MIME type detection.*    * [CSV](https://github.com/thephpleague/csv) - A CSV data manipulation library.  * [Flysystem](https://github.com/thephpleague/Flysystem) - Abstraction for local and remote filesystems.  * [Gaufrette](https://github.com/KnpLabs/Gaufrette) - A filesystem abstraction layer.  * [Hoa Mime](https://github.com/hoaproject/Mime) - Another MIME detection library.  * [PHP FFmpeg](https://github.com/PHP-FFmpeg/PHP-FFmpeg/) - A wrapper for the [FFmpeg](https://www.ffmpeg.org/) video library.  * [UnifiedArchive](https://github.com/wapmorgan/UnifiedArchive) - A unified reader and writer of compressed archives.    ### Streams  *Libraries for working with streams.*    * [ByteStream](https://amphp.org/byte-stream/) - An asynchronous stream abstraction.  * [Streamer](https://github.com/fzaninotto/Streamer) - A simple object-orientated stream wrapper library.    ### Dependency Injection  *Libraries that implement the dependency injection design pattern.*    * [Aura.Di](https://github.com/auraphp/Aura.Di) - A serializable dependency injection container with constructor and setter injection, interface and trait awareness, configuration inheritance, and much more.  * [Acclimate](https://github.com/AcclimateContainer/acclimate-container) - A common interface to dependency injection containers and service locators.  * [Auryn](https://github.com/rdlowrey/Auryn) - A recursive dependency injector.  * [Container](https://github.com/thephpleague/container) - Another flexible dependency injection container.  * [Disco](https://github.com/bitExpert/disco) - A PSR-11 compatible, annotation-based dependency injection container.  * [PHP-DI](https://php-di.org/) - A dependency injection container that supports autowiring.  * [Pimple](https://pimple.symfony.com/) - A tiny dependency injection container.  * [Symfony DI](https://github.com/symfony/dependency-injection) - A dependency injection container component.    ### Imagery  *Libraries for manipulating images.*    * [Color Extractor](https://github.com/thephpleague/color-extractor) - A library for extracting colours from images.  * [Glide](https://github.com/thephpleague/glide) - An on-demand image manipulation library.  * [Image Hash](https://github.com/jenssegers/imagehash) - A library for generating perceptual image hashes.  * [Image Optimizer](https://github.com/psliwa/image-optimizer) - A library for optimizing images.  * [Imagine](https://imagine.readthedocs.io/en/latest/index.html) - An image manipulation library.  * [Intervention Image](https://github.com/Intervention/image) - Another image manipulation library.  * [PHP Image Workshop](https://github.com/Sybio/ImageWorkshop) - Another image manipulation library.    ### Testing  *Libraries for testing codebases and generating test data.*    * [Alice](https://github.com/nelmio/alice) - An expressive fixture generation library.  * [AspectMock](https://github.com/Codeception/AspectMock) - A mocking framework for PHPUnit/Codeception.  * [Atoum](https://github.com/atoum/atoum) - A simple testing library.  * [Behat](https://docs.behat.org/en/latest/) - A behaviour driven development (BDD) testing framework.  * [Codeception](https://github.com/Codeception/Codeception) - A full stack testing framework.  * [Faker](https://github.com/fakerphp/faker) - A fake data generator library.  * [HTTP Mock](https://github.com/InterNations/http-mock) - A library for mocking HTTP requests in unit tests.  * [Infection](https://github.com/infection/infection) - An AST-based PHP Mutation testing framework.  * [Kahlan](https://github.com/kahlan/kahlan) - Full stack Unit/BDD testing framework with built-in stub, mock and code-coverage support.  * [Mink](http://mink.behat.org/en/latest/) - Web acceptance testing.  * [Mockery](https://github.com/mockery/mockery) - A mock object library for testing.  * [ParaTest](https://github.com/paratestphp/paratest) - A parallel testing library for PHPUnit.  * [Pest](https://pestphp.com/) - A testing framework with a focus on simplicity.  * [Peridot](https://github.com/peridot-php/peridot) - An event driven test framework.  * [Phake](https://github.com/mlively/Phake) - Another mock object library for testing.  * [Pho](https://github.com/danielstjules/pho) - Another behaviour driven development testing framework.  * [PHP-Mock](https://github.com/php-mock/php-mock) - A mock library for built-in PHP functions (e.g. time()).  * [PHP MySQL Engine](https://github.com/vimeo/php-mysql-engine) -  A MySQL engine written in pure PHP.   * [PHPSpec](https://github.com/phpspec/phpspec) - A design by specification unit testing library.  * [PHPT](https://qa.php.net/write-test.php) - A test tool used by PHP itself.  * [PHPUnit](https://github.com/sebastianbergmann/phpunit) - A unit testing framework.  * [Prophecy](https://github.com/phpspec/prophecy) - A highly opinionated mocking framework.  * [VFS Stream](https://github.com/bovigo/vfsStream) - A virtual filesystem stream wrapper for testing.    ### Continuous Integration  *Libraries and applications for continuous integration.*    * [CircleCI](https://circleci.com) - A continuous integration platform.  * [GitlabCi](https://about.gitlab.com/stages-devops-lifecycle/continuous-integration/) - Let GitLab CI test, build, deploy your code. TravisCi like.  * [Jenkins](https://www.jenkins.io/) - A continuous integration platform with [PHP support](https://www.jenkins.io/solutions/php/).  * [JoliCi](https://github.com/jolicode/JoliCi) - A continuous integration client written in PHP and powered by Docker.  * [PHPCI](https://github.com/dancryer/phpci) - An open source continuous integration platform for PHP.  * [SemaphoreCI](https://semaphoreci.com/) - A continuous integration platform for open source and private projects.  * [Shippable](https://www.shippable.com/) - A Docker based continious integration platform for open source and private projects.  * [Travis CI](https://travis-ci.org/) - A continuous integration platform.  * [Setup PHP](https://github.com/shivammathur/setup-php) - A GitHub Action for PHP.    ### Documentation  *Libraries for generating project documentation.*    * [APIGen](https://github.com/apigen/apigen) - Another API documentation generator.  * [daux.io](https://github.com/dauxio/daux.io) - A documentation generator which uses Markdown files.  * [PHP Documentor 2](https://github.com/phpDocumentor/phpDocumentor) - A documentation generator.  * [phpDox](http://phpdox.de/) - A documentation generator for PHP projects (that is not limited to API documentation).    ### Security  *Libraries for generating secure random numbers, encrypting data and scanning and testing for vulnerabilities.*    * [Halite](https://paragonie.com/project/halite) - A simple library for encryption using [libsodium](https://github.com/jedisct1/libsodium).  * [HTML Purifier](https://github.com/ezyang/htmlpurifier) - A standards compliant HTML filter.  * [IniScan](https://github.com/psecio/iniscan) - A tool that scans PHP INI files for security.  * [Optimus](https://github.com/jenssegers/optimus) - Id obfuscation based on Knuth's multiplicative hashing method.  * [PHPGGC](https://github.com/ambionics/phpggc) - A library of PHP unserializeable payloads along with a tool to generate them.  * [PHP Encryption](https://github.com/defuse/php-encryption) - Secure PHP Encryption Library.  * [PHP SSH](https://github.com/Herzult/php-ssh) - An experimental object orientated SSH wrapper library.  * [PHPSecLib](http://phpseclib.sourceforge.net/) - A pure PHP secure communications library.  * [random_compat](https://github.com/paragonie/random_compat) - PHP 5.x support for `random_bytes()` and `random_int()`  * [RandomLib](https://github.com/ircmaxell/RandomLib) - A library for generating random numbers and strings.  * [Symfony Security Monitoring](https://security.symfony.com/) - A web tool to check your Composer dependencies for security advisories, previously known as ""SensioLabs Security Check"".  * [SQLMap](https://github.com/sqlmapproject/sqlmap) - An automatic SQL injection and database takeover tool.   * [TCrypto](https://github.com/timoh6/TCrypto) - A simple encrypted key-value storage library.  * [VAddy](https://vaddy.net/) - A continuous security testing platform for web applications.  * [Zap](https://owasp.org/www-project-zap/) - An integrated penetration testing tool for web applications.    ### Passwords  *Libraries and tools for working with and storing passwords.*    * [GenPhrase](https://github.com/timoh6/GenPhrase) - A library for generating secure random passphrases.  * [Password Compat](https://github.com/ircmaxell/password_compat) - A compatibility library for the new PHP 5.5 password functions.  * [Password Policy](https://github.com/ircmaxell/password-policy) - A password policy library for PHP and JavaScript.  * [Password Validator](https://github.com/jeremykendall/password-validator) - A library for validating and upgrading password hashes.  * [Password-Generator](https://github.com/hackzilla/password-generator) - PHP library to generate random passwords.  * [PHP Password Lib](https://github.com/ircmaxell/PHP-PasswordLib) - A library for generating and validating passwords.  * [phpass](https://www.openwall.com/phpass/) - A portable password hashing framework.  * [Zxcvbn PHP](https://github.com/bjeavons/zxcvbn-php) - A realistic PHP password strength estimate library based on Zxcvbn JS.    ### Code Analysis  *Libraries and tools for analysing, parsing and manipulating codebases.*    * [Better Reflection](https://github.com/Roave/BetterReflection) - AST-based reflection library that allows analysis and manipulation of code  * [Code Climate](https://codeclimate.com) - An automated code review.  * [GrumPHP](https://github.com/phpro/grumphp) - A PHP code-quality tool.  * [PHP Parser](https://github.com/nikic/PHP-Parser) - A PHP parser written in PHP.  * [PHP Semantic Versioning Checker](https://github.com/tomzx/php-semver-checker) - A command line utility that compares two source sets and determines the appropriate semantic versioning to apply.  * [Phpactor](https://github.com/phpactor/phpactor) - PHP completion, refactoring and introspection tool.  * [PHPLOC](https://github.com/sebastianbergmann/phploc) - A tool for quickly measuring the size of a PHP project.  * [PHPQA](https://github.com/EdgedesignCZ/phpqa) - A tool for running QA tools (phploc, phpcpd, phpcs, pdepend, phpmd, phpmetrics).  * [Qafoo Quality Analyzer](https://github.com/Qafoo/QualityAnalyzer) - A tool to visualize metrics and source code.  * [Rector](https://github.com/rectorphp/rector) - A tool to upgrade and refactor code.  * [Scrutinizer](https://scrutinizer-ci.com/) - A web tool to [scrutinise PHP code](https://github.com/scrutinizer-ci/php-analyzer).  * [UBench](https://github.com/devster/ubench) - A simple micro benchmark library.    ### Code Quality  *Libraries for managing code quality, formatting and linting.*    * [CaptainHook](https://github.com/captainhookphp/captainhook) - An easy-to-use and flexible Git hook library.   * [PHP CodeSniffer](https://github.com/squizlabs/PHP_CodeSniffer) - A library that detects PHP, CSS and JS coding standard violations.  * [PHP CS Fixer](https://github.com/FriendsOfPHP/PHP-CS-Fixer) - A coding standards fixer library.  * [PHP Mess Detector](https://github.com/phpmd/phpmd) - A library that scans code for bugs, sub-optimal code, unused parameters and more.  * [PHPCheckstyle](https://github.com/PHPCheckstyle/phpcheckstyle) - A tool to help adhere to certain coding conventions.  * [PHPCPD](https://github.com/sebastianbergmann/phpcpd) - A library that detects copied and pasted code.    ### Static Analysis  *Libraries for performing static analysis of PHP code.*    * [Exakat](https://github.com/exakat/exakat) - A static analysis engine for PHP.  * [Deptrac](https://github.com/sensiolabs-de/deptrac) - A static code analysis tool that helps to enforce rules for dependencies between software layers.  * [Mondrian](https://github.com/Trismegiste/Mondrian) - A code analysis tool using Graph Theory.  * [phan](https://github.com/phan/phan) - A static analyzer based on PHP 7+ and the php-ast extension.  * [PHP Architecture Tester](https://github.com/carlosas/phpat) - Easy to use architecture testing tool for PHP.  * [PHPCompatibility](https://github.com/PHPCompatibility/PHPCompatibility) - A PHP compatibility checker for PHP CodeSniffer.  * [PhpDependencyAnalysis](https://github.com/mamuz/PhpDependencyAnalysis) - A tool to create customisable dependency graphs.  * [PHP Metrics](https://github.com/phpmetrics/PhpMetrics) - A static metric library.  * [PHP Migration](https://github.com/monque/PHP-Migration) - A static analyzer for PHP version migration.  * [PHPStan](https://github.com/phpstan/phpstan) - A PHP Static Analysis Tool.  * [Psalm](https://github.com/vimeo/psalm) - A static analysis tool for finding errors in PHP applications.    ### Architectural  *Libraries related to design patterns, programming approaches and ways to organize code.*    * [Design Patterns PHP](https://github.com/domnikl/DesignPatternsPHP) - A repository of software patterns implemented in PHP.  * [Finite](https://yohan.giarel.li/Finite/) - A simple PHP finite state machine.  * [Functional PHP](https://github.com/lstrojny/functional-php) - A functional programming library.  * [Iter](https://github.com/nikic/iter) - A library that provides iteration primitives using generators.  * [Patchwork](http://patchwork2.org/) - A library for redefining userland functions.  * [Pipeline](https://github.com/thephpleague/pipeline) - A pipeline pattern implementation.  * [Porter](https://github.com/ScriptFUSION/Porter) - Data import abstraction library for consuming Web APIs and other data sources.  * [Ruler](https://github.com/bobthecow/Ruler) - A simple stateless production rules engine.  * [RulerZ](https://github.com/K-Phoen/rulerz) - A powerful rule engine and implementation of the Specification pattern.    ### Debugging and Profiling  *Libraries and tools for debugging errors and profiling code.*    * [APM](https://pecl.php.net/package/APM) - Monitoring extension collecting errors and statistics into SQLite/MySQL/StatsD.  * [Barbushin PHP Console](https://github.com/barbushin/php-console) - Another web debugging console using Google Chrome.  * [Blackfire.io](https://blackfire.io) - A low-overhead code profiler.  * [Kint](https://github.com/kint-php/kint) - A debugging and profiling tool.  * [Metrics](https://github.com/beberlei/metrics) - A simple metrics API library.  * [PCOV](https://github.com/krakjoe/pcov) - A self contained code coverage compatible driver.  * [PHP Console](https://github.com/Seldaek/php-console) - A web debugging console.  * [PHP Debug Bar](http://phpdebugbar.com/) - A debugging toolbar.  * [PHPBench](https://github.com/phpbench/phpbench) - A benchmarking Framework.  * [PHPSpy](https://github.com/adsr/phpspy) - A low-overhead sampling profiler.  * [Symfony VarDumper](https://github.com/symfony/var-dumper) - A variable dumper component.  * [Tideways.io](https://tideways.com/) - Monitoring and profiling tool.  * [Tracy](https://github.com/nette/tracy) - A simple error detection, logging and time measuring library.  * [Whoops](https://github.com/filp/whoops) - A pretty error handling library.  * [xDebug](https://github.com/xdebug/xdebug) - A debug and profile tool for PHP.  * [XHProf](https://github.com/phacility/xhprof) - A profiling tool originally developed by Facebook.  * [Z-Ray](https://www.zend.com/products/z-ray) - A debug and profile tool for Zend Server.    ### Build Tools  *Project build and automation tools.*    * [Box](https://github.com/box-project/box) - A utility to build PHAR files.  * [Construct](https://github.com/jonathantorres/construct) - A PHP project/micro-package generator.  * [Phing](https://www.phing.info/) - A PHP project build system inspired by Apache Ant.  * [RMT](https://github.com/liip/RMT) - A library for versioning and releasing software.    ### Task Runners  *Libraries for automating and running tasks.*    * [Bldr](https://bldr.io/) - A PHP Task runner built on Symfony components.  * [Jobby](https://github.com/jobbyphp/jobby) - A PHP cron job manager without modifying crontab.  * [Robo](https://github.com/consolidation/Robo) - A PHP Task runner with object-orientated configurations.  * [Task](https://taskphp.github.io/) - A pure PHP task runner inspired by Grunt and Gulp.    ### Navigation  *Tools for building navigation structures.*    * [KnpMenu](https://github.com/KnpLabs/KnpMenu) - A menu library.  * [Menu](https://github.com/spatie/menu) - A flexible menu library with a fluent interface.    ### Asset Management  *Tools for managing, compressing and minifying website assets.*    * [JShrink](https://github.com/tedious/JShrink) - A JavaScript minifier library.  * [Laravel Mix](https://github.com/JeffreyWay/laravel-mix) - An elegant wrapper around Webpack for the 80% use case.  * [Symfony Asset](https://github.com/symfony/asset) - Manages URL generation and versioning of web assets.  * [Symfony Encore](https://github.com/symfony/webpack-encore) - A simple but powerful API for processing and compiling assets built around Webpack.    ### Geolocation  *Libraries for geocoding addresses and working with latitudes and longitudes.*    * [Country List](https://github.com/umpirsky/country-list) - A list of all countries with names and ISO 3166-1 codes.  * [GeoCoder](https://geocoder-php.org/) - A geocoding library.  * [GeoJSON](https://github.com/jmikola/geojson) - A GeoJSON implementation.  * [GeoTools](https://github.com/thephpleague/geotools) - A library of geo-related tools.  * [PHPGeo](https://github.com/mjaschen/phpgeo) - A simple geo library.    ### Date and Time  *Libraries for working with dates and times.*    * [CalendR](https://yohan.giarel.li/CalendR/) - A calendar management library.  * [Carbon](https://github.com/briannesbitt/Carbon) - A simple DateTime API extension.  * [Chronos](https://github.com/cakephp/chronos) - A DateTime API extension supporting both mutable and immutable date/time.  * [Moment.php](https://github.com/fightbulc/moment.php) - Moment.js inspired PHP DateTime handler with i18n support.  * [Yasumi](https://github.com/azuyalabs/yasumi) - An library to help you calculate the dates and names of holidays.    ### Event  *Libraries that are event-driven or implement non-blocking event loops.*  * [Amp](https://github.com/amphp/amp) - An event driven non-blocking I/O library.  * [Broadway](https://github.com/broadway/broadway) - An event source and CQRS library.  * [CakePHP Event](https://github.com/cakephp/event) - An event dispatcher library.  * [Elephant.io](https://github.com/Wisembly/Elephant.io) - Yet another web socket library.  * [Evenement](https://github.com/igorw/evenement) - An event dispatcher library.  * [Event](https://github.com/thephpleague/event) - An event library with a focus on domain events.  * [Hoa EventSource](https://github.com/hoaproject/Eventsource) - An event source library.  * [Hoa WebSocket](https://github.com/hoaproject/Websocket) - Another web socket library.  * [Pawl](https://github.com/ratchetphp/Pawl) - An asynchronous web socket client.  * [Prooph Event Store](https://github.com/prooph/event-store) - An event source component to persist event messages  * [PHP Defer](https://github.com/php-defer/php-defer) - Golang's defer statement for PHP.  * [Ratchet](https://github.com/ratchetphp/Ratchet) - A web socket library.  * [ReactPHP](https://github.com/reactphp/reactphp) - An event driven non-blocking I/O library.  * [RxPHP](https://github.com/ReactiveX/RxPHP) - A reactive extension library.  * [Swoole](https://github.com/swoole/swoole-src) - An event-driven asynchronous and concurrent networking communication framework with high performance for PHP written in C.  * [Workerman](https://github.com/walkor/Workerman) - An event driven non-blocking I/O library.    ### Logging  *Libraries for generating and working with log files.*    * [Monolog](https://github.com/Seldaek/monolog) - A comprehensive logger.    ### E-commerce  *Libraries and applications for taking payments and building online e-commerce stores.*    * [Money](https://github.com/moneyphp/money) - A PHP implementation of Fowler's money pattern.  * [Brick\Money](https://github.com/brick/money) - A money library for PHP, with support for contexts, cash roundings, currency conversion.  * [OmniPay](https://github.com/thephpleague/omnipay) - A framework agnostic multi-gateway payment processing library.  * [Payum](https://github.com/payum/payum) - A payment abstraction library.  * [Shopware](https://github.com/shopware/shopware) - Highly customizable e-commerce software  * [Swap](https://github.com/florianv/swap) - An exchange rates library.  * [Sylius](https://sylius.com/) - An open source e-commerce solution.    ### PDF  *Libraries and software for working with PDF files.*    * [Dompdf](https://github.com/dompdf/dompdf) - A HTML to PDF converter.  * [PHPPdf](https://github.com/psliwa/PHPPdf) - A library for generating PDFs and images from XML.  * [Snappy](https://github.com/KnpLabs/snappy) - A PDF and image generation library.  * [WKHTMLToPDF](https://github.com/wkhtmltopdf/wkhtmltopdf) - A tool to convert HTML to PDF.    ### Office  *Libraries for working with office suite documents.*    * [PHPPowerPoint](https://github.com/PHPOffice/PHPPresentation) - A library for working with Microsoft PowerPoint Presentations.  * [PHPWord](https://github.com/PHPOffice/PHPWord) - A library for working with Microsoft Word documents.  * [PHPSpreadsheet](https://github.com/PHPOffice/PhpSpreadsheet) - A pure PHP library for reading and writing spreadsheet files (successor of PHPExcel).  * [Spout](https://github.com/box/spout) - Read and write spreadsheet files (CSV, XLSX and ODS), in a fast and scalable way .    ### Database  *Libraries for interacting with databases using object-relational mapping (ORM) or datamapping techniques.*    * [Atlas.Orm](https://github.com/atlasphp/Atlas.Orm) - A data mapper implementation for your persistence model in PHP.  * [Aura.Sql](https://github.com/auraphp/Aura.Sql) - Provides an extension to the native PDO along with a profiler and connection locator.  * [Aura.SqlQuery](https://github.com/auraphp/Aura.SqlQuery) - Independent query builders for MySQL, PostgreSQL, SQLite, and Microsoft SQL Server.  * [Baum](https://github.com/etrepat/baum) - A nested set implementation for Eloquent.  * [CakePHP ORM](https://github.com/cakephp/orm) - Object-Relational Mapper, implemented using the DataMapper pattern.  * [Cycle ORM](https://github.com/cycle/orm) - PHP DataMapper, ORM.  * [Doctrine Extensions](https://github.com/Atlantic18/DoctrineExtensions) - A collection of Doctrine behavioural extensions.  * [Doctrine](https://www.doctrine-project.org/) - A comprehensive DBAL and ORM.  * [Laravel Eloquent](https://github.com/illuminate/database) - A simple ORM.  * [Pomm](https://github.com/chanmix51/Pomm) - An Object Model Manager for PostgreSQL.  * [ProxyManager](https://github.com/Ocramius/ProxyManager) - A set of utilities to generate proxy objects for data mappers.  * [RedBean](https://redbeanphp.com/index.php) - A lightweight, configuration-less ORM.  * [Slimdump](https://github.com/webfactory/slimdump) - An easy dumper tool for MySQL.  * [Spot2](https://github.com/spotorm/spot2) - A MySQL datamapper ORM.    ### Migrations  Libraries to help manage database schemas and migrations.    * [Doctrine Migrations](https://www.doctrine-project.org/projects/migrations.html) - A migration library for Doctrine.  * [Migrations](https://github.com/icomefromthenet/Migrations) - A migration management library.  * [Phinx](https://github.com/cakephp/phinx) - Another database migration library.  * [PHPMig](https://github.com/davedevelopment/phpmig) - Another migration management library.  * [Ruckusing](https://github.com/ruckus/ruckusing-migrations) - Database migrations for PHP ala ActiveRecord Migrations with support for MySQL, Postgres, SQLite.    ### NoSQL  *Libraries for working with ""NoSQL"" backends.*    * [PHPMongo](https://github.com/sokil/php-mongo) - A MongoDB ORM.  * [Predis](https://github.com/predis/predis) - A feature complete Redis library.    ### Queue  *Libraries for working with event and task queues.*    * [Bernard](https://github.com/bernardphp/bernard) - A multibackend abstraction library.  * [BunnyPHP](https://github.com/jakubkulhan/bunny) - A performant pure-PHP AMQP (RabbitMQ) sync and also async (ReactPHP) library.  * [Pheanstalk](https://github.com/pheanstalk/pheanstalk) - A Beanstalkd client library.  * [PHP AMQP](https://github.com/php-amqplib/php-amqplib) - A pure PHP AMQP library.  * [Tarantool Queue](https://github.com/tarantool-php/queue) - PHP bindings for Tarantool Queue.  * [Thumper](https://github.com/php-amqplib/Thumper) - A RabbitMQ pattern library.  * [Enqueue](https://github.com/php-enqueue/enqueue-dev) - A message queue packages for PHP that supports RabbitMQ, AMQP, STOMP, Amazon SQS, Redis and Doctrine transports.     ### Search  *Libraries and software for indexing and performing search queries on data.*    * [Elastica](https://github.com/ruflin/Elastica) - A client library for ElasticSearch.  * [ElasticSearch PHP](https://github.com/elastic/elasticsearch-php) - The official client library for [ElasticSearch](https://www.elastic.co/).  * [Solarium](https://www.solarium-project.org/) - A client library for [Solr](https://lucene.apache.org/solr/).  * [SphinxQL Query Builder](https://foolcode.github.io/SphinxQL-Query-Builder/) - A query library for the [Sphinx](https://sphinxsearch.com/) and [Manticore](https://manticoresearch.com/) search engines.    ### Command Line  *Libraries related to the command line.*    * [Aura.Cli](https://github.com/auraphp/Aura.Cli) - Provides the equivalent of request ( Context ) and response ( Stdio ) objects for the command line interface, including Getopt support, and an independent Help object for describing commands.  * [Boris](https://github.com/borisrepl/boris) - A tiny PHP REPL.  * [Cilex](https://github.com/Cilex/Cilex) - A micro framework for building command line tools.  * [CLI Menu](https://github.com/php-school/cli-menu) - A library for building CLI menus.  * [CLIFramework](https://github.com/c9s/CLIFramework) - A command-line framework supports zsh/bash completion generation, subcommands and option constraints. It also powers phpbrew.  * [CLImate](https://github.com/thephpleague/climate) - A library for outputting colours and special formatting.  * [Commando](https://github.com/nategood/commando) - Another simple command line opt parser.  * [Cron Expression](https://github.com/mtdowling/cron-expression) - A library to calculate cron run dates.  * [GetOpt](https://github.com/getopt-php/getopt-php) - A command line opt parser.  * [GetOptionKit](https://github.com/c9s/GetOptionKit) - Another command line opt parser.  * [Hoa Console](https://github.com/hoaproject/Console) - Another command line library.  * [PsySH](https://github.com/bobthecow/psysh) - Another PHP REPL.  * [ShellWrap](https://github.com/MrRio/shellwrap) - A simple command line wrapper library.    ### Authentication and Authorization  *Libraries for implementing user authentication and authorization.*    * [Aura.Auth](https://github.com/auraphp/Aura.Auth) - Provides authentication functionality and session tracking using various adapters.  * [SocialConnect Auth](https://github.com/socialConnect/auth) - An open source social sign (OAuth1\OAuth2\OpenID\OpenIDConnect).  * [Json Web Token](https://github.com/lcobucci/jwt) - Json Tokens to authenticate and transmit information.  * [OAuth 1.0 Client](https://github.com/thephpleague/oauth1-client) - An OAuth 1.0 client library.  * [OAuth 2.0 Client](https://github.com/thephpleague/oauth2-client) - An OAuth 2.0 client library.  * [OAuth2 Server](https://bshaffer.github.io/oauth2-server-php-docs/) - Another OAuth2 server implementation.  * [OAuth2 Server](https://oauth2.thephpleague.com/) - An OAuth2 authentication server, resource server and client library.  * [Opauth](https://github.com/opauth/opauth) - A multi-provider authentication framework.  * [Paseto](https://github.com/paragonie/paseto) - Platform-Agnostic Security Tokens.  * [PHP oAuthLib](https://github.com/Lusitanian/PHPoAuthLib) - Another OAuth library.  * [Sentinel Social](https://cartalyst.com/manual/sentinel-social/2.0) - A library for social network authentication.  * [Sentinel](https://cartalyst.com/manual/sentinel/2.0) - A framework agnostic authentication & authorisation library.  * [TwitterOAuth](https://github.com/abraham/twitteroauth) - A Twitter OAuth library.    ### Markup and CSS  *Libraries for working with markup and CSS formats.    * [Cebe Markdown](https://github.com/cebe/markdown) - An fast and extensible Markdown parser.  * [CommonMark PHP](https://github.com/thephpleague/commonmark) - Highly-extensible Markdown parser which fully supports the [CommonMark spec](https://spec.commonmark.org/).  * [Decoda](https://github.com/milesj/decoda) - A lightweight markup parser library.  * [Essence](https://github.com/essence/essence) - A library for extracting web media.  * [Embera](https://github.com/mpratt/Embera) - An Oembed consumer library.  * [HTML to Markdown](https://github.com/thephpleague/html-to-markdown) - Converts HTML into Markdown.  * [HTML5 PHP](https://github.com/Masterminds/html5-php) - An HTML5 parser and serializer library.  * [Parsedown](https://github.com/erusev/parsedown) - Another Markdown parser.  * [PHP CSS Parser](https://github.com/sabberworm/PHP-CSS-Parser) - A Parser for CSS Files written in PHP.  * [PHP Markdown](https://github.com/michelf/php-markdown) - A Markdown parser.  * [Shiki PHP](https://github.com/spatie/shiki-php) - A [Shiki](https://github.com/shikijs/shiki) code highlighting package in PHP.  * [VObject](https://github.com/sabre-io/vobject) - A library for parsing VCard and iCalendar objects.    ### JSON  *Libraries for working with JSON.*    * [JSON Lint](https://github.com/Seldaek/jsonlint) - A JSON lint utility.  * [JSONMapper](https://github.com/JsonMapper/JsonMapper) - A library for mapping JSON to PHP objects.    ### Strings  *Libraries for parsing and manipulating strings.*    * [Agent](https://github.com/jenssegers/agent) - A PHP desktop/mobile user agent parser, based on Mobiledetect.  * [ANSI to HTML5](https://github.com/sensiolabs/ansi-to-html) - An ANSI to HTML5 converter library.  * [Color Jizz](https://github.com/mikeemoo/ColorJizz-PHP) - A library for manipulating and converting colours.  * [Device Detector](https://github.com/matomo-org/device-detector) - Another library for parsing user agent strings.  * [Hoa String](https://github.com/hoaproject/Ustring) - Another UTF-8 string library.  * [Jieba-PHP](https://github.com/fukuball/jieba-php) - A PHP port of Python's jieba. Chinese text segmentation for natural language processing.  * [Mobile-Detect](https://github.com/serbanghita/Mobile-Detect) - A lightweight PHP class for detecting mobile devices (including tablets).  * [Patchwork UTF-8](https://github.com/nicolas-grekas/Patchwork-UTF8) - A portable library for working with UTF-8 strings.  * [Portable UTF-8](https://github.com/voku/portable-utf8) - A string manipulation library with UTF-8 safe replacement methods.  * [Slugify](https://github.com/cocur/slugify) - A library to convert strings to slugs.  * [SQL Formatter](https://github.com/jdorn/sql-formatter/) - A library for formatting SQL statements.  * [Stringy](https://github.com/voku/Stringy) - A string manipulation library with multibyte support.  * [UA Parser](https://github.com/tobie/ua-parser/tree/master/php) - A library for parsing user agent strings.  * [URLify](https://github.com/jbroadway/urlify) - A PHP port of Django's URLify.js.  * [UUID](https://github.com/ramsey/uuid) - A library for generating UUIDs.    ### Numbers  *Libraries for working with numbers.*    * [Brick\Math](https://github.com/brick/math) - A library providing large number support: `BigInteger`, `BigDecimal` and `BigRational`.  * [ByteUnits](https://github.com/gabrielelana/byte-units) - A library to parse, format and convert byte units in binary and metric systems.  * [DecimalObject](https://github.com/spryker/decimal-object) - A value object to handle decimals/floats easily and more precisely.  * [IP](https://github.com/darsyn/ip) - An immutable value object for working with IPv4 and IPv6 addresses.  * [LibPhoneNumber for PHP](https://github.com/giggsey/libphonenumber-for-php) - A PHP implementation of Google's phone number handling library.  * [PHP Conversion](https://github.com/Crisu83/php-conversion) - Another library for converting between units of measure.  * [PHP Units of Measure](https://github.com/triplepoint/php-units-of-measure) - A library for converting between units of measure.  * [MathPHP](https://github.com/markrogoyski/math-php) - A math library for PHP.     ### Filtering and Validation  *Libraries for filtering and validating data.*    * [Assert](https://github.com/beberlei/assert) - A validation library with a rich set of assertions. Supports assertion chaining and lazy assertions.  * [Aura.Filter](https://github.com/auraphp/Aura.Filter) - Provides tools to validate and sanitize objects and arrays.  * [CakePHP Validation](https://github.com/cakephp/validation) - Another validation library.  * [Filterus](https://github.com/ircmaxell/filterus) - A simple PHP filtering library.  * [ISO-codes](https://github.com/ronanguilloux/IsoCodes) - A library for validating inputs according standards from ISO, International Finance, Public Administrations, GS1, Book Industry, Phone numbers & Zipcodes for many countries.  * [JSON Schema](https://github.com/justinrainbow/json-schema) - A [JSON Schema](https://json-schema.org/) validation library.  * [MetaYaml](https://github.com/romaricdrigon/MetaYaml) - A schema validation library that supports YAML, JSON and XML.  * [Respect Validation](https://github.com/Respect/Validation) - A simple validation library.  * [Upload](https://github.com/brandonsavage/Upload) - A library for handling file uploads and validation.  * [Valitron](https://github.com/vlucas/valitron) - Another validation library.  * [Volan](https://github.com/serkin/Volan) - Another simplified validation library.    ### API  *Libraries and web tools for developing APIs.*    * [API Platform](https://api-platform.com ) - Expose in minutes an hypermedia REST API that embraces JSON-LD, Hydra format.  * [Laminas API Tool Skeleton](https://github.com/laminas-api-tools/api-tools-skeleton) - An API builder built with the Laminas Framework.  * [Drest](https://github.com/leedavis81/drest) - A library for exposing Doctrine entities as REST resource endpoints.  * [HAL](https://github.com/blongden/hal) - A Hypertext Application Language (HAL) builder library.  * [Hateoas](https://github.com/willdurand/Hateoas) - A HATEOAS REST web service library.  * [Negotiation](https://github.com/willdurand/Negotiation) - A content negotiation library.  * [Restler](https://github.com/Luracast/Restler) - A lightweight framework to expose PHP methods as RESTful web API.  * [wsdl2phpgenerator](https://github.com/wsdl2phpgenerator/wsdl2phpgenerator) - A tool to generate PHP classes from SOAP WSDL files.    ### Caching and Locking  *Libraries for caching data and acquiring locks.*    * [APIx Cache](https://github.com/apix/cache) - A thin PSR-6 cache wrapper to various caching backends emphasising cache tagging and indexing.  * [CacheTool](https://github.com/gordalina/cachetool) - A tool to clear APC/opcode caches from the command line.  * [CakePHP Cache](https://github.com/cakephp/cache) - A caching library.  * [Doctrine Cache](https://github.com/doctrine/cache) - A caching library.  * [Metaphore](https://github.com/sobstel/metaphore) - Cache slam defense using a semaphore to prevent dogpile effect.  * [Stash](https://github.com/tedious/Stash) - Another library for caching.  * [Laminas Cache](https://github.com/laminas/laminas-cache) - Another caching library.  * [Lock](https://github.com/php-lock/lock) - A lock library to provide exclusive execution.    ### Data Structure and Storage  *Libraries that implement data structure or storage techniques.*    * [CakePHP Collection](https://github.com/cakephp/collection) - A simple collections library.  * [Fractal](https://github.com/thephpleague/fractal) - A library for converting complex data structures to JSON output.  * [Ginq](https://github.com/akanehara/ginq) - Another PHP library based on .NET's LINQ.  * [JsonMapper](https://github.com/cweiske/jsonmapper) - A library that maps nested JSON structures onto PHP classes.  * [JSON Machine](https://github.com/halaxa/json-machine) - Provides iteration over huge JSONs using simple `foreach`  * [Knapsack](https://github.com/DusanKasan/Knapsack) - Collection library inspired by Clojure's sequences.  * [msgpack.php](https://github.com/rybakit/msgpack.php) - A pure PHP implementation of the [MessagePack](https://msgpack.org/) serialization format.  * [PINQ](https://github.com/TimeToogo/Pinq) - A PHP library based on .NET's LINQ (Language Integrated Query).  * [Serializer](https://github.com/schmittjoh/serializer) - A library for serialising and de-serialising data.  * [YaLinqo](https://github.com/Athari/YaLinqo) - Yet Another LINQ to Objects for PHP.  * [Laminas Serializer](https://github.com/laminas/laminas-serializer) - Another library for serialising and de-serialising data.    ### Notifications  *Libraries for working with notification software.*    * [JoliNotif](https://github.com/jolicode/JoliNotif) - A cross-platform library for desktop notification (support for Growl, notify-send, toaster, etc)  * [Notification Pusher](https://github.com/Ph3nol/NotificationPusher) - A standalone library for device push notifications.  * [Notificato](https://github.com/mac-cain13/notificato) - A library for handling push notifications.  * [Notificator](https://github.com/namshi/notificator) - A lightweight notification library.  * [Php-pushwoosh](https://github.com/gomoob/php-pushwoosh) - A PHP Library to easily send push notifications with the Pushwoosh REST Web Services.    ### Deployment  *Libraries for project deployment.*    * [Deployer](https://github.com/deployphp/deployer) - A deployment tool.  * [Envoy](https://github.com/laravel/envoy) - A tool to run SSH tasks with PHP.  * [Rocketeer](https://github.com/rocketeers/rocketeer) - A fast and easy deployer for the PHP world.    ### Internationalisation and Localisation  *Libraries for Internationalization (I18n) and Localization (L10n).*    * [Aura.Intl](https://github.com/auraphp/Aura.Intl) - Provides internationalization (I18N) tools, specifically package-oriented per-locale message translation.  * [CakePHP I18n](https://github.com/cakephp/i18n) - Message translation and localization for dates and numbers.    ### Serverless  *Libraries and tools to help build serverless web applications.*    * [Bref](https://bref.sh/) - Serverless PHP on AWS Lambda.  * [OpenWhisk](http://openwhisk.apache.org/) - An open-source serverless cloud platform.  * [Serverless Framework](https://www.serverless.com/open-source/) - An open-source framework for building serverless applications.  * [Laravel Vapor](https://vapor.laravel.com/) - A serverless deployment platform for Laravel, powered by AWS.    ## Configuration  *Libraries and tools for configuration.*    * [PHP Dotenv](https://github.com/vlucas/phpdotenv) - Parse and load environment variables from `.env` files.  * [Symfony Dotenv](https://github.com/symfony/dotenv)- Parse and load environment variables from `.env` files.  * [Yo! Symfony TOML](https://github.com/yosymfony/toml) - A PHP parser for [TOML](https://github.com/toml-lang/toml).     ### Third Party APIs  *Libraries for accessing third party APIs.*    * [Amazon Web Service SDK](https://github.com/aws/aws-sdk-php) - The official PHP AWS SDK library.  * [AsyncAWS](https://async-aws.com/) - An unofficial asynchronous PHP AWS SDK.  * [Campaign Monitor](https://campaignmonitor.github.io/createsend-php/) - The official Campaign Monitor PHP library.  * [Github](https://github.com/KnpLabs/php-github-api) - A library to interface with the Github API.  * [Mailgun](https://github.com/mailgun/mailgun-php) The official Mailgun PHP API.  * [Square](https://github.com/square/connect-php-sdk) - The official Square PHP SDK for payments and other Square APIs.  * [Stripe](https://github.com/stripe/stripe-php) - The official Stripe PHP library.  * [Twilio](https://github.com/twilio/twilio-php) - The official Twilio PHP REST API.    ### Extensions  *Libraries to help build PHP extensions.*    * [PHP CPP](https://www.php-cpp.com/) - A C++ library for developing PHP extensions.  * [Zephir](https://github.com/phalcon/zephir) - A compiled language between PHP and C++ for developing PHP extensions.    ### Miscellaneous  *Useful libraries or utilities that don't fit into the categories above.*    * [Annotations](https://github.com/doctrine/annotations) - An annotation library (part of Doctrine).  * [BotMan](https://github.com/botman/botman) - A framework agnostic PHP library to build cross-platform chat bots.  * [ClassPreloader](https://github.com/ClassPreloader/ClassPreloader) - A library for optimising autoloading.  * [Hprose-PHP](https://github.com/hprose/hprose-php) - A cross-language RPC.  * [noCAPTCHA](https://github.com/ARCANEDEV/noCAPTCHA) - Helper for Google's noCAPTCHA (reCAPTCHA).  * [Pagerfanta](https://github.com/whiteoctober/Pagerfanta) - A pagination library.  * [Safe](https://github.com/thecodingmachine/safe) - All PHP functions, rewritten to throw exceptions instead of returning false.  * [SuperClosure](https://github.com/jeremeamia/super_closure) - A library that allows Closures to be serialized.    # Software  *Software for creating a development environment.*    ### PHP Installation  *Tools to help install and manage PHP on your computer.*    * [Brew PHP Switcher](https://github.com/philcook/brew-php-switcher) - Brew PHP switcher.  * [HomeBrew](https://brew.sh/) - A package manager for OSX.  * [Laravel Valet](https://laravel.com/docs/master/valet) - A development environment for macOS.  * [PHP Brew](https://github.com/phpbrew/phpbrew) - A PHP version manager and installer.  * [PHP Build](https://github.com/php-build/php-build) - Another PHP version installer.  * [PHP OSX](https://php-osx.liip.ch/) - A PHP installer for OSX.    ### Development Environment  *Software and tools for creating and sharing a development environment.*    * [Ansible](https://www.ansible.com/) - A radically simple orchestration framework.  * [Docker](https://www.docker.com/) - A containerization platform.  * [Docker PHP Extension Installer](https://github.com/mlocati/docker-php-extension-installer) - Easily install PHP extensions in Docker containers.  * [Expose](https://github.com/beyondcode/expose) - An open source PHP tunneling service.  * [Lando](https://lando.dev/) - Push-button development environments.  * [Laravel Homestead](https://laravel.com/docs/master/homestead) - A local development environment for Laravel.   * [Laradock](http://laradock.io/) - A full PHP development environment based on Docker.  * [Puppet](https://puppet.com/) - A server automation framework and application.  * [Takeout](https://github.com/tighten/takeout) - A Docker-based development-only dependency manager.  * [Vagrant](https://www.vagrantup.com/) - A portable development environment utility.    ### Virtual Machines  *Alternative PHP virtual machines.*    * [Hack](https://hacklang.org/) - A programming language for HHVM.  * [HHVM](https://github.com/facebook/hhvm) - A Virtual Machine, Runtime and JIT for PHP by Facebook.  * [PeachPie](https://github.com/peachpiecompiler/peachpie) - PHP compiler and runtime for .NET and .NET Core.    ### Text Editors and IDEs  *Text Editors and Integrated Development Environments (IDE) with support for PHP.*    * [Eclipse for PHP Developers](https://www.eclipse.org/downloads/) - A PHP IDE based on the Eclipse platform.  * [Apache NetBeans](https://netbeans.apache.org/) - An IDE with support for PHP and HTML5.  * [PhpStorm](https://www.jetbrains.com/phpstorm/) - A commercial PHP IDE.  * [VS Code](https://code.visualstudio.com/) - An open source code editor.    ### Web Applications  *Web-based applications and tools.*    * [3V4L](https://3v4l.org/) - An online PHP & HHVM shell.  * [Adminer](https://www.adminer.org/) - Database management in a single PHP file.  * [Cachet](https://github.com/cachethq/cachet) - The open source status page system.  * [DBV](https://github.com/victorstanciu/dbv) - A database version control application.  * [Lychee](https://github.com/electerious/Lychee) - An easy to use and great looking photo-management-system.  * [MailCatcher](https://github.com/sj26/mailcatcher) - A web tool for capturing and viewing emails.  * [phpMyAdmin](https://github.com/phpmyadmin/phpmyadmin) - A web interface for MySQL/MariaDB.  * [PHP Queue](https://github.com/CoderKungfu/php-queue) - An application for managing queueing backends.  * [phpRedisAdmin](https://github.com/ErikDubbelboer/phpRedisAdmin) - A simple web interface to manage [Redis](https://redis.io/) databases.  * [PHPSandbox](https://phpsandbox.io) - An online IDE for PHP in the browser.    ### Infrastructure  *Infrastructure for providing PHP applications and services.*    * [appserver.io](https://github.com/appserver-io/appserver) - A multithreaded application server for PHP, written in PHP.  * [php-pm](https://github.com/php-pm/php-pm) - A process manager, supercharger and load balancer for PHP applications.  * [RoadRunner](https://github.com/spiral/roadrunner) - High-performance PHP application server, load-balancer and process manager.    # Resources  Various resources, such as books, websites and articles, for improving your PHP development skills and knowledge.    ### PHP Websites  *Useful PHP-related websites.*    * [libs.garden: PHP](https://libs.garden/php) - An overview of fastest growing PHP libraries.  * [Nomad PHP](https://nomadphp.com/) - A online PHP learning resource.  * [Laravel News](https://laravel-news.com/) - The official Laravel blog.  * [PHP Annotated Monthly](https://blog.jetbrains.com/phpstorm/category/php-annotated-monthly/) - A monthly digest of PHP news.  * [PHP Best Practices](https://phpbestpractices.org/) - A PHP best practice guide.  * [PHP FIG](https://www.php-fig.org/) - The PHP Framework Interoperability Group.  * [PHP Package Development Standards](http://php-pds.com) - Package development standards for PHP.  * [PHP School](https://www.phpschool.io/) - Open Source Learning for PHP.  * [PHP Security](https://phpsecurity.readthedocs.io/en/latest/index.html) - A guide to PHP security.  * [PHP The Right Way](https://phptherightway.com/) - A PHP best practice quick reference guide.  * [PHP UG](https://php.ug) - A website to help people locate their nearest PHP user group (UG).  * [PHP Versions](http://phpversions.info/) - Lists which versions of PHP are available on several popular web hosts.  * [PHP Watch](https://php.watch/) - PHP articles, news, upcoming changes, RFCs and more.  * [PHP Weekly](http://www.phpweekly.com/archive.html) - A weekly PHP newsletter.  * [Securing PHP](https://www.securingphp.com/) - A newsletter about PHP security and library recommendations.  * [Seven PHP](https://7php.com/) - A website that interviews members of the PHP community.    ### PHP Books  *Fantastic PHP-related books.*    * [Domain-Driven Design in PHP](https://leanpub.com/ddd-in-php) - Real examples written in PHP showcasing DDD Architectural Styles.  * [Functional Programming in PHP](https://www.functionalphp.com/) - This book will show you how to leverage these new PHP5.3+ features by understanding functional programming principles  * [Grumpy PHPUnit](https://leanpub.com/grumpy-phpunit) - A book about unit testing with PHPUnit by Chris Hartjes.  * [Mastering Object-Orientated PHP](https://www.brandonsavage.net/) - A book about object-orientated PHP by Brandon Savage.  * [Modern PHP New Features and Good Practices](https://www.oreilly.com/library/view/~/9781491905173/) - A book about new PHP features and best practices by Josh Lockhart.  * [Modernizing Legacy Applications in PHP](https://leanpub.com/mlaphp) - A book about modernizing legacy PHP applications by Paul M. Jones.  * [PHP 7 Upgrade Guide](https://leanpub.com/php7) - An ebook covering all of the features and changes in PHP 7 by Colin O'Dell.  * [PHP Pandas](https://daylerees.com/php-pandas/) - A book about learning to write PHP by Dayle Rees.  * [Scaling PHP Applications](https://www.scalingphpbook.com) - An ebook about scaling PHP applications by Steve Corona.  * [Securing PHP: Core Concepts](https://leanpub.com/securingphp-coreconcepts) - A book about common security terms and practices for PHP by Chris Cornutt.  * [Signaling PHP](https://leanpub.com/signalingphp) - A book about catching PCNTL signals in CLI scripts by Cal Evans.  * [The Grumpy Programmer's Guide to Building Testable PHP Applications](https://leanpub.com/grumpy-testing) - A book about building testing PHP applications by Chris Hartjes.  * [XML Parsing with PHP](https://www.phparch.com/books/xml-parsing-with-php/) - This book covers parsing and validating XML documents, leveraging XPath expressions, and working with namespaces as well as how to create and modify XML files programmatically.    ### PHP Videos  *Fantastic PHP-related videos.*    * [Nomad PHP Lightning Talks](https://www.youtube.com/c/nomadphp) - 10 to 15 minute Lightning Talks by PHP community members.  * [PHP UK Conference](https://www.youtube.com/user/phpukconference/videos) - A collection of videos from the PHP UK Conference.  * [Programming with Anthony](https://www.youtube.com/playlist?list=PLM-218uGSX3DQ3KsB5NJnuOqPqc5CW2kW) - A video series by Anthony Ferrara.  * [Taking PHP Seriously](https://www.infoq.com/presentations/php-history/) - A talk outlining PHP's strengths by Keith Adams of Facebook.  * [Laracasts](https://laracasts.com) - Screencasts about Laravel, Vue JS and more.  * [Laravel YouTube Channel](https://www.youtube.com/channel/UCfO2GiQwb-cwJTb1CuRSkwg) - The official Laravel YouTube channel.  * [SymfonyCasts](https://symfonycasts.com/) - Screencasts and tutorials about PHP and Symfony.    ### PHP Podcasts  *Podcasts with a focus on PHP topics.*    * [Laravel Podcast](https://laravelpodcast.com/) - Laravel and PHP development news and discussion.  * [PHP Internals News](https://phpinternals.news) - A podcast about PHP internals.  * [PHP Roundtable](https://www.phproundtable.com/) - The PHP Roundtable is a casual gathering of developers discussing topics that PHP nerds care about.  * [PHP Town Hall](https://phptownhall.com/) - A casual PHP podcast by Ben Edmunds and Phil Sturgeon.  * [Voices of the ElePHPant](https://voicesoftheelephpant.com/) Interviews with the people that make the PHP community special.    ### PHP Newsletters  *PHP-related news directly to your inbox.*    * [PHP Weekly](http://www.phpweekly.com/) - A weekly newsletter about PHP.    ### PHP Reading  *PHP-releated reading materials.*    * [php[architect]](https://www.phparch.com/magazine/) - A monthly magazine dedicated to PHP.    ### PHP Internals Reading  *Reading materials related to the PHP internals or performance.*    * [PHP RFCs](https://wiki.php.net/rfc) - The home of PHP RFCs (Request for Comments).  * [Externals](https://externals.io/) - PHP internal discussions.   * [PHP RFC Watch](https://php-rfc-watch.beberlei.de/) - Watch the latest PHP [RFCs](https://wiki.php.net/rfc).  * [PHP Internals Book](http://www.phpinternalsbook.com) - An online book about PHP internals, written by three core developers. """
Big data;https://github.com/pivotalsoftware/PivotalR;"""PivotalR  =======    PivotalR is a package that enables users of R, the most popular open source statistical programming language  and environment, to interact with [Greenplum Database](https://greenplum.org/)  and the [PostgreSQL](https://www.postgresql.org/) for big data  analytics. It does so by providing an interface to the operations on tables/views in the database. These  operations are almost the same as those of data.frame. Minimal amount of data is transfered between R and  the database. Thus the users of R do not need to learn SQL when they  operate on the objects in the database. PivotalR also lets the user to run the functions of the open source  machine learning package [Apache MADlib](https://madlib.apache.org/) directly from R.    1. An Introduction to PivotalR            vignette(""pivotalr"") # execute in R console to view the PDF file  2. To install PivotalR:      * Get the latest stable version from CRAN by running `install.packages(""PivotalR"")`      * Or try out the latest development version from github by running the following code (need R >= 3.0.2):            ```          ## install.packages(""devtools"") # 'devtools' package is only available for R >= 3.0.2          devtools::install_github(""PivotalR"", ""greenplum-db"")          ```      * Or download the source tarball directly from [**here**](https://github.com/greenplum-db/PivotalR/tarball/master), and then install the tarball            ```          install.packages(""greenplum-db-PivotalR-xxxx.tar.gz"", repos = NULL, type = ""source"")          ```      where ""greenplum-db-PivotalR-xxxx.tar.gz"" is the name of the package that you have downloaded.  3. To get started:      * [Read the wiki](https://github.com/greenplum-db/PivotalR/wiki)      * [Look at some demo code](https://github.com/greenplum-db/PivotalR/wiki/Example)      * [Watch a training video](https://www.youtube.com/watch?v=6cmyRCMY6j0)      * [Read the quick-start guide](https://github.com/wjjung317/gp-r/blob/master/docs/PivotalR-quick-start%20v2.pdf) """
Big data;https://github.com/amplab/velox-modelserver;"""velox-modelserver  =================    Serving machine learning models """
Big data;https://github.com/chihming/awesome-network-embedding;"""# awesome-network-embedding  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)  [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)  [![Gitter chat for developers at https://gitter.im/dmlc/xgboost](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/awesome-network-embedding/Lobby)    Also called network representation learning, graph embedding, knowledge embedding, etc.    The task is to learn the representations of the vertices from a given network.    CALL FOR HELP: I'm planning to re-organize the papers with clear classification index in the near future. Please feel free to submit a commit if you find any interesting related work:)    <img src=""NE.png"" width=""480"">    # Paper References with the implementation(s)  - **GraphGym**    - A platform for designing and evaluating Graph Neural Networks (GNN), NeurIPS 2020    - [[Paper]](https://proceedings.neurips.cc/paper/2020/file/c5c3d4fe6b2cc463c7d7ecba17cc9de7-Paper.pdf)    - [[Python]](https://github.com/snap-stanford/graphgym)  - **FEATHER**    - Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models, CIKM 2020    - [[Paper]](https://arxiv.org/abs/2005.07959)    - [[Python]](https://github.com/benedekrozemberczki/FEATHER)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)  - **HeGAN**    - Adversarial Learning on Heterogeneous Information Networks, KDD 2019    - [[Paper]](https://fangyuan1st.github.io/paper/KDD19_HeGAN.pdf)    - [[Python]](https://github.com/librahu/HeGAN)  - **NetMF**    - Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec, WSDM 2018    - [[Paper]](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)  - **GL2Vec**    - GL2vec: Graph Embedding Enriched by Line Graphs with Edge Features, ICONIP 2019    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)  - **NNSED**    - A Non-negative Symmetric Encoder-Decoder Approach for Community Detection, CIKM 2017    - [[Paper]](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)   - **SymmNMF**    - Symmetric Nonnegative Matrix Factorization for Graph Clustering, SDM 2012    - [[Paper]](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)  - **RECT**    - Network Embedding with Completely-Imbalanced Labels, TKDE 2020    - [[Paper]](https://zhengwang100.github.io/pdf/TKDE20_wzheng.pdf)    - [[Python]](https://github.com/zhengwang100/RECT)   - **GEMSEC**    - GEMSEC: Graph Embedding with Self Clustering, ASONAM 2019    - [[Paper]](https://arxiv.org/abs/1802.03997)    - [[Python]](https://github.com/benedekrozemberczki/GEMSEC)   - **AmpliGraph**    - Library for learning knowledge graph embeddings with TensorFlow     - [[Project]](http://docs.ampligraph.org)    - [[code]](https://github.com/Accenture/AmpliGraph)  - **jodie**    - Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks, KDD'19    - [[Project]](http://snap.stanford.edu/jodie/)    - [[Code]](https://github.com/srijankr/jodie/)  - **PyTorch-BigGraph**    - Pytorch-BigGraph - a distributed system for learning graph embeddings for large graphs, SysML'19    - [[github]](https://github.com/facebookresearch/PyTorch-BigGraph)  - **ATP**    - ATP: Directed Graph Embedding with Asymmetric Transitivity Preservation, AAAI'19    - [[paper]](https://arxiv.org/abs/1811.00839)    - [[code]](https://github.com/zhenv5/atp)  - **MUSAE**    - Multi-scale Attributed Node Embedding, ArXiv 2019    - [[paper]](https://arxiv.org/abs/1909.13021)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/benedekrozemberczki/MUSAE)  - **SEAL-CI**    - Semi-Supervised Graph Classification: A Hierarchical Graph Perspective, WWW'19    - [[paper]](https://arxiv.org/pdf/1904.05003.pdf)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/SEAL-CI)  - **N-GCN and MixHop**    - A Higher-Order Graph Convolutional Layer, NIPS'18 (workshop)    - [[paper]](http://sami.haija.org/papers/high-order-gc-layer.pdf)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/MixHop-and-N-GCN)  - **CapsGNN**    - Capsule Graph Neural Network, ICLR'19    - [[paper]](https://openreview.net/forum?id=Byl8BnRcYm)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/CapsGNN)  - **Splitter**    - Splitter: Learning Node Representations that Capture Multiple Social Contexts, WWW'19    - [[paper]](http://epasto.org/papers/www2019splitter.pdf)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/Splitter)  - **REGAL**    - REGAL: Representation Learning-based Graph Alignment. International Conference on Information and Knowledge Management, CIKM'18    - [[arxiv]](https://arxiv.org/pdf/1802.06257.pdf)    - [[paper]](https://dl.acm.org/citation.cfm?id=3271788)    - [[code]](https://github.com/GemsLab/REGAL)  - **PyTorch Geometric**    - Fast Graph Representation Learning With PyTorch Geometric    - [[paper]](https://arxiv.org/pdf/1903.02428.pdf)    - [[Python PyTorch]](https://github.com/rusty1s/pytorch_geometric)  - **TuckER**    - Tensor Factorization for Knowledge Graph Completion, Arxiv'19    - [[paper]](https://arxiv.org/pdf/1901.09590.pdf)    - [[Python PyTorch]](https://github.com/ibalazevic/TuckER)  - **HypER**    - Hypernetwork Knowledge Graph Embeddings, Arxiv'18    - [[paper]](https://arxiv.org/pdf/1808.07018.pdf)    - [[Python PyTorch]](https://github.com/ibalazevic/HypER)  - **GWNN**    - Graph Wavelet Neural Network, ICLR'19    - [[paper]](https://openreview.net/forum?id=H1ewdiR5tQ)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork)    - [[Python TensorFlow]](https://github.com/Eilene/GWNN)  - **APPNP**    - Combining Neural Networks with Personalized PageRank for Classification on Graphs, ICLR'19    - [[paper]](https://arxiv.org/abs/1810.05997)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/APPNP)    - [[Python TensorFlow]](https://github.com/klicperajo/ppnp)  - **role2vec**    - Learning Role-based Graph Embeddings, IJCAI'18    - [[paper]](https://arxiv.org/pdf/1802.02896.pdf)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/benedekrozemberczki/role2vec)  - **AttentionWalk**    - Watch Your Step: Learning Node Embeddings via Graph Attention, NIPS'18    - [[paper]](https://arxiv.org/pdf/1710.09599.pdf)    - [[Python]](http://sami.haija.org/graph/context)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/AttentionWalk)    - [[Python TensorFlow]](https://github.com/google-research/google-research/tree/master/graph_embedding/watch_your_step/)  - **GAT**    - Graph Attention Networks, ICLR'18    - [[paper]](https://arxiv.org/pdf/1710.10903.pdf)    - [[Python PyTorch]](https://github.com/Diego999/pyGAT)    - [[Python TensorFlow]](https://github.com/PetarV-/GAT)  - **SINE**    - SINE: Scalable Incomplete Network Embedding, ICDM'18    - [[paper]](https://github.com/benedekrozemberczki/SINE/blob/master/paper.pdf)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/SINE/)    - [[C++]](https://github.com/daokunzhang/SINE)  - **SGCN**    - Signed Graph Convolutional Network, ICDM'18    - [[paper]](https://github.com/benedekrozemberczki/SGCN/blob/master/sgcn.pdf)    - [[Python]](https://github.com/benedekrozemberczki/SGCN)  - **TENE**    - Enhanced Network Embedding with Text Information, ICPR'18    - [[paper]](https://github.com/benedekrozemberczki/TENE/blob/master/tene_paper.pdf)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/benedekrozemberczki/TENE)   - **DANMF**    - Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection, CIKM'18    - [[paper]](https://smartyfh.com/Documents/18DANMF.pdf)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/benedekrozemberczki/DANMF)    - [[Matlab]](https://github.com/smartyfh/DANMF)    - **BANE**    - Binarized Attributed Network Embedding, ICDM'18    - [[paper]](https://www.researchgate.net/publication/328688614_Binarized_Attributed_Network_Embedding)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/benedekrozemberczki/BANE)    - [[Matlab]](https://github.com/ICDM2018-BANE/BANE)  - **GCN Insights**    - Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning, AAAI'18    - [[Project]](https://liqimai.github.io/blog/AAAI-18/)    - [[code]](https://github.com/liqimai/gcn/tree/AAAI-18/)  - **PCTADW**    - Learning Embeddings of Directed Networks with Text-Associated Nodes---with Applications in Software Package Dependency Networks    - [[paper]](https://arxiv.org/pdf/1809.02270.pdf)    - [[Python]](https://github.com/shudan/PCTADW)    - [[dataset]](https://doi.org/10.5281/zenodo.1410669)  - **LGCN**    - Large-Scale Learnable Graph Convolutional Networks, KDD'18    - [[paper]](http://www.kdd.org/kdd2018/accepted-papers/view/large-scale-learnable-graph-convolutional-networks)    - [[Python]](https://github.com/HongyangGao/LGCN)  - **AspEm**    - AspEm: Embedding Learning by Aspects in Heterogeneous Information Networks    - [[paper]](http://yushi2.web.engr.illinois.edu/sdm18.pdf)    - [[Python]](https://github.com/ysyushi/aspem)  - **Walklets**    - Don't Walk, Skip! Online Learning of Multi-scale Network Embeddings    - [[paper]](https://arxiv.org/pdf/1605.02115.pdf)    - [[Python Karateclub]](https://github.com/benedekrozemberczki/karateclub)      - [[Python]](https://github.com/benedekrozemberczki/walklets)    - **gat2vec**    - gat2vec: Representation learning for attributed graphs    - [[paper]](https://doi.org/10.1007/s00607-018-0622-9)    - [[Python]](https://github.com/snash4/GAT2VEC)  - **FSCNMF**    - FSCNMF: Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks    - [[paper]](https://arxiv.org/abs/1804.05313)    - [[Python Karateclub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/sambaranban/FSCNMF)      - [[Python]](https://github.com/benedekrozemberczki/FSCNMF)  - **SIDE**    - SIDE: Representation Learning in Signed Directed Networks    - [[paper]](https://datalab.snu.ac.kr/side/resources/side.pdf)    - [[Python]](https://datalab.snu.ac.kr/side/resources/side.zip)    - [[Site]](https://datalab.snu.ac.kr/side/)  - **AWE**    - Anonymous Walk Embeddings, ICML'18    - [[paper]](https://www.researchgate.net/publication/325114285_Anonymous_Walk_Embeddings)    - [[Python]](https://github.com/nd7141/Anonymous-Walk-Embeddings)  - **BiNE**    - BiNE: Bipartite Network Embedding, SIGIR'18    - [[paper]](http://staff.ustc.edu.cn/~hexn/papers/sigir18-bipartiteNE.pdf)    - [[Python]](https://github.com/clhchtcjj/BiNE)  - **HOPE**    - Asymmetric Transitivity Preserving Graph Embedding    - [[KDD 2016]](http://www.kdd.org/kdd2016/papers/files/rfp0184-ouA.pdf)    - [[Python]](https://github.com/AnryYang/HOPE)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)   - **VERSE**    - VERSE, Versatile Graph Embeddings from Similarity Measures    - [[Arxiv]](https://arxiv.org/abs/1803.04742) [[WWW 2018]]    - [[Python]](https://github.com/xgfs/verse)   - **AGNN**    - Attention-based Graph Neural Network for semi-supervised learning    - [[ICLR 2018 OpenReview (rejected)]](https://openreview.net/forum?id=rJg4YGWRb)    - [[Python]](https://github.com/dawnranger/pytorch-AGNN)  - **SEANO**    - Semi-supervised Embedding in Attributed Networks with Outliers    - [[Paper]](https://arxiv.org/pdf/1703.08100.pdf) (SDM 2018)    - [[Python]](http://jiongqianliang.com/SEANO/)     - **Hyperbolics**    - Representation Tradeoffs for Hyperbolic Embeddings     - [[Arxiv]](https://arxiv.org/abs/1804.03329)    - [[Python]](https://github.com/HazyResearch/hyperbolics)     - **DGCNN**    - An End-to-End Deep Learning Architecture for Graph Classiï¬cation     - [[AAAI 2018]](http://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf)    - [[Lua]](https://github.com/muhanzhang/DGCNN) [[Python]](https://github.com/muhanzhang/pytorch_DGCNN)    - **structure2vec**    - Discriminative Embeddings of Latent Variable Models for Structured Data     - [[Arxiv]](https://arxiv.org/abs/1603.05629)    - [[Python]](https://github.com/Hanjun-Dai/pytorch_structure2vec)    - **Decagon**    - Decagon, Graph Neural Network for Multirelational Link Prediction     - [[Arxiv]](https://arxiv.org/abs/1802.00543) [[SNAP]](http://snap.stanford.edu/decagon/) [[ISMB 2018]]    - [[Python]](https://github.com/marinkaz/decagon)      - **DHNE**    - Structural Deep Embedding for Hyper-Networks    - [[AAAI 2018]](http://nrl.thumedialab.com/Structural-Deep-Embedding-for-Hyper-Networks)[[Arxiv]](https://arxiv.org/abs/1711.10146)    - [[Python]](https://github.com/tadpole/DHNE)    - **Ohmnet**    - Feature Learning in Multi-Layer Networks     - [[Arxiv]](https://arxiv.org/abs/1707.04638) [[SNAP]](http://snap.stanford.edu/ohmnet/)     - [[Python]](https://github.com/marinkaz/ohmnet)    - **SDNE**    - Structural Deep Network Embedding     - [[KDD 2016]](http://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf)    - [[Python]](https://github.com/xiaohan2012/sdne-keras)   - **STWalk**    - STWalk: Learning Trajectory Representations in Temporal Graphs]     - [[Arxiv]](https://arxiv.org/abs/1711.04150)    - [[Python]](https://github.com/supriya-pandhre/STWalk)  - **LoNGAE**    - Learning to Make Predictions on Graphs with Autoencoders     - [[Arxiv]](https://arxiv.org/abs/1802.08352)    - [[Python]](https://github.com/vuptran/graph-representation-learning)    - **RSDNE**    - [RSDNE: Exploring Relaxed Similarity and Dissimilarity from Completely-imbalanced Labels for Network Embedding.](https://zhengwang100.github.io/AAAI18_RSDNE.pdf), AAAI 2018    - [[Matlab]](https://github.com/zhengwang100/RSDNE)   - **FastGCN**    - FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling     - [[Arxiv]](https://arxiv.org/abs/1801.10247), [[ICLR 2018 OpenReview]](https://openreview.net/forum?id=rytstxWAW)    - [[Python]](https://github.com/matenure/FastGCN)  - **diff2vec**    - [Fast Sequence Based Embedding with Diffusion Graphs](http://homepages.inf.ed.ac.uk/s1668259/papers/sequence.pdf), CompleNet 2018    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/benedekrozemberczki/diff2vec)   - **Poincare**    - [PoincarÃ© Embeddings for Learning Hierarchical Representations](https://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations), NIPS 2017    - [[PyTorch]](https://github.com/facebookresearch/poincare-embeddings) [[Python]](https://radimrehurek.com/gensim/models/poincare.html) [[C++]](https://github.com/TatsuyaShirakawa/poincare-embedding)  - **PEUNE**    - [PRUNE: Preserving Proximity and Global Ranking for Network Embedding](https://papers.nips.cc/paper/7110-prune-preserving-proximity-and-global-ranking-for-network-embedding), NIPS 2017    - [[code]](https://github.com/ntumslab/PRUNE)  - **ASNE**    - Attributed Social Network Embedding, TKDE'18    - [[arxiv]](https://arxiv.org/abs/1706.01860)    - [[Python]](https://github.com/lizi-git/ASNE)    - [[Fast Python]](https://github.com/benedekrozemberczki/ASNE)  - **GraphWave**    - [Spectral Graph Wavelets for Structural Role Similarity in Networks](http://snap.stanford.edu/graphwave/),     - [[arxiv]](https://arxiv.org/abs/1710.10321), [[ICLR 2018 OpenReview]](https://openreview.net/forum?id=rytstxWAW)    - [[Python]](https://github.com/snap-stanford/graphwave) [[faster version]](https://github.com/benedekrozemberczki/GraphWaveMachine)  - **StarSpace**    - [StarSpace: Embed All The Things!](https://arxiv.org/pdf/1709.03856), arxiv'17    - [[code]](https://github.com/facebookresearch/Starspace)  - **proNet-core**    - Vertex-Context Sampling for Weighted Network Embedding, arxiv'17    - [[arxiv]](https://arxiv.org/abs/1711.00227) [[code]](https://github.com/cnclabs/proNet-core)  - **struc2vec**    - [struc2vec: Learning Node Representations from Structural Identity](https://dl.acm.org/citation.cfm?id=3098061), KDD'17    - [[Python]](https://github.com/leoribeiro/struc2vec)  - **ComE**    - Learning Community Embedding with Community Detection and Node Embedding on Graphs, CIKM'17    - [[Python]](https://github.com/andompesta/ComE)  - **BoostedNE**    - [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627), '18    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/benedekrozemberczki/BoostedFactorization)  - **M-NMF**    - Community Preserving Network Embedding, AAAI'17    - [[Python TensorFlow]](https://github.com/benedekrozemberczki/M-NMF)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)  - **GraphSAGE**    - Inductive Representation Learning on Large Graphs, NIPS'17    - [[arxiv]](https://arxiv.org/abs/1706.02216) [[TF]](https://github.com/williamleif/GraphSAGE) [[PyTorch]](https://github.com/williamleif/graphsage-simple/)   - **ICE**    - [ICE: Item Concept Embedding via Textual Information](http://dl.acm.org/citation.cfm?id=3080807), SIGIR'17    - [[demo]](https://cnclabs.github.io/ICE/) [[code]](https://github.com/cnclabs/ICE)  - **GuidedHeteEmbedding**    - Task-guided and path-augmented heterogeneous network embedding for author identification, WSDM'17    - [[paper]](https://arxiv.org/pdf/1612.02814.pdf) [[code]](https://github.com/chentingpc/GuidedHeteEmbedding)  - **metapath2vec**    - metapath2vec: Scalable Representation Learning for Heterogeneous Networks, KDD'17    - [[paper]](https://www3.nd.edu/~dial/publications/dong2017metapath2vec.pdf) [[project website]](https://ericdongyx.github.io/metapath2vec/m2v.html)  - **GCN**    - Semi-Supervised Classification with Graph Convolutional Networks, ICLR'17    - [[arxiv]](https://arxiv.org/abs/1609.02907)  [[Python Tensorflow]](https://github.com/tkipf/gcn)  - **GAE**    - Variational Graph Auto-Encoders, arxiv    - [[arxiv]](https://arxiv.org/abs/1611.07308) [[Python Tensorflow]](https://github.com/tkipf/gae)  - **CANE**    - CANE: Context-Aware Network Embedding for Relation Modeling, ACL'17    - [[paper]](http://www.thunlp.org/~tcc/publications/acl2017_cane.pdf) [[Python]](https://github.com/thunlp/cane)  - **TransNet**    - TransNet: Translation-Based Network Representation Learning for Social Relation Extraction, IJCAI'17    - [[Python Tensorflow]](https://github.com/thunlp/TransNet)  - **cnn_graph**    - Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering, NIPS'16    - [[Python]](https://github.com/mdeff/cnn_graph)  - **ConvE**    - [Convolutional 2D Knowledge Graph Embeddings](https://arxiv.org/pdf/1707.01476v2.pdf), arxiv    - [[source]](https://github.com/TimDettmers/ConvE)  - **node2vec**    - [node2vec: Scalable Feature Learning for Networks](http://dl.acm.org/citation.cfm?id=2939672.2939754), KDD'16    - [[arxiv]](https://arxiv.org/abs/1607.00653) [[Python]](https://github.com/aditya-grover/node2vec) [[Python-2]](https://github.com/apple2373/node2vec) [[Python-3]](https://github.com/eliorc/node2vec) [[C++]](https://github.com/xgfs/node2vec-c)    - **DNGR**    - [Deep Neural Networks for Learning Graph Representations](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12423), AAAI'16    - [[Matlab]](https://github.com/ShelsonCao/DNGR) [[Python Keras]](https://github.com/MdAsifKhan/DNGR-Keras)  - **HolE**    - [Holographic Embeddings of Knowledge Graphs](http://dl.acm.org/citation.cfm?id=3016172), AAAI'16    - [[Python-sklearn]](https://github.com/mnick/holographic-embeddings) [[Python-sklearn2]](https://github.com/mnick/scikit-kge)  - **ComplEx**    - [Complex Embeddings for Simple Link Prediction](http://dl.acm.org/citation.cfm?id=3045609), ICML'16    - [[arxiv]](https://arxiv.org/abs/1606.06357) [[Python]](https://github.com/ttrouill/complex)  - **MMDW**    - Max-Margin DeepWalk: Discriminative Learning of Network Representation, IJCAI'16    - [[paper]](http://nlp.csai.tsinghua.edu.cn/~lzy/publications/ijcai2016_mmdw.pdf)  [[Java]](https://github.com/thunlp/MMDW)  - **planetoid**    - Revisiting Semi-supervised Learning with Graph Embeddings, ICML'16    - [[arxiv]](https://arxiv.org/abs/1603.08861) [[Python]](https://github.com/kimiyoung/planetoid)  - **graph2vec**    - graph2vec: Learning Distributed Representations of Graphs, KDD'17 MLGWorkshop    - [[arxiv]](https://arxiv.org/abs/1707.05005)    - [[Python gensim]](https://github.com/benedekrozemberczki/graph2vec) [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)  - **PowerWalk**    - [PowerWalk: Scalable Personalized PageRank via Random Walks with Vertex-Centric Decomposition](http://dl.acm.org/citation.cfm?id=2983713), CIKM'16    - [[code]](https://github.com/lqhl/PowerWalk)  - **LINE**    - [LINE: Large-scale information network embedding](http://dl.acm.org/citation.cfm?id=2741093), WWW'15    - [[arxiv]](https://arxiv.org/abs/1503.03578) [[C++]](https://github.com/tangjianpku/LINE) [[Python TF]](https://github.com/snowkylin/line) [[Python Theano/Keras]](https://github.com/VahidooX/LINE)  - **PTE**    - [PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks](http://dl.acm.org/citation.cfm?id=2783307), KDD'15    - [[C++]](https://github.com/mnqu/PTE)  - **GraRep**    - [Grarep: Learning graph representations with global structural information](http://dl.acm.org/citation.cfm?id=2806512), CIKM'15    - [[Matlab]](https://github.com/ShelsonCao/GraRep)    - [[Julia]](https://github.com/xgfs/GraRep.jl)    - [[Python]](https://github.com/benedekrozemberczki/GraRep)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)  - **KB2E**    - [Learning Entity and Relation Embeddings for Knowledge Graph Completion](http://dl.acm.org/citation.cfm?id=2886624), AAAI'15    - [[paper]](http://nlp.csai.tsinghua.edu.cn/~lzy/publications/aaai2015_transr.pdf) [[C++]](https://github.com/thunlp/KB2E)  [[faster version]](https://github.com/thunlp/Fast-TransX)  - **TADW**    - [Network Representation Learning with Rich Text Information](http://dl.acm.org/citation.cfm?id=2832542), IJCAI'15    - [[paper]](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) [[Matlab]](https://github.com/thunlp/tadw) [[Python]](https://github.com/benedekrozemberczki/TADW)  - **DeepWalk**    - [DeepWalk: Online Learning of Social Representations](http://dl.acm.org/citation.cfm?id=2623732), KDD'14    - [[arxiv]](https://arxiv.org/abs/1403.6652) [[Python]](https://github.com/phanein/deepwalk)  [[C++]](https://github.com/xgfs/deepwalk-c)  - **GEM**    - Graph Embedding Techniques, Applications, and Performance: A Survey    - [[arxiv]](https://arxiv.org/abs/1705.02801) [[Python]](https://github.com/palash1992/GEM)  - **DNE-SBP**    - Deep Network Embedding for Graph Representation Learning in Signed Networks    - [[paper]](https://ieeexplore.ieee.org/document/8486671) [[Code]](https://github.com/shenxiaocam/Deep-network-embedding-for-graph-representation-learning-in-signed-networks)    # Paper References    [A Comprehensive Survey on Graph Neural Networks](https://arxiv.org/abs/1901.00596), arxiv'19    [Hierarchical Graph Representation Learning with Differentiable Pooling](https://arxiv.org/pdf/1806.08804.pdf), NIPS'18    **SEMAC**, [Link Prediction via Subgraph Embedding-Based Convex Matrix Completion](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16442), AAAI 2018, [Slides](https://www.slideshare.net/gdm3003/semac-graph-node-embeddings-for-link-prediction)    **MILE**, [MILE: A Multi-Level Framework for Scalable Graph Embedding](https://arxiv.org/pdf/1802.09612.pdf), arxiv'18    **MetaGraph2Vec**, [MetaGraph2Vec: Complex Semantic Path Augmented Heterogeneous Network Embedding](https://arxiv.org/abs/1803.02533)    **PinSAGE**, [Graph Convolutional Neural Networks for Web-Scale Recommender Systems](https://arxiv.org/abs/1806.01973)    [Curriculum Learning for Heterogeneous Star Network Embedding via Deep Reinforcement Learning](https://dl.acm.org/citation.cfm?id=3159711), WSDM '18    [Adversarial Network Embedding](https://arxiv.org/abs/1711.07838), arxiv    **Role2Vec**, [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896)    **edge2vec**, [Feature Propagation on Graph: A New Perspective to Graph Representation  Learning](https://arxiv.org/abs/1804.06111)    **MINES**, [Multi-Dimensional Network Embedding with Hierarchical Structure](http://cse.msu.edu/~mayao4/downloads/Multidimensional_Network_Embedding_with_Hierarchical_Structure.pdf)    [Walk-Steered Convolution for Graph Classification](https://arxiv.org/abs/1804.05837)    [Deep Feature Learning for Graphs](https://arxiv.org/abs/1704.08829), arxiv'17    [Fast Linear Model for Knowledge Graph Embeddings](https://arxiv.org/abs/1710.10881), arxiv'17    [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec](https://arxiv.org/abs/1710.02971), arxiv'17    [A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications](https://arxiv.org/abs/1709.07604), arxiv'17    [Representation Learning on Graphs: Methods and Applications](https://arxiv.org/pdf/1709.05584.pdf), IEEE DEB'17    **CONE**, [CONE: Community Oriented Network Embedding](https://arxiv.org/abs/1709.01554), arxiv'17    **LANE**,   [Label Informed Attributed Network Embedding](http://dl.acm.org/citation.cfm?id=3018667), WSDM'17    **Graph2Gauss**,  [Deep Gaussian Embedding of Attributed Graphs: Unsupervised Inductive Learning via Ranking](https://arxiv.org/abs/1707.03815), arxiv  [[Bonus Animation]](https://twitter.com/abojchevski/status/885502050133585925)    [Scalable Graph Embedding for Asymmetric Proximity](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14696), AAAI'17    [Query-based Music Recommendations via Preference Embedding](http://dl.acm.org/citation.cfm?id=2959169), RecSys'16    [Tri-party deep network representation](http://dl.acm.org/citation.cfm?id=3060886), IJCAI'16    [Heterogeneous Network Embedding via Deep Architectures](http://dl.acm.org/citation.cfm?id=2783296), KDD'15    [Neural Word Embedding As Implicit Matrix Factorization](http://dl.acm.org/citation.cfm?id=2969070), NIPS'14    [Distributed large-scale natural graph factorization](http://dl.acm.org/citation.cfm?id=2488393), WWW'13    [From Node Embedding To Community Embedding](https://arxiv.org/abs/1610.09950), arxiv    [Walklets: Multiscale Graph Embeddings for Interpretable Network Classification](https://arxiv.org/abs/1605.02115), arxiv    [Comprehend DeepWalk as Matrix Factorization](https://arxiv.org/abs/1501.00358), arxiv    # Conference & Workshop    [Graph Neural Networks for Natural Language Processing](https://github.com/svjan5/GNNs-for-NLP), **EMNLP'19**    [SMORe : Modularize Graph Embedding for Recommendation](https://github.com/cnclabs/smore), **RecSys'19**    [13th International Workshop on Mining and Learning with Graphs](http://www.mlgworkshop.org/2017/), **MLG'17**    [WWW-18 Tutorial Representation Learning on Networks](http://snap.stanford.edu/proj/embeddings-www/), **WWW'18**    # Related List    [awesome-graph-classification](https://github.com/benedekrozemberczki/awesome-graph-classification)    [awesome-community-detection](https://github.com/benedekrozemberczki/awesome-community-detection)    [awesome-embedding-models](https://github.com/Hironsan/awesome-embedding-models)    [Must-read papers on network representation learning (NRL) / network embedding (NE)](https://github.com/thunlp/NRLPapers)    [Must-read papers on knowledge representation learning (KRL) / knowledge embedding (KE)](https://github.com/thunlp/KRLPapers)    [Network Embedding Resources](https://github.com/nate-russell/Network-Embedding-Resources)    [awesome-embedding-models](https://github.com/Hironsan/awesome-embedding-models)    [2vec-type embedding models](https://github.com/MaxwellRebo/awesome-2vec)    [Must-read papers on GNN](https://github.com/thunlp/GNNPapers)    [LiteratureDL4Graph](https://github.com/DeepGraphLearning/LiteratureDL4Graph)    [awesome-graph-classification](https://github.com/benedekrozemberczki/awesome-graph-classification)    # Related Project    **Stanford Network Analysis Project** [website](http://snap.stanford.edu/)    **StellarGraph Machine Learning Library** [website](https://www.stellargraph.io) [GitHub](https://github.com/stellargraph/stellargraph) """
Big data;https://github.com/apache/incubator-slider;"""<!---     Licensed to the Apache Software Foundation (ASF) under one or more     contributor license agreements.  See the NOTICE file distributed with     this work for additional information regarding copyright ownership.     The ASF licenses this file to You under the Apache License, Version 2.0     (the ""License""); you may not use this file except in compliance with     the License.  You may obtain a copy of the License at           http://www.apache.org/licenses/LICENSE-2.0       Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an ""AS IS"" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.  -->    # Slider      Slider is a YARN application to deploy existing distributed applications on YARN,   monitor them and make them larger or smaller as desired -even while   the cluster is running.    Clusters can be stopped and restarted later; the distribution  of the deployed application across the YARN cluster is persisted -enabling  a best-effort placement close to the previous locations on a cluster start.  Applications which remember the previous placement of data (such as HBase)  can exhibit fast start-up times from this feature.    YARN itself monitors the health of 'YARN containers"" hosting parts of   the deployed application -it notifies the Slider manager application of container  failure. Slider then asks YARN for a new container, into which Slider deploys  a replacement for the failed component. As a result, Slider can keep the  size of managed applications consistent with the specified configuration, even  in the face of failures of servers in the cluster -as well as parts of the  application itself    ## Open-Source Development    Apache Slider is an effort undergoing incubation at The Apache Software  Foundation (ASF), sponsored by Apache Incubator.     Incubation is required of all newly accepted projects until a further review  indicates that the infrastructure, communications, and decision making process  have stabilized in a manner consistent with other successful ASF projects.  While incubation status is not necessarily a reflection of the completeness  or stability of the code, it does indicate that the project has yet  to be fully endorsed by the ASF.    ### Mailing Lists    We have a single mailing list for developers and users of Slider: dev@slider.incubator.apache.org    1. You can subscribe to this by emailing dev-subscribe@slider.incubator.apache.org from the  email account to which you wish to subscribe from -and follow the instructions returned.  1. You can unsubscribe later by emailing dev-unsubscribe@slider.incubator.apache.org    There is a mailing list of every commit to the source code repository, commits@slider.incubator.apache.org.  This is generally only of interest to active developers.      ### Bug reports    Bug reports and other issues can be filed on the [Apache Jira](https://issues.apache.org/jira/) server.  Please use the SLIDER project for filing the issues.    ### Source code access    Read-only:    *  [https://git.apache.org/repos/asf/incubator-slider.git](https://git.apache.org/repos/asf/incubator-slider.git)  *  [https://github.com/apache/incubator-slider.git](https://github.com/apache/incubator-slider.git)    Read-write (for committers):    *  [https://git-wip-us.apache.org/repos/asf/incubator-slider.git](https://git-wip-us.apache.org/repos/asf/incubator-slider.git)        # License          Licensed under the Apache License, Version 2.0 (the ""License"");      you may not use this file except in compliance with the License.      You may obtain a copy of the License at             (http://www.apache.org/licenses/LICENSE-2.0)            Unless required by applicable law or agreed to in writing, software      distributed under the License is distributed on an ""AS IS"" BASIS,      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.      See the License for the specific language governing permissions and      limitations under the License. See accompanying LICENSE file.    # Export Control    This distribution includes cryptographic software. The country in which you  currently reside may have restrictions on the import, possession, use, and/or  re-export to another country, of encryption software. BEFORE using any  encryption software, please check your country's laws, regulations and  policies concerning the import, possession, or use, and re-export of encryption  software, to see if this is permitted. See <http://www.wassenaar.org/> for more  information.    The U.S. Government Department of Commerce, Bureau of Industry and Security  (BIS), has classified this software as Export Commodity Control Number (ECCN)  5D002.C.1, which includes information security software using or performing  cryptographic functions with asymmetric algorithms. The form and manner of this  Apache Software Foundation distribution makes it eligible for export under the  License Exception ENC Technology Software Unrestricted (TSU) exception (see the  BIS Export Administration Regulations, Section 740.13) for both object code and  source code.    The following provides more details on the included cryptographic software:    Apache Slider uses the built-in java cryptography libraries. See Oracle's  information regarding Java cryptographic export regulations for more details:  http://www.oracle.com/us/products/export/export-regulations-345813.html    Apache Slider uses the SSL libraries from the Jetty project distributed by the  Eclipse Foundation (http://eclipse.org/jetty). """
Big data;https://github.com/CartoDB/cartodb;"""# What is CARTO?    [![Code Climate](https://codeclimate.com/github/CartoDB/cartodb20.png)](https://codeclimate.com/github/CartoDB/cartodb20)  ![Build Status](https://github.com/CartoDB/cartodb/workflows/Node.js%20Tests/badge.svg)    CARTO is an open, powerful, and intuitive platform for discovering and predicting the key insights underlying the location data in our world.    Empower organizations to optimize operational performance, strategic investments, and everyday decisions with CARTO Engineâ€”our embeddable platform for web and mobile appsâ€”and the new CARTO Builder, a drag and drop analysis tool.    It was built to make it easier for people to tell their stories by  providing them with flexible and intuitive ways to create maps and design  geospatial applications. CARTO can be installed on your own server  and we also offer a hosted service at [carto.com](https://carto.com).    If you would like to see some live demos, check out our  [videos](http://www.vimeo.com/channels/carto) on Vimeo.  We hope you like it!    ![Map view](http://cartodb.s3.amazonaws.com/github/map.png)  ![Data View](http://cartodb.s3.amazonaws.com/github/dataset.png)    ## What can I do with CARTO?    With CARTO, you can upload your geospatial data (Shapefiles, GeoJSON,  etc) using a web form and then make it public or private.    After it is uploaded, you can visualize it in a dataset or on a map, search  it using SQL, and apply map styles using CartoCSS. You can even access it  using the CARTO [APIs](https://docs.carto.com/#cartodb-platform), or export it  to a file.    In other words, with CARTO you can make awesome maps and build  powerful geospatial applications! Definitely check out the [CARTO  Platform](https://carto.com/platform) for interactive examples  and code.      ## Installing    Read the [installation guide in CARTO developers documentation](http://cartodb.readthedocs.org/en/latest/install.html)    ## How do I upgrade CARTO?    See [UPGRADE](UPGRADE) for instructions about upgrading CARTO.    For upgrade of Windshaft-CartoDB and CartoDB-SQL-API see the relative  documentation.    ## Developing & Contributing to CARTO    See [our contributing doc](CONTRIBUTING.md) for how you can improve CARTO, but you will need to sign a Contributor License Agreement (CLA) before making a submission, [learn more here](https://carto.com/contributions).    ## Testing    Check the [testing doc](TESTING.md) section.    ## Requirements    CARTO works in any modern browser, but if you want more info:    ![Chrome](https://cdnjs.cloudflare.com/ajax/libs/browser-logos/39.3.0/archive/chrome_12-48/chrome_12-48_48x48.png) | ![Firefox](https://cdnjs.cloudflare.com/ajax/libs/browser-logos/39.3.0/archive/firefox_1.5-3/firefox_1.5-3_48x48.png) | ![IE](https://cdnjs.cloudflare.com/ajax/libs/browser-logos/39.3.0/edge-tile/edge-tile_48x48.png) | ![Opera](https://cdnjs.cloudflare.com/ajax/libs/browser-logos/39.3.0/opera/opera_48x48.png) | ![Safari](https://cdnjs.cloudflare.com/ajax/libs/browser-logos/39.3.0/safari/safari_48x48.png)  --- | --- | --- | --- | --- |  31+ âœ” | 38+ âœ” | 11+ âœ” | 31+ âœ” | 8+ âœ” | """
Big data;https://github.com/jakekgrog/GhostDB;"""![GhostDB logo](https://imgur.com/ZEGFVo6.png)    ![Build](https://github.com/GhostDB/GhostDB/workflows/GhostDB%20Node%20Test/badge.svg)  [![Twitter](https://img.shields.io/badge/ghostdb-v2.0.0-green)](http://www.ghostdbcache.com)    [![Discord](https://img.shields.io/badge/chat-Join%20us!-green?style=for-the-badge&logo=discord&logoColor=ffffff&color=7389D8&labelColor=6A7EC2)](https://discord.gg/ZkT5Sdf)    ## Update - 03/03/2021    ### Where we've been    GhostDB stemmed from a University project. Due to the nature of these projects (time constraints etc.), we feel some corners were cut. For example, we opted for the memcached model of distribution to save on time as it was easier to implement. However, this wasn't the original vision of GhostDB. Myself and Connor also started new jobs and these took up a good chunk of our time. This combined with just finishing a really busy final year in Univeristy, we decided to mothball the project for a while. We're finally returning to it and hopefully transforming it into what we had originally planned.     ### A new roadmap    We are revising our roadmap below and plan to release an updated version soon but before we do here is a brief rundown on what we want    - Transition away from the memcached model and move to a consistent, partition tolerant system (with limited fault tolerance too) by implementing the raft concensus protocol. (This is almost complete)  - Release a CLI to allow users to easily manage their clusters  - Re-build our SDKs from the ground up to allow users to interact with GhostDB with more ease than is currently possible.  - Implement new data types to broaden GhostDBs use cases.  - Local caching to give an even greater performance boost to users.  - Release AWS Amazon Machine Images (AMIs) and Google Compute Engine Images to allow users to easily create GhostDB clusters in the cloud with only a few clicks.  - Updates to the website that include a download centre and documentation improvements.    ### Contributing    Unfortunately, with work and life we simply don't have the time at the moment to manage pull requests from anyone else. However, we are still accepting issues and are encouraging them.    And of course, we also want to continue improving on our performance :)    ## :books: Overview    GhostDB is a distributed, in-memory, general purpose key-value data store that delivers microsecond performance at any scale.    GhostDB is designed to speed up dynamic database or API driven websites by storing data in RAM in order to reduce the number of times an external data source such as a database or API must be read. GhostDB provides a very large hash table that is distributed across multiple machines and stores large numbers of key-value pairs within the hash table.    ## :car: Roadmap    > GhostDB was a university project - it is not fully featured but we're getting there!    This is a high-level roadmap of what we want GhostDB to become by the end of 2020. If you have any feature requests please create one from the [template](https://github.com/jakekgrog/GhostDB/blob/master/docs/FEATURE_REQUEST.md) and label it as `feature request`!    - First hand support for list, set, stack and queue data structures  - Atomic command queues  - Subscribable streams  - Monitoring & administration dashboard  - Enhanced security features  - Transition to TCP sockets as transport protocol  - CLI  - Support for a wide range of programming languages    ## :wrench: Installation    To install GhostDB please consult the [installation guide](https://github.com/jakekgrog/GhostDB/blob/master/docs/INSTALL.md) for a quick walkthrough on setting up the system.    ## :hammer: Cluster Configuration    To configure a GhostDB cluster please follow the instructions in the [configuration guide](https://github.com/jakekgrog/GhostDB/blob/master/docs/INSTALL.md)    ## :pencil2: Authors    **Jake Grogan**    - Email: <jake.kgrogan@gmail.com>  - Github: [@jakekgrog](https://github.com/jakekgrog)    **Connor Mulready**    - Github: [@nohclu](https://github.com/nohclu)    ## :star: Show your support    Give a :star: if this project helped you! """
Big data;https://github.com/pilosa/pilosa;"""<p>      <a href=""https://www.pilosa.com"">          <img src=""https://www.pilosa.com/img/logo.svg"" width=""50%"">      </a>  </p>    [![CircleCI](https://circleci.com/gh/pilosa/pilosa/tree/master.svg?style=shield)](https://circleci.com/gh/pilosa/pilosa/tree/master)  [![GoDoc](https://godoc.org/github.com/pilosa/pilosa?status.svg)](https://godoc.org/github.com/pilosa/pilosa)  [![Go Report Card](https://goreportcard.com/badge/github.com/pilosa/pilosa)](https://goreportcard.com/report/github.com/pilosa/pilosa)  [![license](https://img.shields.io/github/license/pilosa/pilosa.svg)](https://github.com/pilosa/pilosa/blob/master/LICENSE)  [![CLA Assistant](https://cla-assistant.io/readme/badge/pilosa/pilosa)](https://cla-assistant.io/pilosa/pilosa)  [![GitHub release](https://img.shields.io/github/release/pilosa/pilosa.svg)](https://github.com/pilosa/pilosa/releases)    ## An open source, distributed bitmap index.  - [Docs](#docs)  - [Getting Started](#getting-started)  - [Data Model](#data-model)  - [Query Language](#query-language)  - [Client Libraries](#client-libraries)  - [Get Support](#get-support)  - [Contributing](#contributing)    Want to contribute? One of the easiest ways is to [tell us how you're using (or want to use) Pilosa](https://github.com/pilosa/pilosa/issues/1074). We learn from every discussion!    ## Docs    See our [Documentation](https://www.pilosa.com/docs/) for information about installing and working with Pilosa.      ## Getting Started    1.  [Install Pilosa](https://www.pilosa.com/docs/installation/).    2.  [Start Pilosa](https://www.pilosa.com/docs/getting-started/#starting-pilosa) with the default configuration:        ```shell      pilosa server      ```            and verify that it's running:            ```shell      curl localhost:10101/nodes      ```    3.  Follow along with the [Sample Project](https://www.pilosa.com/docs/getting-started/#sample-project) to get a better understanding of Pilosa's capabilities.      ## Data Model    Check out how the Pilosa [Data Model](https://www.pilosa.com/docs/data-model/) works.      ## Query Language    You can interact with Pilosa directly in the console using the [Pilosa Query Language](https://www.pilosa.com/docs/query-language/) (PQL).      ## Client Libraries    There are supported libraries for the following languages:  - [Go](https://www.pilosa.com/docs/client-libraries/#go)  - [Java](https://www.pilosa.com/docs/client-libraries/#java)  - [Python](https://www.pilosa.com/docs/client-libraries/#python)    ## Licenses    The core Pilosa code base and all default builds (referred to as Pilosa Community Edition) are licensed completely under the Apache License, Version 2.0.  If you build Pilosa with the `enterprise` build tag (Pilosa Enterprise Edition), then that build will include features licensed under the GNU Affero General  Public License (AGPL). Enterprise code is located entirely in the [github.com/pilosa/pilosa/enterprise](https://github.com/pilosa/pilosa/tree/master/enterprise)  directory. See [github.com/pilosa/pilosa/NOTICE](https://github.com/pilosa/pilosa/blob/master/NOTICE) and  [github.com/pilosa/pilosa/LICENSE](https://github.com/pilosa/pilosa/blob/master/LICENSE) for more information about Pilosa licenses.    ## Get Support    There are [several channels](https://www.pilosa.com/community/#support) available for you to reach out to us for support. The Slack channel (#pilosa in the [Golang](https://invite.slack.golangbridge.org/) team) is the most active.    ## Contributing    Pilosa is an open source project. Please see our [Contributing Guide](CONTRIBUTING.md) for information about how to get involved. """
Big data;https://github.com/ZEPL/zeppelin;"""# Zeppelin has moved to Apache.    Zeppelin's has moved to Apache incubator.   This github repository is not going to be synced to the ASF's one after 20/Mar/2015.  Please consider using ASF repo for source code access and JIRA for issues.  <br />  <br />  ###Please use new github repo to create pull-request.    Github : [https://github.com/apache/incubator-zeppelin](https://github.com/apache/incubator-zeppelin)      ###Please create issue on JIRA.  JIRA : [https://issues.apache.org/jira/browse/ZEPPELIN](https://issues.apache.org/jira/browse/ZEPPELIN) """
Big data;https://github.com/apache/incubator-superset;"""<!--  Licensed to the Apache Software Foundation (ASF) under one  or more contributor license agreements.  See the NOTICE file  distributed with this work for additional information  regarding copyright ownership.  The ASF licenses this file  to you under the Apache License, Version 2.0 (the  ""License""); you may not use this file except in compliance  with the License.  You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing,  software distributed under the License is distributed on an  ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY  KIND, either express or implied.  See the License for the  specific language governing permissions and limitations  under the License.  -->    # Superset    [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)  [![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/apache/superset?sort=semver)](https://github.com/apache/superset/tree/latest)  [![Build Status](https://github.com/apache/superset/workflows/Python/badge.svg)](https://github.com/apache/superset/actions)  [![PyPI version](https://badge.fury.io/py/apache-superset.svg)](https://badge.fury.io/py/apache-superset)  [![Coverage Status](https://codecov.io/github/apache/superset/coverage.svg?branch=master)](https://codecov.io/github/apache/superset)  [![PyPI](https://img.shields.io/pypi/pyversions/apache-superset.svg?maxAge=2592000)](https://pypi.python.org/pypi/apache-superset)  [![Get on Slack](https://img.shields.io/badge/slack-join-orange.svg)](https://join.slack.com/t/apache-superset/shared_invite/zt-uxbh5g36-AISUtHbzOXcu0BIj7kgUaw)  [![Documentation](https://img.shields.io/badge/docs-apache.org-blue.svg)](https://superset.apache.org)    <img    src=""https://github.com/apache/superset/raw/master/superset-frontend/src/assets/branding/superset-logo-horiz-apache.png""    alt=""Superset""    width=""500""  />    A modern, enterprise-ready business intelligence web application.    [**Why Superset?**](#why-superset) |  [**Supported Databases**](#supported-databases) |  [**Installation and Configuration**](#installation-and-configuration) |  [**Release Notes**](RELEASING/README.md#release-notes-for-recent-releases) |  [**Get Involved**](#get-involved) |  [**Contributor Guide**](#contributor-guide) |  [**Resources**](#resources) |  [**Organizations Using Superset**](RESOURCES/INTHEWILD.md)    ## Why Superset?    Superset is a modern data exploration and data visualization platform. Superset can replace or augment proprietary business intelligence tools for many teams.    Superset provides:    - A **no-code interface** for building charts quickly  - A powerful, web-based **SQL Editor** for advanced querying  - A **lightweight semantic layer** for quickly defining custom dimensions and metrics  - Out of the box support for **nearly any SQL** database or data engine  - A wide array of **beautiful visualizations** to showcase your data, ranging from simple bar charts to geospatial visualizations  - Lightweight, configurable **caching layer** to help ease database load  - Highly extensible **security roles and authentication** options  - An **API** for programmatic customization  - A **cloud-native architecture** designed from the ground up for scale    ## Screenshots & Gifs    **Large Gallery of Visualizations**    <kbd><a href=""https://superset.apache.org/gallery""><img title=""Gallery"" src=""superset-frontend/src/assets/images/screenshots/gallery.jpg""/></a></kbd><br/>    **Craft Beautiful, Dynamic Dashboards**    <kbd><img title=""View Dashboards"" src=""superset-frontend/src/assets/images/screenshots/slack_dash.jpg""/></kbd><br/>    **No-Code Chart Builder**    <kbd><img title=""Slice & dice your data"" src=""superset-frontend/src/assets/images/screenshots/explore.jpg""/></kbd><br/>    **Powerful SQL Editor**    <kbd><img title=""SQL Lab"" src=""superset-frontend/src/assets/images/screenshots/sql_lab.jpg""/></kbd><br/>    ## Supported Databases    Superset can query data from any SQL-speaking datastore or data engine (Presto, Trino, Athena, [and more](https://superset.apache.org/docs/databases/installing-database-drivers/)) that has a Python DB-API driver and a SQLAlchemy dialect.    Here are some of the major database solutions that are supported:    <p align=""center"">    <img src=""superset-frontend/src/assets/images/redshift.png"" alt=""redshift"" border=""0"" width=""106"" height=""41""/>    <img src=""superset-frontend/src/assets/images/google-biquery.png"" alt=""google-biquery"" border=""0"" width=""114"" height=""43""/>    <img src=""superset-frontend/src/assets/images/snowflake.png"" alt=""snowflake"" border=""0"" width=""152"" height=""46""/>    <img src=""superset-frontend/src/assets/images/trino.png"" alt=""trino"" border=""0"" width=""46"" height=""46""/>    <img src=""superset-frontend/src/assets/images/presto.png"" alt=""presto"" border=""0"" width=""152"" height=""46""/>    <img src=""superset-frontend/src/assets/images/druid.png"" alt=""druid"" border=""0"" width=""135"" height=""37"" />    <img src=""superset-frontend/src/assets/images/firebolt.png"" alt=""firebolt"" border=""0"" width=""133"" height=""21.5"" />    <img src=""superset-frontend/src/assets/images/timescale.png"" alt=""timescale"" border=""0"" width=""102"" height=""26.8"" />      <img src=""superset-frontend/src/assets/images/rockset.png"" alt=""rockset"" border=""0"" width=""125"" height=""51"" />    <img src=""superset-frontend/src/assets/images/postgresql.png"" alt=""postgresql"" border=""0"" width=""132"" height=""81"" />    <img src=""superset-frontend/src/assets/images/mysql.png"" alt=""mysql"" border=""0"" width=""119"" height=""62"" />    <img src=""superset-frontend/src/assets/images/mssql-server.png"" alt=""mssql-server"" border=""0"" width=""93"" height=""74"" />    <img src=""superset-frontend/src/assets/images/db2.png"" alt=""db2"" border=""0"" width=""62"" height=""62"" />    <img src=""superset-frontend/src/assets/images/sqlite.png"" alt=""sqlite"" border=""0"" width=""102"" height=""45"" />    <img src=""superset-frontend/src/assets/images/sybase.png"" alt=""sybase"" border=""0"" width=""128"" height=""47"" />    <img src=""superset-frontend/src/assets/images/mariadb.png"" alt=""mariadb"" border=""0"" width=""83"" height=""63"" />    <img src=""superset-frontend/src/assets/images/vertica.png"" alt=""vertica"" border=""0"" width=""128"" height=""40"" />    <img src=""superset-frontend/src/assets/images/oracle.png"" alt=""oracle"" border=""0"" width=""121"" height=""66"" />    <img src=""superset-frontend/src/assets/images/firebird.png"" alt=""firebird"" border=""0"" width=""86"" height=""56"" />    <img src=""superset-frontend/src/assets/images/greenplum.png"" alt=""greenplum"" border=""0"" width=""140"" height=""45"" />    <img src=""superset-frontend/src/assets/images/clickhouse.png"" alt=""clickhouse"" border=""0"" width=""133"" height=""34"" />    <img src=""superset-frontend/src/assets/images/exasol.png"" alt=""exasol"" border=""0"" width=""106"" height=""59"" />    <img src=""superset-frontend/src/assets/images/monet-db.png"" alt=""monet-db"" border=""0"" width=""106"" height=""46"" />    <img src=""superset-frontend/src/assets/images/apache-kylin.png"" alt=""apache-kylin"" border=""0"" width=""56"" height=""64""/>    <img src=""superset-frontend/src/assets/images/hologres.png"" alt=""hologres"" border=""0"" width=""71"" height=""64""/>    <img src=""superset-frontend/src/assets/images/netezza.png"" alt=""netezza"" border=""0"" width=""64"" height=""64""/>    <img src=""superset-frontend/src/assets/images/pinot.png"" alt=""pinot"" border=""0"" width=""165"" height=""64""/>    <img src=""superset-frontend/src/assets/images/teradata.png"" alt=""teradata"" border=""0"" width=""165"" height=""64""/>    <img src=""superset-frontend/src/assets/images/yugabyte.png"" alt=""yugabyte"" border=""0"" width=""180"" height=""31""/>  </p>    **A more comprehensive list of supported databases** along with the configuration instructions can be found  [here](https://superset.apache.org/docs/databases/installing-database-drivers).    Want to add support for your datastore or data engine? Read more [here](https://superset.apache.org/docs/frequently-asked-questions#does-superset-work-with-insert-database-engine-here) about the technical requirements.    ## Installation and Configuration    [Extended documentation for Superset](https://superset.apache.org/docs/installation/installing-superset-using-docker-compose)    ## Get Involved    - Ask and answer questions on [StackOverflow](https://stackoverflow.com/questions/tagged/apache-superset) using the **apache-superset** tag  - [Join our community's Slack](https://join.slack.com/t/apache-superset/shared_invite/zt-uxbh5g36-AISUtHbzOXcu0BIj7kgUaw)    and please read our [Slack Community Guidelines](https://github.com/apache/superset/blob/master/CODE_OF_CONDUCT.md#slack-community-guidelines)  - [Join our dev@superset.apache.org Mailing list](https://lists.apache.org/list.html?dev@superset.apache.org)    ## Contributor Guide    Interested in contributing? Check out our  [CONTRIBUTING.md](https://github.com/apache/superset/blob/master/CONTRIBUTING.md)  to find resources around contributing along with a detailed guide on  how to set up a development environment.    ## Resources    - Getting Started with Superset    - [Superset in 2 Minutes using Docker Compose](https://superset.apache.org/docs/installation/installing-superset-using-docker-compose#installing-superset-locally-using-docker-compose)    - [Installing Database Drivers](https://superset.apache.org/docs/databases/dockeradddrivers)    - [Building New Database Connectors](https://preset.io/blog/building-database-connector/)    - [Create Your First Dashboard](https://superset.apache.org/docs/creating-charts-dashboards/first-dashboard)    - [Comprehensive Tutorial for Contributing Code to Apache Superset  ](https://preset.io/blog/tutorial-contributing-code-to-apache-superset/)  - [Documentation for Superset End-Users (by Preset)](https://docs.preset.io/docs/terminology)  - Deploying Superset    - [Official Docker image](https://hub.docker.com/r/apache/superset)    - [Helm Chart](https://github.com/apache/superset/tree/master/helm/superset)  - Recordings of Past [Superset Community Events](https://preset.io/events)    - [Live Demo: Interactive Time-series Analysis with Druid and Superset](https://preset.io/events/2021-03-02-interactive-time-series-analysis-with-druid-and-superset/)    - [Live Demo: Visualizing MongoDB and Pinot Data using Trino](https://preset.io/events/2021-04-13-visualizing-mongodb-and-pinot-data-using-trino/)  	- [Superset Contributor Bootcamp](https://preset.io/events/superset-contributor-bootcamp-dec-21/)  	- [Introduction to the Superset API](https://preset.io/events/introduction-to-the-superset-api/)  	- [Apache Superset 1.3 Meetup](https://preset.io/events/apache-superset-1-3/)  	- [Building a Database Connector for Superset](https://preset.io/events/2021-02-16-building-a-database-connector-for-superset/)  - Visualizations    - [Building Custom Viz Plugins](https://superset.apache.org/docs/installation/building-custom-viz-plugins)    - [Managing and Deploying Custom Viz Plugins](https://medium.com/nmc-techblog/apache-superset-manage-custom-viz-plugins-in-production-9fde1a708e55)    - [Why Apache Superset is Betting on Apache ECharts](https://preset.io/blog/2021-4-1-why-echarts/)    - [Superset API](https://superset.apache.org/docs/rest-api) """
Big data;https://github.com/kairosdb/kairosdb;"""![KairosDB](webroot/img/kairosdb.png)  [![Build Status](https://travis-ci.org/kairosdb/kairosdb.svg?branch=develop)](https://travis-ci.org/kairosdb/kairosdb)    KairosDB is a fast distributed scalable time series database written on top of Cassandra.    ## Documentation    Documentation is found [here](http://kairosdb.github.io/website/).    [Frequently Asked Questions](https://github.com/kairosdb/kairosdb/wiki/Frequently-Asked-Questions)    ## Installing    Download the latest [KairosDB release](https://github.com/kairosdb/kairosdb/releases).    Installation instructions are found [here](http://kairosdb.github.io/docs/build/html/GettingStarted.html)    If you want to test KairosDB in Kubernetes please follow the instructions from [KairosDB Helm chart](deployment/helm/README.md).    ## Getting Involved    Join the [KairosDB discussion group](https://groups.google.com/forum/#!forum/kairosdb-group).    ## Contributing to KairosDB    Contributions to KairosDB are **very welcome**. KairosDB is mainly developed in Java, but there's a lot of tasks for non-Java programmers too, so don't feel shy and join us!    What you can do for KairosDB:    - [KairosDB Core](https://github.com/kairosdb/kairosdb): join the development of core features of KairosDB.  - [Website](https://github.com/kairosdb/kairosdb.github.io): improve the KairosDB website.  - [Documentation](https://github.com/kairosdb/kairosdb/wiki/Contribute:-Documentation): improve our documentation, it's a very important task.    If you have any questions about how to contribute to KairosDB, [join our discussion group](https://groups.google.com/forum/#!forum/kairosdb-group) and tell us your issue.    ## License  The license is the [Apache License 2.0](http://www.apache.org/licenses/LICENSE-2.0) """
Big data;https://github.com/YugaByte/yugabyte-db;"""<img src=""https://www.yugabyte.com/wp-content/uploads/2021/05/yb_horizontal_alt_color_RGB.png"" align=""center"" alt=""YugabyteDB"" width=""50%""/>    ---------------------------------------    [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)  [![Documentation Status](https://readthedocs.org/projects/ansicolortags/badge/?version=latest)](https://docs.yugabyte.com/)  [![Ask in forum](https://img.shields.io/badge/ask%20us-forum-orange.svg)](https://forum.yugabyte.com/)  [![Slack chat](https://img.shields.io/badge/Slack:-%23yugabyte_db-blueviolet.svg?logo=slack)](https://communityinviter.com/apps/yugabyte-db/register)  [![Analytics](https://yugabyte.appspot.com/UA-104956980-4/home?pixel&useReferer)](https://github.com/yugabyte/ga-beacon)    # What is YugabyteDB?     **YugabyteDB** is a **high-performance, cloud-native distributed SQL database** that aims to support **all PostgreSQL features**. It is best to fit for **cloud-native OLTP (i.e. real-time, business-critical) applications** that need absolute **data correctness** and require at least one of the following: **scalability, high tolerance to failures, or globally-distributed deployments.**    * [Core Features](#core-features)  * [Get Started](#get-started)  * [Build Apps](#build-apps)  * [What's being worked on?](#whats-being-worked-on)  * [Architecture](#architecture)  * [Need Help?](#need-help)  * [Contribute](#contribute)  * [License](#license)  * [Read More](#read-more)    # Core Features     * **Powerful RDBMS capabilities** Yugabyte SQL (*YSQL* for short) reuses the query layer of PostgreSQL (similar to Amazon Aurora PostgreSQL), thereby supporting most of its features (datatypes, queries, expressions, operators and functions, stored procedures, triggers, extensions, etc). Here is a detailed [list of features currently supported by YSQL](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/YSQL-Features-Supported.md).    * **Distributed transactions** The transaction design is based on the Google Spanner architecture. Strong consistency of writes is achieved by using Raft consensus for replication and cluster-wide distributed ACID transactions using *hybrid logical clocks*. *Snapshot*, *serializable* and *read committed* isolation levels are supported. Reads (queries) have strong consistency by default, but can be tuned dynamically to read from followers and read-replicas.    * **Continuous availability** YugabyteDB is extremely resilient to common outages with native failover and repair. YugabyteDB can be configured to tolerate disk, node, zone, region, and cloud failures automatically. For a typical deployment where a YugabyteDB cluster is deployed in one region across multiple zones on a public cloud, the RPO is 0 (meaning no data is lost on failure) and the RTO is 3 seconds (meaning the data being served by the failed node is available in 3 seconds).    * **Horizontal scalability** Scaling a YugabyteDB cluster to achieve more IOPS or data storage is as simple as adding nodes to the cluster.    * **Geo-distributed, multi-cloud** YugabyteDB can be deployed in public clouds and natively inside Kubernetes. It supports deployments that span three or more fault domains, such as multi-zone, multi-region, and multi-cloud deployments. It also supports xCluster asynchronous replication with unidirectional master-slave and bidirectional multi-master configurations that can be leveraged in two-region deployments. To serve (stale) data with low latencies, read replicas are also a supported feature.    * **Multi API design** The query layer of YugabyteDB is built to be extensible. Currently, YugabyteDB supports two distributed SQL APIs: **[Yugabyte SQL (YSQL)](https://docs.yugabyte.com/latest/api/ysql/)**, a fully relational API that re-uses query layer of PostgreSQL, and **[Yugabyte Cloud QL (YCQL)](https://docs.yugabyte.com/latest/api/ycql/)**, a semi-relational SQL-like API with documents/indexing support with Apache Cassandra QL roots.    * **100% open source** YugabyteDB is fully open-source under the [Apache 2.0 license](https://github.com/yugabyte/yugabyte-db/blob/master/LICENSE.md). The open-source version has powerful enterprise features such as distributed backups, encryption of data-at-rest, in-flight TLS encryption, change data capture, read replicas, and more.    Read more about YugabyteDB in our [Docs](https://docs.yugabyte.com/latest/introduction/).    # Get Started    * [Install YugabyteDB](https://docs.yugabyte.com/latest/quick-start/install/)  * [Create a local cluster](https://docs.yugabyte.com/latest/quick-start/create-local-cluster/)  * [Start with Yugabyte Cloud](https://www.yugabyte.com/cloud/)  * [Connect and try out SQL commands](https://docs.yugabyte.com/latest/quick-start/explore-ysql/)  * [Build an app](https://docs.yugabyte.com/latest/quick-start/build-apps/) using a PostgreSQL-compatible driver or ORM.  * Try running a real-world demo application:    * [Microservices-oriented e-commerce app](https://github.com/yugabyte/yugastore-java)    * [Streaming IoT app with Kafka and Spark Streaming](https://docs.yugabyte.com/latest/develop/realworld-apps/iot-spark-kafka-ksql/)    Cannot find what you are looking for? Have a question? Please post your questions or comments on our Community [Slack](https://communityinviter.com/apps/yugabyte-db/register) or [Forum](https://forum.yugabyte.com).    # Build Apps    YugabyteDB supports several languages and client drivers. Below is a brief list.    | Language  | ORM | YSQL Drivers | YCQL Drivers |  | --------- | --- | ------------ | ------------ |  | Java  | [Spring/Hibernate](https://docs.yugabyte.com/latest/quick-start/build-apps/java/ysql-spring-data/) | [PostgreSQL JDBC](https://docs.yugabyte.com/latest/quick-start/build-apps/java/ysql-jdbc/) | [cassandra-driver-core-yb](https://docs.yugabyte.com/latest/quick-start/build-apps/java/ycql/)  | Go  | [Gorm](https://github.com/yugabyte/orm-examples) | [pq](https://docs.yugabyte.com/latest/quick-start/build-apps/go/#ysql) | [gocql](https://docs.yugabyte.com/latest/quick-start/build-apps/go/#ycql)  | NodeJS  | [Sequelize](https://github.com/yugabyte/orm-examples) | [pg](https://docs.yugabyte.com/latest/quick-start/build-apps/nodejs/#ysql) | [cassandra-driver](https://docs.yugabyte.com/latest/quick-start/build-apps/nodejs/#ycql)  | Python  | [SQLAlchemy](https://github.com/yugabyte/orm-examples) | [psycopg2](https://docs.yugabyte.com/latest/quick-start/build-apps/python/#ysql) | [yb-cassandra-driver](https://docs.yugabyte.com/latest/quick-start/build-apps/python/#ycql)  | Ruby  | [ActiveRecord](https://github.com/yugabyte/orm-examples) | [pg](https://docs.yugabyte.com/latest/quick-start/build-apps/ruby/#ysql) | [yugabyte-ycql-driver](https://docs.yugabyte.com/latest/quick-start/build-apps/ruby/#ycql)  | C#  | [EntityFramework](https://github.com/yugabyte/orm-examples) | [npgsql](http://www.npgsql.org/) | [CassandraCSharpDriver](https://docs.yugabyte.com/latest/quick-start/build-apps/csharp/#ycql)  | C++ | Not tested | [libpqxx](https://docs.yugabyte.com/latest/quick-start/build-apps/cpp/#ysql) | [cassandra-cpp-driver](https://docs.yugabyte.com/latest/quick-start/build-apps/cpp/#ycql)  | C   | Not tested | [libpq](https://docs.yugabyte.com/latest/quick-start/build-apps/c/#ysql) | Not tested    # What's being worked on?    > This section was last updated in **March, 2022**.    ## Current roadmap    Here is a list of some of the key features being worked on for the upcoming releases (the YugabyteDB [**v2.13 latest release**](https://docs.yugabyte.com/latest/releases/release-notes/v2.13/) has been released in **March, 2022**, and the [**v2.12 stable release**](https://blog.yugabyte.com/announcing-yugabytedb-2-12/) was released in **Feb 2022**).    | Feature                                         | Status    | Release Target | Progress        |  Comments     |  | ----------------------------------------------- | --------- | -------------- | --------------- | ------------- |  |[Faster Bulk-Data Loading in YugabyteDB](https://github.com/yugabyte/yugabyte-db/issues/11765)| PROGRESS| v2.15 |[Track](https://github.com/yugabyte/yugabyte-db/issues/11765)| Master issue to track improvements to make it easier and faster to get large amounts of data into YugabyteDB.  |[Database-level multi-tenancy with tablegroups](https://github.com/yugabyte/yugabyte-db/issues/11665)| PROGRESS| v2.15 |[Track](https://github.com/yugabyte/yugabyte-db/issues/11665)| Master issue to track Database-level multi-tenancy using tablegroups.  |[Upgrade to PostgreSQL v13](https://github.com/yugabyte/yugabyte-db/issues/9797)| PROGRESS| v2.15 |[Track](https://github.com/yugabyte/yugabyte-db/issues/9797)| For latest features, new PostgreSQL extensions, performance, and community fixes  |Support for  [in-cluster PITR](https://github.com/yugabyte/yugabyte-db/issues/7120)  | PROGRESS| v2.15 |[Track](https://github.com/yugabyte/yugabyte-db/issues/7120)|Point in time recovery of YSQL databases, to a fixed point in time, across DDL and DML changes|  | [Automatic tablet splitting enabled by default](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/docdb-automatic-tablet-splitting.md) | PROGRESS  | v2.15 | [Track](https://github.com/yugabyte/yugabyte-db/issues/1004) |Enables changing the number of tablets (which are splits of data) at runtime.|  | YSQL-table statistics and cost based optimizer(CBO) | PROGRESS  |  v2.15 | [Track](https://github.com/yugabyte/yugabyte-db/issues/5242) | Improve YSQL query performance |  | [YSQL-Feature support - ALTER TABLE](https://github.com/yugabyte/yugabyte-db/issues/1124) | PROGRESS | v2.15 | [Track](https://github.com/yugabyte/yugabyte-db/issues/1124) | Support for various `ALTER TABLE` variants |  | [YSQL-Online schema migration](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/online-schema-migrations.md)  | PROGRESS  | v2.15 | [Track](https://github.com/yugabyte/yugabyte-db/issues/4192) | Schema migrations(includes DDL operations) to be safely run concurrently with foreground operations |  | Pessimistic locking Design | PROGRESS  | v2.15  | [Track](https://github.com/yugabyte/yugabyte-db/issues/5680) |  |  | Make [`COLOCATED` tables](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/ysql-colocated-tables.md) default for YSQL | PLANNING  |  | [Track](https://github.com/yugabyte/yugabyte-db/issues/5239)  |  |  | Support for transactions in async [xCluster replication](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/multi-region-xcluster-async-replication.md) | PLANNING  |    | [Track](https://github.com/yugabyte/yugabyte-db/issues/1808) | Apply transactions atomically on consumer cluster. |  | Support for GiST indexes | PLANNING  |    | [Track](https://github.com/yugabyte/yugabyte-db/issues/1337) |Suppor for GiST (Generalized Search Tree) based index|    ## Recently released features    | Feature                                         | Status    | Release Target | Docs / Enhancements |  Comments     |  | ----------------------------------------------- | --------- | -------------- | ------------------- | ------------- |  |[Change Data Capture](https://github.com/yugabyte/yugabyte-db/issues/9019)|  âœ… *DONE*| v2.13 ||Change data capture (CDC) allows multiple downstream apps and services to consume the continuous and never-ending stream(s) of changes to Yugabyte databases|  |[Support for materalized views](https://github.com/yugabyte/yugabyte-db/issues/10102) |  âœ… *DONE*| v2.13 |[Docs](https://docs.yugabyte.com/latest/explore/ysql-language-features/advanced-features/views/#materialized-views)|A materialized view is a pre-computed data set derived from a query specification and stored for later use|  |[Geo-partitioning support](https://github.com/yugabyte/yugabyte-db/issues/9980) for the transaction status table | âœ… *DONE*| v2.13 |[Docs](https://docs.yugabyte.com/latest/explore/multi-region-deployments/row-level-geo-partitioning/)|Instead of central remote transaction execution metatda, it is now optimized for access from different regions. Since the transaction metadata is also geo partitioned, it eliminates the need for round-trip to remote regions to update transaction statuses.|  | Transparently restart transactions |  âœ… *DONE*| v2.13 | |Decrease the incidence of transaction restart errors seen in various scenarios |  | [Row-level geo-partitioning](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/ysql-row-level-partitioning.md) |  âœ… *DONE*| v2.13 |[Docs](https://docs.yugabyte.com/latest/explore/multi-region-deployments/row-level-geo-partitioning/)|Row-level geo-partitioning allows fine-grained control over pinning data in a user table (at a per-row level) to geographic locations, thereby allowing the data residency to be managed at the table-row level.|  | [YSQL-Support `GIN` indexes](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/ysql-gin-indexes.md) |  âœ… *DONE*  | v2.11 | [Docs](https://docs.yugabyte.com/latest/explore/ysql-language-features/gin/) | Support for generalized inverted indexes for container data types like jsonb, tsvector, and array |  | [YSQL-Collation Support](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/ysql-collation-support.md)  | âœ… *DONE*  | v2.11           |[Docs](https://docs.yugabyte.com/latest/explore/ysql-language-features/collations/) |Allows specifying the sort order and character classification behavior of data per-column, or even per-operation according to language and country-specific rules           |  [YSQL-Savepoint Support](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/savepoints.md)  |  âœ… *DONE*  | v2.11     |[Docs](https://docs.yugabyte.com/latest/explore/ysql-language-features/savepoints/) | Useful for implementing complex error recovery in multi-statement transaction|  | [xCluster replication management through Platform](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/platform-xcluster-replication-management.md) | âœ… *DONE* | v2.11           |   [Docs](https://docs.yugabyte.com/latest/yugabyte-platform/create-deployments/async-replication-platform/)     |     | [Spring Data YugabyteDB module](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/spring-data-yugabytedb.md) | âœ… *DONE*  | v2.9 | [Track](https://github.com/yugabyte/yugabyte-db/issues/7956) | Bridges the gap for learning the distributed SQL concepts with familiarity and ease of Spring Data APIs |  | Support Liquibase, Flyway, ORM schema migrations | âœ… *DONE* | v2.9           |           [Docs](https://blog.yugabyte.com/schema-versioning-in-yugabytedb-using-flyway/)      |   | [Support `ALTER TABLE` add primary key](https://github.com/yugabyte/yugabyte-db/issues/1124) | âœ… *DONE* | v2.9 | [Track](https://github.com/yugabyte/yugabyte-db/issues/1124) |  |  | [YCQL-LDAP Support](https://github.com/yugabyte/yugabyte-db/issues/4421) |  âœ… *DONE*  | v2.8           |[Docs](https://docs.yugabyte.com/latest/secure/authentication/ldap-authentication-ycql/#root)  | support LDAP authentication in YCQL API |               | [Platform Alerting and Notification](https://blog.yugabyte.com/yugabytedb-2-8-alerts-and-notifications/) | âœ… *DONE* | v2.8  |  [Docs](https://docs.yugabyte.com/latest/yugabyte-platform/alerts-monitoring/alert/) |  To get notified in real time about database alerts, user defined alert policies notify you when a performance metric rises above or falls below a threshold you set.|        | [Platform API](https://blog.yugabyte.com/yugabytedb-2-8-api-automated-operations/) | âœ… *DONE* | v2.8           |   [Docs](https://api-docs.yugabyte.com/docs/yugabyte-platform/ZG9jOjIwMDY0MTA4-platform-api-overview)              |   Securely Deploy YugabyteDB Clusters Using Infrastructure-as-Code|                # Architecture    <img src=""https://raw.githubusercontent.com/yugabyte/yugabyte-db/master/architecture/images/yb-architecture.jpg"" align=""center"" alt=""YugabyteDB Architecture""/>    Review detailed architecture in our [Docs](https://docs.yugabyte.com/latest/architecture/).    # Need Help?    * You can ask questions, find answers, and help others on our Community [Slack](https://communityinviter.com/apps/yugabyte-db/register), [Forum](https://forum.yugabyte.com), [Stack Overflow](https://stackoverflow.com/questions/tagged/yugabyte-db), as well as Twitter [@Yugabyte](https://twitter.com/yugabyte)    * Please use [GitHub issues](https://github.com/yugabyte/yugabyte-db/issues) to report issues or request new features.     * To Troubleshoot YugabyteDB, cluser/node level isssues, Please refer to [Troubleshooting documentation](https://docs.yugabyte.com/latest/troubleshoot/)    # Contribute    As an an open-source project with a strong focus on the user community, we welcome contributions as GitHub pull requests. See our [Contributor Guides](https://docs.yugabyte.com/latest/contribute/) to get going. Discussions and RFCs for features happen on the design discussions section of [our Forum](https://forum.yugabyte.com).    # License    Source code in this repository is variously licensed under the Apache License 2.0 and the Polyform Free Trial License 1.0.0. A copy of each license can be found in the [licenses](licenses) directory.    The build produces two sets of binaries:    * The entire database with all its features (including the enterprise ones) are licensed under the Apache License 2.0  * The  binaries that contain `-managed` in the artifact and help run a managed service are licensed under the Polyform Free Trial License 1.0.0.    > By default, the build options generate only the Apache License 2.0 binaries.    # Read More    * To see our updates, go to [The Distributed SQL Blog](https://blog.yugabyte.com/).  * For an in-depth design and the YugabyteDB architecture, see our [design specs](https://github.com/yugabyte/yugabyte-db/tree/master/architecture/design).  * Tech Talks and [Videos](https://www.youtube.com/c/YugaByte).  * See how YugabyteDB [compares with other databases](https://docs.yugabyte.com/latest/comparisons/). """
Big data;https://github.com/paulhoule/infovore;"""Overview  --------    Infovore is an RDF processing system that uses Hadoop to process RDF data  sets in the billion triple range and beyond.  Infovore was originally designed to process  the (old) proprietary Freebase dump into RDF,  but once Freebase came out with an official RDF  dump,  Infovore gained the ability to clean and purify the dump,  making it not just possible  but easy to process Freebase data with triple stores such as Virtuoso 7.    Every week we run Infovore in Amazon Elastic/Map reduce in order to produce a product known as  [:BaseKB](http://basekb.com/).    Infovore depends on the [Centipede](https://github.com/paulhoule/centipede/wiki) framework for packaging  and processing command-line arguments.  The [Telepath](https://github.com/paulhoule/telepath/wiki) project  extends the Infovore project in order to process Wikipedia usage information to produce a product called  [:SubjectiveEye3D](https://github.com/paulhoule/telepath/wiki/SubjectiveEye3D).      Supporting  ----------    It costs several hundreds of dollars per month to process and store files in connection with this work.  Please join <a href=""https://www.gittip.com/"">Gittip</a> and make a <a href=""https://www.gittip.com/paulhoule/"">small weekly donation</a> to keep this data free.      Building  --------    Infovore software requires JDK 7.    mvn clean install    Installing  ----------    The following cantrip, run from the top level ""infovore"" directory, initializes the bash shell  for the use of the ""haruhi"" program,  which can be used to run Infovore applications  packaged in the Bakemono Jar.    source haruhi/target/path.sh    More Information  ----------------    See     https://github.com/paulhoule/infovore/wiki     for documentation and join the discussion group at    https://groups.google.com/forum/#!forum/infovore-basekb         """
Big data;https://github.com/UnderstandLingBV/Tuktu;"""# Tuktu - Big Data Science Swiss Army Knife    [![Join the chat at https://gitter.im/UnderstandLingBV/Tuktu](https://badges.gitter.im/UnderstandLingBV/Tuktu.svg)](https://gitter.im/UnderstandLingBV/Tuktu?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![Build Status](https://travis-ci.org/UnderstandLingBV/Tuktu.svg?branch=master)](https://travis-ci.org/UnderstandLingBV/Tuktu) [![Release](https://img.shields.io/github/release/UnderstandLingBV/Tuktu.svg)](https://github.com/UnderstandLingBV/Tuktu/releases/latest) [![Docker Automated build](https://img.shields.io/docker/automated/understandling/tuktu.svg?maxAge=2592000)](https://hub.docker.com/r/understandling/tuktu)    Tuktu is created and officially maintained by [UnderstandLing Intellect](http://www.understandling.com).    ![UnderstandLing Logo](images/ul.png)    # Documentation    For documentation on Tuktu, please see [http://www.tuktu.io](http://www.tuktu.io).    # Docker   Start the container (have [docker](https://www.docker.com/) installed) by running:    - docker pull understandling/tuktu  - docker run -p 9000:9000 -p 2552:2552 --name tuktu -d understandling/tuktu"""
Big data;https://github.com/intel-hadoop/HiBench;"""# HiBench Suite  ## The bigdata micro benchmark suite ##      * Current version: 7.1.1  * Homepage: https://github.com/intel-hadoop/HiBench  * Contents:    1. Overview    2. Getting Started    3. Workloads    4. Supported Releases    ---  ### OVERVIEW ###    HiBench is a big data benchmark suite that helps evaluate different big data frameworks in terms of speed, throughput and system resource utilizations. It contains a set of Hadoop, Spark and streaming workloads, including Sort, WordCount, TeraSort, Repartition, Sleep, SQL, PageRank, Nutch indexing, Bayes, Kmeans, NWeight and enhanced DFSIO, etc. It also contains several streaming workloads for Spark Streaming, Flink, Storm and Gearpump.    ### Getting Started ###   * [Build HiBench](docs/build-hibench.md)   * [Run HadoopBench](docs/run-hadoopbench.md)   * [Run SparkBench](docs/run-sparkbench.md)   * [Run StreamingBench](docs/run-streamingbench.md) (Spark streaming, Flink, Storm, Gearpump)    ### Workloads ###    There are totally 29 workloads in HiBench. The workloads are divided into 6 categories which are micro, ml(machine learning), sql, graph, websearch and streaming.      **Micro Benchmarks:**    1. Sort (sort)        This workload sorts its *text* input data, which is generated using RandomTextWriter.    2. WordCount (wordcount)        This workload counts the occurrence of each word in the input data, which are generated using RandomTextWriter. It is representative of another typical class of real world MapReduce jobs - extracting a small amount of interesting data from large data set.    3. TeraSort (terasort)        TeraSort is a standard benchmark created by Jim Gray. Its input data is generated by Hadoop TeraGen example program.        4. Repartition (micro/repartition)            This workload benchmarks shuffle performance. Input data is generated by Hadoop TeraGen. The workload randomly selects the post-shuffle partition for each record, performs shuffle write and read, evenly repartitioning the records. There are 2 parameters providing options to eliminate data source & sink I/Os: hibench.repartition.cacheinmemory(default: false) and hibench.repartition.disableOutput(default: false), controlling whether or not to 1) cache the input in memory at first 2) write the result to storage    5. Sleep (sleep)        This workload sleep an amount of seconds in each task to test framework scheduler.    6. enhanced DFSIO (dfsioe)        Enhanced DFSIO tests the HDFS throughput of the Hadoop cluster by generating a large number of tasks performing writes and reads simultaneously. It measures the average I/O rate of each map task, the average throughput of each map task, and the aggregated throughput of HDFS cluster. Note: this benchmark doesn't have Spark corresponding implementation.      **Machine Learning:**    1. Bayesian Classification (Bayes)        Naive Bayes is a simple multiclass classification algorithm with the assumption of independence between every pair of features. This workload is implemented in spark.mllib and uses the automatically generated documents whose words follow the zipfian distribution. The dict used for text generation is also from the default linux file /usr/share/dict/linux.words.ords.    2. K-means clustering (Kmeans)        This workload tests the K-means (a well-known clustering algorithm for knowledge discovery and data mining) clustering in spark.mllib. The input data set is generated by GenKMeansDataset based on Uniform Distribution and Guassian Distribution. There is also an optimized K-means implementation based on DAL (Intel Data Analytics Library), which is available in the dal module of sparkbench.        3. Gaussian Mixture Model (GMM)         Gaussian Mixture Model represents a composite distribution whereby points are drawn from one of k Gaussian sub-distributions, each with its own probability. It's implemented in spark.mllib. The input data set is generated by GenKMeansDataset based on Uniform Distribution and Guassian Distribution.        4. Logistic Regression (LR)        Logistic Regression (LR) is a popular method to predict a categorical response. This workload is implemented in spark.mllib with LBFGS optimizer and the input data set is generated by LogisticRegressionDataGenerator based on random balance decision tree. It contains three different kinds of data types, including categorical data, continuous data, and binary data.    5. Alternating Least Squares (ALS)        The alternating least squares (ALS) algorithm is a well-known algorithm for collaborative filtering. This workload is implemented in spark.mllib and the input data set is generated by RatingDataGenerator for a product recommendation system.    6. Gradient Boosted Trees (GBT)        Gradient-boosted trees (GBT) is a popular regression method using ensembles of decision trees. This workload is implemented in spark.mllib and the input data set is generated by GradientBoostedTreeDataGenerator.    7. XGBoost (XGBoost)        XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. This workload is implemented with XGBoost4J-Spark API in spark.mllib and the input data set is generated by GradientBoostedTreeDataGenerator.        8. Linear Regression (Linear)        Linear Regression (Linear) is a workload that implemented in spark.ml with ElasticNet. The input data set is generated by LinearRegressionDataGenerator.    9. Latent Dirichlet Allocation (LDA)        Latent Dirichlet allocation (LDA) is a topic model which infers topics from a collection of text documents. This workload is implemented in spark.mllib and the input data set is generated by LDADataGenerator.    10. Principal Components Analysis (PCA)        Principal component analysis (PCA) is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible. PCA is used widely in dimensionality reduction. This workload is implemented in spark.ml. The input data set is generated by PCADataGenerator.    11. Random Forest (RF)        Random forests (RF) are ensembles of decision trees. Random forests are one of the most successful machine learning models for classification and regression. They combine many decision trees in order to reduce the risk of overfitting. This workload is implemented in spark.mllib and the input data set is generated by RandomForestDataGenerator.    12. Support Vector Machine (SVM)        Support Vector Machine (SVM) is a standard method for large-scale classification tasks. This workload is implemented in spark.mllib and the input data set is generated by SVMDataGenerator.    13. Singular Value Decomposition (SVD)        Singular value decomposition (SVD) factorizes a matrix into three matrices. This workload is implemented in spark.mllib and its input data set is generated by SVDDataGenerator.      **SQL:**    1. Scan (scan) 2. Join (join), 3. Aggregate (aggregation)        These workloads are developed based on SIGMOD 09 paper ""A Comparison of Approaches to Large-Scale Data Analysis"" and HIVE-396. It contains Hive queries (Aggregation and Join) performing the typical OLAP queries described in the paper. Its input is also automatically generated Web data with hyperlinks following the Zipfian distribution.    **Websearch Benchmarks:**    1. PageRank (pagerank)        This workload benchmarks PageRank algorithm implemented in Spark-MLLib/Hadoop (a search engine ranking benchmark included in pegasus 2.0) examples. The data source is generated from Web data whose hyperlinks follow the Zipfian distribution.    2. Nutch indexing (nutchindexing)        Large-scale search indexing is one of the most significant uses of MapReduce. This workload tests the indexing sub-system in Nutch, a popular open source (Apache project) search engine. The workload uses the automatically generated Web data whose hyperlinks and words both follow the Zipfian distribution with corresponding parameters. The dict used to generate the Web page texts is the default linux dict file.    **Graph Benchmark:**    1. NWeight (nweight)         NWeight is an iterative graph-parallel algorithm implemented by Spark GraphX and pregel. The algorithm computes associations between two vertices that are n-hop away.       **Streaming Benchmarks:**    1. Identity (identity)        This workload reads input data from Kafka and then writes result to Kafka immediately, there is no complex business logic involved.    2. Repartition (streaming/repartition)        This workload reads input data from Kafka and changes the level of parallelism by creating more or fewer partitions. It tests the efficiency of data shuffle in the streaming frameworks.        3. Stateful Wordcount (wordcount)        This workload counts words cumulatively received from Kafka every few seconds. This tests the stateful operator performance and Checkpoint/Acker cost in the streaming frameworks.        4. Fixwindow (fixwindow)        The workloads performs a window based aggregation. It tests the performance of window operation in the streaming frameworks.            ### Supported Hadoop/Spark/Flink/Storm/Gearpump releases: ###      - Hadoop: Apache Hadoop 3.0.x, 3.1.x, 3.2.x, 2.x, CDH5, HDP    - Spark: Spark 2.4.x, Spark 3.0.x, Spark 3.1.x    - Flink: 1.0.3    - Storm: 1.0.1    - Gearpump: 0.8.1    - Kafka: 0.8.2.2    ---     """
Big data;https://github.com/rudderlabs/rudder-server;"""<p align=""center"">    <a href=""https://rudderstack.com/"">      <img src=""resources/logo.png"">    </a>  </p>    <p align=""center""><b>The Customer Data Platform for Developers</b></p>    <p align=""center"">    <a href=""https://rudderstack.com/"">      <img src=""https://codebuild.us-east-1.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiT01EQkVPc0NBbDJLV2txTURidkRTMTNmWFRZWUY2dEtia3FRVmFXdXhWeUwzaC9aV3dsWWNNT0NwaVZKd1hKTFVMazB2cDQ5UHlaZTgvbFRER3R5SXRvPSIsIml2UGFyYW1ldGVyU3BlYyI6IktJQVMveHIzQnExZVE5b0YiLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=master"">    </a>    <a href=""https://github.com/rudderlabs/rudder-server/releases"">      <img src=""https://img.shields.io/github/v/release/rudderlabs/rudder-server?color=blue&sort=semver"">    </a>    <a href=""https://rudderstack.com/docs/get-started/installing-and-setting-up-rudderstack/docker/"">      <img src=""https://img.shields.io/docker/pulls/rudderlabs/rudder-server"">    </a>    <a href=""https://github.com/rudderlabs/rudder-server/blob/master/LICENSE"">      <img src=""https://img.shields.io/github/license/rudderlabs/rudder-server"">    </a>  </p>    <p align=""center"">    <b>      <a href=""https://rudderstack.com"">Website</a>      Â·      <a href=""https://rudderstack.com/docs"">Documentation</a>      Â·      <a href=""https://rudderstack.com/blog"">Blog</a>      Â·      <a href=""https://rudderstack.com/join-rudderstack-slack-community"">Slack</a>      Â·      <a href=""https://twitter.com/rudderstack"">Twitter</a>    </b>  </p>    ---    As the leading open source Customer Data Platform (CDP), [**RudderStack**](https://rudderstack.com/) provides data pipelines that make it easy to collect data from every application, website and SaaS platform, then activate it in your warehouse and business tools.    With RudderStack, you can build customer data pipelines that connect your whole customer data stack and then make them smarter by triggering enrichment and activation in customer tools based on analysis in your data warehouse. It's easy-to-use SDKs and event source integrations, Cloud Extract integrations, transformations, and expansive library of destination and warehouse integrations makes building customer data pipelines for both event streaming and cloud-to-warehouse ELT simple.    <p align=""center"">    <a href=""https://rudderstack.com"">      <img src=""https://user-images.githubusercontent.com/59817155/121468374-4ef91e00-c9d8-11eb-8611-28bea18f609d.gif"" alt=""RudderStack"">    </a>  </p>    | Try **RudderStack Cloud Free** - a free tier of [**RudderStack Cloud**](https://rudderstack.com/cloud). Click [**here**](https://app.rudderlabs.com/signup?type=freetrial) to start building a smarter customer data pipeline today, with RudderStack Cloud. |  |:------|    ## Key features    - **Warehouse-first**: RudderStack treats your data warehouse as a first class citizen among destinations, with advanced features and configurable, near real-time sync.    - **Developer-focused**: RudderStack is built API-first. It integrates seamlessly with the tools that the developers already use and love.    - **High Availability**: RudderStack comes with at least 99.99% uptime. We have built a sophisticated error handling and retry system that ensures that your data will be delivered even in the event of network partitions or destinations downtime.    - **Privacy and Security**: You can collect and store your customer data without sending everything to a third-party vendor. With RudderStack, you get fine-grained control over what data to forward to which analytical tool.     - **Unlimited Events**: Event volume-based pricing of most of the commercial systems is broken. With RudderStack, you are be able to collect as much data as possible without worrying about overrunning your event budgets.    - **Segment API-compatible**: RudderStack is fully compatible with the Segment API. So you don't need to change your app if you are using Segment; just integrate the RudderStack SDKs into your app and your events will keep flowing to the destinations (including data warehouses) as before.    - **Production-ready**: Companies like Mattermost, IFTTT, Torpedo, Grofers, 1mg, Nana, OnceHub, and dozens of large companies use RudderStack for collecting their events.    - **Seamless Integration**: RudderStack currently supports integration with over 90 popular [**tool**](https://rudderstack.com/docs/destinations/) and [**warehouse**](https://rudderstack.com/docs/data-warehouse-integrations/) destinations.    - **User-specified Transformation**: RudderStack offers a powerful JavaScript-based event transformation framework which lets you enhance or transform your event data by combining it with your other internal data. Furthermore, as RudderStack runs inside your cloud or on-premise environment, you can easily access your production data to join with the event data.    ## Get started    The easiest way to experience RudderStack is to [**sign up**](https://app.rudderlabs.com/signup?type=freetrial) for **RudderStack Cloud Free** - a completely free tier of [**RudderStack Cloud**](https://rudderstack.com/cloud).    You can also set up RudderStack on your platform of choice with these two easy steps:    ### Step 1: Set up RudderStack    - [**Docker**](https://rudderstack.com/docs/get-started/installing-and-setting-up-rudderstack/docker/)  - [**Kubernetes**](https://rudderstack.com/docs/get-started/installing-and-setting-up-rudderstack/kubernetes/)  - [**Developer machine setup**](https://rudderstack.com/docs/get-started/installing-and-setting-up-rudderstack/developer-machine-setup/)    > **Note**: If you are planning to use RudderStack in production, we STRONGLY recommend using our Kubernetes Helm charts. We update our Docker images with bug fixes much more frequently than our GitHub repo.    ### Step 2: Verify the installation    Once you have installed RudderStack, [**send test events**](https://rudderstack.com/docs/get-started/installing-and-setting-up-rudderstack/sending-test-events/) to verify the setup.    ## Architecture    RudderStack is an independent, stand-alone system with a dependency only on the database (PostgreSQL). Its backend is written in **Go** with a rich UI written in **React.js**.    A high-level view of RudderStackâ€™s architecture is shown below:    ![Architecture](resources/rudder-server-architecture.png)    For more details on the various architectural components, refer to our [**documentation**](https://rudderstack.com/docs/get-started/rudderstack-architecture/).    ## Contribute    We would love to see you contribute to RudderStack. Get more information on how to contribute [**here**](https://github.com/rudderlabs/rudder-server/blob/master/CONTRIBUTING.md).    ## License    RudderStack server is released under the [**AGPLv3 License**](https://github.com/rudderlabs/rudder-server/blob/master/LICENSE).    Read [**our blog**](https://rudderstack.com/blog/rudderstacks-licensing-explained) to know more about how our software is licensed. """
Big data;https://github.com/ottogroup/schedoscope;"""*Schedoscope is no longer under development by OttoGroup. Feel free to fork!*    # ![Schedoscope](https://raw.githubusercontent.com/wiki/ottogroup/schedoscope/images/schedoscope_logo.jpg)    ## Introduction    Schedoscope is a scheduling framework for painfree agile development, testing, (re)loading, and monitoring of your datahub, datalake, or whatever you choose to call your Hadoop data warehouse these days.    Schedoscope makes the headache go away you are certainly going to get when having to frequently rollout and retroactively apply changes to computation logic and data structures in your datahub with traditional ETL job schedulers such as Oozie.    With Schedoscope,  * you never have to create DDL and schema migration scripts;  * you do not have to manually determine which data must be deleted and recomputed in face of retroactive changes to logic or data structures;  * you specify Hive table structures (called ""views""), partitioning schemes, storage formats, dependent views, as well as transformation logic in a concise Scala DSL;  * you have a wide range of options for expressing data transformations - from file operations and MapReduce jobs to Pig scripts, Hive queries, Spark jobs, and Oozie workflows;  * you benefit from Scala's static type system and your IDE's code completion to make less typos that hit you late during deployment or runtime;  * you can easily write unit tests for your transformation logic in [ScalaTest](http://www.scalatest.org/) and run them quickly right out of your IDE;  * you schedule jobs by expressing the views you need - Schedoscope takes care that all required dependencies - and only those-  are computed as well;  * you can easily  export view data in parallel to external systems such as Redis caches, JDBC, or Kafka topics;  * you have Metascope - a nice metadata management and data lineage tracing tool - at your disposal;  * you achieve a higher utilization of your YARN cluster's resources because job launchers are not YARN applications themselves that consume cluster capacitity.     ## Getting Started    Get a glance at     - [Schedoscope's features](https://github.com/ottogroup/schedoscope/wiki/Schedoscope-at-a-Glance)    Build it:         [~]$ git clone https://github.com/ottogroup/schedoscope.git       [~]$ cd schedoscope       [~/schedoscope]$  MAVEN_OPTS='-XX:MaxPermSize=512m -Xmx1G' mvn clean install         Follow the Open Street Map tutorial to install and run Schedoscope in a standard Hadoop distribution image:    - [Open Street Map Tutorial](https://github.com/ottogroup/schedoscope/wiki/Open%20Street%20Map%20Tutorial)    Take a look at the View DSL Primer to get more information about the capabilities of the Schedoscope DSL:    - [Schedoscope View DSL Primer](https://github.com/ottogroup/schedoscope/wiki/Schedoscope%20View%20DSL%20Primer)    Read more about how Schedoscope actually performs its scheduling work:    - [Schedoscope Scheduling](https://github.com/ottogroup/schedoscope/wiki/Scheduling)    More documentation can be found here:    - [Schedoscope Wiki](https://github.com/ottogroup/schedoscope/wiki)    Check out Metascope! It's an add-on to Schedoscope for collaborative metadata management, data discovery, exploration, and data lineage tracing:    - [Metascope Primer](https://github.com/ottogroup/schedoscope/wiki/Metascope%20Primer)    ![Metascope](https://raw.githubusercontent.com/wiki/ottogroup/schedoscope/images/view-lineage.png)    ## When is Schedoscope not for you?    Schedoscope is based on the following assumptions:  * data are largely relational and meaningfully representable as Hive tables;  * there is enough cluster time and capacity to actually allow for retroactive recomputation of data;  * it is acceptable to compile table structures, dependencies, and transformation logic into what is effectively a project-specific scheduler;  * it is acceptable that your scheduler creates and possibly drops Hive tables and databases it manages.    Should any of those assumptions not hold in your context, you should probably look for a different scheduler.    ## Origins    Schedoscope was conceived at the Business Intelligence department of [Otto Group](http://www.ottogroup.com/en/die-otto-group/)    ## Contributions    The following people have contributed to the various parts of Schedoscope so far:     [Utz Westermann](https://github.com/utzwestermann) (maintainer), [Kassem Tohme](https://github.com/ktohme), [Alexander Kolb](https://github.com/lofifnc), [Christian Richter](https://github.com/christianrichter), [Jan Hicken](https://github.com/janhicken), [Diogo Aurelio](https://github.com/diogoaurelio), [Hans-Peter Zorn](https://github.com/hpzorn), [Dominik Benz](https://github.com/dominikbenz), [Annika Seidler](https://github.com/aleveringhaus), [Martin SÃ¤nger](https://github.com/martinsaenger), [Julian Keppel](https://github.com/juliankeppel).    We would love to get contributions from you as well. We haven't got a formalized submission process yet. If you have an idea for a contribution or even coded one already, get in touch with Utz or just send us your pull request. We will work it out from there.    Please help making Schedoscope better!    ## News    ###### 05/08/2018 - Release 0.10.2    We have released Version 0.10.2 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    We have changed the materialization logic of `materializeOnce` views such that they no longer ask their child views to materialize if the `materializeOnce` views have been materialized already. This improves performance.    ###### 04/24/2018 - Release 0.10.1    We have released Version 0.10.1 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This is a bugfix release correcting the order of the TBLPROPERTIES and LOCATION clauses in the Hive DDL generated for views. **Please do note that if you use the `tblProperties` clause in some views, this change affects the DDL checksum making Schedoscope drop and recreate the respective tables.** Hence the version bump to 0.10.1.    Thanks to Julian Keppel for reporting the issue and providing the fix.    ###### 04/06/2018 - Release 0.9.13    We have released Version 0.9.13 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Removed derelict indirect CDH5.12.0 dependencies incurred by Cloudera's Spark 2.2.0-Cloudera2 dependency.    ###### 03/15/2018 - Release 0.9.11    We have released Version 0.9.11 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Added [configuration parameter](https://github.com/ottogroup/schedoscope/wiki/Configuring-Schedoscope) `schedoscope.export.disableAll` to globally disable all view exports. Useful in test environments.    ###### 03/09/2018 - Release 0.9.10    We have released Version 0.9.10 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Upgraded Cloudera dependencies to CDH 5.14.0.    ###### 01/25/2018 - Release 0.9.9    We have released Version 0.9.9 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Export your views to Google Cloud Platform's BigQuery via a simple [`exportAs()`](https://github.com/ottogroup/schedoscope/wiki/Google-BigQuery-Export) statement.    BigQuery export now compresses view data before sending it off to Google Cloud Storage.    ###### 01/24/2018 - Release 0.9.7    We have released Version 0.9.7 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Optimized performance of BigQuery export by moving more work to the map phase of the export job.     ###### 01/23/2018 - Release 0.9.6    We have released Version 0.9.6 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Corrected a problem with command line argument construction within BigQuery `exportAs()` clauses in a Kerberized cluster.    ###### 01/22/2018 - Release 0.9.5    We have released Version 0.9.5 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Export your views to Google Cloud Platform's BigQuery via a simple [`exportAs()`](https://github.com/ottogroup/schedoscope/wiki/Google-BigQuery-Export) statement.    ###### 10/12/2017 - Release 0.9.4    We have released Version 0.9.4 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Emergency bug fix for Schedoscope crashing upon exports. Do not use 0.9.3!    ###### 10/11/2017 - Release 0.9.3    We have released Version 0.9.3 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Minor bug fix. Show view name in resource manager also for transformations of views that have exportAs statements.    ###### 09/21/2017 - Release 0.9.2    We have released Version 0.9.2 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Minor bug fixes. Improved Metascope performance by optionally circumventing the Hive Metastore API and accessing the Metastore DB directly.    ###### 08/17/2017 - Release 0.9.1    We have released Version 0.9.1 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    We fixed a bug in the Spark driver that could lead to incomplete consumption of the error stream of the Spark submit subprocess resulting in transformation freezes.      ###### 08/11/2017 - Release 0.9.0    We have released Version 0.9.0 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This release upgrades Spark transformations from Spark version 1.6.0 to Spark version 2.2.0 based on Cloudera's CDH 5.12 Spark 2.2 beta parcel. As a consequence, Schedoscope has been lifted to Scala 2.11 and JDK8 as well.     This is an incompatible change likely requiring adaptation of Spark jobs, dependencies, and build pipelines of existing Schedoscope projects - hence the incrememtation of the minor release number.    ###### 08/04/2017 - Release 0.8.9    We have released Version 0.8.9 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This release contains the following enhancements and changes:  * Cloudera client libraries updated to CDH-5.12.0;  * a [DistCp transformation](https://github.com/ottogroup/schedoscope/wiki/DistCp-Transformations) for view materialization by parallel, cross-cluser file copying;  * a new [development mode](https://github.com/ottogroup/schedoscope/wiki/Development-Mode) setup that helps developers to easily copy data from a production environment to the direct dependencies of the view they are developing;  * shell transformations had to be moved back into `schedoscope-core` to facilitate development mode;  * a versioning issue with the Scala Maven compiler plugin with regard to Scala 2.10 was fixed so that finally Schedoscope compiles and runs under JDK8 as well.    ###### 07/04/2017 - Release 0.8.7    We have released Version 0.8.7 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This version contains a critical Metascope bugfix introduced with the last version preventing startup. Also, finally Metascope field lineage documentation has been provided in the [View DSL Primer](https://github.com/ottogroup/schedoscope/wiki/Schedoscope-View-DSL-Primer) and the [Metascope Primer](https://github.com/ottogroup/schedoscope/wiki/Metascope-Primer).    ###### 06/23/2017 - Release 0.8.6    We have released Version 0.8.6 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This version includes support for field level data lineage - automatically inferred from Hive transformations, declaratively specifyable for other transformations - in Metascope. Also, Metascope lineage graph rendering has been reworked. Extensive documentation to come.    Schedoscope now fails immediately if a driver specified in schedoscope.conf cannot be found on the classpath.    ###### 05/26/2017 - Release 0.8.5    We have released Version 0.8.5 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This version adds support for float view fields to JDBC exports    ###### 05/24/2017 - Release 0.8.4    We have released Version 0.8.4 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This version removes a race condition the file system driver initialization that seems to have been introduced with CDH-5.10. Also, we have changed the way how we delete and recreate output folders for Map/Reduce transformations to avoid Hive partitions pointing to temporarily non-existing folders.    ###### 04/24/2017 - Release 0.8.3    We have released Version 0.8.3 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This version has been built against Cloudera's CDH 5.10.1 client libraries. The test framework no longer artificially sets the storage formats of views under test to text, making testing of Spark jobs writing Parquet files simpler. The robustness of the Schedoscope HTTP service has been improved in face of invalid view parameters.    ###### 03/24/2017 - Release 0.8.2    We have released Version 0.8.2 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This version provides significant performance improvements when initializing the scheduling state for a large number of views.    ###### 03/18/2017 - Release 0.8.1    We have released Version 0.8.1 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This fixes a critical bug that could result in applying commands to all views in a table and not just the ones addressed. *Do not use Release 0.8.0*    ###### 03/17/2017 - Release 0.8.0      We have released Version 0.8.0 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Schedoscope 0.8.0 includes, among other things:    * significant rework of Schedoscope's actor system that supports testing and uses significantly fewer actors reducing stress for poor Akka;  * support for a lot more [Hive storage formats](https://github.com/ottogroup/schedoscope/wiki/Storage-Formats);  * definition of arbitrary [Hive table properties / SerDes](https://github.com/ottogroup/schedoscope/wiki/Storage-Formats);  * stability, performance, and UI improvements to Metascope;  * the names of views being transformed appear as the job name in the Hadoop resource manager.    Please note that Metascope's database schema has changed with this release, so back up your database before deploying.    ## Community / Forums    - Google Groups: [Schedoscope Users](https://groups.google.com/forum/#!forum/schedoscope-users)  - Twitter: @Schedoscope    ## Build Status    [![Build Status](https://travis-ci.org/ottogroup/schedoscope.svg?branch=master)](https://travis-ci.org/ottogroup/schedoscope)    ## License  Licensed under the [Apache License 2.0](https://github.com/ottogroup/schedoscope/blob/master/LICENSE) """
Big data;https://github.com/akumuli/Akumuli;"""README [![Build Status](https://travis-ci.org/akumuli/Akumuli.svg?branch=master)](https://travis-ci.org/akumuli/Akumuli) [![Coverity Scan Build Status](https://scan.coverity.com/projects/8879/badge.svg)](https://scan.coverity.com/projects/akumuli)   ======    **Akumuli** is a time-series database for modern hardware.   It can be used to capture, store and process time-series data in real-time.   The word ""akumuli"" can be translated from Esperanto as ""accumulate"".    Features  -------    * Column-oriented storage.  * Based on novel [LSM and B+tree hybrid datastructure](http://akumuli.org/akumuli/2017/04/29/nbplustree/) with multiversion concurrency control (no concurrency bugs, parallel writes, optimized for SSD and NVMe).  * Supports both metrics and events.  * Fast and effecient compression algorithm that outperforms 'Gorilla' time-series compression.  * Crash safety and recovery.  * Fast aggregation without pre-configured rollups or materialized views.  * Many queries can be executed without decompressing the data.  * Compressed in-memory storage for recent data.  * Can be used as a server application or embedded library.  * Simple API based on JSON and HTTP.  * Fast range scans and joins, read speed doesn't depend on database cardinality.  * Fast data ingestion:    * 5.4M writes/sec on DigitalOcean droplet with 8-cores 32GB of RAM (using only 6 cores)    * 4.6M writes/sec on DigitalOcean droplet with 8-cores 32GB of RAM (6 cores with enabled WAL)    * 16.1M writes/sec on 32-core Intel Xeon E5-2680 v2 (c3.8xlarge EC2 instance).  * Queries are executed lazily. Query results are produced as long as client reads them.  * Compression algorithm and input parsers are fuzz-tested on every code change.  * Grafana [datasource](https://github.com/akumuli/akumuli-datasource) plugin.  * Fast and compact inverted index for time-series lookup.      Roadmap  ------    |Storage engine features        |Current version|Future versions|  |-------------------------------|---------------|---------------|  |Inserts                        |In order       |Out of order   |  |Updates                        |-              |+              |  |Deletes                        |-              |+              |  |MVCC                           |+              |+              |  |Compression                    |+              |+              |  |Tags                           |+              |+              |  |High-throughput ingestion      |+              |+              |  |High cardinality               |+              |+              |  |Crash recovery                 |+              |+              |  |Incremental backup             |-              |+              |  |Clustering                     |-              |+              |  |Replication                    |-              |+              |  |ARM support                    |+              |+              |  |Windows support                |-              |+              |    |Query language features        |Current version|Future versions|  |-------------------------------|---------------|---------------|  |Range scans                    |+              |+              |  |Merge series                   |+              |+              |  |Aggregate series               |+              |+              |  |Merge & aggregate              |+              |+              |  |Group-aggregate                |+              |+              |  |Group-aggregate & merge        |+              |+              |  |Join                           |+              |+              |  |Join & merge                   |-              |+              |  |Join & group-aggregate         |-              |+              |  |Join & group-aggregate & merge |-              |+              |  |Filter by value                |+              |+              |  |Filter & group-aggregate       |+              |+              |  |Filter & join                  |+              |+              |      Gettings Started  ----------------  * You can find [documentation](https://akumuli.gitbook.io/docs) here  * [Installation & build instructions](https://akumuli.gitbook.io/docs/getting-started)  * [Getting started guide](https://akumuli.gitbook.io/docs/getting-started)  * [Writing data](https://akumuli.gitbook.io/docs/writing-data)    Supported Platforms  -------------------    Akumuli supports 64 and 32-bit Intel processors. It also works on 64 and 32-bit ARM processors but these architectures are not covered by continous integration.    Pre-built [Debian/RPM packages](https://packagecloud.io/Lazin/Akumuli) for the following platforms  are available via packagecloud:    * AMD 64 Ubuntu 14.04  * AMD 64 Ubuntu 16.04  * AMD 64 Ubuntu 18.04  * AMD 64 Debian Jessie  * AMD 64 Debian Stretch  * AMD 64 CentOS 7  * ARM 64 Ubuntu 16.04  * ARM 64 Ubuntu 18.04  * ARM 64 CentOS 7    Docker image is availabe through [Docker Hub](https://hub.docker.com/r/akumuli/akumuli/tags/).    Tools for monitoring  --------------------    Akumuli supports OpenTSDB telnet-style API for writing. This means that many collectors works with it  without any trouble, for instance `netdata`, `collectd`, and `tcollector`. Grafana  [datasource](https://github.com/akumuli/akumuli-datasource) plugin is availabe as well.  Akumuli can be used as a long-term storage for Prometheus using [akumuli-prometheus-adapter](https://github.com/akumuli/akumuli-prometheus-adapter).    [Google group](https://groups.google.com/forum/#!forum/akumuli) """
Big data;https://github.com/dalmatinerdb/dalmatinerdb;"""Read more at the [official site](https://dalmatiner.io/) and the [documentation](https://docs.dalmatiner.io).    # DalmatinerDB  DalmatinerDB is a metric database written in pure Erlang. It takes advantage of some special properties of metrics to make some tradeoffs. The goal is to make a store for metric data (time, value of a metric) that is fast, has a low overhead, and is easy to query and manage.    # Tradeoffs  I try here to be explicit about the tradeoffs we made, so people can decide if they are happy with them (costs vs gains). The acceptable tradeoffs differ from case to case, but I hope the choices we made fit a metric store quite well. If you are comparing DalmatinerDB with X, please don't assume that just because X does not list the tradeoffs they made, they have none; be inquisitive and make a decision based on facts, not marketing.    A detailed comparison between databases can be found here:    [https://docs.google.com/spreadsheets/d/1sMQe9oOKhMhIVw9WmuCEWdPtAoccJ4a-IuZv4fXDHxM/edit#gid=0](https://docs.google.com/spreadsheets/d/1sMQe9oOKhMhIVw9WmuCEWdPtAoccJ4a-IuZv4fXDHxM/edit#gid=0)    ## Let the Filesystem handle it  A lot of work is handed down to the file system, ZFS is exceptionally smart and can do things like checksums, compressions and caching very well. Handing down these tasks to the filesystem simplifies the codebase, and builds on very well tested and highly performant code, instead of trying to reimplement it.    ## Prioritise the overall writes over individual ones  DalmatinerDB offers a 'best effort' on storing the metrics, there is no log for writes (if enabled in ZFS, the ZIL (ZFS Intent Log) can log write operations) or forced sync after each write. This means that if your network fails, packets can get lost, and if your server crashes, unwritten data can be lost.    The point is that losing one or two metric points in a huge series is a non-problem, the importance of a metric is often seen in aggregates, and DalmatinerDB fills in the blanks with the last written value. However there is explicitly no guarantee that data is written, so *this can be an issue if every single point of metric is of importance!*    ## Flat files  Data is stored in a flat binary format, this means that reads and writes can be calculated to a filename+offset by simple math, there is no need for traversing data-structures. This means however that if a metric stops unwritten, points can 'linger' around for a while depending on how the file size was picked.    As an example: if metrics are stored with a precision down to the second, and 1 week of data is stored per file, up to one week of unused data can be stored, but it should be taken into account that with compression this data will be compressed quite well. """
Big data;https://github.com/monksy/awesome-kafka;"""# awesome-kafka    This list is for anyone wishing to learn about [Apache Kafka](http://kafka.apache.org/), but do not have a starting point.    #### How to contribute    Fork the repository, create a contribution, and create a pull request against monksy/awesome-kafka:master.      Table of Contents  =================       * [Learning/Resources](learning.md)     * [Tools/Utilities/Monitoring](tools.md)     * [Client Libraries](clients.md)     * [Libraries with Kafka Support](libraries.md)     * [Kafka Connectors](connectors.md)     * [Testing](testing.md)     * [Projects with Kafka Integrations](integrations.md)     * [Social Resources](social.md)       ## Requests for Help/TODO      - Learning: Organize articles into tutorials, case studies, product related, reference, etc   - Create a section for resources that will help to quickstart learning for kafka and kafka streams   - Add about sections per each page.   - Merge in repo: https://github.com/semantalytics/awesome-kafka   - Find Example projects with kafka   - Organize learning.md by the types/subsystem that they are using   - Add section headers   - Find blogs that are dedicated to Kafka   - Create sections for all the sub-sections      - Kafka Streams       - KSQL       - Kafka Connect      - etc   - Solicit for help from the community    - Get involved with Kafka community (gitter/slack/irc/mailing list)   - Fix links in learning   - Verify links work     ## Where did this information come from?     - Originally from: https://github.com/infoslack/awesome-kafka   - Client Libraries, Tools, etc from https://github.com/dharmeshkakadia/awesome-kafka   - Awesome-data-engineering list for kafka: https://github.com/monksy/awesome-data-engineering"""
Big data;https://github.com/improbable-eng/thanos;"""<p align=""center""><img src=""docs/img/Thanos-logo_fullmedium.png"" alt=""Thanos Logo""></p>    [![Latest Release](https://img.shields.io/github/release/thanos-io/thanos.svg?style=flat-square)](https://github.com/thanos-io/thanos/releases/latest) [![Go Report Card](https://goreportcard.com/badge/github.com/thanos-io/thanos)](https://goreportcard.com/report/github.com/thanos-io/thanos) [![Go Code reference](https://img.shields.io/badge/code%20reference-go.dev-darkblue.svg)](https://pkg.go.dev/github.com/thanos-io/thanos?tab=subdirectories) [![Slack](https://img.shields.io/badge/join%20slack-%23thanos-brightgreen.svg)](https://slack.cncf.io/) [![Netlify Status](https://api.netlify.com/api/v1/badges/664a5091-934c-4b0e-a7b6-bc12f822a590/deploy-status)](https://app.netlify.com/sites/thanos-io/deploys) [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3048/badge)](https://bestpractices.coreinfrastructure.org/projects/3048)    [![CI](https://github.com/thanos-io/thanos/workflows/CI/badge.svg)](https://github.com/thanos-io/thanos/actions?query=workflow%3ACI) [![CI](https://circleci.com/gh/thanos-io/thanos.svg?style=svg)](https://circleci.com/gh/thanos-io/thanos) [![go](https://github.com/thanos-io/thanos/workflows/go/badge.svg)](https://github.com/thanos-io/thanos/actions?query=workflow%3Ago) [![react](https://github.com/thanos-io/thanos/workflows/react/badge.svg)](https://github.com/thanos-io/thanos/actions?query=workflow%3Areact) [![docs](https://github.com/thanos-io/thanos/workflows/docs/badge.svg)](https://github.com/thanos-io/thanos/actions?query=workflow%3Adocs) [![Gitpod ready-to-code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/thanos-io/thanos)    ## Overview    Thanos is a set of components that can be composed into a highly available metric system with unlimited storage capacity, which can be added seamlessly on top of existing Prometheus deployments.    Thanos is a [CNCF](https://www.cncf.io/) Incubating project.    Thanos leverages the Prometheus 2.0 storage format to cost-efficiently store historical metric data in any object storage while retaining fast query latencies. Additionally, it provides a global query view across all Prometheus installations and can merge data from Prometheus HA pairs on the fly.    Concretely the aims of the project are:    1. Global query view of metrics.  2. Unlimited retention of metrics.  3. High availability of components, including Prometheus.    ## Getting Started    * **[Getting Started](https://thanos.io/tip/thanos/getting-started.md/)**  * [Design](https://thanos.io/tip/thanos/design.md/)  * [Blog posts](docs/getting-started.md#blog-posts)  * [Talks](docs/getting-started.md#talks)  * [Proposals](docs/proposals-done)  * [Integrations](docs/integrations.md)    ## Features    * Global querying view across all connected Prometheus servers  * Deduplication and merging of metrics collected from Prometheus HA pairs  * Seamless integration with existing Prometheus setups  * Any object storage as its only, optional dependency  * Downsampling historical data for massive query speedup  * Cross-cluster federation  * Fault-tolerant query routing  * Simple gRPC ""Store API"" for unified data access across all metric data  * Easy integration points for custom metric providers    ## Architecture Overview    Deployment with Sidecar:    ![Sidecar](https://docs.google.com/drawings/d/e/2PACX-1vTBFKKgf8YDInJyRakPE8eZZg9phTlOsBB2ogNkFvhNGbZ8YDvz_cGMbxWZBG1G6hpsQfSX145FpYcv/pub?w=960&h=720)    Deployment with Receive:    ![Receive](https://docs.google.com/drawings/d/e/2PACX-1vTfko27YB_3ab7ZL8ODNG5uCcrpqKxhmqaz3lW-yhGN3_oNxkTrqXmwwlcZjaWf3cGgAJIM4CMwwkEV/pub?w=960&h=720)    ## Thanos Philosophy    The philosophy of Thanos and our community is borrowing much from UNIX philosophy and the golang programming language.    * Each subcommand should do one thing and do it well    * eg. thanos query proxies incoming calls to known store API endpoints merging the result  * Write components that work together    * e.g. blocks should be stored in native prometheus format  * Make it easy to read, write, and, run components    * e.g. reduce complexity in system design and implementation    ## Releases    Main branch should be stable and usable. Every commit to main builds docker image named `main-<date>-<sha>` in [quay.io/thanos/thanos](https://quay.io/repository/thanos/thanos) and [thanosio/thanos dockerhub (mirror)](https://hub.docker.com/r/thanosio/thanos)    We also perform minor releases every 6 weeks.    During that, we build tarballs for major platforms and release docker images.    See [release process docs](docs/release-process.md) for details.    ## Contributing    Contributions are very welcome! See our [CONTRIBUTING.md](CONTRIBUTING.md) for more information.    ## Community    Thanos is an open source project and we value and welcome new contributors and members of the community. Here are ways to get in touch with the community:    * Slack: [#thanos](https://slack.cncf.io/)  * Issue Tracker: [GitHub Issues](https://github.com/thanos-io/thanos/issues)    ## Adopters    See [`Adopters List`](website/data/adopters.yml).    ## Maintainers    See [MAINTAINERS.md](MAINTAINERS.md) """
Big data;https://github.com/onurakpolat/awesome-analytics;"""# Awesome Analytics [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)    A curated list of awesome analytics platforms, resources and other awesomeness. Inspired by [awesome-bigdata](https://github.com/onurakpolat/awesome-bigdata).     Your feedback and contributions are always welcome! Maintained by [@onurakpolat](https://github.com/onurakpolat) & [@koconder](https://github.com/koconder)    - [Awesome Analytics](#awesome-analytics)      - [General analytics](#general-analytics)      - [Real-time](#real-time)      - [Website analytics](#website-analytics)      - [Endpoints](#endpoints)      - [SEO](#seo)      - [Privacy focused analytics](#privacy-focused-analytics)      - [Heatmap analytics](#heatmap-analytics)      - [Analytics layers](#analytics-layers)      - [Mobile analytics](#mobile-analytics)      - [App store analytics](#app-store-analytics)      - [Attribution tracking](#attribution-tracking)      - [Social media analytics](#social-media-analytics)      - [Analytics dashboards](#analytics-dashboards)      - [Developer analytics](#developer-analytics)        - [Other Awesome Lists](#other-awesome-lists)    ## General analytics  * [userTrack](https://www.usertrack.net/) - Self-hosted web analytics with heatmaps, session-recordings, A/B tests and more. `Â©` `Self-Hosted` `PHP`  * [Panelbear](https://panelbear.com/) - free real-time website analytics. Supports custom event tracking, email digests, and site speed metrics. `Â©` `SaaS`  * [PostHog](https://posthog.com) - Open-source product analytics to track users, events, funnels and trends. Alternative to Mixpanel/Amplitude/Heap. ([Source Code](https://github.com/posthog/posthog)) `MIT` `Python`  * [Hotjar](https://www.hotjar.com/) - new and easy way to truly understand your web and mobile site visitors. `Â©` `SaaS`  * [Matomo](https://matomo.org/) - Leading open-source analytics platform that gives you more than just powerful analytics, formerly known as Piwik. ([Source Code](https://github.com/matomo-org/)) `GPL-3.0` `PHP`  * [Heap](https://heap.io) - tracks your app users, clicks, form submissions, and anything else. `Â©` `SaaS`  * [Opentracker](http://www.opentracker.net/) - real time reporting, geo-location user tracking. `Â©` `SaaS`  * [FoxMetrics](http://foxmetrics.com/) - analytics to track your userâ€™s actions and activities. `Â©` `SaaS`  * [Adobe Analytics](https://www.adobe.com/analytics/web-analytics.html) - web data into insights that everyone can act on. `Â©` `SaaS`  * [Google Analytics](https://www.google.com/analytics/) - de facto standard for analytics in the web analytics space. `Â©` `SaaS`  * [Screpy](https://screpy.com) - Screpy is a web analyzer and monitoring tool. Its powered by Google Lighthouse. `Â©` `SaaS`  * [Clicktale](https://www.clicktale.com) - record and watch exactly how a visitor used your website. `Â©` `SaaS`  * [GoSquared](https://www.gosquared.com/) - analytics with visitor tagging to help you dig deeper into one userâ€™s visit. `Â©` `SaaS`  * [Clicky](http://clicky.com/) - track visits and conversions, you can also track your video and audio analytics. `Â©` `SaaS`  * [Woopra](https://www.woopra.com/) - track where your users are coming from. `Â©` `SaaS`  * [Mint](https://haveamint.com/) - self-hosted analytics solution (no longer on sale).  `Â©` `SaaS`  * [Going Up](https://www.goingup.com/) - manage SEO analytics and web app analytics with one tool. `Â©` `SaaS`  * [Chartbeat](https://chartbeat.com/) - beautiful, real-time app analytics tool for web apps. `Â©` `SaaS`  * [Gauges](http://get.gaug.es/) - real-time web analytics tool. `Â©` `SaaS`  * [Indicative](https://www.indicative.com/) - Web & mobile analytics tool, with heavy emphasis on segmentation and funnel visualization. `Â©` `SaaS`  * [Open Web Analytics](http://www.openwebanalytics.com/) - Google Analytics and Matomo alternative. ([Source Code](https://github.com/padams/Open-Web-Analytics)) `GPL-2.0` `PHP`  * [Statcounter](https://statcounter.com/) - one of the ORIGINAL web analytics tools available. `Â©` `SaaS`  * [Adobe Digital Analytics](http://www.adobe.com/data-analytics-cloud/analytics/capabilities.html) - standard analytics tools plus some that large organizations can use. `Â©` `SaaS`  * [Hitslink.com](https://www.hitslink.com/) - real-time analytics, social media traffic reporting, and real-time dynamic segmentation. `Â©` `SaaS`  * [parse.ly](https://www.parse.ly) - real-time web analytics tool with a focus on tracking content. `Â©` `SaaS`  * [Loggr](http://loggr.net/) -  track your user events and monitor your web app. `Â©` `SaaS`  * [Kissmetrics](https://www.kissmetrics.com/) - real-time standard cohort analysis tool. `Â©` `SaaS`  * [Sitemeter](http://sitemeter.com/) - old analytics tool. `Â©` `SaaS`  * [Crawl Track](http://www.crawltrack.net/) - another old analytics tool. `Â©` `SaaS`  * [Sitespect](https://www.sitespect.com/) - full-suite web app analytics tool including A/B testing. `Â©` `SaaS`  * [Rakam](https://rakam.io/) - Custom analytics platform that lets you to create your own analytics service. Integrate with any data source (web, mobile, IoT etc.), analyze data with SQL and create dashboards. ([Source Code](https://github.com/rakam-io/rakam)) `Apache-2.0` `Java`  * [Metabase](https://www.metabase.com) - opensource analytics/BI tool  `Â©` `SaaS`  * [LiveSession](https://livesession.io) - session replay user analytics. `Â©` `SaaS`  * [Glassbox](https://glassboxdigital.com/) - customer experince and session recording analytics. `Â©` `SaaS`  * [Redash](https://redash.io/) - open source analytics/BI tool `Â©` `SaaS`  * [AWStats](http://www.awstats.org/) - Generates web, streaming, ftp or mail server statistics graphically. ([Source Code](https://github.com/eldy/awstats)) `GPL-3.0` `Perl`  * [Countly](https://count.ly) - Real time mobile and web analytics, crash reporting and push notifications platform. ([Source Code](https://github.com/countly)) `AGPL-3.0` `Javascript`  * [Druid](http://druid.io/) - Distributed, column-oriented, real-time analytics data store. ([Source Code](https://github.com/druid-io/druid)) `Apache-2.0` `Java`  * [Hastic](https://hastic.io) - Hackable time series pattern recognition tool with UI for Grafana. ([Source Code](https://github.com/hastic)) `Apache-2.0` `Python/Nodejs`  * [EDA](https://eda.jortilles.com/en/jortilles-english/) - Open source analytics/BI tool.  ([Source Code](https://github.com/jortilles/EDA)) `Apache-2.0` `Angular/Nodejs`  * [Count](https://count.co/) - notebook-based analytics platform, use SQL or drag-and-drop to build queries. `Â©` `SaaS`    ## Real-time    * [GoAccess](http://goaccess.io/) - Real-time web log analyzer and interactive viewer that runs in a terminal. ([Source Code](https://github.com/allinurl/goaccess)) `GPL-2.0` `C`    ## Website analytics    * [KISSS](https://kis3.dev) - Very minimalistic (KISS) website statistics tool. ([Source Code](https://github.com/kis3/kis3)) `MIT` `Go`    ## Endpoints  * [Census](https://getcensus.com/) - The easiest way to sync your customer data from your cloud data warehouse to SaaS applications like Salesforce, Marketo, HubSpot, Zendesk, etc. Census is the operational analytics platform that syncs your data warehouse with all your favorite apps. Get your customer success, sales & marketing teams on the same page by keeping customer data in sync. No engineering favors requiredâ€”just SQL. `SaaS`  * [RudderStack](https://rudderstack.com/) - The warehouse-first customer data platform (CDP) that builds your CDP on your data warehouse for you. RudderStack makes it easy to collect, unify, transform, and store your customer data as well as route it securely to a wide range of common, popular marketing, sales, and product tools (open-source alternative to Segment et al.). ([Source Code](https://github.com/rudderlabs/rudder-server/)) `AGPL-3.0` `Go`  * [Snowplow](http://snowplowanalytics.com/) - Analytics tool for web apps with a lot of data. Have every single event, from your websites, mobile apps, desktop applications and server-side systems, stored in your own data warehouse and available to action in real-time. ([Source Code](https://github.com/snowplow/)) `Apache-2.0` `Scala` `real-time`    ## SEO  * [Serposcope](https://serposcope.serphacker.com/) - Serposcope is a free and open-source rank tracker to monitor websites ranking in Google and improve your SEO performances. ([Source Code](https://github.com/serphacker/serposcope)) `MIT` `Java`    ## Privacy focused analytics    * [Fathom](https://usefathom.com/) - Fathom Analytics provides simple, useful websites stats without tracking or storing personal data of your users `Â©` `SaaS`  * [Plausible Analytics](https://plausible.io/) - Lightweight and [open source](https://github.com/plausible-insights/plausible) web analytics. Doesnâ€™t use cookies and doesn't track personal data. A privacy-friendly alternative to Google Analytics. ([Source Code](https://github.com/plausible/analytics/)) `MIT` `Elixir`  * [GoatCounter](https://www.goatcounter.com) - Easy web statistics without tracking of personal data; `SaaS` `Self-Hosted` ([Source Code](https://github.com/zgoat/goatcounter)) `EUPL-1.2` `Go`  * [Simple Analytics](https://simpleanalytics.io/) - Simple, clean, and friendly analytics for developers `Â©` `SaaS`  * [Nibspace](https://nibspace.com/) - Affordable, lightweight, privacy-friendly website analytics `Â©` `SaaS`  * [Metrical](https://metrical.xyz/) - A privacy-first web analytics tool for everyone. `Â©` `SaaS`  * [Shynet](https://github.com/milesmcc/shynet) - Modern, privacy-friendly, and detailed web analytics that works without cookies or JS. Designed for self-hosting. `Apache-2.0` `Python`  * [Umami](https://umami.is/) - Umami is a simple, easy to use, self-hosted web analytics solution. The goal is to provide you with a friendlier, privacy-focused alternative to Google Analytics and a free, open-sourced alternative to paid solutions. ([Demo](https://app.umami.is/share/ISgW2qz8/flightphp.com), [Source Code](https://github.com/mikecao/umami)) `MIT` `Nodejs`  * [Koko Analytics](https://www.kokoanalytics.com/) - Privacy-friendly and open source analytics plugin for WordPress. ([Source Code](https://github.com/ibericode/koko-analytics/)) `GPL-3.0` `PHP`  * [Offen](https://www.offen.dev/) - Offen is a fair and open web analytics tool. Gain insights while your users have full access to their data. Lightweight, self hosted and free. ([Demo](https://www.offen.dev/try-demo/), [Source Code](https://github.com/offen/offen)) `Apache-2.0` `Go/Docker`  * [Freshlytics](https://github.com/sheshbabu/freshlytics) - Privacy respecting, cookie free and low resource usage analytics platform. `MIT` `Docker/Nodejs`  * [Kindmetrics](https://kindmetrics.io/) - Clean privacy-focused website analytics. ([Source Code](https://github.com/kindmetrics/kindmetrics)) `MIT` `Crystal`  * [Ackee](https://ackee.electerious.com) - Self-hosted analytics tool for those who care about privacy. ([Demo](http://demo.ackee.electerious.com), [Source Code](https://github.com/electerious/Ackee)) `MIT` `Nodejs`  * [piratepx](https://www.piratepx.com/) - Just a little analytics insight for your personal or indie project. 100% free and open source. ([Demo](https://app.piratepx.com/shared/bGQbUJ-YADC_xIGZaYmyqp-J_PD6O1pkCdHmYdIjUvs53ExsImlzFeou4MCuZRbH), [Source](https://github.com/piratepx/app)) `MIT` `Nodejs`  * [Piwik PRO](https://piwik.pro/) - A privacy-friendly alternative to Google Analytics with built-in consent management. Hosted in EU, in your private cloud or on-premises. `Â©` `SaaS` `self-hosted`    ## Heatmap analytics    * [Crazyegg](http://www.crazyegg.com/) - a heatmaps only analytics tool. `Â©` `SaaS`  * [Inspeclet](https://www.inspectlet.com/) - another web app heatmaps tool. `Â©` `SaaS`  * [Mouseflow](http://mouseflow.com/) - live analytics and heatmaps. `Â©` `SaaS`  * [Session Cam](http://www.sessioncam.com/) - heatmaps analytics tool. `Â©` `SaaS`    ## Analytics layers    * [Keen.io](http://keen.io/) - custom-analytics API. `Â©` `SaaS`  * [Popcorn Metrics](http://www.popcornmetrics.com/) - visual editor to capture events and send to other platforms. `Â©` `SaaS`  * [Segment](https://segment.com/) - helps you integrate multiple app analytics tool with one piece of code. `Â©` `SaaS`  * [Iteratively](https://iterative.ly/) - capture clean product analytics consistently across teams & platforms. `Â©` `SaaS`  * [Analytics npm package](https://getanalytics.io/) - A lightweight, extendable analytics library designed to work with any third-party analytics provider to track page views, custom events, & identify users. Works in browsers & node.js. `Â©` `SaaS`    ## Mobile analytics    The tools listed here are not necessarily mobile analytics tools only. However they show a strong mobile focus.    * [Upsight](http://www.upsight.com/) - mobile app analytics tool for developers. `Â©` `SaaS`  * [Appsflyer](http://www.appsflyer.com/) - all-in-one marketing tool with analytics. `Â©` `SaaS`  * [Amazon Pinpoint](https://aws.amazon.com/pinpoint/) - Amazons multi-platform, basic mobile analytics tool. `Â©` `SaaS`  * [Tapstream](https://tapstream.com/) - user lifecycle analytics. `Â©` `SaaS`  * [Honeytracks](https://honeytracks.com/) - mobile app analytics for games. `Â©` `SaaS`  * [Apsalar](https://apsalar.com/) - analytics tool for larger app shops. `Â©` `SaaS`  * [Roambi](http://www.roambi.com/) - 3-in-1 analytics tool that helps you track analytics, handle mobile app business intelligence, and app reporting. `Â©` `SaaS`  * [Appcelerator](http://www.appcelerator.com/platform/appcelerator-analytics/) - entire mobile app marketing suite. `Â©` `SaaS`  * [Flurry](http://www.flurry.com/) - pretty much the â€œindustry standardâ€ for mobile app analytics. `Â©` `SaaS`  * [Countly](http://count.ly/) - open source mobile & web application analytics tool. `Â©` `SaaS`  * [Playtomatic](http://playtomic.org/) - mobile app open source analytics tool for games. `Â©` `SaaS`  * [Capptain](http://www.capptain.com/) - real-time analytics tool with segmentation and push. `Â©` `SaaS`  * [Amplitude](https://amplitude.com/) - real-time mobile analytics with all data provided in redshift. `Â©` `SaaS`  * [Appsee](http://www.appsee.com/) - mobile app analytics platform automatically tracks all users' interactions in your app `Â©` `SaaS`  * [Mixpanel](https://mixpanel.com/) - fully featured mobile analytics platform with segmentation and push. `Â©` `SaaS`  * [Localytics](http://www.localytics.com/) - fast and beautiful real-time mobile analytics platform with in-app and push. `Â©` `SaaS`  * [GameAnalytics](http://www.gameanalytics.com/) - leading game analytics platform. `Â©` `SaaS`  * [Swrve](https://swrve.com) - mobile analytics with segmentation, push, A/B testing and rich messaging `Â©` `SaaS`  * [Firebase](https://firebase.google.com/features/) - a free and unlimited analytics solution for android and iOS `Â©` `SaaS`  * [Liquid](https://onliquid.com/) - real-time mobile analytics, personalization, multivariate testing, audience segmentation and push. `Â©` `SaaS`    ## App store analytics    * [Appfigures](http://appfigures.com/) - app store analytics to track sales, reviews and rankings with an API. `Â©` `SaaS`  * [Appannie](http://www.appannie.com/) - track your app data from iTunes, Google Play & Amazon. `Â©` `SaaS`  * [Distimo](http://www.distimo.com/) - free app store analytics (acquired by [Appannie](http://www.appannie.com/)). `Â©` `SaaS`  * [Priori Data](https://prioridata.com/) - track and benchmark the performance of apps on Apple- and Play store. `Â©` `SaaS`  * [Asking Point](http://www.askingpoint.com/mobile-app-rating-widget) - track your mobile app user ratings. `Â©` `SaaS`  * [Apptrace](http://www.apptrace.com/) - fast and free app store analytics platform. `Â©` `SaaS`    ## Attribution tracking    * [Adjust](http://adjust.com/) - open-source SDK with sophisticated analysis and campaign tracking. `Â©` `SaaS`  * [Clickmeter](https://clickmeter.com) - analytics tool that helps you track marketing campaigns. `Â©` `SaaS`  * [HasOffers Mobile app tracking](http://www.mobileapptracking.com/) - attribution analytics platform. `Â©` `SaaS`    ## Social media analytics    Often there is no clear differentiation between social media management and analytics as most the tools provide analytics.    * [Brandwatch](http://www.brandwatch.com/) - Social media monitoring and analytics. `Â©` `SaaS`  * [Falconsocial](http://www.falconsocial.com/) - communications platform built on social media with analytics. `Â©` `SaaS`  * [Quintly](https://www.quintly.com/) - web-based tool to help you track, benchmark and optimize your social media performance. `Â©` `SaaS`  * [Kred](http://kred.com/) - Klout-like social media analytics platform. `Â©` `SaaS`  * [Buffer](https://bufferapp.com/) - Social media publishing and analytics platform. `Â©` `SaaS`  * [Topsy](http://topsy.com/) - Social analytics tool with search. `Â©` `SaaS`  * [SocialBlade](http://socialblade.com/) - premiere YouTube statistics tracking. `Â©` `SaaS`  * [Hootsuite](https://hootsuite.com/) - Social media management dashbaord. `Â©` `SaaS`  * [Sproutsocial](http://sproutsocial.com/) - Social media management and analytics platform. `Â©` `SaaS`    ## Developer analytics    * [GitSpo](https://gitspo.com/) - Analytics for Open-Source. `Â©` `SaaS`  * [Pull Panda](https://pullpanda.com/analytics) - Metrics and insights for engineering teams `Â©` `SaaS`  * [Screenful](https://screenful.com/) - Visualise and share your project progress `Â©` `SaaS`  * [Haystack](https://usehaystack.io) - Metrics and insights for engineering teams `Â©` `SaaS`  * [GitSpo](https://gitspo.com/) - Analytics for Open-Source. `Â©` `SaaS`  * [Pull Panda](https://pullpanda.com/analytics) - Metrics and insights for engineering teams `Â©` `SaaS`  * [Plandek](https://plandek.com) - Metrics and insights for software delivery `Â©` `SaaS`  * [Screenful](https://screenful.com/) - Visualise and share your project progress `Â©` `SaaS`  * [Moiva.io](https://moiva.io/) - A dashboard with charts and graphs to evaluate and compare any npm package. `Â©` `SaaS`    ## Analytics dashboards    * [Freeboard](https://github.com/Freeboard/freeboard) - open source real-time dashboard builder for IOT and other web mashups. `Â©` `SaaS`  * [Geckboard](https://www.geckoboard.com/) - dashboard for key metrics in one place. `Â©` `SaaS`  * [Klipfolio](https://www.klipfolio.com/) - Klipfolio is an online dashboard platform for building powerful real-time business dashboards for your team or your clients. `Â©` `SaaS`  * [Vizia](https://www.brandwatch.com/products/vizia/) - Visual command center dashboarding solution `Â©` `SaaS`  * [Metabase](https://metabase.com/) - Metabase is the easy, open source way for everyone in your company to ask questions and learn from data. Simple Dashboarding and GUI Query tool, Nightly Emails and Slack Integration w/ PostgreSQL, MySQL, Redshift and other DBs. ([Source Code](https://github.com/metabase/metabase)) `AGPL-3.0` `Java`  * [Chartbrew](https://chartbrew.com) - Chartbrew allows you to query your databases and APIs to create live charts and visualize your data. You can share your charts with anyone or embed them on your own sites, blogs, Notion, etc. ([Demo](https://app.chartbrew.com/live-demo), [Source Code](https://github.com/chartbrew/chartbrew)) `MIT` `NodeJS`  * [Redash](http://redash.io) - connect to over 18 types of databases (SQL and ""NoSQL""), query your data, visualize it and create dashboards. Everything has a URL that can be shared. Slack and HipChat integration. ([Demo](https://demo.redash.io), [Source Code](https://github.com/getredash/redash)) `BSD-2-Clause` `Python`  * [Superset](http://superset.apache.org/) - Modern, enterprise-ready business intelligence web application. ([Source Code](https://github.com/apache/incubator-superset)) `Apache-2.0` `Python`  * [Socioboard](https://socioboard.org/) - `âš ` Social media management, analytics, and reporting platform supporting nine social media networks out-of-the-box. ([Source Code](https://github.com/socioboard/Socioboard-4.0)) `GPL-3.0` `C#/JavaScript`  * [EDA](https://eda.jortilles.com/en/jortilles-english/) - EDA is an user friendly Analtical Tool specially designed for busines users.  ([Source Code](https://github.com/jortilles/EDA)) `Apache-2.0` `Angular/Nodejs`    # Other Awesome Lists  - Other awesome lists [awesome-awesomeness](https://github.com/bayandin/awesome-awesomeness).  - Even more lists [awesome](https://github.com/sindresorhus/awesome).  - Another list? [list](https://github.com/jnv/lists).  - WTF! [awesome-awesome-awesome](https://github.com/t3chnoboy/awesome-awesome-awesome).  - Analytics [awesome-bigdata](https://github.com/onurakpolat/awesome-bigdata). """
Big data;https://github.com/senseidb/zoie;"""What is Zoie  ===============    Zoie is a realtime search/indexing system written in Java.      ------------------------------------    ### Wiki    Wiki is available at:     [http://linkedin.jira.com/wiki/display/ZOIE/Home](http://linkedin.jira.com/wiki/display/ZOIE/Home)    ### Issues    Issues are tracked at:     [http://linkedin.jira.com/browse/ZOIE](http://linkedin.jira.com/browse/ZOIE)    ### Release    Maven:    groupId: com.senseidb.zoie    artifactId: zoie-core    Latest Version: 3.0.0 """
Big data;https://github.com/baidu/tera;"""# Tera - An Internet-Scale Database    [![Build Status](https://travis-ci.org/baidu/tera.svg?branch=master)](https://travis-ci.org/baidu/tera)  [![Coverity Scan Build Status](https://scan.coverity.com/projects/10959/badge.svg)](https://scan.coverity.com/projects/tera)  [![Documentation Status](https://img.shields.io/badge/ä¸­æ–‡æ–‡æ¡£-æœ€æ–°-brightgreen.svg)](readme-cn.md)    Copyright 2015, Baidu, Inc.    Tera is a high performance distributed NoSQL database, which is inspired by google's [BigTable](http://static.googleusercontent.com/media/research.google.com/zh-CN//archive/bigtable-osdi06.pdf) and designed for real-time applications. Tera can easily scale to __petabytes__ of data across __thousands__ of commodity servers. Besides, Tera is widely used in many Baidu products with varied demandsï¼Œwhich range from throughput-oriented applications to latency-sensitive service, including web indexing, WebPage DB, LinkBase DB, etc. ([ä¸­æ–‡](readme-cn.md))    ## Features    * Linear and modular scalability  * Automatic and configurable sharding  * Ranged and hashed sharding strategies  * MVCC  * Column-oriented storage and locality group support  * Strictly consistent  * Automatic failover support  * Online schema change  * Snapshot support  * Support RAMDISK/SSD/DFS tiered cache  * Block cache and Bloom Filters for real-time queries  * Multi-type table support (RAMDISK/SSD/DISK table)  * Easy to use [C++](doc/en/sdk_guide.md)/[Java](doc/en/sdk_guide_java.md)/[Python](doc/en/sdk_guide_python.md)/[REST-ful](doc/en/sdk_guide_http.md) API    ## Data model    Tera is the collection of many sparse, distributed, multidimensional tables. The table is indexed by a row key, column key, and a timestamp; each value in the table is an uninterpreted array of bytes.    * (row:string, (column family+qualifier):string, time:int64) â†’ string    To learn more about the schema, you can refer to [BigTable](http://static.googleusercontent.com/media/research.google.com/zh-CN//archive/bigtable-osdi06.pdf).    ## Architecture    ![æž¶æž„å›¾](resources/images/arch.png)    Tera has three major components: sdk, master and tablet servers.    - __SDK__: a library that is linked into every application client to access Tera cluster.  - __Master__: master is responsible for managing tablet servers and tablets, automatic load balance and garbage collection of files in filesystem.  - __Tablet Server__: tablet server is the core module in tera, and it uses an __enhance__ [Leveldb](https://github.com/google/leveldb) as a basic storage engine. Tablet server manages a set of tablets, handles read/write/scan requests and schedule tablet split and merge online.    ## Building blocks  Tera is built on several pieces of open source infrastructure.    - __Filesystem__ (required)        Tera uses the distributed file system to store transaction log and data files. So Tera uses an abstract file system interface, called Env, to adapt to different implementations of file systems (e.g., [BFS](https://github.com/baidu/bfs), HDFS, HDFS2, POXIS filesystem).    - __Distributed lock service__ (required)        Tera relies on a highly-available and persistent distributed lock service, which is used for a variety of tasks: to ensure that there is at most one active master at any time; to store meta table's location, to discover new tablet server and finalize tablet server deaths. Tera has an adapter class to adapt to different implementations of lock service (e.g., ZooKeeper, [Nexus](https://github.com/baidu/ins))    - __High performance RPC framework__ (required)        Tera is designed to handle a variety of demanding workloads, which range from throughput-oriented applications to latency-sensitive service. So Tera needs a high performance network programming framework. Now Tera heavily relies on [Sofa-pbrpc](https://github.com/baidu/sofa-pbrpc/) to meet the performance demand.    - __Cluster management system__ (not necessary)        A Tera cluster in Baidu typically operates in a shared pool of machines that runs a wide variety of other distributed applications. So Tera can be deployed in a cluster management system [Galaxy](https://github.com/baidu/galaxy), which uses for scheduling jobs, managing resources on shared machines, dealing with machine failures, and monitoring machine status. Besides, Tera can also be deployed on RAW machine or in Docker container.    ## Documents    * [Developer Doc](doc/en/README.md)    ## Quick start  * __How to build__        Use sh [./build.sh](BUILD) to build Tera.    * __How to deploy__        [Pseudo Distributed Mode](doc/en/onebox.md)        [Build on Docker](example/docker)    * __How to access__        [teracli](doc/en/teracli.md)        [API](doc/en/sdk_guide.md)    ## Contributing to Tera  Contributions are welcomed and greatly appreciated.    Read [Roadmap](doc/en/roadmap.md) to get a general knowledge about our development plan.    See [Contributions](doc/en/contributor.md) for more details.    ## Follow us  To join us, please send resume to tera-user at baidu.com.   """
Big data;https://github.com/twitter/heron;"""<!--      Licensed to the Apache Software Foundation (ASF) under one      or more contributor license agreements.  See the NOTICE file      distributed with this work for additional information      regarding copyright ownership.  The ASF licenses this file      to you under the Apache License, Version 2.0 (the      ""License""); you may not use this file except in compliance      with the License.  You may obtain a copy of the License at          http://www.apache.org/licenses/LICENSE-2.0        Unless required by applicable law or agreed to in writing,      software distributed under the License is distributed on an      ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY      KIND, either express or implied.  See the License for the      specific language governing permissions and limitations      under the License.  -->  [![Build Status](https://travis-ci.com/apache/incubator-heron.svg?&branch=master)](https://travis-ci.com/apache/incubator-heron)    ![logo](website2/docs/assets/HeronTextLogo.png)    Heron is a realtime analytics platform developed by Twitter.  It has a wide array of architectural improvements over it's predecessor.    [Heron in Apache Incubation](http://incubator.apache.org/projects/heron.html)    ### Documentation    [https://heron.incubator.apache.org/](https://heron.apache.org/)  </br>  Confluence: https://cwiki.apache.org/confluence/display/HERON    #### Heron Requirements:   * Java 11   * Python 3.6   * Bazel 4.2.2    ## Contact    #### Mailing lists    | Name                                                                      | Scope                           |                                                                |                                                                    |                                                                           |  |:--------------------------------------------------------------------------|:--------------------------------|:---------------------------------------------------------------|:-------------------------------------------------------------------|:--------------------------------------------------------------------------|  | [user@heron.incubator.apache.org](mailto:user@heron.incubator.apache.org) | User-related discussions        | [Subscribe](mailto:user-subscribe@heron.incubator.apache.org)  | [Unsubscribe](mailto:user-unsubscribe@heron.incubator.apache.org)  | [Archives](http://mail-archives.apache.org/mod_mbox/incubator-heron-user/)|  | [dev@heron.incubator.apache.org](mailto:dev@heron.incubator.apache.org)   | Development-related discussions | [Subscribe](mailto:dev-subscribe@heron.incubator.apache.org)   | [Unsubscribe](mailto:dev-unsubscribe@heron.incubator.apache.org)   | [Archives](http://mail-archives.apache.org/mod_mbox/incubator-heron-dev/) |    #### Slack    [Self-Register](http://heronstreaming.herokuapp.com/) to our [Heron Slack Workspace](https://heronstreaming.slack.com/)    #### Meetup Group    [Bay Area Heron Meetup](https://www.meetup.com/Apache-Heron-Bay-Area), *We meet on Third Monday of Every Month in Palo Alto.*    ## For more information:    * Official Heron documentation located at [https://heron.apache.org/](https://heron.apache.org/)  * Official Heron resources, including Conference & Journal Papers, Videos, Blog Posts and selected Press located at [Heron Resources](https://heron.apache.org/resources)  * [Twitter Heron: Stream Processing at Scale](http://dl.acm.org/citation.cfm?id=2742788) (academic paper)  * [Twitter Heron: Stream Processing at Scale](https://www.youtube.com/watch?v=pUaFOuGgmco) (YouTube video)  * [Flying Faster with Twitter Heron](https://blog.twitter.com/2015/flying-faster-with-twitter-heron) (blog post)    ## License    Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 """
Big data;https://github.com/zrosenbauer/awesome-bigtable;"""<div align=""center"">  	<div>  		<img width=""180"" src=""/awesome-logo.png"" alt=""Awesome Bigtable"">  	</div>      <br />  	<p>  		:zap: Delightful list of <a href=""https://cloud.google.com/bigtable/"">Google Bigtable</a> resources, packages and interesting finds.  	</p>  	<br>  	<img src=""https://awesome.re/badge.svg"" alt=""Awesome List"">  </div>    ---    # Awesome Bigtable    [Bigtable](https://cloud.google.com/bigtable) is a fully managed, scalable NoSQL database service for large analytical and operational workloads, built and managed by Google.    ## Contents    - [Tools](#tools)    - [Official Client Libraries](#official-client-libraries)    - [Other Client Libraries](#other-client-libraries)    - [Command-line](#command-line)    - [Emulators](#emulators)    - [Databases](#databases)  - [Resources](#resources)    - [Articles & Blogs](#articles--blogs)    - [Tutorials](#tutorials)  - [Cool Stuff](#cool-stuff)    - [Inspired by Bigtable](#inspired-by-bigtable)    - [Interesting Projects](#interesting-projects)    ---    If you are new to Bigtable I'd recommend checking out the [Bigtable Documentation](https://cloud.google.com/bigtable/docs/). The docs are a great place to start, as you can view a full list of integrations, tutorials and other treats. This list is meant to be a curated list of awesome Bigtable ""things"" to supplement any official documentation.    ## Tools  A curated list of tools that will help you when working with or building on-top of Bigtable.    ### Official Client Libraries  - [C++](https://github.com/GoogleCloudPlatform/google-cloud-cpp/tree/master/google/cloud/bigtable) - Official implementation of the Google Cloud Bigtable C++ client.  - [C#](https://github.com/googleapis/google-cloud-dotnet) - Official implementation of the Google Cloud Bigtable .NET client.  - [Node.js](https://github.com/googleapis/nodejs-bigtable) - Official implementation of the Google Cloud Bigtable Node.js client.  - [Python](https://github.com/googleapis/python-bigtable) - Official implementation of the Google Cloud Bigtable python client.  - [HappyBase](https://github.com/googleapis/google-cloud-python-happybase) - Official client which uses a HappyBase emulation layer which uses Bigtable as the underlying storage layer.  - [Java](https://github.com/googleapis/java-bigtable) - Official implementation of the Google Cloud Bigtable Java client.  - [HBase Java](https://github.com/GoogleCloudPlatform/cloud-bigtable-client) - Official Java libraries and HBase client extensions for accessing Google Cloud Bigtable.  - [Go](https://github.com/googleapis/google-cloud-go/tree/master/bigtable) - Official implementation of the Google Cloud Bigtable Go client.  - [PHP](https://github.com/googleapis/google-cloud-php-bigtable) - Official implementation of the Google Cloud Bigtable PHP client.    ### Other Client Libraries  - [Simple Bigtable](https://github.com/spotify/simple-bigtable) - Java based client built and maintained by Spotify.  - [Rust Bigtable](https://github.com/durch/rust-bigtable) - Rust library for working with Google Bigtable Data API.  - [AsyncBigtable](https://github.com/OpenTSDB/asyncbigtable) - Implementation of AsyncHBase but on top of Google's Cloud Bigtable service.    ### Command-line  - [cbt](https://cloud.google.com/bigtable/docs/cbt-overview) - Official command-line interface for performing several different operations on Cloud Bigtable.   - [btcli](https://github.com/takashabe/btcli) - CLI client for the Bigtable with auto-completion.    ### Emulators  - [Google Emulator](https://cloud.google.com/bigtable/docs/emulator) - Official in-memory emulator for Cloud Bigtable, included with the Google Cloud SDK.  - [Spotify Docker Bigtable](https://github.com/spotify/docker-bigtable) - Docker container with an in memory implementation of Google Cloud Bigtable.  - [Shopify Bigtable Emulator](https://github.com/Shopify/bigtable-emulator) - In memory Go implementation of Bigtable.  - [LittleTable](https://github.com/steveniemitz/littletable) - In-memory JVM-based emulator for Bigtable.    ### Databases  - [Heroic](https://github.com/spotify/heroic) - Scalable time series database based on Bigtable, Cassandra, and Elasticsearch.  - [Janusgraph](https://github.com/JanusGraph/janusgraph) - Open-source, distributed graph database that can use Bigtable as its storage layer.  - [GeoMesa](https://github.com/locationtech/geomesa) - Suite of tools for working with big geo-spatial data in a distributed fashion, that can leverage Bigtable as its backend.  - [GeoWave](https://github.com/locationtech/geowave) - Tool that provides geospatial and temporal indexing on top of Accumulo, HBase, Bigtable, Cassandra, and DynamoDB.  - [HGraphDB](https://github.com/rayokota/hgraphdb) - Client layer for using HBase (Bigtable) as a graph database.  - [OpenTSDB](https://github.com/GoogleCloudPlatform/opentsdb-bigtable) - An Open Source Time Series Data Base that can levearge Bigtable as its storage layer.  - [Cattle DB](https://github.com/wuttem/cattledb) - Timeseries store built on top of Bigtable.  - [YildizDB](https://github.com/yildizdb/yildiz) - Graph database layer on top of Bigtable.    ## Resources  A curated list of resources to help you get off the ground with Bigtable.    ### Articles & Blogs  - [Bigtable: A Distributed Storage System for Structured Data](https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf) - Published on 2006.  - [A NoSQL massively parallel table](https://www.cs.rutgers.edu/~pxk/417/notes/content/bigtable.html) - Published on 2011-11.  - [How we moved our Historical Stats from MySQL to Bigtable with zero downtime](https://www.fastly.com/blog/how-we-moved-our-historical-stats-from-mysql-bigtable-zero-downtime) - Published on 2017-07.  - [Medium @duhroach](https://medium.com/@duhroach) - Bigtable centric posts by Colt McAnlis, DA @ Google.    - [Cloud Bigtable Performance 101](https://medium.com/@duhroach/cloud-bigtable-performance-101-8bf884bc1d1c) - Published on 2018-11.    - [The right Cloud Bigtable index makes all the difference.](https://medium.com/@duhroach/the-right-cloud-bigtable-index-makes-all-the-difference-3bcabe9bd65a) - Published on 2019-1.    - [Cloud Bigtable : Getting the geography right](https://medium.com/@duhroach/cloud-bigtable-getting-the-geography-right-645577216516) - Published on 2019-1.    - [Using Cloud Bigtable Monitoring UI](https://medium.com/@duhroach/using-cloud-bigtable-monitoring-ui-40d3f4c726d6) - Published on 2019-1.  - [Bigtable: storing Protobuf bytes in one column vs splitting the content into column families/qualifiers](https://tech.travelaudience.com/bigtable-storing-protobuf-bytes-in-one-column-vs-splitting-the-content-into-column-families-c231bdff8db7) - Published on 2018-1.  - [Using Google Cloud Emulators in Integration Tests](https://medium.com/google-cloud/using-google-cloud-emulators-for-integration-tests-7812890ebe0d) - Published on 2017-6.  - [The Joy and Pain of using Google Bigtable](https://syslog.ravelin.com/the-joy-and-pain-of-using-google-bigtable-4210604c75be) - Published on 2019-1.    ### Tutorials  - [Google Tutorials for Bigtable](https://cloud.google.com/bigtable/docs/tutorials) - List of official tutorials related to Bigtable.   - [Cloud Bigtable Examples](https://github.com/GoogleCloudPlatform/cloud-bigtable-examples) - Repo containing official examples of using Bigtable.  - [Introduction to Google Cloud Bigtable](https://cloudacademy.com/course/introduction-to-google-cloud-bigtable/) - CloudAcademy provided intro tutorial to Bigtable (membership required).    ## Cool Stuff  A list of cool things related to Bigtable.    ### Inspired by Bigtable  - [Apache Cassandra](http://cassandra.apache.org/) - Highly-scalable partitioned row store.  - [Apache HBase](https://hbase.apache.org/) - The Hadoop database, a distributed, scalable, big data store.  - [Apache Accumulo](https://github.com/apache/accumulo) - Sorted, distributed key/value store that provides robust, scalable data storage and retrieval.  - [Tera](https://github.com/baidu/tera) - High performance distributed NoSQL database.  - [obigstore](https://github.com/mfp/obigstore) - Database with Bigtable-like data model atop LevelDB.    ### Interesting Projects  - [YildizDB Bigtable](https://github.com/yildizdb/bigtable) - TypeScript Bigtable Client with ðŸ”‹ðŸ”‹ included.  - [Bigtable Autoscaler](https://github.com/spotify/bigtable-autoscaler) - Service that autoscales Bigtable clusters based on CPU load.    <!--lint ignore no-emphasis-as-heading-->  **Awesome mentioned badge**    If your package or repository is mentioned in this list feel free to add the Awesome mentioned badge to your README.md.    ```md  [![Mentioned in Awesome Bigtable](https://awesome.re/mentioned-badge-flat.svg)](https://github.com/zrosenbauer/awesome-bigtable)  ```    ---    **Logo Source:** https://logomakr.com/4gLK5l """
Big data;https://github.com/gionkunz/chartist-js;"""# Big welcome by the Chartist Guy    [![Join the chat at https://gitter.im/gionkunz/chartist-js](https://badges.gitter.im/gionkunz/chartist-js.svg)](https://gitter.im/gionkunz/chartist-js?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  [![npm version](http://img.shields.io/npm/v/chartist.svg)](https://npmjs.org/package/chartist) [![build status](http://img.shields.io/travis/gionkunz/chartist-js.svg)](https://travis-ci.org/gionkunz/chartist-js) [![Inline docs](http://inch-ci.org/github/gionkunz/chartist-js.svg?branch=develop)](http://inch-ci.org/github/gionkunz/chartist-js)    ![The Chartist Guy](https://raw.github.com/gionkunz/chartist-js/develop/site/images/chartist-guy.gif ""The Chartist Guy"")    *Checkout the documentation site at http://gionkunz.github.io/chartist-js/*    *Checkout this lightning talk that gives you an overview of Chartist in 5 minutes https://www.youtube.com/watch?v=WdYzPhOB_c8*    *Guest talk of the Chartist.js Guy at the Treehouse Show https://www.youtube.com/watch?v=h9oH0iDaZDQ&t=2m40s*    Chartist.js is a simple responsive charting library built with SVG. There are hundreds of nice charting libraries already  out there, but they are either:    * not responsive  * use the wrong technologies for illustration (canvas)  * are not flexible enough while keeping the configuration simple  * are not friendly to your own code  * are not friendly to designers  * have unnecessary dependencies to monolithic libraries   * more annoying things    That's why we started Chartist.js and our goal is to solve all of the above issues.    ## What is it made for?    Chartist's goal is to provide a simple, lightweight and unintrusive library to responsively craft charts on your website.   It's important to understand that one of the main intentions of Chartist.js is to rely on standards rather than providing   it's own solution to a problem which is already solved by those standards. We need to leverage the power of browsers   today and say good bye to the idea of solving all problems ourselves.    Chartist works with inline-SVG and therefore leverages the power of the DOM to provide parts of its functionality. This   also means that Chartist does not provide it's own event handling, labels, behaviors or anything else that can just be   done with plain HTML, JavaScript and CSS. The single and only responsibility of Chartist is to help you drawing ""Simple   responsive Charts"" using inline-SVG in the DOM, CSS to style and JavaScript to provide an API for configuring your charts.    ## Example site    You can visit this Site http://gionkunz.github.io/chartist-js/ which is in fact a build of the current project.  We are still developing and constantly add features but you can already use Chartist.js in your projects as we have   reached a stable and reliable state already.    ## Version notes    We are currently still heavily developing in order to make Chartist.js better. Your help is needed! Please contribute  to the project if you like the idea and the concept and help us to bring nice looking responsive open-source charts  to the masses.    ### Important missing stuff    1. Jasmine Tests!  2. Documentation: JSDoc, Getting started documentation and landing page  3. Better accessibility using ARIA and other optimizations  4. Better interfaces to the library (i.e. jQuery with data-* attributes for configuration), Angular.js directive etc.  5. Richer Sass / CSS framework  6. Other charts types (spider etc.)    ## Plugins    Some features aren't right for the core product  but there is a great set of plugins available  which add features like:    * [Axis labels](http://gionkunz.github.io/chartist-js/plugins.html#axis-title-plugin)  * [Tooltips at data points](https://gionkunz.github.io/chartist-js/plugins.html#tooltip-plugin)  * [Coloring above/below a threshold](https://gionkunz.github.io/chartist-js/plugins.html#threshold-plugin)    and more.    See all the plugins [here](https://gionkunz.github.io/chartist-js/plugins.html).    ## Contribution    We are looking for people who share the idea of having a simple, flexible charting library that is responsive and uses  modern and future-proof technologies. The goal of this project is to create a responsive charting library where developers  have their joy in using it and designers love it because of the designing flexibility they have.    Contribute if you like the Chartist Guy! """
Big data;https://github.com/airbnb/airpal;"""# DEPREACTED - Airpal    Airpal is deprecated, and most functionality and feature work has been moved to SQL Lab within [Apache Superset](https://www.github.com/apache/superset).    ---    Airpal is a web-based, query execution tool which leverages Facebook's [PrestoDB](http://prestodb.io)  to make authoring queries and retrieving results simple for users.  Airpal provides the ability to find tables, see metadata, browse sample rows,  write and edit queries, then submit queries all in a web interface. Once  queries are running, users can track query progress and when finished,  get the results back through the browser as a CSV (download it or share it  with friends). The results of a query can be used to generate a new Hive table  for subsequent analysis, and Airpal maintains a searchable history of all  queries run within the tool.    * [Features](#features)  * [Requirements](#requirements)  * [Launching](#steps-to-launch)  * [Presto Compatibility Chart](#compatibility-chart)    ![Airpal UI](screenshots/demo.gif)    ## Features    * Optional [Access Control](docs/USER_ACCOUNTS.md)  * Syntax highlighting  * Results exported to a CSV for download or a Hive table  * Query history for self and others  * Saved queries  * Table finder to search for appropriate tables  * Table explorer to visualize schema of table and first 1000 rows    ## Requirements    * Java 7 or higher  * MySQL database  * [Presto](http://prestodb.io) 0.77 or higher  * S3 bucket (to store CSVs)  * Gradle 2.2 or higher      ## Steps to launch    1. Build Airpal        We'll be using [Gradle](https://www.gradle.org/) to build the back-end Java code      and a [Node.js](http://nodejs.org/)-based build pipeline ([Browserify](http://browserify.org/)      and [Gulp](http://gulpjs.com/)) to build the front-end Javascript code.        If you have `node` and `npm` installed locally, and wish to use      them, simply run:        ```      ./gradlew clean shadowJar -Dairpal.useLocalNode      ```        Otherwise, `node` and `npm` will be automatically downloaded for you      by running:        ```      ./gradlew clean shadowJar      ```        Specify Presto version by `-Dairpal.prestoVersion`:        ```      ./gradlew -Dairpal.prestoVersion=0.145 clean shadowJar      ```    1. Create a MySQL database for Airpal. We recommend you call it `airpal` and will assume that for future steps.    1. Create a `reference.yml` file to store your configuration options.        Start by copying over the example configuration, `reference.example.yml`.        ```      cp reference.example.yml reference.yml      ```      Then edit it to specify your MySQL credentials, and your S3 credentials if      using S3 as a storage layer (Airpal defaults to local file storage, for      demonstration purposes).    1. Migrate your database.        ```      java -Duser.timezone=UTC \           -cp build/libs/airpal-*-all.jar com.airbnb.airpal.AirpalApplication db migrate reference.yml      ```    1. Run Airpal.        ```      java -server \           -Duser.timezone=UTC \           -cp build/libs/airpal-*-all.jar com.airbnb.airpal.AirpalApplication server reference.yml      ```    1. Visit Airpal.      Assuming you used the default settings in `reference.yml` you can      now open http://localhost:8081 to use Airpal. Note that you might      have to change the host, depending on where you deployed it.    *Note:* To override the configuration specified in `reference.yml`, you may  specify certain settings on the command line in [the traditional Dropwizard  fashion](https://dropwizard.github.io/dropwizard/manual/core.html#configuration),  like so:    ```  java -Ddw.prestoCoordinator=http://presto-coordinator-url.com \       -Ddw.s3AccessKey=$ACCESS_KEY \       -Ddw.s3SecretKey=$SECRET_KEY \       -Ddw.s3Bucket=airpal \       -Ddw.dataSourceFactory.url=jdbc:mysql://127.0.0.1:3306/airpal \       -Ddw.dataSourceFactory.user=airpal \       -Ddw.dataSourceFactory.password=$YOUR_PASSWORD \       -Duser.timezone=UTC \       -cp build/libs/airpal-*-all.jar db migrate reference.yml  ```      ## Compatibility Chart    Airpal Version | Presto Versions Tested  ---------------|-----------------------  0.1            | 0.77, 0.87, 0.145    ## In the Wild  Organizations and projects using `airpal` can list themselves [here](INTHEWILD.md).    ## Contributors    - Andy Kramolisch [@andykram](https://github.com/andykram)  - Harry Shoff [@hshoff](https://github.com/hshoff)  - Josh Perez [@goatslacker](https://github.com/goatslacker)  - Spike Brehm [@spikebrehm](https://github.com/spikebrehm)  - Stefan Vermaas [@stefanvermaas](https://github.com/stefanvermaas) """
Big data;https://github.com/papertrail/kestrel;"""Kestrel  =======    [![Project status](https://img.shields.io/badge/status-active-green.svg)](#status)    Kestrel is based on Blaine Cook's ""starling"" simple, distributed message  queue, with added features and bulletproofing, as well as the scalability  offered by actors and the JVM.    Each server handles a set of reliable, ordered message queues. When you put a  cluster of these servers together, *with no cross communication*, and pick a  server at random whenever you do a `set` or `get`, you end up with a reliable,  *loosely ordered* message queue.    In many situations, loose ordering is sufficient. Dropping the requirement on  cross communication makes it horizontally scale to infinity and beyond: no  multicast, no clustering, no ""elections"", no coordination at all. No talking!  Shhh!    For more information about what it is and how to use it, check out  the included [guide](https://github.com/robey/kestrel/blob/master/docs/guide.md).    Kestrel has a mailing list here:  [kestrel-talk@googlegroups.com](http://groups.google.com/group/kestrel-talk)    Author's address: Robey Pointer \<robeypointer@gmail.com>      Status  ------    We ([Papertrail](https://papertrailapp.com/)) use Kestrel extensively and are taking on maintaining a fork to fold in bug fixes and small improvements that we find while operating it.      Twitter's Status Statement  --------------------------    We've deprecated Kestrel because internally we've shifted our attention to an alternative project based on DistributedLog, and we no longer have the resources to contribute fixes or accept pull requests. While Kestrel is a great solution up to a certain point (simple, fast, durable, and easy to deploy), it hasn't been able to cope with Twitter's massive scale (in terms of number of tenants, QPS, operability, diversity of workloads etc.) or operating environment (an Aurora cluster without persistent storage).    Features  --------    Kestrel is:    - fast        It runs on the JVM so it can take advantage of the hard work people have      put into java performance.    - small        Currently about 2500 lines of scala, because it relies on Netty (a rough      equivalent of Danger's ziggurat or Ruby's EventMachine) -- and because      Scala is extremely expressive.    - durable        Queues are stored in memory for speed, but logged into a journal on disk      so that servers can be shutdown or moved without losing any data.    - reliable        A client can ask to ""tentatively"" fetch an item from a queue, and if that      client disconnects from kestrel before confirming ownership of the item,      the item is handed to another client. In this way, crashing clients don't      cause lost messages.      Anti-Features  -------------    Kestrel is not:    - strongly ordered        While each queue is strongly ordered on each machine, a cluster will      appear ""loosely ordered"" because clients pick a machine at random for      each operation. The end result should be ""mostly fair"".    - transactional        This is not a database. Item ownership is transferred with acknowledgement,      but kestrel does not support grouping multiple operations into an atomic      unit.      Downloading it  --------------    The latest release is always on the homepage here:    - [http://robey.github.io/kestrel/](http://robey.github.io/kestrel/)    Or the latest development versions & branches are on github:    - [http://gitub.com/robey/kestrel](https://github.com/twitter/kestrel)      Building it  -----------    Kestrel requires java 6 and sbt 0.11.2. Presently some sbt plugins used by kestrel  depend on that exact version of sbt. On OS X 10.5, you may have to hard-code  an annoying `JAVA_HOME` to use java 6:        $ export JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home    Building from source is easy:        $ sbt clean update package-dist    Scala libraries and dependencies will be downloaded from maven repositories  the first time you do a build. The finished distribution will be in `dist`.      Running it  ----------    You can run kestrel by hand, in development mode, via:        $ ./dist/kestrel-VERSION/scripts/devel.sh    Like all ostrich-based servers, it uses the ""stage"" property to determine  which config file to load, so `devel.sh` sets `-Dstage=development`.    When running it as a server, a startup script is provided in  `dist/kestrel-VERSION/scripts/kestrel.sh`. The script assumes you have  `daemon`, a standard daemonizer for Linux, but also available  [here](http://libslack.org/daemon/) for all common unix platforms.    The created archive `kestrel-VERSION.zip` can be expanded into a place  like `/usr/local` (or wherever you like) and executed within its own folder as  a self-contained package. All dependent jars are included. The current startup  script, however, assumes that kestrel has been deployed to  `/usr/local/kestrel/current` (e.g., as if by capistrano), and the startup  script loads kestrel from that path.    The default configuration puts logfiles into `/var/log/kestrel/` and queue  journal files into `/var/spool/kestrel/`.    The startup script logs extensive GC information to a file named `stdout` in  the log folder. If kestrel has problems starting up (before it can initialize  logging), it will usually appear in `error` in the same folder.      Configuration  -------------    Queue configuration is described in detail in `docs/guide.md` (an operational  guide). Scala docs for the config variables are  [here](http://robey.github.io/kestrel/api/main/api/net/lag/kestrel/config/KestrelConfig.html).      Performance  -----------    Several performance tests are included. To run them, first start up a kestrel instance  locally.        $ sbt clean update package-dist      $ ./dist/kestrel-VERSION/scripts/devel.sh    ## Put-many    This test just spams a kestrel server with ""put"" operations, to see how  quickly it can absorb and journal them.    A sample run on a 2010 MacBook Pro:        $ ./dist/kestrel/scripts/load/put-many -n 100000      Put 100000 items of 1024 bytes to localhost:22133 in 1 queues named spam        using 100 clients.      Finished in 6137 msec (61.4 usec/put throughput).      Transactions: min=71.00; max=472279.00 472160.00 469075.00;        median=3355.00; average=5494.69 usec      Transactions distribution: 5.00%=485.00 10.00%=1123.00 25.00%=2358.00        50.00%=3355.00 75.00%=4921.00 90.00%=7291.00 95.00%=9729.00        99.00%=50929.00 99.90%=384638.00 99.99%=467899.00    ## Many-clients    This test has one producer that trickles out one item at a time, and a pile of  consumers fighting for each item. It usually takes exactly as long as the  number of items times the delay, but is useful as a validation test to make  sure kestrel works as advertised without blowing up.    A sample run on a 2010 MacBook Pro:        $ ./dist/kestrel/scripts/load/many-clients      many-clients: 100 items to localhost using 100 clients, kill rate 0%,        at 100 msec/item      Received 100 items in 11046 msec.    This test always takes about 11 seconds -- it's a load test instead of a  speed test.    ## Flood    This test starts up one producer and one consumer, and just floods items  through kestrel as fast as it can.    A sample run on a 2010 MacBook Pro:        $ ./dist/kestrel/scripts/load/flood      flood: 1 threads each sending 10000 items of 1kB through spam      Finished in 1563 msec (156.3 usec/put throughput).      Consumer(s) spun 0 times in misses.    ## Packing    This test starts up one producer and one consumer, seeds the queue with a  bunch of items to cause it to fall behind, then does cycles of flooding items  through the queue, separated by pauses. It's meant to test kestrel's behavior  with a queue that's fallen behind and *stays* behind indefinitely, to make  sure the journal files are packed periodically without affecting performance  too badly.    A sample run on a 2010 MacBook Pro:        $ ./dist/kestrel/scripts/load/packing -c 10 -q small      packing: 25000 items of 1kB with 1 second pauses      Wrote 25000 items starting at 0.      cycle: 1      Wrote 25000 items starting at 25000.      Read 25000 items in 5279 msec. Consumer spun 0 times in misses.      cycle: 2      Wrote 25000 items starting at 50000.      Read 25000 items in 4931 msec. Consumer spun 0 times in misses.      ...      cycle: 10      Wrote 25000 items starting at 250000.      Read 25000 items in 5304 msec. Consumer spun 0 times in misses.      Read 25000 items in 3370 msec. Consumer spun 0 times in misses.    You can see the journals being packed in the kestrel log. Like  ""many-clients"", this test is a load test instead of a speed test.    ## Leaky-reader    This test starts a producer and several consumers, with the consumers  occasionally ""forgetting"" to acknowledge an item that they've read. It  verifies that the un-acknowledged items are eventually handed off to another  consmer.    A sample run:        $ ./dist/kestrel/scripts/load/leaky-reader -n 100000 -t 10      leaky-reader: 10 threads each sending 100000 items through spam      Flushing queues first.      1000      2000      100000      Finished in 40220 msec (40.2 usec/put throughput).      Completed all reads    Like ""many-clients"", it's just a load test. """
Big data;https://github.com/Netflix/atlas;"""# Atlas    Backend for managing dimensional time series data.    ## Links    * [Documentation](https://github.com/Netflix/atlas/wiki)  * [Mailing List](https://groups.google.com/forum/#!forum/netflix-atlas)  * [Issues](https://github.com/Netflix/atlas/issues)  * [Releases](https://github.com/Netflix/atlas/releases)    ## License    Copyright 2014-2021 Netflix, Inc.    Licensed under the Apache License, Version 2.0 (the â€œLicenseâ€); you may not use this file except in compliance with the License. You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an â€œAS ISâ€ BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. """
Big data;https://github.com/skale-me/skale-engine;"""**Development activity is stopped, and this project is now archived.**    ![logo](docs/images/logo-skale.png)    [![Build Status](https://travis-ci.org/skale-me/skale.svg?branch=master)](https://travis-ci.org/skale-me/skale)  [![Build Status](https://ci.appveyor.com/api/projects/status/github/skale-me/skale?svg=true)](https://ci.appveyor.com/project/skaleme/skale)  [![npm badge](https://img.shields.io/npm/v/skale.svg)](https://www.npmjs.com/package/skale)      High performance distributed data processing and machine learning.    Skale provides a high-level API in Javascript and an optimized  parallel execution engine on top of NodeJS.    ## Features  * Pure javascript implementation of a Spark like engine  * Multiple data sources: filesystems, databases, cloud (S3, azure)  * Multiple data formats: CSV, JSON, Columnar (Parquet)...  * 50 high level operators to build parallel apps  * Machine learning: scalable classification, regression, clusterization  * Run interactively in a nodeJS REPL shell  * Docker [ready](docker/), simple local mode or full distributed mode  * Very fast, see [benchmark](benchmark/)    ## Quickstart  ```sh  npm install skale  ```    Word count example:     ```javascript  var sc = require('skale').context();    sc.textFile('/my/path/*.txt')    .flatMap(line => line.split(' '))    .map(word => [word, 1])    .reduceByKey((a, b) => a + b, 0)    .count(function (err, result) {      console.log(result);      sc.end();    });  ```    ### Local mode  In local mode, worker processes are automatically forked and  communicate with app through child process IPC channel. This is  the simplest way to operate, and it allows to use all machine  available cores.    To run in local mode, just execute your app script:  ```sh  node my_app.js  ```    or with debug traces:  ```sh  SKALE_DEBUG=2 node my_app.js  ```    ### Distributed mode  In distributed mode, a cluster server process and worker processes  must be started prior to start app. Processes communicate with each  other via raw TCP or via websockets.    To run in distributed cluster mode, first start a cluster server  on `server_host`:  ```sh  ./bin/server.js  ```    On each worker host, start a worker controller process which connects  to server:  ```sh  ./bin/worker.js -H server_host  ```    Then run your app, setting the cluster server host in environment:  ```sh  SKALE_HOST=server_host node my_app.js  ```    The same with debug traces:  ```sh  SKALE_HOST=server_host SKALE_DEBUG=2 node my_app.js  ```    ## Resources    * [Contributing guide](CONTRIBUTING.md)  * [Documentation](https://skale-me.github.io/skale)  * [Gitter](https://gitter.im/skale-me/skale-engine) for support and    discussion  * [Mailing list](https://groups.google.com/forum/#!forum/skale)    for discussion about use and development    ## Authors    The original authors of skale are [Cedric Artigue](https://github.com/CedricArtigue) and [Marc Vertes](https://github.com/mvertes).    [List of all  contributors](https://github.com/skale-me/skale/graphs/contributors)    ## License    [Apache-2.0](LICENSE)    ## Credits    <div>Logo Icon made by <a href=""https://www.flaticon.com/authors/smashicons"" title=""Smashicons"">Smashicons</a> from <a href=""https://www.flaticon.com/"" title=""Flaticon"">www.flaticon.com</a> is licensed by <a href=""http://creativecommons.org/licenses/by/3.0/"" title=""Creative Commons BY 3.0"" target=""_blank"">CC 3.0 BY</a></div> """
Big data;https://github.com/pingcap/tikv;"""<img src=""images/tikv-logo.png"" alt=""tikv_logo"" width=""300""/>    ## [Website](https://tikv.org) | [Documentation](https://tikv.org/docs/latest/concepts/overview/) | [Community Chat](https://tikv.org/chat)    [![Build Status](https://ci.pingcap.net/buildStatus/icon?job=tikv_ghpr_build_master)](https://ci.pingcap.net/blue/organizations/jenkins/tikv_ghpr_build_master/activity)  [![Coverage Status](https://codecov.io/gh/tikv/tikv/branch/master/graph/badge.svg)](https://codecov.io/gh/tikv/tikv)  [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/2574/badge)](https://bestpractices.coreinfrastructure.org/projects/2574)    TiKV is an open-source, distributed, and transactional key-value database. Unlike other traditional NoSQL systems, TiKV not only provides classical key-value APIs, but also transactional APIs with ACID compliance. Built in Rust and powered by Raft, TiKV was originally created to complement [TiDB](https://github.com/pingcap/tidb), a distributed HTAP database compatible with the MySQL protocol.    The design of TiKV ('Ti' stands for titanium) is inspired by some great distributed systems from Google, such as BigTable, Spanner, and Percolator, and some of the latest achievements in academia in recent years, such as the Raft consensus algorithm.    If you're interested in contributing to TiKV, or want to build it from source, see [CONTRIBUTING.md](./CONTRIBUTING.md).    ![cncf_logo](images/cncf.png#gh-light-mode-only)  ![cncf_logo](images/cncf-white.png#gh-dark-mode-only)    TiKV is a graduated project of the [Cloud Native Computing Foundation](https://cncf.io/) (CNCF). If you are an organization that wants to help shape the evolution of technologies that are container-packaged, dynamically-scheduled and microservices-oriented, consider joining the CNCF. For details about who's involved and how TiKV plays a role, read the CNCF [announcement](https://www.cncf.io/announcements/2020/09/02/cloud-native-computing-foundation-announces-tikv-graduation/).    ---    With the implementation of the Raft consensus algorithm in Rust and consensus state stored in RocksDB, TiKV guarantees data consistency. [Placement Driver (PD)](https://github.com/pingcap/pd/), which is introduced to implement auto-sharding, enables automatic data migration. The transaction model is similar to Google's Percolator with some performance improvements. TiKV also provides snapshot isolation (SI), snapshot isolation with lock (SQL: `SELECT ... FOR UPDATE`), and externally consistent reads and writes in distributed transactions.    TiKV has the following key features:    - **Geo-Replication**        TiKV uses [Raft](http://raft.github.io/) and the Placement Driver to support Geo-Replication.    - **Horizontal scalability**        With PD and carefully designed Raft groups, TiKV excels in horizontal scalability and can easily scale to 100+ TBs of data.    - **Consistent distributed transactions**        Similar to Google's Spanner, TiKV supports externally-consistent distributed transactions.    - **Coprocessor support**        Similar to HBase, TiKV implements a coprocessor framework to support distributed computing.    - **Cooperates with [TiDB](https://github.com/pingcap/tidb)**        Thanks to the internal optimization, TiKV and TiDB can work together to be a compelling database solution with high horizontal scalability, externally-consistent transactions, support for RDBMS, and NoSQL design patterns.    ## Governance    See [Governance](https://github.com/tikv/community/blob/master/GOVERNANCE.md).    ## Documentation    For instructions on deployment, configuration, and maintenance of TiKV,see TiKV documentation on our [website](https://tikv.org/docs/4.0/tasks/introduction/). For more details on concepts and designs behind TiKV, see [Deep Dive TiKV](https://tikv.org/deep-dive/introduction/).    > **Note:**  >  > We have migrated our documentation from the [TiKV's wiki page](https://github.com/tikv/tikv/wiki/) to the [official website](https://tikv.org/docs). The original Wiki page is discontinued. If you have any suggestions or issues regarding documentation, offer your feedback [here](https://github.com/tikv/website).    ## TiKV adopters    You can view the list of [TiKV Adopters](https://tikv.org/adopters/).    ## TiKV software stack    ![The TiKV software stack](images/tikv_stack.png)    - **Placement Driver:** PD is the cluster manager of TiKV, which periodically checks replication constraints to balance load and data automatically.  - **Store:** There is a RocksDB within each Store and it stores data into the local disk.  - **Region:** Region is the basic unit of Key-Value data movement. Each Region is replicated to multiple Nodes. These multiple replicas form a Raft group.  - **Node:** A physical node in the cluster. Within each node, there are one or more Stores. Within each Store, there are many Regions.    When a node starts, the metadata of the Node, Store and Region are recorded into PD. The status of each Region and Store is reported to PD regularly.    ## Quick start    ### Deploy a playground with TiUP    The most quickest to try out TiKV with TiDB is using TiUP, a component manager for TiDB.    You can see [this page](https://docs.pingcap.com/tidb/stable/quick-start-with-tidb#deploy-a-local-test-environment-using-tiup-playground) for a step by step tutorial.    ### Deploy a playground with binary    TiKV is able to run separately with PD, which is the minimal deployment required.    1. Download and extract binaries.    ```bash  $ export TIKV_VERSION=v4.0.12  $ export GOOS=darwin  # only {darwin, linux} are supported  $ export GOARCH=amd64 # only {amd64, arm64} are supported  $ curl -O  https://tiup-mirrors.pingcap.com/tikv-$TIKV_VERSION-$GOOS-$GOARCH.tar.gz  $ curl -O  https://tiup-mirrors.pingcap.com/pd-$TIKV_VERSION-$GOOS-$GOARCH.tar.gz  $ tar -xzf tikv-$TIKV_VERSION-$GOOS-$GOARCH.tar.gz  $ tar -xzf pd-$TIKV_VERSION-$GOOS-$GOARCH.tar.gz  ```    2. Start PD instance.    ```bash  $ ./pd-server --name=pd --data-dir=/tmp/pd/data --client-urls=""http://127.0.0.1:2379"" --peer-urls=""http://127.0.0.1:2380"" --initial-cluster=""pd=http://127.0.0.1:2380"" --log-file=/tmp/pd/log/pd.log  ```    3. Start TiKV instance.    ```bash  $ ./tikv-server --pd-endpoints=""127.0.0.1:2379"" --addr=""127.0.0.1:20160"" --data-dir=/tmp/tikv/data --log-file=/tmp/tikv/log/tikv.log  ```    4. Install TiKV Client(Python) and verify the deployment, required Python 3.5+.    ```bash  $ pip3 install -i https://test.pypi.org/simple/ tikv-client  ```    ```python  from tikv_client import RawClient    client = RawClient.connect(""127.0.0.1:2379"")    client.put(b'foo', b'bar')  print(client.get(b'foo')) # b'bar'    client.put(b'foo', b'baz')  print(client.get(b'foo')) # b'baz'  ```    ### Deploy a cluster with TiUP    You can see [this manual](./doc/deploy.md) of production-like cluster deployment presented by @c4pt0r.    ### Build from source    See [CONTRIBUTING.md](./CONTRIBUTING.md).    ## Client drivers    Currently, the interfaces to TiKV are the [TiDB Go client](https://github.com/pingcap/tidb/tree/master/store/tikv) and the [TiSpark Java client](https://github.com/pingcap/tispark/tree/master/tikv-client/src/main/java/com/pingcap/tikv).    These are the clients for TiKV:    - [Go](https://github.com/tikv/client-go) (The most stable and widely used)  - [Java](https://github.com/tikv/client-java)  - [Rust](https://github.com/tikv/client-rust)  - [C](https://github.com/tikv/client-c)    If you want to try the Go client, see [Go Client](https://tikv.org/docs/4.0/reference/clients/go/).    ## Security    ### Security audit    A third-party security auditing was performed by Cure53. See the full report [here](./security/Security-Audit.pdf).    ### Reporting Security Vulnerabilities    To report a security vulnerability, please send an email to [TiKV-security](mailto:tikv-security@lists.cncf.io) group.    See [Security](./security/SECURITY.md) for the process and policy followed by the TiKV project.    ## Communication    Communication within the TiKV community abides by [TiKV Code of Conduct](./CODE_OF_CONDUCT.md). Here is an excerpt:    > In the interest of fostering an open and welcoming environment, we as  contributors and maintainers pledge to making participation in our project and  our community a harassment-free experience for everyone, regardless of age, body  size, disability, ethnicity, sex characteristics, gender identity and expression,  level of experience, education, socio-economic status, nationality, personal  appearance, race, religion, or sexual identity and orientation.    ### Social Media    - [Twitter](https://twitter.com/tikvproject)  - [Blog](https://tikv.org/blog/)  - [Reddit](https://www.reddit.com/r/TiKV)  - Post questions or help answer them on [Stack Overflow](https://stackoverflow.com/questions/tagged/tikv)    ### Slack    Join the TiKV community on [Slack](https://slack.tidb.io/invite?team=tikv-wg&channel=general) - Sign up and join channels on TiKV topics that interest you.    ## License    TiKV is under the Apache 2.0 license. See the [LICENSE](./LICENSE) file for details.    ## Acknowledgments    - Thanks [etcd](https://github.com/coreos/etcd) for providing some great open source tools.  - Thanks [RocksDB](https://github.com/facebook/rocksdb) for their powerful storage engines.  - Thanks [rust-clippy](https://github.com/rust-lang/rust-clippy). We do love the great project. """
Big data;https://github.com/benedekrozemberczki/karateclub;"""   ![Version](https://badge.fury.io/py/karateclub.svg?style=plastic)   ![License](https://img.shields.io/github/license/benedekrozemberczki/karateclub.svg)  [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/karateclub.svg)](https://github.com/benedekrozemberczki/karateclub/archive/master.zip)   [![Arxiv](https://img.shields.io/badge/ArXiv-2003.04819-orange.svg)](https://arxiv.org/abs/2003.04819)  [![build badge](https://github.com/benedekrozemberczki/karateclub/workflows/CI/badge.svg)](https://github.com/benedekrozemberczki/karateclub/actions?query=workflow%3ACI)   [![coverage badge](https://codecov.io/gh/benedekrozemberczki/karateclub/branch/master/graph/badge.svg)](https://codecov.io/github/benedekrozemberczki/karateclub?branch=master)  <p align=""center"">    <img width=""90%"" src=""https://github.com/benedekrozemberczki/karateclub/blob/master/karatelogo.jpg?sanitize=true"" />  </p>    ----------------------------------------------------------      **Karate Club** is an unsupervised machine learning extension library for [NetworkX](https://networkx.github.io/).      Please look at the **[Documentation](https://karateclub.readthedocs.io/)**, relevant **[Paper](https://arxiv.org/abs/2003.04819)**, **[Promo Video](https://www.youtube.com/watch?v=t212-ntxu2U)**, and **[External Resources](https://karateclub.readthedocs.io/en/latest/notes/resources.html)**.    *Karate Club* consists of state-of-the-art methods to do unsupervised learning on graph structured data. To put it simply it is a Swiss Army knife for small-scale graph mining research. First, it provides network embedding techniques at the node and graph level. Second, it includes a variety of overlapping and non-overlapping community detection methods. Implemented methods cover a wide range of network science ([NetSci](https://netscisociety.net/home), [Complenet](https://complenet.weebly.com/)), data mining ([ICDM](http://icdm2019.bigke.org/), [CIKM](http://www.cikm2019.net/), [KDD](https://www.kdd.org/kdd2020/)), artificial intelligence ([AAAI](http://www.aaai.org/Conferences/conferences.php), [IJCAI](https://www.ijcai.org/)) and machine learning ([NeurIPS](https://nips.cc/), [ICML](https://icml.cc/), [ICLR](https://iclr.cc/)) conferences, workshops, and pieces from prominent journals.    The newly introduced graph classification datasets are available at [SNAP](https://snap.stanford.edu/data/#disjointgraphs), [TUD Graph Kernel Datasets](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets), and [GraphLearning.io](https://chrsmrrs.github.io/datasets/).    --------------------------------------------------------------    **Citing**    If you find *Karate Club* and the new datasets useful in your research, please consider citing the following paper:    ```bibtex  @inproceedings{karateclub,         title = {{Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs}},         author = {Benedek Rozemberczki and Oliver Kiss and Rik Sarkar},         year = {2020},         pages = {3125â€“3132},         booktitle = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)},         organization = {ACM},  }  ```  ----------------------------------------------------------------    **A simple example**    *Karate Club* makes the use of modern community detection techniques quite easy (see [here](https://karateclub.readthedocs.io/en/latest/notes/introduction.html) for the accompanying tutorial). For example, this is all it takes to use on a Watts-Strogatz graph [Ego-splitting](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf):    ```python  import networkx as nx  from karateclub import EgoNetSplitter    g = nx.newman_watts_strogatz_graph(1000, 20, 0.05)    splitter = EgoNetSplitter(1.0)    splitter.fit(g)    print(splitter.get_memberships())  ```    ----------------------------------------------------------------    **Models included**    In detail, the following community detection and embedding methods were implemented.    **Overlapping Community Detection**    * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)    * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)    * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)    * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)    * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)    * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)    **Non-Overlapping Community Detection**    * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)    * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)    * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)    * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)      **Proximity Preserving Node Embedding**    * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?id=2806512) (CIKM 2015)    * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)    * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)    * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)    * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)    * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)    * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)    * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)    * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)    * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)    * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip! Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)    * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)    * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and FÃ©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)    * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)     **Structural Node Level Embedding**    * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)    * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)    **Attributed Node Level Embedding**    * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)    * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)    * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)    * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)     * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)    * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)    * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)    * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)    * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)     **Meta Node Embedding**    * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)    **Graph Level Embedding**    * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)    * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)    * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)    * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)    * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)    * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)    * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)    * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)    * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)     * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)    Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets. For a quick start, check out our [examples](https://github.com/benedekrozemberczki/karateclub/tree/master/examples.py).    If you notice anything unexpected, please open an [issue](https://github.com/benedekrozemberczki/karateclub/issues) and let us know. If you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/karateclub/issues).  We are motivated to constantly make Karate Club even better.      --------------------------------------------------------------------------------    **Installation**    Karate Club can be installed with the following pip command.    ```sh  $ pip install karateclub  ```    As we create new releases frequently, upgrading the package casually might be beneficial.    ```sh  $ pip install karateclub --upgrade  ```    --------------------------------------------------------------------------------    **Running examples**    As part of the documentation we provide a number of use cases to show how the clusterings and embeddings can be utilized for downstream learning. These can accessed [here](https://karateclub.readthedocs.io/en/latest/notes/introduction.html) with detailed line-by-line explanations.      Besides the case studies we provide synthetic examples for each model. These can be tried out by running the example scripts. In order to run one of the examples, the Graph2Vec snippet:    ```sh  $ cd examples/whole_graph_embedding/  $ python graph2vec_example.py  ```    --------------------------------------------------------------------------------    **Running tests**    ```sh  $ python setup.py test  ```    --------------------------------------------------------------------------------    **License**    - [GNU General Public License v3.0](https://github.com/benedekrozemberczki/karateclub/blob/master/LICENSE) """
Big data;https://github.com/rayokota/kareldb;"""# KarelDB - A Relational Database Backed by Apache Kafka    [![Build Status][github-actions-shield]][github-actions-link]  [![Maven][maven-shield]][maven-link]  [![Javadoc][javadoc-shield]][javadoc-link]    [github-actions-shield]: https://github.com/rayokota/kareldb/workflows/build/badge.svg?branch=master  [github-actions-link]: https://github.com/rayokota/kareldb/actions  [maven-shield]: https://img.shields.io/maven-central/v/io.kareldb/kareldb-core.svg  [maven-link]: https://search.maven.org/#search%7Cga%7C1%7Cio.kareldb  [javadoc-shield]: https://javadoc.io/badge/io.kareldb/kareldb-core.svg?color=blue  [javadoc-link]: https://javadoc.io/doc/io.kareldb/kareldb-core    KarelDB is a fully-functional relational database backed by Apache Kafka.    ## Maven    Releases of KarelDB are deployed to Maven Central.    ```xml  <dependency>      <groupId>io.kareldb</groupId>      <artifactId>kareldb-core</artifactId>      <version>1.0.0</version>  </dependency>  ```    ## Server Mode    To run KarelDB, download a [release](https://github.com/rayokota/kareldb/releases), unpack it, and then modify `config/kareldb.properties` to point to an existing Kafka broker.  Then run the following:    ```bash  $ bin/kareldb-start config/kareldb.properties  ```    At a separate terminal, enter the following command to start up `sqlline`, a command-line utility for accessing JDBC databases.    ```  $ bin/sqlline  sqlline version 1.9.0    sqlline> !connect jdbc:avatica:remote:url=http://localhost:8765 admin admin    sqlline> create table books (id int, name varchar, author varchar);  No rows affected (0.114 seconds)    sqlline> insert into books values (1, 'The Trial', 'Franz Kafka');  1 row affected (0.576 seconds)    sqlline> select * from books;  +----+-----------+-------------+  | ID |   NAME    |   AUTHOR    |  +----+-----------+-------------+  | 1  | The Trial | Franz Kafka |  +----+-----------+-------------+  1 row selected (0.133 seconds)  ```    To access a KarelDB server from a remote application, use an Avatica JDBC client.  A list of Avatica JDBC clients can be found [here](https://calcite.apache.org/avatica/docs/).    If multiple KarelDB servers are configured with the same cluster group ID (see [Configuration](#configuration)), then they will form a cluster and one of them will be elected as leader, while the others will become followers (replicas).  If a follower receives a request, it will be forwarded to the leader.  If the leader fails, one of the followers will be elected as the new leader.    ## Embedded Mode    KarelDB can also be used in embedded mode.  Here is an example:    ```java  Properties properties = new Properties();  properties.put(""schemaFactory"", ""io.kareldb.schema.SchemaFactory"");  properties.put(""parserFactory"", ""org.apache.calcite.sql.parser.parserextension.ExtensionSqlParserImpl#FACTORY"");  properties.put(""schema.kind"", ""io.kareldb.kafka.KafkaSchema"");  properties.put(""schema.kafkacache.bootstrap.servers"", bootstrapServers);  properties.put(""schema.kafkacache.data.dir"", ""/tmp"");    try (Connection conn = DriverManager.getConnection(""jdbc:kareldb:"", properties);       Statement s = conn.createStatement()) {          s.execute(""create table books (id int, name varchar, author varchar)"");          s.executeUpdate(""insert into books values(1, 'The Trial', 'Franz Kafka')"");          ResultSet rs = s.executeQuery(""select * from books"");          ...  }  ```    ## ANSI SQL Support    KarelDB supports ANSI SQL, using [Calcite](https://calcite.apache.org/docs/reference.html).      When creating a table, the primary key constraint should be specified after the columns, like so:    ```  CREATE TABLE customers       (id int, name varchar, constraint pk primary key (id));  ```    If no primary key constraint is specified, the first column in the table will be designated as the primary key.    KarelDB extends Calcite's SQL grammar by adding support for ALTER TABLE commands.    ```  alterTableStatement:      ALTER TABLE tableName columnAction [ , columnAction ]*        columnAction:      ( ADD tableElement ) | ( DROP columnName )  ```    KarelDB supports the following SQL types:    - boolean  - integer  - bigint  - real  - double  - varbinary  - varchar  - decimal  - date  - time  - timestamp    ## Basic Configuration    KarelDB has a number of configuration properties that can be specified.  When using KarelDB as an embedded database, these properties should be prefixed with `schema.` before passing them to the JDBC driver.    - `listeners` - List of listener URLs that include the scheme, host, and port.  Defaults to `http://0.0.0.0:8765`.    - `cluster.group.id` - The group ID to be used for leader election.  Defaults to `kareldb`.  - `leader.eligibility` - Whether this node can participate in leader election.  Defaults to true.  - `kafkacache.backing.cache` - The backing cache for KCache, one of `memory` (default), `bdbje`, `lmdb`, `mapdb`, or `rocksdb`.  - `kafkacache.data.dir` - The root directory for backing cache storage.  Defaults to `/tmp`.  - `kafkacache.bootstrap.servers` - A list of host and port pairs to use for establishing the initial connection to Kafka.  - `kafkacache.group.id` - The group ID to use for the internal consumers, which needs to be unique for each node.  Defaults to `kareldb-1`.  - `kafkacache.topic.replication.factor` - The replication factor for the internal topics created by KarelDB.  Defaults to 3.  - `kafkacache.init.timeout.ms` - The timeout for initialization of the Kafka cache, including creation of internal topics.  Defaults to 300 seconds.  - `kafkacache.timeout.ms` - The timeout for an operation on the Kafka cache.  Defaults to 60 seconds.    ## Security    ### HTTPS    To use HTTPS, first configure the `listeners` with an `https` prefix, then specify the following properties with the appropriate values.    ```  ssl.keystore.location=/var/private/ssl/custom.keystore  ssl.keystore.password=changeme  ssl.key.password=changeme  ```    When using the Avatica JDBC client, the `truststore` and `truststore_password` can be passed in the JDBC URL as specified [here](https://calcite.apache.org/avatica/docs/client_reference.html#truststore).    ### HTTP Authentication    KarelDB supports both HTTP Basic Authentication and HTTP Digest Authentication, as shown below:    ```  authentication.method=BASIC  # or DIGEST  authentication.roles=admin,developer,user  authentication.realm=KarelDb-Props  # as specified in JAAS file  ```    In the above example, the JAAS file might look like    ```  KarelDb-Props {    org.eclipse.jetty.jaas.spi.PropertyFileLoginModule required    file=""/path/to/password-file""    debug=""false"";  };  ```    The `ProperyFileLoginModule` can be replaced with other implementations, such as `LdapLoginModule` or `JDBCLoginModule`.    When starting KarelDB, the path to the JAAS file must be set as a system property.    ```bash  $ export KARELDB_OPTS=-Djava.security.auth.login.config=/path/to/the/jaas_config.file  $ bin/kareldb-start config/kareldb-secure.properties  ```    When using the Avatica JDBC client, the `avatica_user` and `avatica_password` can be passed in the JDBC URL as specified [here](https://calcite.apache.org/avatica/docs/client_reference.html#avatica-user).    ### Kafka Authentication    Authentication to a secure Kafka cluster is described [here](https://github.com/rayokota/kcache#security).     ## Implementation Notes    KarelDB stores table data in topics of the form `{tableName}_{generation}`.  A different generation ID is used whenever a table is dropped and re-created.    KarelDB uses three topics to hold metadata:    - `_tables` - A topic that holds the schemas for tables.  - `_commits` - A topic that holds the list of committed transactions.  - `_timestamps` - A topic that stores the maximum timestamp that the transaction manager is allowed to return to clients.    ## Database by Components    KarelDB is an example of a database built mostly by assembling pre-existing components.  In particular, KarelDB uses the following:    - [Apache Kafka](https://kafka.apache.org) - for persistence, using [KCache](https://github.com/rayokota/kcache) as an embedded key-value store  - [Apache Avro](https://avro.apache.org) - for serialization and schema evolution  - [Apache Calcite](https://calcite.apache.org) - for SQL parsing, optimization, and execution  - [Apache Omid](https://omid.incubator.apache.org) - for transaction management and MVCC support  - [Apache Avatica](https://calcite.apache.org/avatica/) - for JDBC functionality    See this [blog](https://yokota.blog/2019/09/23/building-a-relational-database-using-kafka) for more on the design of KarelDB.    ## Future Enhancements     Possible future enhancements include support for secondary indices. """
Big data;https://github.com/baidu/bfs;"""[The Baidu File System](http://github.com/baidu/bfs)  =======     [![Build Status](https://travis-ci.org/baidu/bfs.svg?branch=master)](https://travis-ci.org/baidu/bfs)  [![Build Status](https://scan.coverity.com/projects/8135/badge.svg)](https://scan.coverity.com/projects/myawan-bfs-1/)     The Baidu File System (BFS) is a distributed file system designed to support real-time applications. Like many other distributed file systems, BFS is highly fault-tolerant. But different from others, BFS provides low read/write latency while maintaining high throughput rates. Together with [Galaxy](https://github.com/baidu/galaxy) and [Tera](http://github.com/baidu/tera), BFS supports many real-time products in Baidu, including Baidu webpage database, Baidu incremental indexing system, Baidu user behavior analysis system, etc.    ## Features  1. Continuous availability   	* Nameserver is implemented as a `raft group`, no single point failure.  2. High throughput  	* High performance data engine to maximize IO utils.  3. Low latency  	* Global load balance and slow node detection.  4. Linear scalability  	* Support multi data center deployment and up to 10,000 data nodes.    ## Architecture  ![æž¶æž„å›¾](resources/images/bfs-arch2-mini.png)    ## Quick Start  #### Build        ./build.sh  #### Standalone BFS      cd sandbox      ./deploy.sh      ./start_bfs.sh    ## How to Contribute  1. Please read the [RoadMap](docs/en/roadmap.md) or source code.    2. Find something you are interested in and start working on it.  3. Test your code by simply running `make test` and `make check`.  4. Make a pull request.  5. Once your code has passed the code-review and merged, it will be run on thousands of servers :)      ## Contact us  opensearch@baidu.com    ====    [ç™¾åº¦æ–‡ä»¶ç³»ç»Ÿ](http://github.com/baidu/bfs)  ====    ç™¾åº¦çš„æ ¸å¿ƒä¸šåŠ¡å’Œæ•°æ®åº“ç³»ç»Ÿéƒ½ä¾èµ–åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿä½œä¸ºåº•å±‚å­˜å‚¨ï¼Œæ–‡ä»¶ç³»ç»Ÿçš„å¯ç”¨æ€§å’Œæ€§èƒ½å¯¹ä¸Šå±‚æœç´¢ä¸šåŠ¡çš„ç¨³å®šæ€§ä¸Žæ•ˆæžœæœ‰ç€è‡³å…³é‡è¦çš„å½±å“ã€‚çŽ°æœ‰çš„åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼ˆå¦‚HDFSç­‰ï¼‰æ˜¯ä¸ºç¦»çº¿æ‰¹å¤„ç†è®¾è®¡çš„ï¼Œæ— æ³•åœ¨ä¿è¯é«˜åžåçš„æƒ…å†µä¸‹åšåˆ°ä½Žå»¶è¿Ÿå’ŒæŒç»­å¯ç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»Žæœç´¢çš„ä¸šåŠ¡ç‰¹ç‚¹å‡ºå‘ï¼Œè®¾è®¡äº†ç™¾åº¦æ–‡ä»¶ç³»ç»Ÿã€‚    ## æ ¸å¿ƒç‰¹ç‚¹  1. æŒç»­å¯ç”¨    	* æ•°æ®å¤šæœºæˆ¿ã€å¤šåœ°åŸŸå†—ä½™ï¼Œå…ƒæ•°æ®é€šè¿‡Raftç»´æŠ¤ä¸€è‡´æ€§ï¼Œå•ä¸ªæœºæˆ¿å®•æœºï¼Œä¸å½±å“æ•´ä½“å¯ç”¨æ€§ã€‚    2. é«˜åžå    	* é€šè¿‡é«˜æ€§èƒ½çš„å•æœºå¼•æ“Žï¼Œæœ€å¤§åŒ–å­˜å‚¨ä»‹è´¨IOåžåï¼›    3. ä½Žå»¶æ—¶    	* å…¨å±€è´Ÿè½½å‡è¡¡ã€æ…¢èŠ‚ç‚¹è‡ªåŠ¨è§„é¿    4. æ°´å¹³æ‰©å±•    	* è®¾è®¡æ”¯æŒä¸¤åœ°ä¸‰æœºæˆ¿ï¼Œ1ä¸‡+å°æœºå™¨ç®¡ç†ã€‚      ## æž¶æž„  ![æž¶æž„å›¾](resources/images/bfs-arch2-mini.png)    ## å¿«é€Ÿè¯•ç”¨  #### æž„å»º      ./build.sh  #### å•æœºç‰ˆBFS      cd sandbox      ./deploy.sh      ./start_bfs.sh    ## å¦‚ä½•å‚ä¸Žå¼€å‘  1. é˜…è¯»[RoadMap](docs/cn/roadmap.md)æ–‡ä»¶æˆ–è€…æºä»£ç ï¼Œäº†è§£æˆ‘ä»¬å½“å‰çš„å¼€å‘æ–¹å‘  2. æ‰¾åˆ°è‡ªå·±æ„Ÿå…´è¶£å¼€å‘çš„çš„åŠŸèƒ½æˆ–æ¨¡å—  3. è¿›è¡Œå¼€å‘ï¼Œå¼€å‘å®ŒæˆåŽè‡ªæµ‹åŠŸèƒ½æ˜¯å¦æ­£ç¡®ï¼Œå¹¶è¿è¡Œmake teståŠmake checkæ£€æŸ¥æ˜¯å¦å¯ä»¥é€šè¿‡å·²æœ‰çš„æµ‹è¯•case  4. å‘èµ·pull request  5. åœ¨code-reviewé€šè¿‡åŽï¼Œä½ çš„ä»£ç ä¾¿æœ‰æœºä¼šè¿è¡Œåœ¨ç™¾åº¦çš„æ•°ä¸‡å°æœåŠ¡å™¨ä¸Š~      ## è”ç³»æˆ‘ä»¬  é‚®ä»¶ï¼šopensearch@baidu.com    QQç¾¤ï¼š188471131   """
Big data;https://github.com/VictoriaMetrics/VictoriaMetrics;"""# VictoriaMetrics    [![Latest Release](https://img.shields.io/github/release/VictoriaMetrics/VictoriaMetrics.svg?style=flat-square)](https://github.com/VictoriaMetrics/VictoriaMetrics/releases/latest)  [![Docker Pulls](https://img.shields.io/docker/pulls/victoriametrics/victoria-metrics.svg?maxAge=604800)](https://hub.docker.com/r/victoriametrics/victoria-metrics)  [![Slack](https://img.shields.io/badge/join%20slack-%23victoriametrics-brightgreen.svg)](https://slack.victoriametrics.com/)  [![GitHub license](https://img.shields.io/github/license/VictoriaMetrics/VictoriaMetrics.svg)](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/LICENSE)  [![Go Report](https://goreportcard.com/badge/github.com/VictoriaMetrics/VictoriaMetrics)](https://goreportcard.com/report/github.com/VictoriaMetrics/VictoriaMetrics)  [![Build Status](https://github.com/VictoriaMetrics/VictoriaMetrics/workflows/main/badge.svg)](https://github.com/VictoriaMetrics/VictoriaMetrics/actions)  [![codecov](https://codecov.io/gh/VictoriaMetrics/VictoriaMetrics/branch/master/graph/badge.svg)](https://codecov.io/gh/VictoriaMetrics/VictoriaMetrics)    <img src=""logo.png"" width=""300"" alt=""VictoriaMetrics logo"">    VictoriaMetrics is a fast, cost-effective and scalable monitoring solution and time series database.    VictoriaMetrics is available in [binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases),  [Docker images](https://hub.docker.com/r/victoriametrics/victoria-metrics/), [Snap packages](https://snapcraft.io/victoriametrics)  and [source code](https://github.com/VictoriaMetrics/VictoriaMetrics). Just download VictoriaMetrics and follow [these instructions](#how-to-start-victoriametrics).  Then read [Prometheus setup](#prometheus-setup) and [Grafana setup](#grafana-setup) docs.    Cluster version of VictoriaMetrics is available [here](https://docs.victoriametrics.com/Cluster-VictoriaMetrics.html).    [Contact us](mailto:info@victoriametrics.com) if you need enterprise support for VictoriaMetrics. See [features available in enterprise package](https://victoriametrics.com/products/enterprise/). Enterprise binaries can be downloaded and evaluated for free from [the releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases).      ## Prominent features    VictoriaMetrics has the following prominent features:    * It can be used as long-term storage for Prometheus. See [these docs](#prometheus-setup) for details.  * It can be used as drop-in replacement for Prometheus in Grafana, because it supports [Prometheus querying API](#prometheus-querying-api-usage).  * It can be used as drop-in replacement for Graphite in Grafana, because it supports [Graphite API](#graphite-api-usage).  * It features easy setup and operation:    * VictoriaMetrics consists of a single [small executable](https://medium.com/@valyala/stripping-dependency-bloat-in-victoriametrics-docker-image-983fb5912b0d) without external dependencies.    * All the configuration is done via explicit command-line flags with reasonable defaults.    * All the data is stored in a single directory pointed by `-storageDataPath` command-line flag.    * Easy and fast backups from [instant snapshots](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282) to S3 or GCS can be done with [vmbackup](https://docs.victoriametrics.com/vmbackup.html) / [vmrestore](https://docs.victoriametrics.com/vmrestore.html) tools. See [this article](https://medium.com/@valyala/speeding-up-backups-for-big-time-series-databases-533c1a927883) for more details.  * It implements PromQL-based query language - [MetricsQL](https://docs.victoriametrics.com/MetricsQL.html), which provides improved functionality on top of PromQL.  * It provides global query view. Multiple Prometheus instances or any other data sources may ingest data into VictoriaMetrics. Later this data may be queried via a single query.  * It provides high performance and good vertical and horizontal scalability for both [data ingestion](https://medium.com/@valyala/high-cardinality-tsdb-benchmarks-victoriametrics-vs-timescaledb-vs-influxdb-13e6ee64dd6b) and [data querying](https://medium.com/@valyala/when-size-matters-benchmarking-victoriametrics-vs-timescale-and-influxdb-6035811952d4). It [outperforms InfluxDB and TimescaleDB by up to 20x](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae).  * It [uses 10x less RAM than InfluxDB](https://medium.com/@valyala/insert-benchmarks-with-inch-influxdb-vs-victoriametrics-e31a41ae2893) and [up to 7x less RAM than Prometheus, Thanos or Cortex](https://valyala.medium.com/prometheus-vs-victoriametrics-benchmark-on-node-exporter-metrics-4ca29c75590f) when dealing with millions of unique time series (aka [high cardinality](https://docs.victoriametrics.com/FAQ.html#what-is-high-cardinality)).  * It is optimized for time series with [high churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate).  * It provides high data compression, so [up to 70x more data points](https://medium.com/@valyala/when-size-matters-benchmarking-victoriametrics-vs-timescale-and-influxdb-6035811952d4) may be crammed into limited storage comparing to TimescaleDB and [up to 7x less storage space is required compared to Prometheus, Thanos or Cortex](https://valyala.medium.com/prometheus-vs-victoriametrics-benchmark-on-node-exporter-metrics-4ca29c75590f).  * It is optimized for storage with high-latency IO and low IOPS (HDD and network storage in AWS, Google Cloud, Microsoft Azure, etc). See [disk IO graphs from these benchmarks](https://medium.com/@valyala/high-cardinality-tsdb-benchmarks-victoriametrics-vs-timescaledb-vs-influxdb-13e6ee64dd6b).  * A single-node VictoriaMetrics may substitute moderately sized clusters built with competing solutions such as Thanos, M3DB, Cortex, InfluxDB or TimescaleDB. See [vertical scalability benchmarks](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae), [comparing Thanos to VictoriaMetrics cluster](https://medium.com/@valyala/comparing-thanos-to-victoriametrics-cluster-b193bea1683) and [Remote Write Storage Wars](https://promcon.io/2019-munich/talks/remote-write-storage-wars/) talk from [PromCon 2019](https://promcon.io/2019-munich/talks/remote-write-storage-wars/).  * It protects the storage from data corruption on unclean shutdown (i.e. OOM, hardware reset or `kill -9`) thanks to [the storage architecture](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).  * It supports metrics' scraping, ingestion and [backfilling](#backfilling) via the following protocols:    * [Metrics scraping from Prometheus exporters](#how-to-scrape-prometheus-exporters-such-as-node-exporter).    * [Prometheus remote write API](#prometheus-setup).    * [Prometheus exposition format](#how-to-import-data-in-prometheus-exposition-format).    * [InfluxDB line protocol](#how-to-send-data-from-influxdb-compatible-agents-such-as-telegraf) over HTTP, TCP and UDP.    * [Graphite plaintext protocol](#how-to-send-data-from-graphite-compatible-agents-such-as-statsd) with [tags](https://graphite.readthedocs.io/en/latest/tags.html#carbon).    * [OpenTSDB put message](#sending-data-via-telnet-put-protocol).    * [HTTP OpenTSDB /api/put requests](#sending-opentsdb-data-via-http-apiput-requests).    * [JSON line format](#how-to-import-data-in-json-line-format).    * [Arbitrary CSV data](#how-to-import-csv-data).    * [Native binary format](#how-to-import-data-in-native-format).  * It supports metrics' relabeling. See [these docs](#relabeling) for details.  * It can deal with [high cardinality issues](https://docs.victoriametrics.com/FAQ.html#what-is-high-cardinality) and [high churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate) issues via [series limiter](#cardinality-limiter).  * It ideally works with big amounts of time series data from APM, Kubernetes, IoT sensors, connected cars, industrial telemetry, financial data and various [Enterprise workloads](https://victoriametrics.com/products/enterprise/).  * It has open source [cluster version](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster).    See also [various Articles about VictoriaMetrics](https://docs.victoriametrics.com/Articles.html).      ## Case studies and talks    Case studies:    * [AbiosGaming](https://docs.victoriametrics.com/CaseStudies.html#abiosgaming)  * [adidas](https://docs.victoriametrics.com/CaseStudies.html#adidas)  * [Adsterra](https://docs.victoriametrics.com/CaseStudies.html#adsterra)  * [ARNES](https://docs.victoriametrics.com/CaseStudies.html#arnes)  * [Brandwatch](https://docs.victoriametrics.com/CaseStudies.html#brandwatch)  * [CERN](https://docs.victoriametrics.com/CaseStudies.html#cern)  * [COLOPL](https://docs.victoriametrics.com/CaseStudies.html#colopl)  * [Dreamteam](https://docs.victoriametrics.com/CaseStudies.html#dreamteam)  * [Fly.io](https://docs.victoriametrics.com/CaseStudies.html#flyio)  * [German Research Center for Artificial Intelligence](https://docs.victoriametrics.com/CaseStudies.html#german-research-center-for-artificial-intelligence)  * [Grammarly](https://docs.victoriametrics.com/CaseStudies.html#grammarly)  * [Groove X](https://docs.victoriametrics.com/CaseStudies.html#groove-x)  * [Idealo.de](https://docs.victoriametrics.com/CaseStudies.html#idealode)  * [MHI Vestas Offshore Wind](https://docs.victoriametrics.com/CaseStudies.html#mhi-vestas-offshore-wind)  * [Razorpay](https://docs.victoriametrics.com/CaseStudies.html#razorpay)  * [Percona](https://docs.victoriametrics.com/CaseStudies.html#percona)  * [Sensedia](https://docs.victoriametrics.com/CaseStudies.html#sensedia)  * [Smarkets](https://docs.victoriametrics.com/CaseStudies.html#smarkets)  * [Synthesio](https://docs.victoriametrics.com/CaseStudies.html#synthesio)  * [Wedos.com](https://docs.victoriametrics.com/CaseStudies.html#wedoscom)  * [Wix.com](https://docs.victoriametrics.com/CaseStudies.html#wixcom)  * [Zerodha](https://docs.victoriametrics.com/CaseStudies.html#zerodha)  * [zhihu](https://docs.victoriametrics.com/CaseStudies.html#zhihu)    See also [articles and slides about VictoriaMetrics from our users](https://docs.victoriametrics.com/Articles.html#third-party-articles-and-slides-about-victoriametrics)      ## Operation    ## How to start VictoriaMetrics    Just download [VictoriaMetrics executable](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) or [Docker image](https://hub.docker.com/r/victoriametrics/victoria-metrics/) and start it with the desired command-line flags.    The following command-line flags are used the most:    * `-storageDataPath` - VictoriaMetrics stores all the data in this directory. Default path is `victoria-metrics-data` in the current working directory.  * `-retentionPeriod` - retention for stored data. Older data is automatically deleted. Default retention is 1 month. See [the Retention section](#retention) for more details.    Other flags have good enough default values, so set them only if you really need this. Pass `-help` to see [all the available flags with description and default values](#list-of-command-line-flags).    See how to [ingest data to VictoriaMetrics](#how-to-import-time-series-data), how to [query VictoriaMetrics via Grafana](#grafana-setup), how to [query VictoriaMetrics via Graphite API](#graphite-api-usage) and how to [handle alerts](#alerting).    VictoriaMetrics accepts [Prometheus querying API requests](#prometheus-querying-api-usage) on port `8428` by default.    It is recommended setting up [monitoring](#monitoring) for VictoriaMetrics.      ### Environment variables    Each flag value can be set via environment variables according to these rules:    * The `-envflag.enable` flag must be set.  * Each `.` char in flag name must be substituted with `_` (for example `-insert.maxQueueDuration <duration>` will translate to `insert_maxQueueDuration=<duration>`).  * For repeating flags an alternative syntax can be used by joining the different values into one using `,` char as separator (for example `-storageNode <nodeA> -storageNode <nodeB>` will translate to `storageNode=<nodeA>,<nodeB>`).  * Environment var prefix can be set via `-envflag.prefix` flag. For instance, if `-envflag.prefix=VM_`, then env vars must be prepended with `VM_`.      ### Configuration with snap package      Snap package for VictoriaMetrics is available [here](https://snapcraft.io/victoriametrics).    Command-line flags for Snap package can be set with following command:    ```text  echo 'FLAGS=""-selfScrapeInterval=10s -search.logSlowQueryDuration=20s""' > $SNAP_DATA/var/snap/victoriametrics/current/extra_flags  snap restart victoriametrics  ```    Do not change value for `-storageDataPath` flag, because snap package has limited access to host filesystem.      Changing scrape configuration is possible with text editor:    ```text  vi $SNAP_DATA/var/snap/victoriametrics/current/etc/victoriametrics-scrape-config.yaml  ```    After changes were made, trigger config re-read with the command `curl 127.0.0.1:8248/-/reload`.      ## Prometheus setup    Add the following lines to Prometheus config file (it is usually located at `/etc/prometheus/prometheus.yml`) in order to send data to VictoriaMetrics:    ```yml  remote_write:    - url: http://<victoriametrics-addr>:8428/api/v1/write  ```    Substitute `<victoriametrics-addr>` with hostname or IP address of VictoriaMetrics.  Then apply new config via the following command:    ```bash  kill -HUP `pidof prometheus`  ```    Prometheus writes incoming data to local storage and replicates it to remote storage in parallel.  This means that data remains available in local storage for `--storage.tsdb.retention.time` duration  even if remote storage is unavailable.    If you plan sending data to VictoriaMetrics from multiple Prometheus instances, then add the following lines into `global` section  of [Prometheus config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file):    ```yml  global:    external_labels:      datacenter: dc-123  ```    This instructs Prometheus to add `datacenter=dc-123` label to each sample before sending it to remote storage.  The label name can be arbitrary - `datacenter` is just an example. The label value must be unique  across Prometheus instances, so time series could be filtered and grouped by this label.    For highly loaded Prometheus instances (200k+ samples per second) the following tuning may be applied:    ```yaml  remote_write:    - url: http://<victoriametrics-addr>:8428/api/v1/write      queue_config:        max_samples_per_send: 10000        capacity: 20000        max_shards: 30  ```    Using remote write increases memory usage for Prometheus by up to ~25%. If you are experiencing issues with  too high memory consumption of Prometheus, then try to lower `max_samples_per_send` and `capacity` params. Keep in mind that these two params are tightly connected.  Read more about tuning remote write for Prometheus [here](https://prometheus.io/docs/practices/remote_write).    It is recommended upgrading Prometheus to [v2.12.0](https://github.com/prometheus/prometheus/releases) or newer, since previous versions may have issues with `remote_write`.    Take a look also at [vmagent](https://docs.victoriametrics.com/vmagent.html) and [vmalert](https://docs.victoriametrics.com/vmalert.html),  which can be used as faster and less resource-hungry alternative to Prometheus.      ## Grafana setup    Create [Prometheus datasource](http://docs.grafana.org/features/datasources/prometheus/) in Grafana with the following url:    ```url  http://<victoriametrics-addr>:8428  ```    Substitute `<victoriametrics-addr>` with the hostname or IP address of VictoriaMetrics.    Then build graphs and dashboards for the created datasource using [PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/) or [MetricsQL](https://docs.victoriametrics.com/MetricsQL.html).      ## How to upgrade VictoriaMetrics    It is safe upgrading VictoriaMetrics to new versions unless [release notes](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) say otherwise. It is safe skipping multiple versions during the upgrade unless [release notes](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) say otherwise. It is recommended performing regular upgrades to the latest version, since it may contain important bug fixes, performance optimizations or new features.    It is also safe downgrading to older versions unless [release notes](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) say otherwise.    The following steps must be performed during the upgrade / downgrade procedure:    * Send `SIGINT` signal to VictoriaMetrics process in order to gracefully stop it.  * Wait until the process stops. This can take a few seconds.  * Start the upgraded VictoriaMetrics.    Prometheus doesn't drop data during VictoriaMetrics restart. See [this article](https://grafana.com/blog/2019/03/25/whats-new-in-prometheus-2.8-wal-based-remote-write/) for details. The same applies also to [vmagent](https://docs.victoriametrics.com/vmagent.html).      ## How to apply new config to VictoriaMetrics    VictoriaMetrics is configured via command-line flags, so it must be restarted when new command-line flags should be applied:    * Send `SIGINT` signal to VictoriaMetrics process in order to gracefully stop it.  * Wait until the process stops. This can take a few seconds.  * Start VictoriaMetrics with the new command-line flags.    Prometheus doesn't drop data during VictoriaMetrics restart. See [this article](https://grafana.com/blog/2019/03/25/whats-new-in-prometheus-2.8-wal-based-remote-write/) for details. The same applies alos to [vmagent](https://docs.victoriametrics.com/vmagent.html).      ## How to scrape Prometheus exporters such as [node-exporter](https://github.com/prometheus/node_exporter)    VictoriaMetrics can be used as drop-in replacement for Prometheus for scraping targets configured in `prometheus.yml` config file according to [the specification](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file). Just set `-promscrape.config` command-line flag to the path to `prometheus.yml` config - and VictoriaMetrics should start scraping the configured targets. Currently the following [scrape_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config) types are supported:    * [static_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#static_config)  * [file_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#file_sd_config)  * [kubernetes_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config)  * [ec2_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config)  * [gce_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#gce_sd_config)  * [consul_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config)  * [dns_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dns_sd_config)  * [openstack_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#openstack_sd_config)  * [docker_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#docker_sd_config)  * [dockerswarm_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dockerswarm_sd_config)  * [eureka_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#eureka_sd_config)  * [digitalocean_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#digitalocean_sd_config)  * [http_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#http_sd_config)      File a [feature request](https://github.com/VictoriaMetrics/VictoriaMetrics/issues) if you need support for other `*_sd_config` types.    The file pointed by `-promscrape.config` may contain `%{ENV_VAR}` placeholders, which are substituted by the corresponding `ENV_VAR` environment variable values.    VictoriaMetrics also supports [importing data in Prometheus exposition format](#how-to-import-data-in-prometheus-exposition-format).    See also [vmagent](https://docs.victoriametrics.com/vmagent.html), which can be used as drop-in replacement for Prometheus.      ## How to send data from DataDog agent    VictoriaMetrics accepts data from [DataDog agent](https://docs.datadoghq.com/agent/) or [DogStatsD]() via [""submit metrics"" API](https://docs.datadoghq.com/api/latest/metrics/#submit-metrics) at `/datadog/api/v1/series` path.    Run DataDog agent with `DD_DD_URL=http://victoriametrics-host:8428/datadog` environment variable in order to write data to VictoriaMetrics at `victoriametrics-host` host. Another option is to set `dd_url` param at [DataDog agent configuration file](https://docs.datadoghq.com/agent/guide/agent-configuration-files/) to `http://victoriametrics-host:8428/datadog`.    VictoriaMetrics doesn't check `DD_API_KEY` param, so it can be set to arbitrary value.    Example on how to send data to VictoriaMetrics via DataDog ""submit metrics"" API from command line:    ```bash  echo '  {    ""series"": [      {        ""host"": ""test.example.com"",        ""interval"": 20,        ""metric"": ""system.load.1"",        ""points"": [[          0,          0.5        ]],        ""tags"": [          ""environment:test""        ],        ""type"": ""rate""      }    ]  }  ' | curl -X POST --data-binary @- http://localhost:8428/datadog/api/v1/series  ```    The imported data can be read via [export API](https://docs.victoriametrics.com/#how-to-export-data-in-json-line-format):    ```bash  curl http://localhost:8428/api/v1/export -d 'match[]=system.load.1'  ```    This command should return the following output if everything is OK:    ```  {""metric"":{""__name__"":""system.load.1"",""environment"":""test"",""host"":""test.example.com""},""values"":[0.5],""timestamps"":[1632833641000]}  ```    Extra labels may be added to all the written time series by passing `extra_label=name=value` query args.  For example, `/datadog/api/v1/series?extra_label=foo=bar` would add `{foo=""bar""}` label to all the ingested metrics.      ## How to send data from InfluxDB-compatible agents such as [Telegraf](https://www.influxdata.com/time-series-platform/telegraf/)    Use `http://<victoriametric-addr>:8428` url instead of InfluxDB url in agents' configs.  For instance, put the following lines into `Telegraf` config, so it sends data to VictoriaMetrics instead of InfluxDB:    ```toml  [[outputs.influxdb]]    urls = [""http://<victoriametrics-addr>:8428""]  ```    Another option is to enable TCP and UDP receiver for InfluxDB line protocol via `-influxListenAddr` command-line flag  and stream plain InfluxDB line protocol data to the configured TCP and/or UDP addresses.    VictoriaMetrics performs the following transformations to the ingested InfluxDB data:    * [`db` query arg](https://docs.influxdata.com/influxdb/v1.7/tools/api/#write-http-endpoint) is mapped into `db` label value    unless `db` tag exists in the InfluxDB line. The `db` label name can be overriden via `-influxDBLabel` command-line flag.  * Field names are mapped to time series names prefixed with `{measurement}{separator}` value, where `{separator}` equals to `_` by default. It can be changed with `-influxMeasurementFieldSeparator` command-line flag. See also `-influxSkipSingleField` command-line flag. If `{measurement}` is empty or if `-influxSkipMeasurement` command-line flag is set, then time series names correspond to field names.  * Field values are mapped to time series values.  * Tags are mapped to Prometheus labels as-is.    For example, the following InfluxDB line:    ```raw  foo,tag1=value1,tag2=value2 field1=12,field2=40  ```    is converted into the following Prometheus data points:    ```raw  foo_field1{tag1=""value1"", tag2=""value2""} 12  foo_field2{tag1=""value1"", tag2=""value2""} 40  ```    Example for writing data with [InfluxDB line protocol](https://docs.influxdata.com/influxdb/v1.7/write_protocols/line_protocol_tutorial/)  to local VictoriaMetrics using `curl`:    ```bash  curl -d 'measurement,tag1=value1,tag2=value2 field1=123,field2=1.23' -X POST 'http://localhost:8428/write'  ```    An arbitrary number of lines delimited by '\n' (aka newline char) can be sent in a single request.  After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match={__name__=~""measurement_.*""}'  ```    The `/api/v1/export` endpoint should return the following response:    ```jsonl  {""metric"":{""__name__"":""measurement_field1"",""tag1"":""value1"",""tag2"":""value2""},""values"":[123],""timestamps"":[1560272508147]}  {""metric"":{""__name__"":""measurement_field2"",""tag1"":""value1"",""tag2"":""value2""},""values"":[1.23],""timestamps"":[1560272508147]}  ```    Note that InfluxDB line protocol expects [timestamps in *nanoseconds* by default](https://docs.influxdata.com/influxdb/v1.7/write_protocols/line_protocol_tutorial/#timestamp),  while VictoriaMetrics stores them with *milliseconds* precision.    Extra labels may be added to all the written time series by passing `extra_label=name=value` query args.  For example, `/write?extra_label=foo=bar` would add `{foo=""bar""}` label to all the ingested metrics.    Some plugins for Telegraf such as [fluentd](https://github.com/fangli/fluent-plugin-influxdb), [Juniper/open-nti](https://github.com/Juniper/open-nti)  or [Juniper/jitmon](https://github.com/Juniper/jtimon) send `SHOW DATABASES` query to `/query` and expect a particular database name in the response.  Comma-separated list of expected databases can be passed to VictoriaMetrics via `-influx.databaseNames` command-line flag.    ## How to send data from Graphite-compatible agents such as [StatsD](https://github.com/etsy/statsd)    Enable Graphite receiver in VictoriaMetrics by setting `-graphiteListenAddr` command line flag. For instance,  the following command will enable Graphite receiver in VictoriaMetrics on TCP and UDP port `2003`:    ```bash  /path/to/victoria-metrics-prod -graphiteListenAddr=:2003  ```    Use the configured address in Graphite-compatible agents. For instance, set `graphiteHost`  to the VictoriaMetrics host in `StatsD` configs.    Example for writing data with Graphite plaintext protocol to local VictoriaMetrics using `nc`:    ```bash  echo ""foo.bar.baz;tag1=value1;tag2=value2 123 `date +%s`"" | nc -N localhost 2003  ```    VictoriaMetrics sets the current time if the timestamp is omitted.  An arbitrary number of lines delimited by `\n` (aka newline char) can be sent in one go.  After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match=foo.bar.baz'  ```    The `/api/v1/export` endpoint should return the following response:    ```bash  {""metric"":{""__name__"":""foo.bar.baz"",""tag1"":""value1"",""tag2"":""value2""},""values"":[123],""timestamps"":[1560277406000]}  ```    ## Querying Graphite data    Data sent to VictoriaMetrics via `Graphite plaintext protocol` may be read via the following APIs:    * [Graphite API](#graphite-api-usage)  * [Prometheus querying API](#prometheus-querying-api-usage). See also [selecting Graphite metrics](#selecting-graphite-metrics).  * [go-graphite/carbonapi](https://github.com/go-graphite/carbonapi/blob/main/cmd/carbonapi/carbonapi.example.victoriametrics.yaml)    ## Selecting Graphite metrics    VictoriaMetrics supports `__graphite__` pseudo-label for selecting time series with Graphite-compatible filters in [MetricsQL](https://docs.victoriametrics.com/MetricsQL.html). For example, `{__graphite__=""foo.*.bar""}` is equivalent to `{__name__=~""foo[.][^.]*[.]bar""}`, but it works faster and it is easier to use when migrating from Graphite to VictoriaMetrics. See [docs for Graphite paths and wildcards](https://graphite.readthedocs.io/en/latest/render_api.html#paths-and-wildcards). VictoriaMetrics also supports [label_graphite_group](https://docs.victoriametrics.com/MetricsQL.html#label_graphite_group) function for extracting the given groups from Graphite metric name.    The `__graphite__` pseudo-label supports e.g. alternate regexp filters such as `(value1|...|valueN)`. They are transparently converted to `{value1,...,valueN}` syntax [used in Graphite](https://graphite.readthedocs.io/en/latest/render_api.html#paths-and-wildcards). This allows using [multi-value template variables in Grafana](https://grafana.com/docs/grafana/latest/variables/formatting-multi-value-variables/) inside `__graphite__` pseudo-label. For example, Grafana expands `{__graphite__=~""foo.($bar).baz""}` into `{__graphite__=~""foo.(x|y).baz""}` if `$bar` template variable contains `x` and `y` values. In this case the query is automatically converted into `{__graphite__=~""foo.{x,y}.baz""}` before execution.    ## How to send data from OpenTSDB-compatible agents    VictoriaMetrics supports [telnet put protocol](http://opentsdb.net/docs/build/html/api_telnet/put.html)  and [HTTP /api/put requests](http://opentsdb.net/docs/build/html/api_http/put.html) for ingesting OpenTSDB data.  The same protocol is used for [ingesting data in KairosDB](https://kairosdb.github.io/docs/PushingData.html).    ### Sending data via `telnet put` protocol    Enable OpenTSDB receiver in VictoriaMetrics by setting `-opentsdbListenAddr` command line flag. For instance,  the following command enables OpenTSDB receiver in VictoriaMetrics on TCP and UDP port `4242`:    ```bash  /path/to/victoria-metrics-prod -opentsdbListenAddr=:4242  ```    Send data to the given address from OpenTSDB-compatible agents.    Example for writing data with OpenTSDB protocol to local VictoriaMetrics using `nc`:    ```bash  echo ""put foo.bar.baz `date +%s` 123 tag1=value1 tag2=value2"" | nc -N localhost 4242  ```    An arbitrary number of lines delimited by `\n` (aka newline char) can be sent in one go.  After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match=foo.bar.baz'  ```    The `/api/v1/export` endpoint should return the following response:    ```bash  {""metric"":{""__name__"":""foo.bar.baz"",""tag1"":""value1"",""tag2"":""value2""},""values"":[123],""timestamps"":[1560277292000]}  ```    ### Sending OpenTSDB data via HTTP `/api/put` requests    Enable HTTP server for OpenTSDB `/api/put` requests by setting `-opentsdbHTTPListenAddr` command line flag. For instance,  the following command enables OpenTSDB HTTP server on port `4242`:    ```bash  /path/to/victoria-metrics-prod -opentsdbHTTPListenAddr=:4242  ```    Send data to the given address from OpenTSDB-compatible agents.    Example for writing a single data point:    ```bash  curl -H 'Content-Type: application/json' -d '{""metric"":""x.y.z"",""value"":45.34,""tags"":{""t1"":""v1"",""t2"":""v2""}}' http://localhost:4242/api/put  ```    Example for writing multiple data points in a single request:    ```bash  curl -H 'Content-Type: application/json' -d '[{""metric"":""foo"",""value"":45.34},{""metric"":""bar"",""value"":43}]' http://localhost:4242/api/put  ```    After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match[]=x.y.z' -d 'match[]=foo' -d 'match[]=bar'  ```    The `/api/v1/export` endpoint should return the following response:    ```bash  {""metric"":{""__name__"":""foo""},""values"":[45.34],""timestamps"":[1566464846000]}  {""metric"":{""__name__"":""bar""},""values"":[43],""timestamps"":[1566464846000]}  {""metric"":{""__name__"":""x.y.z"",""t1"":""v1"",""t2"":""v2""},""values"":[45.34],""timestamps"":[1566464763000]}  ```    Extra labels may be added to all the imported time series by passing `extra_label=name=value` query args.  For example, `/api/put?extra_label=foo=bar` would add `{foo=""bar""}` label to all the ingested metrics.      ## Prometheus querying API usage    VictoriaMetrics supports the following handlers from [Prometheus querying API](https://prometheus.io/docs/prometheus/latest/querying/api/):    * [/api/v1/query](https://prometheus.io/docs/prometheus/latest/querying/api/#instant-queries)  * [/api/v1/query_range](https://prometheus.io/docs/prometheus/latest/querying/api/#range-queries)  * [/api/v1/series](https://prometheus.io/docs/prometheus/latest/querying/api/#finding-series-by-label-matchers)  * [/api/v1/labels](https://prometheus.io/docs/prometheus/latest/querying/api/#getting-label-names)  * [/api/v1/label/.../values](https://prometheus.io/docs/prometheus/latest/querying/api/#querying-label-values)  * [/api/v1/status/tsdb](https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-stats). See [these docs](#tsdb-stats) for details.  * [/api/v1/targets](https://prometheus.io/docs/prometheus/latest/querying/api/#targets) - see [these docs](#how-to-scrape-prometheus-exporters-such-as-node-exporter) for more details.    These handlers can be queried from Prometheus-compatible clients such as Grafana or curl.  All the Prometheus querying API handlers can be prepended with `/prometheus` prefix. For example, both `/prometheus/api/v1/query` and `/api/v1/query` should work.      ### Prometheus querying API enhancements    VictoriaMetrics accepts optional `extra_label=<label_name>=<label_value>` query arg, which can be used for enforcing additional label filters for queries. For example,  `/api/v1/query_range?extra_label=user_id=123&extra_label=group_id=456&query=<query>` would automatically add `{user_id=""123"",group_id=""456""}` label filters to the given `<query>`. This functionality can be used for limiting the scope of time series visible to the given tenant. It is expected that the `extra_label` query args are automatically set by auth proxy sitting in front of VictoriaMetrics. See [vmauth](https://docs.victoriametrics.com/vmauth.html) and [vmgateway](https://docs.victoriametrics.com/vmgateway.html) as examples of such proxies.    VictoriaMetrics accepts optional `extra_filters[]=series_selector` query arg, which can be used for enforcing arbitrary label filters for queries. For example,  `/api/v1/query_range?extra_filters[]={env=~""prod|staging"",user=""xyz""}&query=<query>` would automatically add `{env=~""prod|staging"",user=""xyz""}` label filters to the given `<query>`. This functionality can be used for limiting the scope of time series visible to the given tenant. It is expected that the `extra_filters[]` query args are automatically set by auth proxy sitting in front of VictoriaMetrics. See [vmauth](https://docs.victoriametrics.com/vmauth.html) and [vmgateway](https://docs.victoriametrics.com/vmgateway.html) as examples of such proxies.    VictoriaMetrics accepts relative times in `time`, `start` and `end` query args additionally to unix timestamps and [RFC3339](https://www.ietf.org/rfc/rfc3339.txt).  For example, the following query would return data for the last 30 minutes: `/api/v1/query_range?start=-30m&query=...`.    VictoriaMetrics accepts `round_digits` query arg for `/api/v1/query` and `/api/v1/query_range` handlers. It can be used for rounding response values to the given number of digits after the decimal point. For example, `/api/v1/query?query=avg_over_time(temperature[1h])&round_digits=2` would round response values to up to two digits after the decimal point.    By default, VictoriaMetrics returns time series for the last 5 minutes from `/api/v1/series`, while the Prometheus API defaults to all time.  Use `start` and `end` to select a different time range.    Additionally VictoriaMetrics provides the following handlers:    * `/vmui` - Basic Web UI. See [these docs](#vmui).  * `/api/v1/series/count` - returns the total number of time series in the database. Some notes:    * the handler scans all the inverted index, so it can be slow if the database contains tens of millions of time series;    * the handler may count [deleted time series](#how-to-delete-time-series) additionally to normal time series due to internal implementation restrictions;  * `/api/v1/labels/count` - returns a list of `label: values_count` entries. It can be used for determining labels with the maximum number of values.  * `/api/v1/status/active_queries` - returns a list of currently running queries.  * `/api/v1/status/top_queries` - returns the following query lists:    * the most frequently executed queries - `topByCount`    * queries with the biggest average execution duration - `topByAvgDuration`    * queries that took the most time for execution - `topBySumDuration`      The number of returned queries can be limited via `topN` query arg. Old queries can be filtered out with `maxLifetime` query arg.    For example, request to `/api/v1/status/top_queries?topN=5&maxLifetime=30s` would return up to 5 queries per list, which were executed during the last 30 seconds.    VictoriaMetrics tracks the last `-search.queryStats.lastQueriesCount` queries with durations at least `-search.queryStats.minQueryDuration`.      ## Graphite API usage    VictoriaMetrics supports the following Graphite APIs, which are needed for [Graphite datasource in Grafana](https://grafana.com/docs/grafana/latest/datasources/graphite/):    * Render API - see [these docs](#graphite-render-api-usage).  * Metrics API - see [these docs](#graphite-metrics-api-usage).  * Tags API - see [these docs](#graphite-tags-api-usage).    All the Graphite handlers can be pre-pended with `/graphite` prefix. For example, both `/graphite/metrics/find` and `/metrics/find` should work.    VictoriaMetrics accepts optional query args: `extra_label=<label_name>=<label_value>` and `extra_filters[]=series_selector` query args for all the Graphite APIs. These args can be used for limiting the scope of time series visible to the given tenant. It is expected that the `extra_label` query arg is automatically set by auth proxy sitting in front of VictoriaMetrics. See [vmauth](https://docs.victoriametrics.com/vmauth.html) and [vmgateway](https://docs.victoriametrics.com/vmgateway.html) as examples of such proxies.    [Contact us](mailto:sales@victoriametrics.com) if you need assistance with such a proxy.    VictoriaMetrics supports `__graphite__` pseudo-label for filtering time series with Graphite-compatible filters in [MetricsQL](https://docs.victoriametrics.com/MetricsQL.html). See [these docs](#selecting-graphite-metrics).      ### Graphite Render API usage    [VictoriaMetrics Enterprise](https://victoriametrics.com/products/enterprise/) supports [Graphite Render API](https://graphite.readthedocs.io/en/stable/render_api.html) subset  at `/render` endpoint, which is used by [Graphite datasource in Grafana](https://grafana.com/docs/grafana/latest/datasources/graphite/).  When configuring Graphite datasource in Grafana, the `Storage-Step` http request header must be set to a step between Graphite data points stored in VictoriaMetrics. For example, `Storage-Step: 10s` would mean 10 seconds distance between Graphite datapoints stored in VictoriaMetrics.  Enterprise binaries can be downloaded and evaluated for free from [the releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases).      ### Graphite Metrics API usage    VictoriaMetrics supports the following handlers from [Graphite Metrics API](https://graphite-api.readthedocs.io/en/latest/api.html#the-metrics-api):    * [/metrics/find](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-find)  * [/metrics/expand](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-expand)  * [/metrics/index.json](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-index-json)    VictoriaMetrics accepts the following additional query args at `/metrics/find` and `/metrics/expand`:    * `label` - for selecting arbitrary label values. By default `label=__name__`, i.e. metric names are selected.    * `delimiter` - for using different delimiters in metric name hierachy. For example, `/metrics/find?delimiter=_&query=node_*` would return all the metric name prefixes      that start with `node_`. By default `delimiter=.`.      ### Graphite Tags API usage    VictoriaMetrics supports the following handlers from [Graphite Tags API](https://graphite.readthedocs.io/en/stable/tags.html):    * [/tags/tagSeries](https://graphite.readthedocs.io/en/stable/tags.html#adding-series-to-the-tagdb)  * [/tags/tagMultiSeries](https://graphite.readthedocs.io/en/stable/tags.html#adding-series-to-the-tagdb)  * [/tags](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags)  * [/tags/{tag_name}](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags)  * [/tags/findSeries](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags)  * [/tags/autoComplete/tags](https://graphite.readthedocs.io/en/stable/tags.html#auto-complete-support)  * [/tags/autoComplete/values](https://graphite.readthedocs.io/en/stable/tags.html#auto-complete-support)  * [/tags/delSeries](https://graphite.readthedocs.io/en/stable/tags.html#removing-series-from-the-tagdb)      ## vmui    VictoriaMetrics provides UI for query troubleshooting and exploration. The UI is available at `http://victoriametrics:8428/vmui`.  The UI allows exploring query results via graphs and tables. Graphs support scrolling and zooming:    * Drag the graph to the left / right in order to move the displayed time range into the past / future.  * Hold `Ctrl` (or `Cmd` on MacOS) and scroll up / down in order to zoom in / out the graph.    Query history can be navigated by holding `Ctrl` (or `Cmd` on MacOS) and pressing `up` or `down` arrows on the keyboard while the cursor is located in the query input field.    When querying the [backfilled data](https://docs.victoriametrics.com/#backfilling), it may be useful disabling response cache by clicking `Enable cache` checkbox.    VMUI automatically adjusts the interval between datapoints on the graph depending on the horizontal resolution and on the selected time range. The step value can be customized by clickhing `Override step value` checkbox.    VMUI allows investigating correlations between two queries on the same graph. Just click `+Query` button, enter the second query in the newly appeared input field and press `Ctrl+Enter`. Results for both queries should be displayed simultaneously on the same graph. Every query has its own vertical scale, which is displayed on the left and the right side of the graph. Lines for the second query are dashed.    See the [example VMUI at VictoriaMetrics playground](https://play.victoriametrics.com/select/accounting/1/6a716b0f-38bc-4856-90ce-448fd713e3fe/prometheus/graph/?g0.expr=100%20*%20sum(rate(process_cpu_seconds_total))%20by%20(job)&g0.range_input=1d).      ## How to build from sources    We recommend using either [binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) or  [docker images](https://hub.docker.com/r/victoriametrics/victoria-metrics/) instead of building VictoriaMetrics  from sources. Building from sources is reasonable when developing additional features specific  to your needs or when testing bugfixes.    ### Development build    1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.17.  2. Run `make victoria-metrics` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics` binary and puts it into the `bin` folder.    ### Production build    1. [Install docker](https://docs.docker.com/install/).  2. Run `make victoria-metrics-prod` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics-prod` binary and puts it into the `bin` folder.    ### ARM build    ARM build may run on Raspberry Pi or on [energy-efficient ARM servers](https://blog.cloudflare.com/arm-takes-wing/).    ### Development ARM build    1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.17.  2. Run `make victoria-metrics-arm` or `make victoria-metrics-arm64` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics-arm` or `victoria-metrics-arm64` binary respectively and puts it into the `bin` folder.    ### Production ARM build    1. [Install docker](https://docs.docker.com/install/).  2. Run `make victoria-metrics-arm-prod` or `make victoria-metrics-arm64-prod` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics-arm-prod` or `victoria-metrics-arm64-prod` binary respectively and puts it into the `bin` folder.    ### Pure Go build (CGO_ENABLED=0)    `Pure Go` mode builds only Go code without [cgo](https://golang.org/cmd/cgo/) dependencies.    1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.17.  2. Run `make victoria-metrics-pure` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics-pure` binary and puts it into the `bin` folder.    ### Building docker images    Run `make package-victoria-metrics`. It builds `victoriametrics/victoria-metrics:<PKG_TAG>` docker image locally.  `<PKG_TAG>` is auto-generated image tag, which depends on source code in the repository.  The `<PKG_TAG>` may be manually set via `PKG_TAG=foobar make package-victoria-metrics`.    The base docker image is [alpine](https://hub.docker.com/_/alpine) but it is possible to use any other base image  by setting it via `<ROOT_IMAGE>` environment variable.  For example, the following command builds the image on top of [scratch](https://hub.docker.com/_/scratch) image:    ```bash  ROOT_IMAGE=scratch make package-victoria-metrics  ```    ## Start with docker-compose    [Docker-compose](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/deployment/docker/docker-compose.yml)  helps to spin up VictoriaMetrics, [vmagent](https://docs.victoriametrics.com/vmagent.html) and Grafana with one command.  More details may be found [here](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/master/deployment/docker#folder-contains-basic-images-and-tools-for-building-and-running-victoria-metrics-in-docker).      ## Setting up service    Read [these instructions](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/43) on how to set up VictoriaMetrics as a service in your OS.  There is also [snap package for Ubuntu](https://snapcraft.io/victoriametrics).      ## How to work with snapshots    VictoriaMetrics can create [instant snapshots](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282)  for all the data stored under `-storageDataPath` directory.  Navigate to `http://<victoriametrics-addr>:8428/snapshot/create` in order to create an instant snapshot.  The page will return the following JSON response:    ```json  {""status"":""ok"",""snapshot"":""<snapshot-name>""}  ```    Snapshots are created under `<-storageDataPath>/snapshots` directory, where `<-storageDataPath>`  is the command-line flag value. Snapshots can be archived to backup storage at any time  with [vmbackup](https://docs.victoriametrics.com/vmbackup.html).    The `http://<victoriametrics-addr>:8428/snapshot/list` page contains the list of available snapshots.    Navigate to `http://<victoriametrics-addr>:8428/snapshot/delete?snapshot=<snapshot-name>` in order  to delete `<snapshot-name>` snapshot.    Navigate to `http://<victoriametrics-addr>:8428/snapshot/delete_all` in order to delete all the snapshots.    Steps for restoring from a snapshot:    1. Stop VictoriaMetrics with `kill -INT`.  2. Restore snapshot contents from backup with [vmrestore](https://docs.victoriametrics.com/vmrestore.html)     to the directory pointed by `-storageDataPath`.  3. Start VictoriaMetrics.    ## How to delete time series    Send a request to `http://<victoriametrics-addr>:8428/api/v1/admin/tsdb/delete_series?match[]=<timeseries_selector_for_delete>`,  where `<timeseries_selector_for_delete>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)  for metrics to delete. After that all the time series matching the given selector are deleted. Storage space for  the deleted time series isn't freed instantly - it is freed during subsequent [background merges of data files](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).  Note that background merges may never occur for data from previous months, so storage space won't be freed for historical data.  In this case [forced merge](#forced-merge) may help freeing up storage space.    It is recommended verifying which metrics will be deleted with the call to `http://<victoria-metrics-addr>:8428/api/v1/series?match[]=<timeseries_selector_for_delete>`  before actually deleting the metrics.  By default this query will only scan series in the past 5 minutes, so you may need to  adjust `start` and `end` to a suitable range to achieve match hits.    The `/api/v1/admin/tsdb/delete_series` handler may be protected with `authKey` if `-deleteAuthKey` command-line flag is set.    The delete API is intended mainly for the following cases:    * One-off deleting of accidentally written invalid (or undesired) time series.  * One-off deleting of user data due to [GDPR](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation).    Using the delete API is not recommended in the following cases, since it brings a non-zero overhead:    * Regular cleanups for unneeded data. Just prevent writing unneeded data into VictoriaMetrics.    This can be done with [relabeling](#relabeling).    See [this article](https://www.robustperception.io/relabelling-can-discard-targets-timeseries-and-alerts) for details.  * Reducing disk space usage by deleting unneeded time series. This doesn't work as expected, since the deleted    time series occupy disk space until the next merge operation, which can never occur when deleting too old data.    [Forced merge](#forced-merge) may be used for freeing up disk space occupied by old data.    It's better to use the `-retentionPeriod` command-line flag for efficient pruning of old data.      ## Forced merge    VictoriaMetrics performs [data compactions in background](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282)  in order to keep good performance characteristics when accepting new data. These compactions (merges) are performed independently on per-month partitions.  This means that compactions are stopped for per-month partitions if no new data is ingested into these partitions.  Sometimes it is necessary to trigger compactions for old partitions. For instance, in order to free up disk space occupied by [deleted time series](#how-to-delete-time-series).  In this case forced compaction may be initiated on the specified per-month partition by sending request to `/internal/force_merge?partition_prefix=YYYY_MM`,  where `YYYY_MM` is per-month partition name. For example, `http://victoriametrics:8428/internal/force_merge?partition_prefix=2020_08` would initiate forced  merge for August 2020 partition. The call to `/internal/force_merge` returns immediately, while the corresponding forced merge continues running in background.    Forced merges may require additional CPU, disk IO and storage space resources. It is unnecessary to run forced merge under normal conditions,  since VictoriaMetrics automatically performs [optimal merges in background](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282)  when new data is ingested into it.      ## How to export time series    VictoriaMetrics provides the following handlers for exporting data:    * `/api/v1/export` for exporing data in JSON line format. See [these docs](#how-to-export-data-in-json-line-format) for details.  * `/api/v1/export/csv` for exporting data in CSV. See [these docs](#how-to-export-csv-data) for details.  * `/api/v1/export/native` for exporting data in native binary format. This is the most efficient format for data export.    See [these docs](#how-to-export-data-in-native-format) for details.      ### How to export data in JSON line format    Send a request to `http://<victoriametrics-addr>:8428/api/v1/export?match[]=<timeseries_selector_for_export>`,  where `<timeseries_selector_for_export>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)  for metrics to export. Use `{__name__!=""""}` selector for fetching all the time series.  The response would contain all the data for the selected time series in [JSON streaming format](https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON).  Each JSON line contains samples for a single time series. An example output:    ```jsonl  {""metric"":{""__name__"":""up"",""job"":""node_exporter"",""instance"":""localhost:9100""},""values"":[0,0,0],""timestamps"":[1549891472010,1549891487724,1549891503438]}  {""metric"":{""__name__"":""up"",""job"":""prometheus"",""instance"":""localhost:9090""},""values"":[1,1,1],""timestamps"":[1549891461511,1549891476511,1549891491511]}  ```    Optional `start` and `end` args may be added to the request in order to limit the time frame for the exported data. These args may contain either  unix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values.    Optional `max_rows_per_line` arg may be added to the request for limiting the maximum number of rows exported per each JSON line.  Optional `reduce_mem_usage=1` arg may be added to the request for reducing memory usage when exporting big number of time series.  In this case the output may contain multiple lines with samples for the same time series.    Pass `Accept-Encoding: gzip` HTTP header in the request to `/api/v1/export` in order to reduce network bandwidth during exporing big amounts  of time series data. This enables gzip compression for the exported data. Example for exporting gzipped data:    ```bash  curl -H 'Accept-Encoding: gzip' http://localhost:8428/api/v1/export -d 'match[]={__name__!=""""}' > data.jsonl.gz  ```    The maximum duration for each request to `/api/v1/export` is limited by `-search.maxExportDuration` command-line flag.    Exported data can be imported via POST'ing it to [/api/v1/import](#how-to-import-data-in-json-line-format).    The [deduplication](#deduplication) is applied to the data exported via `/api/v1/export` by default. The deduplication  isn't applied if `reduce_mem_usage=1` query arg is passed to the request.      ### How to export CSV data    Send a request to `http://<victoriametrics-addr>:8428/api/v1/export/csv?format=<format>&match=<timeseries_selector_for_export>`,  where:    * `<format>` must contain comma-delimited label names for the exported CSV. The following special label names are supported:    * `__name__` - metric name    * `__value__` - sample value    * `__timestamp__:<ts_format>` - sample timestamp. `<ts_format>` can have the following values:      * `unix_s` - unix seconds      * `unix_ms` - unix milliseconds      * `unix_ns` - unix nanoseconds      * `rfc3339` - [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) time      * `custom:<layout>` - custom layout for time that is supported by [time.Format](https://golang.org/pkg/time/#Time.Format) function from Go.    * `<timeseries_selector_for_export>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)  for metrics to export.    Optional `start` and `end` args may be added to the request in order to limit the time frame for the exported data. These args may contain either  unix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values.    The exported CSV data can be imported to VictoriaMetrics via [/api/v1/import/csv](#how-to-import-csv-data).    The [deduplication](#deduplication) is applied for the data exported in CSV by default. It is possible to export raw data without de-duplication by passing `reduce_mem_usage=1` query arg to `/api/v1/export/csv`.      ### How to export data in native format    Send a request to `http://<victoriametrics-addr>:8428/api/v1/export/native?match[]=<timeseries_selector_for_export>`,  where `<timeseries_selector_for_export>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)  for metrics to export. Use `{__name__=~"".*""}` selector for fetching all the time series.    On large databases you may experience problems with limit on unique timeseries (default value is 300000). In this case you need to adjust `-search.maxUniqueTimeseries` parameter:    ```bash  # count unique timeseries in database  wget -O- -q 'http://your_victoriametrics_instance:8428/api/v1/series/count' | jq '.data[0]'    # relaunch victoriametrics with search.maxUniqueTimeseries more than value from previous command  ```    Optional `start` and `end` args may be added to the request in order to limit the time frame for the exported data. These args may contain either  unix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values.    The exported data can be imported to VictoriaMetrics via [/api/v1/import/native](#how-to-import-data-in-native-format).  The native export format may change in incompatible way between VictoriaMetrics releases, so the data exported from the release X  can fail to be imported into VictoriaMetrics release Y.    The [deduplication](#deduplication) isn't applied for the data exported in native format. It is expected that the de-duplication is performed during data import.      ## How to import time series data    Time series data can be imported into VictoriaMetrics via any supported ingestion protocol:    * [Prometheus remote_write API](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write). See [these docs](#prometheus-setup) for details.  * DataDog `submit metrics` API. See [these docs](#how-to-send-data-from-datadog-agent) for details.  * InfluxDB line protocol. See [these docs](#how-to-send-data-from-influxdb-compatible-agents-such-as-telegraf) for details.  * Graphite plaintext protocol. See [these docs](#how-to-send-data-from-graphite-compatible-agents-such-as-statsd) for details.  * OpenTSDB telnet put protocol. See [these docs](#sending-data-via-telnet-put-protocol) for details.  * OpenTSDB http `/api/put` protocol. See [these docs](#sending-opentsdb-data-via-http-apiput-requests) for details.  * `/api/v1/import` for importing data obtained from [/api/v1/export](#how-to-export-data-in-json-line-format).    See [these docs](#how-to-import-data-in-json-line-format) for details.  * `/api/v1/import/native` for importing data obtained from [/api/v1/export/native](#how-to-export-data-in-native-format).    See [these docs](#how-to-import-data-in-native-format) for details.  * `/api/v1/import/csv` for importing arbitrary CSV data. See [these docs](#how-to-import-csv-data) for details.  * `/api/v1/import/prometheus` for importing data in Prometheus exposition format. See [these docs](#how-to-import-data-in-prometheus-exposition-format) for details.      ### How to import data in JSON line format    Example for importing data obtained via [/api/v1/export](#how-to-export-data-in-json-line-format):    ```bash  # Export the data from <source-victoriametrics>:  curl http://source-victoriametrics:8428/api/v1/export -d 'match={__name__!=""""}' > exported_data.jsonl    # Import the data to <destination-victoriametrics>:  curl -X POST http://destination-victoriametrics:8428/api/v1/import -T exported_data.jsonl  ```    Pass `Content-Encoding: gzip` HTTP request header to `/api/v1/import` for importing gzipped data:    ```bash  # Export gzipped data from <source-victoriametrics>:  curl -H 'Accept-Encoding: gzip' http://source-victoriametrics:8428/api/v1/export -d 'match={__name__!=""""}' > exported_data.jsonl.gz    # Import gzipped data to <destination-victoriametrics>:  curl -X POST -H 'Content-Encoding: gzip' http://destination-victoriametrics:8428/api/v1/import -T exported_data.jsonl.gz  ```    Extra labels may be added to all the imported time series by passing `extra_label=name=value` query args.  For example, `/api/v1/import?extra_label=foo=bar` would add `""foo"":""bar""` label to all the imported time series.    Note that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.    VictoriaMetrics parses input JSON lines one-by-one. It loads the whole JSON line in memory, then parses it and then saves the parsed samples into persistent storage. This means that VictoriaMetrics can occupy big amounts of RAM when importing too long JSON lines. The solution is to split too long JSON lines into smaller lines. It is OK if samples for a single time series are split among multiple JSON lines.      ### How to import data in native format    The specification of VictoriaMetrics' native format may yet change and is not formally documented yet. So currently we do not recommend that external clients attempt to pack their own metrics in native format file.    If you have a native format file obtained via [/api/v1/export/native](#how-to-export-data-in-native-format) however this is the most efficient protocol for importing data in.    ```bash  # Export the data from <source-victoriametrics>:  curl http://source-victoriametrics:8428/api/v1/export/native -d 'match={__name__!=""""}' > exported_data.bin    # Import the data to <destination-victoriametrics>:  curl -X POST http://destination-victoriametrics:8428/api/v1/import/native -T exported_data.bin  ```    Extra labels may be added to all the imported time series by passing `extra_label=name=value` query args.  For example, `/api/v1/import/native?extra_label=foo=bar` would add `""foo"":""bar""` label to all the imported time series.    Note that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.      ### How to import CSV data    Arbitrary CSV data can be imported via `/api/v1/import/csv`. The CSV data is imported according to the provided `format` query arg.  The `format` query arg must contain comma-separated list of parsing rules for CSV fields. Each rule consists of three parts delimited by a colon:    ```  <column_pos>:<type>:<context>  ```    * `<column_pos>` is the position of the CSV column (field). Column numbering starts from 1. The order of parsing rules may be arbitrary.  * `<type>` describes the column type. Supported types are:    * `metric` - the corresponding CSV column at `<column_pos>` contains metric value, which must be integer or floating-point number.      The metric name is read from the `<context>`. CSV line must have at least a single metric field. Multiple metric fields per CSV line is OK.    * `label` - the corresponding CSV column at `<column_pos>` contains label value. The label name is read from the `<context>`.      CSV line may have arbitrary number of label fields. All these labels are attached to all the configured metrics.    * `time` - the corresponding CSV column at `<column_pos>` contains metric time. CSV line may contain either one or zero columns with time.      If CSV line has no time, then the current time is used. The time is applied to all the configured metrics.      The format of the time is configured via `<context>`. Supported time formats are:      * `unix_s` - unix timestamp in seconds.      * `unix_ms` - unix timestamp in milliseconds.      * `unix_ns` - unix timestamp in nanoseconds. Note that VictoriaMetrics rounds the timestamp to milliseconds.      * `rfc3339` - timestamp in [RFC3339](https://tools.ietf.org/html/rfc3339) format, i.e. `2006-01-02T15:04:05Z`.      * `custom:<layout>` - custom layout for the timestamp. The `<layout>` may contain arbitrary time layout according to [time.Parse rules in Go](https://golang.org/pkg/time/#Parse).    Each request to `/api/v1/import/csv` may contain arbitrary number of CSV lines.    Example for importing CSV data via `/api/v1/import/csv`:    ```bash  curl -d ""GOOG,1.23,4.56,NYSE"" 'http://localhost:8428/api/v1/import/csv?format=2:metric:ask,3:metric:bid,1:label:ticker,4:label:market'  curl -d ""MSFT,3.21,1.67,NASDAQ"" 'http://localhost:8428/api/v1/import/csv?format=2:metric:ask,3:metric:bid,1:label:ticker,4:label:market'  ```    After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match[]={ticker!=""""}'  ```    The following response should be returned:  ```bash  {""metric"":{""__name__"":""bid"",""market"":""NASDAQ"",""ticker"":""MSFT""},""values"":[1.67],""timestamps"":[1583865146520]}  {""metric"":{""__name__"":""bid"",""market"":""NYSE"",""ticker"":""GOOG""},""values"":[4.56],""timestamps"":[1583865146495]}  {""metric"":{""__name__"":""ask"",""market"":""NASDAQ"",""ticker"":""MSFT""},""values"":[3.21],""timestamps"":[1583865146520]}  {""metric"":{""__name__"":""ask"",""market"":""NYSE"",""ticker"":""GOOG""},""values"":[1.23],""timestamps"":[1583865146495]}  ```    Extra labels may be added to all the imported lines by passing `extra_label=name=value` query args.  For example, `/api/v1/import/csv?extra_label=foo=bar` would add `""foo"":""bar""` label to all the imported lines.    Note that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.      ### How to import data in Prometheus exposition format    VictoriaMetrics accepts data in [Prometheus exposition format](https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format)  and in [OpenMetrics format](https://github.com/OpenObservability/OpenMetrics/blob/master/specification/OpenMetrics.md)  via `/api/v1/import/prometheus` path. For example, the following line imports a single line in Prometheus exposition format into VictoriaMetrics:    ```bash  curl -d 'foo{bar=""baz""} 123' -X POST 'http://localhost:8428/api/v1/import/prometheus'  ```    The following command may be used for verifying the imported data:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match={__name__=~""foo""}'  ```    It should return something like the following:    ```  {""metric"":{""__name__"":""foo"",""bar"":""baz""},""values"":[123],""timestamps"":[1594370496905]}  ```    Pass `Content-Encoding: gzip` HTTP request header to `/api/v1/import/prometheus` for importing gzipped data:    ```bash  # Import gzipped data to <destination-victoriametrics>:  curl -X POST -H 'Content-Encoding: gzip' http://destination-victoriametrics:8428/api/v1/import/prometheus -T prometheus_data.gz  ```    Extra labels may be added to all the imported metrics by passing `extra_label=name=value` query args.  For example, `/api/v1/import/prometheus?extra_label=foo=bar` would add `{foo=""bar""}` label to all the imported metrics.    If timestamp is missing in `<metric> <value> <timestamp>` Prometheus exposition format line, then the current timestamp is used during data ingestion.  It can be overriden by passing unix timestamp in *milliseconds* via `timestamp` query arg. For example, `/api/v1/import/prometheus?timestamp=1594370496905`.    VictoriaMetrics accepts arbitrary number of lines in a single request to `/api/v1/import/prometheus`, i.e. it supports data streaming.    Note that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.    VictoriaMetrics also may scrape Prometheus targets - see [these docs](#how-to-scrape-prometheus-exporters-such-as-node-exporter).        ## Relabeling    VictoriaMetrics supports Prometheus-compatible relabeling for all the ingested metrics if `-relabelConfig` command-line flag points  to a file containing a list of [relabel_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config) entries.  The `-relabelConfig` also can point to http or https url. For example, `-relabelConfig=https://config-server/relabel_config.yml`.  See [this article with relabeling tips and tricks](https://valyala.medium.com/how-to-use-relabeling-in-prometheus-and-victoriametrics-8b90fc22c4b2).    Example contents for `-relabelConfig` file:  ```yml  # Add {cluster=""dev""} label.  - target_label: cluster    replacement: dev    # Drop the metric (or scrape target) with `{__meta_kubernetes_pod_container_init=""true""}` label.  - action: drop    source_labels: [__meta_kubernetes_pod_container_init]    regex: true  ```    See [these docs](https://docs.victoriametrics.com/vmagent.html#relabeling) for more details about relabeling in VictoriaMetrics.      ## Federation    VictoriaMetrics exports [Prometheus-compatible federation data](https://prometheus.io/docs/prometheus/latest/federation/)  at `http://<victoriametrics-addr>:8428/federate?match[]=<timeseries_selector_for_federation>`.    Optional `start` and `end` args may be added to the request in order to scrape the last point for each selected time series on the `[start ... end]` interval.  `start` and `end` may contain either unix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values. By default, the last point  on the interval `[now - max_lookback ... now]` is scraped for each time series. The default value for `max_lookback` is `5m` (5 minutes), but it can be overridden.  For instance, `/federate?match[]=up&max_lookback=1h` would return last points on the `[now - 1h ... now]` interval. This may be useful for time series federation  with scrape intervals exceeding `5m`.      ## Capacity planning    VictoriaMetrics uses lower amounts of CPU, RAM and storage space on production workloads compared to competing solutions (Prometheus, Thanos, Cortex, TimescaleDB, InfluxDB, QuestDB, M3DB) according to [our case studies](https://docs.victoriametrics.com/CaseStudies.html).    VictoriaMetrics capacity scales linearly with the available resources. The needed amounts of CPU and RAM highly depends on the workload - the number of [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series), series [churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate), query types, query qps, etc. It is recommended setting up a test VictoriaMetrics for your production workload and iteratively scaling CPU and RAM resources until it becomes stable according to [troubleshooting docs](#troubleshooting). A single-node VictoriaMetrics works perfectly with the following production workload according to [our case studies](https://docs.victoriametrics.com/CaseStudies.html):    * Ingestion rate: 1.5+ million samples per second  * Active time series: 50+ million  * Total time series: 5+ billion  * Time series churn rate: 150+ million of new series per day  * Total number of samples: 10+ trillion  * Queries: 200+ qps  * Query latency (99th percentile): 1 second    The needed storage space for the given retention (the retention is set via `-retentionPeriod` command-line flag) can be extrapolated from disk space usage in a test run. For example, if `-storageDataPath` directory size becomes 10GB after a day-long test run on a production workload, then it will need at least `10GB*100=1TB` of disk space for `-retentionPeriod=100d` (100-days retention period).    It is recommended leaving the following amounts of spare resources:    * 50% of free RAM for reducing the probability of OOM (out of memory) crashes and slowdowns during temporary spikes in workload.  * 50% of spare CPU for reducing the probability of slowdowns during temporary spikes in workload.  * At least 30% of free storage space at the directory pointed by `-storageDataPath` command-line flag. See also `-storage.minFreeDiskSpaceBytes` command-line flag description [here](#list-of-command-line-flags).      ## High availability    * Install multiple VictoriaMetrics instances in distinct datacenters (availability zones).  * Pass addresses of these instances to [vmagent](https://docs.victoriametrics.com/vmagent.html) via `-remoteWrite.url` command-line flag:    ```bash  /path/to/vmagent -remoteWrite.url=http://<victoriametrics-addr-1>:8428/api/v1/write -remoteWrite.url=http://<victoriametrics-addr-2>:8428/api/v1/write  ```    Alternatively these addresses may be passed to `remote_write` section in Prometheus config:    ```yml  remote_write:    - url: http://<victoriametrics-addr-1>:8428/api/v1/write      queue_config:        max_samples_per_send: 10000    # ...    - url: http://<victoriametrics-addr-N>:8428/api/v1/write      queue_config:        max_samples_per_send: 10000  ```    * Apply the updated config:    ```bash  kill -HUP `pidof prometheus`  ```    It is recommended to use [vmagent](https://docs.victoriametrics.com/vmagent.html) instead of Prometheus for highly loaded setups.    * Now Prometheus should write data into all the configured `remote_write` urls in parallel.  * Set up [Promxy](https://github.com/jacksontj/promxy) in front of all the VictoriaMetrics replicas.  * Set up Prometheus datasource in Grafana that points to Promxy.    If you have Prometheus HA pairs with replicas `r1` and `r2` in each pair, then configure each `r1`  to write data to `victoriametrics-addr-1`, while each `r2` should write data to `victoriametrics-addr-2`.    Another option is to write data simultaneously from Prometheus HA pair to a pair of VictoriaMetrics instances  with the enabled de-duplication. See [this section](#deduplication) for details.      ## Deduplication    VictoriaMetrics de-duplicates data points if `-dedup.minScrapeInterval` command-line flag is set to positive duration. For example, `-dedup.minScrapeInterval=60s` would de-duplicate data points on the same time series if they fall within the same discrete 60s bucket.  The earliest data point will be kept. In the case of equal timestamps, an arbitrary data point will be kept. See [this comment](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/2112#issuecomment-1032587618) for more details on how downsampling works.    The `-dedup.minScrapeInterval=D` is equivalent to `-downsampling.period=0s:D` if [downsampling](#downsampling) is enabled. It is safe to use deduplication and downsampling simultaneously.    The recommended value for `-dedup.minScrapeInterval` must equal to `scrape_interval` config from Prometheus configs. It is recommended to have a single `scrape_interval` across all the scrape targets. See [this article](https://www.robustperception.io/keep-it-simple-scrape_interval-id) for details.    The de-duplication reduces disk space usage if multiple identically configured [vmagent](https://docs.victoriametrics.com/vmagent.html) or Prometheus instances in HA pair  write data to the same VictoriaMetrics instance. These vmagent or Prometheus instances must have identical  `external_labels` section in their configs, so they write data to the same time series.      ## Storage    VictoriaMetrics stores time series data in [MergeTree](https://en.wikipedia.org/wiki/Log-structured_merge-tree)-like   data structures. On insert, VictoriaMetrics accumulates up to 1s of data and dumps it on disk to  `<-storageDataPath>/data/small/YYYY_MM/` subdirectory forming a `part` with the following   name pattern: `rowsCount_blocksCount_minTimestamp_maxTimestamp`. Each part consists of two ""columns"":  values and timestamps. These are sorted and compressed raw time series values. Additionally, part contains  index files for searching for specific series in the values and timestamps files.    `Parts` are periodically merged into the bigger parts. The resulting `part` is constructed   under `<-storageDataPath>/data/{small,big}/YYYY_MM/tmp` subdirectory. When the resulting `part` is complete, it is atomically moved from the `tmp`   to its own subdirectory, while the source parts are atomically removed. The end result is that the source   parts are substituted by a single resulting bigger `part` in the `<-storageDataPath>/data/{small,big}/YYYY_MM/` directory.  Information about merging process is available in [single-node VictoriaMetrics](https://grafana.com/dashboards/10229)   and [clustered VictoriaMetrics](https://grafana.com/grafana/dashboards/11176) Grafana dashboards.   See more details in [monitoring docs](#monitoring).    The `merge` process is usually named ""compaction"", because the resulting `part` size is usually smaller than   the sum of the source `parts`. There are following benefits of doing the merge process:  * it improves query performance, since lower number of `parts` are inspected with each query;  * it reduces the number of data files, since each `part`contains fixed number of files;   * better compression rate for the resulting part.    Newly added `parts` either appear in the storage or fail to appear.   Storage never contains partially created parts. The same applies to merge process â€” `parts` are either fully   merged into a new `part` or fail to merge. There are no partially merged `parts` in MergeTree.   `Part` contents in MergeTree never change. Parts are immutable. They may be only deleted after the merge   to a bigger `part` or when the `part` contents goes outside the configured `-retentionPeriod`.    See [this article](https://valyala.medium.com/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282) for more details.    See also [how to work with snapshots](#how-to-work-with-snapshots).    ## Retention    Retention is configured with the `-retentionPeriod` command-line flag, which takes a number followed by a time unit character - `h(ours)`, `d(ays)`, `w(eeks)`, `y(ears)`. If the time unit is not specified, a month is assumed. For instance, `-retentionPeriod=3` means that the data will be stored for 3 months and then deleted. The default retention period is one month.    Data is split in per-month partitions inside `<-storageDataPath>/data/{small,big}` folders.  Data partitions outside the configured retention are deleted on the first day of the new month.  Each partition consists of one or more data parts with the following name pattern `rowsCount_blocksCount_minTimestamp_maxTimestamp`.  Data parts outside of the configured retention are eventually deleted during   [background merge](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).    The maximum disk space usage for a given `-retentionPeriod` is going to be (`-retentionPeriod` + 1) months.  For example, if `-retentionPeriod` is set to 1, data for January is deleted on March 1st.    Please note, the time range covered by data part is not limited by retention period unit. Hence, data part may contain data  for multiple days and will be deleted only when fully outside of the configured retention.    It is safe to extend `-retentionPeriod` on existing data. If `-retentionPeriod` is set to a lower  value than before, then data outside the configured period will be eventually deleted.    VictoriaMetrics does not support indefinite retention, but you can specify an arbitrarily high duration, e.g. `-retentionPeriod=100y`.    ## Multiple retentions    A single instance of VictoriaMetrics supports only a single retention, which can be configured via `-retentionPeriod` command-line flag. If you need multiple retentions, then you may start multiple VictoriaMetrics instances with distinct values for the following flags:    * `-retentionPeriod`  * `-storageDataPath`, so the data for each retention period is saved in a separate directory  * `-httpListenAddr`, so clients may reach VictoriaMetrics instance with proper retention    Then set up [vmauth](https://docs.victoriametrics.com/vmauth.html) in front of VictoriaMetrics instances,  so it could route requests from particular user to VictoriaMetrics with the desired retention.  The same scheme could be implemented for multiple tenants in [VictoriaMetrics cluster](https://docs.victoriametrics.com/Cluster-VictoriaMetrics.html).  See [these docs](https://docs.victoriametrics.com/guides/guide-vmcluster-multiple-retention-setup.html) for multi-retention setup details.      ## Downsampling    [VictoriaMetrics Enterprise](https://victoriametrics.com/products/enterprise/) supports multi-level downsampling with `-downsampling.period` command-line flag. For example:    * `-downsampling.period=30d:5m` instructs VictoriaMetrics to [deduplicate](#deduplication) samples older than 30 days with 5 minutes interval.    * `-downsampling.period=30d:5m,180d:1h` instructs VictoriaMetrics to deduplicate samples older than 30 days with 5 minutes interval and to deduplicate samples older than 180 days with 1 hour interval.    Downsampling is applied independently per each time series. It can reduce disk space usage and improve query performance if it is applied to time series with big number of samples per each series. The downsampling doesn't improve query performance if the database contains big number of time series with small number of samples per each series (aka [high churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate)), since downsampling doesn't reduce the number of time series. So the majority of time is spent on searching for the matching time series. It is possible to use recording rules in [vmalert](https://docs.victoriametrics.com/vmalert.html) in order to reduce the number of time series. See [these docs](https://docs.victoriametrics.com/vmalert.html#downsampling-and-aggregation-via-vmalert).    The downsampling can be evaluated for free by downloading and using enterprise binaries from [the releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases).      ## Multi-tenancy    Single-node VictoriaMetrics doesn't support multi-tenancy. Use [cluster version](https://docs.victoriametrics.com/Cluster-VictoriaMetrics.html#multitenancy) instead.      ## Scalability and cluster version    Though single-node VictoriaMetrics cannot scale to multiple nodes, it is optimized for resource usage - storage size / bandwidth / IOPS, RAM, CPU.  This means that a single-node VictoriaMetrics may scale vertically and substitute a moderately sized cluster built with competing solutions  such as Thanos, Uber M3, InfluxDB or TimescaleDB. See [vertical scalability benchmarks](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae).    So try single-node VictoriaMetrics at first and then [switch to cluster version](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster) if you still need  horizontally scalable long-term remote storage for really large Prometheus deployments.  [Contact us](mailto:info@victoriametrics.com) for enterprise support.      ## Alerting    It is recommended using [vmalert](https://docs.victoriametrics.com/vmalert.html) for alerting.    Additionally, alerting can be set up with the following tools:    * With Prometheus - see [the corresponding docs](https://prometheus.io/docs/alerting/overview/).  * With Promxy - see [the corresponding docs](https://github.com/jacksontj/promxy/blob/master/README.md#how-do-i-use-alertingrecording-rules-in-promxy).  * With Grafana - see [the corresponding docs](https://grafana.com/docs/alerting/rules/).      ## Security    Do not forget protecting sensitive endpoints in VictoriaMetrics when exposing it to untrusted networks such as the internet.  Consider setting the following command-line flags:    * `-tls`, `-tlsCertFile` and `-tlsKeyFile` for switching from HTTP to HTTPS.  * `-httpAuth.username` and `-httpAuth.password` for protecting all the HTTP endpoints    with [HTTP Basic Authentication](https://en.wikipedia.org/wiki/Basic_access_authentication).  * `-deleteAuthKey` for protecting `/api/v1/admin/tsdb/delete_series` endpoint. See [how to delete time series](#how-to-delete-time-series).  * `-snapshotAuthKey` for protecting `/snapshot*` endpoints. See [how to work with snapshots](#how-to-work-with-snapshots).  * `-forceMergeAuthKey` for protecting `/internal/force_merge` endpoint. See [force merge docs](#forced-merge).  * `-search.resetCacheAuthKey` for protecting `/internal/resetRollupResultCache` endpoint. See [backfilling](#backfilling) for more details.  * `-configAuthKey` for protecting `/config` endpoint, since it may contain sensitive information such as passwords.  - `-pprofAuthKey` for protecting `/debug/pprof/*` endpoints, which can be used for [profiling](#profiling).    Explicitly set internal network interface for TCP and UDP ports for data ingestion with Graphite and OpenTSDB formats.  For example, substitute `-graphiteListenAddr=:2003` with `-graphiteListenAddr=<internal_iface_ip>:2003`.    Prefer authorizing all the incoming requests from untrusted networks with [vmauth](https://docs.victoriametrics.com/vmauth.html)  or similar auth proxy.      ## Tuning    * There is no need for VictoriaMetrics tuning since it uses reasonable defaults for command-line flags,    which are automatically adjusted for the available CPU and RAM resources.  * There is no need for Operating System tuning since VictoriaMetrics is optimized for default OS settings.    The only option is increasing the limit on [the number of open files in the OS](https://medium.com/@muhammadtriwibowo/set-permanently-ulimit-n-open-files-in-ubuntu-4d61064429a).    The recommendation is not specific for VictoriaMetrics only but also for any service which handles many HTTP connections and stores data on disk.  * VictoriaMetrics is a write-heavy application and its performance depends on disk performance. So be careful with other    applications or utilities (like [fstrim](http://manpages.ubuntu.com/manpages/bionic/man8/fstrim.8.html))    which could [exhaust disk resources](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/1521).  * The recommended filesystem is `ext4`, the recommended persistent storage is [persistent HDD-based disk on GCP](https://cloud.google.com/compute/docs/disks/#pdspecs),    since it is protected from hardware failures via internal replication and it can be [resized on the fly](https://cloud.google.com/compute/docs/disks/add-persistent-disk#resize_pd).    If you plan to store more than 1TB of data on `ext4` partition or plan extending it to more than 16TB,    then the following options are recommended to pass to `mkfs.ext4`:    ```bash  mkfs.ext4 ... -O 64bit,huge_file,extent -T huge  ```    ## Monitoring    VictoriaMetrics exports internal metrics in Prometheus format at `/metrics` page.  These metrics may be collected by [vmagent](https://docs.victoriametrics.com/vmagent.html)  or Prometheus by adding the corresponding scrape config to it.  Alternatively they can be self-scraped by setting `-selfScrapeInterval` command-line flag to duration greater than 0.  For example, `-selfScrapeInterval=10s` would enable self-scraping of `/metrics` page with 10 seconds interval.    There are officials Grafana dashboards for [single-node VictoriaMetrics](https://grafana.com/dashboards/10229) and [clustered VictoriaMetrics](https://grafana.com/grafana/dashboards/11176). There is also an [alternative dashboard for clustered VictoriaMetrics](https://grafana.com/grafana/dashboards/11831).    Graphs on these dashboard contain useful hints - hover the `i` icon at the top left corner of each graph in order to read it.    It is recommended setting up alerts in [vmalert](https://docs.victoriametrics.com/vmalert.html) or in Prometheus from [this config](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/deployment/docker/alerts.yml).    The most interesting metrics are:    * `vm_cache_entries{type=""storage/hour_metric_ids""}` - the number of time series with new data points during the last hour    aka [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series).  * `increase(vm_new_timeseries_created_total[1h])` - time series [churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate) during the previous hour.  * `sum(vm_rows{type=~""storage/.*""})` - total number of `(timestamp, value)` data points in the database.  * `sum(rate(vm_rows_inserted_total[5m]))` - ingestion rate, i.e. how many samples are inserted int the database per second.  * `vm_free_disk_space_bytes` - free space left at `-storageDataPath`.  * `sum(vm_data_size_bytes)` - the total size of data on disk.  * `increase(vm_slow_row_inserts_total[5m])` - the number of slow inserts during the last 5 minutes.    If this number remains high during extended periods of time, then it is likely more RAM is needed for optimal handling    of the current number of [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series).  * `increase(vm_slow_metric_name_loads_total[5m])` - the number of slow loads of metric names during the last 5 minutes.    If this number remains high during extended periods of time, then it is likely more RAM is needed for optimal handling    of the current number of [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series).    VictoriaMetrics also exposes currently running queries with their execution times at `/api/v1/status/active_queries` page.    See the example of alerting rules for VM components [here](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/deployment/docker/alerts.yml).      ## TSDB stats    VictoriaMetrics returns TSDB stats at `/api/v1/status/tsdb` page in the way similar to Prometheus - see [these Prometheus docs](https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-stats). VictoriaMetrics accepts the following optional query args at `/api/v1/status/tsdb` page:    * `topN=N` where `N` is the number of top entries to return in the response. By default top 10 entries are returned.    * `date=YYYY-MM-DD` where `YYYY-MM-DD` is the date for collecting the stats. By default the stats is collected for the current day.    * `match[]=SELECTOR` where `SELECTOR` is an arbitrary [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors) for series to take into account during stats calculation. By default all the series are taken into account.    * `extra_label=LABEL=VALUE`. See [these docs](#prometheus-querying-api-enhancements) for more details.      ## Cardinality limiter    By default VictoriaMetrics doesn't limit the number of stored time series. The limit can be enforced by setting the following command-line flags:    * `-storage.maxHourlySeries` - limits the number of time series that can be added during the last hour. Useful for limiting the number of [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series).  * `-storage.maxDailySeries` - limits the number of time series that can be added during the last day. Useful for limiting daily [churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate).    Both limits can be set simultaneously. If any of these limits is reached, then incoming samples for new time series are dropped. A sample of dropped series is put in the log with `WARNING` level.    The exceeded limits can be [monitored](#monitoring) with the following metrics:    * `vm_hourly_series_limit_rows_dropped_total` - the number of metrics dropped due to exceeded hourly limit on the number of unique time series.  * `vm_daily_series_limit_rows_dropped_total` - the number of metrics dropped due to exceeded daily limit on the number of unique time series.    These limits are approximate, so VictoriaMetrics can underflow/overflow the limit by a small percentage (usually less than 1%).    See also more advanced [cardinality limiter in vmagent](https://docs.victoriametrics.com/vmagent.html#cardinality-limiter).      ## Troubleshooting    * It is recommended to use default command-line flag values (i.e. don't set them explicitly) until the need    of tweaking these flag values arises.    * It is recommended inspecting logs during troubleshooting, since they may contain useful information.    * It is recommended upgrading to the latest available release from [this page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases),    since the encountered issue could be already fixed there.    * It is recommended to have at least 50% of spare resources for CPU, disk IO and RAM, so VictoriaMetrics could handle short spikes in the workload without performance issues.    * VictoriaMetrics requires free disk space for [merging data files to bigger ones](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).    It may slow down when there is no enough free space left. So make sure `-storageDataPath` directory    has at least 20% of free space. The remaining amount of free space    can be [monitored](#monitoring) via `vm_free_disk_space_bytes` metric. The total size of data    stored on the disk can be monitored via sum of `vm_data_size_bytes` metrics.    See also `vm_merge_need_free_disk_space` metrics, which are set to values higher than 0    if background merge cannot be initiated due to free disk space shortage. The value shows the number of per-month partitions,    which would start background merge if they had more free disk space.    * VictoriaMetrics buffers incoming data in memory for up to a few seconds before flushing it to persistent storage.    This may lead to the following ""issues"":    * Data becomes available for querying in a few seconds after inserting. It is possible to flush in-memory buffers to persistent storage      by requesting `/internal/force_flush` http handler. This handler is mostly needed for testing and debugging purposes.    * The last few seconds of inserted data may be lost on unclean shutdown (i.e. OOM, `kill -9` or hardware reset).      See [this article for technical details](https://valyala.medium.com/wal-usage-looks-broken-in-modern-time-series-databases-b62a627ab704).    * If VictoriaMetrics works slowly and eats more than a CPU core per 100K ingested data points per second,    then it is likely you have too many [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series) for the current amount of RAM.    VictoriaMetrics [exposes](#monitoring) `vm_slow_*` metrics such as `vm_slow_row_inserts_total` and `vm_slow_metric_name_loads_total`, which could be used    as an indicator of low amounts of RAM. It is recommended increasing the amount of RAM on the node with VictoriaMetrics in order to improve    ingestion and query performance in this case.    * If the order of labels for the same metrics can change over time (e.g. if `metric{k1=""v1"",k2=""v2""}` may become `metric{k2=""v2"",k1=""v1""}`),    then it is recommended running VictoriaMetrics with `-sortLabels` command-line flag in order to reduce memory usage and CPU usage.    * VictoriaMetrics prioritizes data ingestion over data querying. So if it has no enough resources for data ingestion,    then data querying may slow down significantly.    * If VictoriaMetrics doesn't work because of certain parts are corrupted due to disk errors,    then just remove directories with broken parts. It is safe removing subdirectories under `<-storageDataPath>/data/{big,small}/YYYY_MM` directories    when VictoriaMetrics isn't running. This recovers VictoriaMetrics at the cost of data loss stored in the deleted broken parts.    In the future, `vmrecover` tool will be created for automatic recovering from such errors.    * If you see gaps on the graphs, try resetting the cache by sending request to `/internal/resetRollupResultCache`.    If this removes gaps on the graphs, then it is likely data with timestamps older than `-search.cacheTimestampOffset`    is ingested into VictoriaMetrics. Make sure that data sources have synchronized time with VictoriaMetrics.      If the gaps are related to irregular intervals between samples, then try adjusting `-search.minStalenessInterval` command-line flag    to value close to the maximum interval between samples.    * If you are switching from InfluxDB or TimescaleDB, then take a look at `-search.maxStalenessInterval` command-line flag.    It may be needed in order to suppress default gap filling algorithm used by VictoriaMetrics - by default it assumes    each time series is continuous instead of discrete, so it fills gaps between real samples with regular intervals.    * Metrics and labels leading to [high cardinality](https://docs.victoriametrics.com/FAQ.html#what-is-high-cardinality) or [high churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate) can be determined at `/api/v1/status/tsdb` page. See [these docs](#tsdb-stats) for details.    * New time series can be logged if `-logNewSeries` command-line flag is passed to VictoriaMetrics.    * VictoriaMetrics limits the number of labels per each metric with `-maxLabelsPerTimeseries` command-line flag.    This prevents from ingesting metrics with too many labels. It is recommended [monitoring](#monitoring) `vm_metrics_with_dropped_labels_total`    metric in order to determine whether `-maxLabelsPerTimeseries` must be adjusted for your workload.    * If you store Graphite metrics like `foo.bar.baz` in VictoriaMetrics, then `{__graphite__=""foo.*.baz""}` filter can be used for selecting such metrics. See [these docs](#selecting-graphite-metrics) for details.    * VictoriaMetrics ignores `NaN` values during data ingestion.      ## Cache removal    VictoriaMetrics uses various internal caches. These caches are stored to `<-storageDataPath>/cache` directory during graceful shutdown (e.g. when VictoriaMetrics is stopped by sending `SIGINT` signal). The caches are read on the next VictoriaMetrics startup. Sometimes it is needed to remove such caches on the next startup. This can be performed by placing `reset_cache_on_startup` file inside the `<-storageDataPath>/cache` directory before the restart of VictoriaMetrics. See [this issue](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/1447) for details.      ## Cache tuning    VictoriaMetrics uses various in-memory caches for faster data ingestion and query performance.  The following metrics for each type of cache are exported at [`/metrics` page](#monitoring):  - `vm_cache_size_bytes` - the actual cache size  - `vm_cache_size_max_bytes` - cache size limit  - `vm_cache_requests_total` - the number of requests to the cache  - `vm_cache_misses_total` - the number of cache misses  - `vm_cache_entries` - the number of entries in the cache    Both Grafana dashboards for [single-node VictoriaMetrics](https://grafana.com/dashboards/10229)  and [clustered VictoriaMetrics](https://grafana.com/grafana/dashboards/11176)  contain `Caches` section with cache metrics visualized. The panels show the current  memory usage by each type of cache, and also a cache hit rate. If hit rate is close to 100%  then cache efficiency is already very high and does not need any tuning.  The panel `Cache usage %` in `Troubleshooting` section shows the percentage of used cache size  from the allowed size by type. If the percentage is below 100%, then no further tuning needed.    Please note, default cache sizes were carefully adjusted accordingly to the most  practical scenarios and workloads. Change the defaults only if you understand the implications.    To override the default values see command-line flags with `-storage.cacheSize` prefix.  See the full description of flags [here](#list-of-command-line-flags).      ## Data migration    ### From VictoriaMetrics    The simplest way to migrate data from one single-node (source) to another (destination), or from one vmstorage node   to another do the following:  1. Stop the VictoriaMetrics (source) with `kill -INT`;  2. Copy (via [rsync](https://en.wikipedia.org/wiki/Rsync) or any other tool) the entire folder specified   via `-storageDataPath` from the source node to the empty folder at the destination node.  3. Once copy is done, stop the VictoriaMetrics (destination) with `kill -INT` and verify that   its `-storageDataPath` points to the copied folder from p.2;  4. Start the VictoriaMetrics (destination). The copied data should be now available.    Things to consider when copying data:  1. Data formats between single-node and vmstorage node aren't compatible and can't be copied.  2. Copying data folder means complete replacement of the previous data on destination VictoriaMetrics.    For more complex scenarios like single-to-cluster, cluster-to-single, re-sharding or migrating only a fraction  of data - see [vmctl. Migrating data from VictoriaMetrics](https://docs.victoriametrics.com/vmctl.html#migrating-data-from-victoriametrics).      ### From other systems    Use [vmctl](https://docs.victoriametrics.com/vmctl.html) for data migration. It supports the following data migration types:    * From Prometheus to VictoriaMetrics  * From InfluxDB to VictoriaMetrics  * From VictoriaMetrics to VictoriaMetrics  * From OpenTSDB to VictoriaMetrics    See [vmctl docs](https://docs.victoriametrics.com/vmctl.html) for more details.      ## Backfilling    VictoriaMetrics accepts historical data in arbitrary order of time via [any supported ingestion method](#how-to-import-time-series-data).  See [how to backfill data with recording rules in vmalert](https://docs.victoriametrics.com/vmalert.html#rules-backfilling).  Make sure that configured `-retentionPeriod` covers timestamps for the backfilled data.    It is recommended disabling query cache with `-search.disableCache` command-line flag when writing  historical data with timestamps from the past, since the cache assumes that the data is written with  the current timestamps. Query cache can be enabled after the backfilling is complete.    An alternative solution is to query `/internal/resetRollupResultCache` url after backfilling is complete. This will reset  the query cache, which could contain incomplete data cached during the backfilling.    Yet another solution is to increase `-search.cacheTimestampOffset` flag value in order to disable caching  for data with timestamps close to the current time. Single-node VictoriaMetrics automatically resets response  cache when samples with timestamps older than `now - search.cacheTimestampOffset` are ingested to it.      ## Data updates    VictoriaMetrics doesn't support updating already existing sample values to new ones. It stores all the ingested data points  for the same time series with identical timestamps. While it is possible substituting old time series with new time series via  [removal of old time series](#how-to-delete-time-series) and then [writing new time series](#backfilling), this approach  should be used only for one-off updates. It shouldn't be used for frequent updates because of non-zero overhead related to data removal.      ## Replication    Single-node VictoriaMetrics doesn't support application-level replication. Use cluster version instead.  See [these docs](https://docs.victoriametrics.com/Cluster-VictoriaMetrics.html#replication-and-data-safety) for details.    Storage-level replication may be offloaded to durable persistent storage such as [Google Cloud disks](https://cloud.google.com/compute/docs/disks#pdspecs).    See also [high availability docs](#high-availability) and [backup docs](#backups).      ## Backups    VictoriaMetrics supports backups via [vmbackup](https://docs.victoriametrics.com/vmbackup.html)  and [vmrestore](https://docs.victoriametrics.com/vmrestore.html) tools.  We also provide [vmbackupmanager](https://docs.victoriametrics.com/vmbackupmanager.html) tool for enterprise subscribers.  Enterprise binaries can be downloaded and evaluated for free from [the releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases).      ## Benchmarks    Note, that vendors (including VictoriaMetrics) are often biased when doing such tests. E.g. they try highlighting   the best parts of their product, while highlighting the worst parts of competing products.   So we encourage users and all independent third parties to conduct their becnhmarks for various products   they are evaluating in production and publish the results.    As a reference, please see [benchmarks](https://docs.victoriametrics.com/Articles.html#benchmarks) conducted by  VictoriaMetrics team. Please also see the [helm chart](https://github.com/VictoriaMetrics/benchmark)   for running ingestion benchmarks based on node_exporter metrics.      ## Profiling    VictoriaMetrics provides handlers for collecting the following [Go profiles](https://blog.golang.org/profiling-go-programs):    * Memory profile. It can be collected with the following command (replace `0.0.0.0` with hostname if needed):    <div class=""with-copy"" markdown=""1"">    ```bash  curl http://0.0.0.0:8428/debug/pprof/heap > mem.pprof  ```    </div>    * CPU profile. It can be collected with the following command (replace `0.0.0.0` with hostname if needed):    <div class=""with-copy"" markdown=""1"">    ```bash  curl http://0.0.0.0:8428/debug/pprof/profile > cpu.pprof  ```    </div>    The command for collecting CPU profile waits for 30 seconds before returning.    The collected profiles may be analyzed with [go tool pprof](https://github.com/google/pprof).      ## Integrations    * [Helm charts for single-node and cluster versions of VictoriaMetrics](https://github.com/VictoriaMetrics/helm-charts).  * [Kubernetes operator for VictoriaMetrics](https://github.com/VictoriaMetrics/operator).  * [netdata](https://github.com/netdata/netdata) can push data into VictoriaMetrics via `Prometheus remote_write API`.    See [these docs](https://github.com/netdata/netdata#integrations).  * [go-graphite/carbonapi](https://github.com/go-graphite/carbonapi) can use VictoriaMetrics as time series backend.    See [this example](https://github.com/go-graphite/carbonapi/blob/main/cmd/carbonapi/carbonapi.example.victoriametrics.yaml).  * [Ansible role for installing single-node VictoriaMetrics](https://github.com/dreamteam-gg/ansible-victoriametrics-role).  * [Ansible role for installing cluster VictoriaMetrics](https://github.com/Slapper/ansible-victoriametrics-cluster-role).  * [Snap package for VictoriaMetrics](https://snapcraft.io/victoriametrics).  * [vmalert-cli](https://github.com/aorfanos/vmalert-cli) - a CLI application for managing [vmalert](https://docs.victoriametrics.com/vmalert.html).      ## Third-party contributions    * [Unofficial yum repository](https://copr.fedorainfracloud.org/coprs/antonpatsev/VictoriaMetrics/) ([source code](https://github.com/patsevanton/victoriametrics-rpm))  * [Prometheus -> VictoriaMetrics exporter #1](https://github.com/ryotarai/prometheus-tsdb-dump)  * [Prometheus -> VictoriaMetrics exporter #2](https://github.com/AnchorFree/tsdb-remote-write)  * [Prometheus Oauth proxy](https://gitlab.com/optima_public/prometheus_oauth_proxy) - see [this article](https://medium.com/@richard.holly/powerful-saas-solution-for-detection-metrics-c67b9208d362) for details.      ## Contacts    Contact us with any questions regarding VictoriaMetrics at [info@victoriametrics.com](mailto:info@victoriametrics.com).      ## Community and contributions    Feel free asking any questions regarding VictoriaMetrics:    * [slack](https://slack.victoriametrics.com/)  * [linkedin](https://www.linkedin.com/company/victoriametrics/)  * [reddit](https://www.reddit.com/r/VictoriaMetrics/)  * [telegram-en](https://t.me/VictoriaMetrics_en)  * [telegram-ru](https://t.me/VictoriaMetrics_ru1)  * [articles and talks about VictoriaMetrics in Russian](https://github.com/denisgolius/victoriametrics-ru-links)  * [google groups](https://groups.google.com/forum/#!forum/victorametrics-users)    If you like VictoriaMetrics and want to contribute, then we need the following:    * Filing issues and feature requests [here](https://github.com/VictoriaMetrics/VictoriaMetrics/issues).  * Spreading a word about VictoriaMetrics: conference talks, articles, comments, experience sharing with colleagues.  * Updating documentation.    We are open to third-party pull requests provided they follow [KISS design principle](https://en.wikipedia.org/wiki/KISS_principle):    * Prefer simple code and architecture.  * Avoid complex abstractions.  * Avoid magic code and fancy algorithms.  * Avoid [big external dependencies](https://medium.com/@valyala/stripping-dependency-bloat-in-victoriametrics-docker-image-983fb5912b0d).  * Minimize the number of moving parts in the distributed system.  * Avoid automated decisions, which may hurt cluster availability, consistency or performance.    Adhering `KISS` principle simplifies the resulting code and architecture, so it can be reviewed, understood and verified by many people.    ## Reporting bugs    Report bugs and propose new features [here](https://github.com/VictoriaMetrics/VictoriaMetrics/issues).      ## VictoriaMetrics Logo    [Zip](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/VM_logo.zip) contains three folders with different image orientations (main color and inverted version).    Files included in each folder:    * 2 JPEG Preview files  * 2 PNG Preview files with transparent background  * 2 EPS Adobe Illustrator EPS10 files    ### Logo Usage Guidelines    #### Font used    * Lato Black  * Lato Regular    #### Color Palette    * HEX [#110f0f](https://www.color-hex.com/color/110f0f)  * HEX [#ffffff](https://www.color-hex.com/color/ffffff)    ### We kindly ask    * Please don't use any other font instead of suggested.  * There should be sufficient clear space around the logo.  * Do not change spacing, alignment, or relative locations of the design elements.  * Do not change the proportions of any of the design elements or the design itself. You    may resize as needed but must retain all proportions.      ## List of command-line flags    Pass `-help` to VictoriaMetrics in order to see the list of supported command-line flags with their description:    ```    -bigMergeConcurrency int      	The maximum number of CPU cores to use for big merges. Default value is used if set to 0    -configAuthKey string      	Authorization key for accessing /config page. It must be passed via authKey query arg    -csvTrimTimestamp duration      	Trim timestamps when importing csv data to this duration. Minimum practical duration is 1ms. Higher duration (i.e. 1s) may be used for reducing disk space usage for timestamp data (default 1ms)    -datadog.maxInsertRequestSize size      	The maximum size in bytes of a single DataDog POST request to /api/v1/series      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 67108864)    -dedup.minScrapeInterval duration      	Leave only the first sample in every time series per each discrete interval equal to -dedup.minScrapeInterval > 0. See https://docs.victoriametrics.com/#deduplication and https://docs.victoriametrics.com/#downsampling    -deleteAuthKey string      	authKey for metrics' deletion via /api/v1/admin/tsdb/delete_series and /tags/delSeries    -denyQueriesOutsideRetention      	Whether to deny queries outside of the configured -retentionPeriod. When set, then /api/v1/query_range would return '503 Service Unavailable' error for queries with 'from' value outside -retentionPeriod. This may be useful when multiple data sources with distinct retentions are hidden behind query-tee    -downsampling.period array      	Comma-separated downsampling periods in the format 'offset:period'. For example, '30d:10m' instructs to leave a single sample per 10 minutes for samples older than 30 days. See https://docs.victoriametrics.com/#downsampling for details      	Supports an array of values separated by comma or specified via multiple flags.    -dryRun      	Whether to check only -promscrape.config and then exit. Unknown config entries aren't allowed in -promscrape.config by default. This can be changed with -promscrape.config.strictParse=false command-line flag    -enableTCP6      	Whether to enable IPv6 for listening and dialing. By default only IPv4 TCP and UDP is used    -envflag.enable      	Whether to enable reading flags from environment variables additionally to command line. Command line flag values have priority over values from environment vars. Flags are read only from command line if this flag isn't set. See https://docs.victoriametrics.com/#environment-variables for more details    -envflag.prefix string      	Prefix for environment variables if -envflag.enable is set    -eula      	By specifying this flag, you confirm that you have an enterprise license and accept the EULA https://victoriametrics.com/assets/VM_EULA.pdf    -finalMergeDelay duration      	The delay before starting final merge for per-month partition after no new data is ingested into it. Final merge may require additional disk IO and CPU resources. Final merge may increase query speed and reduce disk space usage in some cases. Zero value disables final merge    -forceFlushAuthKey string      	authKey, which must be passed in query string to /internal/force_flush pages    -forceMergeAuthKey string      	authKey, which must be passed in query string to /internal/force_merge pages    -fs.disableMmap      	Whether to use pread() instead of mmap() for reading data files. By default mmap() is used for 64-bit arches and pread() is used for 32-bit arches, since they cannot read data files bigger than 2^32 bytes in memory. mmap() is usually faster for reading small data chunks than pread()    -graphiteListenAddr string      	TCP and UDP address to listen for Graphite plaintext data. Usually :2003 must be set. Doesn't work if empty    -graphiteTrimTimestamp duration      	Trim timestamps for Graphite data to this duration. Minimum practical duration is 1s. Higher duration (i.e. 1m) may be used for reducing disk space usage for timestamp data (default 1s)    -http.connTimeout duration      	Incoming http connections are closed after the configured timeout. This may help to spread the incoming load among a cluster of services behind a load balancer. Please note that the real timeout may be bigger by up to 10% as a protection against the thundering herd problem (default 2m0s)    -http.disableResponseCompression      	Disable compression of HTTP responses to save CPU resources. By default compression is enabled to save network bandwidth    -http.idleConnTimeout duration      	Timeout for incoming idle http connections (default 1m0s)    -http.maxGracefulShutdownDuration duration      	The maximum duration for a graceful shutdown of the HTTP server. A highly loaded server may require increased value for a graceful shutdown (default 7s)    -http.pathPrefix string      	An optional prefix to add to all the paths handled by http server. For example, if '-http.pathPrefix=/foo/bar' is set, then all the http requests will be handled on '/foo/bar/*' paths. This may be useful for proxied requests. See https://www.robustperception.io/using-external-urls-and-proxies-with-prometheus    -http.shutdownDelay duration      	Optional delay before http server shutdown. During this delay, the server returns non-OK responses from /health page, so load balancers can route new requests to other servers    -httpAuth.password string      	Password for HTTP Basic Auth. The authentication is disabled if -httpAuth.username is empty    -httpAuth.username string      	Username for HTTP Basic Auth. The authentication is disabled if empty. See also -httpAuth.password    -httpListenAddr string      	TCP address to listen for http connections (default "":8428"")    -import.maxLineLen size      	The maximum length in bytes of a single line accepted by /api/v1/import; the line length can be limited with 'max_rows_per_line' query arg passed to /api/v1/export      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 104857600)    -influx.databaseNames array      	Comma-separated list of database names to return from /query and /influx/query API. This can be needed for accepting data from Telegraf plugins such as https://github.com/fangli/fluent-plugin-influxdb      	Supports an array of values separated by comma or specified via multiple flags.    -influx.maxLineSize size      	The maximum size in bytes for a single InfluxDB line during parsing      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 262144)    -influxDBLabel string      	Default label for the DB name sent over '?db={db_name}' query parameter (default ""db"")    -influxListenAddr string      	TCP and UDP address to listen for InfluxDB line protocol data. Usually :8189 must be set. Doesn't work if empty. This flag isn't needed when ingesting data over HTTP - just send it to http://<victoriametrics>:8428/write    -influxMeasurementFieldSeparator string      	Separator for '{measurement}{separator}{field_name}' metric name when inserted via InfluxDB line protocol (default ""_"")    -influxSkipMeasurement      	Uses '{field_name}' as a metric name while ignoring '{measurement}' and '-influxMeasurementFieldSeparator'    -influxSkipSingleField      	Uses '{measurement}' instead of '{measurement}{separator}{field_name}' for metic name if InfluxDB line contains only a single field    -influxTrimTimestamp duration      	Trim timestamps for InfluxDB line protocol data to this duration. Minimum practical duration is 1ms. Higher duration (i.e. 1s) may be used for reducing disk space usage for timestamp data (default 1ms)    -insert.maxQueueDuration duration      	The maximum duration for waiting in the queue for insert requests due to -maxConcurrentInserts (default 1m0s)    -logNewSeries      	Whether to log new series. This option is for debug purposes only. It can lead to performance issues when big number of new series are ingested into VictoriaMetrics    -loggerDisableTimestamps      	Whether to disable writing timestamps in logs    -loggerErrorsPerSecondLimit int      	Per-second limit on the number of ERROR messages. If more than the given number of errors are emitted per second, the remaining errors are suppressed. Zero values disable the rate limit    -loggerFormat string      	Format for logs. Possible values: default, json (default ""default"")    -loggerLevel string      	Minimum level of errors to log. Possible values: INFO, WARN, ERROR, FATAL, PANIC (default ""INFO"")    -loggerOutput string      	Output for the logs. Supported values: stderr, stdout (default ""stderr"")    -loggerTimezone string      	Timezone to use for timestamps in logs. Timezone must be a valid IANA Time Zone. For example: America/New_York, Europe/Berlin, Etc/GMT+3 or Local (default ""UTC"")    -loggerWarnsPerSecondLimit int      	Per-second limit on the number of WARN messages. If more than the given number of warns are emitted per second, then the remaining warns are suppressed. Zero values disable the rate limit    -maxConcurrentInserts int      	The maximum number of concurrent inserts. Default value should work for most cases, since it minimizes the overhead for concurrent inserts. This option is tigthly coupled with -insert.maxQueueDuration (default 16)    -maxInsertRequestSize size      	The maximum size in bytes of a single Prometheus remote_write API request      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 33554432)    -maxLabelValueLen int      	The maximum length of label values in the accepted time series. Longer label values are truncated. In this case the vm_too_long_label_values_total metric at /metrics page is incremented (default 16384)    -maxLabelsPerTimeseries int      	The maximum number of labels accepted per time series. Superfluous labels are dropped. In this case the vm_metrics_with_dropped_labels_total metric at /metrics page is incremented (default 30)    -memory.allowedBytes size      	Allowed size of system memory VictoriaMetrics caches may occupy. This option overrides -memory.allowedPercent if set to a non-zero value. Too low a value may increase the cache miss rate usually resulting in higher CPU and disk IO usage. Too high a value may evict too much data from OS page cache resulting in higher disk IO usage      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 0)    -memory.allowedPercent float      	Allowed percent of system memory VictoriaMetrics caches may occupy. See also -memory.allowedBytes. Too low a value may increase cache miss rate usually resulting in higher CPU and disk IO usage. Too high a value may evict too much data from OS page cache which will result in higher disk IO usage (default 60)    -metricsAuthKey string      	Auth key for /metrics. It must be passed via authKey query arg. It overrides httpAuth.* settings    -opentsdbHTTPListenAddr string      	TCP address to listen for OpentTSDB HTTP put requests. Usually :4242 must be set. Doesn't work if empty    -opentsdbListenAddr string      	TCP and UDP address to listen for OpentTSDB metrics. Telnet put messages and HTTP /api/put messages are simultaneously served on TCP port. Usually :4242 must be set. Doesn't work if empty    -opentsdbTrimTimestamp duration      	Trim timestamps for OpenTSDB 'telnet put' data to this duration. Minimum practical duration is 1s. Higher duration (i.e. 1m) may be used for reducing disk space usage for timestamp data (default 1s)    -opentsdbhttp.maxInsertRequestSize size      	The maximum size of OpenTSDB HTTP put request      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 33554432)    -opentsdbhttpTrimTimestamp duration      	Trim timestamps for OpenTSDB HTTP data to this duration. Minimum practical duration is 1ms. Higher duration (i.e. 1s) may be used for reducing disk space usage for timestamp data (default 1ms)    -pprofAuthKey string      	Auth key for /debug/pprof. It must be passed via authKey query arg. It overrides httpAuth.* settings    -precisionBits int      	The number of precision bits to store per each value. Lower precision bits improves data compression at the cost of precision loss (default 64)    -promscrape.cluster.memberNum int      	The number of number in the cluster of scrapers. It must be an unique value in the range 0 ... promscrape.cluster.membersCount-1 across scrapers in the cluster    -promscrape.cluster.membersCount int      	The number of members in a cluster of scrapers. Each member must have an unique -promscrape.cluster.memberNum in the range 0 ... promscrape.cluster.membersCount-1 . Each member then scrapes roughly 1/N of all the targets. By default cluster scraping is disabled, i.e. a single scraper scrapes all the targets    -promscrape.cluster.replicationFactor int      	The number of members in the cluster, which scrape the same targets. If the replication factor is greater than 2, then the deduplication must be enabled at remote storage side. See https://docs.victoriametrics.com/#deduplication (default 1)    -promscrape.config string      	Optional path to Prometheus config file with 'scrape_configs' section containing targets to scrape. The path can point to local file and to http url. See https://docs.victoriametrics.com/#how-to-scrape-prometheus-exporters-such-as-node-exporter for details    -promscrape.config.dryRun      	Checks -promscrape.config file for errors and unsupported fields and then exits. Returns non-zero exit code on parsing errors and emits these errors to stderr. See also -promscrape.config.strictParse command-line flag. Pass -loggerLevel=ERROR if you don't need to see info messages in the output.    -promscrape.config.strictParse      	Whether to deny unsupported fields in -promscrape.config . Set to false in order to silently skip unsupported fields (default true)    -promscrape.configCheckInterval duration      	Interval for checking for changes in '-promscrape.config' file. By default the checking is disabled. Send SIGHUP signal in order to force config check for changes    -promscrape.consul.waitTime duration      	Wait time used by Consul service discovery. Default value is used if not set    -promscrape.consulSDCheckInterval duration      	Interval for checking for changes in Consul. This works only if consul_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config for details (default 30s)    -promscrape.digitaloceanSDCheckInterval duration      	Interval for checking for changes in digital ocean. This works only if digitalocean_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#digitalocean_sd_config for details (default 1m0s)    -promscrape.disableCompression      	Whether to disable sending 'Accept-Encoding: gzip' request headers to all the scrape targets. This may reduce CPU usage on scrape targets at the cost of higher network bandwidth utilization. It is possible to set 'disable_compression: true' individually per each 'scrape_config' section in '-promscrape.config' for fine grained control    -promscrape.disableKeepAlive      	Whether to disable HTTP keep-alive connections when scraping all the targets. This may be useful when targets has no support for HTTP keep-alive connection. It is possible to set 'disable_keepalive: true' individually per each 'scrape_config' section in '-promscrape.config' for fine grained control. Note that disabling HTTP keep-alive may increase load on both vmagent and scrape targets    -promscrape.discovery.concurrency int      	The maximum number of concurrent requests to Prometheus autodiscovery API (Consul, Kubernetes, etc.) (default 100)    -promscrape.discovery.concurrentWaitTime duration      	The maximum duration for waiting to perform API requests if more than -promscrape.discovery.concurrency requests are simultaneously performed (default 1m0s)    -promscrape.dnsSDCheckInterval duration      	Interval for checking for changes in dns. This works only if dns_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dns_sd_config for details (default 30s)    -promscrape.dockerSDCheckInterval duration      	Interval for checking for changes in docker. This works only if docker_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#docker_sd_config for details (default 30s)    -promscrape.dockerswarmSDCheckInterval duration      	Interval for checking for changes in dockerswarm. This works only if dockerswarm_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dockerswarm_sd_config for details (default 30s)    -promscrape.dropOriginalLabels      	Whether to drop original labels for scrape targets at /targets and /api/v1/targets pages. This may be needed for reducing memory usage when original labels for big number of scrape targets occupy big amounts of memory. Note that this reduces debuggability for improper per-target relabeling configs    -promscrape.ec2SDCheckInterval duration      	Interval for checking for changes in ec2. This works only if ec2_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config for details (default 1m0s)    -promscrape.eurekaSDCheckInterval duration      	Interval for checking for changes in eureka. This works only if eureka_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#eureka_sd_config for details (default 30s)    -promscrape.fileSDCheckInterval duration      	Interval for checking for changes in 'file_sd_config'. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#file_sd_config for details (default 5m0s)    -promscrape.gceSDCheckInterval duration      	Interval for checking for changes in gce. This works only if gce_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#gce_sd_config for details (default 1m0s)    -promscrape.httpSDCheckInterval duration      	Interval for checking for changes in http endpoint service discovery. This works only if http_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#http_sd_config for details (default 1m0s)    -promscrape.kubernetes.apiServerTimeout duration      	How frequently to reload the full state from Kuberntes API server (default 30m0s)    -promscrape.kubernetesSDCheckInterval duration      	Interval for checking for changes in Kubernetes API server. This works only if kubernetes_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config for details (default 30s)    -promscrape.maxDroppedTargets int      	The maximum number of droppedTargets to show at /api/v1/targets page. Increase this value if your setup drops more scrape targets during relabeling and you need investigating labels for all the dropped targets. Note that the increased number of tracked dropped targets may result in increased memory usage (default 1000)    -promscrape.maxResponseHeadersSize size      	The maximum size of http response headers from Prometheus scrape targets      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 4096)    -promscrape.maxScrapeSize size      	The maximum size of scrape response in bytes to process from Prometheus targets. Bigger responses are rejected      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 16777216)    -promscrape.minResponseSizeForStreamParse size      	The minimum target response size for automatic switching to stream parsing mode, which can reduce memory usage. See https://docs.victoriametrics.com/vmagent.html#stream-parsing-mode      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 1000000)    -promscrape.noStaleMarkers      	Whether to disable sending Prometheus stale markers for metrics when scrape target disappears. This option may reduce memory usage if stale markers aren't needed for your setup. This option also disables populating the scrape_series_added metric. See https://prometheus.io/docs/concepts/jobs_instances/#automatically-generated-labels-and-time-series    -promscrape.openstackSDCheckInterval duration      	Interval for checking for changes in openstack API server. This works only if openstack_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#openstack_sd_config for details (default 30s)    -promscrape.seriesLimitPerTarget int      	Optional limit on the number of unique time series a single scrape target can expose. See https://docs.victoriametrics.com/vmagent.html#cardinality-limiter for more info    -promscrape.streamParse      	Whether to enable stream parsing for metrics obtained from scrape targets. This may be useful for reducing memory usage when millions of metrics are exposed per each scrape target. It is posible to set 'stream_parse: true' individually per each 'scrape_config' section in '-promscrape.config' for fine grained control    -promscrape.suppressDuplicateScrapeTargetErrors      	Whether to suppress 'duplicate scrape target' errors; see https://docs.victoriametrics.com/vmagent.html#troubleshooting for details    -promscrape.suppressScrapeErrors      	Whether to suppress scrape errors logging. The last error for each target is always available at '/targets' page even if scrape errors logging is suppressed    -relabelConfig string      	Optional path to a file with relabeling rules, which are applied to all the ingested metrics. The path can point either to local file or to http url. See https://docs.victoriametrics.com/#relabeling for details. The config is reloaded on SIGHUP signal    -relabelDebug      	Whether to log metrics before and after relabeling with -relabelConfig. If the -relabelDebug is enabled, then the metrics aren't sent to storage. This is useful for debugging the relabeling configs    -retentionPeriod value      	Data with timestamps outside the retentionPeriod is automatically deleted      	The following optional suffixes are supported: h (hour), d (day), w (week), y (year). If suffix isn't set, then the duration is counted in months (default 1)    -search.cacheTimestampOffset duration      	The maximum duration since the current time for response data, which is always queried from the original raw data, without using the response cache. Increase this value if you see gaps in responses due to time synchronization issues between VictoriaMetrics and data sources. See also -search.disableAutoCacheReset (default 5m0s)    -search.disableAutoCacheReset      	Whether to disable automatic response cache reset if a sample with timestamp outside -search.cacheTimestampOffset is inserted into VictoriaMetrics    -search.disableCache      	Whether to disable response caching. This may be useful during data backfilling    -search.graphiteMaxPointsPerSeries int      	The maximum number of points per series Graphite render API can return (default 1000000)    -search.graphiteStorageStep duration      	The interval between datapoints stored in the database. It is used at Graphite Render API handler for normalizing the interval between datapoints in case it isn't normalized. It can be overriden by sending 'storage_step' query arg to /render API or by sending the desired interval via 'Storage-Step' http header during querying /render API (default 10s)    -search.latencyOffset duration      	The time when data points become visible in query results after the collection. Too small value can result in incomplete last points for query results (default 30s)    -search.logSlowQueryDuration duration      	Log queries with execution time exceeding this value. Zero disables slow query logging (default 5s)    -search.maxConcurrentRequests int      	The maximum number of concurrent search requests. It shouldn't be high, since a single request can saturate all the CPU cores. See also -search.maxQueueDuration (default 8)    -search.maxExportDuration duration      	The maximum duration for /api/v1/export call (default 720h0m0s)    -search.maxLookback duration      	Synonym to -search.lookback-delta from Prometheus. The value is dynamically detected from interval between time series datapoints if not set. It can be overridden on per-query basis via max_lookback arg. See also '-search.maxStalenessInterval' flag, which has the same meaining due to historical reasons    -search.maxPointsPerTimeseries int      	The maximum points per a single timeseries returned from /api/v1/query_range. This option doesn't limit the number of scanned raw samples in the database. The main purpose of this option is to limit the number of per-series points returned to graphing UI such as Grafana. There is no sense in setting this limit to values bigger than the horizontal resolution of the graph (default 30000)    -search.maxQueryDuration duration      	The maximum duration for query execution (default 30s)    -search.maxQueryLen size      	The maximum search query length in bytes      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 16384)    -search.maxQueueDuration duration      	The maximum time the request waits for execution when -search.maxConcurrentRequests limit is reached; see also -search.maxQueryDuration (default 10s)    -search.maxSamplesPerQuery int      	The maximum number of raw samples a single query can process across all time series. This protects from heavy queries, which select unexpectedly high number of raw samples. See also -search.maxSamplesPerSeries (default 1000000000)    -search.maxSamplesPerSeries int      	The maximum number of raw samples a single query can scan per each time series. This option allows limiting memory usage (default 30000000)    -search.maxStalenessInterval duration      	The maximum interval for staleness calculations. By default it is automatically calculated from the median interval between samples. This flag could be useful for tuning Prometheus data model closer to Influx-style data model. See https://prometheus.io/docs/prometheus/latest/querying/basics/#staleness for details. See also '-search.maxLookback' flag, which has the same meaning due to historical reasons    -search.maxStatusRequestDuration duration      	The maximum duration for /api/v1/status/* requests (default 5m0s)    -search.maxStepForPointsAdjustment duration      	The maximum step when /api/v1/query_range handler adjusts points with timestamps closer than -search.latencyOffset to the current time. The adjustment is needed because such points may contain incomplete data (default 1m0s)    -search.maxTagKeys int      	The maximum number of tag keys returned from /api/v1/labels (default 100000)    -search.maxTagValueSuffixesPerSearch int      	The maximum number of tag value suffixes returned from /metrics/find (default 100000)    -search.maxTagValues int      	The maximum number of tag values returned from /api/v1/label/<label_name>/values (default 100000)    -search.maxUniqueTimeseries int      	The maximum number of unique time series each search can scan. This option allows limiting memory usage (default 300000)    -search.minStalenessInterval duration      	The minimum interval for staleness calculations. This flag could be useful for removing gaps on graphs generated from time series with irregular intervals between samples. See also '-search.maxStalenessInterval'    -search.noStaleMarkers      	Set this flag to true if the database doesn't contain Prometheus stale markers, so there is no need in spending additional CPU time on its handling. Staleness markers may exist only in data obtained from Prometheus scrape targets    -search.queryStats.lastQueriesCount int      	Query stats for /api/v1/status/top_queries is tracked on this number of last queries. Zero value disables query stats tracking (default 20000)    -search.queryStats.minQueryDuration duration      	The minimum duration for queries to track in query stats at /api/v1/status/top_queries. Queries with lower duration are ignored in query stats (default 1ms)    -search.resetCacheAuthKey string      	Optional authKey for resetting rollup cache via /internal/resetRollupResultCache call    -search.treatDotsAsIsInRegexps      	Whether to treat dots as is in regexp label filters used in queries. For example, foo{bar=~""a.b.c""} will be automatically converted to foo{bar=~""a\\.b\\.c""}, i.e. all the dots in regexp filters will be automatically escaped in order to match only dot char instead of matching any char. Dots in "".+"", "".*"" and "".{n}"" regexps aren't escaped. This option is DEPRECATED in favor of {__graphite__=""a.*.c""} syntax for selecting metrics matching the given Graphite metrics filter    -selfScrapeInstance string      	Value for 'instance' label, which is added to self-scraped metrics (default ""self"")    -selfScrapeInterval duration      	Interval for self-scraping own metrics at /metrics page    -selfScrapeJob string      	Value for 'job' label, which is added to self-scraped metrics (default ""victoria-metrics"")    -smallMergeConcurrency int      	The maximum number of CPU cores to use for small merges. Default value is used if set to 0    -snapshotAuthKey string      	authKey, which must be passed in query string to /snapshot* pages    -sortLabels      	Whether to sort labels for incoming samples before writing them to storage. This may be needed for reducing memory usage at storage when the order of labels in incoming samples is random. For example, if m{k1=""v1"",k2=""v2""} may be sent as m{k2=""v2"",k1=""v1""}. Enabled sorting for labels can slow down ingestion performance a bit    -storage.cacheSizeIndexDBDataBlocks size      	Overrides max size for indexdb/dataBlocks cache. See https://docs.victoriametrics.com/Single-server-VictoriaMetrics.html#cache-tuning      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 0)    -storage.cacheSizeIndexDBIndexBlocks size      	Overrides max size for indexdb/indexBlocks cache. See https://docs.victoriametrics.com/Single-server-VictoriaMetrics.html#cache-tuning      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 0)    -storage.cacheSizeStorageTSID size      	Overrides max size for storage/tsid cache. See https://docs.victoriametrics.com/Single-server-VictoriaMetrics.html#cache-tuning      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 0)    -storage.maxDailySeries int      	The maximum number of unique series can be added to the storage during the last 24 hours. Excess series are logged and dropped. This can be useful for limiting series churn rate. See also -storage.maxHourlySeries    -storage.maxHourlySeries int      	The maximum number of unique series can be added to the storage during the last hour. Excess series are logged and dropped. This can be useful for limiting series cardinality. See also -storage.maxDailySeries    -storage.minFreeDiskSpaceBytes size      	The minimum free disk space at -storageDataPath after which the storage stops accepting new data      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 10000000)    -storageDataPath string      	Path to storage data (default ""victoria-metrics-data"")    -tls      	Whether to enable TLS (aka HTTPS) for incoming requests. -tlsCertFile and -tlsKeyFile must be set if -tls is set    -tlsCertFile string      	Path to file with TLS certificate. Used only if -tls is set. Prefer ECDSA certs instead of RSA certs as RSA certs are slower. The provided certificate file is automatically re-read every second, so it can be dynamically updated    -tlsKeyFile string      	Path to file with TLS key. Used only if -tls is set. The provided key file is automatically re-read every second, so it can be dynamically updated    -version      	Show VictoriaMetrics version  ``` """
Big data;https://github.com/johnsonc/lambdo;"""[![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/conceptoriented/Lobby)  [![License: MIT](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://github.com/asavinov/lambdo/blob/master/LICENSE)  [![Python 3.6](https://img.shields.io/badge/python-3.6-brightgreen.svg)](https://www.python.org/downloads/release/python-360/)    # Feature engineering and machine learning: together at last!    Lambdo is a workflow engine which significantly simplifies the analysis process by *unifying* feature engineering and machine learning operations. Lambdo data analysis workflow does not distinguish between them and any node can be treated either as a feature or as prediction, and both of them can be trained.    Such a unification is possible because of the underlying *column-oriented* data processing paradigm which treats columns as first-class elements of the data processing pipeline having the same rights as tables. In Lambdo, a workflow consist of *table population* operations which process sets of records and *column evaluation* operations which produce new columns (features) from existing columns. This radically changes the way data is processed. The same approach is used also in Bistro: <https://github.com/asavinov/bistro>    Here are some unique distinguishing features of Lambdo:    * **No difference between features and models.** Lambdo unifies feature engineering and machine learning so that a workflow involves many feature definitions and many machine learning algorithms. It is especially important for deep learning where abstract intermediate features have to be learned.  * **One workflow for both prediction and training.** Lambdo nodes combine applying a transformation with training its model so that nodes of a workflow can be re-trained when required. This also guarantees that the same features will be used for both learning phase and prediction phase.  * **Columns first.**] Lambdo workflow use column operations along with table operations which makes many operations much simpler.  * **User-defined functions for extensibility.** Lambdo relies on user-defined functions which can be as simple as format conversion and as complex as deep learning networks.  * **Analysis of time-series and forecasting made easy.** Lambdo makes time series analysis much simpler by providing many using mechanisms like column families (for example, several moving averages with different window sizes), window-awareness (generation of windows is a built-in function), pre-defined functions for extracting goals.  * **As flexible as programming and as easy as IDE.** Lambdo is positioned between (Python) programming and interactive environments (like KNIME)    # Contents    * [Why Lambdo?](#why-lambdo)    * [Why feature engineering?](#why-feature-engineering)    * [Uniting feature engineering with data mining](#uniting-feature-engineering-with-data-mining)    * [Any transformation model has an automatic training procedure](#any-transformation-model-has-an-automatic-training-procedure)    * [Columns first: Column-orientation as a basis for feature engineering](#columns-first-column-orientation-as-a-basis-for-feature-engineering)    * [Time series first: Time series analysis and forecasting made easy](#time-series-first-time-series-analysis-and-forecasting-made-easy)    * [Getting started with Lambdo](#getting-started-with-lambdo)    * [Workflow definition](#workflow-definition)      * [Workflow structure](#workflow-structure)      * [Imports](#imports)    * [Table definition](#table-definition)      * [Table population function](#table-population-function)      * [Column filter](#column-filter)      * [Row filter](#row-filter)    * [Column definition](#column-definition)      * [Column evaluation function](#column-evaluation-function)      * [Function scopes](#function-scopes)      * [Training a model](#training-a-model)    * [Examples and analysis templates](#examples-and-analysis-templates)    * [Example 1: Input and output](#example-1-input-and-output)    * [Example 2: Record-based features](#example-2-record-based-features)    * [Example 3: User-defined record-based features](#example-3-user-defined-record-based-features)    * [Example 4: Table-based features](#example-4-table-based-features)    * [How to install](#how-to-install)    * [Install from source code](#install-from-source-code)    * [Install from package](#install-from-package)    * [How to test](#how-to-test)    * [How to use](#how-to-use)    # Why Lambdo?    ## Why feature engineering?    In many cases, defining good features is more important than choosing and tuning a machine learning algorithm to be applied to the data. Hence, the quality of the data analysis result depends more on the quality of the generated features than on the machine learning model.    Such high importance of having good features is explained by the following factors:    *	It is a quite rare situation when you have enough data and even if you have it then it then probably you do not have enough computing resources to process it. In this situation, manually defined or automatically mined features compensate this lack of data or computing resources to process it. Essentially, we combine expert systems with data mining.    *	Feature engineering is a mechanism of creating new levels of abstraction in knowledge representation because each (non-trivial) feature extract and makes explicit some piece of knowledge hidden in the data. It is almost precisely what deep learning is intended for. In this sense, feature engineering does what hidden layers of a neural network do or what the convolutional layer of a neural network does.    ## Uniting feature engineering with data mining    Let us assume that we want to compute moving average for a stock price. For each record, we compute an average value of this and some previous prices. This operation is interpreted as a transformation which generates a new feature stored as a column. Its result is defined by one parameter: window size (the number of days to be averaged including this day), and this number is essentially our transformation model.    Now let us assume that we want to add a second column which stores prices for tomorrow predicted using some algorithm from this and some previous values. We could develop a rather simple model which extrapolates price using previous values. This forecasting model, when applied, will also generate a new column with some values. Its result could depend on how many previous values are used by the extrapolation algorithm and this number is essentially our forecasting model.    An important observation here is that there is no difference between generating a new feature using some transformation model and generating a prediction using some forecast model. Technically, these are simply some transformations using some parameters, and these parameters are referred to as a model. Although there exist some exceptions where this analogy does not work, Lambdo assumes that it is so and follows the principle that    > Both generating a feature and applying a machine learning algorithm are data transformations parameterized by their specific models    Lambdo simply does not distinguish between them by assuming that any transformation that needs to be done is described by its (Python) function name and a (Python) model object. Lambdo will execute this function but it is unaware of its purpose. It can be a procedure for extracting dates from a string or it can be a prediction using a deep neural network model. In all these cases, the function will add new (derived) column to the existing table.    ## Any transformation model has an automatic training procedure    One difference between feature generation and machine learning is that machine learning models cannot exist without an algorithm for their training - the whole idea of machine learning is that models are learned automatically from data rather than defined manually. On the hand, features can be defined either manually or learned from data. Since Lambdo is intended for unifying feature engineering and machine learning, it makes the following assumption:    > Any transformation has an accompanying function for generating its models    This means that first we define some transformation which is supposed to be applied to the data and produce a new feature or analysis result. However, the model for this transformation can be generated automatically (if necessary) before applying it. For example, even for computing moving averages an important question is what window size to choose, and instead of guessing we can delegate this question to the training procedure (which could use auto-regression to choose the best window size). Importantly, both procedures â€“ applying a transformation and training its model â€“ are part of the same workflow where they can be intermediate nodes and not only the final predicting node.    ## Columns first: Column-orientation as a basis for feature engineering    Assume that we compute a moving average for a time series. The result of this operation is a new column. Now assume that we apply a clustering algorithm to records from a data set. In this case the result is again a new column. In fact, generating new columns is opposed to generating a new table and we can see these two operations in many data processing and data analysis approaches. Formally, we distinguish between set operations and operations with functions. Lambdo uses the following principle:    > Lambdo workflow consists of a graph of table definitions and each tables consists of a number of column definitions    A table definition describes how some set is populated using data in already existing tables. A typical table definition is a read/write operation or resampling time series or pivoting. A column definition describes how new values are evaluated using data in other columns in this and other tables.    This approach relies on the principles of the concept-oriented data model which also underlies [Bistro](https://github.com/asavinov/bistro) â€“ an open source system for batch and stream data processing.    ## Time series first: Time series analysis and forecasting made easy    Most existing machine learning algorithms are not time-aware and they cannot be directly applied to time series. Although Lambdo is a general purpose workflow engine which can be applied to any data, its functionality was developed with time series analysis and forecasting in mind. Therefore, it includes many mechanisms which make feature engineering much easier when working with time series:    *	Easily defining a family of columns which are features with only minor changes in their definitions. A typical but wide spread example is a family of features which use different window sizes.    *	Predefined column definitions for typical goal functions to be predicted or used for training intermediate features. Note that time series analysis is almost always supervised learning but there are different formulations for what we want to forecast.    *	Lambdo is window-aware workflow engine and for any transformation it is necessary to define its scope which means the number of rows the function will be applied to. This parameter is essentially the length of the history (number of previous records to be processed by the function).    *	Lambdo is going to be also object-aware which means it can partition the whole data set according to the value of the selected column interpreted as an object. This allows us to analyze data coming from multiple objects like devices, sensors, stock symbols etc.    *	Easy control of when to train which nodes. The problem here is that frequently a workflow has to be re-trained periodically but we do not want to re-train all nodes. This mechanism allows us to specify criteria for re-training its models.    # Getting started with Lambdo    Lambdo implements a novel *column-oriented* approach to data analysis by unifying various kinds of data processing: feature engineering, data transformations and data mining.    ## Workflow definition    ### Workflow structure    Lambdo is intended for processing data by using two types of transformations:    * Table transformations produce a new table given one or more input tables.  * Column transformations produce a new column in the table given one or more input columns.    A workflow is a number of table definitions each having a number of column definitions. These tables and columns compose a graph where edges are dependencies. If an element (table or column) in this graph has another element as its inputs then this dependency is an edge. If an element (table or column) does not have inputs then it is a data source. If an element does not have dependents then it is a data sink.    This data processing logic of Lambdo is represented in JSON format and stored in a workflow file and having the following structure:     ```javascript  {    ""tables"": [      ""table"": { ""function"": ""my_table_func_1"", ""columns"": [...] }      ""table"": {        ""function"": ""my_table_func_2"",        ""columns"": {          ""column"": { ""function"": ""my_column_func_1"", ... }          ""column"": { ""function"": ""my_column_func_2"", ... }        }      ""table"": { ""function"": ""my_table_func_3"", ""columns"": [...] }    ]  }  ```    Each table and column definition has to specify a (Python) function name which will actually do data processing. Table definition will use functions for data population. Column definitions will use functions for evaluating new columns. When Lambo executes a workflow, it populates tables according to their definitions and evaluates columns (within tables) according to their definitions. Here it is important to understand that tables are used for set operations while columns are used for operations with mathematical functions.    ### Imports    Data processing in Lambdo relies on Python functions which do real data processing. Before these functions can be executed, they have to be imported. The location of functions to be imported is specified in a special field. For example, if the functions we want to use for data processing are in the `examples.example1` module and `datetime` module then we specify them as follows:    ```json  {    ""id"": ""Example 1"",    ""imports"": [""examples.example1"", ""datetime""],  }  ```    Now we can use functions from these modules in the workflow table and column definitions.    ## Table definition    ### Table population function    A table definition has to provide some Python function which will *populate* this table. This function can be standard (built-in) Python function or it could be part of an imported module like `scale` function from the `sklearn.preprocessing` module or `BaseLibSVM.predict` function from the `sklearn.svm.base` module. Functions can be also defined for this specific workflow if they encode some domain-specific feature definition.    For example, if we want to read data then such a table could be defined as follows:    ```json  {    ""id"": ""My table"",    ""function"": ""pandas:read_csv"",    ""inputs"": [],    ""model"": {      ""filepath_or_buffer"": ""my_file.csv"",      ""nrows"": 100    }  }  ```    Here we used a standard function from `pandas` but it could be any other function which returns a `DataFrame`.    Any function takes parameters which are referred to as a *model* and passed to the function. In the above example, we passed input file name and maximum umber of records to be read.    ### Column filter    Frequently, it is necessary to generate some intermediate features (columns) which are not needed for the final analysis. Such features should be removed from the table. This can be done by specifying a *column filter* and this selection of necessary columns is performed always when all features within this table have been generated.    We can specify a list of columns, which have to be selected and passed to the next nodes in the graph:    ```json  {    ""id"": ""My table"",    ""function"": ""pandas:read_csv"",    ""column_filter"": [""Open"", ""Close""]  },  ```    Alternatively, we can specify columns, which have to be excluded from the selected features to be passed to the next nodes:    ```json  {    ""id"": ""My table"",    ""function"": ""pandas:read_csv"",    ""column_filter"": {""exclude"": [""Date""]}  },  ```    The next table will then receive a table with all columns generated in this table excepting the `Date` column (which contains time stamps not needed for analysis).    ### Row filter    Not all records in the table need to be analyzed and such records can be excluded before the table is passed to the next node for processing. Records to be removed are specified in the row filter which provides several methods for removal.    Many analysis algorithms cannot deal with `NaN` values and the simplest way to solve this problem is to remove all records which have at least one `NaN`:    ```json  {    ""id"": ""My table"",    ""function"": ""pandas:read_csv"",    ""row_filter"": {""dropna"": true}  },  ```    The `dropna` can also specify a list of columns and then only the values of these columns will be checked.    Another way to filter rows is to specify columns with boolean values and then the result table will retain only rows with `True` in these columns:    ```json  {    ""id"": ""My table"",    ""function"": ""pandas:read_csv"",    ""row_filter"": {""predicate"": [""Selection""]}  },  ```    A column with binary values can be defined precisely as any other derived column using a function, which knows which records are needed for analysis. (This column can be then removed by using a column filter.)    It is also possible to reandomly shuffle records by specifying the portion we want to keep in the table. This filter will keep only 80% of randomly selected records:    ```json  {    ""id"": ""My table"",    ""function"": ""pandas:read_csv"",    ""row_filter"": {""sample"": {""frac"": 0.8}  },  ```    You can specify `""sample"":true` if all records have to be shuffled.    The records can be also selected by specifying their integer position: start, end (exclusive) and step. The following filter will select every second record:    ```json  {    ""id"": ""My table"",    ""function"": ""pandas:read_csv"",    ""row_filter"": {""slice"": {""start"": 0, ""step"": 2}  },  ```    ## Column definition    ### Column evaluation function    A column definition specifies how its values are computed from the values stored in other columns. The way these values are computed is implemented by some Python function which can be either a standard Python function, a function from some existing module or a user-defined function. Lambdo simply gets the name of this function from the workflow and then calls it to generate this column values.    A function is specified as a pair of its module and function name separated by a colon:    ```javascript  ""function"": ""my_module:my_function""  ```    It is assumed that the first argument of the function is data to be processed and the second argument is a model which parameterizes this transformation. Note however that some function can take other parameters and also the type of these arguments can vary.    ### Function scopes    What data a transformation function receives in its first argument? There are different options starting from a single value and ending with a whole input table. This is determined by the column definition parameter called `scope` which takes the following values:    * Scope `one` or `1` means that Lambdo will apply this function to every row of the input table and the function is expected to return a single value stored as the column value for this record. Type of data passed to the function depends on how many columns the `input` has.    * If `input` has only one column then the function will receive a single value.    * If `input` has more than 1 columns then the function will receive a `Series` object with their field values.  * Scope `all` means that the function will be applied to all rows of the table, that is, there will be one call and the whole table will be passed as a parameter. Type of the argument is `DataFrame`.  * Otherwise the system assume that the function has to be applied to all subsets of the table rows, called windows. Size of the window (number of records in one group) is scope value. For example, `scope: 5` means that each window will consists of 5 records. Type of this group depends on the number of columns in the `input`:     * If `input` has only one column then the function will receive a `Series` of values.    * If `input` has more than 1 columns then the function will receive a `DataFrame` object with the records from the group.    ### Training a model    A new feature is treated as a transformation, which results in a new column with the values derived from the data in other columns. This transformation is performed using some *model*, which is simply a set of parameters. A model can be specified explicitly by-value if we know these parameters. However, model parameters can be derived from the data using a separate procedure, called *training*. The transformation is then applied *after* the training.    How a model is trained is specified in a block within a column definition:    ```json  {    ""id"": ""Prediction"",    ""function"": ""examples.example1:gb_predict"",    ""scope"": ""all"",    ""inputs"": {""exclude"": [""Labels""]},    ""train"": {      ""function"": ""examples.example1:gb_fit"",      ""model"": {""n_estimators"": 500, ""max_depth"": 4, ""min_samples_split"": 2, ""learning_rate"": 0.01, ""loss"": ""ls""},      ""outputs"": [""Labels""]    }  }  ```    Here we need to specify a function which will perform such a training: `""function"": ""examples.example1:gb_fit""`. The training function also needs its own hyper-parameters, for example: `""model"": {""max_depth"": 4}`. Finally, the training procedure (in the case of supervised learning) needs labels: `""outputs"": [""Labels""]`. Note also that excluded the `Labels` from the input so that they are not used as features for training.    Lambdo will first train a model by using the input data and then use this model for prediction.    # Examples and analysis templates    ## Example 1: Input and output    Assume that the data is stored in a CSV file and we want to use this data to produce new features or for data analysis. Loading data is a table population operation which is defined in some table node of the workflow. How the table is populated depends on the `function` of this definition. In our example, we want to re-use a standard `pandas` for loading CSV files. Such a table node is defined as follows:    ```json  {    ""id"": ""Source table"",    ""function"": ""pandas:read_csv"",    ""inputs"": [],    ""model"": {      ""filepath_or_buffer"": ""my_file.csv"",      ""nrows"": 100    }  }  ```    After executing this node, it will store the data from this file. We could also use any other function for loading or generating data. For example, it could a function which produces random data.    Data output can also be performed by using a standard `pandas` function:    ```json  {    ""id"": ""Source table"",    ""function"": ""pandas:DataFrame.to_csv"",    ""inputs"": ""Source table"",    ""model"": {      ""path_or_buf"": ""my_output.csv"",      ""index"": false    }  }  ```    Note that the `inputs` fields points to the table which needs to be processed. The result of its execution will be a new CSV file.    Run this example from command line by executing:    ```console  $ lambdo examples/example1.json  ```    Another useful standard function for storing a table is `to_json` with a possible model like `{""path_or_buf"": ""my_file.json.gz"", ""orient""=""records"", ""lines""=True, ""compression""=""gzip""}` (the file will be compressed). To read a JSON file into a table, use the function `read_json`.    ## Example 2: Record-based features    The table definition where we load data has no column definitions. However, we can easily add them. A typical use case is where we want to change the format or data type of some columns. For example, if the source file has a text field with a time stamp then we might want to convert it the `datetime` object which is done by defining a new column:    ```json  {      ""id"": ""Source table"",      ""function"": ""pandas:read_csv"",      ""inputs"": [],      ""model"": {          ""filepath_or_buffer"": ""my_file.csv""      },      ""columns"": [        {            ""id"": ""Datetime"",            ""function"": ""pandas.core.tools.datetimes:to_datetime"",            ""scope"": ""one"",            ""inputs"": ""Date""        }      ]  }  ```    The most important parameter in this column definition is `scope`. If it is `one` (or `1`) then the function will be applied to each row of the table. In other words, this function will get *one* row as its first argument. After evaluating this column definition, the table will get a new column `Datetime` storing time stamp objects (which are more convenient for further data transformations).     If we do not need the source (string) column then the new column may get the same name `""id"": ""Date""` and it will overwrite the already existing column. Also, if the source column has a non-standard format then it can be specified in the model `""model"": {""format"": ""%Y-%m-%d""}` which will be passed to the function.    ```json  {      ""id"": ""Datetime"",      ""function"": ""pandas.core.tools.datetimes:to_datetime"",      ""scope"": ""one"",      ""inputs"": ""Date"",      ""model"": {""format"": ""%Y-%m-%d""}  }  ```    If some source or intermediate columns are not needed for later analysis then they can be excluded by adding a column filter to the table definition where we can specify columns to retain as a list like `""column_filter"": [""Open"", ""High"", ""Low"", ""Close"", ""Volume""]` or to exclude like `""column_filter"": {""exclude"": [""Adj Close""]}`.    Execute this workflow as follows:    ```console  $ lambdo examples/example2.json  ```    ## Example 3: User-defined record-based features    Let us now assume that we want to analyze the difference between high and low daily prices and hence we need to derive such a column from two input columns `High` and `Low`. There is no such a standard function and hence we need to define our own domain-specific function which will return the derived value given some input values. This user-defined function is defined in a Python source file:    ```python  def diff_fn(X):      """"""      Difference between first and second fields of the input Series.      """"""      if len(X) < 2: return None      if not X[0] or not X[1]: return None      if pd.isna(X[0]) or pd.isna(X[1]): return None      return X[0] - X[1]  ```    This function will get a Series object for each row of the input table. For each pair of numbers it will return their difference.    In order for the workflow to load this function definition, we need to specify its location in the workflow:    ```json  {    ""id"": ""Example 3"",    ""imports"": [""examples.example3""],  }  ```    The column definition, which uses this function is defined as follows:    ```json  {    ""id"": ""diff_high_low"",    ""function"": ""examples.example3:diff_fn"",    ""inputs"": [""High"", ""Low""]  }  ```    We specified two columns which have to be passed as parameters to the user-defined functions: `""inputs"": [""High"", ""Low""]`. The same function could be also applied to other column where we want to find difference. This function will be called for each row of the table and its return values will be stored in the new column.    Each function including this one can accept additional arguments via its `model` (similar to how we passed data format in the previous example).    ## Example 4: Table-based features    A record-based function with scope 1 will be applied to each row of the table and get this row fields in arguments. There will be as many calls as there are rows in the table. If `scope` is equal to `all` then the function will be called only one time and it will get all rows it has to process. Earlier, we described how string dates can be converted to datetime object by applying the transformation function to each row. The same result can be obtained if we pass the whole column to the transformation function. The only field that has to be changed in this definition is `scope`, which is now equals `all`:    ```json  {    ""id"": ""Datetime"",    ""function"": ""pandas.core.tools.datetimes:to_datetime"",    ""scope"": ""all"",    ""inputs"": ""Date"",    ""model"": {""format"": ""%Y-%m-%d""}  }  ```    The result will be the same but this column will be evaluated faster.    Such functions which get all rows have to know how to iterate over the rows and they return one column rather than a single value. Such functions can apply any kind of computations because they have the whole data set. Therefore, such functions are used for more complex transformations including forecasts using some model.    Another example of applying a function to all rows is shifting a column. For example, if our goal is forecasting the closing price tomorrow then we need shift this column one step backwards:    ```json  {    ""id"": ""Close_Tomorrow"",    ""function"": ""pandas.core.series:Series.shift"",    ""scope"": ""all"",    ""inputs"": [""Close""],    ""model"": {""periods"": -1}  }  ```    Values of this new column will be equal to the value of the specified input column taken from the next record.    ## Example 5: Window-based rolling aggregation    Lambdo is focused on time series analysis where important pieces of behavior (features) are hidden in sequences of events. Therefore, one of the main goals of feature engineering is making such features explicitly as attribute values by extracting data from the history. Normally it is done by using rolling aggregation where a function is applied to some historic window of recent events and returns one value, which characterizes the behavior. In Lambdo, it is possible to specify an arbitrary (Python) function, which encodes some domain-specific logic specific for this feature.    For window-based columns, the most important parameter is `scope` which is an integer specifying the number of events to be passed to the function as its first argument. For example, if `""scope"": 10` then the Python function will always get 10 elements of the time series: this element and 9 previous elements. It can be a series of 10 values or a sub-table with 10 rows depending on other parameters. The function then analyzes these 10 events and returns one single value, which is stored as a value of this derived column.    Assume that we want to find running average volume for 10 days. This can be done as follows:    ```json  {    ""id"": ""mean_Volume"",    ""function"": ""numpy.core.fromnumeric:mean"",    ""scope"": 10,    ""inputs"": [""Volume""]  }  ```    Each value of the derived column `mean_Volume` will store average volume for the last 10 days.     Note that instead of `mean` we could use arbitrary Python function including user-defined functions. Such a function will be called for each row in the table and it will get 10 values of the volume for the last 10 days (including this one). For example, we could write a function which counts the number of peaks (local maximums) in volume or we could find some more complex pattern. Also, if `inputs` has more columns then the functions will get a data frame as input with the columns specified in `inputs`.    Typically in time series analysis we use several running aggregations with different window sizes. Such columns can can be defined independently but their definitions will differ only in one parameter: `scope`. In order to simplify such definitions Lambdo allows for defining a base definition and extensions. For example, if we want to define average volumes with windows 10, 5 and 2 then this can be done by definition scopes in the extensions:    ```json  {    ""id"": ""mean_Volume"",    ""function"": ""numpy.core.fromnumeric:mean"",    ""inputs"": [""Volume""],    ""extensions"": [      {""scope"": ""10""},      {""scope"": ""5""},      {""scope"": ""2""}    ]  }  ```    The number of columns defined is equal to the number of extensions, that is, three columns in this examples. The names of the columns by default will be `id` of this family definition and the suffix `_N` where `N` is an index of the extension. In our example, three columns will be added after evaluating this definition: `mean_Volume_0`, `mean_Volume_1` and `mean_Volume_2`.    Moving averages can produce empty values, which we want to exclude from analysis, for example, because other analysis algorithms are not able to process them. Each table definition allows for filtering its records at the end before the table data is passed to the next node. In order to exclude all rows with empty values we add this block to the end of the table definition:    ```json  ""row_filter"": {""dropna"": true}  ```    Run this example and check out its result which will contain three new columns with moving averages of the volume:    ```console  $ lambdo examples/example5.json  ```    ## Example 6: Training a model    All previous examples assumed that a column definition is treated as a data transformation performed via a Python function which also takes parameters of this transformation, which is called a model. The model describes how specifically the transformation has to be performed. One of the main features of Lambdo is that it treats such transformations as applying a data mining model. In other words, the result of applying a data mining model is a new column. For example, this column could store the cluster number this row belongs to or likelihood this object (row) is some object. What is specific to data mining is that its models are not specified explicitly but rather are trained from the data. This possibility to train a model (as opposed to providing an explicit model) is provided by Lambdo for any kind of column definition. If a model is absent and the training function is provided, then the model will be trained before it is applied to the data.    How a model has to be train is specified in a workflow using the `train` block of a column definition:    ```json  ""columns"": [    {      ""id"": ""My Column"",      ""function"": ""my_transform_function"",        ""train"": {        ""function"": ""my_train_function"",        ""model"": {""hyper_param"": 123}      }    }  ]  ```    This column definition does not have a model specified but it does specify a function for generating (training) such a model. It also provides a hyper-model for this training function which specifies how to do training. The training function gets the data and the hyper-model in its arguments and returns a trained model which is then used for generating the column data.    Here is an example of a column definition which trains and applies a gradient boosting data mining model:    ```json  {    ""id"": ""Close_Tomorrow_Predict"",    ""function"": ""examples.example6:gb_predict"",    ""scope"": ""all"",    ""data_type"": ""ndarray"",    ""inputs"": {""exclude"": [""Date"", ""Close_Tomorrow""]},    ""train"": {      ""function"": ""examples.example6:gb_fit"",      ""row_filter"": {""slice"": {""end"": 900}},      ""model"": {""n_estimators"": 500, ""max_depth"": 4, ""min_samples_split"": 2, ""learning_rate"": 0.01, ""loss"": ""ls""},      ""outputs"": [""Close_Tomorrow""]    }  }  ```    This definition has the following new elements:    * `data_type` field indicates that the functions accepts `ndarray` and not `DataFrame`.  * `inputs` field allows us to select columns we want to use. We want to exclude the `Date` column because its data type is not supported and `Close_Tomorrow` column which is a goal  * `row_filter` is used to limit the number of records for training  * `model` in the training section provides hyper-parameters for the training function  * `outputs` field specifies labels for the training procedure    Thus in this definition we want to use 900 records and all columns except for `Date` for training by using `Close_Tomorrow` as labels. The resulted model is then applied to *all* the data and the predictions are stored as the `Close_Tomorrow_Predict` column.    Run this example and check out its results in the last column with predictions:    ```console  $ lambdo examples/example6.json  ```    ## Example 7: Reading and writing models    If a column (feature definition) model is not specified then Lambdo will try to generate it by using the train function. This trained model will be then applied to the input data. However, after finishing executing the workflow, this model will be lost. In many scenarios we would like to retain some trained models. In particular, it is necessary if we explicitly separate two phases: training and predicting. The goal of the training phase is to generate a prediction model by using some (typically large amount of) input data. The model resulted from the training phase can be then used for prediction (by this same workflow because we want to have the same features). Therefore, we do not want to train it again but rather load it from the location where it was stored during training.    The mechanism of storing and loading models is implemented by Lambdo as one of its main features. The idea is that workflow field values can specified either by-value or by-reference. Specifying a value by-value means that we simply provide a JSON object for the corresponding JSON field. However, we can also point to values by providing a reference which will be used by the system for reading or writing it.    The general rule is that if a JSON field value is a string which starts from `$` sign then it is a reference. If we want to store some model in a file (and not directly in the workflow) then the location is specified as follows:    ```json  ""model"": ""$file:my_model.pkl""  ```    Now Lambdo will try to load this model from the file. If it succeeds then the model will be used for transformation (no training needed). If it fails, for example, the file does not exist, then Lambdo will generate this model by using the training function, store the model in the file and then use it for generating the column as usual.    Example 7 has one small modification with respect to Example 6: its trained model is stored in a file. As a result, we can apply this workflow to a large data set for training, and then this same workflow with the present model can be applied to smaller data sets for prediction.    ## Example 8: Joining input tables    TBD    # How to install    ## Install from source code    Check out the source code and execute this command in the project directory (where `setup.py` is located):    ```console  $ pip install .  ```    Or alternatively:    ```console  $ python setup.py install  ```    ## Install from package    Create wheel package:    ```console  $ python setup.py bdist_wheel  ```    The `whl` package will be stored in the `dist` folder and can then be installed using `pip`.    Execute this command by specifying the `whl` file as a parameter:    ```console  $ pip install dist\lambdo-0.1.0-py3-none-any.whl  ```    # How to test    Run tests:    ```console  $ python -m unittest discover -s ./tests  ```    or    ```console  $ python setup.py test  ```    # How to use    If you execute `lambdo` without any options then it will return a short help by describing its usage.    A workflow file is needed to analyze data. Very simple workflows for test purposes can be found in the `tests` directory. More complex workflows can be found in the `examples` directory. To execute a workflow start `lambdo` with this workflow file name as a parameter:    ```console  $ lambdo examples/example1.json  ```    The workflow reads a CSV file, computes some features from the time series data, trains a model by applying it them to the data and finally writes the result to an output CSV file. """
Big data;https://github.com/sonalgoyal/hiho;"""# HIHO: Hadoop In, Hadoop Out.     > Hadoop Data Integration, deduplication, incremental update and more.      This branch is for support for HIHO on Apache Hadoop 0.21.    ## Import from a database to HDFS    **query based import**      Join multiple tables, provide where conditions, dynamically bind parameters to SQL queries to get data to Hadoop. As simple as creating a simple config and running the job.    	bin/hadoop jar hiho.jar co.nubetech.hiho.job.DBQueryInputJob -conf dbInputQueryDelimited.xml    or     	${HIHO_HOME}/scripts/hiho import   		-jdbcDriver <jdbcDriver>   		-jdbcUrl <jdbcUrl>   		-jdbcUsername <jdbcUsername>   		-jdbcPassword <jdbcPassword>   		-inputQuery <inputQuery>   		-inputBoundingQuery <inputBoundingQuery>   		-outputPath <outputPath>   		-outputStrategy <outputStrategy>   		-delimiter <delimiter>   		-numberOfMappers <numberOfMappers>   		-inputOrderBy <inputOrderBy>      **table based import**      	bin/hadoop jar hiho.jar co.nubetech.hiho.job.DBQueryInputJob -conf dbInputTableDelimited.xml    or      	${HIHO_HOME}/scripts/hiho import   		-jdbcDriver <jdbcDriver>   		-jdbcUrl <jdbcUrl>   		-jdbcUsername <jdbcUsername>   		-jdbcPassword <jdbcPassword>  		-outputPath <outputPath>   		-outputStrategy <outputStrategy>   		-delimiter <delimiter>   		-numberOfMappers <numberOfMappers>   		-inputOrderBy <inputOrderBy>   		-inputTableName <inputTableName>   		-inputFieldNames <inputFieldNames>    **incremental import** by appending to existing `HDFS` location so that all data is in one place.  just specify `isAppend = true` in the configurations and import. Import will be written to existing HDFS folder.    **configurable format for data import: delimited, avro** by specifying the `mapreduce.jdbc.hiho.input.outputStrategy` as DELIMITED or AVRO.     **Note:**     1. Please specify delimiter in double qoutes, because in some cases such as semi colon ';' it breaks For example `-delimiter "";""`. If you are specifing * for `inputFieldNames` then also you put in double qoutes      ## Export to Databases    **high performance MySQL loading using LOAD DATA INFILE**    	${HIHO_HOME}/scripts/hiho export mysql  		-inputPath <inputPath>   		-url <url>   		-userName <userName>   		-password <password>   		-querySuffix  <querySuffix>    **high performance Oracle loading by creating external tables.** See [expert opinion](http://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:6611962171229)    For information on external tables, check [here](http://download.oracle.com/docs/cd/B12037_01/server.101/b10825/et_concepts.htm)    On the Oracle server    1. Make a folder    		mkdir -p ageTest    2. Create a directory through the Oracle Client (sqlplus) and grant it privileges.    		sqlplus>create or replace directory age_ext as '/home/nube/age';    3. Allow ftp to the Oracle server  	  		${HIHO_HOME}/scripts/hiho export oracle   			-inputPath <inputPath>   			-oracleFtpAddress <oracleFtpAddress>   			-oracleFtpPortNumber <oracleFtpPortNumber>   			-oracleFtpUserName <oracleFtpUserName>   			-oracleFtpPassword <oracleFtpPassword>   			-oracleExternalTableDirectory <oracleExternalTableDirectory>   			-driver <driver>   			-url <url>   			-userName <userName>   			-password <password>   			-externalTable <createExternalTableQuery>    **custom loading and export to any database** by emitting own `GenericDBWritables`. Check `DelimitedLoadMapper`    ## Export to SalesForce      **send computed map reduce results to Salesforce.**    For this, you need to have a developer account with Bulk API enabled. You can join at http://developer.force.com/join     If you get message:     >[LoginFault [ApiFault  exceptionCode='INVALID_LOGIN' exceptionMessage='Invalid username, password, security token; or user locked out.'  ""Invalid username, password, security token; or user locked out. Are you at a new location? When accessing Salesforce--either via a desktop client or the API--from outside of your companyâ€™s trusted networks, you must add a security token to your password to log in. To receive a new security token, log in to [Salesforce](http://www.salesforce.com) and click Setup | My Personal Information | Reset Security Token.""  login and get the security token.     then try    	sfUserName - Name of Salesforce account  	sfPassword - Password and security token. The Security Token can be obtained by logging in to the Salesforce.com site and clicking on Reset Security Token.  	sfObjectType - The Salesforce object to export  	sfHeaders - header describing the Salesforce object properties. For more information, refer to the Bulk API Developer's Guide.    	${HIHO_HOME}/scripts/hiho export saleforce   		-inputPath <inputPath>   		-sfUserName <sfUserName>   		-sfPassword <sfPassword>   		-sfObjectType <sfObjectType>   		-sfHeaders <sfHeaders>      ## Export results to an FTP Server.    Use the `co.nubetech.hiho.mapreduce.lib.output.FTPOutputFormat` directly in your job, just like `FileOutputFormat`. For usage, check `co.nubetech.hiho.job.ExportToFTPserver`. This job writes the output directly to an FTP server.   It can be invoked as:    	${HIHO_HOME}/scripts/hiho export ftp   		-inputPath <inputPath>   		-outputPath <outputPath>   		-ftpUserName <ftpUserName>   		-ftpAddress <ftpAddress>   		-ftpPortNumper <ftpPortNumper>   		-ftpPassword <ftpPassword>    Where:    	ftpUserName - FTP server login username  	ftpAddress - FTP server address  	ftpPortNumper - FTP port  	ftpPassword - FTP server password  	outputPath is the location on the FTP server to which the output will be written. It should be a complete directory path - /home/sgoyal/output        ## Export to Hive  This is used to export data from any other database to Hive database.   Hive export can be done in two method query base and table based configuration needed are     	mapreduce.jdbc.hiho.input.loadTo - this configuration defines in which database you want to load your data from HDFS for eg:- hive  	mapreduce.jdbc.hiho.input.loadToPath - Our program also generates script for all queries , this configuration defines where to store that script on your local system    	mapreduce.jdbc.hiho.hive.driver - name of hive jdbc driver For eg:- org.apache.hadoop.hive.jdbc.HiveDriver  	mapreduce.jdbc.hiho.hive.url - hive url for jdbc connection For eg:- jdbc:hive:// (for embedded mode),jdbc:hive://localhost:10000/default for standalone mode  	mapreduce.jdbc.hiho.hive.usrName - user name for jdbc connection   	mapreduce.jdbc.hiho.hive.password - password for jdbc connection  	mapreduce.jdbc.hiho.hive.partitionBy - This configuration is when we want to create partitioned hive table. For eg :- country:string:us;name:string:jack (basic partition), country:string:us;name:string (static and one dynamic partition), country:string (dynamic partition) till now we allow only one dynamic partition  											We also allow to store data in a table for multiple partition at a time for that value is given as country:string:us,uk,aus for this we need to define three different queries or table in there respective configurations   	mapreduce.jdbc.hiho.hive.ifNotExists - set true if you want include 'if not exits' clause in your create table query  	mapreduce.jdbc.hiho.hive.tableName - write the name for the table in the hive you want to create  	mapreduce.jdbc.hiho.hive.sortedBy - this can be only used if clusteredBy configuration is defined, in this give the name of column by which u want to sort your data  	mapreduce.jdbc.hiho.hive.clusteredBy - This configuration defines name of column by which you want to cluster your data and define the number of buckets you want to create. For eg:- name:2     Execution command for table based     	bin/hadoop jar ~/workspace/hiho/build/classes/hiho.jar co.nubetech.hiho.job.DBQueryInputJob -conf  ~/workspace/hiho/conf/dbInputTableDelimitedHive.xml    or    	${HIHO_HOME}/scripts/hiho import   		-jdbcDriver <jdbcDriver>   		-jdbcUrl <jdbcUrl>   		-jdbcUsername <jdbcUsername>   		-jdbcPassword <jdbcPassword>   		-outputPath <outputPath>   		-outputStrategy <outputStrategy>   		-delimiter <delimiter>   		-numberOfMappers <numberOfMappers>   		-inputOrderBy <inputOrderBy>   		-inputTableName <inputTableName>   		-inputFieldNames <inputFieldNames>   		-inputLoadTo hive   		-inputLoadToPath <inputLoadToPath>   		-hiveDriver <hiveDriver>    		-hiveUrl <hiveUrl>   		-hiveUsername <hiveUsername>   		-hivePassword <hivePassword>   		-hivePartitionBy <hivePartitionBy>   		-hiveIfNotExists <hiveIfNotExists>   		-hiveTableName <hiveTableName>   		-hiveSortedBy <hiveSortedBy>   		-hiveClusteredBy <hiveClusteredBy>      For query based    	bin/hadoop jar ~/workspace/hiho/build/classes/hiho.jar co.nubetech.hiho.job.DBQueryInputJob -conf  ~/workspace/hiho/conf/dbInputQueryDelimitedHive.xml  or    	${HIHO_HOME}/scripts/hiho import   		-jdbcDriver <jdbcDriver>   		-jdbcUrl <jdbcUrl>   		-jdbcUsername <jdbcUsername>   		-jdbcPassword <jdbcPassword>   		-outputPath <outputPath>   		-outputStrategy <outputStrategy>   		-delimiter <delimiter>   		-numberOfMappers <numberOfMappers>   		-inputOrderBy <inputOrderBy>   		-inputLoadTo hive   		-inputLoadToPath <inputLoadToPath>   		-hiveDriver <hiveDriver>    		-hiveUrl <hiveUrl>   		-hiveUsername <hiveUsername>   		-hivePassword <hivePassword>   		-hivePartitionBy <hivePartitionBy>   		-hiveIfNotExists <hiveIfNotExists>   		-hiveTableName <hiveTableName>   		-hiveSortedBy <hiveSortedBy>   		-hiveClusteredBy <hiveClusteredBy>    **Notes:**      1. Hive table name is mandatory when you are quering more than one query or table that is the case of multiple partition  2. Please note that sorted feature will not work untill clustered feature is defined      ## Dedup details    	bin/hadoop jar ~/workspace/HIHO/deploy/hiho.jar co.nubetech.hiho.dedup.DedupJob -inputFormat <inputFormat> -dedupBy <""key"" or ""value""> -inputKeyClassName <inputKeyClassName> -inputValueClassName <inputValueClassName> -inputPath <inputPath> -outputPath <outputPath> -delimeter <delimeter> -column <column> -outputFormat <outputFormat>    Alternatively Dedup can also be executed as:-  Running HadoopTransform script present in `$HIHO_HOME/scripts/`    	${HIHO_HOME}/scripts/hiho dedup   		-inputFormat <inputFormat>   		-dedupBy <""key"" or ""value"">   		-inputKeyClassName <inputKeyClassName>   		-inputValueClassName <inputValueClassName>   		-inputPath <inputPath>   		-outputPath <outputPath>   		-delimeter <delimeter> -column <column>    **Example For Deduplication with key:**      For Sequence Files:     	${HIHO_HOME}/scripts/hiho dedup   		-inputFormat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat   		-inputKeyClassName org.apache.hadoop.io.IntWritable   		-inputValueClassName org.apache.hadoop.io.Text   		-inputPath testData/dedup/inputForSeqTest   		-outputPath output -dedupBy key    For Delimited Text Files:     	${HIHO_HOME}/scripts/hiho dedup   		-inputFormat co.nubetech.hiho.dedup.DelimitedTextInputFormat   		-inputKeyClassName org.apache.hadoop.io.Text   		-inputValueClassName org.apache.hadoop.io.Text   		-inputPath testData/dedup/textFilesForTest   		-outputPath output -delimeter ,   		-column 1   		-dedupBy key    **Example For Deduplication with value:**      For Sequence Files:     	${HIHO_HOME}/scripts/hiho dedup   		-inputFormat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat   		-inputKeyClassName org.apache.hadoop.io.IntWritable   		-inputValueClassName org.apache.hadoop.io.Text   		-inputPath testData/dedup/inputForSeqTest   		-outputPath output -dedupBy value    For Delimited Text Files:     	${HIHO_HOME}/scripts/hiho dedup   		-inputFormat co.nubetech.hiho.dedup.DelimitedTextInputFormat   		-inputKeyClassName org.apache.hadoop.io.Text   		-inputValueClassName org.apache.hadoop.io.Text   		-inputPath testData/dedup/textFilesForTest   		-outputPath output   		-dedupBy value    ## Merge details:     	${HIHO_HOME}/scripts/hiho merge   		-newPath <newPath>   		-oldPath <oldPath>   		-mergeBy <""key"" or ""value"">   		-outputPath <outputPath>   		-inputFormat <inputFormat>   		-inputKeyClassName <inputKeyClassName>   		-inputValueClassName <inputValueClassName>   		-outputFormat <outputFormat>    **Example For Merge with key:**      For Sequence Files:    	${HIHO_HOME}/scripts/hiho merge   		-newPath testData/merge/inputNew/input1.seq   		-oldPath testData/merge/inputOld/input2.seq   		-mergeBy key -outputPath output    		-inputFormat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat   		-inputKeyClassName org.apache.hadoop.io.IntWritable   		-inputValueClassName org.apache.hadoop.io.Text    For Delimited Text Files:    	${HIHO_HOME}/scripts/hiho merge   		-newPath testData/merge/inputNew/fileInNewPath.txt   		-oldPath testData/merge/inputOld/fileInOldPath.txt   		-mergeBy key   		-outputPath output   		-inputFormat co.nubetech.hiho.dedup.DelimitedTextInputFormat   		-inputKeyClassName org.apache.hadoop.io.Text   		-inputValueClassName org.apache.hadoop.io.Text    **Example For Merge with value:**      For Sequence Files:    	${HIHO_HOME}/scripts/hiho merge   		-newPath testData/merge/inputNew/input1.seq   		-oldPath testData/merge/inputOld/input2.seq   		-mergeBy value   		-outputPath output    		-inputFormat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat   		-inputKeyClassName org.apache.hadoop.io.IntWritable   		-inputValueClassName org.apache.hadoop.io.Text    For Delimited Text Files:    	${HIHO_HOME}/scripts/hiho merge   		-newPath testData/merge/inputNew/fileInNewPath.txt   		-oldPath testData/merge/inputOld/fileInOldPath.txt   		-mergeBy value   		-outputPath output   		-inputFormat co.nubetech.hiho.dedup.DelimitedTextInputFormat   		-inputKeyClassName org.apache.hadoop.io.Text   		-inputValueClassName org.apache.hadoop.io.Text    ## Export to DB:    	bin/hadoop jar deploy/hiho-0.4.0.jar co.nubetech.hiho.job.ExportToDB    		-jdbcDriver <jdbcDriverName>    		-jdbcUrl <jdbcUrl>    		-jdbcUsername <jdbcUserName>    		-jdbcPassword <jdbcPassword>   		-delimiter <delimiter>   		-numberOfMappers <numberOfMappers>   		-tableName <tableName>   		-columnNames <columnNames>   		-inputPath <inputPath>   or    	${HIHO_HOME}/scripts/hiho export db   		-jdbcDriver <jdbcDriverName>    		-jdbcUrl <jdbcUrl>    		-jdbcUsername <jdbcUserName>    		-jdbcPassword <jdbcPassword>   		-delimiter <delimiter>   		-numberOfMappers <numberOfMappers>   		-tableName <tableName>   		-columnNames <columnNames>   		-inputPath <inputPath>     ## New Features in this release  - incremental import and introduction of AppendFileInputFormat  - Oracle export  - FTP Server integration  - Salesforce  - Support for Apache Hadoop 0.20  - Support for Apache Hadoop 0.21  - Generic dedup and merge    ## Other improvements  - Ivy based build and dependency management  - Junit and mockito based test cases     **Note:**     1. To run `TestExportToMySQLDB` we need to add `hiho-0.4.0.jar`, all jars of hadoop and hadoop lib with also `mysql-connector-java.jar`  		in the `classpath`. """
Big data;https://github.com/jhuckaby/Cronicle;"""# Important Upgrade Note!    For those upgrading multi-server clusters from Cronicle v0.8 to v0.9, you must upgrade **all** of your servers for them to be able to communicate. In v0.9 we have upgraded to socket.io v4.4, which is incompatible with socket.io v1.7.3 that previous Cronicle versions used. We had to upgrade this dependency due to high severity vulnerabilities. Since this is a breaking API change for them, you cannot run ""mixed"" server clusters with some servers on Cronicle v0.8 and others on v0.9. They all have to be on v0.8 or they all have to be on v0.9.    So, it is recommended that you first disable the main scheduler on your master server (checkbox in top-right corner of the UI), wait for all jobs to complete, then shut down all servers and upgrade them all together. Then start them all up again, and finally re-enable the scheduler.    # Overview    **Cronicle** is a multi-server task scheduler and runner, with a web based front-end UI.  It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer.  It's basically a fancy [Cron](https://en.wikipedia.org/wiki/Cron) replacement written in [Node.js](https://nodejs.org/).  You can give it simple shell commands, or write Plugins in virtually any language.    ![Main Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/job-details-complete.png)    ## Features at a Glance    * Single or multi-server setup.  * Automated failover to backup servers.  * Auto-discovery of nearby servers.  * Real-time job status with live log viewer.  * Plugins can be written in any language.  * Schedule events in multiple timezones.  * Optionally queue up long-running events.  * Track CPU and memory usage for each job.  * Historical stats with performance graphs.  * Simple JSON messaging system for Plugins.  * Web hooks for external notification systems.  * Simple REST API for scheduling and running events.  * API Keys for authenticating remote apps.    ## Table of Contents    <details><summary>Table of Contents</summary>    <!-- toc -->  * [Glossary](#glossary)  - [Installation](#installation)  - [Setup](#setup)  	* [Single Server](#single-server)  	* [Single Primary with Workers](#single-primary-with-workers)  	* [Multi-Server Cluster](#multi-server-cluster)  		+ [Load Balancers](#load-balancers)  		+ [Ops Notes](#ops-notes)  - [Configuration](#configuration)  	* [Basics](#basics)  		+ [base_app_url](#base_app_url)  		+ [email_from](#email_from)  		+ [smtp_hostname](#smtp_hostname)  		+ [smtp_port](#smtp_port)  		+ [mail_options](#mail_options)  		+ [secret_key](#secret_key)  		+ [log_dir](#log_dir)  		+ [log_filename](#log_filename)  		+ [log_columns](#log_columns)  		+ [log_archive_path](#log_archive_path)  		+ [copy_job_logs_to](#copy_job_logs_to)  		+ [queue_dir](#queue_dir)  		+ [pid_file](#pid_file)  		+ [debug_level](#debug_level)  		+ [maintenance](#maintenance)  		+ [list_row_max](#list_row_max)  		+ [job_data_expire_days](#job_data_expire_days)  		+ [child_kill_timeout](#child_kill_timeout)  		+ [dead_job_timeout](#dead_job_timeout)  		+ [master_ping_freq](#master_ping_freq)  		+ [master_ping_timeout](#master_ping_timeout)  		+ [udp_broadcast_port](#udp_broadcast_port)  		+ [scheduler_startup_grace](#scheduler_startup_grace)  		+ [universal_web_hook](#universal_web_hook)  		+ [web_hook_custom_data](#web_hook_custom_data)  		+ [web_hook_custom_opts](#web_hook_custom_opts)  		+ [web_hook_text_templates](#web_hook_text_templates)  		+ [ssl_cert_bypass](#ssl_cert_bypass)  		+ [job_memory_max](#job_memory_max)  		+ [job_memory_sustain](#job_memory_sustain)  		+ [job_cpu_max](#job_cpu_max)  		+ [job_cpu_sustain](#job_cpu_sustain)  		+ [job_log_max_size](#job_log_max_size)  		+ [job_env](#job_env)  		+ [server_comm_use_hostnames](#server_comm_use_hostnames)  		+ [web_direct_connect](#web_direct_connect)  		+ [web_socket_use_hostnames](#web_socket_use_hostnames)  		+ [socket_io_transports](#socket_io_transports)  	* [Storage Configuration](#storage-configuration)  		+ [Filesystem](#filesystem)  		+ [Couchbase](#couchbase)  		+ [Amazon S3](#amazon-s3)  	* [Web Server Configuration](#web-server-configuration)  	* [User Configuration](#user-configuration)  	* [Email Configuration](#email-configuration)  - [Web UI](#web-ui)  	* [Home Tab](#home-tab)  		+ [General Stats](#general-stats)  		+ [Active Jobs](#active-jobs)  		+ [Upcoming Events](#upcoming-events)  	* [Schedule Tab](#schedule-tab)  		+ [Edit Event Tab](#edit-event-tab)  			- [Event ID](#event-id)  			- [Event Name](#event-name)  			- [Event Enabled](#event-enabled)  			- [Event Category](#event-category)  			- [Event Target](#event-target)  				* [Algorithm](#algorithm)  				* [Multiplexing](#multiplexing)  			- [Event Plugin](#event-plugin)  			- [Event Timing](#event-timing)  			- [Event Concurrency](#event-concurrency)  			- [Event Timeout](#event-timeout)  			- [Event Retries](#event-retries)  			- [Event Options](#event-options)  				* [Run All Mode](#run-all-mode)  				* [Detached Mode](#detached-mode)  				* [Allow Queued Jobs](#allow-queued-jobs)  				* [Chain Reaction](#chain-reaction)  			- [Event Time Machine](#event-time-machine)  			- [Event Notification](#event-notification)  				* [Event Web Hook](#event-web-hook)  			- [Event Resource Limits](#event-resource-limits)  			- [Event Notes](#event-notes)  			- [Run Now](#run-now)  	* [Completed Jobs Tab](#completed-jobs-tab)  		+ [Event History Tab](#event-history-tab)  		+ [Event Stats Tab](#event-stats-tab)  	* [Job Details Tab](#job-details-tab)  	* [My Account Tab](#my-account-tab)  	* [Administration Tab](#administration-tab)  		+ [Activity Log Tab](#activity-log-tab)  		+ [API Keys Tab](#api-keys-tab)  		+ [Categories Tab](#categories-tab)  		+ [Plugins Tab](#plugins-tab)  			- [Plugin Parameters](#plugin-parameters)  			- [Advanced Plugin Options](#advanced-plugin-options)  		+ [Servers Tab](#servers-tab)  			- [Server Groups](#server-groups)  		+ [Users Tab](#users-tab)  - [Plugins](#plugins)  	* [Writing Plugins](#writing-plugins)  		+ [JSON Input](#json-input)  		+ [JSON Output](#json-output)  			- [Reporting Progress](#reporting-progress)  			- [Performance Metrics](#performance-metrics)  				* [Nested Metrics](#nested-metrics)  			- [Changing Notification Settings](#changing-notification-settings)  			- [Chain Reaction Control](#chain-reaction-control)  				* [Chain Data](#chain-data)  			- [Custom Data Tables](#custom-data-tables)  			- [Custom HTML Content](#custom-html-content)  			- [Updating The Event](#updating-the-event)  		+ [Job Environment Variables](#job-environment-variables)  	* [Sample Node Plugin](#sample-node-plugin)  	* [Sample Perl Plugin](#sample-perl-plugin)  	* [Sample PHP Plugin](#sample-php-plugin)  	* [Built-in Shell Plugin](#built-in-shell-plugin)  	* [Built-in HTTP Request Plugin](#built-in-http-request-plugin)  		+ [HTTP Request Chaining](#http-request-chaining)  - [Command Line](#command-line)  	* [Starting and Stopping](#starting-and-stopping)  	* [Environment Variables](#environment-variables)  	* [Storage Maintenance](#storage-maintenance)  	* [Recover Admin Access](#recover-admin-access)  	* [Server Startup](#server-startup)  	* [Upgrading Cronicle](#upgrading-cronicle)  	* [Data Import and Export](#data-import-and-export)  	* [Storage Migration Tool](#storage-migration-tool)  - [Inner Workings](#inner-workings)  	* [Cron Noncompliance](#cron-noncompliance)  	* [Storage](#storage)  	* [Logs](#logs)  	* [Keeping Time](#keeping-time)  	* [Primary Server Failover](#primary-server-failover)  		+ [Unclean Shutdown](#unclean-shutdown)  - [API Reference](#api-reference)  	* [JSON REST API](#json-rest-api)  		+ [Redirects](#redirects)  	* [API Keys](#api-keys)  	* [Standard Response Format](#standard-response-format)  	* [API Calls](#api-calls)  		+ [get_schedule](#get_schedule)  		+ [get_event](#get_event)  		+ [create_event](#create_event)  		+ [update_event](#update_event)  		+ [delete_event](#delete_event)  		+ [run_event](#run_event)  		+ [get_job_status](#get_job_status)  		+ [get_active_jobs](#get_active_jobs)  		+ [update_job](#update_job)  		+ [abort_job](#abort_job)  	* [Event Data Format](#event-data-format)  		+ [Event Timing Object](#event-timing-object)  - [Development](#development)  	* [Installing Dev Tools](#installing-dev-tools)  	* [Manual Installation](#manual-installation)  	* [Starting in Debug Mode](#starting-in-debug-mode)  	* [Running Unit Tests](#running-unit-tests)  - [Companies Using Cronicle](#companies-using-cronicle)  - [Colophon](#colophon)  - [License](#license)    </details>    ## Glossary    A quick introduction to some common terms used in Cronicle:    | Term | Description |  |------|-------------|  | **Primary Server** | The primary server which keeps time and runs the scheduler, assigning jobs to other servers, and/or itself. |  | **Backup Server** | A worker server which will automatically become primary and take over duties if the current primary dies. |  | **Worker Server** | A server which sits idle until it is assigned jobs by the primary server. |  | **Server Group** | A named group of servers which can be targeted by events, and tagged as ""primary eligible"", or ""worker only"". |  | **API Key** | A special key that can be used by external apps to send API requests into Cronicle.  Remotely trigger jobs, etc. |  | **User** | A human user account, which has a username and a password.  Passwords are salted and hashed with [bcrypt](https://en.wikipedia.org/wiki/Bcrypt). |  | **Plugin** | Any executable script in any language, which runs a job and reads/writes JSON to communicate with Cronicle. |  | **Schedule** | The list of events, which are scheduled to run at particular times, on particular servers. |  | **Category** | Events can be assigned to categories which define defaults and optionally a color highlight in the UI. |  | **Event** | An entry in the schedule, which may run once or many times at any interval.  Each event points to a Plugin, and a server or group to run it. |  | **Job** | A running instance of an event.  If an event is set to run hourly, then a new job will be created every hour. |    # Installation    Please note that Cronicle currently only works on POSIX-compliant operating systems, which basically means Unix/Linux and OS X.  You'll also need to have [Node.js LTS](https://nodejs.org/en/download/) pre-installed on your server.  Please note that we **only support the Active LTS versions of Node.js**.  Cronicle may not work on the ""current"" release channel.  See [Node.js Releases](https://nodejs.org/en/about/releases/) for details.    Once you have Node.js LTS installed, type this as root:    ```  curl -s https://raw.githubusercontent.com/jhuckaby/Cronicle/master/bin/install.js | node  ```    This will install the latest stable release of Cronicle and all of its [dependencies](#colophon) under: `/opt/cronicle/`    If you'd rather install it manually (or something went wrong with the auto-installer), here are the raw commands:    ```  mkdir -p /opt/cronicle  cd /opt/cronicle  curl -L https://github.com/jhuckaby/Cronicle/archive/v1.0.0.tar.gz | tar zxvf - --strip-components 1  npm install  node bin/build.js dist  ```    Replace `v1.0.0` with the desired Cronicle version from the [release list](https://github.com/jhuckaby/Cronicle/releases), or `master` for the head revision (unstable).    # Setup    If this is your first time installing, please read the [Configuration](#configuration) section first.  You'll likely want to customize a few configuration parameters in the `/opt/cronicle/conf/config.json` file before proceeding.  At the very least, you should set these properties:    | Key | Description |  |-----|-------------|  | `base_app_url` | A fully-qualified URL to Cronicle on your server, including the `http_port` if non-standard.  This is used in e-mails to create self-referencing URLs. |  | `email_from` | The e-mail address to use as the ""From"" address when sending out notifications. |  | `smtp_hostname` | The hostname of your SMTP server, for sending mail.  This can be `127.0.0.1` or `localhost` if you have [sendmail](https://en.wikipedia.org/wiki/Sendmail) running locally. |  | `secret_key` | For multi-server setups (see below) all your servers must share the same secret key.  Any randomly generated string is fine. |  | `job_memory_max` | The default maximum memory limit for each job (can also be customized per event and per category). |  | `http_port` | The web server port number for the user interface.  Defaults to 3012. |    Now then, the only other decision you have to make is what to use as a storage back-end.  Cronicle can use local disk (easiest setup), [Couchbase](http://www.couchbase.com/nosql-databases/couchbase-server) or [Amazon S3](https://aws.amazon.com/s3/).  For single server installations, or even single primary with multiple workers, local disk is probably just fine, and this is the default setting.  But if you want to run a true multi-server cluster with automatic primary failover, please see [Multi-Server Cluster](#multi-server-cluster) for details.    With that out of the way, run the following script to initialize the storage system.  You only need to do this once, *and only on the primary server*.  Do not run this on any worker servers:    ```  /opt/cronicle/bin/control.sh setup  ```    Among other things, this creates an administrator user account you can use to login right away.  The username is `admin` and the password is `admin`.  It is recommended you change the password as soon as possible, for security purposes (or just create your own administrator account and delete `admin`).    At this point you should be able to start the service and access the web UI.  Enter this command:    ```  /opt/cronicle/bin/control.sh start  ```    Give it a minute to decide to become primary, then send your browser to the server on the correct port:    ```  http://YOUR_SERVER_HOSTNAME:3012/  ```    You only need to include the port number in the URL if you are using a non-standard HTTP port (see [Web Server Configuration](#web-server-configuration)).    See the [Web UI](#web-ui) section below for instructions on using the Cronicle web interface.    ## Single Server    For a single server installation, there is nothing more you need to do.  After installing the package, running the `bin/control.sh setup` script and starting the service, Cronicle should be 100% ready to go.  You can always add more servers later (see below).    ## Single Primary with Workers    The easiest multi-server Cronicle setup is a single ""primary"" server with one or more workers.  This means that one server is the scheduler, so it keeps track of time, and assigns jobs for execution.  Jobs may be assigned to any number of worker servers, and even the primary itself.  Worker servers simply sit idle and wait for jobs to be assigned by the primary server.  Workers never take over primary scheduling duties, even if the primary server goes down.    This is the simplest multi-server setup because the primary server can use local disk for all its storage.  Workers do not need access to the file storage.  This is the default configuration, so you don't have to change anything at all.  What it means is, all the scheduling data, event categories, user accounts, sessions, plugins, job logs and other data is stored as plain JSON files on local disk.  Cronicle can also be configured to use a NoSQL database such as [Couchbase](http://www.couchbase.com/nosql-databases/couchbase-server) or [Amazon S3](https://aws.amazon.com/s3/), but this is not required.    So by default, when you run the setup script above, the current server is placed into a ""Primary Group"", meaning it is the only server that is eligible to become primary.  If you then install Cronicle on additional servers, they will become workers only.  You can change all this from the UI, but please read the next section before running multiple primary backup servers.    When installing Cronicle onto worker servers, please do not run the `bin/control.sh setup` script.  Instead, simply copy over your `conf/config.json` file, and then start the service.    ## Multi-Server Cluster    Cronicle also has the ability to run with one or more ""backup"" servers, which can become primary if need be.  Failover is automatic, and the cluster negotiates who should be primary at any given time.  But in order for this to work, all the primary eligible servers need access to the same storage data.  This can be achieved in one of three ways:    * Use a shared filesystem such as [NFS](https://en.wikipedia.org/wiki/Network_File_System).  * Use a [Couchbase](http://www.couchbase.com/nosql-databases/couchbase-server) server.  * Use an [Amazon S3](https://aws.amazon.com/s3/) bucket.    See the [Storage Configuration](#storage-configuration) section below for details on these.    The other thing you'll need to do is make sure all your primary backup servers are in the appropriate server group.  By default, a single ""Primary Group"" is created which only contains your primary primary server.  Using the UI, you can simply change the hostname regular expression so it encompasses all your eligible servers, or you can just add additional groups that match each backup server.  More details can be found in the [Servers Tab](#servers-tab) section below.    ### Load Balancers    You can run Cronicle behind a load balancer, as long as you ensure that only the primary server and eligible backup servers are in the load balancer pool.  Do not include any worker-only servers, as they typically do not have access to the back-end storage system, and cannot serve up the UI.    You can then set the [base_app_url](#base_app_url) configuration parameter to point to the load balancer, instead of an individual server, and also use that hostname when loading the UI in your browser.    Note that Web UI needs to make API calls and open [WebSocket](https://en.wikipedia.org/wiki/WebSocket) connections to the primary server directly, so it needs to also be accessible directly via its hostname.    You must set the [web_direct_connect](#web_direct_connect) configuration property to `true`.  This ensures that the Web UI will make API and WebSocket connections directly to the primary server, rather than via the load balancer hostname.    ### Ops Notes    For teams setting up multi-server clusters, here are some operational concerns to keep in mind:    * All servers should have the same exact configuration file (`/opt/cronicle/conf/config.json`).  * All servers need to have correct clocks (timezones do not matter, clock sync does).  * Server auto-discovery happens via UDP broadcast on port 3014 (by default).  This is not required.  * The primary server will also open TCP [WebSocket](https://en.wikipedia.org/wiki/WebSocket) connections to each worker on the web server port.  * Each server in the cluster needs to have a fully-qualified hostname that resolves in DNS.  * The server hostnames determine priority of which server becomes primary (alphabetical sort).  * All servers need to have unique hostnames (very bad things will happen otherwise).  * All servers need to have at least one active IPv4 interface.  * For the ""live log"" feature in the UI to work, the user needs a network route to the server running the job, via its hostname.  * If you have to change any server IP addresses, they'll have to be removed and re-added to the cluster.  * See the [Cron Noncompliance](#cron-noncompliance) section for differences in how Cronicle schedules events, versus the Unix Cron standard.    # Configuration    The main Cronicle configuration file is in JSON format, and can be found here:    ```  /opt/cronicle/conf/config.json  ```    Please edit this file directly.  It will not be touched by any upgrades.  A pristine copy of the default configuration can always be found here: `/opt/cronicle/sample_conf/config.json`.    ## Basics    Here are descriptions of the top-level configuration parameters:    ### base_app_url    This should be set to a fully-qualified URL, pointing to your Cronicle server, including the HTTP port number if non-standard.  Do not include a trailing slash.  This is used in e-mails to create self-referencing URLs.  Example:    ```  http://local.cronicle.com:3012  ```    If you are running Cronicle behind a load balancer, this should be set to the load balanced virtual hostname.    ### email_from    The e-mail address to use as the ""From"" address when sending out notifications.  Most SMTP servers require this to be a valid address to accept mail.    ### smtp_hostname    The hostname of your SMTP server, for sending mail.  This can be set to `127.0.0.1` or `localhost` if you have [sendmail](https://en.wikipedia.org/wiki/Sendmail) running locally.    ### smtp_port    The port number to use when communicating with the SMTP server.  The default is `25`.    ### mail_options    Set specific mailer options, such as SMTP SSL and authentication, passed directly to [pixl-mail](https://www.npmjs.com/package/pixl-mail#options) (and then to [nodemailer](https://nodemailer.com/)).  Example:    ```js  ""mail_options"": {  	""secure"": true,  	""auth"": { ""user"": ""fsmith"", ""pass"": ""12345"" },  	""connectionTimeout"": 10000,  	""greetingTimeout"": 10000,  	""socketTimeout"": 10000  }  ```    The `connectionTimeout`, `greetingTimeout` and `socketTimeout` properties are all expressed in milliseconds.    You can also use `mail_options` to use local [sendmail](https://nodemailer.com/transports/sendmail/), if you have that configured on your server.  To do this, set the following properties, and tune as needed:    ```js  ""mail_options"": {  	""sendmail"": true,  	""newline"": ""unix"",  	""path"": ""/usr/sbin/sendmail""  }  ```    You can omit `smtp_hostname` and `smtp_port` if you are using sendmail.    ### secret_key    For multi-server setups, all your Cronicle servers need to share the same secret key.  Any randomly generated string is fine.    The install script will automatically set to this to a random string for you, but you are free to change it to anything you want.  Just make sure all your servers have the same shared secret key.  This is used to authenticate internal server-to-server API requests.    ### log_dir    The directory where logs will be written, before they are archived.  This can be a partial path, relative to the Cronicle base directory (`/opt/cronicle`) or a full path to a custom location.  It defaults to `logs` (i.e. `/opt/cronicle/logs`).    ### log_filename    The filename to use when writing logs.  You have three options here: a single combined log file for all logs, multiple log files for each component, or multiple log files for each category (debug, transaction, error).  See the [Logs](#logs) section below for details.    ### log_columns    This is an array of column IDs to log.  You are free to reorder or remove some of these, but do not change the names.  They are specific IDs that match up to log function calls in the code.  See the [Logs](#logs) section below for details.    ### log_archive_path    Every night at midnight (local server time), the logs can be archived (gzipped) to a separate location.  This parameter specifies the path, and the directory naming / filenaming convention of the archive files.  It can utilize date placeholders including `[yyyy]`, `[mm]` and `[dd]`.    This can be a partial path, relative to the Cronicle base directory (`/opt/cronicle`) or a full path to a custom location.  It defaults to `logs/archives/[yyyy]/[mm]/[dd]/[filename]-[yyyy]-[mm]-[dd].log.gz`.    ### copy_job_logs_to    Your job logs (i.e. the output from Plugins) are stored separately from the main Cronicle log files.  They are actually written to the Cronicle storage system, and made available in the UI.  This parameter allows you to copy them to a separate directory on disk, presumably for some kind of log processing system (such as [Splunk](http://www.splunk.com/)) to then pick them up, index them, etc.  It is optional, and defaults to disabled.    This can be a partial path, relative to the Cronicle base directory (`/opt/cronicle`) or a full path to a custom location.    ### queue_dir    The queue directory is used internally for misc. background tasks, such as handling detached jobs sending messages back to the daemon.  You shouldn't ever have to deal with this directly, and the directory is auto-created on install.      This can be a partial path, relative to the Cronicle base directory (`/opt/cronicle`) or a full path to a custom location.    ### pid_file    The PID file is simply a text file containing the Process ID of the main Cronicle daemon.  It is used by the `control.sh` script to stop the daemon, and detect if it is running.  You should never have to deal with this file directly, and it defaults to living in the `logs` directory which is auto-created.      This can be a partial path, relative to the Cronicle base directory (`/opt/cronicle`) or a full path to a custom location.  However, it should probably not be changed, as the `control.sh` script expects it to live in `logs/cronicled.pid`.    ### debug_level    The level of verbosity in the debug logs.  It ranges from `1` (very quiet) to `10` (extremely loud).  The default value is `9`.    ### maintenance    Cronicle needs to run storage maintenance once per day, which generally involves deleting expired records and trimming lists which have grown too large.  This only runs on the primary server, and typically only takes a few seconds, depending on the number of events you have.  The application is still usable during this time, but UI performance may be slightly impacted.    By default the maintenance is set to run at 4:00 AM (local server time).  Feel free to change this to a more convenient time for your server environment.  The format of the parameter is `HH:MM`.    ### list_row_max    This parameter controls how many items are kept in historical lists such as the [Activity Log](#activity-log-tab), [Completed Jobs](#completed-jobs-tab), and [Event History](#event-history-tab).  When this limit is exceeded, the oldest entries are removed during the nightly maintenance run.  The default limit is `10000` items.    This has no real effect on performance -- only space on disk (or Couchbase / S3).    ### job_data_expire_days    Completed job data is only kept around for this number of days.  This includes job logs and the metadata for each completed job (stats, etc.).  The default value is `180` days, but feel free to tune this for your own environment.    This has no real effect on performance -- only space on disk (or Couchbase / S3).    ### child_kill_timeout    This is the number of seconds to allow child processes to exit after sending a TERM signal (for aborted jobs, server shutdown, etc.).  If they do not exit within the specified timeout, a KILL signal is sent.  The default value is `10` seconds.    ### dead_job_timeout    When the primary server loses connectivity with a worker that had running jobs on it, they go into a ""limbo"" state for a period of time, before they are finally considered lost.  The `dead_job_timeout` parameter specifies the amount of time before these wayward jobs are aborted (and possibly retried, depending on the event settings).  The default value is `120` seconds.    This parameter exists because certain networks may have unreliable connections between servers, and it is possible a server may drop for a few seconds, then come right back.  If a short hiccup like that occurs, you probably don't want to abort all the running jobs right away.  Also, when you are upgrading Cronicle itself, you don't want detached jobs to be interrupted.    The worst case scenario is that a remote server with running jobs goes MIA for longer than the `dead_job_timeout`, the primary server aborts all the jobs, then the server reappears and finishes the jobs.  This creates a bit of a mess, because the jobs are reported as both errors and successes.  The latter success prevails in the end, but the errors stay in the logs and event history.    ### master_ping_freq    For multi-server clusters, this specifies how often the primary server should send out pings to worker servers, to let them know who is the boss.  The default is `20` seconds.    ### master_ping_timeout    For multi-server clusters, this specifies how long to wait after receiving a ping, before a backup server considers the primary server to be dead.  At this point a new primary server will be chosen.  The default value is `60` seconds.    ### udp_broadcast_port    For auto-discovery of nearby servers, this specifies the UDP port to use for broadcasting.  Do not worry if your network doesn't support UDP broadcast, as this is just an optional feature where nearby servers will show up in the UI.  The default port is `3014`.    ### scheduler_startup_grace    When the scheduler first starts up on the primary server, it waits for a few seconds before actually assigning jobs.  This is to allow all the servers in the cluster to check in and register themselves with the primary server.  The default value is `10` seconds, which should be plenty of time.    Once a server becomes primary, it should immediately attempt to connect to all remote servers right away.  So in theory this grace period could be as short as 1 second or less, but a longer delay allows for any random network connectivity errors to work themselves out.    ### universal_web_hook    While you can specify a web hook in the UI per each category and/or per each event, this parameter allows you to define a universal one, which is *always* fired for *every* job regardless of UI settings.  It should be a fully-qualified URL to an API endpoint that accepts an HTTP POST containing JSON data.    Web hooks are fired at the start and the end of each job (success or fail).  A JSON record is sent in the HTTP POST body, which contains all the relevant information about the job, including an `action` property, which will be set to `job_start` at the start and `job_complete` at the end of the job.  See the [Web Hooks](#event-web-hook) section below for more on the data format.    To include custom HTTP request headers with your web hook, append them onto the end of the URL using this format: `[header: My-Header: My-Value]`.  Make sure to include a space before the opening bracket.  Example URL:    ```  https://myserver.com/api/chat.postMessage [header: My-Header: My-Value]  ```    ### web_hook_custom_data    If you need to include custom JSON data with the web hook HTTP POST, you can do so by specifying a `web_hook_custom_data` property, and any keys/values will be merged in with the event data as it is sent to the web hook URL.  Example:    ```js  ""web_hook_custom_data"": {  	""my_custom_key1"": ""My custom value 1"",  	""my_custom_key2"": ""My custom value 2""  }  ```    In this example `my_custom_key1` and `my_custom_key2` will be merged in with the event data that usually accompanies the web hook post data.  See the [Web Hooks](#event-web-hook) section below for more on the data format.    ### web_hook_custom_opts    If you need to customize the low-level properties sent to the Node.js [http.request](https://nodejs.org/api/http.html#http_http_request_options_callback) method for making outbound web hook requests, use the `web_hook_custom_opts` property.  Using this you can set things like a proxy host and port.  Example use:    ```js  ""web_hook_custom_opts"": {  	""host"": ""my-corp-proxy.com"",  	""port"": 8080  }  ```    See the [http.request](https://nodejs.org/api/http.html#http_http_request_options_callback) docs for all the possible properties you can set here.    ### web_hook_text_templates    The web hook JSON POST data includes a `text` property which is a simple summary of the action taking place, which is compatible with [Slack Webhook Integrations](https://api.slack.com/incoming-webhooks).  These text strings are generated based on the action, and use the following templates:    ```js  ""web_hook_text_templates"": {  	""job_start"": ""Job started on [hostname]: [event_title] [job_details_url]"",  	""job_complete"": ""Job completed successfully on [hostname]: [event_title] [job_details_url]"",  	""job_failure"": ""Job failed on [hostname]: [event_title]: Error [code]: [description] [job_details_url]"",  	""job_launch_failure"": ""Failed to launch scheduled event: [event_title]: [description] [edit_event_url]""  }  ```    You can customize these text strings by including a `web_hook_text_templates` object in your configuration, and setting each of the action properties within.  Also, you can use this to *disable* any of the web hook actions, by simply removing certain action keys.  For example, if you don't want to fire a web hook for starting a job, remove the `job_start` key.  If you only want web hooks to fire for errors, remove both the `job_start` and `job_complete` keys.    The text string templates can use any data values from the web hook JSON data by inserting `[square_bracket]` placeholders.  See the [Web Hooks](#event-web-hook) section below for more on the data format, and which values are available.    ### ssl_cert_bypass    If you are having trouble getting HTTPS web hooks or SSL SMTP e-mails to work, you might need to set `ssl_cert_bypass` to true.  This causes Node.js to blindly accept all SSL connections, even when it cannot validate the SSL certificate.  This effectively sets the following environment variable at startup:    ```js  process.env.NODE_TLS_REJECT_UNAUTHORIZED = ""0"";  ```    Please only do this if you understand the security ramifications, and *completely trust* the host(s) you are connecting to, and the network you are on.  Skipping the certificate validation step should really only be done in special circumstances, such as trying to hit one of your own internal servers with a self-signed cert.    For legacy compatibility, the old `web_hook_ssl_cert_bypass` property is still accepted, and has the same effect as `ssl_cert_bypass`.    ### job_memory_max    This parameter allows you to set a default memory usage limit for jobs, specified in bytes.  This is measured as the total usage of the job process *and any sub-processes spawned or forked by the main process*.  If the memory limit is exceeded, the job is aborted.  The default value is `1073741824` (1 GB).  To disable set it to `0`.    Memory limits can also be customized in the UI per each category and/or per each event (see [Event Resource Limits](#event-resource-limits) below).  Doing either overrides the primary default.    ### job_memory_sustain    When using the [job_memory_max](#job_memory_max) feature, you can optionally specify how long a job is allowed exceed the maximum memory limit until it is aborted.  For example, you may want to allow jobs to spike over 1 GB of RAM, but not use it sustained for more a certain amount of time.  That is what the `job_memory_sustain` property allows, and it accepts a value in seconds.  It defaults to `0` (abort instantly when exceeded).    Memory limits can also be customized in the UI per each category and/or per each event (see [Event Resource Limits](#event-resource-limits) below).  Doing either overrides the default.    ### job_cpu_max    This parameter allows you to set a default CPU usage limit for jobs, specified in percentage of one CPU core.  This is measured as the total CPU usage of the job process *and any sub-processes spawned or forked by the main process*.  If the CPU limit is exceeded, the job is aborted.  The default value is `0` (disabled).  For example, to allow jobs to use up to 2 CPU cores, specify `200` as the limit.    CPU limits can also be customized in the UI per each category and/or per each event (see [Event Resource Limits](#event-resource-limits) below).  Doing either overrides the default.    ### job_cpu_sustain    When using the [job_cpu_max](#job_cpu_max) feature, you can optionally specify how long a job is allowed exceed the maximum CPU limit until it is aborted.  For example, you may want to allow jobs to use up to 2 CPU cores, but not use them sustained for more a certain amount of time.  That is what the `job_cpu_sustain` property allows, and it accepts a value in seconds.  It defaults to `0` (abort instantly when exceeded).    CPU limits can also be customized in the UI per each category and/or per each event (see [Event Resource Limits](#event-resource-limits) below).  Doing either overrides the default.    ### job_log_max_size    This parameter allows you to set a default log file size limit for jobs, specified in bytes.  If the file size limit is exceeded, the job is aborted.  The default value is `0` (disabled).    Job log file size limits can also be customized in the UI per each category and/or per each event (see [Event Resource Limits](#event-resource-limits) below).  Doing either overrides the default.    ### job_env    Place any key/value pairs you want into the `job_env` object, and they will become environment variables passed to all job processes, as they are spawned.  Note that these can be overridden by event parameters with the same names.  The `job_env` can be thought of as a way to specify universal default environment variables for all your jobs.  Example:    ```js  ""job_env"": {  	""TZ"": ""America/Los_Angeles"",  	""LANG"": ""en_US.UTF-8""  },  ```    ### server_comm_use_hostnames    Setting this parameter to `true` will force the Cronicle servers to connect to each other using hostnames rather than LAN IP addresses.  This is mainly for special situations where your local server IP addresses may change, and you would prefer to rely on DNS instead.  The default is `false` (disabled), meaning connect using IP addresses.    ### web_direct_connect    When this property is set to `false` (which is the default), the Cronicle Web UI will connect to whatever hostname/port is on the URL.  It is expected that this hostname/port will always resolve to your primary server.  This is useful for single server setups, situations when your users do not have direct access to your Cronicle servers via their IPs or hostnames, or if you are running behind some kind of reverse proxy.    If you set this parameter to `true`, then the Cronicle web application will connect *directly* to your individual Cronicle servers.  This is more for multi-server configurations, especially when running behind a [load balancer](#load-balancers) with multiple backup servers.  The Web UI must always connect to the primary server, so if you have multiple backup servers, it needs a direct connection.    Note that the ability to watch live logs for active jobs requires a direct web socket connection to the server running the job.  For that feature, this setting has no effect (it always attempts to connect directly).    ### web_socket_use_hostnames    Setting this parameter to `true` will force Cronicle's Web UI to connect to the back-end servers using their hostnames rather than IP addresses.  This includes both AJAX API calls and Websocket streams.  You should only need to enable this in special situations where your users cannot access your servers via their LAN IPs, and you need to proxy them through a hostname (DNS) instead.  The default is `false` (disabled), meaning connect using IP addresses.    This property only takes effect if [web_direct_connect](#web_direct_connect) is also set to `true`.    ### socket_io_transports    This is an advanced configuration property that you will probably never need to worry about.  This allows you to customize the [socket.io transports](https://socket.io/docs/client-api/) used to connect to the server for real-time updates.  By default, this property is set internally to an array containing the `websocket` transport only, e.g.    ```js  ""socket_io_transports"": [""websocket""]  ```    However, if you are trying to run Cronicle in an environment where WebSockets are not allowed (perhaps an ancient firewall or proxy), you can change this array to contain the `polling` transport first, e.g.    ```js  ""socket_io_transports"": [""polling"", ""websocket""]  ```    However, please only do this if you know exactly what you are doing, and why.    ### max_jobs    You can optionally set a global maximum number of concurrent jobs to allow.  This is across all servers and categories, and is designed as an ""emergency brake"" for runaway events.  The property is called `max_jobs`.  The default is `0` (no limit).  Example:    ```js  ""max_jobs"": 256  ```    ## Storage Configuration    The `Storage` object contains settings for the Cronicle storage system.  This is built on the [pixl-server-storage](https://www.npmjs.com/package/pixl-server-storage) module, which can write everything to local disk (the default), [Couchbase](http://www.couchbase.com/nosql-databases/couchbase-server) or [Amazon S3](https://aws.amazon.com/s3/).    To select a storage engine, place one of the following values into the `engine` property:    ### Filesystem    The default storage method is to use local disk (can also be an NFS mount, for multi-server setups with failover support).  For this, set the `engine` property to `Filesystem`, and declare a sub-object with the same name, with a couple more properties:    ```js  {  	""Storage"": {  		""engine"": ""Filesystem"",  		""Filesystem"": {  			""base_dir"": ""data"",  			""key_namespaces"": 1  		}  	}  }  ```    The `base_dir` is the base directory to store everything under.  It can be a fully-qualified filesystem path, or a relative path to the Cronicle base directory (e.g. `/opt/cronicle`).  In this case it will be `/opt/cronicle/data`.    For more details on using the Filesystem as a backing store, please read the [Local Filesystem section in the pixl-server-storage docs](https://www.npmjs.com/package/pixl-server-storage#local-filesystem).    ### Couchbase    To use Couchbase as a backing store for Cronicle, please read the [Couchbase section in the pixl-server-storage docs](https://www.npmjs.com/package/pixl-server-storage#couchbase).  It has complete details for how to setup the storage object.  Example configuration:    ```js  {  	""Storage"": {  		""engine"": ""Couchbase"",  		""Couchbase"": {  			""connectString"": ""couchbase://127.0.0.1"",  			""bucket"": ""default"",  			""username"": """",  			""password"": """",  			""serialize"": false,  			""keyPrefix"": ""cronicle""  		}  	}  }  ```    If you are sharing a bucket with other applications, use the `keyPrefix` property to keep the Cronicle data separate, in its own ""directory"".  For example, set `keyPrefix` to `""cronicle""` to keep all the Cronicle-related records in a top-level ""cronicle"" directory in the bucket.    Note that for Couchbase Server v5.0+ (Couchbase Node SDK 2.5+), you will have to supply both a `username` and `password` for a valid user created in the Couchbase UI.  Prior to v5+ you could omit the `username` and only specify a `password`, or no password at all if your bucket has no authentication.    You'll also need to install the npm [couchbase](https://www.npmjs.com/package/couchbase) module:    ```  cd /opt/cronicle  npm install couchbase  ```    After configuring Couchbase, you'll need to run the Cronicle setup script manually, to recreate all the base storage records needed to bootstrap the system:    ```  /opt/cronicle/bin/control.sh setup  ```    ### Amazon S3    To use Amazon S3 as a backing store for Cronicle, please read the [Amazon S3 section in the pixl-server-storage docs](https://www.npmjs.com/package/pixl-server-storage#amazon-s3).  It has complete details for how to setup the storage object.  Example configuration:    ```js  {  	""Storage"": {  		""engine"": ""S3"",  		""AWS"": {  			""accessKeyId"": ""YOUR_AMAZON_ACCESS_KEY"",   			""secretAccessKey"": ""YOUR_AMAZON_SECRET_KEY"",   			""region"": ""us-west-1"",  			""correctClockSkew"": true,  			""maxRetries"": 5,  			""httpOptions"": {  				""connectTimeout"": 5000,  				""timeout"": 5000  			}  		},  		""S3"": {  			""keyPrefix"": ""cronicle"",  			""fileExtensions"": true,  			""params"": {  				""Bucket"": ""YOUR_S3_BUCKET_ID""  			}  		}  	}  }  ```    If you are sharing a bucket with other applications, use the `keyPrefix` property to keep the Cronicle data separate, in its own ""directory"".  For example, set `keyPrefix` to `""cronicle""` to keep all the Cronicle-related records in a top-level ""cronicle"" directory in the bucket.  A trailing slash will be automatically added to the prefix if missing.    It is recommended that you always set the S3 `fileExtensions` property to `true` for new installs.  This makes the Cronicle S3 records play nice with sync / copy tools such as [Rclone](https://rclone.org/).  See [Issue #60](https://github.com/jhuckaby/Cronicle/issues/60) for more details.  Do not change this property on existing installs -- use the [Storage Migration Tool](#storage-migration-tool).    To use S3 you'll also need to install the npm [aws-sdk](https://www.npmjs.com/package/aws-sdk) module:    ```  cd /opt/cronicle  npm install aws-sdk  ```    After configuring S3, you'll need to run the Cronicle setup script manually, to recreate all the base storage records needed to bootstrap the system:    ```  /opt/cronicle/bin/control.sh setup  ```    If you're worried about Amazon S3 costs, you probably needn't.  With a typical setup running ~30 events per hour (about ~25,000 events per month), this translates to approximately 350,000 S3 PUTs plus 250,000 S3 GETs, or about $2 USD per month.  Add in 100GB of data storage and it's another $3.    ## Web Server Configuration    Cronicle has an embedded web server which handles serving up the user interface, as well as some server-to-server communication that takes place between the primary and workers.  This is configured in the `WebServer` object, and there are only a handful of parameters you should ever need to configure:    ```js  {  	""WebServer"": {  		""http_port"": 3012,  		  		""https"": 0,  		""https_port"": 3013,  		""https_cert_file"": ""conf/ssl.crt"",  		""https_key_file"": ""conf/ssl.key""  	}  }  ```    Changing the `http_port` is probably the most common thing you will want to customize.  For example, if you don't have anything else running on port 80, you will probably want to change it to that, so you can access the UI without entering a port number.    This is also where you can enable HTTPS, if you want the UI to be SSL encrypted.  Set the `https` property to `1` to enable, and configure the `https_port` as you see fit (the standard HTTPS port is `443`).  You will have to supply your own SSL certificate files (sample self-signed certs are provided for testing, but they will generate browser warnings).    For more details on the web server component, please see the [pixl-server-web](https://www.npmjs.com/package/pixl-server-web#configuration) module documentation.    ## User Configuration    Cronicle has a simple user login and management system, which is built on the [pixl-server-user](https://www.npmjs.com/package/pixl-server-user) module.  It handles creating new users, assigning permissions, and login / session management.  It is configured in the `User` object, and there are only a couple of parameters you should ever need to configure:    ```js  {  	""User"": {  		""free_accounts"": 0,  		  		""default_privileges"": {  			""admin"": 0,  			""create_events"": 1,  			""edit_events"": 1,  			""delete_events"": 1,  			""run_events"": 0,  			""abort_events"": 0,  			""state_update"": 0  		}  	}  }  ```    The `free_accounts` property specifies whether guests visiting the UI can create their own accounts, or not.  This defaults to `0` (disabled), but you can set it to `1` to enable.  This feature should only be used when your install of Cronicle is running on a private network, and you trust all your employees.    The `default_privileges` object specifies which privileges new accounts will receive by default.  Here is a list of all the possible privileges and what they mean:    | Privilege ID | Description |  |--------------|-------------|  | `admin` | User is a full administrator.  **This automatically grants ALL privileges, current and future.** |  | `create_events` | User is allowed to create new events and add them to the schedule. |  | `edit_events` | User is allowed to edit and save events -- even those created by others. |  | `delete_events` | User is allowed to delete events -- event those created by others. |  | `run_events` | User is allowed to run events on-demand by clicking the ""Run"" button in the UI. |  | `abort_events` | User is allowed to abort jobs in progress, even those for events created by others. |  | `state_update` | User is allowed to enable or disable the primary scheduler. |    By default new users have the `create_events`, `edit_events` and `delete_events` privileges, and nothing else.  Note that when an administrator creates new accounts via the UI, (s)he can customize the privileges at that point.  The configuration only sets the defaults.    For more details on the user manager component, please see the [pixl-server-user](https://www.npmjs.com/package/pixl-server-user#configuration) module documentation.    ## Email Configuration    Cronicle will send a number of different types of e-mails in response to certain events.  These are mostly confirmations of actions, or just simple notifications.  Most of these can be disabled in the UI if desired.  The e-mail content is also configurable, including the `From` and `Subject` headers, and is based on plain text e-mail template files located on disk:    | Action | Email Template | Description |  |--------|----------------|-------------|  | **New User Account** | `conf/emails/welcome_new_user.txt` | Sent when a new user account is created. |  | **Changed Password** | `conf/emails/changed_password.txt` | Sent when a user changes their password. |  | **Recover Password** | `conf/emails/recover_password.txt` | Sent when a user requests password recovery. |  | **Job Succeeded** | `conf/emails/job_success.txt` | Conditionally sent when a job completes successfully (depends on event configuration). |  | **Job Failed** | `conf/emails/job_fail.txt` | Conditionally sent when a job fails (depends on event configuration). |  | **Event Error** | `conf/emails/event_error.txt` | Sent when a job fails to launch (depends on event configuration). |    Feel free to edit these files to your liking.  Note that any text in `[/square_brackets]` is a placeholder which gets swapped out with live data relevant to the event which fired off the e-mail.    Here is an example e-mail template file:    ```  To: [/notify_success]  From: [/config/email_from]  Subject: Cronicle Job Completed Successfully: [/event_title]    Date/Time: [/nice_date_time]  Event Title: [/event_title]  Category: [/category_title]  Server Target: [/nice_target]  Plugin: [/plugin_title]    Job ID: [/id]  Hostname: [/hostname]  PID: [/pid]  Elapsed Time: [/nice_elapsed]  Performance Metrics: [/perf]  Avg. Memory Usage: [/nice_mem]  Avg. CPU Usage: [/nice_cpu]    Job Details:  [/job_details_url]    Job Debug Log ([/nice_log_size]):  [/job_log_url]    Edit Event:  [/edit_event_url]    Description:  [/description]    Event Notes:  [/notes]    Regards,  The Cronicle Team  ```    The stock e-mail templates shipped with Cronicle are plain text, but you can provide your own rich HTML e-mail templates if you want.  Simply start the e-mail body content (what comes after the Subject line) with an HTML open tag, e.g. `<div>`, and the e-mails will be sent as HTML instead of text.    You can include any property from the main `conf/config.json` file by using the syntax `[/config/KEY]`.  Also, to include environment variables, use the syntax `[/env/ENV_KEY]`, for example `[/env/NODE_ENV]`.    # Web UI    This section describes the Cronicle web user interface.  It has been tested extensively in Safari, Chrome and Firefox.  Recent versions of IE should also work (11 and Edge).    ## Home Tab    ![Home Tab Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/home.png)    The **Home** tab, also known as the dashboard, is the default page shown after you log in.  It displays basic information about the application, currently active (running) jobs, and a list of upcoming jobs in the next 24 hours.  The page is split vertically into three main sections:    ### General Stats    This section contains a summary of various Cronicle stats.  Some are current totals, and others are daily totals.  Here is a list of everything that is displayed:    | Statistic | Description |  |-----------|-------------|  | **Total Events** | Current number of active events in the schedule. |  | **Total Categories** | Current number of event categories in the system. |  | **Total Plugins** | Current number of registered Plugins in the system. |  | **Jobs Completed Today** | Number of jobs completed today (resets at midnight, local server time). |  | **Jobs Failed Today** | Number of jobs that failed today (resets at midnight, local server time). |  | **Job Success Rate** | Percentage of completed jobs that succeeded today (resets at midnight, local server time). |  | **Total Servers** | Current total number of servers in the Cronicle cluster. |  | **Total CPU in Use** | Current total CPU in use (all servers, all jobs, all processes). |  | **Total RAM in Use** | Current total RAM in use (all servers, all jobs, all processes). |  | **Primary Server Uptime** | Elapsed time since Cronicle on the primary server was restarted. |  | **Average Job Duration** | The average elapsed time for all completed jobs today (resets at midnight, local server time). |  | **Average Job Log Size** | The average job log file size for all completed jobs today (resets at midnight, local server time). |    ### Active Jobs    This table lists all the currently active (running) jobs, and various information about them.  The table columns are:    | Column | Description |  |--------|-------------|  | **Job ID** | A unique ID assigned to the job.  Click this to see live job progress (see [Job Details Tab](#job-details-tab) below). |  | **Event Name** | The name of the scheduled event which started the job. |  | **Category** | The category to which the event is assigned. |  | **Hostname** | The server hostname which is running the job. |  | **Elapsed** | The current elapsed time since the job started. |  | **Progress** | A visual representation of the job's progress, if available. |  | **Remaining** | The estimated remaining time, if available. |  | **Actions** | Click *Abort* to cancel the job. |    ### Upcoming Events    This table lists all the upcoming scheduled events in the next 24 hours, and various information about them.  The table columns are:    | Column | Description |  |--------|-------------|  | **Event Name** | The name of the scheduled event.  Click this to edit the event (see [Edit Event Tab](#edit-event-tab) below). |  | **Category** | The category to which the event is assigned. |  | **Plugin** | The Plugin which will be loaded to run the event. |  | **Target** | The server target (server group or individual server hostname) which will run the event. |  | **Scheduled Time** | When the event is scheduled to run (in your local timezone unless otherwise specified). |  | **Countdown** | How much time remains until the event runs. |  | **Actions** | Click *Edit Event* to edit the event (see [Edit Event Tab](#edit-event-tab) below). |    ## Schedule Tab    ![Schedule Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/schedule.png)    This tab displays the list of all events currently in the schedule, including both active and disabled events.  From here you can add new events, edit existing events, run events on-demand, and jump to locations such as [Event History](#event-history-tab) and [Event Stats](#event-stats-tab).  The schedule table has the following columns:    | Column | Description |  |--------|-------------|  | **Active** | This checkbox indicates whether the event is active or disabled.  Click it to toggle the state. |  | **Event Name** | The name of the scheduled event.  Click this to edit the event (see [Edit Event Tab](#edit-event-tab) below). |  | **Timing** | A summary of the event's timing settings (daily, hourly, etc.). |  | **Category** | The category to which the event is assigned. |  | **Plugin** | The Plugin which will be loaded to run the event. |  | **Target** | The server target (server group or individual server hostname) which will run the event. |  | **Status** | Current status of the event (idle or number of running jobs). |  | **Actions** | A list of actions to run on the event.  See below. |    Here are the actions you can run on each event from the Schedule tab:    | Action | Description |  |--------|-------------|  | **Run** | Immediately runs the event (starts an on-demand job), regardless of the event's timing settings. |  | **Edit** | This jumps over to the [Edit Event Tab](#edit-event-tab) to edit the event. |  | **Stats** | This jumps over to the [Event Stats Tab](#event-stats-tab) to see statistics about the event and past jobs. |  | **History** | This jumps over to the [Event History Tab](#event-history-tab) to see the event's history of completed jobs. |    ### Edit Event Tab    ![Edit Event Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/edit-event.png)    The Edit Event Tab displays a form for editing scheduled events, and creating new ones.  Here are all the form fields and what they mean:    #### Event ID    Each event has a unique ID which is used when making API calls, and can be ignored otherwise.  This is only displayed when editing events.    #### Event Name    Each event has a name, which can be anything you like.  It is displayed on the Schedule tab, and in reports, e-mails, etc.    #### Event Enabled    This checkbox specifies whether the event is enabled (active) in the scheduler, and will fire off jobs according to the [Event Timing](#event-timing), or disabled.  If disabled, you can still run on-demand jobs by clicking the ""Run Now"" button.    #### Event Category    All events are assigned to a particular category.  If you don't want to create categories, just assign your events to the provided ""General"" category.  Categories can define limits such as max concurrent jobs, max RAM per job and max CPU per job.  See the [Categories Tab](#categories-tab) below for more details on creating categories.    #### Event Target    In a multi-server cluster, events can be targeted to run on individual servers, or server groups.  Both are listed in the drop-down menu.  If a server group is targeted, one of the group's servers is chosen each time the event runs a job.  You can decide which algorithm to use for picking servers from the group (see below).  Also, see the [Servers Tab](#servers-tab) for more details on creating server groups.    ##### Algorithm    When you target a server group for your event, a supplementary menu appears to select an ""algorithm"".  This is simply the method by which Cronicle picks a server in the group to run your job.  The default is ""Random"" (i.e. select a random server from the group for each job), but there are several others as well:    | Algorithm ID | Algorithm Name | Description |  |--------------|----------------|-------------|  | `random` | **Random** | Pick a random server from the group. |  | `round_robin` | **Round Robin** | Pick each server in sequence (alphabetically sorted). |  | `least_cpu` | **Least CPU Usage** | Pick the server with the least amount of CPU usage in the group. |  | `least_mem` | **Least Memory Usage** | Pick the server with the least amount of memory usage in the group. |  | `prefer_first` | **Prefer First** | Prefer the first server in the group (alphabetically sorted), only picking alternatives if the first server is unavailable. |  | `prefer_last` | **Prefer Last** | Prefer the last server in the group (alphabetically sorted), only picking alternatives if the last server is unavailable. |  | `multiplex` | **Multiplex** | Run the event on **all** servers in the group simultaneously (see below). |    ##### Multiplexing    If the event targets a server group, you have the option of ""multiplexing"" it.  That is, the event will run jobs on *all* the servers in the group at once, rather than picking one server at random.    When this feature is enabled, an optional ""Stagger"" text field will appear.  This allows you to stagger (progressively delay) the launch of jobs across the servers, so they don't all start at the same exact time.    #### Event Plugin    Whenever Cronicle runs an event, a ""Plugin"" is loaded to handle the job.  This is basically a shell command which runs as its own process, reads JSON from STDIN to receive metadata about the job, and writes JSON to STDOUT to report progress and completion.  Plugins can be written in virtually any language.  See the [Plugins](#plugins) section below for more details.    If the Plugin defines any custom parameters, they are editable per event.  This feature allows the Plugin to define a set of UI elements (text fields, checkboxes, drop-down menus, etc.) which the event editor can provide values for.  Then, when jobs are started, the Plugin is provided a JSON document containing all the custom keys and values set by the UI for the event.    #### Event Timing    ![Event Timing Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/edit-event-timing.png)    Events are scheduled to run at various dates and times using a visual multi-selector widget, as shown above.  This allows you to multi-select any combination of years, months, days, weekdays, hours and/or minutes, for an event to run on.  It will also repeat on a recurring basis, each time the server clock matches your selections.  This is very similar to the [Unix Cron](https://en.wikipedia.org/wiki/Cron) format, but with a more visual user interface.    If you leave all the boxes unchecked in a particular time scale, it means the same as ""all"" (similar to the Cron asterisk `*` operator).  So if you leave everything blank in all the categories and select only the "":00"" minute, it means: every year, every month, every day, every weekday, and every hour on the "":00"" minute.  Or in other words, hourly.    If you click the ""Import..."" link, you can import date/time settings from a Crontab, i.e. the famous `* * * * *` syntax.  This saves you the hassle of having to translate your existing Crontabs over to the Cronicle UI by hand.    By default, events are scheduled in your current local timezone, but you can customize this using the menu at the top-right.  The menu is pre-populated with all the [IANA standard timezones](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).  Also, the timezone selection is saved with each event, so Cronicle always knows exactly when the they should all run, regardless of the server timezone.    The Cronicle scheduling system is versatile, but it can't do *everything*.  For example, you cannot schedule an event to run at two different times on two different days, such as 2:30 PM on Monday and 8:30 AM on Tuesday.  For that, you'd have to create separate events.    #### Event Concurrency    The Event Concurrency selector allows you to specify how many simultaneous jobs are allowed to run for the event.  For example, multiple jobs may need to run if your event is scheduled to run hourly, but it takes longer than a hour to complete a job.  Or, a job may be already running and someone clicks the ""Run Now"" link on the Schedule tab.  This selector specifies the maximum allowed jobs to run concurrently for each event.    If an event cannot start a job due to the concurrency limit, an error is logged (see the [Activity Log Tab](#activity-log-tab) below).  What happens next depends on the [Event Options](#event-options).  If the event has **Run All (Catch-Up)** mode enabled, then the scheduler will keep trying to run every scheduled job indefinitely.  Otherwise, it will simply wait until the next scheduled run.    #### Event Timeout    You can optionally specify an event timeout, which is a maximum run time for event jobs.  If a job takes longer than the specified timeout period, it is aborted, and logged as a failed job.  To disable the timeout and allow jobs to run indefinitely, set this field to `0`.    #### Event Retries    If a job throws an internal error (meaning, it returns a non-zero `code` in the JSON response, or a shell command exits with a non-zero exit code), you can have Cronicle automatically retry it up to 32 times.  Aborting a job (either manually or by a timeout) does not trigger a retry.    If the retries is set to a non-zero amount, a ""Retry Delay"" text field will appear.  This allows you to have Cronicle wait a certain amount of time between retries, if you want.  The idea is to reduce bashing on services that may be overloaded.    Note that the [Event Timeout](#event-timeout) applies to the total run time of the job, *which includes all retries*.  For example, if you set the timeout to 10 minutes, and the job takes 9 minutes and fails, any retries will then only have 1 minute to complete the job.  So please set the timeout accordingly.    #### Event Options    This section allows you to select options on how jobs are handled for the event:    ##### Run All Mode    When Run All (Catch-Up) Mode mode is enabled on an event, the scheduler will do its best to ensure that *every* scheduled job will run, even if they have to run late.  This is useful for time-sensitive events such as generating reports.  So for example, if you have an event scheduled to run hourly, but something prevents it from starting or completing (see below), the scheduler will *keep trying indefinitely* until each separate hourly job runs.  If the event cannot run for multiple hours, the jobs will simply queue up, and the scheduler will run them all in order, as quickly as its rules allow.    If any of the following situations occur, and the event has Run All (Catch-Up) mode enabled, the scheduler will queue up and re-run all missed jobs:    * Job could not run due to concurrency limits.  * Job could not run due to the target server being unavailable.  * Job could not run due to the event category or Plugin being disabled.  * Server running the job was shut down.  * Server running the job crashed.  * Job was aborted due to exceeding a timeout limit.  * Job was aborted due to exceeding a resource limit (RAM or CPU).    The only time a Catch-Up job is *not* re-run is when one of the following actions occur:    * Job is manually aborted via the Web UI or API.  * Job fails due to error thrown from inside the Plugin (user code generated error).    You can see all queued jobs on the [Home Tab](#home-tab).  They will be listed in the [Upcoming Events](#upcoming-events) table, and have their ""Countdown"" column set to ""Now"".  To jump over the queue and reset an event that has fallen behind, use the [Event Time Machine](#event-time-machine) feature.    When Run All (Catch-Up) mode is disabled, and a job cannot run or fails due to any of the reasons listed above, the scheduler simply logs an error, and resumes normal operations.  The event will not run until the next scheduled time, if any.  This is more suitable for events that are not time-sensitive, such as log rotation.    ##### Detached Mode    When Uninterruptible (Detached) mode is enabled on an event, jobs are spawned as standalone background processes, which are not interrupted for things like the Cronicle daemon restarting.  This is designed mainly for critical operations that *cannot* be stopped in the middle for whatever reason.    Please use this mode with caution, and only when truly needed, as there are downsides.  First of all, since the process runs detached and standalone, there are no real-time updates.  Meaning, the progress bar and time remaining displays are delayed by up to a minute.  Also, when your job completes, there is a delay of up to a minute before Cronicle realizes and marks the job as complete.    It is much better to design your jobs to be interrupted, if at all possible.  Note that Cronicle will re-run interrupted jobs if they have [Run All Mode](#run-all-mode) set.  So Detached Mode should only be needed in very special circumstances.    ##### Allow Queued Jobs    By default, when jobs cannot run due to concurrency settings, or other issues like an unavailable target server, an error is generated.  That is, unless you enable the event queue.  With queuing enabled, jobs that can't run immediately are queued up, and executed on a first come, first serve basis, as quickly as conditions allow.    When the queue is enabled on an event, a new ""Queue Limit"" section will appear in the form, allowing you to set the maximum queue length per event.  If this limit is reached, no additional jobs can be queued, and an error will be generated.    You can track the progress of your event queues on the [Home Tab](#home-tab).  Queued events and counts appear in a table between the [Active Jobs](#active-jobs) and [Upcoming Events](#upcoming-events) sections.  From there you can also ""flush"" an event queue (i.e. delete all queued jobs), in case one grows out of control.    ##### Chain Reaction    Chain Reaction mode allows you to select an event which will be launched automatically each time the current event completes a job.  You are essentially ""chaining"" two events together, so one always runs at the completion of the other.  This chain can be any number of events long, and the events can all run on different servers.    You can optionally select different events to run if the current job succeeds or fails.  For example, you may have a special error handling / notification event, which needs to run upon specific event failures.    You can have more control over this process by using the JSON API in your Plugins.  See [Chain Reaction Control](#chain-reaction-control) below for details.    #### Event Time Machine    When editing an existing event that has Run All (Catch-Up) mode enabled, the **Event Time Machine** will appear.  This is a way to reset the internal ""clock"" for an event, allowing you to re-run past jobs, or skip over a queue of stuck jobs.    For each event in the schedule, Cronicle keeps an internal clock called a ""cursor"".  If you imagine time running along a straight line, the event cursors are points along that line.  When the primary server ticks a new minute, it shifts all the event cursors forward up to the current minute, running any scheduled events along the way.    So for example, if you needed to re-run a daily 4 AM report event, you can just edit the cursor clock and set it back to 3:59 AM.  The cursor will catch up to the current time as quickly as it can, stopping only to run any scheduled events along the way.  You can also use this feature to ""jump"" over a queue, if jobs have stacked up for an event.  Just set the cursor clock to the current time, and the scheduler will resume jobs from that point onward.    The event clock for the Time Machine is displayed and interpreted in the event's currently selected timezone.    #### Event Notification    Cronicle can optionally send out an e-mail notification to a custom list of recipients for each job's completion (for multiple, separate addresses by commas).  You can also specify different e-mail addresses for when job succeeds, vs. when it fails.  The e-mails will contain a plethora of information about the event, the job, and the error if applicable.  Example email contents:    ```  To: ops@local.cronicle.com  From: notify@local.cronicle.com  Subject: Cronicle Job Failed: Rebuild Indexes    Date/Time: 2015/10/24 19:47:50 (GMT-7)  Event Title: Rebuild Indexes  Category: Database  Server Target: db01.prod  Plugin: DB Indexer    Job ID: jig5wyx9801  Hostname: db01.prod  PID: 4796  Elapsed Time: 1 minute, 31 seconds  Performance Metrics: scale=1&total=30.333&db_query=1.699&db_connect=1.941&log_read=2.931&gzip_data=3.773  Memory Usage: 274.5 MB Avg, 275.1 MB Peak  CPU Usage: 31.85% Avg, 36.7% Peak  Error Code: 999    Error Description:  Failed to write to file: /backup/db/schema.sql: Out of disk space    Job Details:  http://local.syncronic.com:3012/#JobDetails?id=jig5wyx9801    Job Debug Log (18.2 K):  http://local.syncronic.com:3012/api/app/get_job_log?id=jig5wyx9801    Edit Event:  http://local.syncronic.com:3012/#Schedule?sub=edit_event&id=3c182051    Event Notes:  This event handles reindexing our primary databases nightly.  Contact Daniel in Ops for details.    Regards,  The Cronicle Team  ```    You have control over much of the content of these e-mails.  The **Error Code** and **Description** are entirely generated by your own Plugins, and can be as custom and verbose as you want.  The **Performance Metrics** are also generated by your Plugins (if applicable), and the **Event Notes** are taken straight from the UI for the event (see [Event Notes](#event-notes) below).  Finally, the entire e-mail template can be customized to including additional information or to fit your company's brand.  HTML formatted e-mails are supported as well.    See the [Email Configuration](#email-configuration) section for more details on customization.    ##### Event Web Hook    Another optional notification method for events is a ""web hook"".  This means Cronicle will send an HTTP POST to a custom URL that you specify, both at the start and the end of each job, and include full details in JSON format.  Your own API endpoint will receive the JSON POST from Cronicle, and then your code can fire off its own custom notification.    You can determine if the request represents a start or the end of a job by looking at the `action` property.  It will be set to `job_start` or `job_complete` respectively.  Here is a list of all the JSON properties that will be included in the web hook, and what they mean:    | JSON Property | Description |  |---------------|-------------|  | `action` | Specifies whether the web hook signifies the start (`job_start`) or end (`job_complete`) of a job. |  | `base_app_url` | The [base_app_url](#base_app_url) configuration property. |  | `category` | The Category ID to which the event is assigned. |  | `category_title` | The title of the Category to which the event is assigned. |  | `code` | The response code as specified by your Plugin (only applicable for `job_complete` hooks). |  | `cpu` | An object representing the min, max, average and latest CPU usage for the job (only applicable for `job_complete` hooks). |  | `description` | A custom text string populated by your Plugin, typically contains the error message on failure. |  | `edit_event_url` | A fully-qualified URL to edit the event in the Cronicle UI. |  | `elapsed` | The total elapsed time for the job, in seconds (only applicable for `job_complete` hooks). |  | `event` | The ID of the event which spawned the job. |  | `event_title` | The title of the event which spawned the job. |  | `hostname` | The hostname of the server which ran (or is about to run) the event. |  | `id` | An auto-assigned unique ID for the job, which can be used in API calls to query for status. |  | `job_details_url` | A fully-qualified URL to view the job details in the Cronicle UI. |  | `log_file_size` | The size of the job's log file in bytes (only applicable for `job_complete` hooks). |  | `mem` | An object representing the min, max, average and latest memory usage for the job (only applicable for `job_complete` hooks). |  | `nice_target` | Will be set to the title of the target server group, or exact server hostname, depending on how the event is configured. |  | `params` | An object containing all the UI selections for the Plugin custom parameters, from the event. |  | `perf` | An object or string containing performance metrics, as reported by your Plugin (only applicable for `job_complete` hooks). |  | `pid` | The Process ID (PID) of the main job process which ran your Plugin code (only applicable for `job_complete` hooks). |  | `plugin` | The ID of the Plugin assigned to the event. |  | `plugin_title` | The title of the Plugin assigned to the event. |  | `source` | A string describing who or what started the job (user or API).  Will be blank if launched normally by the scheduler. |  | `text` | A simple text string describing the action that took place.  Useful for [Slack Webhook Integrations](https://api.slack.com/incoming-webhooks). |  | `time_end` | The Epoch timestamp of when the job ended (only applicable for `job_complete` hooks). |  | `time_start` | The Epoch timestamp of when the job started. |    Here is an example web hook JSON record (`job_complete` version shown):    ```js  {  	""action"": ""job_complete"",  	""base_app_url"": ""http://localhost:3012"",  	""category"": ""general"",  	""category_title"": ""General"",  	""code"": 0,  	""cpu"": {  		""min"": 23.4,  		""max"": 23.4,  		""total"": 23.4,  		""count"": 1,  		""current"": 23.4  	},  	""description"": ""Success!"",  	""edit_event_url"": ""http://localhost:3012/#Schedule?sub=edit_event&id=3c182051"",  	""elapsed"": 90.414,  	""event"": ""3c182051"",  	""event_title"": ""Test Event 2"",  	""hostname"": ""joeretina.local"",  	""id"": ""jihuyalli01"",  	""job_details_url"": ""http://localhost:3012/#JobDetails?id=jihuyalli01"",  	""log_file_size"": 25119,  	""mem"": {  		""min"": 190459904,  		""max"": 190459904,  		""total"": 190459904,  		""count"": 1,  		""current"": 190459904  	},  	""nice_target"": ""joeretina.local"",  	""params"": {  		""db_host"": ""idb01.mycompany.com"",  		""verbose"": 1,  		""cust"": ""Marketing""  	},  	""perf"": ""scale=1&total=90.103&db_query=0.237&db_connect=6.888&log_read=9.781&gzip_data=12.305&http_post=14.867"",  	""pid"": 72589,  	""plugin"": ""test"",  	""plugin_title"": ""Test Plugin"",  	""source"": ""Manual (admin)"",  	""text"": ""Job completed successfully on joeretina.local: Test Event 2 http://localhost:3012/#JobDetails?id=jihuyalli01"",  	""time_end"": 1449431930.628,  	""time_start"": 1449431840.214  }  ```    In addition to `job_start` and `job_complete`, there is one other special hook action that may be sent, and that is `job_launch_failure`.  This happens if a scheduled event completely fails to start a job, due to an unrecoverable error (such as an unavailable target server or group).  In this case the `code` property will be non-zero, and the `description` property will contain a summary of the error.    Only a small subset of the properties shown above will be included with a `job_launch_failure`, as a job object was never successfully created, so there will be no `hostname`, `pid`, `elapsed`, `log_file_size`, etc.    To include custom HTTP request headers with your web hook, append them onto the end of the URL using this format: `[header: My-Header: My-Value]`.  Make sure to include a space before the opening bracket.  Example URL:    ```  https://myserver.com/api/chat.postMessage [header: My-Header: My-Value]  ```    #### Event Resource Limits    ![Resource Limits Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/edit-event-res-limits-new.png)    Cronicle can automatically limit the server resource consumption of your jobs, by monitoring their CPU, memory and/or log file size, and aborting them if your limits are exceeded.  You can also specify ""sustain"" times for CPU and memory, so no action is taken until the limits are exceeded for a certain amount of time.    CPU and RAM usage are measured every 10 seconds, by looking at the process spawned for the job, *and any child processes that may have also been spawned by your code*.  So if you fork your own child subprocess, or shell out to a command-line utility, all the memory is totaled up, and compared against the resource limits for the job.    #### Event Notes    Event notes are for your own internal use.  They are displayed to users when editing an event, and in all e-mails regarding successful or failed jobs.  For example, you could use this to describe the event to members of your team who may not be familiar, and possibly provide a link to other documentation.  There is no character limit, so knock yourself out.    #### Run Now    To run an event immediately, click the **Run Now** button.  This will run the current event regardless of its timing settings, and whether the event is enabled or disabled in the schedule.  This is simply an on-demand job which is created and executed right away.    If you need to customize the internal clock for the job, hold Shift which clicking the Run Now button.  This will bring up a dialog allowing you to set the date and time which the Plugin will see as the ""current time"", for your on-demand job only.  It will not affect any other jobs or the event itself.  This is useful for re-running past jobs with a Plugin that honors the `now` timestamp in the job data.    ## Completed Jobs Tab    ![Completed Jobs Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/completed-jobs.png)    This tab shows you all recently completed jobs, for all events, and whether they succeeded or failed.  Cronicle will keep up to [list_row_max](#list_row_max) job completions in storage (default is 10,000).  The jobs are sorted by completion date/time, with the latest at the top.  Use the pagination controls on the top right to jump further back in time.  The table columns are:    | Column | Description |  |--------|-------------|  | **Job ID** | A unique ID assigned to the job.  Click this to see details (see [Job Details Tab](#job-details-tab) below). |  | **Event Name** | The name of the scheduled event for the job.  Click this to see the event history (see [Event History Tab](#event-history-tab) below). |  | **Category** | The category to which the event is assigned. |  | **Plugin** | The Plugin which was used to run the job. |  | **Hostname** | The hostname of the server which ran the job. |  | **Result** | This shows whether the job completed successfully, or returned an error. |  | **Start Date/Time** | The date/time when the job first started. |  | **Elapsed Time** | The total elapsed time of the job (including any retries). |    ### Event History Tab    ![Event History Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/event-history.png)    This tab shows you all recently completed jobs for one specific event.  Cronicle will keep up to [list_row_max](#list_row_max) job completions in storage (default is 10,000).  The jobs are sorted by completion date/time, with the latest at the top.  Use the pagination controls on the top right to jump further back in time.  The table columns are:    | Column | Description |  |--------|-------------|  | **Job ID** | A unique ID assigned to the job.  Click this to see details (see [Job Details Tab](#job-details-tab) below). |  | **Hostname** | The hostname of the server which ran the job. |  | **Result** | This shows whether the job completed successfully, or returned an error. |  | **Start Date/Time** | The date/time when the job first started. |  | **Elapsed Time** | The total elapsed time of the job (including any retries). |  | **Avg CPU** | The average CPU percentage used by the job process (including any subprocesses), where 100% equals one CPU core. |  | **Avg Mem** | The average memory used by the job process (including any subprocesses). |    ### Event Stats Tab    ![Event Stats Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/event-stats.png)    This tab contains statistics about a specific event, including basic information and performance graphs.  The data is calculated from the last 50 completed jobs.  Here is a list of all the stats that are displayed at the top of the screen:    | Statistic | Description |  |-----------|-------------|  | **Event Name** | The name of the event being displayed. |  | **Category Name** | The category to which the event is assigned. |  | **Event Timing** | The timing settings for the event (daily, hourly, etc.). |  | **Username** | The username of the user who first created the event. |  | **Plugin Name** | The Plugin selected to run jobs for the event. |  | **Event Target** | The server group or individual server selected to run jobs for the event. |  | **Avg. Elapsed** | The average elapsed time of jobs for the event. |  | **Avg. CPU** | The average CPU used by jobs for the event. |  | **Avg. Memory** | The average RAM used by jobs for the event. |  | **Success Rate** | The success percentage rate of jobs for the event. |  | **Last Result** | Shows the result of the latest job completion (success or fail). |  | **Avg. Log Size** | The average log file size of jobs for the event. |    Below the stats are a number of graphs:    ![Graphs Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/event-stats-graphs.png)    The first graph shows performance of your Plugin's metrics over time.  The different categories shown are entirely driven by your custom code.  You can choose to provide performance metrics or not, and add as many custom categories as you like.  For details, see the [Writing Plugins](#writing-plugins) and [Performance Metrics](#performance-metrics) sections below.    Below the performance history graph are the **CPU Usage History** and **Memory Usage History** graphs.  These display your event's server resource usage over time.  Each dot on the graph is a particular job run, and the history goes back for 50 runs, if available.    ## Job Details Tab    ![Job In Progress Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/job-live-progress.png)    The Job Details Tab is used to view jobs currently in progress, and to see details about completed jobs.  The display is slightly different for each case.  For jobs in progress, you'll see some statistics about the job which are updated live (see below), and three large donut charts: The current job progress (if provided by the Plugin), and the current CPU and memory usage of the job.  Below the donuts you'll find the live job log file, updated in real-time.  The stats consist of:    | Statistic | Description |  |-----------|-------------|  | **Job ID** | A unique ID assigned to the job. |  | **Event Name** | The name of the event being displayed. |  | **Event Timing** | The timing settings for the event (daily, hourly, etc.). |  | **Category Name** | The category to which the event is assigned. |  | **Plugin Name** | The Plugin selected to run jobs for the event. |  | **Event Target** | The server group or individual server selected to run jobs for the event. |  | **Job Source** | The source of the job (who started it, the scheduler or manually by hand). |  | **Server Hostname** | The hostname of the server which is running the job. |  | **Process ID** | The ID of the process currently running the job. |  | **Job Started** | The date/time of when the job first started. |  | **Elapsed Time** | The current elapsed time of the job. |  | **Remaining Time** | The estimated remaining time (if available). |    The CPU donut chart visually indicates how close your job is to using a full CPU core.  If it uses more than that (i.e. multiple threads or sub-processes), the chart simply shows ""full"" (solid color).  The Memory donut visually indicates your job's memory vs. the ""maximum"" configured resource limit, or 1 GB if resource limits are not in effect for the job.  The colors are green if the donut is under 50% filled, yellow if between 50% and 75%, and red if over 75%.    When the job completes, or when viewing details about a previously completed job, the display looks slightly different:    ![Job Success Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/job-details-complete.png)    The only difference in the statistics table is that the right-hand column contains the **Job Completed** date/time instead of the remaining time.  Below that, the left-hand donut graph now shows performance metrics from your Plugin (if provided).  You can choose to provide performance metrics or not, and add as many custom categories as you like.  For details, see the [Writing Plugins](#writing-plugins) and [Performance Metrics](#performance-metrics) sections below.    The CPU and Memory donut charts now show the average values over the course of your job run, rather than the ""current"" values while the job is still running.  The same visual maximums and color rules apply (see above).    Here is how the Job Details tab looks when a job fails:    ![Job Failed Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/job-details-error.png)    The error message is displayed in a banner along the top of the screen.  The banner will be yellow if the job was aborted, or red if an error was returned from your Plugin.    In this case the job was aborted, so the Plugin had no chance to report performance metrics, hence the left-hand pie chart is blank.    ## My Account Tab    ![My Account Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/my-account.png)    On this tab you can edit your own account profile, i.e. change your name, e-mail address, and/or password.  You also have the option of completely deleting your account via the ""Delete Account"" button.  For security purposes, in order to save any changes you must enter your existing account password.    Your user avatar image is automatically pulled from the free [Gravatar.com](https://en.gravatar.com/) service, using your e-mail address.  To customize your image, please [login or create a Gravatar account](https://en.gravatar.com/connect/) using the same e-mail address as the one in your Cronicle account.    ## Administration Tab    This tab is only visible and accessible to administrator level users.  It allows you to view the global activity log, manage API keys, categories, Plugins, servers and users.  To access all the various functions, use the tabs along the left side of the page.    ### Activity Log Tab    ![Activity Log Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-activity-log.png)    All activity in Cronicle is logged, and viewable on the **Activity Log** tab.  The log is presented as a paginated table, sorted by descending date/time (newest activity at the top).  Pagination controls are located in the top-right corner.  The table columns are as follows:    | Column | Description |  |--------|-------------|  | **Date/Time** | The date and time of the activity (adjusted for your local timezone). |  | **Type** | The type of activity (error, event, category, server, etc.). |  | **Description** | A description of the activity (varies based on the type). |  | **Username** | The username or API Key associated with the activity, if applicable. |  | **IP Address** | The IP address associated with the activity, if applicable. |  | **Actions** | A set of actions to take on the activity (usually links to more info). |    There are several different types of activity that can appear in the activity log:    | Activity Type | Description |  |---------------|-------------|  | **Error** | Error messages, such as failing to send e-mail or server clocks out of sync. |  | **Warning** | Warning messages, such as a failure to launch a scheduled event. |  | **API Key** | API Key related activity, such as a user creating, modifying or deleting keys. |  | **Category** | Category related activity, such as a user creating, modifying or deleting categories. |  | **Event** | Event related activity, such as a user creating, modifying or deleting events. |  | **Group** | Server Group related activity, such as a user creating, modifying or deleting groups. |  | **Job** | Job related activity, including every job completion (success or fail). |  | **Plugin** | Plugin related activity, such as a user creating, modifying or deleting Plugins. |  | **Server** | Multi-Server related activity, such as servers being added or removed from the cluster. |  | **Scheduler** | Scheduler related activity, such as the scheduler being enabled or disabled. |  | **User** | User related activity, such as a user being created, modified or deleted. |    Cronicle will keep the latest [list_row_max](#list_row_max) activity log entries in storage (the default is 10,000).    ### API Keys Tab    ![API Keys Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-api-keys.png)    [API Keys](#api-keys) allow you to register external applications or services to use the REST API.  This tab lists all the API Keys registered in the system, and allows you to edit, delete and add new keys.  The table columns include:    | Column | Description |  |--------|-------------|  | **App Title** | The application title associated with the key. |  | **API Key** | The API Key itself, which is a 32-character hexadecimal string. |  | **Status** | The status of the key, which can be active or disabled. |  | **Author** | The username who originally created the key. |  | **Created** | The date when the key was created. |  | **Actions** | A list of actions to take (edit and delete). |    When you create or edit a key, you will see this screen:    ![Editing API Key Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-api-keys-edit-2.png)    The API Key form contains the following elements:    - The **API Key** itself, which is an automatically generated 32-character hexadecimal string.  You can manually customize this if desired, or click **Generate Random** to generate a new random key.  - The **Status** which is either `Active` or `Disabled`.  Disable an API Key if you have a misbehaving app or an exposed key, and all API calls will be rejected.  Only active keys are allowed to make any calls.  - The **App Title** which is the name of your app that will be using the key.  This is displayed in various places including the activity log.  - An **App Description** which is an optional verbose description of your app, just so other users can understand what the purpose of each API key is.  - A set of **Privileges**, which grant the API Key access to specific features of Cronicle.  This is the same system used to grant user level permissions.    For more details on how to use the API Key system in your apps, see the [API Keys](#api-keys) section below.    ### Categories Tab    Events can be assigned to custom categories that you define.  This is a great way to enable/disable groups of events at once, set a maximum concurrent job limit for the entire category, set default notification and resource limits, and highlight events with a specific color.    ![Categories Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-categories.png)    On this screen is the list of all event categories.  The table columns are as follows:    | Column | Description |  |--------|-------------|  | **Title** | The title of the category, used for display purposes. |  | **Description** | An optional description of the category. |  | **Assigned Events** | The number of events assigned to the category. |  | **Max Concurrent** | The maximum number of concurrent jobs to allow. |  | **Actions** | A list of actions to take (edit and delete). |    When you create or edit a category, you are taken to this screen:    ![Edit Category Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-category-edit.png)    The category edit form contains the following elements:    - The **Category ID** which is an auto-generated alphanumeric ID used only by the API.  This cannot be changed.  - The **Category Title** which is used for display purposes.  - The **Status** which controls whether the category is active or disabled.  This also affects all events assigned to the category, meaning if the category is disabled, all the events assigned to it will also be effectively disabled as well (the scheduler won't start jobs for them).  - An optional **Description** of the category, for your own use.  - A **Max Concurrent** selector, which allows you to set the maximum allowed jobs to run concurrently across all events in the category.  This is not a default setting that is applied per event, but rather a total overall limit for all assigned events.  - An optional **Highlight Color** which is displayed as the background color on the main [Schedule Tab](#schedule-tab) for all events assigned to the category.  - A set of **Default Notification Options**, which include separate e-mail lists for successful and failed jobs, and a default [Web Hook](#event-web-hook) URL.  These settings may be overridden per each event.  - A set of **Default Resource Limits**, which include separate CPU and Memory limits.  Note that these are measured per job, and not as a category total.  These settings may be overridden per each event (see [Event Resource Limits](#event-resource-limits) for more details).    ### Plugins Tab    [Plugins](#plugins) are used to run jobs, and can be written in virtually any language.  They are spawned as sub-processes, launched via a custom command-line script that you provide.  Communication is achieved via reading and writing JSON to STDIN / STDOUT.  For more details, see the [Plugins](#plugins) section below.    ![Plugins Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-plugins.png)    On this screen you'll find a list of all the Plugins registered in the system.  The table columns are as follows:    | Column | Description |  |--------|-------------|  | **Plugin Name** | The name of the Plugin, used for display purposes. |  | **Author** | The username of the user who originally created the Plugin. |  | **Number of Events** | The number of events using the Plugin. |  | **Created** | The date when the Plugin was first created. |  | **Modified** | The date when the Plugin was last modified. |  | **Actions** | A list of actions to take (edit and delete). |    When you create or edit a Plugin, you are taken to this screen:    ![Edit Plugin Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-plugin-edit.png)    The Plugin edit form contains the following elements:    - The **Plugin ID** which is an auto-generated alphanumeric ID used only by the API.  This cannot be changed.  - The **Plugin Name** which is used for display purposes.  - The **Status** which controls whether the Plugin is active or disabled.  This also affects all events assigned to the Plugin, meaning if the Plugin is disabled, all the events assigned to it will also be effectively disabled as well (the scheduler won't start jobs for them).  - The **Executable** path, which is executed in a sub-process to run jobs.  You may include command-line arguments here if your Plugin requires them, but no shell redirects or pipes (see the [Shell Plugin](#built-in-shell-plugin) if you need those).  - A set of custom **Parameters** which you can define here, and then edit later for each event.  These parameters are then passed to your Plugin when jobs run. See below for details.  - A set of **Advanced Options**, including customizing the working directory, and user/group (all optional).  See below for details.    #### Plugin Parameters    The Parameter system allows you to define a set of UI controls for your Plugin (text fields, text boxes, checkboxes, drop-down menus, etc.) which are then presented to the user when editing events.  This can be useful if your Plugin has configurable behavior which you want to expose to people creating events in the schedule.  Each Parameter has an ID which identifies the user's selection, and is included in the JSON data sent to your Plugin for each job.    When adding or editing Plugin Parameters, you will be presented with this dialog:    ![Edit Plugin Param Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-plugin-edit-param.png)    Here you will need to provide:    - A **Parameter ID** which uniquely identifies the parameter in the JSON data sent to your Plugin.  - A **Label** for display purposes (used in the UI only).  - A **Control Type** selection, which can be a text field, a text box, a checkbox, a drop-down menu, or a hidden variable.    Depending on the control type selected, you may need to provide different pieces of information to render the control in the UI, such as a comma-separated list of values for a menu, a default text field value and size, etc.    Parameter values for each event are passed to your Plugin when jobs are launched.  These arrive as a JSON object (located in `params`), as well as upper-case [environment variables](https://en.wikipedia.org/wiki/Environment_variable).  So for example, you can use Plugin Parameters to set custom environment variables for your jobs, or override existing ones such as `PATH`.  Example of this:    ![Edit Plugin PATH Example](https://pixlcore.com/software/cronicle/screenshots-new/admin-plugin-edit-path.png)    For more details on how to use these parameters in your Plugin code, see the [Plugins](#plugins) section below.    #### Advanced Plugin Options    The **Advanced** options pane expands to the following:    ![Edit Plugin Advanced Panel](https://pixlcore.com/software/cronicle/screenshots-new/admin-plugin-edit-advanced-uid.png)    Here you can customize the following settings:    | Option | Description |  |--------|-------------|  | **Working Directory** | This is the directory path to set as the [current working directory](https://en.wikipedia.org/wiki/Working_directory) when your jobs are launched.  It defaults to the Cronicle base directory (`/opt/cronicle`). |  | **Run as User** | You can optionally run your jobs as another user by entering their UID here.  A username string is also acceptable (and recommended, as UIDs may differ between servers). |    ### Servers Tab    ![Servers Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-servers.png)    When Cronicle is configured to run in a multi-server environment, this tab allows you to manage the cluster.  At the top of the page you'll see a list of all the servers currently registered, their status, and controls to restart or shut them down.  The table columns include:    | Column | Description |  |--------|-------------|  | **Hostname** | The hostname of the server. |  | **IP Address** | The IP address of the server. |  | **Groups** | A list of the groups to which the server belongs. |  | **Status** | The current server status (primary, backup or worker). |  | **Active Jobs** | The number of active jobs currently running on the server. |  | **Uptime** | The elapsed time since Cronicle was restarted on the server. |  | **CPU** | The current CPU usage on the server (all jobs). |  | **Memory** | The current memory usage on the server (all jobs). |  | **Actions** | A list of actions to take (restart and shutdown). |    There is also an **Add Server** button, but note that servers on the same LAN should be automatically discovered and added to the cluster.  You will only need to manually add a server if it lives in a remote location, outside of local UDP broadcast range.    #### Server Groups    Below the server cluster you'll find a list of server groups.  These serve two purposes.  First, you can define groups in order to target events at them.  For example, an event can target the group of servers instead of an individual server, and one of the servers will be picked for each job (or, if [Multiplex](#multiplexing) is enabled, all the servers at once).  Second, you can use server groups to define which of your servers are eligible to become the primary server, if the current primary is shut down.    When Cronicle is first installed, two server groups are created by default.  A ""Primary Group"" which contains only the current (primary) server, and an ""All Servers"" group, which contains all the servers (current and future).  Groups automatically add servers by a hostname-based regular expression match.  Therefore, when additional servers join the cluster, they will be assigned to groups automatically via their hostname.    The list of server groups contains the following columns:    | Column | Description |  |--------|-------------|  | **Title** | The title of the server group. |  | **Hostname Match** | A hostname regular expression to automatically add servers to the group. |  | **Number of Servers** | The number of servers currently in the group. |  | **Number of Events** | The number of events currently targeting the group. |  | **Class** | The server group classification (whether it supports becoming primary or not). |  | **Actions** | A list of actions to take (edit and delete). |    When adding or editing a server group, you will be presented with this dialog:    ![Edit Server Group Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-servers-group-edit.png)    Here you will need to provide:    - A **Group Title** which is used for display purposes.  - A **Hostname Match** which is a regular expression applied to every server hostname (used to automatically add servers to the group).  - A **Server Class** which sets the servers in your group as ""Primary Eligible"" or ""Worker Only"".    Note that ""Primary Eligible"" servers all need to be properly configured and have access to your storage back-end.  Meaning, if you opted to use the filesystem, you'll need to make sure it is mounted (via NFS or similar mechanism) on all the servers who could become primary.  Or, if you opted to use a NoSQL DB such as Couchbase or S3, they need all the proper settings and/or credentials to connect.  For more details, see the [Multi-Server Cluster](#multi-server-cluster) section.    ### Users Tab    ![Users Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-users.png)    This tab shows you all the user accounts registered in the system, and allows you to edit, delete and add new ones.  The table columns are as follows:    | Column | Description |  |--------|-------------|  | **Username** | The account username. |  | **Full Name** | The user's full name. |  | **Email Address** | The user's e-mail address. |  | **Status** | The account status (active or disabled). |  | **Type** | The account type (standard or administrator). |  | **Created** | The date when the user was first created. |  | **Actions** | A list of actions to take (edit and delete). |    When you create or edit a user, you are taken to this screen:    ![Edit User Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-users-edit.png)    The user edit form contains the following elements:    - The **Username**, which cannot be edited after the user is created.  Usernames may contain alphanumeric characters, periods and dashes.  - The **Account Status** which is either `Active` or `Disabled`.  Disabled accounts cannot login.  - The user's **Full Name**, which is used for display purposes.  - The user's **Email Address**, which is used for event and error notifications.  - The account **Password**, which can be reset here, as well as randomized.  - A set of **Privileges**, which grant the user access to specific features of Cronicle.    # Plugins    Plugins handle running your events, and reporting status back to the Cronicle daemon.  They can be written in virtually any language, as they are really just command-line executables.  Cronicle spawns a sub-process for each job, executes a command-line you specify, and then uses [pipes](https://en.wikipedia.org/wiki/Pipeline_%28Unix%29) to pass in job information and retrieve status, all in JSON format.    So you can write a Plugin in your language of choice, as long as it can read and write JSON.  Also, Cronicle ships with a built-in Plugin for handling shell scripts, which makes things even easier if you just have some simple shell commands to run, and don't want to have to deal with JSON at all.  See [Shell Plugin](#built-in-shell-plugin) below for more on this.    ## Writing Plugins    To write your own Plugin, all you need is to provide a command-line executable, and have it read and write JSON over [STDIN and STDOUT](https://en.wikipedia.org/wiki/Standard_streams).  Information about the current job is passed as a JSON document to your STDIN, and you can send back status updates and completion events simply by writing JSON to your STDOUT.    Please note that regardless of your choice of programming language, your Plugin needs to be a real command-line executable.  So it needs to have execute file permissions (usually `0755`), and a [shebang](https://en.wikipedia.org/wiki/Shebang_%28Unix%29) line at the top of the file, indicating which interpreter to use.  For example, if you write a Node.js Plugin, you need something like this at the very top of your `.js` file:    ```  	#!/usr/bin/node  ```    The location of the `node` binary may vary on your servers.    ### JSON Input    As soon as your Plugin is launched as a sub-process, a JSON document is piped to its STDIN stream, describing the job.  This will be compacted onto a single line followed by an EOL, so you can simply read a line, and not have to worry about locating the start and end of the JSON.  Here is an example JSON document (pretty-printed here for display purposes):    ```js  {  	""id"": ""jihuxvagi01"",  	""hostname"": ""joeretina.local"",  	""command"": ""/usr/local/bin/my-plugin.js"",  	""event"": ""3c182051"",  	""now"": 1449431125,  	""log_file"": ""/opt/cronicle/logs/jobs/jihuxvagi01.log"",  	""params"": {  		""myparam1"": ""90"",  		""myparam2"": ""Value""  	}  }  ```    There may be other properties in the JSON (many are copied over from the event), but these are the relevant ones:    | Property Name | Description |  |---------------|-------------|  | `id` | A unique alphanumeric ID assigned to the job. |  | `hostname` | The hostname of the server currently processing the job. |  | `command` | The exact command that was executed to start your Plugin. |  | `event` | The Event ID of the event that fired off the job. |  | `now` | A Unix timestamp representing the ""current"" time for the job (very important -- see below). |  | `log_file` | A fully-qualified filesystem path to a unique log file specifically for the job, which you can use. |  | `params` | An object containing your Plugin's custom parameters, filled out with values from the Event Editor. |    The `now` property is special.  It may or may not be the ""current"" time, depending on how and why the job started.  Meaning, if the associated event is set to [Run All Mode](#run-all-mode) it is possible that a previous job couldn't run for some reason, and Cronicle is now ""catching up"" by running missed jobs.  In this situation, the `now` property will be set to the time when the event *should have* run.  For example, if you have a Plugin that generates daily reports, you'll need to know *which day* to run the report for.  Normally this will be the current date, but if a day was missed for some reason, and Cronicle is catching up by running yesterday's job, you should use the `now` time to determine which day's data to pull for your report.    The `log_file` is designed for your Plugin's own use.  It is a unique file created just for the job, and its contents are displayed in real-time via the Cronicle UI on the [Job Details Tab](#job-details-tab).  You can use any log format you want, just make sure you open the file in ""append"" mode (it contains a small header written by the daemon).  Note that you can also just print to STDOUT or STDERR, and these are automatically appended to the log file for you.    The `params` object will contain all the custom parameter keys defined when you created the Plugin (see the [Plugins Tab](#plugins-tab) section), and values populated from the event editor, when the Plugin was selected for the event.  The keys should match the parameter IDs you defined.    ### JSON Output    Your Plugin is expected to write JSON to STDOUT in order to report status back to the Cronicle daemon.  At the very least, you need to notify Cronicle that the job was completed.  This is done by printing a JSON object with a `complete` property set to `1` (or any true value).  You need to make sure the JSON is compacted onto a single line, and ends with a single EOL character (`\n` on Unix).  Example:    ```js  { ""complete"": 1 }  ```    This tells Cronicle that the job was completed, and your process is about to exit.  By default, the job is considered a success.  However, if the job failed and you need to report an error, you must include a `code` property set to any non-zero error code you want, and a `description` property set to a custom error message.  Include these along with the `complete` property in the JSON.  Example:    ```js  { ""complete"": 1, ""code"": 999, ""description"": ""Failed to connect to database."" }  ```    Your error code and description will be displayed on the [Job Details Tab](#job-details-tab), and in any e-mail notifications and/or web hooks sent out for the event completion.    If your Plugin writes anything other than JSON to STDOUT (or STDERR), it is automatically appended to your log file.  This is so you don't have to worry about using existing code or utilities that may emit some kind of output.  Cronicle is very forgiving in this regard.    #### Reporting Progress    In addition to reporting success or failure at the end of a job, you can also optionally report progress at custom intervals while your job is running.  This is how Cronicle can display its visual progress meter in the UI, as well as calculate the estimated time remaining.  To update the progress of a job, simply print a JSON document with a `progress` property, set to a number between `0.0` and `1.0`.  Example:    ```js  { ""progress"": 0.5 }  ```    This would show progress at 50% completion, and automatically calculate the estimated time remaining based on the duration and progress so far.  You can repeat this as often as you like, with as granular progress as you can provide.    #### Performance Metrics    You can optionally include performance metrics at the end of a job, which are displayed as a pie chart on the [Job Details Tab](#job-details-tab).  These metrics can consist of any categories you like, and the JSON format is a simple `perf` object where the values represent the amount of time spent in seconds.  Example:    ```js  { ""perf"": { ""db"": 18.51, ""http"": 3.22, ""gzip"": 0.84 } }  ```    The perf keys can be anything you want.  They are just arbitrary categories you can make up, which represent how your Plugin spent its time during the job.    Cronicle accepts a number of different formats for the perf metrics, to accommodate various performance tracking libraries.  For example, you can provide the metrics in query string format, like this:    ```js  { ""perf"": ""db=18.51&http=3.22&gzip=0.84"" }  ```    If your metrics include a `total` (or `t`) in addition to other metrics, this is assumed to represent the total time, and will automatically be excluded from the pie chart (but included in the performance history graph).    If you track metrics in units other than seconds, you can provide the `scale`.  For example, if your metrics are all in milliseconds, just set the `scale` property to `1000`.  Example:    ```js  { ""perf"": { ""scale"": 1000, ""db"": 1851, ""http"": 3220, ""gzip"": 840 } }  ```    The slightly more complex format produced by our own [pixl-perf](https://www.npmjs.com/package/pixl-perf) library is also supported.    ##### Nested Metrics    In order for the pie chart to be accurate, your perf metrics must not overlap each other.  Each metric should represent a separate period of time.  Put another way, if all the metrics were added together, they should equal the total time.  To illustrate this point, consider the following ""bad"" example:    ```js  { ""perf"": { ""database"": 18.51, ""run_sql_query"": 3.22, ""connect_to_db"": 0.84 } }  ```    In this case the Plugin is tracking three different metrics, but the `database` metric encompasses *all* database related activities, including the `run_sql_query` and `connect_to_db`.  So the `database` metric overlaps the others.  Cronicle has no way of knowing this, so the pie chart would be quite inaccurate, because the three metrics do not add up to the total time.    However, if you want to track nested metrics as well as a parent metric, just make sure you prefix your perf keys properly.  In the above example, all you would need to do is rename the keys like this:    ```js  { ""perf"": { ""db"": 18.51, ""db_run_sql"": 3.22, ""db_connect"": 0.84 } }  ```    Cronicle will automatically detect that the `db` key is used as a prefix in the other two keys, and it will be omitted from the pie chart.  Only the nested `db_run_sql` and `db_connect` keys will become slices of the pie, as they should add up to the total in this case.    Note that *all* the metrics are included in the performance history graph, as that is a line graph, not a pie chart, so it doesn't matter that everything add up to the total time.    #### Changing Notification Settings    Notification settings for the job are configured in the UI at the event level, and handled automatically after your Plugin exits.  E-mail addresses may be entered for both successful and failure results.  However, your Plugin running the job can alter these settings on-the-fly.    For example, if you only want to send a successful e-mail in certain cases, and want to disable it based on some outcome from inside the Plugin, just print some JSON to STDOUT like this:    ```js  { ""notify_success"": """" }  ```    This will disable the e-mail that is normally sent upon success.  Similarly, if you want to disable the failure e-mail, print this to STDOUT:    ```js  { ""notify_fail"": """" }  ```    Another potential use of this feature is to change who gets e-mailed, based on a decision made inside your Plugin.  For example, you may have multiple error severity levels, and want to e-mail a different set of people for the really severe ones.  To do that, just specify a new set of e-mail addresses in the `notify_fail` property:    ```js  { ""notify_fail"": ""emergency-ops-pager@mycompany.com"" }  ```    These JSON updates can be sent as standalone records as shown here, at any time during your job run, or you can batch everything together at the very end:    ```js  { ""complete"": 1, ""code"": 999, ""description"": ""Failed to connect to database."", ""perf"": { ""db"": 18.51, ""db_run_sql"": 3.22, ""db_connect"": 0.84 }, ""notify_fail"": ""emergency-ops-pager@mycompany.com"" }  ```    #### Chain Reaction Control    You can enable or disable [Chain Reaction](#chain-reaction) mode on the fly, by setting the `chain` property in your JSON output.  This allows you to designate another event to launch as soon as the current job completes, or to clear the property (set it to false or a blank string) to disable Chain Reaction mode if it was enabled in the UI.    To enable a chain reaction, you need to know the Event ID of the event you want to trigger.  You can determine this by editing the event in the UI and copy the Event ID from the top of the form, just above the title.  Then you can specify the ID in your jobs by printing some JSON to STDOUT like this:    ```js  { ""chain"": ""e29bf12db"" }  ```    Remember that your job must complete successfully in order to trigger the chain reaction, and fire off the next event.  However, if you want to run a event only on job failure, set the `chain_error` property instead:    ```js  { ""chain_error"": ""e29bf12db"" }  ```    You set both the `chain` and `chain_error` properties, to run different events on success / failure.    To disable chain reaction mode, set the `chain` and `chain_error` properties to false or empty strings:    ```js  { ""chain"": """", ""chain_error"": """" }  ```    ##### Chain Data    When a chained event runs, some additional information is included in the initial JSON job object sent to STDIN:    | Property Name | Description |  |---------------|-------------|  | `source_event` | The ID of the original event that started the chain reaction. |  | `chain_code` | The error code from the original job, or `0` for success. |  | `chain_description` | The error description from the original job, if applicable. |  | `chain_data` | Custom user data, if applicable (see below). |    You can pass custom JSON data to the next event in the chain, when using a [Chain Reaction](#chain-reaction) event.  Simply specify a JSON property called `chain_data` in your JSON output, and pass in anything you want (can be a complex object / array tree), and the next event will receive it.  Example:    ```js  { ""chain"": ""e29bf12db"", ""chain_data"": { ""custom_key"": ""foobar"", ""value"": 42 } }  ```    So in this case when the event `e29bf12db` runs, it will be passed your `chain_data` object as part of the JSON sent to it when the job starts.  The Plugin code running the chained event can access the data by parsing the JSON and grabbing the `chain_data` property.    #### Custom Data Tables    If your Plugin produces statistics or other tabular data at the end of a run, you can have Cronicle render this into a table on the Job Details page.  Simply print a JSON object with a property named `table`, containing the following keys:    | Property Name | Description |  |---------------|-------------|  | `title` | Optional title displayed above the table, defaults to ""Job Stats"". |  | `header` | Optional array of header columns, displayed in shaded bold above the main data rows. |  | `rows` | **Required** array of rows, with each one being its own inner array of column values. |  | `caption` | Optional caption to show under the table (centered, small gray text). |    Here is an example data table.  Note that this has been expanded for documentation purposes, but in practice your JSON needs to be compacted onto a single line when printed to STDOUT.    ```js  {  	""table"": {  		""title"": ""Sample Job Stats"",  		""header"": [  			""IP Address"", ""DNS Lookup"", ""Flag"", ""Count"", ""Percentage""  		],  		""rows"": [  			[""62.121.210.2"", ""directing.com"", ""MaxEvents-ImpsUserHour-DMZ"", 138, ""0.0032%"" ],  			[""97.247.105.50"", ""hsd2.nm.comcast.net"", ""MaxEvents-ImpsUserHour-ILUA"", 84, ""0.0019%"" ],  			[""21.153.110.51"", ""grandnetworks.net"", ""InvalidIP-Basic"", 20, ""0.00046%"" ],  			[""95.224.240.69"", ""hsd6.mi.comcast.net"", ""MaxEvents-ImpsUserHour-NM"", 19, ""0.00044%"" ],  			[""72.129.60.245"", ""hsd6.nm.comcast.net"", ""InvalidCat-Domestic"", 17, ""0.00039%"" ],  			[""21.239.78.116"", ""cable.mindsprung.com"", ""InvalidDog-Exotic"", 15, ""0.00037%"" ],  			[""172.24.147.27"", ""cliento.mchsi.com"", ""MaxEvents-ClicksPer"", 14, ""0.00035%"" ],  			[""60.203.211.33"", ""rgv.res.com"", ""InvalidFrog-Croak"", 14, ""0.00030%"" ],  			[""24.8.8.129"", ""dsl.att.com"", ""Pizza-Hawaiian"", 12, ""0.00025%"" ],  			[""255.255.1.1"", ""favoriteisp.com"", ""Random-Data"", 10, ""0%"" ]  		],  		""caption"": ""This is an example stats table you can generate from within your Plugin code.""  	}  }  ```    This would produce a table like the following:    ![Custom Stats Table Example](https://pixlcore.com/software/cronicle/screenshots-new/job-details-custom-stats.png)    #### Custom HTML Content    If you would prefer to generate your own HTML content from your Plugin code, and just have it rendered into the Job Details page, you can do that as well.  Simply print a JSON object with a property named `html`, containing the following keys:    | Property Name | Description |  |---------------|-------------|  | `title` | Optional title displayed above the section, defaults to ""Job Report"". |  | `content` | **Required** Raw HTML content to render into the page. |  | `caption` | Optional caption to show under your HTML (centered, small gray text). |    Here is an example HTML report.  Note that this has been expanded for documentation purposes, but in practice your JSON needs to be compacted onto a single line when printed to STDOUT.    ```js  {  	""html"": {  		title: ""Sample Job Report"",  		content: ""This is <b>HTML</b> so you can use <i>styling</i> and such."",  		caption: ""This is a caption displayed under your HTML content.""  	}  }  ```    If your Plugin generates plain text instead of HTML, you can just wrap it in a `<pre>` block, which will preserve formatting such as whitespace.    #### Updating The Event    Your job can optionally trigger an event update when it completes.  This can be used to do things such as disable the event (remove it from the schedule) in response to a catastrophic error, or change the event's timing, change the server or group target, and more.    To update the event for a job, simply include an `update_event` object in your Plugin's JSON output, containing any properties from the [Event Data Format](#event-data-format).  Example:    ```js  {  	""update_event"": {  		""enabled"": 0  	}  }  ```    This would cause the event to be disabled, so the schedule would no longer launch it.  Note that you can only update the event once, and it happens at the completion of your job.    ### Job Environment Variables    When processes are spawned to run jobs, your Plugin executable is provided with a copy of the current environment, along with the following custom environment variables:    | Variable | Description |  |----------|-------------|  | `$CRONICLE` | The current Cronicle version, e.g. `1.0.0`. |  | `$JOB_ALGO` | Specifies the algorithm that was used for picking the server from the target group. See [Algorithm](#algorithm). |  | `$JOB_CATCH_UP` | Will be set to `1` if the event has [Run All Mode](#run-all-mode) mode enabled, `0` otherwise. |  | `$JOB_CATEGORY_TITLE` | The Category Title to which the event is assigned.  See [Categories Tab](#categories-tab). |  | `$JOB_CATEGORY` | The Category ID to which the event is assigned.  See [Categories Tab](#categories-tab). |  | `$JOB_CHAIN` | The chain reaction event ID to launch if job completes successfully.  See [Chain Reaction](#chain-reaction). |  | `$JOB_CHAIN_ERROR` | The chain reaction event ID to launch if job fails.  See [Chain Reaction](#chain-reaction). |  | `$JOB_COMMAND` | The command-line executable that was launched for the current Plugin. |  | `$JOB_CPU_LIMIT` | Limits the CPU to the specified percentage (100 = 1 core), abort if exceeded. See [Event Resource Limits](#event-resource-limits). |  | `$JOB_CPU_SUSTAIN` | Only abort if the CPU limit is exceeded for this many seconds. See [Event Resource Limits](#event-resource-limits). |  | `$JOB_DETACHED` | Specifies whether [Detached Mode](#detached-mode) is enabled or not. |  | `$JOB_EVENT_TITLE` | A display name for the event, shown on the [Schedule Tab](#schedule-tab) as well as in reports and e-mails. |  | `$JOB_EVENT` | The ID of the event that launched the job. |  | `$JOB_HOSTNAME` | The hostname of the server chosen to run the current job. |  | `$JOB_ID` | The unique alphanumeric ID assigned to the job. |  | `$JOB_LOG` | The filesystem path to the job log file, which you can append to if you want.  However, STDOUT or STDERR are both piped to the log already. |  | `$JOB_MEMORY_LIMIT` | Limits the memory usage to the specified amount, in bytes. See [Event Resource Limits](#event-resource-limits). |  | `$JOB_MEMORY_SUSTAIN` | Only abort if the memory limit is exceeded for this many seconds. See [Event Resource Limits](#event-resource-limits). |  | `$JOB_MULTIPLEX` | Will be set to `1` if the event has [Multiplexing](#multiplexing) enabled, `0` otherwise. |  | `$JOB_NOTES` | Text notes saved with the event, included in e-mail notifications. See [Event Notes](#event-notes). |  | `$JOB_NOTIFY_FAIL` | List of e-mail recipients to notify upon job failure (CSV). See [Event Notification](#event-notification). |  | `$JOB_NOTIFY_SUCCESS` | List of e-mail recipients to notify upon job success (CSV). See [Event Notification](#event-notification). |  | `$JOB_NOW` | The Epoch timestamp of the job time (may be in the past if re-running a missed job). |  | `$JOB_PLUGIN_TITLE` | The Plugin Title for the associated event. |  | `$JOB_PLUGIN` | The Plugin ID for the associated event. |  | `$JOB_RETRIES` | The number of retries to allow before reporting an error. See [Event Retries](#event-retries). |  | `$JOB_RETRY_DELAY` | Optional delay between retries, in seconds. See [Event Retries](#event-retries). |  | `$JOB_SOURCE` | String representing who launched the job, will be `Scheduler` or `Manual (USERNAME)`. |  | `$JOB_STAGGER` | If [Multiplexing](#multiplexing) is enabled, this specifies the number of seconds to wait between job launches. |  | `$JOB_TIME_START` | The starting time of the job, in Epoch seconds. |  | `$JOB_TIMEOUT` | The event timeout (max run time) in seconds, or `0` if no timeout is set. |  | `$JOB_TIMEZONE` | The timezone for interpreting the event timing settings. Needs to be an [IANA timezone string](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).  See [Event Timing](#event-timing). |  | `$JOB_WEB_HOOK` | An optional URL to hit for the start and end of each job. See [Event Web Hook](#event-web-hook). |    In addition, any [Plugin Parameters](#plugin-parameters) are also passed as environment variables.  The keys are converted to upper-case, as that seems to be the standard.  So for example, you can customize the `PATH` by declaring it as a Plugin Parameter:    ![Edit Plugin PATH Example](https://pixlcore.com/software/cronicle/screenshots-new/admin-plugin-edit-path.png)    You can also include inline variables in the parameter value itself, using the syntax `$VARNAME`.  So for example, if you wanted to *append* to the current `PATH` instead of having to set it from scratch, you could:    ![Edit Plugin PATH Inline Example](https://pixlcore.com/software/cronicle/screenshots-new/admin-plugin-edit-path-inline.png)    Please note that if you do overwrite the entire path, you must include the location of the Node.js `node` binary (typically in `/usr/bin` or `/usr/local/bin`).  Otherwise, things will not work well.    ## Sample Node Plugin    Here is a sample Plugin written in [Node.js](https://nodejs.org/):    ```js  #!/usr/bin/env node    var rl = require('readline').createInterface({ input: process.stdin });    rl.on('line', function(line) {  	// got line from stdin, parse JSON  	var job = JSON.parse(line);  	console.log(""Running job: "" + job.id + "": "" + JSON.stringify(job) );  	  	// Update progress at 50%  	setTimeout( function() {  		console.log(""Halfway there!"");  		process.stdout.write( JSON.stringify({ progress: 0.5 }) + ""\n"" );  	}, 5 * 1000 );  	  	// Write completion to stdout  	setTimeout( function() {  		console.log(""Job complete, exiting."");  		  		process.stdout.write( JSON.stringify({  			complete: 1,  			code: 0  		}) + ""\n"" );  		  	}, 10 * 1000 );  	  	// close readline interface  	rl.close();  });  ```    ## Sample Perl Plugin    Here is a sample Plugin written in [Perl](https://www.perl.org/), using the [JSON](http://search.cpan.org/perldoc?JSON) module:    ```perl  #!/usr/bin/env perl    use strict;  use JSON;    # set output autoflush  $| = 1;    # read line from stdin -- it should be our job JSON  my $line = <STDIN>;  my $job = decode_json($line);    print ""Running job: "" . $job->{id} . "": "" . encode_json($job) . ""\n"";    # report progress at 50%  sleep 5;  print ""Halfway there!\n"";  print encode_json({ progress => 0.5 }) . ""\n"";    sleep 5;  print ""Job complete, exiting.\n"";    # All done, send completion via JSON  print encode_json({  	complete => 1,  	code => 0  }) . ""\n"";    exit(0);    1;  ```    ## Sample PHP Plugin    Here is a sample Plugin written in [PHP](https://php.net/):    ```php  #!/usr/bin/env php  <?php    // make sure php flushes after every print  ob_implicit_flush();    // read line from stdin -- it should be our job JSON  $line = fgets( STDIN );  $job = json_decode($line, true);    print( ""Running job: "" . $job['id'] . "": "" . json_encode($job) . ""\n"" );    // report progress at 50%  sleep(5);  print( ""Halfway there!\n"" );  print( json_encode(array( 'progress' => 0.5 )) . ""\n"" );    sleep(5);  print( ""Job complete, exiting.\n"" );    // All done, send completion via JSON  print( json_encode(array(  	'complete' => 1,  	'code' => 0  )) . ""\n"" );    exit(0);  ?>  ```    ## Built-in Shell Plugin    Cronicle ships with a built-in ""Shell Plugin"", which you can use to execute arbitrary shell scripts.  Simply select the Shell Plugin from the [Edit Event Tab](#edit-event-tab), and enter your script.  This is an easy way to get up and running quickly, because you don't have to worry about reading or writing JSON.    The Shell Plugin determines success or failure based on the [exit code](https://en.wikipedia.org/wiki/Exit_status) of your script.  This defaults to `0` representing success.  Meaning, if you want to trigger an error, exit with a non-zero status code, and make sure you print your error message to STDOUT or STDERR (both will be appended to your job's log file).  Example:    ```sh  #!/bin/bash    # Perform tasks or die trying...  /usr/local/bin/my-task-1.bin || exit 1  /usr/local/bin/my-task-2.bin || exit 1  /usr/local/bin/my-task-3.bin || exit 1  ```    You can still report intermediate progress with the Shell Plugin.  It can accept JSON in the [standard output format](#json-output) if enabled, but there is also a shorthand.  You can echo a single number on its own line, from 0 to 100, with a `%` suffix, and that will be interpreted as the current progress.  Example:    ```sh  #!/bin/bash    # Perform some long-running task...  /usr/local/bin/my-task-1.bin || exit 1  echo ""25%""    # And another...  /usr/local/bin/my-task-2.bin || exit 1  echo ""50%""    # And another...  /usr/local/bin/my-task-3.bin || exit 1  echo ""75%""    # And the final task...  /usr/local/bin/my-task-4.bin || exit 1  ```    This would allow Cronicle to show a graphical progress bar on the [Home](#home-tab) and [Job Details](#job-details-tab) tabs, and estimate the time remaining based on the elapsed time and current progress.    **Pro-Tip:** The Shell Plugin actually supports any interpreted scripting language, including Node.js, PHP, Perl, Python, and more.  Basically, any language that supports a [Shebang](https://en.wikipedia.org/wiki/Shebang_%28Unix%29) line will work in the Shell Plugin.  Just change the `#!/bin/sh` to point to your interpreter binary of choice.    ## Built-in HTTP Request Plugin    Cronicle ships with a built-in ""HTTP Request"" Plugin, which you can use to send simple GET, HEAD or POST requests to any URL, and log the response.  You can specify custom HTTP request headers, and also supply regular expressions to match a successful response based on the content.  Here is the user interface when selected:    ![HTTP Request Plugin](https://pixlcore.com/software/cronicle/screenshots-new/http-request-plugin.png)    Here are descriptions of the parameters:    | Plugin Parameter | Description |  |------------------|-------------|  | **Method** | Select the HTTP request method, either GET, HEAD or POST. |  | **URL** | Enter your fully-qualified URL here, which must begin with either `http://` or `https://`. |  | **Headers** | Optionally include any custom request headers here, one per line. |  | **POST Data** | If you are sending a HTTP POST, enter the raw POST data here. |  | **Timeout** | Enter the timeout in seconds, which is measured as the time to first byte in the response. |  | **Follow Redirects** | Check this box to automatically follow HTTP redirect responses (up to 32 of them). |  | **SSL Cert Bypass** | Check this box if you need to make HTTPS requests to servers with invalid SSL certificates (self-signed or other). |  | **Success Match** | Optionally enter a regular expression here, which is matched against the response body.  If specified, this must match to consider the job a success. |  | **Error Match** | Optionally enter a regular expression here, which is matched against the response body.  If this matches the response body, then the job is aborted with an error. |    ### HTTP Request Chaining    The HTTP Request Plugin supports Cronicle's [Chain Reaction](#chain-reaction) system in two ways.  First, information about the HTTP response is passed into the [Chain Data](#chain-data) object, so downstream chained events can read and act on it.  Specifically, all the HTTP response headers, and possibly even the content body itself (if formatted as JSON and smaller than 1 MB) are included.  Example:    ```js  ""chain_data"": {  	""headers"": {  		""date"": ""Sat, 14 Jul 2018 20:14:01 GMT"",  		""server"": ""Apache/2.4.28 (Unix) LibreSSL/2.2.7 PHP/5.6.30"",  		""last-modified"": ""Sat, 14 Jul 2018 20:13:54 GMT"",  		""etag"": ""\""2b-570fb3c47e480\"""",  		""accept-ranges"": ""bytes"",  		""content-length"": ""43"",  		""connection"": ""close"",  		""content-type"": ""application/json"",  		""x-uuid"": ""7617a494-823f-4566-8f8b-f479c2a6e707""  	},  	""json"": {  		""key1"": ""value1"",  		""key2"": 12345  	}  }  ```    In this example an HTTP request was made that returned those specific response headers (the header names are converted to lower-case), and the body was also formatted as JSON, so the JSON data itself is parsed and included in a property named `json`.  Downstream events that are chain-linked to the HTTP Request event can read these properties and act on them.    Secondly, you can chain an HTTP Request into *another* HTTP Request, and use the chained data values from the previous response in the next request.  To do this, you need to utilize a special `[/bracket/slash]` placeholder syntax in the second request, to lookup values in the `chain_data` object from the first one.  You can use these placeholders in the **URL**, **Request Headers** and **POST Data** text fields.  Example:    ![HTTP Request Chain Data](https://pixlcore.com/software/cronicle/screenshots-new/http-request-chained.png)    Here you can see we are using two placeholders, one in the URL and another in the HTTP request headers.  These are looking up values from a *previous* HTTP Request event, and passing them into the next request.  Specifically, we are using:    | Placeholder | Description |  |-------------|-------------|  | `[/chain_data/json/key1]` | This placeholder is looking up the `key` value from the JSON data (body content) of the previous HTTP response.  Using our example response shown above, this would resolve to `value1`. |  | `[/chain_data/headers/x-uuid]` | This placeholder is looking up the `X-UUID` response header from the previous HTTP response.  Using our example response shown above, this would resolve to `7617a494-823f-4566-8f8b-f479c2a6e707`. |    So once the second request is sent off, after placeholder expansion the URL would actually resolve to:    ```  http://myserver.com/test.json?key=value1  ```    And the headers would expand to:    ```  User-Agent: Mozilla/5.0  X-UUID: 7617a494-823f-4566-8f8b-f479c2a6e707  ```    You can chain as many requests together as you like, but note that each request can only see and act on chain data from the *previous* request (the one that directly chained to it).    # Command Line    Here are all the Cronicle services available to you on the command line.  Most of these are accessed via the following shell script:    ```  /opt/cronicle/bin/control.sh [COMMAND]  ```    Here are all the accepted commands:    | Command | Description |  |---------|-------------|  | `start` | Starts Cronicle in daemon mode. See [Starting and Stopping](#starting-and-stopping). |  | `stop` | Stops the Cronicle daemon and waits for exit. See [Starting and Stopping](#starting-and-stopping). |  | `restart` | Calls `stop`, then `start`, in sequence. See [Starting and Stopping](#starting-and-stopping).  |  | `status` | Checks whether Cronicle is currently running. See [Starting and Stopping](#starting-and-stopping).  |  | `setup` | Runs initial storage setup (for first time install). See [Setup](#setup). |  | `maint` | Runs daily storage maintenance routine. See [Storage Maintenance](#storage-maintenance). |  | `admin` | Creates new emergency admin account (specify user / pass). See [Recover Admin Access](#recover-admin-access). |  | `export` | Exports data to specified file. See [Data Import and Export](#data-import-and-export). |  | `import` | Imports data from specified file. See [Data Import and Export](#data-import-and-export). |  | `upgrade` | Upgrades Cronicle to the latest stable (or specify version). See [Upgrading Cronicle](#upgrading-cronicle). |  | `version` | Outputs the current Cronicle package version and exits. |  | `help` | Displays a list of available commands and exits. |    ## Starting and Stopping    To start the service, use the `start` command:    ```  /opt/cronicle/bin/control.sh start  ```    And to stop it, the `stop` command:    ```  /opt/cronicle/bin/control.sh stop  ```    You can also issue a quick stop + start with the `restart` command:    ```  /opt/cronicle/bin/control.sh restart  ```    The `status` command will tell you if the service is running or not:    ```  /opt/cronicle/bin/control.sh status  ```    ## Environment Variables    Cronicle supports a special environment variable syntax, which can specify command-line options as well as override any configuration settings.  The variable name syntax is `CRONICLE_key` where `key` is one of several command-line options (see table below) or a JSON configuration property path.  These can come in handy for automating installations, and using container systems.      For overriding configuration properties by environment variable, you can specify any top-level JSON key from `config.json`, or a *path* to a nested property using double-underscore (`__`) as a path separator.  For boolean properties, you can specify `1` for true and `0` for false.  Here is an example of some of the possibilities available:    | Variable | Sample Value | Description |  |----------|--------------|-------------|  | `CRONICLE_foreground` | `1` | Run Cronicle in the foreground (no background daemon fork). |  | `CRONICLE_echo` | `1` | Echo the event log to the console (STDOUT), use in conjunction with `CRONICLE_foreground`. |  | `CRONICLE_color` | `1` | Echo the event log with color-coded columns, use in conjunction with `CRONICLE_echo`. |  | `CRONICLE_base_app_url` | `http://cronicle.mycompany.com` | Override the [base_app_url](#base_app_url) configuration property. |  | `CRONICLE_email_from` | `cronicle@mycompany.com` | Override the [email_from](#email_from) configuration property. |  | `CRONICLE_smtp_hostname` | `mail.mycompany.com` | Override the [smtp_hostname](#smtp_hostname) configuration property. |  | `CRONICLE_secret_key` | `CorrectHorseBatteryStaple` | Override the [secret_key](#secret_key) configuration property. |  | `CRONICLE_web_socket_use_hostnames` | `1` | Override the [web_socket_use_hostnames](#web_socket_use_hostnames) configuration property. |  | `CRONICLE_server_comm_use_hostnames` | `1` | Override the [server_comm_use_hostnames](#server_comm_use_hostnames) configuration property. |  | `CRONICLE_WebServer__http_port` | `80` | Override the `http_port` property *inside* the [WebServer](#web-server-configuration) object. |  | `CRONICLE_WebServer__https_port` | `443` | Override the `https_port` property *inside* the [WebServer](#web-server-configuration) object. |  | `CRONICLE_Storage__Filesystem__base_dir` | `/data/cronicle` | Override the `base_dir` property *inside* the [Filesystem](#filesystem) object *inside* the [Storage](#storage-configuration) object. |    Almost every [configuration property](#configuration) can be overridden using this environment variable syntax.  The only exceptions are things like arrays, e.g. [log_columns](#log_columns) and [socket_io_transports](#socket_io_transports).    ## Storage Maintenance    Storage maintenance automatically runs every morning at 4 AM local server time (this is [configurable](#maintenance) if you want to change it).  The operation is mainly for deleting expired records, and pruning lists that have grown too large.  However, if the Cronicle service was stopped and you missed a day or two, you can force it to run at any time.  Just execute this command on your primary server:    ```  /opt/cronicle/bin/control.sh maint  ```    This will run maintenance for the current day.  However, if the service was down for more than one day, please run the command for each missed day, providing the date in `YYYY-MM-DD` format:    ```  /opt/cronicle/bin/control.sh maint 2015-10-29  /opt/cronicle/bin/control.sh maint 2015-10-30  ```    ## Recover Admin Access    Lost access to your admin account?  You can create a new temporary administrator account on the command-line.  Just execute this command on your primary server:    ```  /opt/cronicle/bin/control.sh admin USERNAME PASSWORD  ```    Replace `USERNAME` with the desired username, and `PASSWORD` with the desired password for the new account.  Note that the new user will not show up in the main list of users in the UI.  But you will be able to login using the provided credentials.  This is more of an emergency operation, just to allow you to get back into the system.  *This is not a good way to create permanent users*.  Once you are logged back in, you should consider creating another account from the UI, then deleting the emergency admin account.    ## Server Startup    Here are the instructions for making Cronicle automatically start on server boot (Linux only).  Type these commands as root:    ```  cp /opt/cronicle/bin/cronicled.init /etc/init.d/cronicled  chmod 775 /etc/init.d/cronicled  ```    Then, if you have a RedHat-style Linux (i.e. Fedora, CentOS), type this:    ```  chkconfig cronicled on  ```    Or, if you have Debian-style Linux (i.e. Ubuntu), type this:    ```  update-rc.d cronicled defaults  ```    For multi-server clusters, you'll need to repeat these steps on each server.    **Important Note:** When Cronicle starts on server boot, it typically does not have a proper user environment, namely a `PATH` environment variable.  So if your scripts rely on binary executables in alternate locations, e.g. `/usr/local/bin`, you may have to restore the `PATH` and other variables inside your scripts by redeclaring them.    ## Upgrading Cronicle    To upgrade Cronicle, you can use the built-in `upgrade` command:    ```  /opt/cronicle/bin/control.sh upgrade  ```    This will upgrade the app and all dependencies to the latest stable release, if a new one is available.  It will not affect your data storage, users, or configuration settings.  All those will be preserved and imported to the new version.  For multi-server clusters, you'll need to repeat this command on each server.    Alternately, you can specify the exact version you want to upgrade (or downgrade) to:    ```  /opt/cronicle/bin/control.sh upgrade 1.0.4  ```    If you upgrade to the `HEAD` version, this will grab the very latest from GitHub.  Note that this is primarily for developers or beta-testers, and is likely going to contain bugs.  Use at your own risk:    ```  /opt/cronicle/bin/control.sh upgrade HEAD  ```    ## Data Import and Export    Cronicle can import and export data via the command-line, to/from a plain text file.  This data includes all the ""vital"" storage records such as Users, Plugins, Categories, Servers, Server Groups, API Keys and all Scheduled Events.  It *excludes* things like user sessions, job completions and job logs.    To export your Cronicle data, issue this command on your primary server:    ```  /opt/cronicle/bin/control.sh export /path/to/cronicle-data-backup.txt --verbose  ```    The `--verbose` flag makes it emit some extra information to the console.  Omit that if you want it to run silently.  Omit the filename if you want it to export the data to STDOUT instead of a file.    To import data back into the system, **first make sure Cronicle is stopped on all servers**, and then run this command:    ```  /opt/cronicle/bin/control.sh import /path/to/cronicle-data-backup.txt  ```    If you want daily backups of the data which auto-expire after a year, a simple shell script can do it for ya:    ```sh  #!/bin/bash  DATE_STAMP=`date ""+%Y-%m-%d""`  BACKUP_DIR=""/backup/cronicle/data""  BACKUP_FILE=""$BACKUP_DIR/backup-$DATE_STAMP.txt""    mkdir -p $BACKUP_DIR  /opt/cronicle/bin/control.sh export $BACKUP_FILE --verbose  find $BACKUP_DIR -mtime +365 -type f -exec rm -v {} \;  ```    ## Storage Migration Tool    If you need to migrate your Cronicle storage data to a new location or even a new engine, a simple built-in migration tool is provided.  This tool reads *all* Cronicle storage records and writes them back out, using two different storage configurations (old and new).    To use the tool, first edit your Cronicle's `conf/config.json` file on your primary server, and locate the `Storage` object.  This should point to your *current* storage configuration, i.e. where we are migrating *from*.  Then, add a new object right next to it, and name it `NewStorage`.  This should point to your *new* storage location and/or storage engine, i.e. where we are migrating *to*.    The contents of the `NewStorage` object should match whatever you'd typically put into `Storage`, if setting up a new install.  See the [Storage Configuration](#storage-configuration) section for details.  It can point to any of the supported engines.  Here is an example that would migrate from the local filesystem to Amazon S3:    ```js  {  	""Storage"": {  		""engine"": ""Filesystem"",  		""Filesystem"": {  			""base_dir"": ""data"",  			""key_namespaces"": 1  		}  	},  	  	""NewStorage"": {  		""engine"": ""S3"",  		""AWS"": {  			""accessKeyId"": ""YOUR_AMAZON_ACCESS_KEY"",   			""secretAccessKey"": ""YOUR_AMAZON_SECRET_KEY"",   			""region"": ""us-west-1"",  			""correctClockSkew"": true,  			""maxRetries"": 5,  			""httpOptions"": {  				""connectTimeout"": 5000,  				""timeout"": 5000  			}  		},  		""S3"": {  			""keyPrefix"": ""cronicle"",  			""fileExtensions"": true,  			""params"": {  				""Bucket"": ""YOUR_S3_BUCKET_ID""  			}  		}  	}  }  ```    You could also use this to migrate between two AWS regions, S3 buckets or key prefixes on S3.  Just point `Storage` and `NewStorage` to the same engine, e.g. `S3`, and change only the region, bucket or prefix in the `NewStorage` object.    When you are ready to proceed, make sure you **shut down Cronicle** on all your servers.  You should not migrate storage while Cronicle is running, as it can result in corrupted data.    All good?  Okay then, on your Cronicle primary server as root (superuser), issue this command:    ```  /opt/cronicle/bin/control.sh migrate  ```    The following command-line arguments are supported:    | Argument | Description |  |----------|-------------|  | `--debug` | Echo all debug log messages to the console.  This also disables the progress bar. |  | `--verbose` | Print the key of each record as it is migrated.  This also disables the progress bar. |  | `--dryrun` | Do not write any changes to new storage (except for a single test record, which is then deleted).  Used for debugging and troubleshooting. |    It is recommended that you first run the migrate command with `--dryrun` to make sure that it can read and write to the two storage locations.  The script also logs all debug messages and transactions to `logs/StorageMigration.log`.    Once the migration is complete and you have verified that your data is where you expect, edit the `conf/config.json` file one last time, and remove the old `Storage` object, and then rename `NewStorage` to `Storage`, effectively replacing it.  Cronicle will now access your storage data from the new location.  Make a backup of the file in case you ever need to roll back.    If you have multiple Cronicle servers, make sure you sync your `conf/config.json` between all servers!  They all need to be identical.    Finally, restart Cronicle, and all should be well.    # Inner Workings    This section contains details on some of the inner workings of Cronicle.    ## Cron Noncompliance    Cronicle has a custom-built scheduling system that is *loosely* based on Unix [Cron](https://en.wikipedia.org/wiki/Cron).  It does not, however, conform to the [specification](https://linux.die.net/man/5/crontab) written by [Paul Vixie](https://en.wikipedia.org/wiki/Paul_Vixie).  Namely, it differs in the following ways:    - Month days and weekdays are intersected when both are present  	- If you specify both month days and weekdays, *both must match* for Cronicle to fire an event.  Vixie Cron behaves differently, in that it will fire if *either* matches.  This was a deliberate design decision to enable more flexibility in scheduling.    When importing Crontab syntax:    - Cronicle does not support the concept of running jobs on reboot, so the `@reboot` macro is disallowed.  - If a 6th column is specified, it is assumed to be years.  - Weekdays `0` and `7` are both considered to be Sunday.    For more details on Cronicle's scheduler implementation, see the [Event Timing Object](#event-timing-object).    ## Storage    The storage system in Cronicle is built on the [pixl-server-storage](https://www.npmjs.com/package/pixl-server-storage) module, which is basically a key/value store.  It can write everything to local disk (the default), [Couchbase](http://www.couchbase.com/nosql-databases/couchbase-server) or [Amazon S3](https://aws.amazon.com/s3/).    Writing to local disk should work just fine for most installations, even with multi-server setups, as long as there is only one primary server.  However, if you want redundant backups with auto-failover, and/or redundant data storage, then you either have to use a shared filesystem such as NFS, or switch to Couchbase or S3.    With an NFS shared filesystem, your primary and backup servers can have access to the same Cronicle data storage.  Only the primary server performs data writes, so there should never be any data corruption.  The easiest way to set up a shared filesystem is to configure Cronicle to point at your NFS mount in the `conf/config.json` file, then run the setup script.  The filesystem storage location is in the `base_dir` property, which is found in the `Storage` / `Filesystem` objects:    ```js  {  	""Storage"": {  		""engine"": ""Filesystem"",  		""list_page_size"": 50,  		""concurrency"": 4,  		  		""Filesystem"": {  			""base_dir"": ""/PATH/TO/YOUR/NFS/MOUNT"",  			""key_namespaces"": 1  		}  	}  }  ```    Then run the setup script as instructed in the [Setup](#setup) section.  Make sure all your backup servers have the NFS filesystem mounted in the same location, and then copy the `conf/config.json` file to all the servers.  **Do not run the setup script more than once.**    Setting up Couchbase or S3 is handled in much the same way.  Edit the `conf/config.json` file to point to the service of your choice, then run the setup script to create the initial storage records.  See the [Couchbase](#couchbase) or [Amazon S3](#amazon-s3) configuration sections for more details.    ## Logs    Cronicle writes its logs in a plain text, square-bracket delimited column format, which looks like this:    ```  [1450993152.554][2015/12/24 13:39:12][joeretina.local][Cronicle][debug][3][Cronicle starting up][]  [1450993152.565][2015/12/24 13:39:12][joeretina.local][Cronicle][debug][4][Server is eligible to become primary (Main Group)][]  [1450993152.566][2015/12/24 13:39:12][joeretina.local][Cronicle][debug][3][We are becoming the primary server][]  [1450993152.576][2015/12/24 13:39:12][joeretina.local][Cronicle][debug][2][Startup complete, entering main loop][]  ```    The log columns are defined as follows, from left to right:    | Log Column | Description |  |------------|-------------|  | `hires_epoch` | A date/time stamp in high-resolution [Epoch time](https://en.wikipedia.org/wiki/Unix_time). |  | `date` | A human-readable date/time stamp in the format: `YYYY/MM/DD HH:MI:SS` (local server time) |  | `hostname` | The hostname of the server that wrote the log entry (useful for multi-server setups if you merge your logs together). |  | `component` | The component name which generated the log entry.  See below for a list of all the components. |  | `category` | The category of the log entry, which will be one of `debug`, `transaction` or `error`. |  | `code` | Debug level (1 to 10), transaction or error code. |  | `msg` | Debug, transaction or error message text. |  | `data` | Additional JSON data, may or may not present. |    The columns are configurable via the [log_columns](#log_columns) property in the `conf/config.json` file:    ```js  {  	""log_columns"": [""hires_epoch"", ""date"", ""hostname"", ""component"", ""category"", ""code"", ""msg"", ""data""]  }  ```    Feel free to reorder or remove columns, but don't rename any.  The IDs are special, and match up to keywords in the source code.    By default, logging consists of several different files, each for a specific component of the system.  After starting up Cronicle, you will find these log files in the [log_dir](#log_dir) directory:    | Log Filename | Description |  |--------------|-------------|  | `Cronicle.log` | The main component will contain most of the app logic (scheduler, jobs, startup, shutdown, etc.). |  | `Error.log` | The error log will contain all errors, including job failures, server disconnects, etc. |  | `Transaction.log` | The transaction log will contain all transactions, including API actions, job completions, etc. |  | `API.log` | The API component log will contain information about incoming HTTP API calls. |  | `Storage.log` | The storage component log will contain information about data reads and writes. |  | `Filesystem.log` | Only applicable if you use the local filesystem storage back-end. |  | `Couchbase.log` | Only applicable if you use the [Couchbase](#couchbase) storage back-end. |  | `S3.log` | Only applicable if you use the [Amazon S3](#amazon-s3) storage back-end. |  | `User.log` | The user component log will contain user related information such as logins and logouts. |  | `WebServer.log` | The web server component log will contain information about HTTP requests and connections. |  | `crash.log` | If Cronicle crashed for any reason, you should find a date/time and stack trace in this log. |  | `install.log` | Contains detailed installation notes from npm, and the build script. |    The [log_filename](#log_filename) configuration property controls this, and by default it is set to the following:    ```js  {  	""log_filename"": ""[component].log"",  }  ```    This causes the value of the `component` column to dictate the actual log filename.  If you would prefer that everything be logged to a single combo file instead, just change this to a normal string without brackets, such as:    ```js  {  	""log_filename"": ""event.log"",  }  ```    ## Keeping Time    Cronicle manages job scheduling for its events by using a ""cursor"" system, as opposed to a classic queue.  Each event has its own internal cursor (pointer) which contains a timestamp.  This data is stored in RAM but is also persisted to disk for failover.  When the clock advances a minute, the scheduler iterates over all the active events, and moves their cursors forward, always trying to keep them current.  It launches any necessary jobs along the way.    For events that have [Run All Mode](#run-all-mode) set, a cursor may pause or even move backwards, depending on circumstances.  If a job fails to launch, the cursor stays back in the previous minute so it can try again.  In this way jobs virtually ""queue up"" as time advances.  When the blocking issue is resolved (resource constraint or other), the event cursor will be moved forward as quickly as resources and settings allow, so it can ""catch up"" to current time.    Also, you have the option of manually resetting an event's cursor using the [Time Machine](#event-time-machine) feature.  This way you can manually have it re-run past jobs, or hop over a ""queue"" that has built up.    ## Primary Server Failover    In a [Multi-Server Cluster](#multi-server-cluster), you can designate a number of servers as primary backups, using the [Server Groups](#server-groups) feature.  These backup servers will automatically take over as primary if something happens to the current primary server (shutdown or crash).  Your servers will automatically negotiate who should become primary, both at startup and at failover, based on an alphabetical sort of their hostnames.  Servers which sort higher will become primary before servers that sort lower.    Upon startup there is a ~60 second delay before a primary server is chosen.  This allows time for all the servers in the cluster to auto-discover each other.    ### Unclean Shutdown    Cronicle is designed to handle server failures.  If a worker server goes down for any reason, the cluster will automatically adjust.  Any active jobs on the dead server will be failed and possibly retried after a short period of time (see [dead_job_timeout](#dead_job_timeout)), and new jobs will be reassigned to other servers as needed.    If a primary server goes down, one of the backups will take over within 60 seconds (see [master_ping_timeout](#master_ping_timeout)).  The same rules apply for any jobs that were running on the primary server.  They'll be failed and retried as needed by the new primary server, with one exception: unclean shutdown.    When a primary server experiences a catastrophic failure such as a daemon crash, kernel panic or power loss, it has no time to do anything, so any active jobs on the server are instantly dead.  The jobs will eventually be logged as failures, and the logs recovered when the server comes back online.  However, if the events had [Run All Mode](#run-all-mode) enabled, they won't be auto-retried when the new primary takes over, because it has no way of knowing a job was even running.  And by the time the old primary server is brought back online, days or weeks may have passed, so it would be wrong to blindly rewind the event clock to before the event ran.    So in summary, the only time human intervention may be required is if a primary server dies unexpectedly due to an unclean shutdown, and it had active jobs running on it, and those jobs had [Run All Mode](#run-all-mode) set.  In that case, you may want to use the [Time Machine](#event-time-machine) feature to reset the event clock, to re-run any missed jobs.    # API Reference    ## JSON REST API    All API calls expect JSON as input (unless they are simple HTTP GETs), and will return JSON as output.  The main API endpoint is:    ```  /api/app/NAME/v1  ```    Replace `NAME` with the specific API function you are calling (see below for list).  All requests should be HTTP GET or HTTP POST as the API dictates, and should be directed at the Cronicle primary server on the correct TCP port (the default is `3012` but is often reconfigured to be `80`).  Example URL:    ```  http://myserver.com:3012/api/app/get_schedule/v1  ```    For web browser access, [JSONP](https://en.wikipedia.org/wiki/JSONP) response style is supported for all API calls, by including a `callback` query parameter.  However, all responses include a `Access-Control-Allow-Origin: *` header, so cross-domain [XHR](https://en.wikipedia.org/wiki/XMLHttpRequest) requests will work as well.    ### Redirects    If you are running a multi-server Cronicle cluster with multiple primary backup servers behind a load balancer, you may receive a `HTTP 302` response if you hit a non-primary server for an API request.  In this case, the `Location` response header will contain the proper primary server hostname.  Please repeat your request pointed at the correct server.  Most HTTP request libraries have an option to automatically follow redirects, so you can make this process automatic.    It is recommended (although not required) that you cache the primary server hostname if you receive a 302 redirect response, so you can make subsequent calls to the primary server directly, without requiring a round trip.      ## API Keys    API Keys allow you to register external applications or services to use the REST API.  These can be thought of as special user accounts specifically for applications.  API calls include running jobs on-demand, monitoring job status, and managing the schedule (creating, editing and/or deleting events).  Each API key can be granted a specific set of privileges.    To create an API Key, you must first be an administrator level user.  Login to the Cronicle UI, proceed to the [API Keys Tab](#api-keys-tab), and click the ""Add API Key..."" button.  Fill out the form and click the ""Create Key"" button at the bottom of the page.    API Keys are randomly generated hexadecimal strings, and are 32 characters in length.  Example:    ```  0095f5b664b93304d5f8b1a61df605fb  ```    You must include a valid API Key with every API request.  There are three ways to do this: include a `X-API-Key` HTTP request header, an `api_key` query string parameter, or an `api_key` JSON property.    Here is a raw HTTP request showing all three methods of passing the API Key (only one of these is required):    ```  GET /api/app/get_schedule/v1?api_key=0095f5b664b93304d5f8b1a61df605fb HTTP/1.1  Host: myserver.com  X-API-Key: 0095f5b664b93304d5f8b1a61df605fb  Content-Type: application/json    {""offset"": 0, ""limit"": 50, ""api_key"": ""0095f5b664b93304d5f8b1a61df605fb""}  ```    ## Standard Response Format    Regardless of the specific API call you requested, all responses will be in JSON format, and include at the very least a `code` property.  This will be set to `0` upon success, or any other value if an error occurred.  In the event of an error, a `description` property will also be included, containing the error message itself.  Individual API calls may include additional properties, but these two are standard fare in all cases.  Example successful response:    ```js  { ""code"": 0 }  ```    Example error response:    ```js  {""code"": ""session"", ""description"": ""No Session ID or API Key could be found""}  ```    ## API Calls    Here is the list of supported API calls:    ### get_schedule    ```  /api/app/get_schedule/v1  ```    This fetches scheduled events and returns details about them.  It supports pagination to fetch chunks, with the default being the first 50 events.  Both HTTP GET (query string) or HTTP POST (JSON data) are acceptable.  Parameters:    | Parameter Name | Description |  |----------------|-------------|  | `offset` | (Optional) The offset into the data to start returning records, defaults to 0. |  | `limit` | (Optional) The number of records to return, defaults to 50. |    Example request:    ```js  {  	""offset"": 0,  	""limit"": 1000  }  ```    Example response:    ```js  {  	""code"": 0,  	""rows"": [  		{  			""enabled"": 1,  			""params"": {  				""script"": ""#!/bin/sh\n\n/usr/local/bin/db-reindex.pl\n""  			},  			""timing"": {  				""minutes"": [ 10 ]  			},  			""max_children"": 1,  			""timeout"": 3600,  			""catch_up"": false,  			""plugin"": ""shellplug"",  			""title"": ""Rebuild Indexes"",  			""category"": ""general"",  			""target"": ""c33ff006"",  			""multiplex"": 0,  			""retries"": 0,  			""detached"": 0,  			""notify_success"": """",  			""notify_fail"": """",  			""web_hook"": """",  			""notes"": """",  			""id"": ""29bf12db"",  			""modified"": 1445233242,  			""created"": 1445233022,  			""username"": ""admin"",  			""timezone"": ""America/Los_Angeles""  		}  	],  	""list"": {  		""page_size"": 50,  		""first_page"": 0,  		""last_page"": 0,  		""length"": 12,  		""type"": ""list""  	}  }  ```    In addition to the [Standard Response Format](#standard-response-format), this API will include the following:    The `rows` array will contain an element for every matched event in the requested set.  It will contain up to `limit` elements.  See the [Event Data Format](#event-data-format) section below for details on the event object properties themselves.    The `list` object contains internal metadata about the list structure in storage.  You can probably ignore this, except perhaps the `list.length` property, which will contain the total number of events in the schedule, regardless if your `offset` and `limit` parameters.  This can be useful for building pagination systems.    ### get_event    ```  /api/app/get_event/v1  ```    This fetches details about a single event, given its ID or exact title.  Both HTTP GET (query string) or HTTP POST (JSON data) are acceptable.  Parameters:    | Parameter Name | Description |  |----------------|-------------|  | `id` | The ID of the event you wish to fetch details on. |  | `title` | The exact title of the event you wish to fetch details on (case-sensitive). |    Example request:    ```js  {  	""id"": ""540cf457""  }  ```    Example response:    ```js  {  	""code"": 0,  	""event"": {  		""enabled"": 0,  		""params"": {  			""script"": ""#!/bin/sh\n\n/usr/local/bin/s3-backup-logs.pl\n""  		},  		""timing"": {  			""minutes"": [ 5 ]  		},  		""max_children"": 1,  		""timeout"": 3600,  		""catch_up"": false,  		""plugin"": ""shellplug"",  		""title"": ""Backup Logs to S3"",  		""category"": ""ad8190ff"",  		""target"": ""all"",  		""multiplex"": 0,  		""retries"": 0,  		""detached"": 0,  		""notify_success"": """",  		""notify_fail"": """",  		""web_hook"": """",  		""notes"": """",  		""id"": ""540cf457"",  		""modified"": 1449941100,  		""created"": 1445232960,  		""username"": ""admin"",  		""retry_delay"": 0,  		""cpu_limit"": 0,  		""cpu_sustain"": 0,  		""memory_limit"": 0,  		""memory_sustain"": 0,  		""log_max_size"": 0,  		""timezone"": ""America/Los_Angeles""  	}  }  ```    In addition to the [Standard Response Format](#standard-response-format), this API will include the following:    The `event` object will contain the details for the requested event.  See the [Event Data Format](#event-data-format) section below for details on the event object properties themselves.    If [Allow Queued Jobs](#allow-queued-jobs) is enabled on the event, the API response will also include a `queue` property, which will be set to the number of jobs currently queued up.    If there are any active jobs currently running for the event, they will also be included in the response, in a `jobs` array.  Each job object will contain detailed information about the running job.  See [get_job_status](#get_job_status) below for more details.    ### create_event    ```  /api/app/create_event/v1  ```    This creates a new event and adds it to the schedule.  API Keys require the `create_events` privilege to use this API.  Only HTTP POST (JSON data) is acceptable.  The required parameters are as follows:    | Parameter Name | Description |  |----------------|-------------|  | `title` | **(Required)** A display name for the event, shown on the [Schedule Tab](#schedule-tab) as well as in reports and e-mails. |  | `enabled` | **(Required)** Specifies whether the event is enabled (active in the scheduler) or not.  Should be set to 1 or 0. |  | `category` | **(Required)** The Category ID to which the event will be assigned.  See [Categories Tab](#categories-tab). |  | `plugin` | **(Required)** The ID of the Plugin which will run jobs for the event. See [Plugins Tab](#plugins-tab). |  | `target` | **(Required)** Events can target a [Server Group](#server-groups) (Group ID), or an individual server (hostname). |    In addition to the required parameters, almost anything in the [Event Data Object](#event-data-format) can also be included here.  Example request:    ```js  {  	""catch_up"": 1,  	""category"": ""43f8c57e"",  	""cpu_limit"": 100,  	""cpu_sustain"": 0,  	""detached"": 0,  	""enabled"": 1,  	""log_max_size"": 0,  	""max_children"": 1,  	""memory_limit"": 0,  	""memory_sustain"": 0,  	""modified"": 1451185588,  	""multiplex"": 0,  	""notes"": ""This event handles database maintenance."",  	""notify_fail"": """",  	""notify_success"": """",  	""params"": {  		""db_host"": ""idb01.mycompany.com"",  		""verbose"": 1,  		""cust"": ""Sales""  	},  	""plugin"": ""test"",  	""retries"": 0,  	""retry_delay"": 30,  	""target"": ""db1.int.myserver.com"",  	""timeout"": 3600,  	""timezone"": ""America/New_York"",  	""timing"": {  		""hours"": [ 21 ],  		""minutes"": [ 20, 40 ]  	},  	""title"": ""DB Reindex"",  	""web_hook"": ""http://myserver.com/notify-chronos.php""  }  ```    Example response:    ```js  {  	""code"": 0,  	""id"": ""540cf457""  }  ```    In addition to the [Standard Response Format](#standard-response-format), the ID of the new event will be returned in the `id` property.    ### update_event    ```  /api/app/update_event/v1  ```    This updates an existing event given its ID, replacing any properties you specify.  API Keys require the `edit_events` privilege to use this API.  Only HTTP POST (JSON data) is acceptable.  The parameters are as follows:    | Parameter Name | Description |  |----------------|-------------|  | `id` | **(Required)** The ID of the event you wish to update. |  | `reset_cursor` | (Optional) Reset the event clock to the given Epoch timestamp (see [Event Time Machine](#event-time-machine)). |  | `abort_jobs` | (Optional) If you are disabling the event by setting `enabled` to 0, you may also abort any running jobs if you want. |    Include anything from the [Event Data Object](#event-data-format) to update (i.e. replace) the values.  Anything omitted is preserved.  Example request:    ```js  {  	""id"": ""3c182051"",  	""enabled"": 0,  }  ```    Example request with everything updated:    ```js  {  	""id"": ""3c182051"",  	""reset_cursor"": 1451185588,  	""abort_jobs"": 0,  	""catch_up"": 1,  	""category"": ""43f8c57e"",  	""cpu_limit"": 100,  	""cpu_sustain"": 0,  	""detached"": 0,  	""enabled"": 1,  	""log_max_size"": 0,  	""max_children"": 1,  	""memory_limit"": 0,  	""memory_sustain"": 0,  	""multiplex"": 0,  	""notes"": ""This event handles database maintenance."",  	""notify_fail"": """",  	""notify_success"": """",  	""params"": {  		""db_host"": ""idb01.mycompany.com"",  		""verbose"": 1,  		""cust"": ""Marketing""  	},  	""plugin"": ""test"",  	""retries"": 0,  	""retry_delay"": 30,  	""target"": ""db1.int.myserver.com"",  	""timeout"": 3600,  	""timezone"": ""America/New_York"",  	""timing"": {  		""hours"": [ 21 ],  		""minutes"": [ 20, 40 ]  	},  	""title"": ""DB Reindex"",  	""username"": ""admin"",  	""web_hook"": ""http://myserver.com/notify-chronos.php""  }  ```    Example response:    ```js  {  	""code"": 0  }  ```    See the [Standard Response Format](#standard-response-format) for details.    ### delete_event    ```  /api/app/delete_event/v1  ```    This deletes an existing event given its ID.  Note that the event must not have any active jobs still running (or else an error will be returned).  API Keys require the `delete_events` privilege to use this API.  Only HTTP POST (JSON data) is acceptable.  The parameters are as follows:    | Parameter Name | Description |  |----------------|-------------|  | `id` | **(Required)** The ID of the event you wish to delete. |    Example request:    ```js  {  	""id"": ""3c182051""  }  ```    Example response:    ```js  {  	""code"": 0  }  ```    See the [Standard Response Format](#standard-response-format) for details.    ### run_event    ```  /api/app/run_event/v1  ```    This immediately starts an on-demand job for an event, regardless of the schedule.  This is effectively the same as a user clicking the ""Run Now"" button in the UI.  API Keys require the `run_events` privilege to use this API.  Both HTTP GET (query string) or HTTP POST (JSON data) are acceptable.  You can specify the target event by its ID or exact title:    | Parameter Name | Description |  |----------------|-------------|  | `id` | The ID of the event you wish to run a job for. |  | `title` | The exact title of the event you wish to run a job for (case-sensitive). |    You can also include almost anything from the [Event Data Object](#event-data-format) to customize the settings for the job.  Anything omitted is pulled from the event object.  Example request:    ```js  {  	""id"": ""3c182051""  }  ```    Example request with everything customized:    ```js  {  	""id"": ""3c182051"",  	""category"": ""43f8c57e"",  	""cpu_limit"": 100,  	""cpu_sustain"": 0,  	""detached"": 0,  	""log_max_size"": 0,  	""max_children"": 1,  	""memory_limit"": 0,  	""memory_sustain"": 0,  	""multiplex"": 0,  	""notify_fail"": """",  	""notify_success"": """",  	""params"": {  		""db_host"": ""idb01.mycompany.com"",  		""verbose"": 1,  		""cust"": ""Marketing""  	},  	""plugin"": ""test"",  	""retries"": 0,  	""retry_delay"": 30,  	""target"": ""db1.internal.myserver.com"",  	""timeout"": 3600,  	""title"": ""DB Reindex"",  	""username"": ""admin"",  	""web_hook"": ""http://myserver.com/notify-chronos.php""  }  ```    Note that the `params` object can be omitted entirely, or sparsely populated, and any missing properties that are defined in the event are automatically merged in.  This allows your API client to only specify the `params` it needs to (including arbitrary new ones).    Example response:    ```js  {  	""code"": 0,  	""ids"": [""23f5c37f"", ""f8ac3082""]  }  ```    In addition to the [Standard Response Format](#standard-response-format), the IDs of all the launched jobs will be returned in the `ids` array.  Typically only a single job is launched, but it may be multiple if the event has [Multiplexing](#multiplexing) enabled and targets a group with multiple servers.    If [Allow Queued Jobs](#allow-queued-jobs) is enabled on the event, the API response will also include a `queue` property, which will be set to the number of jobs currently queued up.    **Advanced Tip:** If you do not wish to merge the POST data into the `params` (for example if you are using a webhook that provides other data as JSON), then you can add `&post_data=1` to the query string. If you do this, then the POST data will be available in the `post_data` key of the `params` object.    ### get_job_status    ```  /api/app/get_job_status/v1  ```    This fetches status for a job currently in progress, or one already completed.  Both HTTP GET (query string) or HTTP POST (JSON data) are acceptable.  Parameters:    | Parameter Name | Description |  |----------------|-------------|  | `id` | **(Required)** The ID of the job you wish to fetch status on. |    Example request:    ```js  {  	""id"": ""jiinxhh5203""  }  ```    Example response:    ```js  {  	""code"": 0,  	""job"": {  		""params"": {  			""db_host"": ""idb01.mycompany.com"",  			""verbose"": 1,  			""cust"": ""Marketing""  		},  		""timeout"": 3600,  		""catch_up"": 1,  		""plugin"": ""test"",  		""category"": ""43f8c57e"",  		""retries"": 0,  		""detached"": 0,  		""notify_success"": ""jhuckaby@test.com"",  		""notify_fail"": ""jhuckaby@test.com"",  		""web_hook"": ""http://myserver.com/notify-chronos.php"",  		""notes"": ""Joe testing."",  		""multiplex"": 0,  		""memory_limit"": 0,  		""memory_sustain"": 0,  		""cpu_limit"": 0,  		""cpu_sustain"": 0,  		""log_max_size"": 0,  		""retry_delay"": 30,  		""timezone"": ""America/New_York"",  		""source"": ""Manual (admin)"",  		""id"": ""jiiqjexr701"",  		""time_start"": 1451341765.987,  		""hostname"": ""joeretina.local"",  		""command"": ""bin/test-plugin.js"",  		""event"": ""3c182051"",  		""now"": 1451341765,  		""event_title"": ""Test Event 2"",  		""plugin_title"": ""Test Plugin"",  		""category_title"": ""Test Cat"",  		""nice_target"": ""joeretina.local"",  		""log_file"": ""/opt/cronicle/logs/jobs/jiiqjexr701.log"",  		""pid"": 11743,  		""progress"": 1,  		""cpu"": {  			""min"": 19,  			""max"": 19,  			""total"": 19,  			""count"": 1,  			""current"": 19  		},  		""mem"": {  			""min"": 214564864,  			""max"": 214564864,  			""total"": 214564864,  			""count"": 1,  			""current"": 214564864  		},  		""complete"": 1,  		""code"": 0,  		""description"": ""Success!"",  		""perf"": ""scale=1&total=90.319&db_query=3.065&db_connect=5.096&log_read=7.425&gzip_data=11.094&http_post=17.72"",  		""log_file_size"": 25110,  		""time_end"": 1451341856.61,  		""elapsed"": 90.62299990653992  	}  }  ```    In addition to the [Standard Response Format](#standard-response-format), the job details can be found in the `job` object.    In the `job` object you'll find all the standard [Event Data Object](#event-data-format) properties, as well as the following properties unique to this API:    | Property Name | Description |  |---------------|-------------|  | `hostname` | The hostname of the server currently running, or the server who ran the job. |  | `source` | If the job was started manually via user or API, this will contain a text string identifying who it was. |  | `log_file` | A local filesystem path to the job's log file (only applicable if job is in progress). |  | `pid` | The main PID of the job process that was spawned. |  | `progress` | Current progress of the job, from `0.0` to `1.0`, as reported by the Plugin (optional). |  | `complete` | Will be set to `1` when the job is complete, omitted if still in progress. |  | `code` | A code representing job success (`0`) or failure (any other value).  Only applicable for completed jobs. |  | `description` | If the job failed, this will contain the error message.  Only applicable for completed jobs. |  | `perf` | [Performance metrics](#performance-metrics) for the job, if reported by the Plugin (optional). Only applicable for completed jobs. |  | `time_start` | A Unix Epoch timestamp of when the job started. |  | `time_end` | A Unix Epoch timestamp of when the job completed. Only applicable for completed jobs. |  | `elapsed` | The elapsed time of the job, in seconds. |  | `cpu` | An object representing the CPU use of the job.  See below. |  | `mem` | An object representing the memory use of the job.  See below. |    Throughout the course of a job, its process CPU and memory usage are measured periodically, and tracked in these objects:    ```js  {  	""cpu"": {  		""min"": 19,  		""max"": 19,  		""total"": 19,  		""count"": 1,  		""current"": 19  	},  	""mem"": {  		""min"": 214564864,  		""max"": 214564864,  		""total"": 214564864,  		""count"": 1,  		""current"": 214564864  	}  }  ```    The CPU is measured as percentage of one CPU core, so 100 means that a full CPU core is in use.  It may also go above 100, if multiple threads or sub-processes are in use.  The current value can be found in `current`, and the minimum (`min`) and maximum (`max`) readings are also tracked.  To compute the average, divide the `total` value by the `count`.    The memory usage is measured in bytes.  The current value can be found in `current`, and the minimum (`min`) and maximum (`max`) readings are also tracked.  To compute the average, divide the `total` value by the `count`.    ### get_active_jobs    ```  /api/app/get_active_jobs/v1  ```    This fetches status for **all active** jobs, and returns them all at once.  It takes no parameters (except an [API Key](#api-keys) of course).  The response format is as follows:    ```js  {  	""code"": 0,  	""jobs"": {  		""jk6lmar4c01"": {  			...  		},  		""jk6lmar4d04"": {  			...  		}  	}  }  ```    In addition to the [Standard Response Format](#standard-response-format), the response object will contain a `jobs` object.  This object will have zero or more nested objects, each representing one active job.  The inner property names are the Job IDs, and the contents are the status, progress, and other information about the active job.  For details on the job objects, see the [get_job_status](#get_job_status) API call above, as the parameters of each job will be the same as that API.    ### update_job    ```  /api/app/update_job/v1  ```    This updates a job that is already in progress.  Only certain job properties may be changed when the job is running, and those are listed below.  This is typically used to adjust timeouts, resource limits, or user notification settings.  API Keys require the `edit_events` privilege to use this API.  Only HTTP POST (JSON data) is acceptable.  The parameters are as follows:    | Parameter Name | Description |  |----------------|-------------|  | `id` | **(Required)** The ID of the job you wish to update. |  | `timeout` | (Optional) The total run time in seconds to allow, before the job is aborted. |  | `retries` | (Optional) The number of retries before the job is reported a failure. |  | `retry_delay` | (Optional) The number of seconds between retries. |  | `chain` | (Optional) Launch another event when the job completes successfully (see [Chain Reaction](#chain-reaction)). |  | `chain_error` | (Optional) Launch another event when the job fails (see [Chain Reaction](#chain-reaction)). |  | `notify_success` | (Optional) A comma-separated list of e-mail addresses to notify on job success. |  | `notify_fail` | (Optional) A comma-separated list of e-mail addresses to notify on job failure. |  | `web_hook` | (Optional) A fully-qualified URL to ping when the job completes. |  | `cpu_limit` | (Optional) The maximum allowed CPU before the job is aborted (100 = 1 CPU core). |  | `cpu_sustain` | (Optional) The number of seconds to allow the max CPU to be exceeded. |  | `memory_limit` | (Optional) The maximum allowed memory usage (in bytes) before the job is aborted. |  | `memory_sustain` | (Optional) The number of seconds to allow the max memory to be exceeded. |  | `log_max_size` | (Optional) The maximum allowed job log file size (in bytes) before the job is aborted. |    As shown above, you can include *some* of the properties from the [Event Data Object](#event-data-format) to customize the job in progress.  Example request:    ```js  {  	""id"": ""j3c182051"",  	""timeout"": 300,  	""notify_success"": ""email@server.com""  }  ```    Example response:    ```js  {  	""code"": 0  }  ```    See the [Standard Response Format](#standard-response-format) for details.    ### abort_job    ```  /api/app/abort_job/v1  ```    This aborts a running job given its ID.  API Keys require the `abort_events` privilege to use this API.  Only HTTP POST (JSON data) is acceptable.  The parameters are as follows:    | Parameter Name | Description |  |----------------|-------------|  | `id` | **(Required)** The ID of the job you wish to abort. |    Example request:    ```js  {  	""id"": ""jiinxhh5203""  }  ```    Example response:    ```js  {  	""code"": 0  }  ```    See the [Standard Response Format](#standard-response-format) for details.    ## Event Data Format    Here are descriptions of all the properties in the event object, which is common in many API calls:    | Event Property | Format | Description |  |----------------|--------|-------------|  | `algo` | String | Specifies the algorithm to use for picking a server from the target group. See [Algorithm](#algorithm). |  | `api_key` | String | The API Key of the application that originally created the event (if created via API). |  | `catch_up` | Boolean | Specifies whether the event has [Run All Mode](#run-all-mode) enabled or not. |  | `category` | String | The Category ID to which the event is assigned.  See [Categories Tab](#categories-tab). |  | `chain` | String | The chain reaction event ID to launch when jobs complete successfully.  See [Chain Reaction](#chain-reaction). |  | `chain_error` | String | The chain reaction event ID to launch when jobs fail.  See [Chain Reaction](#chain-reaction). |  | `cpu_limit` | Number | Limit the CPU to the specified percentage (100 = 1 core), abort if exceeded. See [Event Resource Limits](#event-resource-limits). |  | `cpu_sustain` | Number | Only abort if the CPU limit is exceeded for this many seconds. See [Event Resource Limits](#event-resource-limits). |  | `created` | Number | The date/time of the event's initial creation, in Epoch seconds. |  | `detached` | Boolean | Specifies whether [Detached Mode](#detached-mode) is enabled or not. |  | `enabled` | Boolean | Specifies whether the event is enabled (active in the scheduler) or not. |  | `id` | String | A unique ID assigned to the event when it was first created. |  | `log_max_size` | Number | Limit the job log file size to the specified amount, in bytes.  See [Event Resource Limits](#event-resource-limits). |  | `max_children` | Number | The total amount of concurrent jobs allowed to run. See [Event Concurrency](#event-concurrency). |  | `memory_limit` | Number | Limit the memory usage to the specified amount, in bytes. See [Event Resource Limits](#event-resource-limits). |  | `memory_sustain` | Number | Only abort if the memory limit is exceeded for this many seconds. See [Event Resource Limits](#event-resource-limits). |  | `modified` | Number | The date/time of the event's last modification, in Epoch seconds. |  | `multiplex` | Boolean | Specifies whether the event has [Multiplexing](#multiplexing) mode is enabled or not. |  | `notes` | String | Text notes saved with the event, included in e-mail notifications. See [Event Notes](#event-notes). |  | `notify_fail` | String | List of e-mail recipients to notify upon job failure (CSV). See [Event Notification](#event-notification). |  | `notify_success` | String | List of e-mail recipients to notify upon job success (CSV). See [Event Notification](#event-notification). |  | `params` | Object | An object containing the Plugin's custom parameters, filled out with values from the Event Editor. See [Plugins Tab](#plugins-tab). |  | `plugin` | String | The ID of the Plugin which will run jobs for the event. See [Plugins Tab](#plugins-tab). |  | `queue` | Boolean | Allow jobs to be queued up when they can't run immediately. See [Allow Queued Jobs](#allow-queued-jobs). |  | `queue_max` | Number | Maximum queue length, when `queue` is enabled. See [Allow Queued Jobs](#allow-queued-jobs). |  | `retries` | Number | The number of retries to allow before reporting an error. See [Event Retries](#event-retries). |  | `retry_delay` | Number | Optional delay between retries, in seconds. See [Event Retries](#event-retries). |  | `stagger` | Number | If [Multiplexing](#multiplexing) is enabled, this specifies the number of seconds to wait between job launches. |  | `target` | String | Events can target a [Server Group](#server-groups) (Group ID), or an individual server (hostname). |  | `timeout` | Number | The maximum allowed run time for jobs, specified in seconds. See [Event Timeout](#event-timeout). |  | `timezone` | String | The timezone for interpreting the event timing settings. Needs to be an [IANA timezone string](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).  See [Event Timing](#event-timing). |  | `timing` | Object | An object describing when to run scheduled jobs.  See [Event Timing Object](#event-timing-object) below for details. |  | `title` | String | A display name for the event, shown on the [Schedule Tab](#schedule-tab) as well as in reports and e-mails. |  | `username` | String | The username of the user who originally created the event (if created in the UI). |  | `web_hook` | String | An optional URL to hit for the start and end of each job. See [Event Web Hook](#event-web-hook). |    ### Event Timing Object    The `timing` object describes the event's timing settings (when and how frequent it should run jobs).  It works similarly to the [Unix Cron](https://en.wikipedia.org/wiki/Cron) system, with selections of years, months, days, weekdays, hours and/or minutes.  Each property should be an array of numerical values.  If omitted, it means the same as ""all"" in that category (i.e. asterisk `*` in Cron syntax).    For example, an event with this timing object would run once per hour, on the hour:    ```js  {  	""minutes"": [0]  }  ```    It essentially means every year, every month, every day, every hour, but only on the ""0"" minute.  The scheduler ticks only once a minute, so this only results in running one job for each matching minute.    For another example, this would run twice daily, at 4:30 AM and 4:30 PM:    ```js  {  	""hours"": [4, 16],  	""minutes"": [30]  }  ```    For a more complex example, this would run only in year 2015, from March to May, on the 1st and 15th of the month (but only if also weekdays), at 6AM to 10AM, and on the :15 and :45 of those hours:    ```js  {  	""years"": [2015],  	""months"": [3, 4, 5],  	""days"": [1, 15],  	""weekdays"": [1, 2, 3, 4, 5],  	""hours"": [6, 7, 8, 9, 10],  	""minutes"": [15, 45]  }  ```    Here is a list of all the timing object properties and their descriptions:    | Timing Property | Range | Description |  |-----------------|-------|-------------|  | `years` | âˆž | One or more years in YYYY format. |  | `months` | 1 - 12 | One or more months, where January is 1 and December is 12. |  | `days` | 1 - 31 | One or more month days, from 1 to 31. |  | `weekdays` | 0 - 6 | One or more weekdays, where Sunday is 0, and Saturday is 6 |  | `hours` | 0 - 23 | One or more hours in 24-hour time, from 0 to 23. |  | `minutes` | 0 - 59 | One or more minutes, from 0 to 59. |    # Development    Cronicle runs as a component in the [pixl-server](https://www.npmjs.com/package/pixl-server) framework.  It is highly recommended to read and understand that module and its component system before attempting to develop Cronicle.  The following server components are also used:    | Module Name | Description | License |  |-------------|-------------|---------|  | [pixl-server-api](https://www.npmjs.com/package/pixl-server-api) | A JSON API component for the pixl-server framework. | MIT |  | [pixl-server-storage](https://www.npmjs.com/package/pixl-server-storage) | A key/value/list storage component for the pixl-server framework. | MIT |  | [pixl-server-user](https://www.npmjs.com/package/pixl-server-user) | A basic user login system for the pixl-server framework. | MIT |  | [pixl-server-web](https://www.npmjs.com/package/pixl-server-web) | A web server component for the pixl-server framework. | MIT |    In addition, Cronicle uses the following server-side PixlCore utility modules:    | Module Name | Description | License |  |-------------|-------------|---------|  | [pixl-args](https://www.npmjs.com/package/pixl-args) | A simple module for parsing command line arguments. | MIT |  | [pixl-class](https://www.npmjs.com/package/pixl-class) | A simple module for creating classes, with inheritance and mixins. | MIT |  | [pixl-config](https://www.npmjs.com/package/pixl-config) | A simple JSON configuration loader. | MIT |  | [pixl-json-stream](https://www.npmjs.com/package/pixl-json-stream) | Provides an easy API for sending and receiving JSON records over standard streams (pipes or sockets). | MIT |  | [pixl-logger](https://www.npmjs.com/package/pixl-logger) | A simple logging class which generates bracket delimited log columns. | MIT |  | [pixl-mail](https://www.npmjs.com/package/pixl-mail) | A very simple class for sending e-mail via SMTP. | MIT |  | [pixl-perf](https://www.npmjs.com/package/pixl-perf) | A simple, high precision performance tracking system. | MIT |  | [pixl-request](https://www.npmjs.com/package/pixl-request) | A very simple module for making HTTP requests. | MIT |  | [pixl-tools](https://www.npmjs.com/package/pixl-tools) | A set of miscellaneous utility functions for Node.js. | MIT |  | [pixl-unit](https://www.npmjs.com/package/pixl-unit) | A very simple unit test runner for Node.js. | MIT |    For the client-side, the Cronicle web application is built on the [pixl-webapp](https://www.npmjs.com/package/pixl-webapp) HTML5/CSS/JavaScript framework:    | Module Name | Description | License |  |-------------|-------------|---------|  | [pixl-webapp](https://www.npmjs.com/package/pixl-webapp) | A client-side JavaScript framework, designed to be a base for web applications. | MIT |    ## Installing Dev Tools    For Debian (Ubuntu) OSes:    ```  apt-get install build-essential  ```    For RedHat (Fedora / CentOS):    ```  yum install gcc-c++ make  ```    For Mac OS X, download [Apple's Xcode](https://developer.apple.com/xcode/download/), and then install the [command-line tools](https://developer.apple.com/downloads/).    ## Manual Installation    Here is how you can download the very latest Cronicle dev build and install it manually (may contain bugs!):    ```  git clone https://github.com/jhuckaby/Cronicle.git  cd Cronicle  npm install  node bin/build.js dev  ```    This will keep all JavaScript and CSS unobfuscated (original source served as separate files).    I highly recommend placing the following `.gitignore` file at the base of the project, if you plan on committing changes and sending pull requests:    ```  .gitignore  /node_modules  /work  /logs  /queue  /data  /conf  htdocs/index.html  htdocs/js/common  htdocs/js/external/*  htdocs/fonts/*  htdocs/css/base.css  htdocs/css/c3*  htdocs/css/font*  htdocs/css/mat*  ```    ## Starting in Debug Mode    To start Cronicle in debug mode, issue the following command:    ```  ./bin/debug.sh  ```    This will launch the service without forking a daemon process, and echo the entire debug log contents to the console.  This is great for debugging server-side issues.  Beware of file permissions if you run as a non-root user.  Hit Ctrl-C to shut down the service when in this mode.    Also, you can force it to become the primary server right away, so there is no delay before you can use the web app:    ```  ./bin/debug.sh --master  ```    Do not use the `--master` switch on multiple servers in a cluster.  For multi-server setups, it is much better to wait for Cronicle to decide who should become primary (~60 seconds after startup).    Please note that when starting Cronicle in debug mode, all existing events with [Run All Mode](#run-all-mode) set will instantly be ""caught up"" to the current time, and not run any previous jobs.  Also, some features are not available in debug mode, namely the ""Restart"" and ""Shut Down"" links in the UI.    ## Running Unit Tests    Cronicle comes with a full unit test suite, which runs via the [pixl-unit](https://www.npmjs.com/package/pixl-unit) module (which should be installed automatically).  To run the unit tests, make sure Cronicle isn't already running, and type this:    ```  npm test  ```    If any tests fail, please open a [GitHub issue](https://github.com/jhuckaby/Cronicle/issues) and include the full unit test log, which can be found here:    ```  /opt/cronicle/logs/unit.log  ```    # Companies Using Cronicle  Cronicle is known to be in use by the following companies:  - [Agnes & Dora](https://agnesanddora.com)  - [Sling TV](https://sling.com)  # Colophon    We stand on the shoulders of giants.  Cronicle was inspired by a PHP application called **Ubercron**, which was designed and programmed by [Larry Azlin](http://azlin.com/).  Cheers Larry!    Cronicle was built using these awesome Node modules:    | Module Name | Description | License |  |-------------|-------------|---------|  | [async](https://www.npmjs.com/package/async) | Higher-order functions and common patterns for asynchronous code. | MIT |  | [bcrypt-node](https://www.npmjs.com/package/bcrypt-node) | Native JS implementation of BCrypt for Node. | BSD 3-Clause |  | [chart.js](https://www.npmjs.com/package/chart.js) | Simple HTML5 charts using the canvas element. | MIT |  | [daemon](https://www.npmjs.com/package/daemon) | Add-on for creating \*nix daemons. | MIT |  | [errno](https://www.npmjs.com/package/errno) | Node.js libuv errno details exposed. | MIT |  | [font-awesome](https://www.npmjs.com/package/font-awesome) | The iconic font and CSS framework. | OFL-1.1 and MIT |  | [form-data](https://www.npmjs.com/package/form-data) | A library to create readable ""multipart/form-data"" streams. Can be used to submit forms and file uploads to other web applications. | MIT |  | [formidable](https://www.npmjs.com/package/formidable) | A Node.js module for parsing form data, especially file uploads. | MIT |  | [glob](https://www.npmjs.com/package/glob) | Filesystem globber (`*.js`). | ISC |  | [jstimezonedetect](https://www.npmjs.com/package/jstimezonedetect) | Automatically detects the client or server timezone. | MIT |  | [jquery](https://www.npmjs.com/package/jquery) | JavaScript library for DOM operations. | MIT |  | [mdi](https://www.npmjs.com/package/mdi) | Material Design Webfont. This includes the Stock and Community icons in a single webfont collection. | OFL-1.1 and MIT |  | [mkdirp](https://www.npmjs.com/package/mkdirp) | Recursively mkdir, like `mkdir -p`. | MIT |  | [moment](https://www.npmjs.com/package/moment) | Parse, validate, manipulate, and display dates. | MIT |  | [moment-timezone](https://www.npmjs.com/package/moment-timezone) | Parse and display moments in any timezone. | MIT |  | [netmask](https://www.npmjs.com/package/netmask) | Parses and understands IPv4 CIDR blocks so they can be explored and compared. | MIT |  | [node-static](https://www.npmjs.com/package/node-static) | A simple, compliant file streaming module for node. | MIT |  | [nodemailer](https://www.npmjs.com/package/nodemailer) | Easy as cake e-mail sending from your Node.js applications. | MIT |  | [shell-quote](https://www.npmjs.com/package/shell-quote) | Quote and parse shell commands. | MIT |  | [socket.io](https://www.npmjs.com/package/socket.io) | Node.js real-time framework server (Websockets). | MIT |  | [socket.io-client](https://www.npmjs.com/package/socket.io-client) | Client library for server-to-server socket.io connections. | MIT |  | [uglify-js](https://www.npmjs.com/package/uglify-js) | JavaScript parser, mangler/compressor and beautifier toolkit. | BSD-2-Clause |  | [zxcvbn](https://www.npmjs.com/package/zxcvbn) | Realistic password strength estimation, from Dropbox. | MIT |    # License    The MIT License (MIT)    Copyright (c) 2015 - 2022 Joseph Huckaby    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. """
Big data;https://github.com/jacomyal/sigma.js;"""[![Build Status](https://travis-ci.org/jacomyal/sigma.js.svg)](https://travis-ci.org/jacomyal/sigma.js)    sigma.js - v1.2.1  =================    Sigma is a JavaScript library dedicated to graph drawing, mainly developed by [@jacomyal](https://github.com/jacomyal) and [@Yomguithereal](https://github.com/Yomguithereal).    ### Resources    [The website](http://sigmajs.org) provides a global overview of the project, and the documentation is available in the [GitHub Wiki](https://github.com/jacomyal/sigma.js/wiki).    Also, the `plugins` and `examples` directories contain various use-cases that might help you understand how to use sigma.    ### How to use it    To use it, clone the repository:    ```  git clone git@github.com:jacomyal/sigma.js.git  ```    To build the code:     - Install [Node.js](http://nodejs.org/).   - Install [gjslint](https://developers.google.com/closure/utilities/docs/linter_howto?hl=en).   - Use `npm install` to install sigma development dependencies.   - Use `npm run build` to minify the code with [Uglify](https://github.com/mishoo/UglifyJS). The minified file `sigma.min.js` will then be accessible in the `build/` folder.    Also, you can customize the build by adding or removing files from the `coreJsFiles` array in `Gruntfile.js` before applying the grunt task.    ### Contributing    You can contribute by submitting [issues tickets](http://github.com/jacomyal/sigma.js/issues) and proposing [pull requests](http://github.com/jacomyal/sigma.js/pulls). Make sure that tests and linting pass before submitting any pull request by running the command `grunt`.    The whole source code is validated by the [Google Closure Linter](https://developers.google.com/closure/utilities/) and [JSHint](http://www.jshint.com/), and the comments are written in [JSDoc](http://en.wikipedia.org/wiki/JSDoc) (tags description is available [here](https://developers.google.com/closure/compiler/docs/js-for-compiler)). """
Big data;https://github.com/datasalt/pangool;"""Pangool  =======    Pangool is a framework on top of Hadoop that implements Tuple MapReduce.    More info in:    http://datasalt.github.io/pangool """
Big data;https://github.com/transceptor-technology/siridb-server;"""[![CI](https://github.com/SiriDB/siridb-server/workflows/CI/badge.svg)](https://github.com/SiriDB/siridb-server/actions)  [![Release Version](https://img.shields.io/github/release/SiriDB/siridb-server)](https://github.com/SiriDB/siridb-server/releases)    SiriDB Server  =============  SiriDB is a highly-scalable, robust and super fast time series database.    ---------------------------------------    * [Installation](#installation)      * [Ubuntu](#ubuntu)      * [Compile from source](#compile-from-source)        * [Linux](#linux)        * [OSX](#osx)        * [Docker](#docker)        * [Configuration](#configuration)      * [Build Debian package](#build-debian-package)      * [Run integration tests](#run-integration-tests)    * [Create or expand a database](#create-or-expand-a-database)    * [Using SiriDB](#using-siridb)      * [SiriDB Connectors](#siridb-connectors)      * [SiriDB HTTP](#siridb-http)      * [SiriDB Prompt](#siridb-prompt)      * [Grafana](#grafana)    * [API/Query language](#query-language)    ---------------------------------------    ## Installation  ### Ubuntu  For Ubuntu we have a deb package available which can be downloaded [here](https://github.com/SiriDB/siridb-server/releases/latest).    Note: SiriDB requires *libexpat1*, *libuv1*, *libpcre2-8-0* and *libcleri0* these libraries can be easily installed using apt:  ```  apt install libexpat1 libuv1 libpcre2-8-0 libcleri0  ```    >Library `libcleri0` is available from Ubuntu 18.04, for older versions a deb package can be found here:  >https://github.com/cesbit/libcleri/releases/latest    The .deb package installs a configuration file at `/etc/siridb/siridb.conf`. You might want to view or change this file before starting SiriDB.    ### Compile from source  >From version 2.0.19 libcleri is not included as part of this source anymore  >and needs to be installed separately. libcleri can be found here:  >[https://github.com/cesbit/libcleri](https://github.com/cesbit/libcleri)  >or can be installed using `apt`.    #### Linux  Install the following requirements: (Ubuntu 18.04)  ```  sudo apt install libcleri-dev  sudo apt install libpcre2-dev  sudo apt install libuv1-dev  sudo apt install libyajl-dev  sudo apt install uuid-dev  ```    Compile (replace Release with Debug for a debug build):  ```  cd ./Release  make clean  make test  make  ```    Install  ```  sudo make install  ```    #### OSX  >Make sure [libcleri](https://github.com/cesbit/libcleri) is installed!    Install the following requirements:  ```  brew install pcre2  brew install libuv  brew install yajl  brew install ossp-uuid  ```  Compile (replace Release with Debug for a debug build):  ```  cd ./Release  export CFLAGS=""-I/usr/local/include""  export LDFLAGS=""-L/usr/local/lib""  make clean  make test  make  ```    Install  ```  sudo make install  ```    #### Docker    ```bash  docker run \      -d \      -p 9000:9000 \      -p 9080:9080 \      -p 8080:8080 \      -v ~/siridb-data:/var/lib/siridb \      ghcr.io/siridb/siridb-server:latest      ```    #### Configuration  SiriDB accepts a configuration file or environment variable as configuration. By default SiriDB will search for the configuration file in `/etc/siridb/siridb.conf` but alternatively you can specify a custom path by using the `-c/--config` argument or use environment variable.    An example configuration file can be found here:  [https://github.com/SiriDB/siridb-server/blob/master/siridb.conf](https://github.com/SiriDB/siridb-server/blob/master/siridb.conf)    ### Build Debian package:    Install required packages (*autopkgtest is required for running the tests*)  ```  apt-get install devscripts lintian help2man autopkgtest  ```    Create archive  ```  git archive -o ../siridb-server_2.0.31.orig.tar.gz master  ```    Run tests  ```  autopkgtest -B -- null  ```    Build package  ```  debuild -us -uc  ```    ## Run integration tests  The simplest way to run the integration tests is to use [docker](https://docs.docker.com/install/).    Build integration test image  ```  docker build -t siridb/itest -f itest/Dockerfile .  ```    Run integration tests  ```  docker run siridb/itest:latest  ```    ## Create or expand a database  [SiriDB Admin](https://github.com/SiriDB/siridb-admin) can be used for creating a new database or expanding an existing database with a new server. Documentation on how to install and use the admin tool can be found at the [siridb-admin](https://github.com/SiriDB/siridb-admin#readme) github project. Binaries are available for most platforms and can be downloaded from [here](https://github.com/SiriDB/siridb-admin/releases/latest). As an alternative it is possible to use a simple [HTTP API](https://docs.siridb.net/connect/http_api/) for creating or expanding a SiriDB database.    ## Using SiriDB  SiriDB has several tools available to connect to a SiriDB database.    ### SiriDB Connectors  The following native connectors are available:   - [C/C++](https://github.com/SiriDB/libsiridb#readme)   - [Python](https://github.com/SiriDB/siridb-connector#readme)   - [Go](https://github.com/SiriDB/go-siridb-connector#readme)   - [Node.js](https://github.com/SiriDB/siridb-nodejs-addon#readme)    When not using one of the above, you can still communicate to SiriDB using [SiriDB HTTP](#siridb-http).    ### SiriDB HTTP  [SiriDB HTTP](https://github.com/SiriDB/siridb-http#readme) provides a HTTP API for SiriDB and has support for JSON, MsgPack, Qpack, CSV and Socket.io. SiriDB HTTP also has an optional web interface and SSL support.    ### SiriDB Prompt  [SiriDB Prompt](https://github.com/SiriDB/siridb-prompt#readme) can be used as a command line SiriDB client with auto-completion support and can be used to load json or csv data into a SiriDB database. Click [here](https://github.com/SiriDB/siridb-prompt/blob/master/README.md) for more information about SiriDB Prompt.    ### Grafana  [SiriDB Grafana Datasource](https://github.com/SiriDB/grafana-siridb-http-datasource#readme) is a plugin for Grafana. See the following blog article on how to configure and use this plugin: https://github.com/SiriDB/grafana-siridb-http-example.    ## Query language  Documentation about the query language can be found at https://siridb.net/documentation. """
Big data;https://github.com/benedekrozemberczki/shapley;"""[pypi-image]: https://badge.fury.io/py/shapley.svg  [pypi-url]: https://pypi.python.org/pypi/shapley  [size-image]: https://img.shields.io/github/repo-size/benedekrozemberczki/shapley.svg  [size-url]: https://github.com/benedekrozemberczki/shapley/archive/master.zip  [build-image]: https://github.com/benedekrozemberczki/shapley/workflows/CI/badge.svg  [build-url]: https://github.com/benedekrozemberczki/shapley/actions?query=workflow%3ACI  [docs-image]: https://readthedocs.org/projects/shapley/badge/?version=latest  [docs-url]: https://shapley.readthedocs.io/en/latest/?badge=latest  [coverage-image]: https://codecov.io/gh/benedekrozemberczki/shapley/branch/master/graph/badge.svg  [coverage-url]: https://codecov.io/github/benedekrozemberczki/shapley?branch=master  [arxiv-image]: https://img.shields.io/badge/ArXiv-2101.02153-orange.svg  [arxiv-url]: https://arxiv.org/abs/2101.02153    <p align=""center"">    <img width=""90%"" src=""https://github.com/benedekrozemberczki/shapley/raw/master/shapley.jpg?sanitize=true"" />  </p>    [![PyPI Version][pypi-image]][pypi-url]  [![Docs Status][docs-image]][docs-url]  [![Repo size][size-image]][size-url]  [![Code Coverage][coverage-image]][coverage-url]  [![Build Status][build-image]][build-url]  [![Arxiv][arxiv-image]][arxiv-url]    **[Documentation](https://shapley.readthedocs.io)** | **[External Resources](https://shapley.readthedocs.io/en/latest/notes/resources.html)**  | **[Research Paper](https://arxiv.org/abs/2101.02153)**    *Shapley* is a Python library for evaluating binary classifiers in a machine learning ensemble.    The library consists of various methods to compute (approximate) the Shapley value of players (models) in weighted voting games (ensemble games) - a class of transferable utility cooperative games. We covered the exact enumeration based computation and various widely know approximation methods from economics and computer science research papers. There are also functionalities to identify the heterogeneity of the player pool based on the [Shapley entropy](https://arxiv.org/abs/2101.02153). In addition, the framework comes with a [detailed documentation](https://shapley.readthedocs.io/en/latest/), an intuitive [tutorial](https://shapley.readthedocs.io/en/latest/notes/introduction.html), 100% test coverage, and illustrative toy [examples](https://github.com/benedekrozemberczki/shapley/tree/master/examples).    ----------------------------------------------------------    **Citing**      If you find *Shapley* useful in your research please consider adding the following citation:    ```bibtex  @inproceedings{rozemberczki2021shapley,        title = {{The Shapley Value of Classifiers in Ensemble Games}},         author = {Benedek Rozemberczki and Rik Sarkar},        year = {2021},        booktitle={Proceedings of the 30th ACM International Conference on Information and Knowledge Management},        pages = {1558â€“1567},  }  ```    --------------------------------------------------------------    **A simple example**    Shapley makes solving voting games quite easy - see the accompanying [tutorial](https://shapley.readthedocs.io/en/latest/notes/introduction.html#applications). For example, this is all it takes to solve a weighted voting game with defined on the fly with permutation sampling:    ```python  import numpy as np  from shapley import PermutationSampler    W = np.random.uniform(0, 1, (1, 7))  W = W/W.sum()  q = 0.5    solver = PermutationSampler()  solver.solve_game(W, q)  shapley_values = solver.get_solution()  ```  ----------------------------------------------------------------------------------    **Methods Included**    In detail, the following methods can be used.      * **[Expected Marginal Contribution Approximation](https://shapley.readthedocs.io/en/latest/modules/root.html#shapley.solvers.expected_marginal_contributions.ExpectedMarginalContributions)** from Fatima *et al.*: [A Linear Approximation Method for the Shapley Value](https://www.sciencedirect.com/science/article/pii/S0004370208000696)    * **[Multilinear Extension](https://shapley.readthedocs.io/en/latest/modules/root.html#shapley.solvers.multilinear_extension.MultilinearExtension)** from Owen: [Multilinear Extensions of Games](https://www.jstor.org/stable/2661445?seq=1#metadata_info_tab_contents)    * **[Monte Carlo Permutation Sampling](https://shapley.readthedocs.io/en/latest/modules/root.html#shapley.solvers.permutation_sampler.PermutationSampler)** from Maleki *et al.*: [Bounding the Estimation Error of Sampling-based Shapley Value Approximation](https://arxiv.org/abs/1306.4265)    * **[Exact Enumeration](https://shapley.readthedocs.io/en/latest/modules/root.html#shapley.solvers.exact_enumeration.ExactEnumeration)** from Shapley: [A Value for N-Person Games](https://www.rand.org/pubs/papers/P0295.html)    --------------------------------------------------------------------------------      Head over to our [documentation](https://shapley.readthedocs.io) to find out more about installation, creation of datasets and a full list of implemented methods and available datasets.  For a quick start, check out the [examples](https://github.com/benedekrozemberczki/shapley/tree/master/examples) in the `examples/` directory.    If you notice anything unexpected, please open an [issue](https://benedekrozemberczki/shapley/issues). If you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/shapley/issues).    --------------------------------------------------------------------------------    **Installation**    ```  $ pip install shapley  ```    **Running tests**    ```  $ python setup.py test  ```  ----------------------------------------------------------------------------------    **Running examples**    ```  $ cd examples  $ python permutation_sampler_example.py  ```    ----------------------------------------------------------------------------------    **License**    - [MIT License](https://github.com/benedekrozemberczki/shapley/blob/master/LICENSE) """
Big data;https://github.com/numenta/nupic;"""# <img src=""http://numenta.org/87b23beb8a4b7dea7d88099bfb28d182.svg"" alt=""NuPIC Logo"" width=100/> NuPIC    ## Numenta Platform for Intelligent Computing    The Numenta Platform for Intelligent Computing (**NuPIC**) is a machine intelligence platform that implements the [HTM learning algorithms](https://numenta.com/resources/papers-videos-and-more/). HTM is a detailed computational theory of the neocortex. At the core of HTM are time-based continuous learning algorithms that store and recall spatial and temporal patterns. NuPIC is suited to a variety of problems, particularly anomaly detection and prediction of streaming data sources. For more information, see [numenta.org](http://numenta.org) or the [NuPIC Forum](https://discourse.numenta.org/c/nupic).    For usage guides, quick starts, and API documentation, see <http://nupic.docs.numenta.org/>.    ## This project is in Maintenance Mode    We plan to do minor releases only, and limit changes in NuPIC and NuPIC Core to:    - Fixing critical bugs.  - Features needed to support ongoing research.    ## Installing NuPIC    NuPIC binaries are available for:    - Linux x86 64bit  - OS X 10.9  - OS X 10.10  - Windows 64bit    ### Dependencies    The following dependencies are required to install NuPIC on all operating systems.    - [Python 2.7](https://www.python.org/)  - [pip](https://pip.pypa.io/en/stable/installing/)>=8.1.2  - [setuptools](https://setuptools.readthedocs.io)>=25.2.0  - [wheel](http://pythonwheels.com)>=0.29.0  - [numpy](http://www.numpy.org/)  - C++ 11 compiler like [gcc](https://gcc.gnu.org/) (4.8+) or [clang](http://clang.llvm.org/)    Additional OS X requirements:    - [Xcode command line tools](https://developer.apple.com/library/ios/technotes/tn2339/_index.html)    ### Install    Run the following to install NuPIC:        pip install nupic    ### Test        # From the root of the repo:      py.test tests/unit    ### _Having problems?_    - You may need to use the `--user` flag for the commands above to install in a non-system location (depends on your environment). Alternatively, you can execute the `pip` commands with `sudo` (not recommended).  - You may need to add the `--use-wheel` option if you have an older pip version (wheels are now the default binary package format for pip).    For any other installation issues, please see our [search our forums](https://discourse.numenta.org/search?q=tag%3Ainstallation%20category%3A10) (post questions there). You can report bugs at https://github.com/numenta/nupic/issues.    Live Community Chat: [![Gitter](https://img.shields.io/badge/gitter-join_chat-blue.svg?style=flat)](https://gitter.im/numenta/public?utm_source=badge)    ### Installing NuPIC From Source    To install from local source code, run from the repository root:        pip install .    Use the optional `-e` argument for a developer install.    If you want to build the dependent `nupic.bindings` from source, you should build and install from [`nupic.core`](https://github.com/numenta/nupic.core) prior to installing nupic (since a PyPI release will be installed if `nupic.bindings` isn't yet installed).    - Build:  [![Build Status](https://travis-ci.org/numenta/nupic.png?branch=master)](https://travis-ci.org/numenta/nupic)  [![AppVeyor Status](https://ci.appveyor.com/api/projects/status/4toemh0qtr21mk6b/branch/master?svg=true)](https://ci.appveyor.com/project/numenta-ci/nupic/branch/master)  [![CircleCI](https://circleci.com/gh/numenta/nupic.svg?style=svg)](https://circleci.com/gh/numenta/nupic)  - To cite this codebase: [![DOI](https://zenodo.org/badge/19461/numenta/nupic.svg)](https://zenodo.org/badge/latestdoi/19461/numenta/nupic) """
Big data;https://github.com/pingcap/tidb;"""![](docs/logo_with_text.png)    [![LICENSE](https://img.shields.io/github/license/pingcap/tidb.svg)](https://github.com/pingcap/tidb/blob/master/LICENSE)  [![Language](https://img.shields.io/badge/Language-Go-blue.svg)](https://golang.org/)  [![Build Status](https://travis-ci.org/pingcap/tidb.svg?branch=master)](https://travis-ci.org/pingcap/tidb)  [![Go Report Card](https://goreportcard.com/badge/github.com/pingcap/tidb)](https://goreportcard.com/report/github.com/pingcap/tidb)  [![GitHub release](https://img.shields.io/github/tag/pingcap/tidb.svg?label=release)](https://github.com/pingcap/tidb/releases)  [![GitHub release date](https://img.shields.io/github/release-date/pingcap/tidb.svg)](https://github.com/pingcap/tidb/releases)  [![CircleCI Status](https://circleci.com/gh/pingcap/tidb.svg?style=shield)](https://circleci.com/gh/pingcap/tidb)  [![Coverage Status](https://codecov.io/gh/pingcap/tidb/branch/master/graph/badge.svg)](https://codecov.io/gh/pingcap/tidb)  [![GoDoc](https://img.shields.io/badge/Godoc-reference-blue.svg)](https://godoc.org/github.com/pingcap/tidb)    ## What is TiDB?    TiDB (""Ti"" stands for Titanium) is an open-source NewSQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads. It is MySQL compatible and features horizontal scalability, strong consistency, and high availability.    - __Horizontal Scalability__        TiDB expands both SQL processing and storage by simply adding new nodes. This makes infrastructure capacity planning both easier and more cost-effective than traditional relational databases which only scale vertically.    - __MySQL Compatible Syntax__        TiDB acts like it is a MySQL 5.7 server to your applications. You can continue to use all of the existing MySQL client libraries, and in many cases, you will not need to change a single line of code in your application. Because TiDB is built from scratch, not a MySQL fork, please check out the list of [known compatibility differences](https://docs.pingcap.com/tidb/stable/mysql-compatibility).    - __Distributed Transactions__        TiDB internally shards table into small range-based chunks that we refer to as ""Regions"". Each Region defaults to approximately 100 MiB in size, and TiDB uses an [optimized](https://pingcap.com/blog/async-commit-the-accelerator-for-transaction-commit-in-tidb-5.0) Two-phase commit to ensure that Regions are maintained in a transactionally consistent way.    - __Cloud Native__        TiDB is designed to work in the cloud -- public, private, or hybrid -- making deployment, provisioning, operations, and maintenance simple.        The storage layer of TiDB, called TiKV, is a [Cloud Native Computing Foundation (CNCF) Graduated](https://www.cncf.io/announcements/2020/09/02/cloud-native-computing-foundation-announces-tikv-graduation/) project. The architecture of the TiDB platform also allows SQL processing and storage to be scaled independently of each other in a very cloud-friendly manner.    - __Minimize ETL__        TiDB is designed to support both transaction processing (OLTP) and analytical processing (OLAP) workloads. This means that while you may have traditionally transacted on MySQL and then Extracted, Transformed and Loaded (ETL) data into a column store for analytical processing, this step is no longer required.    - __High Availability__        TiDB uses the Raft consensus algorithm to ensure that data is highly available and safely replicated throughout storage in Raft groups. In the event of failure, a Raft group will automatically elect a new leader for the failed member, and self-heal the TiDB cluster without any required manual intervention. Failure and self-healing operations are also transparent to applications.    For more details and latest updates, see [TiDB docs](https://docs.pingcap.com/tidb/stable) and [release notes](https://docs.pingcap.com/tidb/dev/release-notes).    ## Community    You can join these groups and chats to discuss and ask TiDB related questions:    - [TiDB Internals Forum](https://internals.tidb.io/)  - [Slack Channel](https://slack.tidb.io/invite?team=tidb-community&channel=everyone&ref=pingcap-tidb)  - [TiDB User Group Forum (Chinese)](https://asktug.com)    In addition, you may enjoy following:    - [@PingCAP](https://twitter.com/PingCAP) on Twitter  - Question tagged [#tidb on StackOverflow](https://stackoverflow.com/questions/tagged/tidb)  - The PingCAP Team [English Blog](https://en.pingcap.com/blog) and [Chinese Blog](https://pingcap.com/blog-cn/)    For support, please contact [PingCAP](http://bit.ly/contact_us_via_github).    ## Quick start    ### To start using TiDB Cloud    We provide TiDB Cloud - a fully-managed Database as a Service for you.    See [TiDB Cloud Quick Start](https://docs.pingcap.com/tidbcloud/public-preview/tidb-cloud-quickstart).    ### To start using TiDB    See [Quick Start Guide](https://docs.pingcap.com/tidb/stable/quick-start-with-tidb).    ### To start developing TiDB    See [Get Started](https://pingcap.github.io/tidb-dev-guide/get-started/introduction.html) chapter of [TiDB Dev Guide](https://pingcap.github.io/tidb-dev-guide/index.html).    ## Contributing    The [community repository](https://github.com/pingcap/community) hosts all information about the TiDB community, including how to contribute to TiDB, how TiDB community is governed, how special interest groups are organized, etc.    [<img src=""docs/contribution-map.png"" alt=""contribution-map"" width=""180"">](https://github.com/pingcap/tidb-map/blob/master/maps/contribution-map.md#tidb-is-an-open-source-distributed-htap-database-compatible-with-the-mysql-protocol)    Contributions are welcomed and greatly appreciated. See [Contribution to TiDB](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/introduction.html) for details on typical contribution workflows. For more contributing information, click on the contributor icon above.    ## Adopters    View the current list of in-production TiDB adopters [here](https://docs.pingcap.com/tidb/stable/adopters).    ## Case studies    - [English](https://pingcap.com/case-studies)  - [ç®€ä½“ä¸­æ–‡](https://pingcap.com/cases-cn/)    ## Architecture    ![architecture](./docs/architecture.png)    ## License    TiDB is under the Apache 2.0 license. See the [LICENSE](./LICENSE) file for details.    ## Acknowledgments    - Thanks [cznic](https://github.com/cznic) for providing some great open source tools.  - Thanks [GolevelDB](https://github.com/syndtr/goleveldb), [BoltDB](https://github.com/boltdb/bolt), and [RocksDB](https://github.com/facebook/rocksdb) for their powerful storage engines. """
Big data;https://github.com/ankane/blazer;"""# Blazer    Explore your data with SQL. Easily create charts and dashboards, and share them with your team.    [Try it out](https://blazer.dokkuapp.com)    [![Screenshot](https://blazer.dokkuapp.com/assets/blazer-a10baa40fef1ca2f5bb25fc97bcf261a6a54192fb1ad0f893c0f562b8c7c4697.png)](https://blazer.dokkuapp.com)    Blazer is also available as a [Docker image](https://github.com/ankane/blazer-docker).    :tangerine: Battle-tested at [Instacart](https://www.instacart.com/opensource)    [![Build Status](https://github.com/ankane/blazer/workflows/build/badge.svg?branch=master)](https://github.com/ankane/blazer/actions)    ## Features    - **Multiple data sources** - PostgreSQL, MySQL, Redshift, and [many more](#full-list)  - **Variables** - run the same queries with different values  - **Checks & alerts** - get emailed when bad data appears  - **Audits** - all queries are tracked  - **Security** - works with your authentication system    ## Docs    - [Installation](#installation)  - [Queries](#queries)  - [Charts](#charts)  - [Dashboards](#dashboards)  - [Checks](#checks)  - [Cohorts](#cohorts)  - [Anomaly Detection](#anomaly-detection)  - [Forecasting](#forecasting)  - [Uploads](#uploads)  - [Data Sources](#data-sources)  - [Query Permissions](#query-permissions)    ## Installation    Add this line to your applicationâ€™s Gemfile:    ```ruby  gem ""blazer""  ```    Run:    ```sh  rails generate blazer:install  rails db:migrate  ```    And mount the dashboard in your `config/routes.rb`:    ```ruby  mount Blazer::Engine, at: ""blazer""  ```    For production, specify your database:    ```ruby  ENV[""BLAZER_DATABASE_URL""] = ""postgres://user:password@hostname:5432/database""  ```    Blazer tries to protect against queries which modify data (by running each query in a transaction and rolling it back), but a safer approach is to use a read-only user. [See how to create one](#permissions).    #### Checks (optional)    Be sure to set a host in `config/environments/production.rb` for emails to work.    ```ruby  config.action_mailer.default_url_options = {host: ""blazer.dokkuapp.com""}  ```    Schedule checks to run (with cron, [Heroku Scheduler](https://elements.heroku.com/addons/scheduler), etc). The default options are every 5 minutes, 1 hour, or 1 day, which you can customize. For each of these options, set up a task to run.    ```sh  rake blazer:run_checks SCHEDULE=""5 minutes""  rake blazer:run_checks SCHEDULE=""1 hour""  rake blazer:run_checks SCHEDULE=""1 day""  ```    You can also set up failing checks to be sent once a day (or whatever you prefer).    ```sh  rake blazer:send_failing_checks  ```    Hereâ€™s what it looks like with cron.    ```  */5 * * * * rake blazer:run_checks SCHEDULE=""5 minutes""  0   * * * * rake blazer:run_checks SCHEDULE=""1 hour""  30  7 * * * rake blazer:run_checks SCHEDULE=""1 day""  0   8 * * * rake blazer:send_failing_checks  ```    For Slack notifications, create an [incoming webhook](https://slack.com/apps/A0F7XDUAZ-incoming-webhooks) and set:    ```sh  BLAZER_SLACK_WEBHOOK_URL=https://hooks.slack.com/...  ```    Name the webhook â€œBlazerâ€ and add a cool icon.    ## Authentication    Donâ€™t forget to protect the dashboard in production.    ### Basic Authentication    Set the following variables in your environment or an initializer.    ```ruby  ENV[""BLAZER_USERNAME""] = ""andrew""  ENV[""BLAZER_PASSWORD""] = ""secret""  ```    ### Devise    ```ruby  authenticate :user, ->(user) { user.admin? } do    mount Blazer::Engine, at: ""blazer""  end  ```    ### Other    Specify a `before_action` method to run in `blazer.yml`.    ```yml  before_action_method: require_admin  ```    You can define this method in your `ApplicationController`.    ```ruby  def require_admin    # depending on your auth, something like...    redirect_to root_path unless current_user && current_user.admin?  end  ```    Be sure to render or redirect for unauthorized users.    ## Permissions    Blazer runs each query in a transaction and rolls it back to prevent queries from modifying data. As an additional line of defense, we recommend using a read only user.    ### PostgreSQL    Create a user with read only permissions:    ```sql  BEGIN;  CREATE ROLE blazer LOGIN PASSWORD 'secret123';  GRANT CONNECT ON DATABASE database_name TO blazer;  GRANT USAGE ON SCHEMA public TO blazer;  GRANT SELECT ON ALL TABLES IN SCHEMA public TO blazer;  ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO blazer;  COMMIT;  ```    ### MySQL    Create a user with read only permissions:    ```sql  GRANT SELECT, SHOW VIEW ON database_name.* TO blazer@â€™127.0.0.1â€² IDENTIFIED BY â€˜secret123â€˜;  FLUSH PRIVILEGES;  ```    ### MongoDB    Create a user with read only permissions:    ```  db.createUser({user: ""blazer"", pwd: ""password"", roles: [""read""]})  ```    Also, make sure authorization is enabled when you start the server.    ## Sensitive Data    If your database contains sensitive or personal data, check out [Hypershield](https://github.com/ankane/hypershield) to shield it.    ## Encrypted Data    If you need to search encrypted data, use [blind indexing](https://github.com/ankane/blind_index).    You can have Blazer transform specific variables with:    ```ruby  Blazer.transform_variable = lambda do |name, value|    value = User.generate_email_bidx(value) if name == ""email_bidx""    value  end  ```    ## Queries    ### Variables    Create queries with variables.    ```sql  SELECT * FROM users WHERE gender = {gender}  ```    Use `{start_time}` and `{end_time}` for time ranges. [Example](https://blazer.dokkuapp.com/queries/9-time-range-selector?start_time=1997-10-03T05%3A00%3A00%2B00%3A00&end_time=1997-10-04T04%3A59%3A59%2B00%3A00)    ```sql  SELECT * FROM ratings WHERE rated_at >= {start_time} AND rated_at <= {end_time}  ```    ### Smart Variables    [Example](https://blazer.dokkuapp.com/queries/1-smart-variable)    Suppose you have the query:    ```sql  SELECT * FROM users WHERE occupation_id = {occupation_id}  ```    Instead of remembering each occupationâ€™s id, users can select occupations by name.    Add a smart variable with:    ```yml  smart_variables:    occupation_id: ""SELECT id, name FROM occupations ORDER BY name ASC""  ```    The first column is the value of the variable, and the second column is the label.    You can also use an array or hash for static data and enums.    ```yml  smart_variables:    period: [""day"", ""week"", ""month""]    status: {0: ""Active"", 1: ""Archived""}  ```    ### Linked Columns    [Example](https://blazer.dokkuapp.com/queries/3-linked-column) - title column    Link results to other pages in your apps or around the web. Specify a column name and where it should link to. You can use the value of the result with `{value}`.    ```yml  linked_columns:    user_id: ""/admin/users/{value}""    ip_address: ""https://www.infosniper.net/index.php?ip_address={value}""  ```    ### Smart Columns    [Example](https://blazer.dokkuapp.com/queries/2-smart-column) - occupation_id column    Suppose you have the query:    ```sql  SELECT name, city_id FROM users  ```    See which city the user belongs to without a join.    ```yml  smart_columns:    city_id: ""SELECT id, name FROM cities WHERE id IN {value}""  ```    You can also use a hash for static data and enums.    ```yml  smart_columns:    status: {0: ""Active"", 1: ""Archived""}  ```    ### Caching    Blazer can automatically cache results to improve speed. It can cache slow queries:    ```yml  cache:    mode: slow    expires_in: 60 # min    slow_threshold: 15 # sec  ```    Or it can cache all queries:    ```yml  cache:    mode: all    expires_in: 60 # min  ```    Of course, you can force a refresh at any time.    ## Charts    Blazer will automatically generate charts based on the types of the columns returned in your query.    **Note:** The order of columns matters.    ### Line Chart    There are two ways to generate line charts.    2+ columns - timestamp, numeric(s) - [Example](https://blazer.dokkuapp.com/queries/4-line-chart-format-1)    ```sql  SELECT date_trunc('week', created_at), COUNT(*) FROM users GROUP BY 1  ```    3 columns - timestamp, string, numeric - [Example](https://blazer.dokkuapp.com/queries/5-line-chart-format-2)      ```sql  SELECT date_trunc('week', created_at), gender, COUNT(*) FROM users GROUP BY 1, 2  ```    ### Column Chart    There are also two ways to generate column charts.    2+ columns - string, numeric(s) - [Example](https://blazer.dokkuapp.com/queries/6-column-chart-format-1)    ```sql  SELECT gender, COUNT(*) FROM users GROUP BY 1  ```    3 columns - string, string, numeric - [Example](https://blazer.dokkuapp.com/queries/7-column-chart-format-2)    ```sql  SELECT gender, zip_code, COUNT(*) FROM users GROUP BY 1, 2  ```    ### Scatter Chart    2 columns - both numeric - [Example](https://blazer.dokkuapp.com/queries/16-scatter-chart)    ```sql  SELECT x, y FROM table  ```    ### Pie Chart    2 columns - string, numeric - and last column named `pie` - [Example](https://blazer.dokkuapp.com/queries/17-pie-chart)    ```sql  SELECT gender, COUNT(*) AS pie FROM users GROUP BY 1  ```    ### Maps    Columns named `latitude` and `longitude` or `lat` and `lon` or `lat` and `lng` - [Example](https://blazer.dokkuapp.com/queries/15-map)    ```sql  SELECT name, latitude, longitude FROM cities  ```    To enable, get an access token from [Mapbox](https://www.mapbox.com/) and set `ENV[""MAPBOX_ACCESS_TOKEN""]`.    ### Targets    Use the column name `target` to draw a line for goals. [Example](https://blazer.dokkuapp.com/queries/8-target-line)    ```sql  SELECT date_trunc('week', created_at), COUNT(*) AS new_users, 100000 AS target FROM users GROUP BY 1  ```    ## Dashboards    Create a dashboard with multiple queries. [Example](https://blazer.dokkuapp.com/dashboards/1-dashboard-demo)    If the query has a chart, the chart is shown. Otherwise, youâ€™ll see a table.    If any queries have variables, they will show up on the dashboard.    ## Checks    Checks give you a centralized place to see the health of your data. [Example](https://blazer.dokkuapp.com/checks)    Create a query to identify bad rows.    ```sql  SELECT * FROM ratings WHERE user_id IS NULL /* all ratings should have a user */  ```    Then create check with optional emails if you want to be notified. Emails are sent when a check starts failing, and when it starts passing again.    ## Cohorts    Create a cohort analysis from a simple SQL query. [Example](https://blazer.dokkuapp.com/queries/19-cohort-analysis-from-first-order)    Create a query with the comment `/* cohort analysis */`. The result should have columns named `user_id` and `conversion_time` and optionally `cohort_time`.    You can generate cohorts from the first conversion time:    ```sql  /* cohort analysis */  SELECT user_id, created_at AS conversion_time FROM orders  ```    (the first conversion isnâ€™t counted in the first time period with this format)    Or from another time, like sign up:    ```sql  /* cohort analysis */  SELECT users.id AS user_id, orders.created_at AS conversion_time, users.created_at AS cohort_time  FROM users LEFT JOIN orders ON orders.user_id = users.id  ```    This feature requires PostgreSQL or MySQL 8.    ## Anomaly Detection    Blazer supports three different approaches to anomaly detection.    ### Prophet    Add [prophet-rb](https://github.com/ankane/prophet) to your Gemfile:    ```ruby  gem ""prophet-rb""  ```    And add to `config/blazer.yml`:    ```yml  anomaly_checks: prophet  ```    ### Trend    [Trend](https://trendapi.org/) uses an external service by default, but you can run it on your own infrastructure as well.    Add [trend](https://github.com/ankane/trend) to your Gemfile:    ```ruby  gem ""trend""  ```    And add to `config/blazer.yml`:    ```yml  anomaly_checks: trend  ```    For the [self-hosted API](https://github.com/ankane/trend-api), create an initializer with:    ```ruby  Trend.url = ""http://localhost:8000""  ```    ### AnomalyDetection.rb (experimental)    Add [anomaly_detection](https://github.com/ankane/AnomalyDetection.rb) to your Gemfile:    ```ruby  gem ""anomaly_detection""  ```    And add to `config/blazer.yml`:    ```yml  anomaly_checks: anomaly_detection  ```    ## Forecasting    Blazer supports for two different forecasting methods. [Example](https://blazer.dokkuapp.com/queries/18-forecast?forecast=t)    A forecast link will appear for queries that return 2 columns with types timestamp and numeric.    ### Prophet    Add [prophet-rb](https://github.com/ankane/prophet) to your Gemfile:    ```ruby  gem ""prophet-rb"", "">= 0.2.1""  ```    And add to `config/blazer.yml`:    ```yml  forecasting: prophet  ```    ### Trend    [Trend](https://trendapi.org/) uses an external service by default, but you can run it on your own infrastructure as well.    Add [trend](https://github.com/ankane/trend) to your Gemfile:    ```ruby  gem ""trend""  ```    And add to `config/blazer.yml`:    ```yml  forecasting: trend  ```    For the [self-hosted API](https://github.com/ankane/trend-api), create an initializer with:    ```ruby  Trend.url = ""http://localhost:8000""  ```    ## Uploads    Creating database tables from CSV files. [Example](https://blazer.dokkuapp.com/uploads)    Run:    ```sh  rails generate blazer:uploads  rails db:migrate  ```    And add to `config/blazer.yml`:    ```yml  uploads:    url: postgres://...    schema: uploads    data_source: main  ```    This feature requires PostgreSQL. Create a new schema just for uploads.    ```sql  CREATE SCHEMA uploads;  ```    ## Data Sources    Blazer supports multiple data sources :tada:    Add additional data sources in `config/blazer.yml`:    ```yml  data_sources:    main:      url: <%= ENV[""BLAZER_DATABASE_URL""] %>      # timeout, smart_variables, linked_columns, smart_columns    catalog:      url: <%= ENV[""CATALOG_DATABASE_URL""] %>      # ...    redshift:      url: <%= ENV[""REDSHIFT_DATABASE_URL""] %>      # ...  ```    ### Full List    - [Amazon Athena](#amazon-athena)  - [Amazon Redshift](#amazon-redshift)  - [Apache Drill](#apache-drill)  - [Apache Hive](#apache-hive)  - [Apache Ignite](#apache-ignite)  - [Apache Spark](#apache-spark)  - [Cassandra](#cassandra)  - [Druid](#druid)  - [Elasticsearch](#elasticsearch)  - [Google BigQuery](#google-bigquery)  - [IBM DB2 and Informix](#ibm-db2-and-informix)  - [InfluxDB](#influxdb)  - [MongoDB](#mongodb-1)  - [MySQL](#mysql-1)  - [Neo4j](#neo4j)  - [OpenSearch](#opensearch)  - [Oracle](#oracle)  - [PostgreSQL](#postgresql-1)  - [Presto](#presto)  - [Salesforce](#salesforce)  - [Socrata Open Data API (SODA)](#socrata-open-data-api-soda)  - [Snowflake](#snowflake)  - [SQLite](#sqlite)  - [SQL Server](#sql-server)    You can also [create an adapter](#creating-an-adapter) for any other data store.    **Note:** In the examples below, we recommend using environment variables for urls.    ```yml  data_sources:    my_source:      url: <%= ENV[""BLAZER_MY_SOURCE_URL""] %>  ```    ### Amazon Athena    Add [aws-sdk-athena](https://github.com/aws/aws-sdk-ruby) and [aws-sdk-glue](https://github.com/aws/aws-sdk-ruby) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: athena      database: database        # optional settings      output_location: s3://some-bucket/      workgroup: primary      access_key_id: ...      secret_access_key: ...  ```    Hereâ€™s an example IAM policy:    ```json  {      ""Version"": ""2012-10-17"",      ""Statement"": [          {              ""Effect"": ""Allow"",              ""Action"": [                  ""athena:GetQueryExecution"",                  ""athena:GetQueryResults"",                  ""athena:StartQueryExecution""              ],              ""Resource"": [                  ""arn:aws:athena:region:account-id:workgroup/primary""              ]          },          {              ""Effect"": ""Allow"",              ""Action"": [                  ""glue:GetTable"",                  ""glue:GetTables""              ],              ""Resource"": [                  ""arn:aws:glue:region:account-id:catalog"",                  ""arn:aws:glue:region:account-id:database/default"",                  ""arn:aws:glue:region:account-id:table/default/*""              ]          }      ]  }  ```    You also need to configure [S3 permissions](https://aws.amazon.com/premiumsupport/knowledge-center/access-denied-athena/).    ### Amazon Redshift    Add [activerecord6-redshift-adapter](https://github.com/kwent/activerecord6-redshift-adapter) or [activerecord5-redshift-adapter](https://github.com/ConsultingMD/activerecord5-redshift-adapter) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: redshift://user:password@hostname:5439/database  ```    ### Apache Drill    Add [drill-sergeant](https://github.com/ankane/drill-sergeant) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: drill      url: http://hostname:8047  ```    ### Apache Hive    Add [hexspace](https://github.com/ankane/hexspace) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: hive      url: sasl://user:password@hostname:10000/database  ```    Use a [read-only user](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Authorization). Requires [HiveServer2](https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2).    ### Apache Ignite    Add [ignite-client](https://github.com/ankane/ignite-ruby) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: ignite://user:password@hostname:10800  ```    ### Apache Spark    Add [hexspace](https://github.com/ankane/hexspace) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: spark      url: sasl://user:password@hostname:10000/database  ```    Use a read-only user. Requires the [Thrift server](https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html).    ### Cassandra    Add [cassandra-driver](https://github.com/datastax/ruby-driver) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: cassandra://user:password@hostname:9042/keyspace  ```    ### Druid    Enable [SQL support](http://druid.io/docs/latest/querying/sql.html#configuration) on the broker and set:    ```yml  data_sources:    my_source:      adapter: druid      url: http://hostname:8082  ```    ### Elasticsearch    Add [elasticsearch](https://github.com/elastic/elasticsearch-ruby) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: elasticsearch      url: http://user:password@hostname:9200  ```    ### Google BigQuery    Add [google-cloud-bigquery](https://github.com/GoogleCloudPlatform/google-cloud-ruby/tree/master/google-cloud-bigquery) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: bigquery      project: your-project      keyfile: path/to/keyfile.json  ```    ### IBM DB2 and Informix    Add [ibm_db](https://github.com/ibmdb/ruby-ibmdb) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: ibm-db://user:password@hostname:50000/database  ```    ### InfluxDB    Add [influxdb](https://github.com/influxdata/influxdb-ruby) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: influxdb      url: http://user:password@hostname:8086/database  ```    Supports [InfluxQL](https://docs.influxdata.com/influxdb/v1.8/query_language/explore-data/)    ### MongoDB    *Requires MongoDB < 4.2 at the moment*    Add [mongo](https://github.com/mongodb/mongo-ruby-driver) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: mongodb://user:password@hostname:27017/database  ```    ### MySQL    Add [mysql2](https://github.com/brianmario/mysql2) to your Gemfile (if itâ€™s not there) and set:    ```yml  data_sources:    my_source:      url: mysql2://user:password@hostname:3306/database  ```    ### Neo4j    Add [neo4j-core](https://github.com/neo4jrb/neo4j-core) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: neo4j      url: http://user:password@hostname:7474  ```    ### OpenSearch    Add [opensearch-ruby](https://github.com/opensearch-project/opensearch-ruby) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: opensearch      url: http://user:password@hostname:9200  ```    ### Oracle    Add [activerecord-oracle_enhanced-adapter](https://github.com/rsim/oracle-enhanced) and [ruby-oci8](https://github.com/kubo/ruby-oci8) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: oracle-enhanced://user:password@hostname:1521/database  ```    ### PostgreSQL    Add [pg](https://github.com/ged/ruby-pg) to your Gemfile (if itâ€™s not there) and set:    ```yml  data_sources:    my_source:      url: postgres://user:password@hostname:5432/database  ```    ### Presto    Add [presto-client](https://github.com/treasure-data/presto-client-ruby) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: presto://user@hostname:8080/catalog  ```    ### Salesforce    Add [restforce](https://github.com/restforce/restforce) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: salesforce  ```    And set the appropriate environment variables:    ```sh  SALESFORCE_USERNAME=""username""  SALESFORCE_PASSWORD=""password""  SALESFORCE_SECURITY_TOKEN=""security token""  SALESFORCE_CLIENT_ID=""client id""  SALESFORCE_CLIENT_SECRET=""client secret""  SALESFORCE_API_VERSION=""41.0""  ```    Supports [SOQL](https://developer.salesforce.com/docs/atlas.en-us.soql_sosl.meta/soql_sosl/sforce_api_calls_soql.htm)    ### Socrata Open Data API (SODA)    Set:    ```yml  data_sources:    my_source:      adapter: soda      url: https://soda.demo.socrata.com/resource/4tka-6guv.json      app_token: ...  ```    Supports [SoQL](https://dev.socrata.com/docs/functions/)    ### Snowflake    First, install ODBC. For Homebrew, use:    ```sh  brew install unixodbc  ```    For Ubuntu, use:    ```sh  sudo apt-get install unixodbc-dev  ```    For Heroku, use the [Apt buildpack](https://github.com/heroku/heroku-buildpack-apt) and create an `Aptfile` with:    ```text  unixodbc-dev  https://sfc-repo.snowflakecomputing.com/odbc/linux/2.21.5/snowflake-odbc-2.21.5.x86_64.deb  ```    > This installs the driver at `/app/.apt/usr/lib/snowflake/odbc/lib/libSnowflake.so`    Then, download the [Snowflake ODBC driver](https://docs.snowflake.net/manuals/user-guide/odbc-download.html). Add [odbc_adapter](https://github.com/localytics/odbc_adapter) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: snowflake      conn_str: Driver=/path/to/libSnowflake.so;uid=user;pwd=password;server=host.snowflakecomputing.com  ```    ### SQLite    Add [sqlite3](https://github.com/sparklemotion/sqlite3-ruby) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: sqlite3:path/to/database.sqlite3  ```    ### SQL Server    Add [tiny_tds](https://github.com/rails-sqlserver/tiny_tds) and [activerecord-sqlserver-adapter](https://github.com/rails-sqlserver/activerecord-sqlserver-adapter) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: sqlserver://user:password@hostname:1433/database  ```    ## Creating an Adapter    Create an adapter for any data store with:    ```ruby  class FooAdapter < Blazer::Adapters::BaseAdapter    # code goes here  end    Blazer.register_adapter ""foo"", FooAdapter  ```    See the [Presto adapter](https://github.com/ankane/blazer/blob/master/lib/blazer/adapters/presto_adapter.rb) for a good example. Then use:    ```yml  data_sources:    my_source:      adapter: foo      url: http://user:password@hostname:9200/  ```    ## Query Permissions    Blazer supports a basic permissions model.    1. Queries without a name are unlisted  2. Queries whose name starts with `#` are only listed to the creator  3. Queries whose name starts with `*` can only be edited by the creator    ## Learn SQL    Have team members who want to learn SQL? Here are a few great, free resources.    - [The Data School](https://dataschool.com/learn-sql/)  - [SQLBolt](https://sqlbolt.com/)    ## Useful Tools    For an easy way to group by day, week, month, and more with correct time zones, check out [Groupdate.sql](https://github.com/ankane/groupdate.sql).    ## Standalone Version    Looking for a standalone version? Check out [Ghost Blazer](https://github.com/buren/ghost_blazer).    ## Performance    By default, queries take up a request while they are running. To run queries asynchronously, add to your config:    ```yml  async: true  ```    **Note:** Requires Rails 5+ and caching to be enabled. If you have multiple web processes, your app must use a centralized cache store like Memcached or Redis.    ```ruby  config.cache_store = :mem_cache_store  ```    ## Archiving    Archive queries that havenâ€™t been viewed in over 90 days.    ```sh  rake blazer:archive_queries  ```    ## Content Security Policy    If views are stuck with a `Loading...` message, there might be a problem with strict CSP settings in your app. This can be checked with Firefox or Chrome dev tools. You can allow Blazer to override these settings for its controllers with:    ```yml  override_csp: true  ```    ## Upgrading    ### 2.3    To archive queries, create a migration    ```sh  rails g migration add_status_to_blazer_queries  ```    with:    ```ruby  add_column :blazer_queries, :status, :string  Blazer::Query.update_all(status: ""active"")  ```    ### 2.0    To use Slack notifications, create a migration    ```sh  rails g migration add_slack_channels_to_blazer_checks  ```    with:    ```ruby  add_column :blazer_checks, :slack_channels, :text  ```    ## History    View the [changelog](https://github.com/ankane/blazer/blob/master/CHANGELOG.md)    ## Thanks    Blazer uses a number of awesome open source projects, including [Rails](https://github.com/rails/rails/), [Vue.js](https://github.com/vuejs/vue), [jQuery](https://github.com/jquery/jquery), [Bootstrap](https://github.com/twbs/bootstrap), [Selectize](https://github.com/brianreavis/selectize.js), [StickyTableHeaders](https://github.com/jmosbech/StickyTableHeaders), [Stupid jQuery Table Sort](https://github.com/joequery/Stupid-Table-Plugin), and [Date Range Picker](https://github.com/dangrossman/bootstrap-daterangepicker).    Demo data from [MovieLens](https://grouplens.org/datasets/movielens/).    ## Want to Make Blazer Better?    Thatâ€™s awesome! Here are a few ways you can help:    - [Report bugs](https://github.com/ankane/blazer/issues)  - Fix bugs and [submit pull requests](https://github.com/ankane/blazer/pulls)  - Write, clarify, or fix documentation  - Suggest or add new features    Check out the [dev app](https://github.com/ankane/blazer-dev) to get started. """
Big data;https://github.com/Treode/store;"""# TreodeDB    TreodeDB is a distributed database that provides multirow atomic writes, and it&#700;s designed for RESTful services.    TreodeDB    - is a key-value store  - offers replication for fault tolerance  - offers sharding for scalability  - offers transactions to provide consistency  - tracks versioned data to [extend transactions through a CDN or cache][cbw]  - can feed an [Apache Spark][apache-spark]&trade; RDD or an [Apache Hadoop][apache-hadoop]&trade; InputFormat for analytics  - can feed an [Apache Spark][apache-spark]&trade; DStream for streaming analytics    ![Architecture][arch]      ## Documentation    - [User Docs][user-docs]  - Presentation: [slides][presentation-slides], [video][presentation-video]      [apache-hadoop]: https://hadoop.apache.org ""Apache Hadoop&trade;""    [apache-spark]: https://spark.apache.org ""Apache Spark&trade;""    [arch]: architecture.png ""Architecture""    [cbw]: http://treode.github.io/cbw/ ""Conditional Batch Write""    [presentation-slides]: http://goo.gl/le0rjT ""Slides, SF Bay Chapter of the ACM, Mar 18 2015""    [presentation-video]: https://www.youtube.com/watch?v=sI8vtAjO7x4&list=PL87GtQd0bfJyd9_TEKLbuTTdLFCedM-yw ""Video, SF Bay Chapter of the ACM, Mar 18 2015""    [user-docs]: http://treode.github.io ""TreodeDB Walkthroughs"" """
Big data;https://github.com/damballa/parkour;"""# Parkour    [![Build Status](https://secure.travis-ci.org/damballa/parkour.png)](http://travis-ci.org/damballa/parkour)    Hadoop MapReduce in idiomatic Clojure.  Parkour takes your Clojure codeâ€™s  functional gymnastics and sends it free-running across the urban environment of  your Hadoop cluster.    Parkour is a Clojure library for writing distributed programs in the MapReduce  pattern which run on the Hadoop MapReduce platform.  Parkour does its best to  avoid being yet another â€œframeworkâ€ â€“ if you know Hadoop, and you know Clojure,  then youâ€™re most of the way to knowing Parkour.  By combining functional  programming, direct access to Hadoop features, and interactive iteration on live  data, Parkour supports rapid development of highly efficient Hadoop MapReduce  applications.    ## Installation    Parkour is available on Clojars.  Add this `:dependency` to your Leiningen  `project.clj`:    ```clj  [com.damballa/parkour ""0.6.3""]  ```    ## Usage    The [Parkour introduction][intro] contains an overview of the key concepts, but  here is the classic â€œword countâ€ example, in Parkour:    ```clj  (defn word-count-m    [coll]    (->> coll         (r/mapcat #(str/split % #""\s+""))         (r/map #(-> [% 1]))))    (defn word-count    [conf lines]    (-> (pg/input lines)        (pg/map #'word-count-m)        (pg/partition [Text LongWritable])        (pg/combine #'ptb/keyvalgroups-r #'+)        (pg/output (seqf/dsink [Text LongWritable]))        (pg/fexecute conf `word-count)))  ```    ## Documentation    Parkourâ€™s documentation is divided into a number of separate sections:    - [Introduction][intro] â€“ A getting-started introduction, with an overview of    Parkourâ€™s key concepts.  - [Motivation][motivation] â€“ An explanation of the goals Parkour exists to    achieve, with comparison to other libraries and frameworks.  - [Namespaces][namespaces] â€“ A tour of Parkourâ€™s namespaces, explaining how each    set of functionality fits into the whole.  - [REPL integration][repl] â€“ A quick guide to using Parkour from a    cluster-connected REPL, for iterative development against live data.  - [MapReduce in depth][mr-detailed] â€“ An in-depth examination of the interfaces    Parkour uses to run your code in MapReduce jobs.  - [Serialization][serialization] â€“ How Parkour integrates Clojure with Hadoop    serialization mechanisms.  - [Unified I/O][unified-io] â€“ Unified collection-like local and distributed I/O    via Parkour dseqs and dsinks.  - [Distributed values][dvals] â€“ Parkourâ€™s value-oriented interface to the Hadoop    distributed cache.  - [Multiple I/O][multi-io] â€“ Configuring multiple inputs and/or outputs for    single Hadoop MapReduce jobs.  - [Reducers vs seqs][reducers-vs-seqs] â€“ Why Parkourâ€™s default idiom uses    reducers, and when to use seqs instead.  - [Testing][testing] â€“ Patterns for testing Parkour MapReduce jobs.  - [Deployment][deployment] â€“ Running Parkour applications on a Hadoop cluster.  - [Reference][api] â€“ Generated API reference, via [codox][codox].    ## Contributing    There is a [Parkour mailing list][mailing-list] hosted by  [Librelist](http://librelist.com/) for announcements and discussion.  To join  the mailing list, just email parkour@librelist.org; your first message to that  address will subscribe you without being posted.  Please report issues on the  [GitHub issue tracker][issues].  And of course, pull requests welcome!    ## License    Copyright Â© 2013-2015 Marshall Bockrath-Vandegrift & Damballa, Inc.    Distributed under the Apache License, Version 2.0.    [intro]: https://github.com/damballa/parkour/blob/master/doc/intro.md  [motivation]: https://github.com/damballa/parkour/blob/master/doc/motivation.md  [namespaces]: https://github.com/damballa/parkour/blob/master/doc/namespaces.md  [repl]: https://github.com/damballa/parkour/blob/master/doc/repl.md  [mr-detailed]: https://github.com/damballa/parkour/blob/master/doc/mr-detailed.md  [serialization]: https://github.com/damballa/parkour/blob/master/doc/serialization.md  [unified-io]: https://github.com/damballa/parkour/blob/master/doc/unified-io.md  [dvals]: https://github.com/damballa/parkour/blob/master/doc/dvals.md  [multi-io]: https://github.com/damballa/parkour/blob/master/doc/multi-io.md  [reducers-vs-seqs]: https://github.com/damballa/parkour/blob/master/doc/reducers-vs-seqs.md  [testing]: https://github.com/damballa/parkour/blob/master/doc/testing.md  [deployment]: https://github.com/damballa/parkour/blob/master/doc/deployment.md  [api]: http://damballa.github.io/parkour/  [codox]: https://github.com/weavejester/codox  [mailing-list]: http://librelist.com/browser/parkour/  [issues]: https://github.com/damballa/parkour/issues """
Big data;https://github.com/apache/pulsar;"""<!--        Licensed to the Apache Software Foundation (ASF) under one      or more contributor license agreements.  See the NOTICE file      distributed with this work for additional information      regarding copyright ownership.  The ASF licenses this file      to you under the Apache License, Version 2.0 (the      ""License""); you may not use this file except in compliance      with the License.  You may obtain a copy of the License at          http://www.apache.org/licenses/LICENSE-2.0        Unless required by applicable law or agreed to in writing,      software distributed under the License is distributed on an      ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY      KIND, either express or implied.  See the License for the      specific language governing permissions and limitations      under the License.    -->    ![logo](site2/website/static/img/pulsar.svg)    Pulsar is a distributed pub-sub messaging platform with a very  flexible messaging model and an intuitive client API.    Learn more about Pulsar at https://pulsar.apache.org    ## Main features    * Horizontally scalable (Millions of independent topics and millions    of messages published per second)  * Strong ordering and consistency guarantees  * Low latency durable storage  * Topic and queue semantics  * Load balancer  * Designed for being deployed as a hosted service:    * Multi-tenant    * Authentication    * Authorization    * Quotas    * Support mixing very different workloads    * Optional hardware isolation  * Keeps track of consumer cursor position  * REST API for provisioning, admin and stats  * Geo replication  * Transparent handling of partitioned topics  * Transparent batching of messages    ## Repositories    This repository is the main repository of Apache Pulsar. Pulsar PMC also maintains other repositories for  components in the Pulsar ecosystem, including connectors, adapters, and other language clients.    - [Pulsar Core](https://github.com/apache/pulsar)    ### Helm Chart    - [Pulsar Helm Chart](https://github.com/apache/pulsar-helm-chart)    ### Ecosystem    - [Pulsar Adapters](https://github.com/apache/pulsar-adapters)  - [Pulsar Connectors](https://github.com/apache/pulsar-connectors)  - [Pulsar SQL (Pulsar Presto Connector)](https://github.com/apache/pulsar-presto)    ### Clients    - [.NET/C# Client](https://github.com/apache/pulsar-dotpulsar)  - [Go Client](https://github.com/apache/pulsar-client-go)  - [NodeJS Client](https://github.com/apache/pulsar-client-node)  - [Ruby Client](https://github.com/apache/pulsar-client-ruby)    ### Dashboard & Management Tools    - [Pulsar Manager](https://github.com/apache/pulsar-manager)    ### Documentation    - [Pulsar Translation](https://github.com/apache/pulsar-translation)    ### CI/CD    - [Pulsar CI](https://github.com/apache/pulsar-test-infra)    ## Build Pulsar    Requirements:   * Java [JDK 11](https://adoptium.net/?variant=openjdk11) or [JDK 8](https://adoptium.net/?variant=openjdk8)   * Maven 3.6.1+   * zip    Compile and install:    ```bash  $ mvn install -DskipTests  ```    Compile and install individual module  ```bash  $ mvn -pl module-name (e.g: pulsar-broker) install -DskipTests  ```    ## Minimal build (This skips most of external connectors and tiered storage handlers)  ```  mvn install -Pcore-modules,-main -DskipTests  ```    Run Unit Tests:    ```bash  $ mvn test  ```    Run Individual Unit Test:    ```bash  $ mvn -pl module-name (e.g: pulsar-client) test -Dtest=unit-test-name (e.g: ConsumerBuilderImplTest)  ```    Run Selected Test packages:    ```bash  $ mvn test -pl module-name (for example, pulsar-broker) -Dinclude=org/apache/pulsar/**/*.java  ```    Start standalone Pulsar service:    ```bash  $ bin/pulsar standalone  ```    Check https://pulsar.apache.org for documentation and examples.    ## Build custom docker images    Docker images must be built with Java 8 for `branch-2.7` or previous branches because of  [issue 8445](https://github.com/apache/pulsar/issues/8445).  Java 11 is the recommended JDK version in `master`/`branch-2.8`.    This builds the docker images `apachepulsar/pulsar-all:latest` and `apachepulsar/pulsar:latest`.    ```bash  mvn clean install -DskipTests  mvn package -Pdocker,-main -am -pl docker/pulsar-all -DskipTests  ```    After the images are built, they can be tagged and pushed to your custom repository.   Here's an example of a bash script that tags the docker images with the current version and git revision and   pushes them to `localhost:32000/apachepulsar`.    ```bash  image_repo_and_project=localhost:32000/apachepulsar  pulsar_version=$(mvn initialize help:evaluate -Dexpression=project.version -pl . -q -DforceStdout)  gitrev=$(git rev-parse HEAD | colrm 10)  tag=""${pulsar_version}-${gitrev}""  echo ""Using tag $tag""  docker tag apachepulsar/pulsar-all:latest ${image_repo_and_project}/pulsar-all:$tag  docker push ${image_repo_and_project}/pulsar-all:$tag  docker tag apachepulsar/pulsar:latest ${image_repo_and_project}/pulsar:$tag  docker push ${image_repo_and_project}/pulsar:$tag  ```    ## Setting up your IDE    Apache Pulsar is using [lombok](https://projectlombok.org/) so you have to ensure your IDE setup with  required plugins.    ### Intellij    #### Configure Project JDK to Java 11 JDK    1. Open **Project Settings**.         Click **File** -> **Project Structure** -> **Project Settings** -> **Project**.       2. Select the JDK version.            From the JDK version drop-down list, select **Download JDK...** or choose an existing recent Java 11 JDK version.    3. In the download dialog, select version **11**. You can pick a version from many vendors. Unless you have a specific preference, choose **Eclipse Temurin (AdoptOpenJDK (Hotspot))**.       #### Configure Java version for Maven in IntelliJ    1. Open Maven Importing Settings dialog by going to      **Settings** -> **Build, Execution, Deployment** -> **Build Tools** -> **Maven** -> **Importing**.    2. Choose **Use Project JDK** for **JDK for Importer** setting. This uses the Java 11 JDK for running Maven      when importing the project to IntelliJ. Some of the configuration in the Maven build is conditional based on      the JDK version. Incorrect configuration gets chosen when the ""JDK for Importer"" isn't the same as the ""Project JDK"".    3. Validate that the JRE setting in **Maven** -> **Runner** dialog is set to **Use Project JDK**.    #### Configure annotation processing in IntelliJ    1. Open Annotation Processors Settings dialog box by going to     **Settings** -> **Build, Execution, Deployment** -> **Compiler** -> **Annotation Processors**.    2. Select the following buttons:     1. **Enable annotation processing**     2. **Obtain processors from project classpath**     3. Store generated sources relative to: **Module content root**    3. Set the generated source directories to be equal to the Maven directories:     1. Set ""Production sources directory:"" to ""target/generated-sources/annotations"".     2. Set ""Test sources directory:"" to ""target/generated-test-sources/test-annotations"".    4. Click **OK**.    5. Install the lombok plugin in intellij.    #### Configure code style    1. Open Code Style Settings dialog box by going to **Settings** -> **Editor** -> **Code Style**.    2. Click on the :gear: symbol -> **Import scheme** -> **Intellij IDEA code style XML**    3. Pick the file `${pulsar_dir}/src/idea-code-style.xml`    4. On the dialog box that opens, click **OK**.    5. Ensure the scheme you just created is selected in **Scheme** dropdown then click **OK**.    #### Configure Checkstyle    1. Install the Checkstyle-IDEA plugin.    2. Open Checkstyle Settings dialog box by going to **Settings** -> **Tools** -> **Checkstyle**.    3. Set **Checkstyle version** to **8.37**.    4. Set **Scan scope** to **Only Java sources (including tests)**.    5. Click **+** button in the **Configuration** section to open a dialog to choose the checkfile file.     1. Enter a **Description**. For example, Pulsar.     2. Select **Use a local checkstyle file**.     3. Set **File** to **buildtools/src/main/resources/pulsar/checkstyle.xml**.     4. Select **Store relative to project location**.     5. Click **Next** -> **Next** -> **Finish**.    6. Activate the configuration you just added by toggling the corresponding box.    7. Click **OK**.    #### Further configuration in IntelliJ     * When working on the Pulsar core modules in IntelliJ, reduce the number of active projects in IntelliJ to speed up IDE actions and reduce unrelated IDE warnings.    * In IntelliJ's Maven UI's tree view under ""Profiles""      * Activate ""core-modules"" Maven profile      * De-activate ""main"" Maven profile      * Run the ""Reload All Maven Projects"" action from the Maven UI toolbar. You can also find the action by the name in the IntelliJ ""Search Everywhere"" window that gets activated by pressing the **Shift** key twice.    * Run the ""Generate Sources and Update Folders For All Projects"" action from the Maven UI toolbar. You can also find the action by the name in the IntelliJ ""Search Everywhere"" window that gets activated by pressing the **Shift** key twice. Running the action takes about 10 minutes for all projects. This is faster when the ""core-modules"" profile is the only active profile.      #### IntelliJ usage tips    * In the case of compilation errors with missing Protobuf classes, ensure to run the ""Generate Sources and Update Folders For All Projects"" action.    * All of the Pulsar source code doesn't compile properly in IntelliJ and there are compilation errors.    * Use the ""core-modules"" profile if working on the Pulsar core modules since the source code for those modules can be compiled in IntelliJ.    * Sometimes it might help to mark a specific project ignored in IntelliJ Maven UI by right-clicking the project name and select **Ignore Projects** from the menu.    * Currently, it is not always possible to run unit tests directly from the IDE because of the compilation issues. As a workaround, individual test classes can be run by using the `mvn test -Dtest=TestClassName` command.        * The above steps have all been performed, but a test still won't run.    * In this case, try the following steps:      1. Close IntelliJ.      2. Run `mvn clean install -DskipTests` on the command line.      3. Reopen IntelliJ.    * If that still doesn't work:      1. Verify Maven is using a supported version. Currently, the supported version of Maven is specified in the          <requireMavenVersion> section of the main pom.xml file.      2. Try ""restart and clear caches"" in IntelliJ and repeat the above steps to reload projects and generate sources.    ### Eclipse    Follow the instructions [here](https://howtodoinjava.com/automation/lombok-eclipse-installation-examples/)  to configure your Eclipse setup.    ## Build Pulsar docs    Refer to the docs [README](site2/README.md).    ## Contact    ##### Mailing lists    | Name                                                                          | Scope                           |                                                                 |                                                                     |                                                                              |  |:------------------------------------------------------------------------------|:--------------------------------|:----------------------------------------------------------------|:--------------------------------------------------------------------|:-----------------------------------------------------------------------------|  | [users@pulsar.apache.org](mailto:users@pulsar.apache.org) | User-related discussions        | [Subscribe](mailto:users-subscribe@pulsar.apache.org) | [Unsubscribe](mailto:users-unsubscribe@pulsar.apache.org) | [Archives](http://mail-archives.apache.org/mod_mbox/pulsar-users/) |  | [dev@pulsar.apache.org](mailto:dev@pulsar.apache.org)     | Development-related discussions | [Subscribe](mailto:dev-subscribe@pulsar.apache.org)   | [Unsubscribe](mailto:dev-unsubscribe@pulsar.apache.org)   | [Archives](http://mail-archives.apache.org/mod_mbox/pulsar-dev/)   |    ##### Slack    Pulsar slack channel at https://apache-pulsar.slack.com/    You can self-register at https://apache-pulsar.herokuapp.com/    ##### Report a security vulnerability    To report a vulnerability for Pulsar, contact the [Apache Security Team](https://www.apache.org/security/). When reporting a vulnerability to [security@apache.org](mailto:security@apache.org), you can copy your email to [private@pulsar.apache.org](mailto:private@pulsar.apache.org) to send your report to the Apache Pulsar Project Management Committee. This is a private mailing list.    ## License    Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0    ## Crypto Notice    This distribution includes cryptographic software. The country in which you currently reside may have restrictions on the import, possession, use, and/or re-export to another country, of encryption software. BEFORE using any encryption software, please check your country's laws, regulations and policies concerning the import, possession, or use, and re-export of encryption software, to see if this is permitted. See <http://www.wassenaar.org/> for more information.    The U.S. Government Department of Commerce, Bureau of Industry and Security (BIS), has classified this software as Export Commodity Control Number (ECCN) 5D002.C.1, which includes information security software using or performing cryptographic functions with asymmetric algorithms. The form and manner of this Apache Software Foundation distribution makes it eligible for export under the License Exception ENC Technology Software Unrestricted (TSU) exception (see the BIS Export Administration Regulations, Section 740.13) for both object code and source code.    The following provides more details on the included cryptographic software: Pulsar uses the SSL library from Bouncy Castle written by http://www.bouncycastle.org. """
Big data;https://github.com/square/cubism;"""# Cubism.js    Cubism.js is a [D3](http://d3js.org) plugin for visualizing time series. Use Cubism to construct better realtime dashboards, pulling data from [Graphite](https://github.com/square/cubism/wiki/Graphite), [Cube](https://github.com/square/cubism/wiki/Cube) and other sources. Cubism is available under the [Apache License](LICENSE).    Want to learn more? [See the wiki.](https://github.com/square/cubism/wiki) """
Big data;https://github.com/etsy/Conjecture;"""# Conjecture [![Build Status](https://travis-ci.org/etsy/Conjecture.svg?branch=master)](https://travis-ci.org/etsy/Conjecture)    Conjecture is a framework for building machine learning models in Hadoop using the Scalding DSL.  The goal of this project is to enable the development of statistical models as viable components  in a wide range of product settings. Applications include classification and categorization,  recommender systems, ranking, filtering, and regression (predicting real-valued numbers).  Conjecture has been designed with a primary emphasis on flexibility and can handle a wide variety of inputs.  Integration with Hadoop and scalding enable seamless handling of extremely large data volumes,  and integration with established ETL processes. Predicted labels can either be consumed directly  by the web stack using the dataset loader, or models can be deployed and consumed by live web code.  Currently, binary classification (assigning one of two possible labels to input data points)  is the most mature component of the Conjecture package.    # Tutorial  There are a few stages involved in training a machine learning model using Conjecture.    ## Create Training Data  We represent the training data as ""feature vectors"" which are just mappings of feature names to real values.  In this case we represent them as a java map of strings to doubles  (although we have a class StringKeyedVector which provides convenience methods for feature vector construction).  We also need the true label of each instance, which we represent as 0 and 1  (the mapping of these binary labels to e.g., ""male"" and ""female"" is up to the user).  We construct BinaryLabeledInstances, which are just wrappers for a feature vector and a label.        val bl = new BinaryLabeledInstance(0.0)      bl.addTerm(""bias"", 1.0)      bl.addTerm(""some_feature"", 0.5)    ## Training a Classifier  Classifiers are essentially trained by presenting the labeled instances to them.  There are several kinds   of linear classifiers we implement, among them:    * Logistic regression,  * Perceptron,  * MIRA (a large margin perceptron model),  * Passive aggressive.    These models all have several options, such as learning rate, regularization parameters and so on.  We supply  reasonable defaults for these parameters although they can be changed readily.  To train a linear model  simply call the update function with the labeled instance:        val p = new LogisticRegression()      p.update(bl)    In order to make this procedure tractable for large datasets, we provided scalding wrappers for the training.  These operate by training several small models on mappers, then aggregating them into a final complete model  on the reducers.  This wrapper is called like so:        new BinaryModelTrainer(args)        .train(instances, 'instance, 'model)        .write(SequenceFile(""model""))        .map('model -> 'model){ x : UpdateableBinaryModel => new com.google.gson.Gson.toJson(x) }        .write(Tsv(""model_json""))    This code segment will train a model using a pipe called instances which has a field called instance which contains  the BinaryLabeledInstance objects.  It produces a pipe with a single field containing the completed model, which can  then be written to disk.    This class uses the command line args object from scalding, in order to let you set some options on the command line.  Some useful options are:    | Argument                            | Possible values                               | Default            | Meaning                                          |  |-------------------------------------|-----------------------------------------------|--------------------|--------------------------------------------------|  | --model                             | mira, logistic_regression, passive_aggressive | passive_aggressive | The type of model to use.                        |  | --iters                             | 1, 2, 3...                                    | 1                  | The number of iterations of training to perform. |  | --zero_class_prob, --one_class_prob | [0, 1]                                        | 1                  |                                                  |    To see all the command line options, see the BinaryModelTrainer class.    ## Evaluating a Classifier  It is important to get a sense of the performance you can expect out of your classifier on unseen data.  In order to do this we recommend to use cross validation.  In essence, your input set of instances is split up into testing and training portions (multiple different ways),  then a classifier is trained on each training portion, and evaluated (against the true labels which are present)  using the testing portion.  This is all wrapped up in a class called BinaryCrossValidator, it is used like so:        new BinaryCrossValidator(args, 5)        .crossValidate(instances, 'instance)        .write(Tsv(""model_xval""))    This class also takes the command line arguments, which it passes to a model trainer for each fold.  This allows the specification of options to the cross validated models on the command line.  The output contains statistics about the performance of the model as well as the confusion matrices  for each fold.    A script is included which cross validates a logistic regression model on the iris dataset.       """
Big data;https://github.com/twitter/fatcache;"""# fatcache     [![status: retired](https://opensource.twitter.dev/status/retired.svg)](https://opensource.twitter.dev/status/#retired)  [![Build Status](https://travis-ci.org/twitter/fatcache.png?branch=master)](https://travis-ci.org/twitter/fatcache)    fatcache is no longer actively maintained.  See [twitter/pelikan](https://github.com/twitter/pelikan) for our latest caching work.    **fatcache** is memcache on SSD. Think of fatcache as a cache for your big data.    ## Overview    There are two ways to think of SSDs in system design. One is to think of SSD as an extension of disk, where it plays the role of making disks fast and the other is to think of them as an extension of memory, where it plays the role of making memory fat. The latter makes sense when persistence (non-volatility) is unnecessary and data is accessed over the network. Even though memory is thousand times faster than SSD, network connected SSD-backed memory makes sense, if we design the system in a way that network latencies dominate over the SSD latencies by a large factor.    To understand why network connected SSD makes sense, it is important to understand the role distributed memory plays in large-scale web architecture. In recent years, terabyte-scale, distributed, in-memory caches have become a fundamental building block of any web architecture. In-memory indexes, hash tables, key-value stores and caches are increasingly incorporated for scaling throughput and reducing latency of persistent storage systems. However, power consumption, operational complexity and single node DRAM cost make horizontally scaling this architecture challenging. The current cost of DRAM per server increases dramatically beyond approximately 150 GB, and power cost scales similarly as DRAM density increases.    Fatcache extends a volatile, in-memory cache by incorporating SSD-backed storage.    SSD-backed memory presents a viable alternative for applications with large workloads that need to maintain high hit rate for high performance. SSDs have higher capacity per dollar and lower power consumption per byte, without degrading random read latency beyond network latency.    Fatcache achieves performance comparable to an in-memory cache by focusing on two design criteria:    - Minimize disk reads on cache hit  - Eliminate small, random disk writes    The latter is important due to SSDs' unique write characteristics. Writes and in-place updates to SSDs degrade performance due to an erase-and-rewrite penalty and garbage collection of dead blocks. Fatcache batches small writes to obtain consistent performance and increased disk lifetime.    SSD reads happen at a page-size granularity, usually 4 KB. Single page read access times are approximately 50 to 70 usec and a single [commodity SSD](http://ark.intel.com/products/56569/Intel-SSD-320-Series-600GB-2_5in-SATA-3Gbs-25nm-ML) can sustain nearly 40K read IOPS at a 4 KB page size. 70 usec read latency dictates that disk latency will overtake typical network latency after a small number of reads. Fatcache reduces disk reads by maintaining an in-memory index for all on-disk data.    ## Batched Writes    There have been attempts to use an SSD as a swap layer to implement SSD-backed memory. This method degrades write performance and SSD lifetime with many small, random writes. Similar issues occur when an SSD is simply mmaped.    To minimize the number of small, random writes, fatcache treats the SSD as a log-structured object store. All writes are aggregated in memory and written to the end of the circular log in batches - usually multiples of 1 MB.    By managing an SSD as a log-structured store, no disk updates are in-place and objects won't have a fixed address on disk. To locate an object, fatcache maintains an in-memory index. An on-disk object without an index entry is a candidate for garbage collection, which occurs during capacity-triggered eviction.    ## In-memory index    Fatcache maintains an in-memory index for all data stored on disk. An in-memory index serves two purposes:    - Cheap object existence checks  - On-disk object address storage    An in-memory index is preferable to an on-disk index to minimize disk lookups to locate and read an object. Furthermore, in-place index updates become complicated by an SSD's unique write characteristics. An in-memory index avoids these shortcomings and ensures there are no disk accesses on cache miss and only a single disk access on cache hit.    Maintaining an in-memory index of all on-disk data requires a compact representation of the index. The fatcache index has the following format:    ```c  struct itemx {    STAILQ_ENTRY(itemx) tqe;    /* link in index / free q */    uint8_t             md[20]; /* sha1 message digest */    uint32_t            sid;    /* owner slab id */    uint32_t            offset; /* item offset from owner slab base */    rel_time_t          expiry; /* expiry in secs */    uint64_t            cas;    /* cas */  } __attribute__ ((__packed__));  ```    Each index entry contains both object-specific information (key name, &c.) and disk-related information (disk address, &c.). The entries are stored in a chained hash table. To avoid long hash bin traversals, the number of hash bins is fixed to the expected number of index entries.    To further reduce the memory consumed by the index, we store the SHA-1 hash of the key in each index entry, instead of the key itself. The SHA-1 hash acts as the unique identifier for each object. The on-disk object format contains the complete object key and value. False positives from SHA-1 hash collisions are detected after object retrieval from the disk by comparison with the requested key. If there are collisions on the write path, new objects with the same hash key simply overwrite previous objects.    The index entry (struct itemx) on a 64-bit system is 48 bytes in size. It is possible to further reduce index entry size to 32 bytes, if CAS is unsupported, MD5 hashing is used, and the next pointer is reduced to 4 bytes.    At this point, it is instructive to consider the relative size of fatcache's index and the on-disk data. With a 44 byte index entry, an index consuming 48 MB of memory can address 1M objects. If the average object size is 1 KB, then a 48 MB index can address 1 GB of on-disk storage - a 23x memory overcommit. If the average object size is 500 bytes, then a 48 MB index can address 500 MB of SSD - a 11x memory overcommit. Index size and object size relate in this way to determine the addressable capacity of the SSD.    ## Build    To build fatcache from a [distribution tarball](http://code.google.com/p/fatcache/downloads/list):        $ ./configure      $ make      $ sudo make install    To build fatcache from a [distribution tarball](http://code.google.com/p/fatcache/downloads/list) in _debug mode_:        $ CFLAGS=""-ggdb3 -O0"" ./configure --enable-debug=full      $ make      $ sudo make install    To build fatcache from source with _debug logs enabled_ and _assertions disabled_:        $ git clone git@github.com:twitter/fatcache.git      $ cd fatcache      $ autoreconf -fvi      $ ./configure --enable-debug=log      $ make      $ src/fatcache -h    ## Help        Usage: fatcache [-?hVdS] [-o output file] [-v verbosity level]                 [-p port] [-a addr] [-e hash power]                 [-f factor] [-n min item chunk size] [-I slab size]                 [-i max index memory[ [-m max slab memory]                 [-z slab profile] [-D ssd device] [-s server id]        Options:        -h, --help                  : this help        -V, --version               : show version and exit        -d, --daemonize             : run as a daemon        -S, --show-sizes            : print slab, item and index sizes and exit        -o, --output=S              : set the logging file (default: stderr)        -v, --verbosity=N           : set the logging level (default: 6, min: 0, max: 11)        -p, --port=N                : set the port to listen on (default: 11211)        -a, --addr=S                : set the address to listen on (default: 0.0.0.0)        -e, --hash-power=N          : set the item index hash table size as a power of two (default: 20)        -f, --factor=D              : set the growth factor of slab item sizes (default: 1.25)        -n, --min-item-chunk-size=N : set the minimum item chunk size in bytes (default: 84 bytes)        -I, --slab-size=N           : set slab size in bytes (default: 1048576 bytes)        -i, --max-index-memory=N    : set the maximum memory to use for item indexes in MB (default: 64 MB)        -m, --max-slab-memory=N     : set the maximum memory to use for slabs in MB (default: 64 MB)        -z, --slab-profile=S        : set the profile of slab item chunk sizes (default: n/a)        -D, --ssd-device=S          : set the path to the ssd device file (default: n/a)        -s, --server-id=I/N         : set fatcache instance to be I out of total N instances (default: 0/1)    ## Performance    - Initial performance results are available [here](https://github.com/twitter/fatcache/blob/master/notes/performance.md).    ## Future Work    - fatcache deals with two kinds of IOs - disk IO and network IO. Network IO in fatcache is async, but disk IO is sync. It is recommended to run multiple instances of fatcache on a single machine to exploit CPU and SSD parallelism. However, by making disk IO async (using libaio, perhaps), it would be possible for a single instance to completely exploit all available SSD device parallelism.  - observability in fatcache through stats    ## Issues and Support    Have a bug or question? Please create an issue here on GitHub!    https://github.com/twitter/fatcache/issues    ## Contributors    * Manju Rajashekhar ([@manju](https://twitter.com/manju))  * Yao Yue ([@thinkingfish](https://twitter.com/thinkingfish))    ## License    Copyright 2013 Twitter, Inc.    Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 """
Big data;https://github.com/krestenkrab/hanoidb;"""# HanoiDB Indexed Key/Value Storage    [![Build Status](https://travis-ci.org/krestenkrab/hanoidb.svg?branch=master)](https://travis-ci.org/krestenkrab/hanoidb)    HanoiDB implements an indexed, key/value storage engine.  The primary index is  a log-structured merge tree (LSM-BTree) implemented using ""doubling sizes""  persistent ordered sets of key/value pairs, similar is some regards to  [LevelDB](http://code.google.com/p/leveldb/).  HanoiDB includes a visualizer  which when used to watch a living database resembles the ""Towers of Hanoi""  puzzle game, which inspired the name of this database.    ## Features  - Insert, Delete and Read all have worst case *O*(log<sub>2</sub>(*N*)) latency.  - Incremental space reclaimation: The cost of evicting stale key/values    is amortized into insertion    - you don't need a separate eviction thread to keep memory use low    - you don't need to schedule merges to happen at off-peak hours  - Operations-friendly ""append-only"" storage    - allows you to backup live system    - crash-recovery is very fast and the logic is straight forward    - all data subject to CRC32 checksums    - data can be compressed on disk to save space  - Efficient range queries    - Riak secondary indexing    - Fast key and bucket listing  - Uses bloom filters to avoid unnecessary lookups on disk  - Time-based expiry of data    - configure the database to expire data older than n seconds    - specify a lifetime in seconds for any particular key/value pair  - Efficient resource utilization    - doesn't store all keys in memory    - uses a modest number of file descriptors proportional to the number of levels    - I/O is generally balanced between random and sequential    - low CPU overhead  - ~2000 lines of pure Erlang code in src/*.erl    HanoiDB is developed by Trifork, a Riak expert solutions provider, and Basho  Technologies, makers of Riak.  HanoiDB can be used in Riak via the  `riak_kv_tower_backend` repository.    ### Configuration options    Put these values in your `app.config` in the `hanoidb` section    ```erlang   {hanoidb, [            {data_root, ""./data/hanoidb""},              %% Enable/disable on-disk compression.            %%            {compress, none | gzip},              %% Expire (automatically delete) entries after N seconds.            %% When this value is 0 (zero), entries never expire.            %%            {expiry_secs, 0},              %% Sync strategy `none' only syncs every time the            %% nursery runs full, which is currently hard coded            %% to be evert 256 inserts or deletes.            %%            %% Sync strategy `sync' will sync the nursery log            %% for every insert or delete operation.            %%            {sync_strategy, none | sync | {seconds, N}},              %% The page size is a minimum page size, when a page fills            %% up to beyond this size, it is written to disk.            %% Compression applies to such units of page size.            %%            {page_size, 8192},              %% Read/write buffer sizes apply to merge processes.            %% A merge process has two read buffers and a write            %% buffer, and there is a merge process *per level* in            %% the database.            %%            {write_buffer_size, 524288},  % 512kB            {read_buffer_size, 524288},  % 512kB              %% The merge strategy is one of `fast' or `predictable'.            %% Both have same log2(N) worst case, but `fast' is            %% sometimes faster; yielding latency fluctuations.            %%            {merge_strategy, fast | predictable},              %% ""Level0"" files has 2^N KVs in it, defaulting to 1024.            %% If the database is to contain very small KVs, this is            %% likely too small, and will result in many unnecessary            %% file operations.  (Subsequent levels double in size).            {top_level, 10}  % 1024 Key/Values           ]},  ```      ### Contributors    - Kresten Krab Thorup @krestenkrab  - Greg Burd @gburd  - Jesper Louis Andersen @jlouis  - Steve Vinoski @vinoski  - Erik SÃ¸e SÃ¸rensen, @eriksoe  - Yamamoto Takashi @yamt  - Joseph Wayne Norton @norton """
Big data;https://github.com/rackerlabs/blueflood;"""<p align=""center"">   <img src=""http://blueflood.io/images/bf-bg-color.png"" width=""220"" height=""232"" align=center>  </p>    # Blueflood    [![Build Status](https://travis-ci.org/rackerlabs/blueflood.svg?branch=master)](https://travis-ci.org/rackerlabs/blueflood)  [![Coveralls](https://coveralls.io/repos/github/rackerlabs/blueflood/badge.svg?branch=master)](https://github.com/rackerlabs/blueflood/releases)  [![Releases](http://img.shields.io/badge/rax-release-v1.0.1956.svg)](https://coveralls.io/github/mmi-cookbooks/metrics-repose?branch=master)  [![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0)    [Discuss](https://groups.google.com/forum/#!forum/blueflood-discuss) - [Code](http://github.com/rackerlabs/blueflood) - [Site](http://blueflood.io)    ## Introduction    Blueflood is a multi-tenant, distributed metric processing system. Blueflood is capable of ingesting, rolling up and serving metrics at a massive scale.      ## Getting Started    The latest code will always be here on Github.        git clone https://github.com/rackerlabs/blueflood.git      cd blueflood        You can run the entire suite of tests using Maven:        mvn test integration-test    ### Building    Build an ['uber jar'](http://stackoverflow.com/questions/11947037/what-is-an-uber-jar) using maven:        mvn package -P all-modules    The uber jar will be found in ${BLUEFLOOD_DIR}/blueflood-all/target/blueflood-all-${VERSION}-jar-with-dependencies.jar.  This jar contains all the dependencies necessary to run Blueflood with a very simple classpath.    Build a docker image:        mvn clean package  docker:build -Pall-modules    ### Running    The best place to start is the [10 minute guide](https://github.com/rackerlabs/blueflood/wiki/10-Minute-Guide).  In a nutshell, you must do this:        java -cp /path/to/uber.jar \      -Dblueflood.config=file:///path/to/blueflood.conf \      -Dlog4j.configuration=file:///path/to/log4j.properties \      com.rackspacecloud.blueflood.service.BluefloodServiceStarter        Each configuration option can be found in Configuration.java.  Each of those can be overridden on the command line by  doing:        -DCONFIG_OPTION=NEW_VALUE    ## Additional Tools    The Blueflood team maintains a number of tools that are related to the project, but not essential components of it. These tools are kept in various other repos:    * Performance Tests: Scripts for load testing a blueflood installation using [The Grinder](http://grinder.sourceforge.net/). https://github.com/rackerlabs/raxmetrics-perf-test-scripts  * Carbon Forwarder: a process that receives data from carbon (one of the components of [Graphite](https://graphiteapp.org/)) and sends it to a Blueflood instance. https://github.com/rackerlabs/blueflood-carbon-forwarder  * Blueflood-Finder: a plugin for graphite-web and graphite-api that allows them to using a Blueflood instance as a data backend. https://github.com/rackerlabs/blueflood-graphite-finder  * StatsD plugin: a statsD backend that sends metrics a Blueflood instance. https://github.com/rackerlabs/blueflood-statsd-backend    ## Contributing    First, we welcome bug reports and contributions.  If you would like to contribute code, just fork this project and send us a pull request.  If you would like to contribute documentation, you should get familiar with  [our wiki](https://github.com/rackerlabs/blueflood/wiki)    Also, we have set up a [Google Group](https://groups.google.com/forum/#!forum/blueflood-discuss) to answer questions.    If you prefer IRC, most of the Blueflood developers are in #blueflood on Freenode.     ## License    Copyright 2013-2017 Rackspace    Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at       http://www.apache.org/licenses/LICENSE-2.0     Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.   """
Big data;https://github.com/dgraph-io/dgraph;"""![](/logo.png)    **The Only Native GraphQL Database With A Graph Backend.**    [![Wiki](https://img.shields.io/badge/res-wiki-blue.svg)](https://dgraph.io/docs/)  [![Build Status](https://teamcity.dgraph.io/guestAuth/app/rest/builds/buildType:(id:Dgraph_Ci)/statusIcon.svg)](https://teamcity.dgraph.io/viewLog.html?buildTypeId=Dgraph_Ci&buildId=lastFinished&guest=1)  [![Coverage Status](https://coveralls.io/repos/github/dgraph-io/dgraph/badge.svg?branch=master)](https://coveralls.io/github/dgraph-io/dgraph?branch=master)  [![Go Report Card](https://goreportcard.com/badge/github.com/dgraph-io/dgraph)](https://goreportcard.com/report/github.com/dgraph-io/dgraph)    Dgraph is a horizontally scalable and distributed GraphQL database with a graph backend. It provides ACID transactions, consistent replication, and linearizable reads. It's built from the ground up to perform for  a rich set of queries. Being a native GraphQL database, it tightly controls how the  data is arranged on disk to optimize for query performance and throughput,  reducing disk seeks and network calls in a cluster.      Dgraph's goal is to provide [Google](https://www.google.com) production level scale and throughput,  with low enough latency to be serving real-time user queries, over terabytes of structured data.  Dgraph supports [GraphQL query syntax](https://dgraph.io/docs/master/query-language/), and responds in [JSON](http://www.json.org/) and [Protocol Buffers](https://developers.google.com/protocol-buffers/) over [GRPC](http://www.grpc.io/) and HTTP.    **Use [Discuss Issues](https://discuss.dgraph.io/c/issues/dgraph/38) for reporting issues about this repository.**    ## Status    Dgraph is [at version v21.03.0][rel] and is production-ready. Apart from the vast open source community, it is being used in  production at multiple Fortune 500 companies, and by  [Intuit Katlas](https://github.com/intuit/katlas) and [VMware Purser](https://github.com/vmware/purser).    [rel]: https://github.com/dgraph-io/dgraph/releases/tag/v21.03.0    ## Quick Install    The quickest way to install Dgraph is to run this command on Linux or Mac.    ```bash  curl https://get.dgraph.io -sSf | bash  ```    ## Install with Docker    If you're using Docker, you can use the [official Dgraph image](https://hub.docker.com/r/dgraph/dgraph/).    ```bash  docker pull dgraph/dgraph:latest  ```    ## Install from Source    If you want to install from source, install Go 1.13+ or later and the following dependencies:    ### Ubuntu    ```bash  sudo apt-get update  sudo apt-get install gcc make  ```    ### Build and Install    Then clone the Dgraph repository and use `make install` to install the Dgraph binary to `$GOPATH/bin`.    ```bash  git clone https://github.com/dgraph-io/dgraph.git  cd ./dgraph  make install  ```    ## Get Started  **To get started with Dgraph, follow:**    - Installation to queries in 3 steps via [dgraph.io/docs/](https://dgraph.io/docs/get-started/).  - A longer interactive tutorial via [dgraph.io/tour/](https://dgraph.io/tour/).  - Tutorial and  presentation videos on [YouTube channel](https://www.youtube.com/channel/UCghE41LR8nkKFlR3IFTRO4w/featured).    ## Is Dgraph the right choice for me?    - Do you have more than 10 SQL tables connected via foreign keys?  - Do you have sparse data, which doesn't elegantly fit into SQL tables?  - Do you want a simple and flexible schema, which is readable and maintainable    over time?  - Do you care about speed and performance at scale?    If the answers to the above are YES, then Dgraph would be a great fit for your  application. Dgraph provides NoSQL like scalability while providing SQL like  transactions and the ability to select, filter, and aggregate data points. It  combines that with distributed joins, traversals, and graph operations, which  makes it easy to build applications with it.    ## Dgraph compared to other graph DBs    | Features | Dgraph | Neo4j | Janus Graph |  | -------- | ------ | ----- | ----------- |  | Architecture | Sharded and Distributed | Single server (+ replicas in enterprise) | Layer on top of other distributed DBs |  | Replication | Consistent | None in community edition (only available in enterprise) | Via underlying DB |  | Data movement for shard rebalancing | Automatic | Not applicable (all data lies on each server) | Via underlying DB |  | Language | GraphQL inspired | Cypher, Gremlin | Gremlin |  | Protocols | Grpc / HTTP + JSON / RDF | Bolt + Cypher | Websocket / HTTP |  | Transactions | Distributed ACID transactions | Single server ACID transactions | Not typically ACID  | Full-Text Search | Native support | Native support | Via External Indexing System |  | Regular Expressions | Native support | Native support | Via External Indexing System |  | Geo Search | Native support | External support only | Via External Indexing System |  | License | Apache 2.0 | GPL v3 | Apache 2.0 |    ## Users  - **Dgraph official documentation is present at [dgraph.io/docs/](https://dgraph.io/docs/).**  - For feature requests or questions, visit    [https://discuss.dgraph.io](https://discuss.dgraph.io).  - Check out [the demo at dgraph.io](http://dgraph.io) and [the visualization at    play.dgraph.io](http://play.dgraph.io/).  - Please see [releases tab](https://github.com/dgraph-io/dgraph/releases) to    find the latest release and corresponding release notes.  - [See the Roadmap](https://discuss.dgraph.io/t/dgraph-product-roadmap-2021/12284) for a list of    working and planned features.  - Read about the latest updates from the Dgraph team [on our    blog](https://open.dgraph.io/).  - Watch tech talks on our [YouTube    channel](https://www.youtube.com/channel/UCghE41LR8nkKFlR3IFTRO4w/featured).    ## Developers  - See a list of issues [that we need help with](https://github.com/dgraph-io/dgraph/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22).  - Please see [Contributing to Dgraph](https://github.com/dgraph-io/dgraph/blob/master/CONTRIBUTING.md) for guidelines on contributions.    ## Client Libraries  The Dgraph team maintains several [officially supported client libraries](https://dgraph.io/docs/clients/). There are also libraries contributed by the community [unofficial client libraries](https://dgraph.io/docs/clients#unofficial-dgraph-clients).    ## Contact  - Please use [discuss.dgraph.io](https://discuss.dgraph.io) for documentation, questions, feature requests and discussions.  - Please use [discuss.dgraph.io](https://discuss.dgraph.io/c/issues/dgraph/38) for filing bugs or feature requests.  - Follow us on Twitter [@dgraphlabs](https://twitter.com/dgraphlabs). """
Big data;https://github.com/linkedin/white-elephant;"""# White Elephant    White Elephant is a Hadoop log aggregator and dashboard which enables   visualization of Hadoop cluster utilization across users.    ![Screenshot](https://s3.amazonaws.com/snaprojects/blog/whitel/whitel_ex2.png)    ## Quick Start    To try out the server with some test data:        cd server      ant      ./startup.sh    Then visit [http://localhost:3000](http://localhost:3000).  It may take a minute for the test data  to load.    ## Hadoop Version Compatibility    White Elephant is compiled and tested against Hadoop 1.0.3 and should work with any 1.0.x version.  Hadoop 2.0 is not yet supported.    ## Server    The server is a JRuby web application.  In a production environment it can be deployed to tomcat  and reads aggregated usage data directly from Hadoop.  This data is stored in an in-memory database  provided by [HyperSQL](http://hsqldb.org/).  Charting is provided by   [Rickshaw](http://code.shutterstock.com/rickshaw/).    ### Getting started    To get started using the server, first set up the environment:        cd server      ant    The default target does several things, among them:    * Installs JRuby to a local directory under .rbenv  * Installs Ruby gems to the above directory  * Downloads JARs  * Creates test data under data/usage    At this point you should be able to start the server:        ./startup.sh    You can now visit [http://localhost:3000](http://localhost:3000).  It may take a minute for the test data  to load.    This uses [trinidad](https://github.com/trinidad/trinidad) to run the JRuby web app in development mode.  Since it is in development mode the app assumes local data should be used,  which it looks for in the directory specified in `config.yml`.    ### Configuration    The server configuration is contained in `config.yml`.  You can see a sample in `sample_config.yml`.    When run in development mode using `./startup.sh`, `sample_config.yml` is used and it follows the  settings specified under `local`.  The only configurable parameter here is `file_pattern`, which specifies   where to load the usage data from on local disk.    When packaged as a WAR it runs in production mode and uses configuration specified under `hadoop`,   the assumption being that the aggregated usage data will be available there.  The following   parameter must be specified:    * **file_pattern**: Glob pattern to load usage files from Hadoop  * **libs**: Directories containing Hadoop JARs (to be added to the classpath)  * **conf_dir**: Directory containing Hadoop configuration (to be added to the classpath)  * **principal**: User name used to access secure Hadoop  * **keytab**: Path to keytab file for user to access secure Hadoop    White Elephant does not assume a specific version of Hadoop, so the JARs are not packaged in the WAR.  Therefore the path to the Hadoop JARs must be specified in the configuration.    ### Deploying    To build a WAR which can be deployed to tomcat:        ant war -Dconfig.path=<path-to-config>    The config file you specify will be packaged as `config.yml` within the WAR.  See `sample_config.yml`  as an example for how to write the config file.    ## Hadoop Log Uploading    The script `hadoop/scripts/statsupload.pl` can be used to upload the Hadoop logs to HDFS  so they can be processed.  Check its documentation for details.    ## Hadoop Jobs    There are three Hadoop jobs, all managed by a job executor which keeps track of what  work needs to be done.    The first two jobs parse and convert raw job configurations and logging into an easier-to-work-with Avro  format. Together, these two datasets can serve as the base data for a variety of usage analytics workflows.    The third and final job reads the Avro-fied log data and aggregates it per hour, writing the data  out in Avro format.  It essentially builds a data cube which can be easily loaded by the  web application into the DB and queried against.    ### Configuration    Some sample configuration files can be found under `hadoop/config/jobs`:    * **base.properties**: Contains most of the configuration  * **white-elephant-full-usage.job**: Job file used when processing all logs.  * **white-elephant-incremental-usage.job**: Job file used when incrementally processing logs.    The `base.properties` file consists of configuration specific to White Elephant and configuration  specifically for Hadoop.  All Hadoop configuration parameter begin with `hadoop-conf`.  The two  job files just have a single settings `incremental` and only differ in the value they use for it.    ### Hadoop Logs    Within `base.properties` is a parameter `logs.root`.  This is the root path where the Hadoop logs  are found which are to be parsed.  The parsing job assumes the logs are stored in Hadoop under daily  directories using the following directory structure:        <logs.root>/<cluster-name>/daily/<yyyy>/<MMdd>    For example, logs on January 23rd, 2013 for the production cluster may be stored in a directory  such as:        /data/hadoop/logs/prod/daily/2013/0123    ### Packaging    To create a zip package containing all files necessary to run the jobs simply run:        ant zip -Djob.config.dir=<path-to-job-config-dir>    The `job.config.dir` should be the directory containing the `.properties` and `.job` files you would  like to include in the package.    If you happen to be using [Azkaban](http://data.linkedin.com/opensource/azkaban) as your job scheduler  of choice then this zip file will work with it as long as you add the Azkaban specific configuration   to `base.properties`.    ### Running    After unzipping the zip package you can run using the `run.sh` script.  This requires a couple environment   variables to be set:    * **HADOOP_CONF_DIR**: Hadoop configuration directory  * **HADOOP_LIB_DIR**: Hadoop JARs directory    To run the full job:        ./run.sh white-elephant-full-usage.job    To run the incremental job:        ./run.sh white-elephant-incremental-usage.job    The incremental job is more efficient as it only processes new data.  The full job reprocesses  *everything*.    ## Contributing    White Elephant is open source and freely available under the Apache 2 license.  As always, we  welcome contributors, so send us your pull requests.    For help please see the [discussion group](http://groups.google.com/group/linkedin-white-elephant).    ## Thanks    White Elephant is built using a suite of great open source projects.  Just to name a few:    * [JRuby](http://jruby.org/)  * [HyperSQL](http://hsqldb.org/)  * [Bootstrap](http://twitter.github.com/bootstrap/)  * [Rickshaw](http://code.shutterstock.com/rickshaw/)  * [Ember.js](http://emberjs.com/)  * [D3](http://d3js.org/)  * [Moment.js](http://momentjs.com/)  * [jQuery](http://jquery.com/)  * [jQuery UI](http://jqueryui.com/)    ## License    Copyright 2012 LinkedIn, Inc    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. """
Big data;https://github.com/WeBankFinTech/Linkis;"""Linkis  ==========    [![License](https://img.shields.io/badge/license-Apache%202-4EB1BA.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)    [English](README.md) | [ä¸­æ–‡](README_CN.md)    # Introduction     Linkis builds a layer of computation middleware between upper applications and underlying engines. By using standard interfaces such as REST/WS/JDBC provided by Linkis, the upper applications can easily access the underlying engines such as MySQL/Spark/Hive/Presto/Flink, etc., and achieve the intercommunication of user resources like unified variables, scripts, UDFs, functions and resource files at the same time.    As a computation middleware, Linkis provides powerful connectivity, reuse, orchestration, expansion, and governance capabilities. By decoupling the application layer and the engine layer, it simplifies the complex network call relationship, and thus reduces the overall complexity and saves the development and maintenance costs as well.    Since the first release of Linkis in 2019, it has accumulated more than **700** trial companies and **1000+** sandbox trial users, which involving diverse industries, from finance, banking, tele-communication, to manufactory, internet companies and so on. Lots of companies have already used Linkis as a unified entrance for the underlying computation and storage engines of the big data platform.      ![linkis-intro-01](https://user-images.githubusercontent.com/7869972/148767375-aeb11b93-16ca-46d7-a30e-92fbefe2bd5e.png)    ![linkis-intro-03](https://user-images.githubusercontent.com/7869972/148767380-c34f44b2-9320-4633-9ec8-662701f41d15.png)    # Features    - **Support for diverse underlying computation storage engines**.        Currently supported computation/storage engines: Spark, Hive, Python, Presto, ElasticSearch, MLSQL, TiSpark, JDBC, Shell, etc;            Computation/storage engines to be supported: Flink(Supported in version >=1.0.2), Impala, etc;            Supported scripting languages: SparkSQL, HiveQL, Python, Shell, Pyspark, R, Scala and JDBC, etc.        - **Powerful task/request governance capabilities**. With services such as Orchestrator, Label Manager and customized Spring Cloud Gateway, Linkis is able to provide multi-level labels based, cross-cluster/cross-IDC fine-grained routing, load balance, multi-tenancy, traffic control, resource control, and orchestration strategies like dual-active, active-standby, etc.      - **Support full stack computation/storage engine**. As a computation middleware, it will receive, execute and manage tasks and requests for various computation storage engines, including batch tasks, interactive query tasks, real-time streaming tasks and storage tasks;    - **Resource management capabilities**.  ResourceManager is not only capable of managing resources for Yarn and Linkis EngineManger as in Linkis 0.X, but also able to provide label-based multi-level resource allocation and recycling, allowing itself to have powerful resource management capabilities across mutiple Yarn clusters and mutiple computation resource types;    - **Unified Context Service**. Generate Context ID for each task/request,  associate and manage user and system resource files (JAR, ZIP, Properties, etc.), result set, parameter variable, function, etc., across user, system, and computing engine. Set in one place, automatic reference everywhere;    - **Unified materials**. System and user-level unified material management, which can be shared and transferred across users and systems.    # Supported engine types    | **Engine** | **Supported Version** | **Linkis 0.X version requirement**| **Linkis 1.X version requirement** | **Description** |  |:---- |:---- |:---- |:---- |:---- |  |Flink |1.12.2|\>=dev-0.12.0, PR #703 not merged yet.|>=1.0.2|	Flink EngineConn. Supports FlinkSQL code, and also supports Flink Jar to Linkis Manager to start a new Yarn application.|  |Impala|\>=3.2.0, CDH >=6.3.0""|\>=dev-0.12.0, PR #703 not merged yet.|ongoing|Impala EngineConn. Supports Impala SQL.|  |Presto|\>= 0.180|\>=0.11.0|ongoing|Presto EngineConn. Supports Presto SQL.|  |ElasticSearch|\>=6.0|\>=0.11.0|ongoing|ElasticSearch EngineConn. Supports SQL and DSL code.|  |Shell|Bash >=2.0|\>=0.9.3|\>=1.0.0_rc1|Shell EngineConn. Supports shell code.|  |MLSQL|\>=1.1.0|\>=0.9.1|ongoing|MLSQL EngineConn. Supports MLSQL code.|  |JDBC|MySQL >=5.0, Hive >=1.2.1|\>=0.9.0|\>=1.0.0_rc1|JDBC EngineConn. Supports MySQL and HiveQL code.|  |Spark|Apache 2.0.0~2.4.7, CDH >=5.4.0|\>=0.5.0|\>=1.0.0_rc1|Spark EngineConn. Supports SQL, Scala, Pyspark and R code.|  |Hive|Apache >=1.0.0, CDH >=5.4.0|\>=0.5.0|\>=1.0.0_rc1|Hive EngineConn. Supports HiveQL code.|  |Hadoop|Apache >=2.6.0, CDH >=5.4.0|\>=0.5.0|ongoing|Hadoop EngineConn. Supports Hadoop MR/YARN application.|  |Python|\>=2.6|\>=0.5.0|\>=1.0.0_rc1|Python EngineConn. Supports python code.|  |TiSpark|1.1|\>=0.5.0|ongoing|TiSpark EngineConn. Support querying TiDB data by SparkSQL.|    # Ecosystem    | Component | Description | Linkis 0.x(recommend 0.11.0) Compatible | Linkis 1.x(recommend 1.0.3) Compatible |  | --------------- | -------------------------------------------------------------------- | --------- | --------- |  | [**DataSphereStudio**](https://github.com/WeBankFinTech/DataSphereStudio/blob/master/README.md) | DataSphere Studio (DSS for short) is WeDataSphere, a one-stop data application development management portal. | DSS 0.9.1[released] | **DSS 1.0.1[developing]** |  | [**Scriptis**](https://github.com/WeBankFinTech/Scriptis) | Support online script writing such as SQL, Pyspark, HiveQL, etc., submit to [Linkis](https://github.com/apache/incubator-linkis) to perform data analysis web tools. | Scriptis merged in DSSï¼ˆDSS 0.9.1[released]ï¼‰ | **In DSS 1.0.1[developing]** |  | [**Schedulis**](https://github.com/WeBankFinTech/Schedulis) | Workflow task scheduling system based on Azkaban secondary development, with financial-grade features such as high performance, high availability and multi-tenant resource isolation. | Schedulis 0.6.1[released] |  **Schedulis0.6.2 [developing]** |  | [**Qualitis**](https://github.com/WeBankFinTech/Qualitis) | Data quality verification tool, providing data verification capabilities such as data integrity and correctness  | Qualitis 0.8.0[released] | **Qualitis 0.9.0 [developing]** |  | [**Streamis**](https://github.com/WeBankFinTech/Streamis) | Streaming application development management tool. It supports the release of Flink Jar and Flink SQL, and provides the development, debugging and production management capabilities of streaming applications, such as: start-stop, status monitoring, checkpoint, etc. | **No support** | **Streamis 0.1.0 [developing]** |  | [**Exchangis**](https://github.com/WeBankFinTech/Exchangis) | A data exchange platform that supports data transmission between structured and unstructured heterogeneous data sources, the upcoming Exchangis1. 0, will be connected with DSS workflow | **No support** | **Exchangis 1.0.0 [developing]**|  | [**Visualis**](https://github.com/WeBankFinTech/Visualis) | A data visualization BI tool based on the second development of Davinci, an open source project of CreditEase, provides users with financial-level data visualization capabilities in terms of data security. | Visualis 0.5.0[released]| **Visualis 1.0.0[developing]**|  | [**Prophecis**](https://github.com/WeBankFinTech/Prophecis) | A one-stop machine learning platform that integrates multiple open source machine learning frameworks. Prophecis' MLFlow can be connected to DSS workflow through AppConn. | Prophecis 0.2.2[released] | **Prophecis 0.3.0 [developing]** |    # Download    Please go to the [Linkis Releases Page](https://github.com/apache/incubator-linkis/releases) to download a compiled distribution or a source code package of Linkis.    # Compile and deploy  Please follow [Compile Guide](https://linkis.apache.org/docs/latest/development/linkis_compile_and_package) to compile Linkis from source code.    Please refer to [Deployment Documents](https://linkis.apache.org/docs/latest/deployment/quick_deploy) to do the deployment.      # Examples and Guidance  You can find examples and guidance for how to use and manage Linkis in [User Manual](https://linkis.apache.org/docs/latest/user_guide/overview), [Engine Usage Documents](https://linkis.apache.org/docs/latest/engine_usage/overview) and [API Documents](https://linkis.apache.org/docs/latest/api/overview).    # Documentation    The documentation of linkis is in [Linkis-Website Git Repository](https://github.com/apache/incubator-linkis-website).    # Architecture  Linkis services could be divided into three categories: computation governance services, public enhancement services and microservice governance services.  - The computation governance services, support the 3 major stages of processing a task/request: submission -> preparation -> execution;  - The public enhancement services, including the material library service, context service, and data source service;  - The microservice governance services, including Spring Cloud Gateway, Eureka and Open Feign.    Below is the Linkis architecture diagram. You can find more detailed architecture docs in [Linkis-Doc/Architecture](https://linkis.apache.org/docs/latest/architecture/overview).  ![architecture](https://user-images.githubusercontent.com/7869972/148767383-f87e84ba-5baa-4125-8b6e-d0aa4f7d3a66.png)    Based on Linkis the computation middleware, we've built a lot of applications and tools on top of it in the big data platform suite [WeDataSphere](https://github.com/WeBankFinTech/WeDataSphere). Below are the currently available open-source projects. More projects upcoming, please stay tuned.    ![wedatasphere_stack_Linkis](https://user-images.githubusercontent.com/7869972/148767389-049361df-3609-4c2f-a4e2-c904c273300e.png)    # Contributing    Contributions are always welcomed, we need more contributors to build Linkis together. either code, or doc, or other supports that could help the community.    For code and documentation contributions, please follow the [contribution guide](https://linkis.apache.org/community/how-to-contribute).    # Contact Us    Any questions or suggestions please kindly submit an issue.    You can scan the QR code below to join our WeChat and QQ group to get more immediate response.    ![introduction05](https://user-images.githubusercontent.com/7869972/148767386-0663f833-547d-4c30-8876-081bb966ffb8.png)    Meetup videos on [Bilibili](https://space.bilibili.com/598542776?from=search&seid=14344213924133040656).    # Who is Using Linkis    We opened [an issue](https://github.com/apache/incubator-linkis/issues/23) for users to feedback and record who is using Linkis.    Since the first release of Linkis in 2019, it has accumulated more than **700** trial companies and **1000+** sandbox trial users, which involving diverse industries, from finance, banking, tele-communication, to manufactory, internet companies and so on. """
Big data;https://github.com/mara/data-integration;"""# Mara Pipelines    [![Build Status](https://travis-ci.org/mara/mara-pipelines.svg?branch=master)](https://travis-ci.org/mara/mara-pipelines)  [![PyPI - License](https://img.shields.io/pypi/l/mara-pipelines.svg)](https://github.com/mara/mara-pipelines/blob/master/LICENSE)  [![PyPI version](https://badge.fury.io/py/mara-pipelines.svg)](https://badge.fury.io/py/mara-pipelines)  [![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://communityinviter.com/apps/mara-users/public-invite)        This package contains a lightweight data transformation framework with a focus on transparency and complexity reduction. It has a number of baked-in assumptions/ principles:    - Data integration pipelines as code: pipelines, tasks and commands are created using declarative Python code.    - PostgreSQL as a data processing engine.    - Extensive web ui. The web browser as the main tool for inspecting, running and debugging pipelines.    - GNU make semantics. Nodes depend on the completion of upstream nodes. No data dependencies or data flows.    - No in-app data processing: command line tools as the main tool for interacting with databases and data.    - Single machine pipeline execution based on Python's [multiprocessing](https://docs.python.org/3.6/library/multiprocessing.html). No need for distributed task queues. Easy debugging and output logging.    - Cost based priority queues: nodes with higher cost (based on recorded run times) are run first.    &nbsp;    ## Installation    To use the library directly, use pip:    ```  pip install mara-pipelines  ```    or     ```  pip install git+https://github.com/mara/mara-pipelines.git  ```    For an example of an integration into a flask application, have a look at the [mara example project 1](https://github.com/mara/mara-example-project-1) and [mara example project 2](https://github.com/mara/mara-example-project-2).    Due to the heavy use of forking, Mara Pipelines does not run natively on Windows. If you want to run it on Windows, then please use Docker or the [Windows Subsystem for Linux](https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux).     &nbsp;    ## Example    Here is a pipeline ""demo"" consisting of three nodes that depend on each other: the task `ping_localhost`, the pipeline `sub_pipeline` and the task `sleep`:    ```python  from mara_pipelines.commands.bash import RunBash  from mara_pipelines.pipelines import Pipeline, Task  from mara_pipelines.ui.cli import run_pipeline, run_interactively    pipeline = Pipeline(      id='demo',      description='A small pipeline that demonstrates the interplay between pipelines, tasks and commands')    pipeline.add(Task(id='ping_localhost', description='Pings localhost',                    commands=[RunBash('ping -c 3 localhost')]))    sub_pipeline = Pipeline(id='sub_pipeline', description='Pings a number of hosts')    for host in ['google', 'amazon', 'facebook']:      sub_pipeline.add(Task(id=f'ping_{host}', description=f'Pings {host}',                            commands=[RunBash(f'ping -c 3 {host}.com')]))    sub_pipeline.add_dependency('ping_amazon', 'ping_facebook')  sub_pipeline.add(Task(id='ping_foo', description='Pings foo',                        commands=[RunBash('ping foo')]), ['ping_amazon'])    pipeline.add(sub_pipeline, ['ping_localhost'])    pipeline.add(Task(id='sleep', description='Sleeps for 2 seconds',                    commands=[RunBash('sleep 2')]), ['sub_pipeline'])  ```    Tasks contain lists of commands, which do the actual work (in this case running bash commands that ping various hosts).     &nbsp;    In order to run the pipeline, a PostgreSQL database needs to be configured for storing run-time information, run output and status of incremental processing:     ```python  import mara_db.auto_migration  import mara_db.config  import mara_db.dbs    mara_db.config.databases \      = lambda: {'mara': mara_db.dbs.PostgreSQLDB(host='localhost', user='root', database='example_etl_mara')}    mara_db.auto_migration.auto_discover_models_and_migrate()  ```    Given that PostgresSQL is running and the credentials work, the output looks like this (a database with a number of tables is created):    ```  Created database ""postgresql+psycopg2://root@localhost/example_etl_mara""    CREATE TABLE data_integration_file_dependency (      node_path TEXT[] NOT NULL,       dependency_type VARCHAR NOT NULL,       hash VARCHAR,       timestamp TIMESTAMP WITHOUT TIME ZONE,       PRIMARY KEY (node_path, dependency_type)  );    .. more tables  ```    ### CLI UI    This runs a pipeline with output to stdout:    ```python  from mara_pipelines.ui.cli import run_pipeline    run_pipeline(pipeline)  ```    ![Example run cli 1](docs/example-run-cli-1.gif)    &nbsp;    And this runs a single node of pipeline `sub_pipeline` together with all the nodes that it depends on:    ```python  run_pipeline(sub_pipeline, nodes=[sub_pipeline.nodes['ping_amazon']], with_upstreams=True)  ```    ![Example run cli 2](docs/example-run-cli-2.gif)    &nbsp;      And finally, there is some sort of menu based on [pythondialog](http://pythondialog.sourceforge.net/) that allows to navigate and run pipelines like this:    ```python  from mara_pipelines.ui.cli import run_interactively    run_interactively()  ```    ![Example run cli 3](docs/example-run-cli-3.gif)        ### Web UI    More importantly, this package provides an extensive web interface. It can be easily integrated into any [Flask](http://flask.pocoo.org/) based app and the [mara example project](https://github.com/mara/mara-example-project) demonstrates how to do this using [mara-app](https://github.com/mara/mara-app).    For each pipeline, there is a page that shows    - a graph of all child nodes and the dependencies between them  - a chart of the overal run time of the pipeline and it's most expensive nodes over the last 30 days (configurable)  - a table of all the pipeline's nodes with their average run times and the resulting queuing priority  - output and timeline for the last runs of the pipeline      ![Mara pipelines web ui 1](docs/mara-pipelines-web-ui-1.png)    For each task, there is a page showing     - the upstreams and downstreams of the task in the pipeline  - the run times of the task in the last 30 days  - all commands of the task  - output of the last runs of the task    ![Mara pipelines web ui 2](docs/mara-pipelines-web-ui-2.png)      Pipelines and tasks can be run from the web ui directly, which is probably one of the main features of this package:     ![Example run web ui](docs/example-run-web-ui.gif)    &nbsp;    # Getting started    Documentation is currently work in progress. Please use the [mara example project 1](https://github.com/mara/mara-example-project-1) and [mara example project 2](https://github.com/mara/mara-example-project-2) as a reference for getting started.      """
Big data;https://github.com/strapdata/elassandra;"""# Elassandra [![Build Status](https://travis-ci.org/strapdata/elassandra.svg)](https://travis-ci.org/strapdata/elassandra) [![Documentation Status](https://readthedocs.org/projects/elassandra/badge/?version=latest)](https://elassandra.readthedocs.io/en/latest/?badge=latest) [![GitHub release](https://img.shields.io/github/v/release/strapdata/elassandra.svg)](https://github.com/strapdata/elassandra/releases/latest)  [![Twitter](https://img.shields.io/twitter/follow/strapdataio?style=social)](https://twitter.com/strapdataio)    ![Elassandra Logo](elassandra-logo.png)    ## [http://www.elassandra.io/](http://www.elassandra.io/)    Elassandra is an [Apache Cassandra](http://cassandra.apache.org) distribution including an [Elasticsearch](https://github.com/elastic/elasticsearch) search engine.  Elassandra is a multi-master multi-cloud database and search engine with support for replicating across multiple datacenters in active/active mode.    Elasticsearch code is embedded in Cassanda nodes providing advanced search features on Cassandra tables and Cassandra serves as an Elasticsearch data and configuration store.    ![Elassandra architecture](/docs/elassandra/source/images/elassandra1.jpg)    Elassandra supports Cassandra vnodes and scales horizontally by adding more nodes without the need to reshard indices.    Project documentation is available at [doc.elassandra.io](http://doc.elassandra.io).    ## Benefits of Elassandra    For Cassandra users, elassandra provides Elasticsearch features :  * Cassandra updates are indexed in Elasticsearch.  * Full-text and spatial search on your Cassandra data.  * Real-time aggregation (does not require Spark or Hadoop to GROUP BY)  * Provide search on multiple keyspaces and tables in one query.  * Provide automatic schema creation and support nested documents using [User Defined Types](https://docs.datastax.com/en/cql/3.1/cql/cql_using/cqlUseUDT.html).  * Provide read/write JSON REST access to Cassandra data.  * Numerous Elasticsearch plugins and products like [Kibana](https://www.elastic.co/guide/en/kibana/current/introduction.html).  * Manage concurrent elasticsearch mappings changes and applies batched atomic CQL schema changes.  * Support [Elasticsearch ingest processors](https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html) allowing to transform input data.    For Elasticsearch users, elassandra provides useful features :  * Elassandra is masterless. Cluster state is managed through [cassandra lightweight transactions](http://www.datastax.com/dev/blog/lightweight-transactions-in-cassandra-2-0).  * Elassandra is a sharded multi-master database, where Elasticsearch is sharded master-slave. Thus, Elassandra has no Single Point Of Write, helping to achieve high availability.  * Elassandra inherits Cassandra data repair mechanisms (hinted handoff, read repair and nodetool repair) providing support for **cross datacenter replication**.  * When adding a node to an Elassandra cluster, only data pulled from existing nodes are re-indexed in Elasticsearch.  * Cassandra could be your unique datastore for indexed and non-indexed data. It's easier to manage and secure. Source documents are now stored in Cassandra, reducing disk space if you need a NoSQL database and Elasticsearch.  * Write operations are not restricted to one primary shard, but distributed across all Cassandra nodes in a virtual datacenter. The number of shards does not limit your write throughput. Adding elassandra nodes increases both read and write throughput.  * Elasticsearch indices can be replicated among many Cassandra datacenters, allowing write to the closest datacenter and search globally.  * The [cassandra driver](http://www.planetcassandra.org/client-drivers-tools/) is Datacenter and Token aware, providing automatic load-balancing and failover.  * Elassandra efficiently stores Elasticsearch documents in binary SSTables without any JSON overhead.    ## Quick start    * [Quick Start](http://doc.elassandra.io/en/latest/quickstart.html) guide to run a single node Elassandra cluster in docker.  * [Deploy Elassandra by launching a Google Kubernetes Engine](./docs/google-kubernetes-tutorial.md):      [![Open in Cloud Shell](https://gstatic.com/cloudssh/images/open-btn.png)](https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/strapdata/elassandra-google-k8s-marketplace&tutorial=docs/google-kubernetes-tutorial.md)      ## Upgrade Instructions      #### Elassandra 6.8.4.2+    <<<<<<< HEAD  Since version 6.8.4.2, the gossip X1 application state can be compressed using a system property. Enabling this settings allows the creation of a lot of virtual indices.  Before enabling this setting, upgrade all the 6.8.4.x nodes to the 6.8.4.2 (or higher). Once all the nodes are in 6.8.4.2, they are able to decompress the application state even if the settings isn't yet configured locally.    #### Elassandra 6.2.3.25+    Elassandra use the Cassandra GOSSIP protocol to manage the Elasticsearch routing table and Elassandra 6.8.4.2+ add support for compression of  the X1 application state to increase the maxmimum number of Elasticsearch indices. For backward compatibility, the compression is disabled by default,   but once all your nodes are upgraded into version 6.8.4.2+, you should enable the X1 compression by adding **-Des.compress_x1=true** in your **conf/jvm.options** and rolling restart all nodes.  Nodes running version 6.8.4.2+ are able to read compressed and not compressed X1.    #### Elassandra 6.2.3.21+    Before version 6.2.3.21, the Cassandra replication factor for the **elasic_admin** keyspace (and elastic_admin_[datacenter.group]) was automatically adjusted to the   number of nodes of the datacenter. Since version 6.2.3.21 and because it has a performance impact on large clusters, it's now up to your Elassandra administrator to   properly adjust the replication factor for this keyspace. Keep in mind that Elasticsearch mapping updates rely on a PAXOS transaction that requires QUORUM nodes to succeed,   so replication factor should be at least 3 on each datacenter.    #### Elassandra 6.2.3.19+    Elassandra 6.2.3.19 metadata version now relies on the Cassandra table **elastic_admin.metadata_log** (that was **elastic_admin.metadata** from 6.2.3.8 to 6.2.3.18)   to keep the elasticsearch mapping update history and automatically recover from a possible PAXOS write timeout issue.     When upgrading the first node of a cluster, Elassandra automatically copy the current **metadata.version** into the new **elastic_admin.metadata_log** table.  To avoid Elasticsearch mapping inconsistency, you must avoid mapping update while the rolling upgrade is in progress. Once all nodes are upgraded,  the **elastic_admin.metadata** is not more used and can be removed. Then, you can get the mapping update history from the new **elastic_admin.metadata_log** and know  which node has updated the mapping, when and for which reason.    #### Elassandra 6.2.3.8+    Elassandra 6.2.3.8+ now fully manages the elasticsearch mapping in the CQL schema through the use of CQL schema extensions (see *system_schema.tables*, column *extensions*). These table extensions and the CQL schema updates resulting of elasticsearch index creation/modification are updated in batched atomic schema updates to ensure consistency when concurrent updates occurs. Moreover, these extensions are stored in binary and support partial updates to be more efficient. As the result, the elasticsearch mapping is not more stored in the *elastic_admin.metadata* table.     WARNING: During the rolling upgrade, elasticserach mapping changes are not propagated between nodes running the new and the old versions, so don't change your mapping while you're upgrading. Once all your nodes have been upgraded to 6.2.3.8+ and validated, apply the following CQL statements to remove useless elasticsearch metadata:  ```bash  ALTER TABLE elastic_admin.metadata DROP metadata;  ALTER TABLE elastic_admin.metadata WITH comment = '';  ```    WARNING: Due to CQL table extensions used by Elassandra, some old versions of **cqlsh** may lead to the following error message **""'module' object has no attribute 'viewkeys'.""**. This comes from the old python cassandra driver embedded in Cassandra and has been reported in [CASSANDRA-14942](https://issues.apache.org/jira/browse/CASSANDRA-14942). Possible workarounds:  * Use the **cqlsh** embedded with Elassandra  * Install a recent version of the  **cqlsh** utility (*pip install cqlsh*) or run it from a docker image:    ```bash  docker run -it --rm strapdata/cqlsh:0.1 node.example.com  ```    #### Elassandra 6.x changes    * Elasticsearch now supports only one document type per index backed by one Cassandra table. Unless you specify an elasticsearch type name in your mapping, data is stored in a cassandra table named **""_doc""**. If you want to search many cassandra tables, you now need to create and search many indices.  * Elasticsearch 6.x manages shard consistency through several metadata fields (_primary_term, _seq_no, _version) that are not used in elassandra because replication is fully managed by cassandra.    ## Installation    Ensure Java 8 is installed and `JAVA_HOME` points to the correct location.    * [Download](https://github.com/strapdata/elassandra/releases) and extract the distribution tarball  * Define the CASSANDRA_HOME environment variable : `export CASSANDRA_HOME=<extracted_directory>`  * Run `bin/cassandra -e`  * Run `bin/nodetool status`  * Run `curl -XGET localhost:9200/_cluster/state`    #### Example    Try indexing a document on a non-existing index:    ```bash  curl -XPUT 'http://localhost:9200/twitter/_doc/1?pretty' -H 'Content-Type: application/json' -d '{      ""user"": ""Poulpy"",      ""post_date"": ""2017-10-04T13:12:00Z"",      ""message"": ""Elassandra adds dynamic mapping to Cassandra""  }'  ```    Then look-up in Cassandra:    ```bash  bin/cqlsh -e ""SELECT * from twitter.\""_doc\""""  ```    Behind the scenes, Elassandra has created a new Keyspace `twitter` and table `_doc`.    ```CQL  admin@cqlsh>DESC KEYSPACE twitter;    CREATE KEYSPACE twitter WITH replication = {'class': 'NetworkTopologyStrategy', 'DC1': '1'}  AND durable_writes = true;    CREATE TABLE twitter.""_doc"" (      ""_id"" text PRIMARY KEY,      message list<text>,      post_date list<timestamp>,      user list<text>  ) WITH bloom_filter_fp_chance = 0.01      AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}      AND comment = ''      AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}      AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}      AND crc_check_chance = 1.0      AND dclocal_read_repair_chance = 0.1      AND default_time_to_live = 0      AND gc_grace_seconds = 864000      AND max_index_interval = 2048      AND memtable_flush_period_in_ms = 0      AND min_index_interval = 128      AND read_repair_chance = 0.0      AND speculative_retry = '99PERCENTILE';  CREATE CUSTOM INDEX elastic__doc_idx ON twitter.""_doc"" () USING 'org.elassandra.index.ExtendedElasticSecondaryIndex';  ```    By default, multi valued Elasticsearch fields are mapped to Cassandra list.  Now, insert a row with CQL :    ```CQL  INSERT INTO twitter.""_doc"" (""_id"", user, post_date, message)  VALUES ( '2', ['Jimmy'], [dateof(now())], ['New data is indexed automatically']);  SELECT * FROM twitter.""_doc"";     _id | message                                          | post_date                           | user  -----+--------------------------------------------------+-------------------------------------+------------     2 |            ['New data is indexed automatically'] | ['2019-07-04 06:00:21.893000+0000'] |  ['Jimmy']     1 | ['Elassandra adds dynamic mapping to Cassandra'] | ['2017-10-04 13:12:00.000000+0000'] | ['Poulpy']    (2 rows)  ```    Then search for it with the Elasticsearch API:    ```bash  curl ""localhost:9200/twitter/_search?q=user:Jimmy&pretty""  ```    And here is a sample response :    ```JSON  {    ""took"" : 3,    ""timed_out"" : false,    ""_shards"" : {      ""total"" : 1,      ""successful"" : 1,      ""skipped"" : 0,      ""failed"" : 0    },    ""hits"" : {      ""total"" : 1,      ""max_score"" : 0.6931472,      ""hits"" : [        {          ""_index"" : ""twitter"",          ""_type"" : ""_doc"",          ""_id"" : ""2"",          ""_score"" : 0.6931472,          ""_source"" : {            ""post_date"" : ""2019-07-04T06:00:21.893Z"",            ""message"" : ""New data is indexed automatically"",            ""user"" : ""Jimmy""          }        }      ]    }  }  ```    ## Support     * Commercial support is available through [Strapdata](http://www.strapdata.com/).   * Community support available via [elassandra google groups](https://groups.google.com/forum/#!forum/elassandra).   * Post feature requests and bugs on https://github.com/strapdata/elassandra/issues    ## License    ```  This software is licensed under the Apache License, version 2 (""ALv2""), quoted below.    Copyright 2015-2018, Strapdata (contact@strapdata.com).    Licensed under the Apache License, Version 2.0 (the ""License""); you may not  use this file except in compliance with the License. You may obtain a copy of  the License at        http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the  License for the specific language governing permissions and limitations under  the License.  ```    ## Acknowledgments    * Elasticsearch, Logstash, Beats and Kibana are trademarks of Elasticsearch BV, registered in the U.S. and in other countries.  * Apache Cassandra, Apache Lucene, Apache, Lucene and Cassandra are trademarks of the Apache Software Foundation.  * Elassandra is a trademark of Strapdata SAS. """
Big data;https://github.com/HumbleSoftware/envisionjs;"""# Envision.js  -------------    Fast interactive HTML5 charts.    ![Google Groups](http://groups.google.com/intl/en/images/logos/groups_logo_sm.gif)    http://groups.google.com/group/envisionjs/    ## Features    * Modern Browsers, IE 6+  * Mobile / Touch Support  * Pre-built Templates  * Adaptable to Existing Libraries    ## Dependencies    Envision.js ships with all it's dependencies.  It uses:    * <a href=""http://documentcloud.github.com/underscore/"">underscore.js</a>  * <a href=""https://github.com/fat/bean"">bean</a>  * <a href=""https://github.com/ded/bonzo"">bonzo</a>  * <a href=""http://humblesoftware.com/flotr2/"">Flotr2</a>    ## Usage    To use Envision.js, include `envision.min.js` and `envision.min.css` in your  page. To display a visualization, either use a Template or create a custom  visualization with the Envision.js API.    ### Templates    Templates are pre-built visualizations for common use-cases.    Example:     ```javascript    var      container = document.getElementById('container'),      x = [],      y1 = [],      y2 = [],      data, options, i;      // Data Format:    data = [      [x, y1], // First Series      [x, y2]  // Second Series    ];      // Sample the sine function for data    for (i = 0; i < 4 * Math.PI; i += 0.05) {      x.push(i);      y1.push(Math.sin(i));      y2.push(Math.sin(i + Math.PI));    }      // TimeSeries Template Options    options = {      // Container to render inside of      container : container,      // Data for detail (top chart) and summary (bottom chart)      data : {        detail : data,        summary : data      }    };      // Create the TimeSeries    new envision.templates.TimeSeries(options);  ```    ### Custom    Developers can use the envision APIs to build custom visualizations.  The  existing templates are a good reference for this.    Example:     ```javascript    var      container = document.getElementById('container'),      x = [],      y1 = [],      y2 = [],      data, i,      detail, detailOptions,      summary, summaryOptions,      vis, selection,      // Data Format:    data = [      [x, y1], // First Series      [x, y2]  // Second Series    ];      // Sample the sine function for data    for (i = 0; i < 4 * Math.PI; i += 0.05) {      x.push(i);      y1.push(Math.sin(i));      y2.push(Math.sin(i + Math.PI));    }    x.push(4 * Math.PI)    y1.push(Math.sin(4 * Math.PI));    y2.push(Math.sin(4 * Math.PI));      // Configuration for detail:    detailOptions = {      name : 'detail',      data : data,      height : 150,      flotr : {        yaxis : {          min : -1.1,          max : 1.1        }      }    };      // Configuration for summary:    summaryOptions = {      name : 'summary',      data : data,      height : 150,      flotr : {        yaxis : {          min : -1.1,          max : 1.1        },        selection : {          mode : 'x'        }      }    };      // Building a custom vis:    vis = new envision.Visualization();    detail = new envision.Component(detailOptions);    summary = new envision.Component(summaryOptions);    interaction = new envision.Interaction();      // Render Visualization    vis      .add(detail)      .add(summary)      .render(container);      // Wireup Interaction    interaction      .leader(summary)      .follower(detail)      .add(envision.actions.selection);  ```    ## API    ### Class `envision.Component`  _Defines a visualization component._    Components are the building blocks of a visualization,   representing one typically graphical piece of the vis.  This class manages  the options, DOM and API construction for an adapter which handles the  actual drawing of the visualization piece.    Adapters can take the form of an actual object, a constructor function  or a function returning an object.  Only one of these will be used.  If  none is submitted, the default adapter Flotr2 is used.    #### Configuration:    An object is submitted to the constructor for configuration.    * `name` A name for the component.  * `element` A container element for the component.  * `height` An explicit component height.  * `width` An explicit component width.  * `data` An array of data.  Data may be formatted for   envision or for the adapter itself, in which case skipPreprocess will  also need to be submitted.  * `skipPreprocess` Skip data preprocessing.  This is useful  when using the native data format for an adapter.  * `adapter` An adapter object.  * `adapterConstructor` An adapter constructor to be  instantiated by the component.  * `adapterCallback` An callback invoked by the component  returning an adapter.  * `config` Configuration for the adapter.    #### Methods:    ##### `render ([element])`  Render the component.    If no element is submitted, the component will  render in the element configured in the constructor.    ##### `draw ([data], [options])`  Draw the component.    ##### `trigger ()`  Trigger an event on the component's API.    Arguments are passed through to the API.    ##### `attach ()`  Attach to an event on the component's API.    Arguments are passed through to the API.    ##### `detach ()`  Detach a listener from an event on the component's API.    Arguments are passed through to the API.    ##### `destroy ()`  Destroy the component.    Empties the container and calls the destroy method on the  component's API.    ### Class `envision.Visualization`  _Defines a visualization of componenents._    This class manages the rendering of a visualization.  It provides convenience methods for adding, removing, and reordered  components dynamically as well as convenience methods for working  with a logical group of components.    #### Configuration:    An object is submitted to the constructor for configuration.    * `name` A name for the visualization.  * `element` A container element for the visualization.    #### Methods:    ##### `render ([element])`  Render the visualization.    If no element is submitted, the visualization will  render in the element configured in the constructor.    This method is chainable.    ##### `add (component)`  Add a component to the visualization.    If the visualization has already been rendered,  it will render the new component.    This method is chainable.    ##### `remove ()`  Remove a component from the visualization.    This removes the components from the list of components in the  visualization and removes its container from the DOM.  It does not  destroy the component.    This method is chainable.    ##### `setPosition (component, newIndex)`  Reorders a component.    This method is chainable.    ##### `indexOf (component)`  Gets the position of a component.    ##### `getComponent (component)`  Gets the component at a position.    ##### `isFirst (component)`  Gets whether or not the component is the first component  in the visualization.    ##### `isLast (component)`  Gets whether or not the component is the last component  in the visualization.    ##### `destroy ()`  Destroys the visualization.    This empties the container and destroys all the components which are part  of the visualization.    ### Class `envision.Preprocessor`  _Data preprocessor._    Data can be preprocessed before it is rendered by an adapter.    This has several important performance considerations.  If data will be   rendered repeatedly or on slower browsers, it will be faster after being  optimized.    First, data outside the boundaries does not need to be rendered.  Second,  the resolution of the data only needs to be at most the number of pixels  in the width of the visualization.    Performing these optimizations will limit memory overhead, important  for garbage collection and performance on old browsers, as well as drawing  overhead, important for mobile devices, old browsers and large data sets.    #### Configuration:    An object is submitted to the constructor for configuration.    * `data` The data for processing.    #### Methods:    ##### `getData ()`  Returns data.    ##### `setData ()`  Set the data object.    ##### `length ()`  Returns the length of the data set.    ##### `bound (min, max)`  Bounds the data set at within a range.    ##### `subsampleMinMax (resolution)`  Subsample data using MinMax.    MinMax will display the extrema of the subsample intervals.  This is  slower than regular interval subsampling but necessary for data that   is very non-homogenous.    ##### `subsample (resolution)`  Subsample data at a regular interval for resolution.    This is the fastest subsampling and good for monotonic data and fairly  homogenous data (not a lot of up and down).    ### Class `envision.Interaction`  _Defines an interaction between components._    This class defines interactions in which actions are triggered  by leader components and reacted to by follower components.  These actions  are defined as configurable mappings of trigger events and event consumers.  It is up to the adapter to implement the triggers and consumers.    A component may be both a leader and a follower.  A leader which is a   follower will react to actions triggered by other leaders, but will safely  not react to its own.  This allows for groups of components to perform a  common action.    Optionally, actions may be supplied with a callback executed before the   action is consumed.  This allows for quick custom functionality to be added  and is how advanced data management (ie. live Ajax data) may be implemented.    This class follow an observer mediator pattern.    #### Configuration:    An object is submitted to the constructor for configuration.    * `leader` Component(s) to lead the  interaction    #### Methods:    ##### `leader (component)`  Add a component as an interaction leader.    ##### `follower (component)`  Add a component as an interaction leader.    ##### `group (components)`  Adds an array of components as both followers and leaders.    ##### `add (action, [options])`  Adds an action to the interaction.    The action may be optionally configured with the options argument.  Currently the accepts a callback member, invoked after an action  is triggered and before it is consumed by followers.    ## Development    This project uses [smoosh](https://github.com/fat/smoosh) to build and [jasmine](http://pivotal.github.com/jasmine/)   with [js-imagediff](https://github.com/HumbleSoftware/js-imagediff) to test.  Tests may be executed by   [jasmine-headless-webkit](http://johnbintz.github.com/jasmine-headless-webkit/) with   `cd spec; jasmine-headless-webkit -j jasmine.yml -c` or by a browser by navigating to   `spec/SpecRunner.html`.   """
Big data;https://github.com/gephi/gephi;"""# Gephi - The Open Graph Viz Platform    [![build](https://github.com/gephi/gephi/actions/workflows/build.yml/badge.svg)](https://github.com/gephi/gephi/actions/workflows/build.yml)  [![Downloads](https://img.shields.io/github/downloads/gephi/gephi/v0.9.2/total.svg)](https://github.com/gephi/gephi/releases/tag/v0.9.2)  [![Downloads](https://img.shields.io/github/downloads/gephi/gephi/v0.9.1/total.svg)](https://github.com/gephi/gephi/releases/tag/v0.9.1)  [![Translation progress](https://hosted.weblate.org/widgets/gephi/-/svg-badge.svg)](https://hosted.weblate.org/engage/gephi/?utm_source=widget)    [Gephi](http://gephi.org) is an award-winning open-source platform for visualizing and manipulating large graphs. It runs on Windows, Mac OS X and Linux. Localization is available in English, French, Spanish, Japanese, Russian, Brazilian Portuguese, Chinese, Czech and German.    - **Fast** Powered by a built-in OpenGL engine, Gephi is able to push the envelope with very large networks. Visualize networks up to a million elements. All actions (e.g. layout, filter, drag) run in real-time.    - **Simple** Easy to install and [get started](https://gephi.github.io/users/quick-start). An UI that is centered around the visualization. Like Photoshopâ„¢ for graphs.    - **Modular** Extend Gephi with [plug-ins](https://gephi.org/plugins). The architecture is built on top of [Apache Netbeans Platform](https://netbeans.apache.org/tutorials/nbm-quick-start.html) and can be extended or reused easily through well-written APIs.    [Download Gephi](https://gephi.github.io/users/download) for Windows, Mac OS X and Linux and consult the [release notes](https://github.com/gephi/gephi/wiki/Releases). Example datasets can be found on our [wiki](https://github.com/gephi/gephi/wiki/Datasets).    ![Gephi](https://gephi.github.io/images/screenshots/select-tool-mini.png)    ## Install and use Gephi    Download and [Install](https://gephi.github.io/users/install/) Gephi on your computer.     Get started with the [Quick Start](https://gephi.github.io/users/quick-start/) and follow the [Tutorials](https://gephi.github.io/users/). Load a sample [dataset](https://github.com/gephi/gephi/wiki/Datasets) and start to play with the data.    If you run into any trouble or have questions consult our [forum](http://forum-gephi.org/).    ## Latest releases    ### Stable    - Latest stable release on [gephi.org](https://gephi.org/users/download/).    ### Nightly builds    Current version is 0.9.3-SNAPSHOT    - [gephi-0.9.3-SNAPSHOT-windows.exe](https://oss.sonatype.org/service/local/artifact/maven/content?r=snapshots&g=org.gephi&a=gephi&v=0.9.3-SNAPSHOT&c=windows&p=exe) (Windows)    - [gephi-0.9.3-SNAPSHOT-macos.dmg](https://oss.sonatype.org/service/local/artifact/maven/content?r=snapshots&g=org.gephi&a=gephi&v=0.9.3-SNAPSHOT&c=macos&p=dmg) (Mac OS X)    - [gephi-0.9.3-SNAPSHOT-linux.tar.gz](https://oss.sonatype.org/service/local/artifact/maven/content?r=snapshots&g=org.gephi&a=gephi&v=0.9.3-SNAPSHOT&c=linux&p=tar.gz) (Linux)    - [gephi-0.9.3-SNAPSHOT-sources.tar.gz](https://oss.sonatype.org/service/local/artifact/maven/content?r=snapshots&g=org.gephi&a=gephi&v=0.9.3-SNAPSHOT&c=sources&p=tar.gz) (Sources)    ## Developer Introduction    Gephi is developed in Java and uses OpenGL for its visualization engine. Built on the top of Netbeans Platform, it follows a loosely-coupled, modular architecture philosophy. Gephi is split into modules, which depend on other modules through well-written APIs. Plugins can reuse existing APIs, create new services and even replace a default implementation with a new one.    Consult the [**Javadoc**](http://gephi.github.io/gephi/0.9.2/apidocs/index.html) for an overview of the APIs.    ### Requirements    - Java JDK 11 (or later)    - [Apache Maven](http://maven.apache.org/) version 3.6.3 or later    ### Checkout and Build the sources    - Fork the repository and clone            git clone git@github.com:username/gephi.git    - Run the following command or [open the project in an IDE](https://github.com/gephi/gephi/wiki/How-to-build-Gephi)            mvn -T 4 clean install    - Once built, one can test running Gephi    		cd modules/application  		mvn nbm:cluster-app nbm:run-platform    ### Create Plug-ins    Gephi is extensible and lets developers create plug-ins to add new features, or to modify existing features. For example, you can create a new layout algorithm, add a metric, create a filter or a tool, support a new file format or database, or modify the visualization.    - [**Plugins Portal**](https://github.com/gephi/gephi/wiki/Plugins)    - [Plugins Quick Start (5 minutes)](https://github.com/gephi/gephi/wiki/Plugin-Quick-Start)    - Browse the [plugins](https://gephi.org/plugins) created by the community    - We've created a [**Plugins Bootcamp**](https://github.com/gephi/gephi-plugins-bootcamp) to learn by examples.    ## Gephi Toolkit    The Gephi Toolkit project packages essential Gephi modules (Graph, Layout, Filters, IOâ€¦) in a standard Java library which any Java project can use for getting things done. It can be used on a server or command-line tool to do the same things Gephi does but automatically.    - [Download](https://gephi.org/toolkit/)    - [GitHub Project](https://github.com/gephi/gephi-toolkit)    - [Toolkit Portal](https://github.com/gephi/gephi/wiki/Toolkit)    ## Localization    We use [Weblate](https://hosted.weblate.org/projects/gephi/) for localization. Follow the guidelines on the [wiki](https://github.com/gephi/gephi/wiki/Localization) for more details how to contribute.    ## License    Gephi main source code is distributed under the dual license [CDDL 1.0](http://www.opensource.org/licenses/CDDL-1.0) and [GNU General Public License v3](http://www.gnu.org/licenses/gpl.html). Read the [Legal FAQs](http://gephi.github.io/legal/faq/)  to learn more.  	  Copyright 2011 Gephi Consortium. All rights reserved.    The contents of this file are subject to the terms of either the GNU  General Public License Version 3 only (""GPL"") or the Common  Development and Distribution License (""CDDL"") (collectively, the  ""License""). You may not use this file except in compliance with the  License. You can obtain a copy of the License at  http://gephi.github.io/developers/license/  or /cddl-1.0.txt and /gpl-3.0.txt. See the License for the  specific language governing permissions and limitations under the  License.  When distributing the software, include this License Header  Notice in each file and include the License files at  /cddl-1.0.txt and /gpl-3.0.txt. If applicable, add the following below the  License Header, with the fields enclosed by brackets [] replaced by  your own identifying information:  ""Portions Copyrighted [year] [name of copyright owner]""    If you wish your version of this file to be governed by only the CDDL  or only the GPL Version 3, indicate your decision by adding  ""[Contributor] elects to include this software in this distribution  under the [CDDL or GPL Version 3] license."" If you do not indicate a  single choice of license, a recipient has the option to distribute  your version of this file under either the CDDL, the GPL Version 3 or  to extend the choice of license to its licensees as provided above.  However, if you add GPL Version 3 code and therefore, elected the GPL  Version 3 license, then the option applies only if the new code is  made subject to such option by the copyright holder. """
Big data;https://github.com/facebookresearch/faiss;"""# Faiss    Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU. It is developed primarily at [Facebook AI Research](https://ai.facebook.com/).    ## News    See [CHANGELOG.md](CHANGELOG.md) for detailed information about latest features.    ## Introduction    Faiss contains several methods for similarity search. It assumes that the instances are represented as vectors and are identified by an integer, and that the vectors can be compared with L2 (Euclidean) distances or dot products. Vectors that are similar to a query vector are those that have the lowest L2 distance or the highest dot product with the query vector. It also supports cosine similarity, since this is a dot product on normalized vectors.    Some of the methods, like those based on binary vectors and compact quantization codes, solely use a compressed representation of the vectors and do not require to keep the original vectors. This generally comes at the cost of a less precise search but these methods can scale to billions of vectors in main memory on a single server. Other methods, like HNSW and NSG add an indexing structure on top of the raw vectors to make searching more efficient.    The GPU implementation can accept input from either CPU or GPU memory. On a server with GPUs, the GPU indexes can be used a drop-in replacement for the CPU indexes (e.g., replace `IndexFlatL2` with `GpuIndexFlatL2`) and copies to/from GPU memory are handled automatically. Results will be faster however if both input and output remain resident on the GPU. Both single and multi-GPU usage is supported.    ## Installing    Faiss comes with precompiled libraries for Anaconda in Python, see [faiss-cpu](https://anaconda.org/pytorch/faiss-cpu) and [faiss-gpu](https://anaconda.org/pytorch/faiss-gpu). The library is mostly implemented in C++, the only dependency is a [BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) implementation. Optional GPU support is provided via CUDA, and and the Python interface is also optional. It compiles with cmake. See [INSTALL.md](INSTALL.md) for details.    ## How Faiss works    Faiss is built around an index type that stores a set of vectors, and provides a function to search in them with L2 and/or dot product vector comparison. Some index types are simple baselines, such as exact search. Most of the available indexing structures correspond to various trade-offs with respect to    - search time  - search quality  - memory used per index vector  - training time  - adding time  - need for external data for unsupervised training    The optional GPU implementation provides what is likely (as of March 2017) the fastest exact and approximate (compressed-domain) nearest neighbor search implementation for high-dimensional vectors, fastest Lloyd's k-means, and fastest small k-selection algorithm known. [The implementation is detailed here](https://arxiv.org/abs/1702.08734).    ## Full documentation of Faiss    The following are entry points for documentation:    - the full documentation can be found on the [wiki page](http://github.com/facebookresearch/faiss/wiki), including a [tutorial](https://github.com/facebookresearch/faiss/wiki/Getting-started), a [FAQ](https://github.com/facebookresearch/faiss/wiki/FAQ) and a [troubleshooting section](https://github.com/facebookresearch/faiss/wiki/Troubleshooting)  - the [doxygen documentation](https://faiss.ai/) gives per-class information extracted from code comments  - to reproduce results from our research papers, [Polysemous codes](https://arxiv.org/abs/1609.01882) and [Billion-scale similarity search with GPUs](https://arxiv.org/abs/1702.08734), refer to the [benchmarks README](benchs/README.md). For [  Link and code: Fast indexing with graphs and compact regression codes](https://arxiv.org/abs/1804.09996), see the [link_and_code README](benchs/link_and_code)    ## Authors    The main authors of Faiss are:  - [HervÃ© JÃ©gou](https://github.com/jegou) initiated the Faiss project and wrote its first implementation  - [Matthijs Douze](https://github.com/mdouze) implemented most of the CPU Faiss  - [Jeff Johnson](https://github.com/wickedfoo) implemented all of the GPU Faiss  - [Lucas Hosseini](https://github.com/beauby) implemented the binary indexes and the build system  - [Chengqi Deng](https://github.com/KinglittleQ) implemented NSG, NNdescent and much of the additive quantization code.    ## Reference    Reference to cite when you use Faiss in a research paper:    ```  @article{johnson2019billion,    title={Billion-scale similarity search with {GPUs}},    author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},    journal={IEEE Transactions on Big Data},    volume={7},    number={3},    pages={535--547},    year={2019},    publisher={IEEE}  }  ```    ## Join the Faiss community    For public discussion of Faiss or for questions, there is a Facebook group at https://www.facebook.com/groups/faissusers/    We monitor the [issues page](http://github.com/facebookresearch/faiss/issues) of the repository.  You can report bugs, ask questions, etc.    ## License    Faiss is MIT-licensed. """
Big data;https://github.com/vega/vega;"""# Vega: A Visualization Grammar <a href=""https://vega.github.io/vega/""><img align=""right"" src=""https://github.com/vega/logos/blob/master/assets/VG_Color@64.png?raw=true"" height=""38""></img></a>    <a href=""https://vega.github.io/vega/examples"">  <img src=""https://vega.github.io/vega/assets/banner.png"" alt=""Vega Examples"" width=""900""></img>  </a>    **Vega** is a *visualization grammar*, a declarative format for creating, saving, and sharing interactive visualization designs. With Vega you can describe data visualizations in a JSON format, and generate interactive views using either HTML5 Canvas or SVG.    For documentation, tutorials, and examples, see the [Vega website](https://vega.github.io/vega). For a description of changes between Vega 2 and later versions, please refer to the [Vega Porting Guide](https://vega.github.io/vega/docs/porting-guide/).    ## Build Instructions    For a basic setup allowing you to build Vega and run examples:    - Clone `https://github.com/vega/vega`.  - Run `yarn` to install dependencies for all packages. If you don't have yarn installed, see https://yarnpkg.com/en/docs/install. We use [Yarn workspaces](https://yarnpkg.com/lang/en/docs/workspaces/) to manage multiple packages within this [monorepo](https://en.wikipedia.org/wiki/Monorepo).  - Once installation is complete, run `yarn test` to run test cases, or run `yarn build` to build output files for all packages.  - After running either `yarn test` or `yarn build`, run `yarn serve` to launch a local web server &mdash; your default browser will open and you can browse to the `""test""` folder to view test specifications.    This repository includes the Vega website and documentation in the `docs` folder. To launch the website locally, first run `bundle install` in the `docs` folder to install the necessary Jekyll libraries. Afterwards, use `yarn docs` to build the documentation and launch a local webserver. After launching, you can open [`http://127.0.0.1:4000/vega/`](http://127.0.0.1:4000/vega/) to see the website.    ## ES5 Support  For backwards compatibility, Vega includes a [babel-ified](https://babeljs.io/) ES5-compatible version of the code in `packages/vega/build-es5` directory. Older browser would also require several polyfill libraries:    ```html  <script src=""https://cdnjs.cloudflare.com/ajax/libs/babel-polyfill/7.4.4/polyfill.min.js""></script>  <script src=""https://cdn.jsdelivr.net/npm/regenerator-runtime@0.13.3/runtime.min.js""></script>  <script src=""https://cdn.jsdelivr.net/npm/whatwg-fetch@3.0.0/dist/fetch.umd.min.js""></script>  ```    ## Contributions, Development, and Support    Interested in contributing to Vega? Please see our [contribution and development guidelines](CONTRIBUTING.md), subject to our [code of conduct](https://vega.github.io/vega/about/code-of-conduct/).    Looking for support, or interested in sharing examples and tips? Post to the [Vega discussion forum](https://groups.google.com/forum/#!forum/vega-js) or join the [Vega slack organization](https://bit.ly/join-vega-slack-2020)! We also have examples available as [Observable notebooks](https://observablehq.com/@vega).    If you're curious about system performance, see some [in-browser benchmarks](https://observablehq.com/@vega/vega-performance-tests). Read about future plans in [our roadmap](https://github.com/vega/roadmap/projects/1). """
Big data;https://github.com/semi-technologies/weaviate;"""<h1>Weaviate <img alt='Weaviate logo' src='https://raw.githubusercontent.com/semi-technologies/weaviate/19de0956c69b66c5552447e84d016f4fe29d12c9/docs/assets/weaviate-logo.png' width='124' align='right' /></h1>    ## The ML-first vector search engine    [![Build Status](https://api.travis-ci.org/semi-technologies/weaviate.svg?branch=master)](https://travis-ci.org/semi-technologies/weaviate/branches)  [![Go Report Card](https://goreportcard.com/badge/github.com/semi-technologies/weaviate)](https://goreportcard.com/report/github.com/semi-technologies/weaviate)  [![Coverage Status](https://codecov.io/gh/semi-technologies/weaviate/branch/master/graph/badge.svg)](https://codecov.io/gh/semi-technologies/weaviate)  [![Slack](https://img.shields.io/badge/slack--channel-blue?logo=slack)](https://join.slack.com/t/weaviate/shared_invite/zt-goaoifjr-o8FuVz9b1HLzhlUfyfddhw)  [![Newsletter](https://img.shields.io/badge/newsletter-blue?logo=revue)](http://weaviate-newsletter.semi.technology/)    ## Description    **Weaviate in a nutshell**: Weaviate is a vector search engine and vector database. Weaviate uses machine learning to vectorize and store data, and to find answers to natural language queries. With Weaviate you can also bring your custom ML models to production scale.    **Weaviate in detail**: Weaviate is a low-latency vector search engine with out-of-the-box support for different media types (text, images, etc.). It offers Semantic Search, Question-Answer-Extraction, Classification, Customizable Models (PyTorch/TensorFlow/Keras), and more. Built from scratch in Go, Weaviate stores both objects and vectors, allowing for combining vector search with structured filtering with the fault-tolerance of a cloud-native database, all accessible through GraphQL, REST, and various language clients.    ## Weaviate helps ...    1. **Software Engineers** ([docs](https://weaviate.io/developers/weaviate/current/)) - Who use Weaviate as an ML-first database for your applications.       * Out-of-the-box modules for: NLP/semantic search, automatic classification and image similarity search.      * Easy to integrate in your current architecture, with full CRUD support like you're used to from other OSS databases.      * Cloud-native, distributed, runs well on Kubernetes and scales with your workloads.    2. **Data Engineers** ([docs](https://weaviate.io/developers/weaviate/current/)) - Who use Weaviate as a vector database that is built up from the ground with ANN at its core, and with the same UX they love from Lucene-based search engines.      * Weaviate has a modular setup that allows to use your own ML models inside Weaviate, but you can also use out-of-the-box ML models (e.g., SBERT, ResNet, fasttext, etc).      * Weaviate takes care of the scalability, so that you don't have to.      * Deploy and maintain ML models in production reliably and efficiently.    3. **Data Scientists** ([docs](https://weaviate.io/developers/weaviate/current/)) - Who use Weaviate for a seamless handover of their Machine Learning models to MLOps.      * Deploy and maintain your ML models in production reliably and efficiently.      * Weaviate's modular design allows you to easily package any custom trained model you want.      * Smooth and accelerated handover of your Machine Learning models to engineers.    ## GraphQL interface demo    <a href=""https://weaviate.io/developers/weaviate/current/"" target=""_blank""><img src=""https://weaviate.io/img/weaviate-demo.gif?i=8"" alt=""Demo of Weaviate"" width=""100%""></a>    <sup>Weaviate GraphQL demo on news article dataset containing: Transformers module, GraphQL usage, semantic search, _additional{} features, Q&A, and Aggregate{} function. You can the demo on this dataset in the GUI here: <a href=""https://console.semi.technology/console/query#weaviate_uri=https://demo.dataset.playground.semi.technology&graphql_query=%7B%0A%20%20Get%20%7B%0A%20%20%20%20Article(%0A%20%20%20%20%20%20nearText%3A%20%7B%0A%20%20%20%20%20%20%20%20concepts%3A%20%5B%22Housing%20prices%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20where%3A%20%7B%0A%20%20%20%20%20%20%20%20operator%3A%20Equal%0A%20%20%20%20%20%20%20%20path%3A%20%5B%22inPublication%22%2C%20%22Publication%22%2C%20%22name%22%5D%0A%20%20%20%20%20%20%20%20valueString%3A%20%22The%20Economist%22%0A%20%20%20%20%20%20%7D%0A%20%20%20%20)%20%7B%0A%20%20%20%20%20%20title%0A%20%20%20%20%20%20inPublication%20%7B%0A%20%20%20%20%20%20%20%20...%20on%20Publication%20%7B%0A%20%20%20%20%20%20%20%20%20%20name%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20_additional%20%7B%0A%20%20%20%20%20%20%20%20certainty%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D"" target=""_blank"">semantic search</a>, <a href=""https://console.semi.technology/console/query#weaviate_uri=https://demo.dataset.playground.semi.technology&graphql_query=%7B%0A%20%20Get%7B%0A%20%20%20%20Article(%0A%20%20%20%20%20%20ask%3A%20%7B%0A%20%20%20%20%20%20%20%20question%3A%20%22What%20did%20Jemina%20Packington%20predict%3F%22%0A%20%20%20%20%20%20%20%20properties%3A%20%5B%22summary%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20limit%3A%201%0A%20%20%20%20)%7B%0A%20%20%20%20%20%20title%0A%20%20%20%20%20%20inPublication%20%7B%0A%20%20%20%20%20%20%20%20...%20on%20Publication%20%7B%0A%20%20%20%20%20%20%20%20%20%20name%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20_additional%20%7B%0A%20%20%20%20%20%20%20%20answer%20%7B%0A%20%20%20%20%20%20%20%20%20%20endPosition%0A%20%20%20%20%20%20%20%20%20%20property%0A%20%20%20%20%20%20%20%20%20%20result%0A%20%20%20%20%20%20%20%20%20%20startPosition%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D"" target=""_blank"">Q&A</a>, <a href=""https://console.semi.technology/console/query#weaviate_uri=https://demo.dataset.playground.semi.technology&graphql_query=%7B%0A%20%20Aggregate%20%7B%0A%20%20%20%20Article%20%7B%0A%20%20%20%20%20%20meta%20%7B%0A%20%20%20%20%20%20%20%20count%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D"" target=""_blank"">Aggregate</a>.</sup>    ## Features    Weaviate makes it easy to use state-of-the-art ML models while giving you the scalability, ease of use, safety and cost-effectiveness of a purpose-built vector database. Most notably:    * **Fast queries**<br>     Weaviate typically performs a 10-NN neighbor search out of millions of objects in considerably less than 100ms.     <br><sub></sub>    * **Any media type with Weaviate Modules**<br>    Use State-of-the-Art ML model inference (e.g. Transformers) for Text, Images, etc. at search and query time to let Weaviate manage the process of vectorizing your data for your - or import your own vectors.    * **Combine vector and scalar search**<br>    Weaviate allows for efficient combined vector and scalar searches, e.g â€œarticles related to the COVID 19 pandemic published within the past 7 daysâ€. Weaviate stores both your objects and the vectors and make sure the retrieval of both is always efficient. There is no need for a third party object storage.     * **Real-time and persistent**<br>  Weaviate letâ€™s you search through your data even if itâ€™s currently being imported or updated. In addition, every write is written to a Write-Ahead-Log (WAL) for immediately persisted writes - even when a crash occurs.    * **Horizontal Scalability**<br>    Scale Weaviate for your exact needs, e.g. High-Availability, maximum ingestion, largest possible dataset size, maximum queries per second, etc. (Multi-Node sharding since `v1.8.0`, Replication under development)     * **Cost-Effectiveness**<br>    Very large datasets do not need to be kept entirely in memory in Weaviate. At the same time available memory can be used to increase the speed of queries. This allows for a conscious speed/cost trade-off to suit every use case.    * **Graph-like connections between objects**<br>    Make arbitrary connections between your objects in a graph-like fashion to resemble real-life connections between your data points. Traverse those connections using GraphQL.    ## Documentation    You can find detailed documentation in the [developers section of our website](https://weaviate.io/developers/weaviate/current/) or directly go to one of the docs using the links in the list below.    ## Additional material    ### Video    - [Weaviate introduction video](https://www.youtube.com/watch?v=IExopg1r4fw)    ### Reading    - [Weaviate is an open-source search engine powered by ML, vectors, graphs, and GraphQL (ZDNet)](https://www.zdnet.com/article/weaviate-an-open-source-search-engine-powered-by-machine-learning-vectors-graphs-and-graphql/)  - [Weaviate, an ANN Database with CRUD support (DB-Engines.com)](https://db-engines.com/en/blog_post/87)  - [A sub-50ms neural search with DistilBERT and Weaviate (Towards Datascience)](https://towardsdatascience.com/a-sub-50ms-neural-search-with-distilbert-and-weaviate-4857ae390154)  - [Getting Started with Weaviate Python Library (Towards Datascience)](https://towardsdatascience.com/getting-started-with-weaviate-python-client-e85d14f19e4f)    ## Examples    You can find [code examples here](https://github.com/semi-technologies/weaviate-examples)    ## Support    - [Stackoverflow for questions](https://stackoverflow.com/questions/tagged/weaviate)  - [Github for issues](https://github.com/semi-technologies/weaviate/issues)  - [Slack channel to connect](https://join.slack.com/t/weaviate/shared_invite/zt-goaoifjr-o8FuVz9b1HLzhlUfyfddhw)  - [Newsletter to stay in the know](http://weaviate-newsletter.semi.technology/)    ## Contributing    - [How to Contribute](https://weaviate.io/developers/contributor-guide/current/) """
Big data;https://github.com/plotly/dash;"""# Dash    [![CircleCI](https://img.shields.io/circleci/project/github/plotly/dash/master.svg)](https://circleci.com/gh/plotly/dash)  [![GitHub](https://img.shields.io/github/license/plotly/dash.svg?color=dark-green)](https://github.com/plotly/dash/blob/master/LICENSE)  [![PyPI](https://img.shields.io/pypi/v/dash.svg?color=dark-green)](https://pypi.org/project/dash/)  [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/dash.svg?color=dark-green)](https://pypi.org/project/dash/)  [![GitHub commit activity](https://img.shields.io/github/commit-activity/y/plotly/dash.svg?color=dark-green)](https://github.com/plotly/dash/graphs/contributors)  [![LGTM Alerts](https://img.shields.io/lgtm/alerts/g/plotly/dash.svg)](https://lgtm.com/projects/g/plotly/dash/alerts)  [![LGTM Grade](https://img.shields.io/lgtm/grade/python/g/plotly/dash.svg)](https://lgtm.com/projects/g/plotly/dash/context:python)    #### *Dash is the most downloaded, trusted Python framework for building ML & data science web apps*.    Built on top of [Plotly.js](https://github.com/plotly/plotly.js), [React](https://reactjs.org/) and [Flask](https://palletsprojects.com/p/flask/), Dash ties modern UI elements like dropdowns, sliders, and graphs directly to your analytical Python code. Read [our tutorial](https://dash.plotly.com/getting-started) (proudly crafted â¤ï¸ with Dash itself).    - [Docs](https://dash.plotly.com/getting-started): Create your first Dash app in under 5 minutes    - [dash.gallery](https://dash.gallery): Dash app gallery with Python & R code    ### Dash App Examples    | Dash App | Description |  |--- | :---: |  |![Sample Dash App](https://user-images.githubusercontent.com/1280389/30086128-9bb4a28e-9267-11e7-8fe4-bbac7d53f2b0.gif) | Hereâ€™s a simple example of a Dash App that ties a Dropdown to a Plotly Graph. As the user selects a value in the Dropdown, the application code dynamically exports data from Google Finance into a Pandas DataFrame. This app was written in just **43** lines of code ([view the source](https://gist.github.com/chriddyp/3d2454905d8f01886d651f207e2419f0)). |  |![Crossfiltering Dash App](https://user-images.githubusercontent.com/1280389/30086123-97c58bde-9267-11e7-98a0-7f626de5199a.gif)|Dash app code is declarative and reactive, which makes it easy to build complex apps that contain many interactive elements. Hereâ€™s an example with 5 inputs, 3 outputs, and cross filtering. This app was composed in just 160 lines of code, all of which were Python.|  |![Dash App with Mapbox map showing walmart store openings](https://user-images.githubusercontent.com/1280389/30086299-768509d0-9268-11e7-8e6b-626ac9ca512c.gif)| Dash uses [Plotly.js](https://github.com/plotly/plotly.js) for charting. About 50 chart types are supported, including maps. |  |![Financial report](https://github.com/plotly/dash-docs/blob/516f80c417051406210b94ea23a6d3b6cd84d146/assets/images/gallery/dash-financial-report.gif)| Dash isn't just for dashboards. You have full control over the look and feel of your applications. Here's a Dash App that's styled to look like a PDF report. |    To learn more about Dash, read the [extensive announcement letter](https://medium.com/@plotlygraphs/introducing-dash-5ecf7191b503) or [jump in with the user guide](https://plotly.com/dash).    ### Dash OSS & Dash Enterprise    With Dash Open Source, Dash apps run on your local laptop or workstation, but cannot be easily accessed by others in your organization.    Scale up with Dash Enterprise when your Dash app is ready for department or company-wide consumption. Or, launch your initiative with Dash Enterprise from the start to unlock developer productivity gains and hands-on acceleration from Plotly's team.    ML Ops Features: A one-stop shop for ML Ops: Horizontally scalable hosting, deployment, and authentication for your Dash apps. No IT or DevOps required.  - [**App manager**](https://plotly.com/dash/app-manager/) Deploy & manage Dash apps without needing IT or a DevOps team. App Manager gives you point & click control over all aspects of your Dash deployments.  - [**Kubernetes scaling**](https://plotly.com/dash/kubernetes/) Ensure high availability of Dash apps and scale horizontally with Dash Enterpriseâ€™s Kubernetes architecture. No IT or Helm required.  - [**No code auth**](https://plotly.com/dash/authentication/) Control Dash app access in a few clicks. Dash Enterprise supports LDAP, AD, PKI, Okta, SAML, OpenID Connect, OAuth, SSO, and simple email authentication.  - [**Job Queue**](https://plotly.com/dash/job-queue/) The Job Queue is the key to building scalable Dash apps. Move heavy computation from synchronous Dash callbacks to the Job Queue for asynchronous background processing.    Low-Code Features: Low-code Dash app capabilities that supercharge developer productivity.  - [**Design Kit**](https://plotly.com/dash/design-kit/) Design like a pro without writing a line of CSS. Easily arrange, style, brand, and customize your Dash apps.  - [**Snapshot Engine**](https://plotly.com/dash/snapshot-engine/) Save & share Dash app views as links or PDFs. Or, run a Python job through Dash and have Snapshot Engine email a report when the job is done.  - [**Dashboard Toolkit**](https://plotly.com/dash/toolkit/) Drag & drop layouts, chart editing, and crossfilter for your Dash apps.  - [**Embedding**](https://plotly.com/dash/embedding/) Natively embed Dash apps in an existing web application or website without the use of IFrames.    Enterprise AI Features: Everything that your data science team needs to rapidly deliver AI/ML research and business initiatives.  - [**AI App Marketplace**](https://plotly.com/dash/ai-and-ml-templates/) Dash Enterprise ships with dozens of Dash app templates for business problems where AI/ML is having the greatest impact.  - [**Big Data for Pything**](https://plotly.com/dash/big-data-for-python/) Connect to Python's most popular big data back ends: Dask, Databricks, NVIDIA RAPIDS, Snowflake, Postgres, Vaex, and more.  - [**GPU & Dask Acceleration**](https://plotly.com/dash/gpu-dask-acceleration/) Dash Enterprise puts Pythonâ€™s most popular HPC stack for GPU and parallel CPU computing in the hands of business users.  - [**Data Science Workspaces**](https://plotly.com/dash/workspaces/) Be productive from Day 1. Write and execute Python, R, & Julia code from Dash Enterprise's onboard code editor.    See [https://plotly.com/contact-us/](https://plotly.com/contact-us/) to get in touch.    ![image](https://images.prismic.io/plotly-marketing-website/493eec39-8467-4610-b9d0-d6ad3ea61423_Dash+Open+source%2BDash+enterprise2-01.jpg?auto=compress,format) """
Big data;https://github.com/cbd/edis;"""An *Erlang* version of [Redis](http://redis.io), with the goal of similar algorithmic performance but support for multiple master nodes and larger-than-RAM datasets. For [More info](http://inakanetworks.com/assets/pdf//Edis_Implementing_Redis_In_Erlang.pdf), see this PDF of a Talk at Erlang Factory 2012.    ## Contact Us  For **questions** or **general comments** regarding the use of this library, please use our public  [hipchat room](http://inaka.net/hipchat).    If you find any **bugs** or have a **problem** while using this library, please [open an issue](https://github.com/inaka/edis/issues/new) in this repo (or a pull request :)).    And you can check all of our open-source projects at [inaka.github.io](http://inaka.github.io)    ## Usage  Just run `$ make run` and open connections with your favourite redis client.    ## Differences with Redis  ### Different Behaviour  * _SAVE_, _BGSAVE_ and _LASTSAVE_ are database dependent. The original Redis saves all databases at once, edis saves just the one you _SELECT_'ed.  * _INFO_ provides much less information and no statistics (so, _CONFIG RESETSTAT_ does nothing at all)  * _MULTI_ doesn't support:    - cross-db commands (i.e. _FLUSHALL_, _SELECT_, _MOVE_)    - non-db commands (i.e _AUTH_, _CONFIG *_, _SHUTDOWN_, _MONITOR_)    - pub/sub commands (i.e. _PUBLISH_, _SUBSCRIBE_, _UNSUBSCRIBE_, _PSUBSCRIBE_, _PUNSUBSCRIBE_)  * _(P)UNSUBSCRIBE_ commands are not allowed outside _PUBSUB_ mode  * _PUBLISH_ response is not precise: it's the amount of all clients subscribed to any channel and/or pattern, not just those that will handle the message. On the other hand it runs in _O(1)_ because it's asynchronous, it just dispatches the message.    ### Missing Features  * Dynamic node configuration (i.e. the _SLAVEOF_ command is not implemented)  * Encoding optimization (i.e. all objects are encoded as binary representations of erlang terms, so for instance ""123"" will never be stored as an int)  * _OBJECT REFCOUNT_ allways returns 1 for existing keys and (nil) otherwise    ### Unsupported Commands  _SYNC_, _SLOWLOG_, _SLAVEOF_, _DEBUG *_    ### License  edis is licensed by Electronic Inaka, LLC under the Apache 2.0 license; see the LICENSE file in this repository.    ### TODO    * Backends    * HanoiDB      * Make use of the efficient range searchs with start/end for searching ranges      * Make use of time-based key expiry      * Finish the TODO items   """
Big data;https://github.com/google/leveldb;"""**LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values.**    [![ci](https://github.com/google/leveldb/actions/workflows/build.yml/badge.svg)](https://github.com/google/leveldb/actions/workflows/build.yml)    Authors: Sanjay Ghemawat (sanjay@google.com) and Jeff Dean (jeff@google.com)    # Features      * Keys and values are arbitrary byte arrays.    * Data is stored sorted by key.    * Callers can provide a custom comparison function to override the sort order.    * The basic operations are `Put(key,value)`, `Get(key)`, `Delete(key)`.    * Multiple changes can be made in one atomic batch.    * Users can create a transient snapshot to get a consistent view of data.    * Forward and backward iteration is supported over the data.    * Data is automatically compressed using the [Snappy compression library](https://google.github.io/snappy/).    * External activity (file system operations etc.) is relayed through a virtual interface so users can customize the operating system interactions.    # Documentation      [LevelDB library documentation](https://github.com/google/leveldb/blob/main/doc/index.md) is online and bundled with the source code.    # Limitations      * This is not a SQL database.  It does not have a relational data model, it does not support SQL queries, and it has no support for indexes.    * Only a single process (possibly multi-threaded) can access a particular database at a time.    * There is no client-server support builtin to the library.  An application that needs such support will have to wrap their own server around the library.    # Getting the Source    ```bash  git clone --recurse-submodules https://github.com/google/leveldb.git  ```    # Building    This project supports [CMake](https://cmake.org/) out of the box.    ### Build for POSIX    Quick start:    ```bash  mkdir -p build && cd build  cmake -DCMAKE_BUILD_TYPE=Release .. && cmake --build .  ```    ### Building for Windows    First generate the Visual Studio 2017 project/solution files:    ```cmd  mkdir build  cd build  cmake -G ""Visual Studio 15"" ..  ```  The default default will build for x86. For 64-bit run:    ```cmd  cmake -G ""Visual Studio 15 Win64"" ..  ```    To compile the Windows solution from the command-line:    ```cmd  devenv /build Debug leveldb.sln  ```    or open leveldb.sln in Visual Studio and build from within.    Please see the CMake documentation and `CMakeLists.txt` for more advanced usage.    # Contributing to the leveldb Project    The leveldb project welcomes contributions. leveldb's primary goal is to be  a reliable and fast key/value store. Changes that are in line with the  features/limitations outlined above, and meet the requirements below,  will be considered.    Contribution requirements:    1. **Tested platforms only**. We _generally_ will only accept changes for     platforms that are compiled and tested. This means POSIX (for Linux and     macOS) or Windows. Very small changes will sometimes be accepted, but     consider that more of an exception than the rule.    2. **Stable API**. We strive very hard to maintain a stable API. Changes that     require changes for projects using leveldb _might_ be rejected without     sufficient benefit to the project.    3. **Tests**: All changes must be accompanied by a new (or changed) test, or     a sufficient explanation as to why a new (or changed) test is not required.    4. **Consistent Style**: This project conforms to the     [Google C++ Style Guide](https://google.github.io/styleguide/cppguide.html).     To ensure your changes are properly formatted please run:       ```     clang-format -i --style=file <file>     ```    We are unlikely to accept contributions to the build configuration files, such  as `CMakeLists.txt`. We are focused on maintaining a build configuration that  allows us to test that the project works in a few supported configurations  inside Google. We are not currently interested in supporting other requirements,  such as different operating systems, compilers, or build systems.    ## Submitting a Pull Request    Before any pull request will be accepted the author must first sign a  Contributor License Agreement (CLA) at https://cla.developers.google.com/.    In order to keep the commit timeline linear  [squash](https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History#Squashing-Commits)  your changes down to a single commit and [rebase](https://git-scm.com/docs/git-rebase)  on google/leveldb/main. This keeps the commit timeline linear and more easily sync'ed  with the internal repository at Google. More information at GitHub's  [About Git rebase](https://help.github.com/articles/about-git-rebase/) page.    # Performance    Here is a performance report (with explanations) from the run of the  included db_bench program.  The results are somewhat noisy, but should  be enough to get a ballpark performance estimate.    ## Setup    We use a database with a million entries.  Each entry has a 16 byte  key, and a 100 byte value.  Values used by the benchmark compress to  about half their original size.        LevelDB:    version 1.1      Date:       Sun May  1 12:11:26 2011      CPU:        4 x Intel(R) Core(TM)2 Quad CPU    Q6600  @ 2.40GHz      CPUCache:   4096 KB      Keys:       16 bytes each      Values:     100 bytes each (50 bytes after compression)      Entries:    1000000      Raw Size:   110.6 MB (estimated)      File Size:  62.9 MB (estimated)    ## Write performance    The ""fill"" benchmarks create a brand new database, in either  sequential, or random order.  The ""fillsync"" benchmark flushes data  from the operating system to the disk after every operation; the other  write operations leave the data sitting in the operating system buffer  cache for a while.  The ""overwrite"" benchmark does random writes that  update existing keys in the database.        fillseq      :       1.765 micros/op;   62.7 MB/s      fillsync     :     268.409 micros/op;    0.4 MB/s (10000 ops)      fillrandom   :       2.460 micros/op;   45.0 MB/s      overwrite    :       2.380 micros/op;   46.5 MB/s    Each ""op"" above corresponds to a write of a single key/value pair.  I.e., a random write benchmark goes at approximately 400,000 writes per second.    Each ""fillsync"" operation costs much less (0.3 millisecond)  than a disk seek (typically 10 milliseconds).  We suspect that this is  because the hard disk itself is buffering the update in its memory and  responding before the data has been written to the platter.  This may  or may not be safe based on whether or not the hard disk has enough  power to save its memory in the event of a power failure.    ## Read performance    We list the performance of reading sequentially in both the forward  and reverse direction, and also the performance of a random lookup.  Note that the database created by the benchmark is quite small.  Therefore the report characterizes the performance of leveldb when the  working set fits in memory.  The cost of reading a piece of data that  is not present in the operating system buffer cache will be dominated  by the one or two disk seeks needed to fetch the data from disk.  Write performance will be mostly unaffected by whether or not the  working set fits in memory.        readrandom  : 16.677 micros/op;  (approximately 60,000 reads per second)      readseq     :  0.476 micros/op;  232.3 MB/s      readreverse :  0.724 micros/op;  152.9 MB/s    LevelDB compacts its underlying storage data in the background to  improve read performance.  The results listed above were done  immediately after a lot of random writes.  The results after  compactions (which are usually triggered automatically) are better.        readrandom  : 11.602 micros/op;  (approximately 85,000 reads per second)      readseq     :  0.423 micros/op;  261.8 MB/s      readreverse :  0.663 micros/op;  166.9 MB/s    Some of the high cost of reads comes from repeated decompression of blocks  read from disk.  If we supply enough cache to the leveldb so it can hold the  uncompressed blocks in memory, the read performance improves again:        readrandom  : 9.775 micros/op;  (approximately 100,000 reads per second before compaction)      readrandom  : 5.215 micros/op;  (approximately 190,000 reads per second after compaction)    ## Repository contents    See [doc/index.md](doc/index.md) for more explanation. See  [doc/impl.md](doc/impl.md) for a brief overview of the implementation.    The public interface is in include/leveldb/*.h.  Callers should not include or  rely on the details of any other header files in this package.  Those  internal APIs may be changed without warning.    Guide to header files:    * **include/leveldb/db.h**: Main interface to the DB: Start here.    * **include/leveldb/options.h**: Control over the behavior of an entire database,  and also control over the behavior of individual reads and writes.    * **include/leveldb/comparator.h**: Abstraction for user-specified comparison function.  If you want just bytewise comparison of keys, you can use the default  comparator, but clients can write their own comparator implementations if they  want custom ordering (e.g. to handle different character encodings, etc.).    * **include/leveldb/iterator.h**: Interface for iterating over data. You can get  an iterator from a DB object.    * **include/leveldb/write_batch.h**: Interface for atomically applying multiple  updates to a database.    * **include/leveldb/slice.h**: A simple module for maintaining a pointer and a  length into some other byte array.    * **include/leveldb/status.h**: Status is returned from many of the public interfaces  and is used to report success and various kinds of errors.    * **include/leveldb/env.h**:  Abstraction of the OS environment.  A posix implementation of this interface is  in util/env_posix.cc.    * **include/leveldb/table.h, include/leveldb/table_builder.h**: Lower-level modules that most  clients probably won't use directly. """
Big data;https://github.com/salesforce/Argus;"""Argus  [![Build Status](https://travis-ci.org/salesforce/Argus.svg?branch=master)](https://travis-ci.org/salesforce/Argus)  [![Static Analysis](https://scan.coverity.com/projects/8155/badge.svg)](https://scan.coverity.com/projects/salesforceeng-argus)  [![argus-sdk - Maven Central](https://maven-badges.herokuapp.com/maven-central/com.salesforce.argus/argus-sdk/badge.svg?maxAge=3600)](https://maven-badges.herokuapp.com/maven-central/com.salesforce.argus/argus-sdk)  =====    Argus is a time-series monitoring and alerting platform. It consists of discrete services to configure alerts, ingest and transform metrics & events, send notifications, create namespaces, and to both establish and enforce policies and quotas for usage.    Its architecture allows any and all of these services to be retargeted to new technology as it becomes available, with little to no impact on the users.    To find out more [see the wiki](https://github.com/salesforce/Argus/wiki) and check out the [release notes](https://github.com/salesforce/Argus/releases).    ![Argus UI](ArgusUIScreenshot.png ""Viewing metric with default UI"")    ## Building Argus    ### Installing The Resource Filters    Argus uses the `argus-build.properties` file as a resource filter for the build and all the module builds.  After you clone the project for the first time, or after you change this file, you must create and install the dependency jars which will contain these filters.  Those dependency jars are then pulled in by the modules, expanded and have their values applied to the module specific builds.  Luckily, it's a straightforward operation.  Just execute the following command from within the parent project, after you first clone the project or after you update the `argus-build.properties` file.    ```  mvn -DskipTests=true -DskipDockerBuild --non-recursive install  ```    ### Running The Unit Tests    Once the resource filters are installed, you can run unit tests.  Running the unit tests doesn't require any changes to the argus-build.properties file.  Just install the resource filters and execute the `test` goal.    ```  mvn test  ```    **Only the unit tests are run by `codecov.io` and as such, the coverage reported by it is significantly less than the coverage obtained by running the full test suite.**    ### Running The Integration Tests    The integration tests for Argus use the `LDAPAuthService` implementation of the `AuthService` interface and the `DefaultTSDBService` implementation of the `TSDBService` interface (which targets OpenTSDB).  Additionally it uses the `RedisCacheService` implementation of the `CacheService` interface to facilitate integration testing of the `BatchService`.  In order to run the integration tests you must update the `argus-build.properties` file to correctly setup the external LDAP you'll be testing against and the OpenTSDB endpoints to use as well as the Redis cluster.  The snippet below shows the specific properties that should be modified in the `argus-build.properties` file.  Of course, after you make these updates, you must re-install the resource filter dependencies as described above and execute the `clean` goal, before running the integration tests.    ```  # The LDAP endpoint to use  service.property.auth.ldap.endpoint=ldaps://ldaps.yourdomain.com:636  # A list of comma separated search paths used to query the DN of users attempting to authenticate.  # This example lists two separate search bases.  One for users and one for service accounts.  service.property.auth.ldap.searchbase=OU=active,OU=user,DC=yourdomain,DC=com:OU=active,OU=robot,DC=yourdomain,DC=com  # This specifies of the DN for the privileged user that is used to bind and subsequently execute the search for user DN's  service.property.auth.ldap.searchdn=CN=argus_admin,OU=active,OU=user,DC=yourdomain,DC=com  # The password for the privileged user above.  service.property.auth.ldap.searchpwd=Argu5R0cks!  # The LDAP field with which the username provided during a login attempt, will be matched.  # This is used so Argus can obtain the DN for the user attempting to login, and subsequently attempt to bind as that user.  service.property.auth.ldap.usernamefield=sAMAccountName  # The TSDB read endpoint  service.property.tsdb.endpoint.read=http://readtsdb.yourdomain.com:4466  # The TSDB write endpoint  service.property.tsdb.endpoint.write=http://writetsdb.yourdomain.com:4477  # The Redis cache cluster information  service.property.cache.redis.cluster=redis0.mycompany.com:6379,redis1.mycompany.com:6389  ```    Once the modifications have been made and the resource filters re-installed, you're ready to run the complete suite of tests, including the integration tests.    ```  mvn verify  ```    ### Generating Coverage Reports    Coverage is calculated everytime tests are run for all modules with the exception of ArgusWeb.  In order to generate a coverage report for a module, just `cd` into the module subdirectory and run the report generation target.    ```  mvn jacoco:report  ```    Coverage reports are generated in the `target/site/jacoco` directory.    ### Deploying & Running Argus    Please [see the wiki](https://github.com/salesforce/Argus/wiki) for information on how to deploy, configure and run Argus. """
Big data;https://github.com/LucidWorks/banana;"""# Banana    The Banana project was forked from [Kibana](https://github.com/elastic/kibana), and works with all kinds of time series  (and non-time series) data stored in [Apache Solr](https://lucene.apache.org/solr/). It uses Kibana's powerful dashboard  configuration capabilities, ports key panels to work with Solr, and provides significant additional capabilities,  including new panels that leverage [D3.js](http://d3js.org).    The goal is to create a rich and flexible UI, enabling users to rapidly develop end-to-end applications that leverage  the power of Apache Solr. Data can be ingested into Solr through a variety of ways, including  [Logstash](https://www.elastic.co/products/logstash), [Flume](https://flume.apache.org) and other connectors.     ## IMPORTANT    Pull the repo from the `release` branch for production deployment; version x.y.z will be tagged as x.y.z    `develop` branch is used for active development and cutting edge features.  `fusion` branch is used for Lucidworks Fusion release. The code base and features are the same as `develop`. The main difference  is in the configuration.     ## Banana 1.6.26    This release includes the following bug fixes and improvement:    1. Enhance heatmap      * Add axis and axis labels      * Add axis grid and ticks      * Add gradient legend and ranges      * Fix heatmap transpose icon      * Enhance positioning and padding of panel elements      * Fix bettermap tooltip and hint text  1. Enhance hits panel      * Add panel horizontal and vertical direction option      * Fix metrics text and label overlap and margins  1. Fix bettermap render issue when resized  1. Fix jshint warnings    ## Older Release Notes    You can find all previous [Release Notes](https://github.com/LucidWorks/banana/wiki/Release-Notes) on our wiki page.    ## Installation and Quick Start  ### Requirements  * A modern web browser. The latest version of [Chrome](http://www.google.com/chrome/) and  [Firefox](https://www.mozilla.org/en-US/firefox/new/) have been tested to work. [Safari](http://www.apple.com/safari/)  also works, except for the ""Export to File"" feature for saving dashboards. We recommend that you use Chrome or Firefox  while building dashboards.  * Solr 6.x or at least 4.4+ (Solr server's endpoint must be open, or a proxy configured to allow access to it).  * A webserver (optional).    ### Installation Options  #### Option 1: Run Banana webapp within your existing Solr instance  ##### Solr 5+ Instructions  1. Run Solr at least once to create the webapp directory (this step might be unnecessary for Solr 6):            cd $SOLR_HOME/bin          ./solr start    2. Copy banana folder to `$SOLR_HOME/server/solr-webapp/webapp/`            cd $SOLR_HOME/server/solr-webapp/webapp          cp -R $BANANA_HOME/src ./banana        NOTES: For production, you should run `grunt build` command to generate the optimized code in `dist` directory. And then copy the `dist` directory to the production web server. For example:            cd $BANANA_HOME          npm install          bower install          grunt build          cp -R ./dist $SOLR_HOME/server/solr-webapp/webapp/banana    3. Browse to [http://localhost:8983/solr/banana/index.html](http://localhost:8983/solr/banana/index.html)    ##### Solr 4 Instructions  1. Run Solr at least once to create the webapp directories:            cd $SOLR_HOME/example          java -jar start.jar        2. Copy banana folder to $SOLR_HOME/example/solr-webapp/webapp/  3. Browse to [http://localhost:8983/solr/banana/src/index.html](http://localhost:8983/solr/banana/src/index.html)    _**NOTES:**_ If your Solr server/port is different from [localhost:8983](http://localhost:8983), edit  banana/src/config.js and banana/src/app/dashboards/default.json to enter the hostname and port that you are using.  Remember that banana runs within the client browser, so provide a fully qualified domain name (FQDN), because the  hostname and port number you provide should be resolvable from the client machines.    If you have not created the data collections and ingested data into Solr, you will see an error message saying  ""Collection not found at .."" You can use any connector to get data into Solr. If you want to use Logstash, please go to  the Solr Output Plug-in for [Logstash Page](https://github.com/LucidWorks/solrlogmanager) for code, documentation and  examples.    #### Option 2: Complete SLK Stack  Lucidworks has packaged Solr, Logstash (with a Solr Output Plug-in), and Banana (the Solr port of Kibana), along with  example collections and dashboards in order to rapidly enable proof-of-concepts and initial development/testing.  See [http://www.lucidworks.com/lucidworks-silk/](http://www.lucidworks.com/lucidworks-silk/).    #### Option 3: Building and installing from a WAR file  _NOTES: This option is only applicable to Solr 5 or 4. Solr 6 has a different architecture._  1. Pull the source code of Banana version that you want from the  [release](https://github.com/LucidWorks/banana/tree/release) branch in the repo;  For example, version *x.y.z* will be tagged as `x.y.z`.    2. Run a command line `ant` from within the banana directory to build the war file:        ```bash          cd $BANANA_HOME          ant      ```  3. The war file will be called *banana-\<buildnumber\>.war* and will be located in $BANANA_HOME/build.  Copy the war file and banana's jetty context file to Solr directories:    * For Solr 5:        ```bash          cp $BANANA_HOME/build/banana-<buildnumber>.war $SOLR_HOME/server/webapps/banana.war          cp $BANANA_HOME/jetty-contexts/banana-context.xml $SOLR_HOME/server/contexts/      ```    * For Solr 4:        ```bash          cp $BANANA_HOME/build/banana-<buildnumber>.war $SOLR_HOME/example/webapps/banana.war          cp $BANANA_HOME/jetty-contexts/banana-context.xml $SOLR_HOME/example/contexts/      ```  4. Run Solr:    * For Solr 5:        ```bash          cd $SOLR_HOME/bin/          ./solr start      ```    * For Solr 4:        ```bash          cd $SOLR_HOME/example/          java -jar start.jar      ```  5. Browse to [http://localhost:8983/banana](http://localhost:8983/banana) (or the FQDN of your Solr server).        #### Option 4: Run Banana webapp in a web server  Banana is an [AngularJS app](https://angularjs.org) and can be run in any webserver that has access to Solr.  You will need to enable [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) on the Solr instances that  you query, or configure a proxy that makes requests to banana and Solr as same-origin. We typically recommend the  latter approach.    ### Storing Dashboards in Solr  If you want to save and load dashboards from Solr, then you need to create a collection called `banana-int` first. For Solr 6, here are the steps:            cd $SOLR_HOME/bin          ./solr create -c banana-int    For Solr 5 and 4, you have to create the `banana-int` collection using the configuration files provided in either  the _resources/banana-int-solr-5.0_ (for Solr 5) directory or the _resources/banana-int-solr-4.5_ directory  (for Solr 4.5). If you are using SolrCloud, you will need to upload the configuration into  [ZooKeeper](https://zookeeper.apache.org) and then create the collection using that configuration.    The Solr server configured in config.js will serve as the default node for each dashboard; you can configure each  dashboard to point to a different Solr endpoint as long as your webserver and Solr put out the correct CORS headers.  See the README file under the  _resources/enable-cors_ directory for a guide.    ### Changes to your dashboards  If you created dashboards for Banana 1.0.0, you did not have a global filtering panel. In some cases, these filter  values can be implicitly set to defaults that may lead to strange search results. We recommend updating your old  dashboards by adding a filtering panel. A good way to do it visually is to put the filtering panel on its own row and  hide it when it is not needed.    ## FAQ    __Q__: How do I secure my Solr endpoint so that users do not have access to it?    __A__: The simplest solution is to use an [Apache](http://projects.apache.org/projects/http_server.html) or  [nginx](http://nginx.org) reverse proxy (See for example https://groups.google.com/forum/#!topic/ajax-solr/pLtYfm83I98).    __Q__: Can I use banana for non-time series data?    __A__: Yes, from version 1.3 onwards, non-time series data are also supported.    ## Resources    1.	Lucidworks SILK: http://www.lucidworks.com/lucidworks-silk/  2.	Webinar on Lucidworks SILK: http://programs.lucidworks.com/SiLK-introduction_Register.html.  3.	Logstash: http://logstash.net/  4.	SILK Use Cases: https://github.com/LucidWorks/silkusecases. Provides example configuration files, schemas and  dashboards required to build applications that use Solr and Banana.    ## Publishing WAR Artifacts to Maven Central    1. 	Get hold of  [maven-ant-tasks-X.X.X.jar](http://search.maven.org/#search|gav|1|g%3A%22org.apache.maven%22%20AND%20a%3A%22maven-ant-tasks%22)  and put it in this directory  2. 	Execute *ant -lib . deploy* from this directory, this will sign the Maven artifacts (currently just .war) and send  them to a [Sonatype OSSRH](https://oss.sonatype.org/) staging repository. Details of how to set this up can be found  [here](http://central.sonatype.org/pages/ossrh-guide.html). N.B. Ensure that you have an *release* profile contained  within ~/.m2/settings.xml  3.	Once you've read, and are happy with the staging repos, close it.     ## Support    Banana uses the dashboard configuration capabilities of Kibana (from which it is forked) and ports key panels to work  with Solr. Moreover, it provides many additional capabilities like heatmaps, range facets, panel specific filters,  global parameters, and visualization of ""group-by"" style queries. We are continuing to add many new panels that go well  beyond what is available in Kibana, helping users build complete applications that leverage the data stored in  Apache Solr, HDFS and a variety of sources in the enterprise.    If you have any questions, please email banana-support@lucidworks.com    ## Trademarks    Kibana is a trademark of Elasticsearch BV    Logstash is a trademark of Elasticsearch BV """
Big data;https://github.com/getredash/redash;"""<p align=""center"">    <img title=""Redash"" src='https://redash.io/assets/images/logo.png' width=""200px""/>  </p>    [![Documentation](https://img.shields.io/badge/docs-redash.io/help-brightgreen.svg)](https://redash.io/help/)  [![Datree](https://s3.amazonaws.com/catalog.static.datree.io/datree-badge-20px.svg)](https://datree.io/?src=badge)  [![Build Status](https://circleci.com/gh/getredash/redash.png?style=shield&circle-token=8a695aa5ec2cbfa89b48c275aea298318016f040)](https://circleci.com/gh/getredash/redash/tree/master)    Redash is designed to enable anyone, regardless of the level of technical sophistication, to harness the power of data big and small. SQL users leverage Redash to explore, query, visualize, and share data from any data sources. Their work in turn enables anybody in their organization to use the data. Every day, millions of users at thousands of organizations around the world use Redash to develop insights and make data-driven decisions.    Redash features:    1. **Browser-based**: Everything in your browser, with a shareable URL.  2. **Ease-of-use**: Become immediately productive with data without the need to master complex software.  3. **Query editor**: Quickly compose SQL and NoSQL queries with a schema browser and auto-complete.  4. **Visualization and dashboards**: Create [beautiful visualizations](https://redash.io/help/user-guide/visualizations/visualization-types) with drag and drop, and combine them into a single dashboard.  5. **Sharing**: Collaborate easily by sharing visualizations and their associated queries, enabling peer review of reports and queries.  6. **Schedule refreshes**: Automatically update your charts and dashboards at regular intervals you define.  7. **Alerts**: Define conditions and be alerted instantly when your data changes.  8. **REST API**: Everything that can be done in the UI is also available through REST API.  9. **Broad support for data sources**: Extensible data source API with native support for a long list of common databases and platforms.    <img src=""https://raw.githubusercontent.com/getredash/website/8e820cd02c73a8ddf4f946a9d293c54fd3fb08b9/website/_assets/images/redash-anim.gif"" width=""80%""/>    ## Getting Started    * [Setting up Redash instance](https://redash.io/help/open-source/setup) (includes links to ready-made AWS/GCE images).  * [Documentation](https://redash.io/help/).    ## Supported Data Sources    Redash supports more than 35 SQL and NoSQL [data sources](https://redash.io/help/data-sources/supported-data-sources). It can also be extended to support more. Below is a list of built-in sources:    - Amazon Athena  - Amazon DynamoDB  - Amazon Redshift  - Axibase Time Series Database  - Cassandra  - ClickHouse  - CockroachDB  - CSV  - Databricks (Apache Spark)  - DB2 by IBM  - Druid  - Elasticsearch  - Firebolt  - Google Analytics  - Google BigQuery  - Google Spreadsheets  - Graphite  - Greenplum  - Hive  - Impala  - InfluxDB  - JIRA  - JSON  - Apache Kylin  - OmniSciDB (Formerly MapD)  - MemSQL  - Microsoft Azure Data Warehouse / Synapse  - Microsoft Azure SQL Database  - Microsoft SQL Server  - MongoDB  - MySQL  - Oracle  - PostgreSQL  - Presto  - Prometheus  - Python  - Qubole  - Rockset  - Salesforce  - ScyllaDB  - Shell Scripts  - Snowflake  - SQLite  - TiDB  - TreasureData  - Vertica  - Yandex AppMetrrica  - Yandex Metrica    ## Getting Help    * Issues: https://github.com/getredash/redash/issues  * Discussion Forum: https://discuss.redash.io/    ## Reporting Bugs and Contributing Code    * Want to report a bug or request a feature? Please open [an issue](https://github.com/getredash/redash/issues/new).  * Want to help us build **_Redash_**? Fork the project, edit in a [dev environment](https://redash.io/help-onpremise/dev/guide.html) and make a pull request. We need all the help we can get!    ## Security    Please email security@redash.io to report any security vulnerabilities. We will acknowledge receipt of your vulnerability and strive to send you regular updates about our progress. If you're curious about the status of your disclosure please feel free to email us again. If you want to encrypt your disclosure email, you can use [this PGP key](https://keybase.io/arikfr/key.asc).    ## License    BSD-2-Clause. """
Big data;https://github.com/CSNW/d3.compose;"""# d3.compose    Compose rich, data-bound charts from charts (like Lines and Bars) and components (like Axis, Title, and Legend) with d3 and d3.chart.    - Advanced layout engine automatically positions and sizes charts and components, layers by z-index, and is responsive by default with automatic scaling  - Standard library of charts and components for quickly creating beautiful charts  - `Chart` and `Component` bases for creating composable and reusable charts and components  - Includes helpers and mixins that cover a range of standard functionality  - CSS class-based styling is extensible and easy to customize to match your site    [![npm version](https://img.shields.io/npm/v/d3.compose.svg?style=flat-square)](https://www.npmjs.com/package/d3.compose)    ## Getting Started    1. Download the [latest release](https://github.com/CSNW/d3.compose/releases)  2. Download the dependencies:        - [D3.js (>= 3.0.0)](http://d3js.org/)      - [d3.chart (>= 0.2.0)](http://misoproject.com/d3-chart/)    3. Add d3.compose and dependencies to your html:        ```html      <!doctype html>      <html>        <head>          <!-- ... -->            <link rel=""stylesheet"" type=""text/css"" href=""d3.compose.css"">        </head>        <body>          <!-- ... -->            <script src=""d3.js""></script>          <script src=""d3.chart.js""></script>            <script src=""d3.compose.js""></script>            <!-- Your code -->        </body>      </html>      ```    4. Create your first chart        ```js      var chart = d3.select('#chart')        .chart('Compose', function(data) {          var scales = {            x: {type: 'ordinal', data: data, key: 'x'},            y: {data: data, key: 'y'}          };            var charts = [            d3c.lines({              data: data,              xScale: scales.x,              yScale: scales.y            })          ];            var yAxis = d3c.axis({scale: scales.y});            return [            [yAxis, d3c.layered(charts)]          ];        })        .width(600)        .height(400);        chart.draw([{x: 0, y: 10}, {x: 10, y: 50}, {x: 20, y: 30}]);      ```    ## Examples and Docs    See [http://CSNW.github.io/d3.compose/](http://CSNW.github.io/d3.compose/) for live examples and docs.    ## Development    1. Install modules `npm install`  2. Test with `npm test` or `npm run test:watch`  3. Build with `npm run build`    Note on testing: Requires Node 4+ (for latest jsdom) and d3.chart doesn't currently support running from within node  and requires the following line be added inside the IIFE in `node_modules/d3.chart.js`: `window = this;` (before `use strict`). This will be resolved by a [pending PR](https://github.com/misoproject/d3.chart/pull/113) to fix this issue with d3.chart (also, the dependency on d3.chart is likely to be removed in a later version of d3.compose).    ### Release    (With all changes merged to master and on master branch)    1. `npm version {patch|minor|major|version}`  2. `npm publish`    ### Docs    1. On master, run `npm run docs`  2. Switch to `gh-pages` branch  3. Navigate to `_tasks` directory (`cd _tasks`)  4. (`npm install` _tasks, if necessary)  5. Run docs task `npm run docs`  6. Navigate back to root  7. View site with `bundle exec jekyll serve`    Note: For faster iteration, create a separate clone, switch to `gh-pages` branch, set `docs_path` environment variable to original clone (e.g. Windows: `SET docs_path=C:\...\d3.compose\_docs\`), and then run steps 3-6. """
Big data;https://github.com/polyaxon/polyaxon;"""[![License: Apache 2](https://img.shields.io/badge/License-apache2-blue.svg?style=flat&longCache=true)](LICENSE)  [![Polyaxon API](https://img.shields.io/docker/pulls/polyaxon/polyaxon-api)](https://hub.docker.com/r/polyaxon/polyaxon-api)  [![Slack](https://img.shields.io/badge/Slack-1.4k%20members-blue.svg?style=flat&logo=slack&longCache=true)](https://polyaxon.com/slack/)    [![Docs](https://img.shields.io/badge/docs-stable-brightgreen.svg?style=flat&longCache=true)](https://polyaxon.com/docs/)  [![Release](https://img.shields.io/badge/release-v1.16.1-brightgreen.svg?longCache=true)](https://polyaxon.com/docs/releases/1-16/)  [![GitHub](https://img.shields.io/badge/issue_tracker-github-blue?style=flat&logo=github&longCache=true)](https://github.com/polyaxon/polyaxon/issues)  [![GitHub](https://img.shields.io/badge/roadmap-github-blue?style=flat&logo=github&longCache=true)](https://github.com/orgs/polyaxon/projects/5)    [![CLI](https://github.com/polyaxon/polyaxon/actions/workflows/core.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/core.yml)  [![Traceml](https://github.com/polyaxon/polyaxon/actions/workflows/traceml.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/traceml.yml)  [![Datatile](https://github.com/polyaxon/polyaxon/actions/workflows/datatile.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/datatile.yml)  [![Platform](https://github.com/polyaxon/polyaxon/actions/workflows/platform.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/platform.yml)  [![Codacy Badge](https://api.codacy.com/project/badge/Grade/90c05b6b112548c1a88b950beceacb69)](https://www.codacy.com/app/polyaxon/polyaxon?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=polyaxon/polyaxon&amp;utm_campaign=Badge_Grade)    <br>  <p align=""center"">    <p align=""center"">      <a href=""https://polyaxon.com/?utm_source=github&utm_medium=logo"" target=""_blank"">        <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/logo/vector/primary-white-default-monochrome.svg"" alt=""polyaxon"" height=""100"">      </a>    </p>    <p align=""center"">      Reproduce, Automate, Scale your data science.    </p>  </p>  <br>      Welcome to Polyaxon, a platform for building, training, and monitoring large scale deep learning applications.  We are making a system to solve reproducibility, automation, and scalability for machine learning applications.    Polyaxon deploys into any data center, cloud provider, or can be hosted and managed by Polyaxon, and it supports all the major deep learning frameworks such as Tensorflow, MXNet, Caffe, Torch, etc.    Polyaxon makes it faster, easier, and more efficient to develop deep learning applications by managing workloads with smart container and node management. And it turns GPU servers into shared, self-service resources for your team or organization.    <br>  <p align=""center"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/demo.gif"" alt=""demo"" width=""80%"">  </p>  <br>    # Install    #### TL;DR;    * Install CLI        ```bash      # Install Polyaxon CLI      $ pip install -U polyaxon      ```     * Create a deployment        ```bash      # Create a namespace      $ kubectl create namespace polyaxon        # Add Polyaxon charts repo      $ helm repo add polyaxon https://charts.polyaxon.com        # Deploy Polyaxon      $ polyaxon admin deploy -f config.yaml        # Access API      $ polyaxon port-forward      ```    Please check [polyaxon installation guide](https://polyaxon.com/docs/setup/)    # Quick start    #### TL;DR;     * Start a project        ```bash      # Create a project      $ polyaxon project create --name=quick-start --description='Polyaxon quick start.'      ```     * Train and track logs & resources        ```bash      # Upload code and start experiments      $ polyaxon run -f experiment.yaml -u -l      ```     * Dashboard        ```bash      # Start Polyaxon dashboard      $ polyaxon dashboard        Dashboard page will now open in your browser. Continue? [Y/n]: y      ```    <br>  <p align=""center"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/compare.png"" alt=""compare"" width=""400"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/dashboards.png"" alt=""dashboards"" width=""400"">  </p>  <br>     * Notebook      ```bash      # Start Jupyter notebook for your project      $ polyaxon run --hub notebook      ```    <br>  <p align=""center"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/notebook.png"" alt=""compare"" width=""400"">  </p>  <br>     * Tensorboard      ```bash      # Start TensorBoard for a run's output      $ polyaxon run --hub tensorboard -P uuid=UUID      ```    <br>  <p align=""center"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/tensorboard.png"" alt=""tensorboard"" width=""400"">  </p>  <br>    Please check our [quick start guide](https://polyaxon.com/docs/intro/quick-start/) to start training your first experiment.    # Distributed job    Polyaxon supports and simplifies distributed jobs.  Depending on the framework you are using, you need to deploy the corresponding operator, adapt your code to enable the distributed training,  and update your polyaxonfile.    Here are some examples of using distributed training:      * [Distributed Tensorflow](https://polyaxon.com/docs/experimentation/distributed/tf-jobs/)   * [Distributed Pytorch](https://polyaxon.com/docs/experimentation/distributed/pytorch-jobs/)   * [Distributed MPI](https://polyaxon.com/docs/experimentation/distributed/mpi-jobs/)   * [Horovod](https://polyaxon.com/integrations/horovod/)   * [Spark](https://polyaxon.com/docs/experimentation/distributed/spark-jobs/)   * [Dask](https://polyaxon.com/docs/experimentation/distributed/dask-jobs/)    # Hyperparameters tuning    Polyaxon has a concept for suggesting hyperparameters and managing their results very similar to Google Vizier called experiment groups.  An experiment group in Polyaxon defines a search algorithm, a search space, and a model to train.     * [Grid search](https://polyaxon.com/docs/automation/optimization-engine/grid-search/)   * [Random search](https://polyaxon.com/docs/automation/optimization-engine/random-search/)   * [Hyperband](https://polyaxon.com/docs/automation/optimization-engine/hyperband/)   * [Bayesian Optimization](https://polyaxon.com/docs/automation/optimization-engine/bayesian-optimization/)   * [Hyperopt](https://polyaxon.com/docs/automation/optimization-engine/hyperopt/)   * [Custom Iterative Optimization](https://polyaxon.com/docs/automation/optimization-engine/iterative/)    # Parallel executions    You can run your processing or model training jobs in parallel, Polyaxon provides a [mapping](https://polyaxon.com/docs/automation/mapping/) abstraction to manage concurrent jobs.    # DAGs and workflows    [Polyaxon DAGs](https://polyaxon.com/docs/automation/flow-engine/) is a tool that provides container-native engine for running machine learning pipelines.   A DAG manages multiple operations with dependencies. Each operation is defined by a component runtime.   This means that operations in a DAG can be jobs, services, distributed jobs, parallel executions, or nested DAGs.       # Architecture    ![Polyaxon architecture](artifacts/polyaxon_architecture.png)    # Documentation    Check out our [documentation](https://polyaxon.com/docs/) to learn more about Polyaxon.    # Dashboard    Polyaxon comes with a dashboard that shows the projects and experiments created by you and your team members.    To start the dashboard, just run the following command in your terminal    ```bash  $ polyaxon dashboard -y  ```    # Project status    Polyaxon is stable and it's running in production mode at many startups and Fortune 500 companies.     # Contributions    Please follow the contribution guide line: *[Contribute to Polyaxon](CONTRIBUTING.md)*.      # Research    If you use Polyaxon in your academic research, we would be grateful if you could cite it.    Feel free to [contact us](mailto:contact@polyaxon.com), we would love to learn about your project and see how we can support your custom need. """
Big data;https://github.com/bloomberg/comdb2;"""## Overview    Comdb2 is a clustered RDBMS built on Optimistic Concurrency Control techniques.   It provides multiple isolation levels, including Snapshot and Serializable Isolation.   Read/Write transactions run on any node, with the client library transparently negotiating connections to lowest cost (latency) node which is available.  The client library provides transparent reconnect.    Work on Comdb2 was started at Bloomberg LP in 2004 and it has been under heavy development since.  More information about the architecture of the project can be found in our [VLDB 2016 paper](http://www.vldb.org/pvldb/vol9/p1377-scotti.pdf) and for more information on usage please look in the [Docs](https://bloomberg.github.io/comdb2/overview_home.html).    ## Documentation    [Comdb2 documentation](http://bloomberg.github.io/comdb2) is included in the `docs` directory.   It can be hosted locally with jekyll by running `jekyll serve` from the `docs` directory.    ## Contributing    Please refer to our [contribution guide](https://bloomberg.github.io/comdb2/contrib.html) for instructions.  We welcome code and idea contributions.    ## Quick Start    On every machine in the cluster:    1. Make sure all machines in the cluster can talk to each other via ssh.       Copy keys around if needed.      2. Install prerequisites:           **Debian/Ubuntu**               ```     sudo apt-get install -y  \         bison                \         build-essential      \         cmake                \         flex                 \         libevent-dev         \         liblz4-dev           \         libprotobuf-c-dev    \         libreadline-dev      \         libsqlite3-dev       \         libssl-dev           \         libunwind-dev        \         ncurses-dev          \         protobuf-c-compiler  \         tcl                  \         uuid-dev             \         zlib1g-dev     ```       **CentOS 7/8**       On CentOS 8, enable the PowerTools repository first:       ```     dnf config-manager --set-enabled powertools     ```       ```     yum install -y       \         byacc            \         cmake            \         epel-release     \         flex             \         gawk             \         gcc              \         gcc-c++          \         libevent-devel   \         libunwind        \         libunwind-devel  \         libuuid          \         libuuid-devel    \         lz4              \         lz4-devel        \         make             \         openssl          \         openssl-devel    \         openssl-libs     \         protobuf-c       \         protobuf-c-devel \         readline-devel   \         rpm-build        \         sqlite           \         sqlite-devel     \         tcl              \         which            \         zlib             \         zlib-devel     ```       **macOS High Sierra (experimental)**       Install Xcode and Homebrew. Then install required libraries:       ```     brew install cmake lz4 openssl protobuf-c readline libevent     ```       To run tests, install following:       ```     brew install coreutils bash gawk jq     export PATH=""/usr/local/opt/coreutils/libexec/gnubin:$PATH""     ```       It is recommended to increase the open file limits, at least in the sessions which start pmux and server.      3. Build and Install Comdb2:       ```     mkdir build && cd build && cmake .. && make && sudo make install     ```    4. Add */opt/bb/bin* to your PATH       ```     export PATH=$PATH:/opt/bb/bin     ```    5. Start pmux:       ```     pmux -n     ```    6. _(optional)_ Comdb2 nodes identify each other by their hostnames.  If the hostname      of each node isn't resolvable from other nodes, we should tell Comdb2 the full      domain name to use for the current node.  Most setups won't have this issue.       Tell comdb2 our FQDN.     ```bash     vi /opt/bb/etc/cdb2/config/comdb2.d/hostname.lrl     add current machine's name, e.g.     hostname machine-1.comdb2.example.com     ```    7. On one machine (say machine-1), create a database - this example creates a database      called _testdb_ stored in _~/db_.       ```     comdb2 --create --dir ~/db testdb     ```          Note: the `--dir PATH` parameter is optional, and if it is omitted comdb2 uses a default root of */opt/bb/var/cdb2/* for creating a database directory to contain the database files, which is named as per the database name parameter; hence in this case  */opt/bb/var/cdb2/testdb*.       The default root will have to be created explicitly with the desired permissions before invoking `comdb2 --create` for a database.       In this quick start, we use the home directory to avoid obfuscating the key steps of the process.             8. Configure the nodes in the cluster:     ```     vi ~/db/testdb.lrl     add     cluster nodes machine-1.comdb2.example.com machine-2.comdb2.example.com     ```       9. On other nodes, copy the database over:     ```     copycomdb2 mptest-1.comdb2.example.com:${HOME}/db/testdb.lrl     ```       0. On all nodes, start the database.     ```     comdb2 --lrl ~/db/testdb.lrl testdb     ```     All nodes will say 'I AM READY.' when ready.          Note: the log dir comdb2 uses by default is */opt/bb/var/log/cdb2/*      If this directory does not have permissions allowing the user to create file, there will be diagnostics output such as:       > [ERROR] error opening '/opt/bb/var/log/cdb2/testdb.longreqs' for logging: 13 Permission denied            This condition will not impact operation of the database for the purposes of this quick start.     Â      1. On any node, start using the database.  You don't have any tables yet.  You can add them with *cdb2sql*      Example -     ```sql     cdb2sql testdb local 'CREATE TABLE t1 {          schema {              int a          }     }'     ```       Database can be queried/updated with cdb2sql:     ```sql     cdb2sql testdb local 'insert into t1(a) values(1)'     (rows inserted=1)     cdb2sql testdb local 'select * from t1'     (a=1)     ```    ## Comdb2 Directory Contents    | Directory | Description |  | --- | --- |  | bbinc/        | Header & Generic include files |  | bdb/          | Table layer |  | berkdb/       | Btrees layer |  | cdb2api/      | Client code |  | cdb2jdbc/     | JDBC driver |  | cmake/        | cmake configuration files |  | comdb2rle/    | Run length encoding |  | contrib/      | Misc useful programs that aren't part of core Comdb2 |  | crc32c/       | Checksum component |  | csc2/         | csc2 processing |  | csc2files/    | csc2 config files |  | cson/         | JSON library |  | datetime/     | Datetime component |  | db/           | Types layer and overall glue |  | dfp/          | Decimal number component |  | dlmalloc/     | Local malloc version |  | docs/         | Documentation |  | lua/          | All things pertaining to lua VM used for stored procedures |  | mem/          | Memory accounting subsystem |  | net/          | Network component |  | pkg/          | deb and rpm packaging rules |  | plugin/       | Plugin subsystem |  | protobuf/     | API to communicate with the server |  | schemachange/ | Code for table create/alter/truncate/etc |  | sockpool/     | sockpool related files  |  | sqlite/       | Sqlite VM SQL engine  |  | tcl/          | Tcl language bindings |  | tests/        | Comdb2 test suite |  | tools/        | Tools that are part of Comdb2 core |  | util/         | Useful generic modules |   """
Big data;https://github.com/mozilla-services/heka;"""# Heka    Data Acquisition and Processing Made Easy    Heka is a tool for collecting and collating data from a number of different  sources, performing ""in-flight"" processing of collected data, and delivering  the results to any number of destinations for further analysis.    Heka is written in [Go](http://golang.org/), but Heka plugins can be written  in either Go or [Lua](http://lua.org). The easiest way to compile Heka is by  sourcing (see below) the build script in the root directory of the project,  which will set up a Go environment, verify the prerequisites, and install all  required dependencies. The build process also provides a mechanism for easily  integrating external plug-in packages into the generated `hekad`. For more  details and additional installation options see  [Installing](https://hekad.readthedocs.org/en/latest/installing.html).    WARNING: YOU MUST *SOURCE* THE BUILD SCRIPT (i.e. `source build.sh`) TO           BUILD HEKA. Setting up the Go build environment requires changes to           the shell environment, if you simply execute the script (i.e.           `./build.sh`) these changes will not be made.             Resources:  * Heka project docs: http://hekad.readthedocs.org/  * GoDoc package docs: http://godoc.org/github.com/mozilla-services/heka  * Mailing list: https://mail.mozilla.org/listinfo/heka  * IRC: #heka on irc.mozilla.org """
Big data;https://github.com/Microsoft/GraphEngine;"""# Graph Engine - Open Source    | - | Windows Multi Targeting | Ubuntu 16.04 .NET Core |  |:------:|:------:|:------:|  |Build|[<img src=""https://trinitygraphengine.visualstudio.com/_apis/public/build/definitions/4cfbb293-cd2c-4f49-aa03-06894081c93b/3/badge""/>](https://trinitygraphengine.visualstudio.com/trinity-ci/_build/index?definitionId=3)|[<img src=""https://trinitygraphengine.visualstudio.com/_apis/public/build/definitions/4cfbb293-cd2c-4f49-aa03-06894081c93b/4/badge""/>](https://trinitygraphengine.visualstudio.com/trinity-ci/_build/index?definitionId=4)|  |Tests|_|_|  |Stress|_|_|    This repository contains the source code of [Graph Engine][graph-engine] and its graph  query language -- [Language Integrated Knowledge Query][likq] (LIKQ).    Microsoft Graph Engine is a distributed  in-memory data processing engine, underpinned by a strongly-typed  in-memory key-value store and a general-purpose distributed computation  engine.    [LIKQ][likq-gh]  is a versatile graph query language atop Graph Engine. It  combines the capability of fast graph exploration with the flexibility  of lambda expression. Server-side computations can be expressed in  lambda expressions, embedded in LIKQ, and executed on Graph Engine servers during graph traversal.    ## How to contribute    If you are interested in contributing to Graph Engine, please fork the  repository and submit pull requests to the `master` branch.    Pull requests, issue reports, and suggestions are welcome.    Please submit bugs and feature requests as [GitHub Issues](https://github.com/Microsoft/GraphEngine/issues).    ## Getting started with Graph Engine    ### NuGet packages and Visual Studio extension    NuGet packages [Graph Engine Core][graph-engine-core] and [LIKQ][likq-nuget] are available in the NuGet Gallery.    If you develop Graph Engine applications using [Visual Studio][vs] on Windows, [Graph Engine VSExtension][vs-extension] can be used to facilitate the development work.    ### Building on Windows    Install [Visual Studio 2017 or 2019][vs] with the following components selected:    - .NET desktop development      - .NET Framework 4 -- 4.6 development tools  - Desktop development with C++      - Windows 10 SDK      - Windows 8.1 SDK and UCRT SDK  - Visual Studio extension development  - .NET Core SDK 3.1  - cmake    [.NET Core SDK][dotnet-download] and [cmake][cmake-download] can alternatively be installed using their standalone installers.    The Windows build will generate multi-targeting nuget packages.  Open a powershell window, run `tools/build.ps1` for Visual Studio 2017 or `tools/build.ps1 -VS2019` for Visual Studio 2019.    The Linux native assemblies will also be packaged (pre-built at `lib`) to allow the Windows build to work for Linux `.Net Core` as well.    ### Building on Linux    Install `libunwind8`, `g++`, `cmake` and `libssl-dev`. For example, run `sudo apt install libunwind8 g++ cmake libssl-dev` for Ubuntu.    Install [.NET Core 3.1][dotnet-download] and execute `bash tools/build.sh`.    The Windows native assemblies will also be packaged so that the  Linux build will work for Windows `.Net Core` as well.    **Note:** Because `.Net Framework` is Windows-only, the packages built on Linux only support `.Net Core`. The build script is tested only on `Ubuntu 16.04`, `Ubuntu 18.04`, and `Ubuntu 20.04`.    ### How to use the built Graph Engine packages    Nuget packages will be built as  `build/GraphEngine**._version_.nupkg`. The folder `build/` will be  registered as a local NuGet repository and the local package cache for  `GraphEngine.Core` will be cleared. After the packages are built, run `dotnet restore` to use the newly built package.    ### Run your first Graph Engine app    Go to the `samples/Friends/Friends` folder, execute `dotnet restore` and `dotnet run` to run the sample project.    ## License    Copyright (c) Microsoft Corporation. All rights reserved.    Licensed under the [MIT][license] License.      <!--  Links  -->    [graph-engine]: https://www.graphengine.io/    [likq]: https://www.graphengine.io/video/likq.video.html    [likq-gh]: https://github.com/Microsoft/GraphEngine/tree/master/src/Modules/LIKQ    [academic-graph-search]: https://azure.microsoft.com/en-us/services/cognitive-services/academic-knowledge/    [vs-extension]: https://visualstudiogallery.msdn.microsoft.com/12835dd2-2d0e-4b8e-9e7e-9f505bb909b8    [graph-engine-core]: https://www.nuget.org/packages/GraphEngine.Core/    [likq-nuget]: https://www.nuget.org/packages/GraphEngine.LIKQ/    [vs]: https://www.visualstudio.com/    [dotnet-download]: https://dotnet.microsoft.com/download/    [cmake-download]: https://cmake.org/download/    [license]: LICENSE.md """
Big data;https://github.com/plotly/plotly.js;"""<a href=""https://plotly.com/javascript/""><img src=""https://images.plot.ly/logo/plotlyjs-logo@2x.png"" height=""70""></a>    [![npm version](https://badge.fury.io/js/plotly.js.svg)](https://badge.fury.io/js/plotly.js)  [![circle ci](https://circleci.com/gh/plotly/plotly.js.png?&style=shield&circle-token=1f42a03b242bd969756fc3e53ede204af9b507c0)](https://circleci.com/gh/plotly/plotly.js)  [![MIT License](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://github.com/plotly/plotly.js/blob/master/LICENSE)    [Plotly.js](https://plotly.com/javascript) is a standalone Javascript data visualization library, and it also powers the Python and R modules named `plotly` in those respective ecosystems (referred to as [Plotly.py](https://plotly.com/python) and [Plotly.R](http://plotly.com/r)).    Plotly.js can be used to produce dozens of chart types and visualizations, including statistical charts, 3D graphs, scientific charts, SVG and tile maps, financial charts and more.    <p align=""center"">      <a href=""https://plotly.com/javascript/"" target=""_blank"">          <img src=""https://raw.githubusercontent.com/cldougl/plot_images/add_r_img/plotly_2017.png"">      </a>  </p>    [Contact us](https://plotly.com/products/consulting-and-oem/) for Plotly.js consulting, dashboard development, application integration, and feature additions.    ## Table of contents    * [Load as a node module](#load-as-a-node-module)  * [Load via script tag](#load-via-script-tag)  * [Bundles](#bundles)  * [Alternative ways to load and build plotly.js](#alternative-ways-to-load-and-build-plotlyjs)  * [Documentation](#documentation)  * [Bugs and feature requests](#bugs-and-feature-requests)  * [Contributing](#contributing)  * [Notable contributors](#notable-contributors)  * [Copyright and license](#copyright-and-license)  * [Community](#community)    ---  ## Load as a node module  Install [a ready-to-use distributed bundle](https://github.com/plotly/plotly.js/blob/master/dist/README.md)  ```sh  npm i --save plotly.js-dist-min  ```    and use import or require in node.js  ```js  // ES6 module  import Plotly from 'plotly.js-dist-min'    // CommonJS  var Plotly = require('plotly.js-dist-min')  ```    You may also consider using [`plotly.js-dist`](https://www.npmjs.com/package/plotly.js-dist) if you prefer using an unminified package.    ---  ## Load via script tag    ### The script HTML element  > In the examples below `Plotly` object is added to the window scope by `script`. The `newPlot` method is then used to draw an interactive figure as described by `data` and `layout` into the desired `div` here named `gd`. As demonstrated in the example above basic knowledge of `html` and [JSON](https://en.wikipedia.org/wiki/JSON) syntax is enough to get started i.e. with/without JavaScript! To learn and build more with plotly.js please visit [plotly.js documentation](https://plotly.com/javascript).    ```html  <head>      <script src=""https://cdn.plot.ly/plotly-2.11.1.min.js""></script>  </head>  <body>      <div id=""gd""></div>        <script>          Plotly.newPlot(""gd"", /* JSON object */ {              ""data"": [{ ""y"": [1, 2, 3] }],              ""layout"": { ""width"": 600, ""height"": 400}          })      </script>  </body>  ```    Alternatively you may consider using [native ES6 import](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules) in the script tag.  ```html  <script type=""module"">      import ""https://cdn.plot.ly/plotly-2.11.1.min.js""      Plotly.newPlot(""gd"", [{ y: [1, 2, 3] }])  </script>  ```    Fastly supports Plotly.js with free CDN service. Read more at <https://www.fastly.com/open-source>.    ### Un-minified versions are also available on CDN  While non-minified source files may contain characters outside UTF-8, it is recommended that you specify the `charset` when loading those bundles.  ```html  <script src=""https://cdn.plot.ly/plotly-2.11.1.js"" charset=""utf-8""></script>  ```    > Please note that as of v2 the ""plotly-latest"" outputs (e.g. https://cdn.plot.ly/plotly-latest.min.js) will no longer be updated on the CDN, and will stay at the last v1 patch v1.58.5. Therefore, to use the CDN with plotly.js v2 and higher, you must specify an exact plotly.js version.    To support MathJax, you could load either version two or version three of MathJax files, for example:  ```html  <script src=""https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG.js""></script>  ```    ```html  <script src=""https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-svg.js""></script>  ```    > When using MathJax version 3, it is also possible to use `chtml` output on the other parts of the page in addition to `svg` output for the plotly graph.  Please refer to `devtools/test_dashboard/index-mathjax3chtml.html` to see an example.      ## Bundles  There are two kinds of plotly.js bundles:  1. Complete and partial official bundles that are distributed to `npm` and the `CDN`, described in [the dist README](https://github.com/plotly/plotly.js/blob/master/dist/README.md).  2. Custom bundles you can create yourself to optimize the size of bundle depending on your needs. Please visit [CUSTOM_BUNDLE](https://github.com/plotly/plotly.js/blob/master/CUSTOM_BUNDLE.md) for more information.    ---  ## Alternative ways to load and build plotly.js  If your library needs to bundle or directly load [plotly.js/lib/index.js](https://github.com/plotly/plotly.js/blob/master/lib/index.js) or parts of its modules similar to [index-basic](https://github.com/plotly/plotly.js/blob/master/lib/index-basic.js) in some other way than via an official or a custom bundle, or in case you want to tweak the default build configurations of `browserify` or `webpack`, etc. then please visit [`BUILDING.md`](https://github.com/plotly/plotly.js/blob/master/BUILDING.md).    ---  ## Documentation    Official plotly.js documentation is hosted at [https://plotly.com/javascript](https://plotly.com/javascript).    These pages are generated by the Plotly [graphing-library-docs repo](https://github.com/plotly/graphing-library-docs) built with [Jekyll](https://jekyllrb.com/) and publicly hosted on GitHub Pages.  For more info about contributing to Plotly documentation, please read through [contributing guidelines](https://github.com/plotly/graphing-library-docs/blob/master/README.md).    ---  ## Bugs and feature requests    Have a bug or a feature request? Please [open a Github issue](https://github.com/plotly/plotly.js/issues/new) keeping in mind the [issue guidelines](https://github.com/plotly/plotly.js/blob/master/.github/ISSUE_TEMPLATE.md). You may also want to read about [how changes get made to Plotly.js](https://github.com/plotly/plotly.js/blob/master/CONTRIBUTING.md)    ---  ## Contributing    Please read through our [contributing guidelines](https://github.com/plotly/plotly.js/blob/master/CONTRIBUTING.md). Included are directions for opening issues, using plotly.js in your project and notes on development.    ---  ## Notable contributors    Plotly.js is at the core of a large and dynamic ecosystem with many contributors who file issues, reproduce bugs, suggest improvements, write code in this repo (and other upstream or downstream ones) and help users in the Plotly community forum. The following people deserve special recognition for their outsized contributions to this ecosystem:    |   | GitHub | Twitter | Status |  |---|--------|---------|--------|  |**Alex C. Johnson**| [@alexcjohnson](https://github.com/alexcjohnson) | | Active, Maintainer |  |**Mojtaba Samimi** | [@archmoj](https://github.com/archmoj) | [@solarchvision](https://twitter.com/solarchvision) | Active, Maintainer |  |**Antoine Roy-Gobeil** | [@antoinerg](https://github.com/antoinerg) | | Active, Maintainer |  |**Nicolas Kruchten** | [@nicolaskruchten](https://github.com/nicolaskruchten) | [@nicolaskruchten](https://twitter.com/nicolaskruchten) | Active, Maintainer |  |**Jon Mease** | [@jonmmease](https://github.com/jonmmease) | [@jonmmease](https://twitter.com/jonmmease) | Active |  |**Ã‰tienne TÃ©treault-Pinard**| [@etpinard](https://github.com/etpinard) | [@etpinard](https://twitter.com/etpinard) | Hall of Fame |  |**Mikola Lysenko**| [@mikolalysenko](https://github.com/mikolalysenko) | [@MikolaLysenko](https://twitter.com/MikolaLysenko) | Hall of Fame |  |**Ricky Reusser**| [@rreusser](https://github.com/rreusser) | [@rickyreusser](https://twitter.com/rickyreusser) | Hall of Fame |  |**Dmitry Yv.** | [@dy](https://github.com/dy) | [@DimaYv](https://twitter.com/dimayv)| Hall of Fame |  |**Robert Monfera**| [@monfera](https://github.com/monfera) | [@monfera](https://twitter.com/monfera) | Hall of Fame |  |**Robert MÃ¶stl** | [@rmoestl](https://github.com/rmoestl) | [@rmoestl](https://twitter.com/rmoestl) | Hall of Fame |  |**Nicolas Riesco**| [@n-riesco](https://github.com/n-riesco) | | Hall of Fame |  |**MiklÃ³s Tusz**| [@mdtusz](https://github.com/mdtusz) | [@mdtusz](https://twitter.com/mdtusz)| Hall of Fame |  |**Chelsea Douglas**| [@cldougl](https://github.com/cldougl) | | Hall of Fame |  |**Ben Postlethwaite**| [@bpostlethwaite](https://github.com/bpostlethwaite) | | Hall of Fame |  |**Chris Parmer**| [@chriddyp](https://github.com/chriddyp) | | Hall of Fame |  |**Alex Vados**| [@alexander-daniel](https://github.com/alexander-daniel) | | Hall of Fame |    ---  ## Copyright and license    Code and documentation copyright 2021 Plotly, Inc.    Code released under the [MIT license](https://github.com/plotly/plotly.js/blob/master/LICENSE).    ### Versioning    This project is maintained under the [Semantic Versioning guidelines](https://semver.org/).    See the [Releases section](https://github.com/plotly/plotly.js/releases) of our GitHub project for changelogs for each release version of plotly.js.    ---  ## Community    * Follow [@plotlygraphs](https://twitter.com/plotlygraphs) on Twitter for the latest Plotly news.  * Implementation help may be found on community.plot.com (tagged [`plotly-js`](https://community.plotly.com/c/plotly-js)) or    on Stack Overflow (tagged [`plotly`](https://stackoverflow.com/questions/tagged/plotly)).  * Developers should use the keyword `plotly` on packages which modify or add to the functionality of plotly.js when distributing through [npm](https://www.npmjs.com/browse/keyword/plotly). """
Big data;https://github.com/VCNC/haeinsa;"""# Haeinsa    [![Build Status](https://travis-ci.org/VCNC/haeinsa.svg?branch=master)](https://travis-ci.org/VCNC/haeinsa)    Haeinsa is linearly scalable multi-row, multi-table transaction library for HBase.  Haeinsa uses two-phase locking and optimistic concurrency control for implementing transaction.  The isolation level of transaction is serializable.    ## Features    Please see Haeinsa [Wiki] for further information.    - **ACID**: Provides multi-row, multi-table transaction with full ACID semantics.  - **[Linearly scalable]**: Can linearly scale out throughput of transaction as scale out your HBase cluster.  - **[Serializability]**: Provide isolation level of serializability.  - **[Low overhead]**: Relatively low overhead compared to other comparable libraries.  - **Fault-tolerant**: Haeinsa is fault-tolerant against both client and HBase failures.  - **[Easy migration]**: Add transaction feature to your own HBase cluster without any change in HBase cluster except adding lock column family.  - **[Used in practice]**: Haeinsa is used in real service.    ## Usage    APIs of Haeinsa is really similar to APIs of HBase. Please see [How to Use] and [API Usage] document for further information.    	HaeinsaTransactionManager tm = new HaeinsaTransactionManager(tablePool);  	HaeinsaTableIface table = tablePool.getTable(""test"");  	byte[] family = Bytes.toBytes(""data"");  	byte[] qualifier = Bytes.toBytes(""status"");    	HaeinsaTransaction tx = tm.begin(); // start transaction    	HaeinsaPut put1 = new HaeinsaPut(Bytes.toBytes(""user1""));  	put1.add(family, qualifier, Bytes.toBytes(""Hello World!""));  	table.put(tx, put1);    	HaeinsaPut put2 = new HaeinsaPut(Bytes.toBytes(""user2""));  	put2.add(family, qualifier, Bytes.toBytes(""Linearly Scalable!""));  	table.put(tx, put2);    	tx.commit(); // commit transaction to HBase    ## Resources    - [Haeinsa Overview Presentation]: Introducing how Haeina works.  - [Announcing Haeinsa]: Blog post of VCNC Engineering Blog (Korean)  - [Haeinsa: Hbase Transaction Library]: Presentation for Deview Conference (Korean)    ## License    	Copyright (C) 2013-2015 VCNC Inc.  	  	Licensed under the Apache License, Version 2.0 (the ""License"");  	you may not use this file except in compliance with the License.  	You may obtain a copy of the License at  	  	        http://www.apache.org/licenses/LICENSE-2.0  	  	Unless required by applicable law or agreed to in writing, software  	distributed under the License is distributed on an ""AS IS"" BASIS,  	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  	See the License for the specific language governing permissions and  	limitations under the License.    [Wiki]: https://github.com/vcnc/haeinsa/wiki  [How to Use]: https://github.com/vcnc/haeinsa/wiki/How-to-Use  [API Usage]: https://github.com/vcnc/haeinsa/wiki/API-Usage  [HBase]: http://hbase.apache.org/  [Serializability]: http://en.wikipedia.org/wiki/Serializability  [Percolator]: http://research.google.com/pubs/pub36726.html  [Haeinsa]: http://en.wikipedia.org/wiki/Haeinsa  [Tripitaka Koreana, or Palman Daejanggyeong]: http://en.wikipedia.org/wiki/Tripitaka_Koreana  [Haeinsa Overview Presentation]: https://speakerdeck.com/vcnc/haeinsa-overview-hbase-transaction-library  [Announcing Haeinsa]: http://engineering.vcnc.co.kr/2013/10/announcing-haeinsa/  [Linearly scalable]: https://github.com/vcnc/haeinsa/wiki/Performance  [Low overhead]: https://github.com/vcnc/haeinsa/wiki/Performance  [Easy Migration]: https://github.com/vcnc/haeinsa/wiki/Migration-from-HBase  [Used in practice]: https://github.com/vcnc/haeinsa/wiki/Use-Case  [Haeinsa: Hbase Transaction Library]: https://speakerdeck.com/vcnc/haeinsa-hbase-transaction-library """
Big data;https://github.com/ecomfe/echarts;"""# Apache ECharts    <a href=""https://echarts.apache.org/"">      <img style=""vertical-align: top;"" src=""./asset/logo.png?raw=true"" alt=""logo"" height=""50px"">  </a>    Apache ECharts is a free, powerful charting and visualization library offering an easy way of adding intuitive, interactive, and highly customizable charts to your commercial products. It is written in pure JavaScript and based on <a href=""https://github.com/ecomfe/zrender"">zrender</a>, which is a whole new lightweight canvas library.    **[ä¸­æ–‡å®˜ç½‘](https://echarts.apache.org/zh/index.html)** | **[ENGLISH HOMEPAGE](https://echarts.apache.org/en/index.html)**    [![License](https://img.shields.io/npm/l/echarts?color=5470c6)](https://github.com/apache/echarts/blob/master/LICENSE) [![Latest npm release](https://img.shields.io/npm/v/echarts?color=91cc75)](https://www.npmjs.com/package/echarts) [![NPM downloads](https://img.shields.io/npm/dm/echarts.svg?label=npm%20downloads&style=flat&color=fac858)](https://www.npmjs.com/package/echarts) [![Contributors](https://img.shields.io/github/contributors/apache/echarts?color=3ba272)](https://github.com/apache/echarts/graphs/contributors)    [![Build Status](https://github.com/apache/echarts/actions/workflows/ci.yml/badge.svg)](https://github.com/apache/echarts/actions/workflows/ci.yml)    ## Get Apache ECharts    You may choose one of the following methods:    + Download from the [official website](https://echarts.apache.org/download.html)  + `npm install echarts --save`  + CDN: [jsDelivr CDN](https://www.jsdelivr.com/package/npm/echarts?path=dist)    ## Docs    + [Get Started](https://echarts.apache.org/handbook)  + [API](https://echarts.apache.org/api.html)  + [Option Manual](https://echarts.apache.org/option.html)  + [Examples](https://echarts.apache.org/examples)    ## Get Help    + [GitHub Issues](https://github.com/apache/echarts/issues) for bug report and feature requests  + Email [dev@echarts.apache.org](mailto:dev@echarts.apache.org) for general questions  + Subscribe to the [mailing list](https://echarts.apache.org/en/maillist.html) to get updated with the project    ## Build    Build echarts source code:    Execute the instructions in the root directory of the echarts:  ([Node.js](https://nodejs.org) is required)    ```shell  # Install the dependencies from NPM:  npm install    # Rebuild source code immediately in watch mode when changing the source code.  npm run dev    # Check correctness of TypeScript code.  npm run checktype    # If intending to build and get all types of the ""production"" files:  npm run release  ```    Then the ""production"" files are generated in the `dist` directory.    More custom build approaches can be checked in this tutorial: [Create Custom Build of ECharts](https://echarts.apache.org/en/tutorial.html#Create%20Custom%20Build%20of%20ECharts) please.    ## Contribution    If you wish to debug locally or make pull requests, please refer to the [contributing](https://github.com/apache/echarts/blob/master/CONTRIBUTING.md) document.    ## Resources    ### Awesome ECharts    [https://github.com/ecomfe/awesome-echarts](https://github.com/ecomfe/awesome-echarts)    ### Extensions    + [ECharts GL](https://github.com/ecomfe/echarts-gl) An extension pack of ECharts, which provides 3D plots, globe visualization, and WebGL acceleration.    + [Liquidfill æ°´çƒå›¾](https://github.com/ecomfe/echarts-liquidfill)    + [Wordcloud å­—ç¬¦äº‘](https://github.com/ecomfe/echarts-wordcloud)    + [Extension for Baidu Map ç™¾åº¦åœ°å›¾æ‰©å±•](https://github.com/apache/echarts/tree/master/extension-src/bmap) An extension provides a wrapper of Baidu Map Service SDK.    + [vue-echarts](https://github.com/ecomfe/vue-echarts) ECharts component for Vue.js    + [echarts-stat](https://github.com/ecomfe/echarts-stat) Statistics tool for ECharts    ## License    ECharts is available under the Apache License V2.    ## Code of Conduct    Please refer to [Apache Code of Conduct](https://www.apache.org/foundation/policies/conduct.html).    ## Paper    Deqing Li, Honghui Mei, Yi Shen, Shuang Su, Wenli Zhang, Junting Wang, Ming Zu, Wei Chen.  [ECharts: A Declarative Framework for Rapid Construction of Web-based Visualization](https://www.sciencedirect.com/science/article/pii/S2468502X18300068).  Visual Informatics, 2018. """
Big data;https://github.com/chrislusf/seaweedfs;"""# SeaweedFS      [![Slack](https://img.shields.io/badge/slack-purple)](https://join.slack.com/t/seaweedfs/shared_invite/enQtMzI4MTMwMjU2MzA3LTEyYzZmZWYzOGQ3MDJlZWMzYmI0OTE4OTJiZjJjODBmMzUxNmYwODg0YjY3MTNlMjBmZDQ1NzQ5NDJhZWI2ZmY)  [![Twitter](https://img.shields.io/twitter/follow/seaweedfs.svg?style=social&label=Follow)](https://twitter.com/intent/follow?screen_name=seaweedfs)  [![Build Status](https://img.shields.io/github/workflow/status/chrislusf/seaweedfs/Go)](https://github.com/chrislusf/seaweedfs/actions/workflows/go.yml)  [![GoDoc](https://godoc.org/github.com/chrislusf/seaweedfs/weed?status.svg)](https://godoc.org/github.com/chrislusf/seaweedfs/weed)  [![Wiki](https://img.shields.io/badge/docs-wiki-blue.svg)](https://github.com/chrislusf/seaweedfs/wiki)  [![Docker Pulls](https://img.shields.io/docker/pulls/chrislusf/seaweedfs?maxAge=4800)](https://hub.docker.com/r/chrislusf/seaweedfs/)  [![SeaweedFS on Maven Central](https://img.shields.io/maven-central/v/com.github.chrislusf/seaweedfs-client)](https://search.maven.org/search?q=g:com.github.chrislusf)      ![SeaweedFS Logo](https://raw.githubusercontent.com/chrislusf/seaweedfs/master/note/seaweedfs.png)    <h2 align=""center""><a href=""https://www.patreon.com/seaweedfs"">Sponsor SeaweedFS via Patreon</a></h2>    SeaweedFS is an independent Apache-licensed open source project with its ongoing development made  possible entirely thanks to the support of these awesome [backers](https://github.com/chrislusf/seaweedfs/blob/master/backers.md).  If you'd like to grow SeaweedFS even stronger, please consider joining our  <a href=""https://www.patreon.com/seaweedfs"">sponsors on Patreon</a>.    Your support will be really appreciated by me and other supporters!    <!--  <h4 align=""center"">Platinum</h4>    <p align=""center"">    <a href="""" target=""_blank"">      Add your name or icon here    </a>  </p>  -->      ### Gold Sponsors  - [![nodion](https://www.nodion.com/img/logo.svg)](https://www.nodion.com)  - ![shuguang](https://raw.githubusercontent.com/chrislusf/seaweedfs/master/note/shuguang.png)    ---      - [Download Binaries for different platforms](https://github.com/chrislusf/seaweedfs/releases/latest)  - [SeaweedFS on Slack](https://join.slack.com/t/seaweedfs/shared_invite/enQtMzI4MTMwMjU2MzA3LTEyYzZmZWYzOGQ3MDJlZWMzYmI0OTE4OTJiZjJjODBmMzUxNmYwODg0YjY3MTNlMjBmZDQ1NzQ5NDJhZWI2ZmY)  - [SeaweedFS on Twitter](https://twitter.com/SeaweedFS)  - [SeaweedFS on Telegram](https://t.me/Seaweedfs)   - [SeaweedFS Mailing List](https://groups.google.com/d/forum/seaweedfs)  - [Wiki Documentation](https://github.com/chrislusf/seaweedfs/wiki)  - [SeaweedFS White Paper](https://github.com/chrislusf/seaweedfs/wiki/SeaweedFS_Architecture.pdf)  - [SeaweedFS Introduction Slides 2021.5](https://docs.google.com/presentation/d/1DcxKWlINc-HNCjhYeERkpGXXm6nTCES8mi2W5G0Z4Ts/edit?usp=sharing)  - [SeaweedFS Introduction Slides 2019.3](https://www.slideshare.net/chrislusf/seaweedfs-introduction)    Table of Contents  =================    * [Quick Start](#quick-start)      * [Quick Start for S3 API on Docker](#quick-start-for-s3-api-on-docker)      * [Quick Start with Single Binary](#quick-start-with-single-binary)  * [Introduction](#introduction)  * [Features](#features)      * [Additional Features](#additional-features)      * [Filer Features](#filer-features)  * [Example: Using Seaweed Object Store](#example-Using-Seaweed-Object-Store)  * [Architecture](#architecture)  * [Compared to Other File Systems](#compared-to-other-file-systems)      * [Compared to HDFS](#compared-to-hdfs)      * [Compared to GlusterFS, Ceph](#compared-to-glusterfs-ceph)      * [Compared to GlusterFS](#compared-to-glusterfs)      * [Compared to Ceph](#compared-to-ceph)  * [Dev Plan](#dev-plan)  * [Installation Guide](#installation-guide)  * [Disk Related Topics](#disk-related-topics)  * [Benchmark](#Benchmark)  * [License](#license)      ## Quick Start for S3 API on Docker ##    `docker run -p 8333:8333 chrislusf/seaweedfs server -s3`    ## Quick Start with Single Binary ##  * Download the latest binary from https://github.com/chrislusf/seaweedfs/releases and unzip a single binary file `weed` or `weed.exe`  * Run `weed server -dir=/some/data/dir -s3` to start one master, one volume server, one filer, and one S3 gateway.    Also, to increase capacity, just add more volume servers by running `weed volume -dir=""/some/data/dir2"" -mserver=""<master_host>:9333"" -port=8081` locally, or on a different machine, or on thousands of machines. That is it!    ## Introduction ##    SeaweedFS is a simple and highly scalable distributed file system. There are two objectives:    1. to store billions of files!  2. to serve the files fast!    SeaweedFS started as an Object Store to handle small files efficiently.   Instead of managing all file metadata in a central master,   the central master only manages volumes on volume servers,   and these volume servers manage files and their metadata.   This relieves concurrency pressure from the central master and spreads file metadata into volume servers,   allowing faster file access (O(1), usually just one disk read operation).    There is only 40 bytes of disk storage overhead for each file's metadata.   It is so simple with O(1) disk reads that you are welcome to challenge the performance with your actual use cases.    SeaweedFS started by implementing [Facebook's Haystack design paper](http://www.usenix.org/event/osdi10/tech/full_papers/Beaver.pdf).   Also, SeaweedFS implements erasure coding with ideas from   [f4: Facebookâ€™s Warm BLOB Storage System](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-muralidhar.pdf), and has a lot of similarities with [Facebookâ€™s Tectonic Filesystem](https://www.usenix.org/system/files/fast21-pan.pdf)    On top of the object store, optional [Filer] can support directories and POSIX attributes.   Filer is a separate linearly-scalable stateless server with customizable metadata stores,   e.g., MySql, Postgres, Redis, Cassandra, HBase, Mongodb, Elastic Search, LevelDB, RocksDB, Sqlite, MemSql, TiDB, Etcd, CockroachDB, etc.    For any distributed key value stores, the large values can be offloaded to SeaweedFS.   With the fast access speed and linearly scalable capacity,   SeaweedFS can work as a distributed [Key-Large-Value store][KeyLargeValueStore].    SeaweedFS can transparently integrate with the cloud.   With hot data on local cluster, and warm data on the cloud with O(1) access time,   SeaweedFS can achieve both fast local access time and elastic cloud storage capacity.  What's more, the cloud storage access API cost is minimized.   Faster and Cheaper than direct cloud storage!    [Back to TOC](#table-of-contents)    ## Additional Features ##  * Can choose no replication or different replication levels, rack and data center aware.  * Automatic master servers failover - no single point of failure (SPOF).  * Automatic Gzip compression depending on file mime type.  * Automatic compaction to reclaim disk space after deletion or update.  * [Automatic entry TTL expiration][VolumeServerTTL].  * Any server with some disk spaces can add to the total storage space.  * Adding/Removing servers does **not** cause any data re-balancing unless triggered by admin commands.  * Optional picture resizing.  * Support ETag, Accept-Range, Last-Modified, etc.  * Support in-memory/leveldb/readonly mode tuning for memory/performance balance.  * Support rebalancing the writable and readonly volumes.  * [Customizable Multiple Storage Tiers][TieredStorage]: Customizable storage disk types to balance performance and cost.  * [Transparent cloud integration][CloudTier]: unlimited capacity via tiered cloud storage for warm data.  * [Erasure Coding for warm storage][ErasureCoding]  Rack-Aware 10.4 erasure coding reduces storage cost and increases availability.    [Back to TOC](#table-of-contents)    ## Filer Features ##  * [Filer server][Filer] provides ""normal"" directories and files via http.  * [File TTL][FilerTTL] automatically expires file metadata and actual file data.  * [Mount filer][Mount] reads and writes files directly as a local directory via FUSE.  * [Filer Store Replication][FilerStoreReplication] enables HA for filer meta data stores.  * [Active-Active Replication][ActiveActiveAsyncReplication] enables asynchronous one-way or two-way cross cluster continuous replication.  * [Amazon S3 compatible API][AmazonS3API] accesses files with S3 tooling.  * [Hadoop Compatible File System][Hadoop] accesses files from Hadoop/Spark/Flink/etc or even runs HBase.  * [Async Replication To Cloud][BackupToCloud] has extremely fast local access and backups to Amazon S3, Google Cloud Storage, Azure, BackBlaze.  * [WebDAV] accesses as a mapped drive on Mac and Windows, or from mobile devices.  * [AES256-GCM Encrypted Storage][FilerDataEncryption] safely stores the encrypted data.  * [Super Large Files][SuperLargeFiles] stores large or super large files in tens of TB.  * [Cloud Drive][CloudDrive] mounts cloud storage to local cluster, cached for fast read and write with asynchronous write back.  * [Gateway to Remote Object Store][GatewayToRemoteObjectStore] mirrors bucket operations to remote object storage, in addition to [Cloud Drive][CloudDrive]    ## Kubernetes ##  * [Kubernetes CSI Driver][SeaweedFsCsiDriver] A Container Storage Interface (CSI) Driver. [![Docker Pulls](https://img.shields.io/docker/pulls/chrislusf/seaweedfs-csi-driver.svg?maxAge=4800)](https://hub.docker.com/r/chrislusf/seaweedfs-csi-driver/)  * [SeaweedFS Operator](https://github.com/seaweedfs/seaweedfs-operator)    [Filer]: https://github.com/chrislusf/seaweedfs/wiki/Directories-and-Files  [SuperLargeFiles]: https://github.com/chrislusf/seaweedfs/wiki/Data-Structure-for-Large-Files  [Mount]: https://github.com/chrislusf/seaweedfs/wiki/FUSE-Mount  [AmazonS3API]: https://github.com/chrislusf/seaweedfs/wiki/Amazon-S3-API  [BackupToCloud]: https://github.com/chrislusf/seaweedfs/wiki/Async-Replication-to-Cloud  [Hadoop]: https://github.com/chrislusf/seaweedfs/wiki/Hadoop-Compatible-File-System  [WebDAV]: https://github.com/chrislusf/seaweedfs/wiki/WebDAV  [ErasureCoding]: https://github.com/chrislusf/seaweedfs/wiki/Erasure-coding-for-warm-storage  [TieredStorage]: https://github.com/chrislusf/seaweedfs/wiki/Tiered-Storage  [CloudTier]: https://github.com/chrislusf/seaweedfs/wiki/Cloud-Tier  [FilerDataEncryption]: https://github.com/chrislusf/seaweedfs/wiki/Filer-Data-Encryption  [FilerTTL]: https://github.com/chrislusf/seaweedfs/wiki/Filer-Stores  [VolumeServerTTL]: https://github.com/chrislusf/seaweedfs/wiki/Store-file-with-a-Time-To-Live  [SeaweedFsCsiDriver]: https://github.com/seaweedfs/seaweedfs-csi-driver  [ActiveActiveAsyncReplication]: https://github.com/chrislusf/seaweedfs/wiki/Filer-Active-Active-cross-cluster-continuous-synchronization  [FilerStoreReplication]: https://github.com/chrislusf/seaweedfs/wiki/Filer-Store-Replication  [KeyLargeValueStore]: https://github.com/chrislusf/seaweedfs/wiki/Filer-as-a-Key-Large-Value-Store  [CloudDrive]: https://github.com/chrislusf/seaweedfs/wiki/Cloud-Drive-Architecture  [GatewayToRemoteObjectStore]: https://github.com/chrislusf/seaweedfs/wiki/Gateway-to-Remote-Object-Storage      [Back to TOC](#table-of-contents)    ## Example: Using Seaweed Object Store ##    By default, the master node runs on port 9333, and the volume nodes run on port 8080.  Let's start one master node, and two volume nodes on port 8080 and 8081. Ideally, they should be started from different machines. We'll use localhost as an example.    SeaweedFS uses HTTP REST operations to read, write, and delete. The responses are in JSON or JSONP format.    ### Start Master Server ###    ```  > ./weed master  ```    ### Start Volume Servers ###    ```  > weed volume -dir=""/tmp/data1"" -max=5  -mserver=""localhost:9333"" -port=8080 &  > weed volume -dir=""/tmp/data2"" -max=10 -mserver=""localhost:9333"" -port=8081 &  ```    ### Write File ###    To upload a file: first, send a HTTP POST, PUT, or GET request to `/dir/assign` to get an `fid` and a volume server url:    ```  > curl http://localhost:9333/dir/assign  {""count"":1,""fid"":""3,01637037d6"",""url"":""127.0.0.1:8080"",""publicUrl"":""localhost:8080""}  ```    Second, to store the file content, send a HTTP multi-part POST request to `url + '/' + fid` from the response:    ```  > curl -F file=@/home/chris/myphoto.jpg http://127.0.0.1:8080/3,01637037d6  {""name"":""myphoto.jpg"",""size"":43234,""eTag"":""1cc0118e""}  ```    To update, send another POST request with updated file content.    For deletion, send an HTTP DELETE request to the same `url + '/' + fid` URL:    ```  > curl -X DELETE http://127.0.0.1:8080/3,01637037d6  ```    ### Save File Id ###    Now, you can save the `fid`, 3,01637037d6 in this case, to a database field.    The number 3 at the start represents a volume id. After the comma, it's one file key, 01, and a file cookie, 637037d6.    The volume id is an unsigned 32-bit integer. The file key is an unsigned 64-bit integer. The file cookie is an unsigned 32-bit integer, used to prevent URL guessing.    The file key and file cookie are both coded in hex. You can store the <volume id, file key, file cookie> tuple in your own format, or simply store the `fid` as a string.    If stored as a string, in theory, you would need 8+1+16+8=33 bytes. A char(33) would be enough, if not more than enough, since most uses will not need 2^32 volumes.    If space is really a concern, you can store the file id in your own format. You would need one 4-byte integer for volume id, 8-byte long number for file key, and a 4-byte integer for the file cookie. So 16 bytes are more than enough.    ### Read File ###    Here is an example of how to render the URL.    First look up the volume server's URLs by the file's volumeId:    ```  > curl http://localhost:9333/dir/lookup?volumeId=3  {""volumeId"":""3"",""locations"":[{""publicUrl"":""localhost:8080"",""url"":""localhost:8080""}]}  ```    Since (usually) there are not too many volume servers, and volumes don't move often, you can cache the results most of the time. Depending on the replication type, one volume can have multiple replica locations. Just randomly pick one location to read.    Now you can take the public url, render the url or directly read from the volume server via url:    ```   http://localhost:8080/3,01637037d6.jpg  ```    Notice we add a file extension "".jpg"" here. It's optional and just one way for the client to specify the file content type.    If you want a nicer URL, you can use one of these alternative URL formats:    ```   http://localhost:8080/3/01637037d6/my_preferred_name.jpg   http://localhost:8080/3/01637037d6.jpg   http://localhost:8080/3,01637037d6.jpg   http://localhost:8080/3/01637037d6   http://localhost:8080/3,01637037d6  ```    If you want to get a scaled version of an image, you can add some params:    ```  http://localhost:8080/3/01637037d6.jpg?height=200&width=200  http://localhost:8080/3/01637037d6.jpg?height=200&width=200&mode=fit  http://localhost:8080/3/01637037d6.jpg?height=200&width=200&mode=fill  ```    ### Rack-Aware and Data Center-Aware Replication ###    SeaweedFS applies the replication strategy at a volume level. So, when you are getting a file id, you can specify the replication strategy. For example:    ```  curl http://localhost:9333/dir/assign?replication=001  ```    The replication parameter options are:    ```  000: no replication  001: replicate once on the same rack  010: replicate once on a different rack, but same data center  100: replicate once on a different data center  200: replicate twice on two different data center  110: replicate once on a different rack, and once on a different data center  ```    More details about replication can be found [on the wiki][Replication].    [Replication]: https://github.com/chrislusf/seaweedfs/wiki/Replication    You can also set the default replication strategy when starting the master server.    ### Allocate File Key on Specific Data Center ###    Volume servers can be started with a specific data center name:    ```   weed volume -dir=/tmp/1 -port=8080 -dataCenter=dc1   weed volume -dir=/tmp/2 -port=8081 -dataCenter=dc2  ```    When requesting a file key, an optional ""dataCenter"" parameter can limit the assigned volume to the specific data center. For example, this specifies that the assigned volume should be limited to 'dc1':    ```   http://localhost:9333/dir/assign?dataCenter=dc1  ```    ### Other Features ###    * [No Single Point of Failure][feat-1]    * [Insert with your own keys][feat-2]    * [Chunking large files][feat-3]    * [Collection as a Simple Name Space][feat-4]    [feat-1]: https://github.com/chrislusf/seaweedfs/wiki/Failover-Master-Server  [feat-2]: https://github.com/chrislusf/seaweedfs/wiki/Optimization#insert-with-your-own-keys  [feat-3]: https://github.com/chrislusf/seaweedfs/wiki/Optimization#upload-large-files  [feat-4]: https://github.com/chrislusf/seaweedfs/wiki/Optimization#collection-as-a-simple-name-space    [Back to TOC](#table-of-contents)    ## Object Store Architecture ##    Usually distributed file systems split each file into chunks, a central master keeps a mapping of filenames, chunk indices to chunk handles, and also which chunks each chunk server has.    The main drawback is that the central master can't handle many small files efficiently, and since all read requests need to go through the chunk master, so it might not scale well for many concurrent users.    Instead of managing chunks, SeaweedFS manages data volumes in the master server. Each data volume is 32GB in size, and can hold a lot of files. And each storage node can have many data volumes. So the master node only needs to store the metadata about the volumes, which is a fairly small amount of data and is generally stable.    The actual file metadata is stored in each volume on volume servers. Since each volume server only manages metadata of files on its own disk, with only 16 bytes for each file, all file access can read file metadata just from memory and only needs one disk operation to actually read file data.    For comparison, consider that an xfs inode structure in Linux is 536 bytes.    ### Master Server and Volume Server ###    The architecture is fairly simple. The actual data is stored in volumes on storage nodes. One volume server can have multiple volumes, and can both support read and write access with basic authentication.    All volumes are managed by a master server. The master server contains the volume id to volume server mapping. This is fairly static information, and can be easily cached.    On each write request, the master server also generates a file key, which is a growing 64-bit unsigned integer. Since write requests are not generally as frequent as read requests, one master server should be able to handle the concurrency well.    ### Write and Read files ###    When a client sends a write request, the master server returns (volume id, file key, file cookie, volume node url) for the file. The client then contacts the volume node and POSTs the file content.    When a client needs to read a file based on (volume id, file key, file cookie), it asks the master server by the volume id for the (volume node url, volume node public url), or retrieves this from a cache. Then the client can GET the content, or just render the URL on web pages and let browsers fetch the content.    Please see the example for details on the write-read process.    ### Storage Size ###    In the current implementation, each volume can hold 32 gibibytes (32GiB or 8x2^32 bytes). This is because we align content to 8 bytes. We can easily increase this to 64GiB, or 128GiB, or more, by changing 2 lines of code, at the cost of some wasted padding space due to alignment.    There can be 4 gibibytes (4GiB or 2^32 bytes) of volumes. So the total system size is 8 x 4GiB x 4GiB which is 128 exbibytes (128EiB or 2^67 bytes).    Each individual file size is limited to the volume size.    ### Saving memory ###    All file meta information stored on an volume server is readable from memory without disk access. Each file takes just a 16-byte map entry of <64bit key, 32bit offset, 32bit size>. Of course, each map entry has its own space cost for the map. But usually the disk space runs out before the memory does.    ### Tiered Storage to the cloud ###    The local volume servers are much faster, while cloud storages have elastic capacity and are actually more cost-efficient if not accessed often (usually free to upload, but relatively costly to access). With the append-only structure and O(1) access time, SeaweedFS can take advantage of both local and cloud storage by offloading the warm data to the cloud.    Usually hot data are fresh and warm data are old. SeaweedFS puts the newly created volumes on local servers, and optionally upload the older volumes on the cloud. If the older data are accessed less often, this literally gives you unlimited capacity with limited local servers, and still fast for new data.     With the O(1) access time, the network latency cost is kept at minimum.     If the hot/warm data is split as 20/80, with 20 servers, you can achieve storage capacity of 100 servers. That's a cost saving of 80%! Or you can repurpose the 80 servers to store new data also, and get 5X storage throughput.    [Back to TOC](#table-of-contents)    ## Compared to Other File Systems ##    Most other distributed file systems seem more complicated than necessary.    SeaweedFS is meant to be fast and simple, in both setup and operation. If you do not understand how it works when you reach here, we've failed! Please raise an issue with any questions or update this file with clarifications.    SeaweedFS is constantly moving forward. Same with other systems. These comparisons can be outdated quickly. Please help to keep them updated.    [Back to TOC](#table-of-contents)    ### Compared to HDFS ###    HDFS uses the chunk approach for each file, and is ideal for storing large files.    SeaweedFS is ideal for serving relatively smaller files quickly and concurrently.    SeaweedFS can also store extra large files by splitting them into manageable data chunks, and store the file ids of the data chunks into a meta chunk. This is managed by ""weed upload/download"" tool, and the weed master or volume servers are agnostic about it.    [Back to TOC](#table-of-contents)    ### Compared to GlusterFS, Ceph ###    The architectures are mostly the same. SeaweedFS aims to store and read files fast, with a simple and flat architecture. The main differences are    * SeaweedFS optimizes for small files, ensuring O(1) disk seek operation, and can also handle large files.  * SeaweedFS statically assigns a volume id for a file. Locating file content becomes just a lookup of the volume id, which can be easily cached.  * SeaweedFS Filer metadata store can be any well-known and proven data stores, e.g., Redis, Cassandra, HBase, Mongodb, Elastic Search, MySql, Postgres, Sqlite, MemSql, TiDB, CockroachDB, Etcd etc, and is easy to customized.  * SeaweedFS Volume server also communicates directly with clients via HTTP, supporting range queries, direct uploads, etc.    | System         | File Metadata                   | File Content Read| POSIX  | REST API | Optimized for large number of small files |  | -------------  | ------------------------------- | ---------------- | ------ | -------- | ------------------------- |  | SeaweedFS      | lookup volume id, cacheable     | O(1) disk seek   |        | Yes      | Yes                       |  | SeaweedFS Filer| Linearly Scalable, Customizable | O(1) disk seek   | FUSE   | Yes      | Yes                       |  | GlusterFS      | hashing          |                  | FUSE, NFS          |          |                           |  | Ceph           | hashing + rules  |                  | FUSE               | Yes      |                           |  | MooseFS        | in memory        |                  | FUSE               |       | No                          |  | MinIO          | separate meta file for each file  |                  |         | Yes   | No                          |    [Back to TOC](#table-of-contents)    ### Compared to GlusterFS ###    GlusterFS stores files, both directories and content, in configurable volumes called ""bricks"".    GlusterFS hashes the path and filename into ids, and assigned to virtual volumes, and then mapped to ""bricks"".    [Back to TOC](#table-of-contents)    ### Compared to MooseFS ###    MooseFS chooses to neglect small file issue. From moosefs 3.0 manual, ""even a small file will occupy 64KiB plus additionally 4KiB of checksums and 1KiB for the header"", because it ""was initially designed for keeping large amounts (like several thousands) of very big files""    MooseFS Master Server keeps all meta data in memory. Same issue as HDFS namenode.     [Back to TOC](#table-of-contents)    ### Compared to Ceph ###    Ceph can be setup similar to SeaweedFS as a key->blob store. It is much more complicated, with the need to support layers on top of it. [Here is a more detailed comparison](https://github.com/chrislusf/seaweedfs/issues/120)    SeaweedFS has a centralized master group to look up free volumes, while Ceph uses hashing and metadata servers to locate its objects. Having a centralized master makes it easy to code and manage.    Same as SeaweedFS, Ceph is also based on the object store RADOS. Ceph is rather complicated with mixed reviews.    Ceph uses CRUSH hashing to automatically manage the data placement, which is efficient to locate the data. But the data has to be placed according to the CRUSH algorithm. Any wrong configuration would cause data loss. Topology changes, such as adding new servers to increase capacity, will cause data migration with high IO cost to fit the CRUSH algorithm. SeaweedFS places data by assigning them to any writable volumes. If writes to one volume failed, just pick another volume to write. Adding more volumes are also as simple as it can be.    SeaweedFS is optimized for small files. Small files are stored as one continuous block of content, with at most 8 unused bytes between files. Small file access is O(1) disk read.    SeaweedFS Filer uses off-the-shelf stores, such as MySql, Postgres, Sqlite, Mongodb, Redis, Elastic Search, Cassandra, HBase, MemSql, TiDB, CockroachCB, Etcd, to manage file directories. These stores are proven, scalable, and easier to manage.    | SeaweedFS         | comparable to Ceph | advantage |  | -------------  | ------------- | ---------------- |  | Master  | MDS | simpler |  | Volume  | OSD | optimized for small files |  | Filer  | Ceph FS | linearly scalable, Customizable, O(1) or O(logN) |    [Back to TOC](#table-of-contents)    ### Compared to MinIO ###    MinIO follows AWS S3 closely and is ideal for testing for S3 API. It has good UI, policies, versionings, etc. SeaweedFS is trying to catch up here. It is also possible to put MinIO as a gateway in front of SeaweedFS later.    MinIO metadata are in simple files. Each file write will incur extra writes to corresponding meta file.    MinIO does not have optimization for lots of small files. The files are simply stored as is to local disks.  Plus the extra meta file and shards for erasure coding, it only amplifies the LOSF problem.    MinIO has multiple disk IO to read one file. SeaweedFS has O(1) disk reads, even for erasure coded files.    MinIO has full-time erasure coding. SeaweedFS uses replication on hot data for faster speed and optionally applies erasure coding on warm data.    MinIO does not have POSIX-like API support.    MinIO has specific requirements on storage layout. It is not flexible to adjust capacity. In SeaweedFS, just start one volume server pointing to the master. That's all.    ## Dev Plan ##    * More tools and documentation, on how to manage and scale the system.  * Read and write stream data.  * Support structured data.    This is a super exciting project! And we need helpers and [support](https://www.patreon.com/seaweedfs)!    [Back to TOC](#table-of-contents)    ## Installation Guide ##    > Installation guide for users who are not familiar with golang    Step 1: install go on your machine and setup the environment by following the instructions at:    https://golang.org/doc/install    make sure you set up your $GOPATH      Step 2: checkout this repo:  ```bash  git clone https://github.com/chrislusf/seaweedfs.git  ```  Step 3: download, compile, and install the project by executing the following command    ```bash  cd seaweedfs/weed && make install  ```    Once this is done, you will find the executable ""weed"" in your `$GOPATH/bin` directory    [Back to TOC](#table-of-contents)    ## Disk Related Topics ##    ### Hard Drive Performance ###    When testing read performance on SeaweedFS, it basically becomes a performance test of your hard drive's random read speed. Hard drives usually get 100MB/s~200MB/s.    ### Solid State Disk ###    To modify or delete small files, SSD must delete a whole block at a time, and move content in existing blocks to a new block. SSD is fast when brand new, but will get fragmented over time and you have to garbage collect, compacting blocks. SeaweedFS is friendly to SSD since it is append-only. Deletion and compaction are done on volume level in the background, not slowing reading and not causing fragmentation.    [Back to TOC](#table-of-contents)    ## Benchmark ##    My Own Unscientific Single Machine Results on Mac Book with Solid State Disk, CPU: 1 Intel Core i7 2.6GHz.    Write 1 million 1KB file:  ```  Concurrency Level:      16  Time taken for tests:   66.753 seconds  Complete requests:      1048576  Failed requests:        0  Total transferred:      1106789009 bytes  Requests per second:    15708.23 [#/sec]  Transfer rate:          16191.69 [Kbytes/sec]    Connection Times (ms)                min      avg        max      std  Total:        0.3      1.0       84.3      0.9    Percentage of the requests served within a certain time (ms)     50%      0.8 ms     66%      1.0 ms     75%      1.1 ms     80%      1.2 ms     90%      1.4 ms     95%      1.7 ms     98%      2.1 ms     99%      2.6 ms    100%     84.3 ms  ```    Randomly read 1 million files:  ```  Concurrency Level:      16  Time taken for tests:   22.301 seconds  Complete requests:      1048576  Failed requests:        0  Total transferred:      1106812873 bytes  Requests per second:    47019.38 [#/sec]  Transfer rate:          48467.57 [Kbytes/sec]    Connection Times (ms)                min      avg        max      std  Total:        0.0      0.3       54.1      0.2    Percentage of the requests served within a certain time (ms)     50%      0.3 ms     90%      0.4 ms     98%      0.6 ms     99%      0.7 ms    100%     54.1 ms  ```    [Back to TOC](#table-of-contents)    ## License ##    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at        http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License.    The text of this page is available for modification and reuse under the terms of the Creative Commons Attribution-Sharealike 3.0 Unported License and the GNU Free Documentation License (unversioned, with no invariant sections, front-cover texts, or back-cover texts).    [Back to TOC](#table-of-contents)    ## Stargazers over time    [![Stargazers over time](https://starchart.cc/chrislusf/seaweedfs.svg)](https://starchart.cc/chrislusf/seaweedfs)   """
Big data;https://github.com/apache/incubator-airflow;"""<!--   Licensed to the Apache Software Foundation (ASF) under one   or more contributor license agreements.  See the NOTICE file   distributed with this work for additional information   regarding copyright ownership.  The ASF licenses this file   to you under the Apache License, Version 2.0 (the   ""License""); you may not use this file except in compliance   with the License.  You may obtain a copy of the License at       http://www.apache.org/licenses/LICENSE-2.0     Unless required by applicable law or agreed to in writing,   software distributed under the License is distributed on an   ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY   KIND, either express or implied.  See the License for the   specific language governing permissions and limitations   under the License.  -->    # Apache Airflow    [![PyPI version](https://badge.fury.io/py/apache-airflow.svg)](https://badge.fury.io/py/apache-airflow)  [![GitHub Build](https://github.com/apache/airflow/workflows/CI%20Build/badge.svg)](https://github.com/apache/airflow/actions)  [![Coverage Status](https://img.shields.io/codecov/c/github/apache/airflow/main.svg)](https://codecov.io/github/apache/airflow?branch=main)  [![License](https://img.shields.io/:license-Apache%202-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0.txt)  [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/apache-airflow.svg)](https://pypi.org/project/apache-airflow/)  [![Docker Pulls](https://img.shields.io/docker/pulls/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow)  [![Docker Stars](https://img.shields.io/docker/stars/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow)  [![PyPI - Downloads](https://img.shields.io/pypi/dm/apache-airflow)](https://pypi.org/project/apache-airflow/)  [![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow)](https://artifacthub.io/packages/search?repo=apache-airflow)  [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)  [![Twitter Follow](https://img.shields.io/twitter/follow/ApacheAirflow.svg?style=social&label=Follow)](https://twitter.com/ApacheAirflow)  [![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://s.apache.org/airflow-slack)    [Apache Airflow](https://airflow.apache.org/docs/apache-airflow/stable/) (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.    When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.    Use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.    <!-- START doctoc generated TOC please keep comment here to allow auto update -->  <!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->  **Table of contents**    - [Project Focus](#project-focus)  - [Principles](#principles)  - [Requirements](#requirements)  - [Getting started](#getting-started)  - [Installing from PyPI](#installing-from-pypi)  - [Official source code](#official-source-code)  - [Convenience packages](#convenience-packages)  - [User Interface](#user-interface)  - [Semantic versioning](#semantic-versioning)  - [Version Life Cycle](#version-life-cycle)  - [Support for Python and Kubernetes versions](#support-for-python-and-kubernetes-versions)  - [Base OS support for reference Airflow images](#base-os-support-for-reference-airflow-images)  - [Approach to dependencies of Airflow](#approach-to-dependencies-of-airflow)  - [Support for providers](#support-for-providers)  - [Contributing](#contributing)  - [Who uses Apache Airflow?](#who-uses-apache-airflow)  - [Who Maintains Apache Airflow?](#who-maintains-apache-airflow)  - [Can I use the Apache Airflow logo in my presentation?](#can-i-use-the-apache-airflow-logo-in-my-presentation)  - [Airflow merchandise](#airflow-merchandise)  - [Links](#links)  - [Sponsors](#sponsors)    <!-- END doctoc generated TOC please keep comment here to allow auto update -->    ## Project Focus    Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include [Luigi](https://github.com/spotify/luigi), [Oozie](https://oozie.apache.org/) and [Azkaban](https://azkaban.github.io/).    Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's [Xcom feature](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#xcoms)). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.    Airflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.    ## Principles    - **Dynamic**: Airflow pipelines are configuration as code (Python), allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically.  - **Extensible**: Easily define your own operators, executors and extend the library so that it fits the level of abstraction that suits your environment.  - **Elegant**: Airflow pipelines are lean and explicit. Parameterizing your scripts is built into the core of Airflow using the powerful **Jinja** templating engine.  - **Scalable**: Airflow has a modular architecture and uses a message queue to orchestrate an arbitrary number of workers.    ## Requirements    Apache Airflow is tested with:    |                     | Main version (dev)  | Stable version (2.2.4)   |  |---------------------|---------------------|--------------------------|  | Python              | 3.7, 3.8, 3.9       | 3.6, 3.7, 3.8, 3.9       |  | Platform            | AMD64/ARM64(\*)     | AMD64                    |  | Kubernetes          | 1.20, 1.21          | 1.18, 1.19, 1.20         |  | PostgreSQL          | 10, 11, 12, 13      | 9.6, 10, 11, 12, 13      |  | MySQL               | 5.7, 8              | 5.7, 8                   |  | SQLite              | 3.15.0+             | 3.15.0+                  |  | MSSQL               | 2017(\*), 2019 (\*) |                          |    \* Experimental    **Note**: MySQL 5.x versions are unable to or have limitations with  running multiple schedulers -- please see the [Scheduler docs](https://airflow.apache.org/docs/apache-airflow/stable/scheduler.html).  MariaDB is not tested/recommended.    **Note**: SQLite is used in Airflow tests. Do not use it in production. We recommend  using the latest stable version of SQLite for local development.    **Note**: Python v3.10 is not supported yet. For details, see [#19059](https://github.com/apache/airflow/issues/19059).    **Note**: Airflow currently can be run on POSIX-compliant Operating Systems. For development it is regularly  tested on fairly modern Linux Distros and recent versions of MacOS.  On Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers.  The work to add Windows support is tracked via [#10388](https://github.com/apache/airflow/issues/10388) but  it is not a high priority. You should only use Linux-based distros as ""Production"" execution environment  as this is the only environment that is supported. The only distro that is used in our CI tests and that  is used in the [Community managed DockerHub image](https://hub.docker.com/p/apache/airflow) is  `Debian Bullseye`.    ## Getting started    Visit the official Airflow website documentation (latest **stable** release) for help with  [installing Airflow](https://airflow.apache.org/docs/apache-airflow/stable/installation.html),  [getting started](https://airflow.apache.org/docs/apache-airflow/stable/start/index.html), or walking  through a more complete [tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html).    > Note: If you're looking for documentation for the main branch (latest development branch): you can find it on [s.apache.org/airflow-docs](https://s.apache.org/airflow-docs/).    For more information on Airflow Improvement Proposals (AIPs), visit  the [Airflow Wiki](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals).    Documentation for dependent projects like provider packages, Docker image, Helm Chart, you'll find it in [the documentation index](https://airflow.apache.org/docs/).    ## Installing from PyPI    We publish Apache Airflow as `apache-airflow` package in PyPI. Installing it however might be sometimes tricky  because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and  applications usually pin them, but we should do neither and both simultaneously. We decided to keep  our dependencies as open as possible (in `setup.py`) so users can install different versions of libraries  if needed. This means that `pip install apache-airflow` will not work from time to time or will  produce unusable Airflow installation.    To have repeatable installation, however, we keep a set of ""known-to-be-working"" constraint  files in the orphan `constraints-main` and `constraints-2-0` branches. We keep those ""known-to-be-working""  constraints files separately per major/minor Python version.  You can use them as constraint files when installing Airflow from PyPI. Note that you have to specify  correct Airflow tag/version/branch and Python versions in the URL.      1. Installing just Airflow:    > Note: Only `pip` installation is currently officially supported.    While it is possible to install Airflow with tools like [Poetry](https://python-poetry.org) or  [pip-tools](https://pypi.org/project/pip-tools), they do not share the same workflow as  `pip` - especially when it comes to constraint vs. requirements management.  Installing via `Poetry` or `pip-tools` is not currently supported.    If you wish to install Airflow using those tools, you should use the constraint files and convert  them to the appropriate format and workflow that your tool requires.      ```bash  pip install 'apache-airflow==2.2.4' \   --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.2.4/constraints-3.7.txt""  ```    2. Installing with extras (i.e., postgres, google)    ```bash  pip install 'apache-airflow[postgres,google]==2.2.4' \   --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.2.4/constraints-3.7.txt""  ```    For information on installing provider packages, check  [providers](http://airflow.apache.org/docs/apache-airflow-providers/index.html).    ## Official source code    Apache Airflow is an [Apache Software Foundation](https://www.apache.org) (ASF) project,  and our official source code releases:    - Follow the [ASF Release Policy](https://www.apache.org/legal/release-policy.html)  - Can be downloaded from [the ASF Distribution Directory](https://downloads.apache.org/airflow)  - Are cryptographically signed by the release manager  - Are officially voted on by the PMC members during the    [Release Approval Process](https://www.apache.org/legal/release-policy.html#release-approval)    Following the ASF rules, the source packages released must be sufficient for a user to build and test the  release provided they have access to the appropriate platform and tools.    ## Convenience packages    There are other ways of installing and using Airflow. Those are ""convenience"" methods - they are  not ""official releases"" as stated by the `ASF Release Policy`, but they can be used by the users  who do not want to build the software themselves.    Those are - in the order of most common ways people install Airflow:    - [PyPI releases](https://pypi.org/project/apache-airflow/) to install Airflow using standard `pip` tool  - [Docker Images](https://hub.docker.com/r/apache/airflow) to install airflow via    `docker` tool, use them in Kubernetes, Helm Charts, `docker-compose`, `docker swarm`, etc. You can    read more about using, customising, and extending the images in the    [Latest docs](https://airflow.apache.org/docs/docker-stack/index.html), and    learn details on the internals in the [IMAGES.rst](https://github.com/apache/airflow/blob/main/IMAGES.rst) document.  - [Tags in GitHub](https://github.com/apache/airflow/tags) to retrieve the git project sources that    were used to generate official source packages via git    All those artifacts are not official releases, but they are prepared using officially released sources.  Some of those artifacts are ""development"" or ""pre-release"" ones, and they are clearly marked as such  following the ASF Policy.    ## User Interface    - **DAGs**: Overview of all DAGs in your environment.      ![DAGs](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/dags.png)    - **Tree**: Tree representation of a DAG that spans across time.      ![Tree](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/tree.png)    - **Graph**: Visualization of a DAG's dependencies and their current status for a specific run.      ![Graph](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/graph.png)    - **Task Duration**: Total time spent on different tasks over time.      ![Task Duration](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/duration.png)    - **Gantt**: Duration and overlap of a DAG.      ![Gantt](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/gantt.png)    - **Code**: Quick way to view source code of a DAG.      ![Code](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/code.png)    ## Semantic versioning    As of Airflow 2.0.0, we support a strict [SemVer](https://semver.org/) approach for all packages released.    There are few specific rules that we agreed to that define details of versioning of the different  packages:    * **Airflow**: SemVer rules apply to core airflow only (excludes any changes to providers).    Changing limits for versions of Airflow dependencies is not a breaking change on its own.  * **Airflow Providers**: SemVer rules apply to changes in the particular provider's code only.    SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version.    For example, `google 4.1.0` and `amazon 3.0.3` providers can happily be installed    with `Airflow 2.1.2`. If there are limits of cross-dependencies between providers and Airflow packages,    they are present in providers as `install_requires` limitations. We aim to keep backwards    compatibility of providers with all previously released Airflow 2 versions but    there will sometimes be breaking changes that might make some, or all    providers, have minimum Airflow version specified. Change of that minimum supported Airflow version    is a breaking change for provider because installing the new provider might automatically    upgrade Airflow (which might be an undesired side effect of upgrading provider).  * **Airflow Helm Chart**: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR    versions for the chart are independent from the Airflow version. We aim to keep backwards    compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might    only work starting from specific Airflow releases. We might however limit the Helm    Chart to depend on minimal Airflow version.  * **Airflow API clients**: SemVer MAJOR and MINOR versions follow MAJOR and MINOR versions of Airflow.    The first MAJOR or MINOR X.Y.0 release of Airflow should always be followed by X.Y.0 release of    all clients. The clients then can release their own PATCH releases with bugfixes,    independently of Airflow PATCH releases.    ## Version Life Cycle    Apache Airflow version life cycle:    <!-- This table is automatically updated by pre-commit scripts/ci/pre-commit/supported_versions.py -->  <!-- Beginning of auto-generated table -->    | Version   | Current Patch/Minor   | State     | First Release   | Limited Support   | EOL/Terminated   |  |-----------|-----------------------|-----------|-----------------|-------------------|------------------|  | 2         | 2.2.4                 | Supported | Dec 17, 2020    | TBD               | TBD              |  | 1.10      | 1.10.15               | EOL       | Aug 27, 2018    | Dec 17, 2020      | June 17, 2021    |  | 1.9       | 1.9.0                 | EOL       | Jan 03, 2018    | Aug 27, 2018      | Aug 27, 2018     |  | 1.8       | 1.8.2                 | EOL       | Mar 19, 2017    | Jan 03, 2018      | Jan 03, 2018     |  | 1.7       | 1.7.1.2               | EOL       | Mar 28, 2016    | Mar 19, 2017      | Mar 19, 2017     |    <!-- End of auto-generated table -->    Limited support versions will be supported with security and critical bug fix only.  EOL versions will not get any fixes nor support.  We always recommend that all users run the latest available minor release for whatever major version is in use.  We **highly** recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.    ## Support for Python and Kubernetes versions    As of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support.  They are based on the official release schedule of Python and Kubernetes, nicely summarized in the  [Python Developer's Guide](https://devguide.python.org/#status-of-python-branches) and  [Kubernetes version skew policy](https://kubernetes.io/docs/setup/release/version-skew-policy/).    1. We drop support for Python and Kubernetes versions when they reach EOL. We drop support for those     EOL versions in main right after EOL date, and it is effectively removed when we release the     first new MINOR (Or MAJOR if there is no new MINOR version) of Airflow     For example, for Python 3.7 it means that we will drop support in main right after 27.06.2023, and     the first MAJOR or MINOR version of Airflow released after will not have it.    2. The ""oldest"" supported version of Python/Kubernetes is the default one until we decide to switch to     later version. ""Default"" is only meaningful in terms of ""smoke tests"" in CI PRs, which are run using this     default version and the default reference image available. Currently `apache/airflow:latest`     and `apache/airflow:2.2.4` images are Python 3.7 images. This means that default reference image will     become the default at the time when we start preparing for dropping 3.7 support which is few months     before the end of life for Python 3.7.    4. We support a new version of Python/Kubernetes in main after they are officially released, as soon as we     make them work in our CI pipeline (which might not be immediate due to dependencies catching up with     new versions of Python mostly) we release new images/support in Airflow based on the working CI setup.    ## Base OS support for reference Airflow images    The Airflow Community provides conveniently packaged container images that are published whenever  we publish an Apache Airflow release. Those images contain:    * Base OS with necessary packages to install Airflow (stable Debian OS)  * Base Python installation in versions supported at the time of release for the MINOR version of    Airflow released (so there could be different versions for 2.3 and 2.2 line for example)  * Libraries required to connect to suppoerted Databases (again the set of databases supported depends    on the MINOR version of Airflow.  * Predefined set of popular providers (for details see the [Dockerfile](Dockerfile)).  * Possibility of building your own, custom image where the user can choose their own set of providers    and libraries (see [Building the image](https://airflow.apache.org/docs/docker-stack/build.html))  * In the future Airflow might also support a ""slim"" version without providers nor database clients installed    The version of the base OS image is the stable version of Debian. Airflow supports using all currently active  stable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline for  building and testing the OS version. Approximately 6 months before the end-of-life of a previous stable  version of the OS, Airflow switches the images released to use the latest supported version of the OS.  For example since Debian Buster end-of-life is August 2022, Airflow switches the images in `main` branch  to use Debian Bullseye in February/March 2022. The version will be used in the next MINOR release after  the switch happens. In case of the Bullseye switch - 2.3.0 version will use Bullseye. The images released  in the previous MINOR version continue to use the version that all other releases for the MINOR version  used.    Users will continue to be able to build their images using stable Debian releases until the end of life and  building and verifying of the images happens in our CI but no unit tests are executed using this image in  the `main` branch.    ## Approach to dependencies of Airflow    Airflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application,  therefore our policies to dependencies has to include both - stability of installation of application,  but also ability to install newer version of dependencies for those users who develop DAGs. We developed  the approach where `constraints` are used to make sure airflow can be installed in a repeatable way, while  we do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-bound  version of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them is  needed because of importance of the dependency as well as risk it involves to upgrade specific dependency.  We also upper-bound the dependencies that we know cause problems.    The constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependencies  automatically (providing that all the tests pass). Our `main` build failures will indicate in case there  are versions of dependencies that break our tests - indicating that we should either upper-bind them or  that we should fix our code/tests to account for the upstream changes from those dependencies.    Whenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should have  a good reason why dependency is upper-bound. And we should also mention what is the condition to remove the  binding.    ### Approach for dependencies for Airflow Core    Those `extras` and `providers` dependencies are maintained in `setup.cfg`.    There are few dependencies that we decided are important enough to upper-bound them by default, as they are  known to follow predictable versioning scheme, and we know that new versions of those are very likely to  bring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions of  the dependencies as they are released, but this is manual process.    The important dependencies are:    * `SQLAlchemy`: upper-bound to specific MINOR version (SQLAlchemy is known to remove deprecations and     introduce breaking changes especially that support for different Databases varies and changes at     various speed (example: SQLAlchemy 1.4 broke MSSQL integration for Airflow)  * `Alembic`: it is important to handle our migrations in predictable and performant way. It is developed     together with SQLAlchemy. Our experience with Alembic is that it very stable in MINOR version  * `Flask`: We are using Flask as the back-bone of our web UI and API. We know major version of Flask     are very likely to introduce breaking changes across those so limiting it to MAJOR version makes sense  * `werkzeug`: the library is known to cause problems in new versions. It is tightly coupled with Flask     libraries, and we should update them together    ### Approach for dependencies in Airflow Providers and extras    Those `extras` and `providers` dependencies are maintained in `setup.py`.    By default, we should not upper-bound dependencies for providers, however each provider's maintainer  might decide to add additional limits (and justify them with comment)    ## Support for providers    Providers released by the community have limitation of a minimum supported version of Airflow. The minimum  version of Airflow is the `MINOR` version (2.1, 2.2 etc.) indicating that the providers might use features  that appeared in this release. The default support timespan for the minimum version of Airflow  (there could be justified exceptions) is that we increase the minimum Airflow version, when 12 months passed  since the first release for the MINOR version of Airflow.    For example this means that by default we upgrade the minimum version of Airflow supported by providers  to 2.2.0 in the first Provider's release after 21st of May 2022 (21st of May 2021 is the date when the  first `PATCHLEVEL` of 2.1 (2.1.0) has been released.    ## Contributing    Want to help build Apache Airflow? Check out our [contributing documentation](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst).    Official Docker (container) images for Apache Airflow are described in [IMAGES.rst](https://github.com/apache/airflow/blob/main/IMAGES.rst).    ## Who uses Apache Airflow?    More than 400 organizations are using Apache Airflow  [in the wild](https://github.com/apache/airflow/blob/main/INTHEWILD.md).    ## Who Maintains Apache Airflow?    Airflow is the work of the [community](https://github.com/apache/airflow/graphs/contributors),  but the [core committers/maintainers](https://people.apache.org/committers-by-project.html#airflow)  are responsible for reviewing and merging PRs as well as steering conversations around new feature requests.  If you would like to become a maintainer, please review the Apache Airflow  [committer requirements](https://github.com/apache/airflow/blob/main/COMMITTERS.rst#guidelines-to-become-an-airflow-committer).    ## Can I use the Apache Airflow logo in my presentation?    Yes! Be sure to abide by the Apache Foundation [trademark policies](https://www.apache.org/foundation/marks/#books) and the Apache Airflow [Brandbook](https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook). The most up to date logos are found in [this repo](/docs/apache-airflow/img/logos) and on the Apache Software Foundation [website](https://www.apache.org/logos/about.html).    ## Airflow merchandise    If you would love to have Apache Airflow stickers, t-shirt, etc. then check out  [Redbubble Shop](https://www.redbubble.com/i/sticker/Apache-Airflow-by-comdev/40497530.EJUG5).    ## Links    - [Documentation](https://airflow.apache.org/docs/apache-airflow/stable/)  - [Chat](https://s.apache.org/airflow-slack)    ## Sponsors    The CI infrastructure for Apache Airflow has been sponsored by:    <!-- Ordered by most recently ""funded"" -->    <a href=""https://astronomer.io""><img src=""https://assets2.astronomer.io/logos/logoForLIGHTbackground.png"" alt=""astronomer.io"" width=""250px""></a>  <a href=""https://aws.amazon.com/opensource/""><img src=""docs/integration-logos/aws/AWS-Cloud-alt_light-bg@4x.png"" alt=""AWS OpenSource"" width=""130px""></a> """
Big data;https://github.com/tidwall/buntdb;"""<p align=""center"">  <img      src=""logo.png""      width=""307"" height=""150"" border=""0"" alt=""BuntDB"">  <br>  <a href=""https://godoc.org/github.com/tidwall/buntdb""><img src=""https://img.shields.io/badge/go-documentation-blue.svg?style=flat-square"" alt=""Godoc""></a>  <a href=""https://github.com/tidwall/buntdb/blob/master/LICENSE""><img src=""https://img.shields.io/github/license/tidwall/buntdb.svg?style=flat-square"" alt=""LICENSE""></a>  </p>    BuntDB is a low-level, in-memory, key/value store in pure Go.  It persists to disk, is ACID compliant, and uses locking for multiple  readers and a single writer. It supports custom indexes and geospatial  data. It's ideal for projects that need a dependable database and favor  speed over data size.    Features  ========    - In-memory database for [fast reads and writes](#performance)  - Embeddable with a [simple API](https://godoc.org/github.com/tidwall/buntdb)  - [Spatial indexing](#spatial-indexes) for up to 20 dimensions; Useful for Geospatial data  - Index fields inside [JSON](#json-indexes) documents  - [Collate i18n Indexes](#collate-i18n-indexes) using the optional [collate package](https://github.com/tidwall/collate)  - Create [custom indexes](#custom-indexes) for any data type  - Support for [multi value indexes](#multi-value-index); Similar to a SQL multi column index  - [Built-in types](#built-in-types) that are easy to get up & running; String, Uint, Int, Float  - Flexible [iteration](#iterating) of data; ascending, descending, and ranges  - [Durable append-only file](#append-only-file) format for persistence  - Option to evict old items with an [expiration](#data-expiration) TTL  - ACID semantics with locking [transactions](#transactions) that support rollbacks      Getting Started  ===============    ## Installing    To start using BuntDB, install Go and run `go get`:    ```sh  $ go get -u github.com/tidwall/buntdb  ```    This will retrieve the library.      ## Opening a database    The primary object in BuntDB is a `DB`. To open or create your  database, use the `buntdb.Open()` function:    ```go  package main    import (  	""log""    	""github.com/tidwall/buntdb""  )    func main() {  	// Open the data.db file. It will be created if it doesn't exist.  	db, err := buntdb.Open(""data.db"")  	if err != nil {  		log.Fatal(err)  	}  	defer db.Close()    	...  }  ```    It's also possible to open a database that does not persist to disk by using `:memory:` as the path of the file.    ```go  buntdb.Open("":memory:"") // Open a file that does not persist to disk.  ```    ## Transactions  All reads and writes must be performed from inside a transaction. BuntDB can have one write transaction opened at a time, but can have many concurrent read transactions. Each transaction maintains a stable view of the database. In other words, once a transaction has begun, the data for that transaction cannot be changed by other transactions.    Transactions run in a function that exposes a `Tx` object, which represents the transaction state. While inside a transaction, all database operations should be performed using this object. You should never access the origin `DB` object while inside a transaction. Doing so may have side-effects, such as blocking your application.    When a transaction fails, it will roll back, and revert all changes that occurred to the database during that transaction. There's a single return value that you can use to close the transaction. For read/write transactions, returning an error this way will force the transaction to roll back. When a read/write transaction succeeds all changes are persisted to disk.    ### Read-only Transactions  A read-only transaction should be used when you don't need to make changes to the data. The advantage of a read-only transaction is that there can be many running concurrently.    ```go  err := db.View(func(tx *buntdb.Tx) error {  	...  	return nil  })  ```    ### Read/write Transactions  A read/write transaction is used when you need to make changes to your data. There can only be one read/write transaction running at a time. So make sure you close it as soon as you are done with it.    ```go  err := db.Update(func(tx *buntdb.Tx) error {  	...  	return nil  })  ```    ## Setting and getting key/values    To set a value you must open a read/write transaction:    ```go  err := db.Update(func(tx *buntdb.Tx) error {  	_, _, err := tx.Set(""mykey"", ""myvalue"", nil)  	return err  })  ```      To get the value:    ```go  err := db.View(func(tx *buntdb.Tx) error {  	val, err := tx.Get(""mykey"")  	if err != nil{  		return err  	}  	fmt.Printf(""value is %s\n"", val)  	return nil  })  ```    Getting non-existent values will cause an `ErrNotFound` error.    ### Iterating  All keys/value pairs are ordered in the database by the key. To iterate over the keys:    ```go  err := db.View(func(tx *buntdb.Tx) error {  	err := tx.Ascend("""", func(key, value string) bool {  		fmt.Printf(""key: %s, value: %s\n"", key, value)  		return true // continue iteration  	})  	return err  })  ```    There is also `AscendGreaterOrEqual`, `AscendLessThan`, `AscendRange`, `AscendEqual`, `Descend`, `DescendLessOrEqual`, `DescendGreaterThan`, `DescendRange`, and `DescendEqual`. Please see the [documentation](https://godoc.org/github.com/tidwall/buntdb) for more information on these functions.      ## Custom Indexes  Initially all data is stored in a single [B-tree](https://en.wikipedia.org/wiki/B-tree) with each item having one key and one value. All of these items are ordered by the key. This is great for quickly getting a value from a key or [iterating](#iterating) over the keys. Feel free to peruse the [B-tree implementation](https://github.com/tidwall/btree).    You can also create custom indexes that allow for ordering and [iterating](#iterating) over values. A custom index also uses a B-tree, but it's more flexible because it allows for custom ordering.    For example, let's say you want to create an index for ordering names:    ```go  db.CreateIndex(""names"", ""*"", buntdb.IndexString)  ```    This will create an index named `names` which stores and sorts all values. The second parameter is a pattern that is used to filter on keys. A `*` wildcard argument means that we want to accept all keys. `IndexString` is a built-in function that performs case-insensitive ordering on the values    Now you can add various names:    ```go  db.Update(func(tx *buntdb.Tx) error {  	tx.Set(""user:0:name"", ""tom"", nil)  	tx.Set(""user:1:name"", ""Randi"", nil)  	tx.Set(""user:2:name"", ""jane"", nil)  	tx.Set(""user:4:name"", ""Janet"", nil)  	tx.Set(""user:5:name"", ""Paula"", nil)  	tx.Set(""user:6:name"", ""peter"", nil)  	tx.Set(""user:7:name"", ""Terri"", nil)  	return nil  })  ```    Finally you can iterate over the index:    ```go  db.View(func(tx *buntdb.Tx) error {  	tx.Ascend(""names"", func(key, val string) bool {  	fmt.Printf(buf, ""%s %s\n"", key, val)  		return true  	})  	return nil  })  ```  The output should be:  ```  user:2:name jane  user:4:name Janet  user:5:name Paula  user:6:name peter  user:1:name Randi  user:7:name Terri  user:0:name tom  ```    The pattern parameter can be used to filter on keys like this:    ```go  db.CreateIndex(""names"", ""user:*"", buntdb.IndexString)  ```    Now only items with keys that have the prefix `user:` will be added to the `names` index.      ### Built-in types  Along with `IndexString`, there is also `IndexInt`, `IndexUint`, and `IndexFloat`.  These are built-in types for indexing. You can choose to use these or create your own.    So to create an index that is numerically ordered on an age key, we could use:    ```go  db.CreateIndex(""ages"", ""user:*:age"", buntdb.IndexInt)  ```    And then add values:    ```go  db.Update(func(tx *buntdb.Tx) error {  	tx.Set(""user:0:age"", ""35"", nil)  	tx.Set(""user:1:age"", ""49"", nil)  	tx.Set(""user:2:age"", ""13"", nil)  	tx.Set(""user:4:age"", ""63"", nil)  	tx.Set(""user:5:age"", ""8"", nil)  	tx.Set(""user:6:age"", ""3"", nil)  	tx.Set(""user:7:age"", ""16"", nil)  	return nil  })  ```    ```go  db.View(func(tx *buntdb.Tx) error {  	tx.Ascend(""ages"", func(key, val string) bool {  	fmt.Printf(buf, ""%s %s\n"", key, val)  		return true  	})  	return nil  })  ```    The output should be:  ```  user:6:age 3  user:5:age 8  user:2:age 13  user:7:age 16  user:0:age 35  user:1:age 49  user:4:age 63  ```    ## Spatial Indexes  BuntDB has support for spatial indexes by storing rectangles in an [R-tree](https://en.wikipedia.org/wiki/R-tree). An R-tree is organized in a similar manner as a [B-tree](https://en.wikipedia.org/wiki/B-tree), and both are balanced trees. But, an R-tree is special because it can operate on data that is in multiple dimensions. This is super handy for Geospatial applications.    To create a spatial index use the `CreateSpatialIndex` function:    ```go  db.CreateSpatialIndex(""fleet"", ""fleet:*:pos"", buntdb.IndexRect)  ```    Then `IndexRect` is a built-in function that converts rect strings to a format that the R-tree can use. It's easy to use this function out of the box, but you might find it better to create a custom one that renders from a different format, such as [Well-known text](https://en.wikipedia.org/wiki/Well-known_text) or [GeoJSON](http://geojson.org/).    To add some lon,lat points to the `fleet` index:    ```go  db.Update(func(tx *buntdb.Tx) error {  	tx.Set(""fleet:0:pos"", ""[-115.567 33.532]"", nil)  	tx.Set(""fleet:1:pos"", ""[-116.671 35.735]"", nil)  	tx.Set(""fleet:2:pos"", ""[-113.902 31.234]"", nil)  	return nil  })  ```    And then you can run the `Intersects` function on the index:    ```go  db.View(func(tx *buntdb.Tx) error {  	tx.Intersects(""fleet"", ""[-117 30],[-112 36]"", func(key, val string) bool {  		...  		return true  	})  	return nil  })  ```    This will get all three positions.    ### k-Nearest Neighbors    Use the `Nearby` function to get all the positions in order of nearest to farthest :    ```go  db.View(func(tx *buntdb.Tx) error {  	tx.Nearby(""fleet"", ""[-113 33]"", func(key, val string, dist float64) bool {  		...  		return true  	})  	return nil  })  ```    ### Spatial bracket syntax    The bracket syntax `[-117 30],[-112 36]` is unique to BuntDB, and it's how the built-in rectangles are processed. But, you are not limited to this syntax. Whatever Rect function you choose to use during `CreateSpatialIndex` will be used to process the parameter, in this case it's `IndexRect`.    - **2D rectangle:** `[10 15],[20 25]`  *Min XY: ""10x15"", Max XY: ""20x25""*    - **3D rectangle:** `[10 15 12],[20 25 18]`  *Min XYZ: ""10x15x12"", Max XYZ: ""20x25x18""*    - **2D point:** `[10 15]`  *XY: ""10x15""*    - **LonLat point:** `[-112.2693 33.5123]`  *LatLon: ""33.5123 -112.2693""*    - **LonLat bounding box:** `[-112.26 33.51],[-112.18 33.67]`  *Min LatLon: ""33.51 -112.26"", Max LatLon: ""33.67 -112.18""*    **Notice:** The longitude is the Y axis and is on the left, and latitude is the X axis and is on the right.    You can also represent `Infinity` by using `-inf` and `+inf`.  For example, you might have the following points (`[X Y M]` where XY is a point and M is a timestamp):  ```  [3 9 1]  [3 8 2]  [4 8 3]  [4 7 4]  [5 7 5]  [5 6 6]  ```    You can then do a search for all points with `M` between 2-4 by calling `Intersects`.    ```go  tx.Intersects(""points"", ""[-inf -inf 2],[+inf +inf 4]"", func(key, val string) bool {  	println(val)  	return true  })  ```    Which will return:    ```  [3 8 2]  [4 8 3]  [4 7 4]  ```    ## JSON Indexes  Indexes can be created on individual fields inside JSON documents. BuntDB uses [GJSON](https://github.com/tidwall/gjson) under the hood.    For example:    ```go  package main    import (  	""fmt""    	""github.com/tidwall/buntdb""  )    func main() {  	db, _ := buntdb.Open("":memory:"")  	db.CreateIndex(""last_name"", ""*"", buntdb.IndexJSON(""name.last""))  	db.CreateIndex(""age"", ""*"", buntdb.IndexJSON(""age""))  	db.Update(func(tx *buntdb.Tx) error {  		tx.Set(""1"", `{""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}`, nil)  		tx.Set(""2"", `{""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}`, nil)  		tx.Set(""3"", `{""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}`, nil)  		tx.Set(""4"", `{""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}`, nil)  		return nil  	})  	db.View(func(tx *buntdb.Tx) error {  		fmt.Println(""Order by last name"")  		tx.Ascend(""last_name"", func(key, value string) bool {  			fmt.Printf(""%s: %s\n"", key, value)  			return true  		})  		fmt.Println(""Order by age"")  		tx.Ascend(""age"", func(key, value string) bool {  			fmt.Printf(""%s: %s\n"", key, value)  			return true  		})  		fmt.Println(""Order by age range 30-50"")  		tx.AscendRange(""age"", `{""age"":30}`, `{""age"":50}`, func(key, value string) bool {  			fmt.Printf(""%s: %s\n"", key, value)  			return true  		})  		return nil  	})  }  ```    Results:    ```  Order by last name  3: {""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}  4: {""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}  1: {""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}  2: {""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}    Order by age  4: {""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}  1: {""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}  2: {""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}  3: {""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}    Order by age range 30-50  1: {""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}  2: {""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}  ```    ## Multi Value Index  With BuntDB it's possible to join multiple values on a single index.  This is similar to a [multi column index](http://dev.mysql.com/doc/refman/5.7/en/multiple-column-indexes.html) in a traditional SQL database.    In this example we are creating a multi value index on ""name.last"" and ""age"":    ```go  db, _ := buntdb.Open("":memory:"")  db.CreateIndex(""last_name_age"", ""*"", buntdb.IndexJSON(""name.last""), buntdb.IndexJSON(""age""))  db.Update(func(tx *buntdb.Tx) error {  	tx.Set(""1"", `{""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}`, nil)  	tx.Set(""2"", `{""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}`, nil)  	tx.Set(""3"", `{""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}`, nil)  	tx.Set(""4"", `{""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}`, nil)  	tx.Set(""5"", `{""name"":{""first"":""Sam"",""last"":""Anderson""},""age"":51}`, nil)  	tx.Set(""6"", `{""name"":{""first"":""Melinda"",""last"":""Prichard""},""age"":44}`, nil)  	return nil  })  db.View(func(tx *buntdb.Tx) error {  	tx.Ascend(""last_name_age"", func(key, value string) bool {  		fmt.Printf(""%s: %s\n"", key, value)  		return true  	})  	return nil  })    // Output:  // 5: {""name"":{""first"":""Sam"",""last"":""Anderson""},""age"":51}  // 3: {""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}  // 4: {""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}  // 1: {""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}  // 6: {""name"":{""first"":""Melinda"",""last"":""Prichard""},""age"":44}  // 2: {""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}  ```    ## Descending Ordered Index  Any index can be put in descending order by wrapping it's less function with `buntdb.Desc`.    ```go  db.CreateIndex(""last_name_age"", ""*"",      buntdb.IndexJSON(""name.last""),      buntdb.Desc(buntdb.IndexJSON(""age"")),  )  ```    This will create a multi value index where the last name is ascending and the age is descending.    ## Collate i18n Indexes    Using the external [collate package](https://github.com/tidwall/collate) it's possible to create  indexes that are sorted by the specified language. This is similar to the [SQL COLLATE keyword](https://msdn.microsoft.com/en-us/library/ms174596.aspx) found in traditional databases.    To install:    ```  go get -u github.com/tidwall/collate  ```    For example:    ```go  import ""github.com/tidwall/collate""    // To sort case-insensitive in French.  db.CreateIndex(""name"", ""*"", collate.IndexString(""FRENCH_CI""))    // To specify that numbers should sort numerically (""2"" < ""12"")  // and use a comma to represent a decimal point.  db.CreateIndex(""amount"", ""*"", collate.IndexString(""FRENCH_NUM""))  ```    There's also support for Collation on JSON indexes:    ```go  db.CreateIndex(""last_name"", ""*"", collate.IndexJSON(""CHINESE_CI"", ""name.last""))  ```    Check out the [collate project](https://github.com/tidwall/collate) for more information.    ## Data Expiration  Items can be automatically evicted by using the `SetOptions` object in the `Set` function to set a `TTL`.    ```go  db.Update(func(tx *buntdb.Tx) error {  	tx.Set(""mykey"", ""myval"", &buntdb.SetOptions{Expires:true, TTL:time.Second})  	return nil  })  ```    Now `mykey` will automatically be deleted after one second. You can remove the TTL by setting the value again with the same key/value, but with the options parameter set to nil.    ## Delete while iterating  BuntDB does not currently support deleting a key while in the process of iterating.  As a workaround you'll need to delete keys following the completion of the iterator.    ```go  var delkeys []string  tx.AscendKeys(""object:*"", func(k, v string) bool {  	if someCondition(k) == true {  		delkeys = append(delkeys, k)  	}  	return true // continue  })  for _, k := range delkeys {  	if _, err = tx.Delete(k); err != nil {  		return err  	}  }  ```    ## Append-only File    BuntDB uses an AOF (append-only file) which is a log of all database changes that occur from operations like `Set()` and `Delete()`.    The format of this file looks like:  ```  set key:1 value1  set key:2 value2  set key:1 value3  del key:2  ...  ```    When the database opens again, it will read back the aof file and process each command in exact order.  This read process happens one time when the database opens.  From there on the file is only appended.    As you may guess this log file can grow large over time.  There's a background routine that automatically shrinks the log file when it gets too large.  There is also a `Shrink()` function which will rewrite the aof file so that it contains only the items in the database.  The shrink operation does not lock up the database so read and write transactions can continue while shrinking is in process.    ### Durability and fsync    By default BuntDB executes an `fsync` once every second on the [aof file](#append-only-file). Which simply means that there's a chance that up to one second of data might be lost. If you need higher durability then there's an optional database config setting `Config.SyncPolicy` which can be set to `Always`.    The `Config.SyncPolicy` has the following options:    - `Never` - fsync is managed by the operating system, less safe  - `EverySecond` - fsync every second, fast and safer, this is the default  - `Always` - fsync after every write, very durable, slower    ## Config    Here are some configuration options that can be use to change various behaviors of the database.    - **SyncPolicy** adjusts how often the data is synced to disk. This value can be Never, EverySecond, or Always. Default is EverySecond.  - **AutoShrinkPercentage** is used by the background process to trigger a shrink of the aof file when the size of the file is larger than the percentage of the result of the previous shrunk file. For example, if this value is 100, and the last shrink process resulted in a 100mb file, then the new aof file must be 200mb before a shrink is triggered. Default is 100.  - **AutoShrinkMinSize** defines the minimum size of the aof file before an automatic shrink can occur. Default is 32MB.  - **AutoShrinkDisabled** turns off automatic background shrinking. Default is false.    To update the configuration you should call `ReadConfig` followed by `SetConfig`. For example:    ```go    var config buntdb.Config  if err := db.ReadConfig(&config); err != nil{  	log.Fatal(err)  }  if err := db.SetConfig(config); err != nil{  	log.Fatal(err)  }  ```    ## Performance    How fast is BuntDB?    Here are some example [benchmarks](https://github.com/tidwall/raft-buntdb#raftstore-performance-comparison) when using BuntDB in a Raft Store implementation.    You can also run the standard Go benchmark tool from the project root directory:    ```  go test --bench=.  ```    ### BuntDB-Benchmark    There's a [custom utility](https://github.com/tidwall/buntdb-benchmark) that was created specifically for benchmarking BuntDB.    *These are the results from running the benchmarks on a MacBook Pro 15"" 2.8 GHz Intel Core i7:*    ```  $ buntdb-benchmark -q  GET: 4609604.74 operations per second  SET: 248500.33 operations per second  ASCEND_100: 2268998.79 operations per second  ASCEND_200: 1178388.14 operations per second  ASCEND_400: 679134.20 operations per second  ASCEND_800: 348445.55 operations per second  DESCEND_100: 2313821.69 operations per second  DESCEND_200: 1292738.38 operations per second  DESCEND_400: 675258.76 operations per second  DESCEND_800: 337481.67 operations per second  SPATIAL_SET: 134824.60 operations per second  SPATIAL_INTERSECTS_100: 939491.47 operations per second  SPATIAL_INTERSECTS_200: 561590.40 operations per second  SPATIAL_INTERSECTS_400: 306951.15 operations per second  SPATIAL_INTERSECTS_800: 159673.91 operations per second  ```    To install this utility:    ```  go get github.com/tidwall/buntdb-benchmark  ```        ## Contact  Josh Baker [@tidwall](http://twitter.com/tidwall)    ## License    BuntDB source code is available under the MIT [License](/LICENSE). """
Big data;https://github.com/SnappyDataInc/snappydata;"""<span style=""background-color:yellow"">  This repository is provided for legacy users and informational purposes only. It may contain security vulnerabilities in the code itself or its dependencies. TIBCO provides no updates, including security updates, to this code. Consistent with the terms of the Apache License 2.0 that apply to the TIBCO code in this repository, the code is provided on an ""as is"" basis, without any warranties or conditions of any kind and in no event and under no legal theory shall TIBCO be liable to you for damages arising as a result of the use or inability to use the code.  </span>      ## Introduction   SnappyData (aka TIBCO ComputeDB) is a distributed, in-memory optimized analytics database. SnappyData delivers high throughput, low latency, and high concurrency for unified analytics workload. By fusing an in-memory hybrid database inside Apache Spark, it provides analytic query processing, mutability/transactions, access to virtually all big data sources and stream processing all in one unified cluster.    One common use case for SnappyData is to provide analytics at interactive speeds over large volumes of data with minimal or no pre-processing of the dataset. For instance, there is no need to often pre-aggregate/reduce or generate cubes over your large data sets for ad-hoc visual analytics. This is made possible by smartly managing data in-memory, dynamically generating code using vectorization optimizations and maximizing the potential of modern multi-core CPUs.  SnappyData enables complex processing on large data sets in sub-second timeframes.     ![SnappyData Positioning](docs/Images/Snappy_intro.1.png)    !!!Note  	*SnappyData is not another Enterprise Data Warehouse (EDW) platform, but rather a high performance computational and caching cluster that augments traditional EDWs and data lakes.*    ### Important Capabilities    *	**Easily discover and catalog big data sets**</br>  	You can connect and discover datasets in SQL DBs, Hadoop, NoSQL stores, file systems, or even cloud data stores such as S3 by using SQL, infer schemas automatically and register them in a secure catalog. A wide variety of data formats are supported out of the box such as JSON, CSV, text, Objects, Parquet, ORC, SQL, XML, and more.    *	**Rich connectivity**</br>  	SnappyData is built with Apache Spark inside. Therefore, any data store that has an Apache Spark connector can be accessed using SQL or by using the Apache Spark RDD/Dataset API. Virtually all modern data stores do have Apache Spark connector. See [Apache Spark Packages](https://spark-packages.org/). You can also dynamically deploy connectors to a running SnappyData cluster.    *	**Virtual or in-memory data**</br>  	You can decide which datasets need to be provisioned into distributed memory or left at the source. When the data is left at source, after being modeled as a virtual/external tables, the analytic query processing is parallelized, and the query fragments are pushed down wherever possible and executed at high speed.  When speed is essential, applications can selectively copy the external data into memory using a single SQL command.    *	**In-memory Columnar + Row store** </br>  	You can choose in-memory data to be stored in any of the following forms:      *	**Columnar**: The form that is compressed and designed for scanning/aggregating large data sets.      *	**Row store**: The form that has an extremely fast key access or highly selective access.  	The columnar store is automatically indexed using a skipping index. Applications can explicitly add indexes for the row store.    *	**High performance** </br>  	When data is loaded, the engine parallelizes all the accesses by carefully taking into account the available distributed cores, the available memory, and whether the source data can be partitioned to deliver extremely high-speed loading. Therefore, unlike a traditional warehouse, you can bring up SnappyData whenever required, load, process, and tear it down. Query processing uses code generation and vectorization techniques to shift the processing to the modern-day multi-core processor and L1/L2/L3 caches to the possible extent.    *	**Flexible rich data transformations** </br>  	External data sets when discovered automatically through schema inference will have the schema of the source. Users can cleanse, blend, reshape data using a SQL function library (Apache Spark SQL+) or even submit Apache Spark jobs and use custom logic. The entire rich Apache Spark API is at your disposal. This logic can be written in SQL, Java, Scala, or even Python.*    *	**Prepares data for data science**</br>   	Through the use of apache Apache Spark API for statistics and machine learning, raw or curated datasets can be easily prepared for machine learning. You can understand the statistical characteristics such as correlation, independence of different variables and so on. You can generate distributed feature vectors from your data that is by using processes such as one-hot encoder, binarizer, and a range of functions built into the Apache Spark ML library. These features can be stored back into column tables and shared across a group of users with security and avoid dumping copies to disk, which is slow and error-prone.     *	**Stream ingestion and liveness** </br>  	While it is common to see query service engines today, most resort to periodic refreshing of data sets from the source as the managed data cannot be mutated â€” for example query engines such as Presto, HDFS formats like parquet, etc. Moreover, when updates can be applied pre-processing, re-shaping of the data is not necessarily simple.      In SnappyData, operational systems can feed data updates through Kafka to SnappyData. The incoming data can be CDC(Change-data-capture) events (insert, updates, or deletes) and can be easily ingested into in-memory tables with ease, consistency, and exactly-once semantics. The Application can apply custom logic to do sophisticated transformations and get the data ready for analytics. This incremental and continuous process is far more efficient than batch refreshes. Refer [Stream Processing with SnappyData](docs/howto/use_stream_processing_with_snappydata.md) </br>      *	**Approximate Query Processing(AQP)** </br>  	When dealing with huge data sets, for example, IoT sensor streaming time-series data, it may not be possible to provision the data in-memory, and if left at the source (say Hadoop or S3) your analytic query processing can take too long. In SnappyData, you can create one or more stratified data samples on the full data set. The query engine automatically uses these samples for aggregation queries, and a nearly accurate answer returned to clients. This can be immensely valuable when visualizing a trend, plotting a graph or bar chart. Refer [AQP](docs/sde/index.md).    *	**Access from anywhere** </br>  	You can use JDBC, ODBC, REST, or any of the Apache Spark APIs. The product is fully compatible with Apache Spark 2.1.1. SnappyData natively supports modern visualization tools such as [TIBCO Spotfire](docs/howto/connecttibcospotfire.md), [Tableau](docs/howto/tableauconnect.md), and [Qlikview](docs/setting_up_jdbc_driver_qlikview.md). Refer       ## Downloading and Installing SnappyData  You can download and install the latest version of SnappyData from [github](https://github.com/TIBCOSoftware/snappydata/releases).  Refer to the [documentation](docs/install/index.md) for installation steps.    ## Getting Started  Multiple options are provided to get started with SnappyData. Easiest way to get going with SnappyData is on your laptop. You can also use any of the following options:    *	On-premise clusters    *	AWS    *	Docker  *	Kubernetes    You can find more information on options for running SnappyData [here](docs/quickstart/index.md).    ## Quick Test to Measure Performance of SnappyData vs Apache Spark    If you are already using Apache Spark, you can experience upto 20x speedup for your query performance with SnappyData. Try this [test](https://github.com/TIBCOSoftware/snappydata/blob/master/examples/quickstart/scripts/Quickstart.scala) using the Spark Shell.    ## Documentation  To understand SnappyData and its features refer to the [documentation](http://tibcosoftware.github.io/snappydata/).    ### Other Relevant content  - [Paper](http://cidrdb.org/cidr2017/papers/p28-mozafari-cidr17.pdf) on Snappydata at Conference on Innovative Data Systems Research (CIDR) - Info on key concepts and motivating problems.  - [Another early Paper](https://www.snappydata.io/snappy-industrial) that focuses on overall architecture, use cases, and benchmarks. ACM Sigmod 2016.  - [TPC-H benchmark](https://www.snappydata.io/whitepapers/snappydata-tpch) comparing Apache Spark with SnappyData  - Checkout the [SnappyData blog](https://www.snappydata.io/blog) for developer content  -	[TIBCO community page](https://community.tibco.com/products/tibco-computedb) for the latest info.    ## Community Support    We monitor the following channels comments/questions:    *	[Stackoverflow](http://stackoverflow.com/questions/tagged/snappydata) ![Stackoverflow](http://i.imgur.com/LPIdp12.png)    *	[Slack](http://snappydata-slackin.herokuapp.com/) ![Slack](http://i.imgur.com/h3sc6GM.png)    *	[Gitter](https://gitter.im/SnappyDataInc/snappydata) ![Gitter](http://i.imgur.com/jNAJeOn.jpg)    *	[Mailing List](https://groups.google.com/forum/#!forum/snappydata-user) ![Mailing List](http://i.imgur.com/YomdH4s.png)    *	[Reddit](https://www.reddit.com/r/snappydata) ![Reddit](http://i.imgur.com/AB3cVtj.png)              *	[JIRA](https://jira.snappydata.io/projects/SNAP/issues) ![JIRA](http://i.imgur.com/E92zntA.png)    ## Link with SnappyData Distribution    ### Using Maven Dependency    SnappyData artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates:    ```  groupId: io.snappydata  artifactId: snappydata-cluster_2.11  version: 1.3.0  ```    ### Using SBT Dependency    If you are using SBT, add this line to your **build.sbt** for core SnappyData artifacts:    ```  libraryDependencies += ""io.snappydata"" % ""snappydata-core_2.11"" % ""1.3.0""  ```    For additions related to SnappyData cluster, use:    ```  libraryDependencies += ""io.snappydata"" % ""snappydata-cluster_2.11"" % ""1.3.0""  ```    You can find more specific SnappyData artifacts [here](http://mvnrepository.com/artifact/io.snappydata)    !!!Note  	If your project fails when resolving the above dependency (that is, it fails to download `javax.ws.rs#javax.ws.rs-api;2.1`), it may be due an issue with its pom file. </br> As a workaround, you can add the below code to your **build.sbt**:    ```  val workaround = {    sys.props += ""packaging.type"" -> ""jar""    ()  }  ```    For more details, refer [https://github.com/sbt/sbt/issues/3618](https://github.com/sbt/sbt/issues/3618).      ## Building from Source  If you would like to build SnappyData from source, refer to the [documentation on building from source](docs/install/building_from_source.md).      ## How is SnappyData Different than Apache Spark?    Apache Spark is a general purpose parallel computational engine for analytics at scale. At its core, it has a batch design center and is capable of working with disparate data sources. While this provides rich unified access to data, this can also be quite inefficient and expensive. Analytic processing requires massive data sets to be repeatedly copied and data to be reformatted to suit Apache Spark. In many cases, it ultimately fails to deliver the promise of interactive analytic performance.  For instance, each time an aggregation is run on a large Cassandra table, it necessitates streaming the entire table into Apache Spark to do the aggregation. Caching within Apache Spark is immutable and results in stale insight.    ### The SnappyData Approach    ##### Snappy Architecture    ![SnappyData Architecture](docs/Images/SnappyArchitecture.png)    SnappyData takes a different approach. SnappyData fuses a low latency, highly available in-memory transactional database ((Pivotal GemFire/Apache Geode) into Apache Spark with shared memory management and optimizations. Data can be managed in columnar form similar to Apache Spark caching or in a row oriented manner commonly used in popular relational databases like postgres). But, many query engine operators are significantly more optimized through better vectorization, code generation and indexing. </br>  The net effect is, an order of magnitude performance improvement when compared to native Apache Spark caching, and more than two orders of magnitude better performance when Apache Spark is used in conjunction with external data sources.  Apache Spark is turned into an in-memory operational database capable of transactions, point reads, writes, working with Streams (Apache Spark) and running analytic SQL queries without losing the computational richness in Apache Spark.      ## Streaming Example - Ad Analytics  Here is a stream + Transactions + Analytics use case example to illustrate the SQL as well as the Apache Spark programming approaches in SnappyData - [Ad Analytics code example](https://github.com/TIBCOSoftware/snappy-poc). Here is a [screencast](https://www.youtube.com/watch?v=bXofwFtmHjE) that showcases many useful features of SnappyData. The example also goes through a benchmark comparing SnappyData to a Hybrid in-memory database and Cassandra.    ## Contributing to SnappyData    If you are interested in contributing, please visit the [community page](http://www.snappydata.io/community) for ways in which you can help.   """
Big data;https://github.com/nathanmarz/elephantdb;"""[![Build Status](https://travis-ci.org/nathanmarz/elephantdb.png?branch=develop)](https://travis-ci.org/nathanmarz/elephantdb)    # ElephantDB 0.5.1 (cascalog 2.x)    ## ElephantDB 0.4.5 (cascalog 1.x)    # About    ElephantDB is a database that specializes in exporting key/value data  from Hadoop. ElephantDB is composed of two components. The first is a  library that is used in MapReduce jobs for creating an indexed  key/value dataset that is stored on a distributed filesystem. The  second component is a daemon that can download a subset of a dataset  and serve it in a read-only, random-access fashion. A group of  machines working together to serve a full dataset is called a ring.    Since ElephantDB server doesn't support random writes, it is almost  laughingly simple. Once the server loads up its subset of the data, it  does very little. This leads to ElephantDB being rock-solid in  production, since there's almost no moving parts.    ElephantDB server has a Thrift interface, so any language can make  reads from it. The database itself is implemented in Clojure.    An ElephantDB datastore contains a fixed number of shards of a ""Local  Persistence"". ElephantDB's local persistence engine is pluggable, and  ElephantDB comes bundled with local persistence implementations for  Berkeley DB Java Edition and LevelDB. On the MapReduce side, each  reducer creates or updates a single shard into the DFS, and on the  server side, each server serves a subset of the shards.    ElephantDB support hot-swapping so that a live server can be updated  with a new set of shards without downtime.    # Questions    Google group: [elephantdb-user](http://groups.google.com/group/elephantdb-user)    # Introduction    [Introduction to ElephantDB](https://speakerdeck.com/sorenmacbeth/introduction-to-elephantdb)    # Tutorials    TODO: Write an updated tutorial for ElephantDB 0.4.x    # Using ElephantDB in MapReduce Jobs    ElephantDB is hosted at [Clojars](http://clojars.org/elephantdb).  Clojars is a maven repo that is trivially easy to use with maven or  leiningen. You should use this dependency when using ElephantDB within  your MapReduce jobs to create ElephantDB datastores. ElephantDB  contains a module elephantdb-cascading which allows you to easily create  datastores from your Cascading workflows. elephantdb-cascalog is available  for use with [Cascalog](http://github.com/nathanmarz/cascalog) >= 1.10.1.    # Deploying ElephantDB server    TODO: Documentation on how to deploy ElephantDB.    # Running the EDB Jar    TODO: Documentation on how to run ElephantDB """
Big data;https://github.com/linkedin/ambry;"""# Ambry    [![Github Actions CI](https://github.com/linkedin/ambry/actions/workflows/github-actions.yml/badge.svg)](https://github.com/linkedin/ambry/actions/workflows/github-actions.yml)  [![codecov.io](https://codecov.io/github/linkedin/ambry/branch/master/graph/badge.svg)](https://codecov.io/github/linkedin/ambry)  [![license](https://img.shields.io/github/license/linkedin/ambry.svg)](LICENSE)    Ambry is a distributed object store that supports storage of trillion of small immutable objects (50K -100K) as well as billions of large objects. It was specifically designed to store and serve media objects in web companies. However, it can be used as a general purpose storage system to store DB backups, search indexes or business reports. The system has the following characterisitics:     1. Highly available and horizontally scalable  2. Low latency and high throughput  3. Optimized for both small and large objects  4. Cost effective  5. Easy to use    Requires at least JDK 1.8.    ## Documentation  Detailed documentation is available at https://github.com/linkedin/ambry/wiki    ## Research  Paper introducing Ambry at [SIGMOD 2016](http://sigmod2016.org/) -> http://dprg.cs.uiuc.edu/data/files/2016/ambry.pdf    Reach out to us at ambrydev@googlegroups.com if you would like us to list a paper that is based off of research on Ambry.    ## Getting Started  ##### Step 1: Download the code, build it and prepare for deployment.  To get the latest code and build it, do        $ git clone https://github.com/linkedin/ambry.git       $ cd ambry      $ ./gradlew allJar      $ cd target      $ mkdir logs  Ambry uses files that provide information about the cluster to route requests from the frontend to servers and for replication between servers. We will use a simple clustermap that contains a single server with one partition. The partition will use `/tmp` as the mount point.  ##### Step 2: Deploy a server.      $ nohup java -Dlog4j.configuration=file:../config/log4j.properties -jar ambry.jar --serverPropsFilePath ../config/server.properties --hardwareLayoutFilePath ../config/HardwareLayout.json --partitionLayoutFilePath ../config/PartitionLayout.json > logs/server.log &    Through this command, we configure the log4j properties, provide the server with configuration options and cluster definitions and redirect output to a log. Note down the process ID returned (`serverProcessID`) because it will be needed for shutdown.    The log will be available at `logs/server.log`. Alternately, you can change the log4j properties to write the log messages to a file instead of standard output.  ##### Step 3: Deploy a frontend.      $ nohup java -Dlog4j.configuration=file:../config/log4j.properties -cp ""*"" com.github.ambry.frontend.AmbryFrontendMain --serverPropsFilePath ../config/frontend.properties --hardwareLayoutFilePath ../config/HardwareLayout.json --partitionLayoutFilePath ../config/PartitionLayout.json > logs/frontend.log &    Note down the process ID returned (`frontendProcessID`) because it will be needed for shutdown. Make sure that the frontend is ready to receive requests.        $ curl http://localhost:1174/healthCheck      GOOD  The log will be available at `logs/frontend.log`. Alternately, you can change the log4j properties to write the log messages to a file instead of standard output.  ##### Step 4: Interact with Ambry !  We are now ready to store and retrieve data from Ambry. Let us start by storing a simple image. For demonstration purposes, we will use an image `demo.gif` that has been copied into the `target` folder.  ###### POST      $ curl -i -H ""x-ambry-service-id:CUrlUpload""  -H ""x-ambry-owner-id:`whoami`"" -H ""x-ambry-content-type:image/gif"" -H ""x-ambry-um-description:Demonstration Image"" http://localhost:1174/ --data-binary @demo.gif      HTTP/1.1 201 Created      Location: AmbryID      Content-Length: 0  The CUrl command creates a `POST` request that contains the binary data in demo.gif. Along with the file data, we provide headers that act as blob properties. These include the size of the blob, the service ID, the owner ID and the content type.    In addition to these properties, Ambry also has a provision for arbitrary user defined metadata. We provide `x-ambry-um-description` as user metadata. Ambry does not interpret this data and it is purely for user annotation.  The `Location` header in the response is the blob ID of the blob we just uploaded.  ###### GET - Blob Info  Now that we stored a blob, let us verify some properties of the blob we uploaded.        $ curl -i http://localhost:1174/AmbryID/BlobInfo      HTTP/1.1 200 OK      x-ambry-blob-size: {Blob size}      x-ambry-service-id: CUrlUpload      x-ambry-creation-time: {Creation time}      x-ambry-private: false      x-ambry-content-type: image/gif      x-ambry-owner-id: {username}      x-ambry-um-desc: Demonstration Image      Content-Length: 0  ###### GET - Blob  Now that we have verified that Ambry returns properties correctly, let us obtain the actual blob.        $ curl http://localhost:1174/AmbryID > demo-downloaded.gif      $ diff demo.gif demo-downloaded.gif       $  This confirms that the data that was sent in the `POST` request matches what we received in the `GET`. If you would like to see the image, simply point your browser to `http://localhost:1174/AmbryID` and you should see the image that was uploaded !  ###### DELETE  Ambry is an immutable store and blobs cannot be updated but they can be deleted in order to make them irretrievable. Let us go ahead and delete the blob we just created.        $ curl -i -X DELETE http://localhost:1174/AmbryID      HTTP/1.1 202 Accepted      Content-Length: 0  You will no longer be able to retrieve the blob properties or data.        $ curl -i http://localhost:1174/AmbryID/BlobInfo      HTTP/1.1 410 Gone      Content-Type: text/plain; charset=UTF-8      Content-Length: 17      Connection: close        Failure: 410 Gone  ##### Step 5: Stop the frontend and server.      $ kill -15 frontendProcessID      $ kill -15 serverProcessID  You can confirm that the services have been shut down by looking at the logs.  ##### Additional information:  In addition to the simple APIs demonstrated above, Ambry provides support for `GET` of only user metadata and `HEAD`. In addition to the `POST` of binary data that was demonstrated, Ambry also supports `POST` of `multipart/form-data` via CUrl or web forms.  Other features of interest include:  * **Time To Live (TTL)**: During `POST`, a TTL in seconds can be provided through the addition of a header named `x-ambry-ttl`. This means that Ambry will stop serving the blob after the TTL has expired. On `GET`, expired blobs behave the same way as deleted blobs.  * **Private**: During `POST`, providing a header named `x-ambry-private` with the value `true` will mark the blob as private. API behavior can be configured based on whether a blob is public or private. """
Big data;https://github.com/deeplearning4j/rl4j;"""# RL4J: Reinforcement Learning for Java    For support questions regarding RL4J, please contact help@pathmind.com.    RL4J is a reinforcement learning framework integrated with deeplearning4j and released under an Apache 2.0 open-source license.     * DQN (Deep Q Learning with double DQN)  * Async RL (A3C, Async NStepQlearning)    Both for Low-Dimensional (array of info) and high-dimensional (pixels) input.      ![DOOM](docs/images/doom.gif)      ![Cartpole](docs/images/cartpole.gif)    A useful blog post to introduce you to reinforcement learning, DQN and Async RL:    [Blog post](https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html)      # Quickstart    * mvn install    # Visualisation    [webapp-rl4j](https://github.com/rubenfiszel/webapp-rl4j)    # Doom    Doom is not ready yet but you can make it work if you feel adventurous with some additional steps:    * You will need vizdoom, compile the native lib and move it into the root of your project in a folder  * export MAVEN_OPTS=-Djava.library.path=THEFOLDEROFTHELIB  * mvn compile exec:java -Dexec.mainClass=""YOURMAINCLASS""    # Malmo (Minecraft)    ![Malmo](docs/images/malmo.gif)    * Download and unzip Malmo from [here](https://github.com/Microsoft/malmo/releases)  * export MALMO_HOME=YOURMALMO_FOLDER  * export MALMO_XSD_PATH=$MALMO_HOME/Schemas  * launch malmo per [instructions](https://github.com/Microsoft/malmo#launching-minecraft-with-our-mod)    # WIP    * Documentation  * Serialization/Deserialization (load save)  * Compression of pixels in order to store 1M state in a reasonnable amount of memory  * Async learning: A3C and nstep learning (requires some missing features from dl4j (calc and apply gradients)).    # Author    [Ruben Fiszel](http://rubenfiszel.github.io/)  """
Big data;https://github.com/harthur/brain;"""*This project has reached the end of its development as a simple neural network library. Feel free to browse the code, but please use other JavaScript neural network libraries in development like [brain.js](https://github.com/BrainJS/brain.js) and [convnetjs](https://github.com/karpathy/convnetjs).*    # brain    `brain` is a JavaScript [neural network](http://neuralnetworksanddeeplearning.com/) library. Here's an example of using it to approximate the XOR function:    ```javascript  var net = new brain.NeuralNetwork();    net.train([{input: [0, 0], output: [0]},             {input: [0, 1], output: [1]},             {input: [1, 0], output: [1]},             {input: [1, 1], output: [0]}]);    var output = net.run([1, 0]);  // [0.987]  ```    There's no reason to use a neural network to figure out XOR however (-: so here's a more involved, realistic example:  [Demo: training a neural network to recognize color contrast](http://harthur.github.com/brain/)    ## Using in node  If you have [node](http://nodejs.org/) you can install with [npm](http://npmjs.org):    ```  npm install brain  ```    ## Using in the browser  Download the latest [brain.js](https://github.com/harthur/brain/tree/gh-pages). Training is computationally expensive, so you should try to train the network offline (or on a Worker) and use the `toFunction()` or `toJSON()` options to plug the pre-trained network in to your website.    ## Training  Use `train()` to train the network with an array of training data. The network has to be trained with all the data in bulk in one call to `train()`. The more training patterns, the longer it will probably take to train, but the better the network will be at classifiying new patterns.    #### Data format  Each training pattern should have an `input` and an `output`, both of which can be either an array of numbers from `0` to `1` or a hash of numbers from `0` to `1`. For the [color constrast demo](http://harthur.github.com/brain/) it looks something like this:    ```javascript  var net = new brain.NeuralNetwork();    net.train([{input: { r: 0.03, g: 0.7, b: 0.5 }, output: { black: 1 }},             {input: { r: 0.16, g: 0.09, b: 0.2 }, output: { white: 1 }},             {input: { r: 0.5, g: 0.5, b: 1.0 }, output: { white: 1 }}]);    var output = net.run({ r: 1, g: 0.4, b: 0 });  // { white: 0.99, black: 0.002 }  ```    #### Options  `train()` takes a hash of options as its second argument:    ```javascript  net.train(data, {    errorThresh: 0.005,  // error threshold to reach    iterations: 20000,   // maximum training iterations    log: true,           // console.log() progress periodically    logPeriod: 10,       // number of iterations between logging    learningRate: 0.3    // learning rate  })  ```    The network will train until the training error has gone below the threshold (default `0.005`) or the max number of iterations (default `20000`) has been reached, whichever comes first.    By default training won't let you know how its doing until the end, but set `log` to `true` to get periodic updates on the current training error of the network. The training error should decrease every time. The updates will be printed to console. If you set `log` to a function, this function will be called with the updates instead of printing to the console.    The learning rate is a parameter that influences how quickly the network trains. It's a number from `0` to `1`. If the learning rate is close to `0` it will take longer to train. If the learning rate is closer to `1` it will train faster but it's in danger of training to a local minimum and performing badly on new data. The default learning rate is `0.3`.    #### Output  The output of `train()` is a hash of information about how the training went:    ```javascript  {    error: 0.0039139985510105032,  // training error    iterations: 406                // training iterations  }  ```    #### Failing  If the network failed to train, the error will be above the error threshold. This could happen because the training data is too noisy (most likely), the network doesn't have enough hidden layers or nodes to handle the complexity of the data, or it hasn't trained for enough iterations.    If the training error is still something huge like `0.4` after 20000 iterations, it's a good sign that the network can't make sense of the data you're giving it.    ## JSON  Serialize or load in the state of a trained network with JSON:    ```javascript  var json = net.toJSON();    net.fromJSON(json);  ```    You can also get a custom standalone function from a trained network that acts just like `run()`:    ```javascript  var run = net.toFunction();    var output = run({ r: 1, g: 0.4, b: 0 });    console.log(run.toString()); // copy and paste! no need to import brain.js  ```    ## Options  `NeuralNetwork()` takes a hash of options:    ```javascript  var net = new brain.NeuralNetwork({    hiddenLayers: [4],    learningRate: 0.6 // global learning rate, useful when training using streams  });  ```    #### hiddenLayers  Specify the number of hidden layers in the network and the size of each layer. For example, if you want two hidden layers - the first with 3 nodes and the second with 4 nodes, you'd give:    ```  hiddenLayers: [3, 4]  ```    By default `brain` uses one hidden layer with size proportionate to the size of the input array.    ## Streams  The network now has a [WriteStream](http://nodejs.org/api/stream.html#stream_class_stream_writable). You can train the network by using `pipe()` to send the training data to the network.    #### Example  Refer to `stream-example.js` for an example on how to train the network with a stream.    #### Initialization  To train the network using a stream you must first create the stream by calling `net.createTrainStream()` which takes the following options:    * `floodCallback()` - the callback function to re-populate the stream. This gets called on every training iteration.  * `doneTrainingCallback(info)` - the callback function to execute when the network is done training. The `info` param will contain a hash of information about how the training went:    ```javascript  {    error: 0.0039139985510105032,  // training error    iterations: 406                // training iterations  }  ```    #### Transform  Use a [Transform](http://nodejs.org/api/stream.html#stream_class_stream_transform) to coerce the data into the correct format. You might also use a Transform stream to normalize your data on the fly. """
Big data;https://github.com/BIDData/BIDMach;"""    BIDMach is a very fast machine learning library. Check the latest <b><a href=""https://github.com/BIDData/BIDMach/wiki/Benchmarks"">benchmarks</a></b>    The github distribution contains source code only. You also need a jdk 8, an installation of NVIDIA CUDA 8.0 (if you want to use a GPU) and CUDNN 5 if you plan to use deep networks. For building you need <a href=""https://maven.apache.org/docs/history.html"">maven 3.X</a>.    After doing <code>git clone</code>, cd to the BIDMach directory, and build and install the jars with <code>mvn install</code>. You can then run bidmach with `./bidmach`. More details on installing and running are available <b><a href=""https://github.com/BIDData/BIDMach/wiki/Installing-and-Running"">here</a></b>.    The main project page is <b><a href=""http://bid2.berkeley.edu/bid-data-project/"">here</a></b>.    Documentation is <b><a href=""https://github.com/BIDData/BIDMach/wiki"">here in the wiki</a></b>    <b>New</b> BIDMach has a <b><a href=""https://groups.google.com/forum/#!forum/bidmach-users-group"">discussion group</a></b> on Google Groups.    BIDMach is a sister project of BIDMat, a matrix library, which is   <b><a href=""https://github.com/BIDData/BIDMat"">also on github</a></b>    BIDData also has a project for deep reinforcement learning. <b><a href=""https://github.com/BIDData/BIDMach_RL"">BIDMach_RL</a></b> contains state-of-the-art implementations of several reinforcement learning algorithms. """
Big data;https://github.com/influxdata/kapacitor;"""# Kapacitor [![Circle CI](https://circleci.com/gh/influxdata/kapacitor/tree/master.svg?style=svg&circle-token=78c97422cf89526309e502a290c230e8a463229f)](https://circleci.com/gh/influxdata/kapacitor/tree/master) [![Docker pulls](https://img.shields.io/docker/pulls/library/kapacitor.svg)](https://hub.docker.com/_/kapacitor/)  Open source framework for processing, monitoring, and alerting on time series data    # Installation    Kapacitor has two binaries:    * kapacitor â€“ a CLI program for calling the Kapacitor API.  * kapacitord â€“ the Kapacitor server daemon.    You can either download the binaries directly from the [downloads](https://influxdata.com/downloads/#kapacitor) page or go get them:    ```sh  go get github.com/influxdata/kapacitor/cmd/kapacitor  go get github.com/influxdata/kapacitor/cmd/kapacitord  ```    # Configuration  An example configuration file can be found [here](https://github.com/influxdata/kapacitor/blob/master/etc/kapacitor/kapacitor.conf)    Kapacitor can also provide an example config for you using this command:    ```sh  kapacitord config  ```      # Getting Started    This README gives you a high level overview of what Kapacitor is and what its like to use it. As well as some details of how it works.  To get started using Kapacitor see [this guide](https://docs.influxdata.com/kapacitor/latest/introduction/getting-started/). After you finish the getting started exercise you can check out the [TICKscripts](https://github.com/influxdata/kapacitor/tree/master/examples/telegraf) for different Telegraf plugins.    # Basic Example    Kapacitor uses a DSL named [TICKscript](https://docs.influxdata.com/kapacitor/latest/tick/) to define tasks.    A simple TICKscript that alerts on high cpu usage looks like this:    ```javascript  stream      |from()          .measurement('cpu_usage_idle')          .groupBy('host')      |window()          .period(1m)          .every(1m)      |mean('value')      |eval(lambda: 100.0 - ""mean"")          .as('used')      |alert()          .message('{{ .Level}}: {{ .Name }}/{{ index .Tags ""host"" }} has high cpu usage: {{ index .Fields ""used"" }}')          .warn(lambda: ""used"" > 70.0)          .crit(lambda: ""used"" > 85.0)            // Send alert to hander of choice.            // Slack          .slack()          .channel('#alerts')            // VictorOps          .victorOps()          .routingKey('team_rocket')            // PagerDuty          .pagerDuty()  ```    Place the above script into a file `cpu_alert.tick` then run these commands to start the task:    ```sh  # Define the task (assumes cpu data is in db 'telegraf')  kapacitor define \      cpu_alert \      -type stream \      -dbrp telegraf.default \      -tick ./cpu_alert.tick  # Start the task  kapacitor enable cpu_alert  ``` """
Big data;https://github.com/gojek/feast;"""<!--Do not modify this file. It is auto-generated from a template (infra/templates/README.md.jinja2)-->    <p align=""center"">      <a href=""https://feast.dev/"">        <img src=""docs/assets/feast_logo.png"" width=""550"">      </a>  </p>  <br />    [![unit-tests](https://github.com/feast-dev/feast/actions/workflows/unit_tests.yml/badge.svg?branch=master&event=push)](https://github.com/feast-dev/feast/actions/workflows/unit_tests.yml)  [![integration-tests-and-build](https://github.com/feast-dev/feast/actions/workflows/master_only.yml/badge.svg?branch=master&event=push)](https://github.com/feast-dev/feast/actions/workflows/master_only.yml)  [![java-integration-tests](https://github.com/feast-dev/feast/actions/workflows/java_master_only.yml/badge.svg?branch=master&event=push)](https://github.com/feast-dev/feast/actions/workflows/java_master_only.yml)  [![linter](https://github.com/feast-dev/feast/actions/workflows/linter.yml/badge.svg?branch=master&event=push)](https://github.com/feast-dev/feast/actions/workflows/linter.yml)  [![Docs Latest](https://img.shields.io/badge/docs-latest-blue.svg)](https://docs.feast.dev/)  [![Python API](https://img.shields.io/readthedocs/feast/master?label=Python%20API)](http://rtd.feast.dev/)  [![License](https://img.shields.io/badge/License-Apache%202.0-blue)](https://github.com/feast-dev/feast/blob/master/LICENSE)  [![GitHub Release](https://img.shields.io/github/v/release/feast-dev/feast.svg?style=flat&sort=semver&color=blue)](https://github.com/feast-dev/feast/releases)    ## Overview    Feast is an open source feature store for machine learning. Feast is the fastest path to productionizing analytic data for model training and online inference.    Please see our [documentation](https://docs.feast.dev/) for more information about the project.    ## ðŸ“ Architecture  ![](docs/assets/feast-marchitecture.png)    The above architecture is the minimal Feast deployment. Want to run the full Feast on Snowflake/GCP/AWS? Click [here](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws).    ## ðŸ£ Getting Started    ### 1. Install Feast  ```commandline  pip install feast  ```    ### 2. Create a feature repository  ```commandline  feast init my_feature_repo  cd my_feature_repo  ```    ### 3. Register your feature definitions and set up your feature store  ```commandline  feast apply  ```    ### 4. Explore your data in the web UI (experimental)    ![Web UI](ui/sample.png)    ### 5. Build a training dataset  ```python  from feast import FeatureStore  import pandas as pd  from datetime import datetime    entity_df = pd.DataFrame.from_dict({      ""driver_id"": [1001, 1002, 1003, 1004],      ""event_timestamp"": [          datetime(2021, 4, 12, 10, 59, 42),          datetime(2021, 4, 12, 8,  12, 10),          datetime(2021, 4, 12, 16, 40, 26),          datetime(2021, 4, 12, 15, 1 , 12)      ]  })    store = FeatureStore(repo_path=""."")    training_df = store.get_historical_features(      entity_df=entity_df,      features = [          'driver_hourly_stats:conv_rate',          'driver_hourly_stats:acc_rate',          'driver_hourly_stats:avg_daily_trips'      ],  ).to_df()    print(training_df.head())    # Train model  # model = ml.fit(training_df)  ```  ```commandline              event_timestamp  driver_id  conv_rate  acc_rate  avg_daily_trips  0 2021-04-12 08:12:10+00:00       1002   0.713465  0.597095              531  1 2021-04-12 10:59:42+00:00       1001   0.072752  0.044344               11  2 2021-04-12 15:01:12+00:00       1004   0.658182  0.079150              220  3 2021-04-12 16:40:26+00:00       1003   0.162092  0.309035              959    ```    ### 6. Load feature values into your online store  ```commandline  CURRENT_TIME=$(date -u +""%Y-%m-%dT%H:%M:%S"")  feast materialize-incremental $CURRENT_TIME  ```    ```commandline  Materializing feature view driver_hourly_stats from 2021-04-14 to 2021-04-15 done!  ```    ### 7. Read online features at low latency  ```python  from pprint import pprint  from feast import FeatureStore    store = FeatureStore(repo_path=""."")    feature_vector = store.get_online_features(      features=[          'driver_hourly_stats:conv_rate',          'driver_hourly_stats:acc_rate',          'driver_hourly_stats:avg_daily_trips'      ],      entity_rows=[{""driver_id"": 1001}]  ).to_dict()    pprint(feature_vector)    # Make prediction  # model.predict(feature_vector)  ```  ```json  {      ""driver_id"": [1001],      ""driver_hourly_stats__conv_rate"": [0.49274],      ""driver_hourly_stats__acc_rate"": [0.92743],      ""driver_hourly_stats__avg_daily_trips"": [72]  }  ```    ## ðŸ“¦ Functionality and Roadmap    The list below contains the functionality that contributors are planning to develop for Feast    * Items below that are in development (or planned for development) will be indicated in parentheses.  * We welcome contribution to all items in the roadmap!  * Want to influence our roadmap and prioritization? Submit your feedback to [this form](https://docs.google.com/forms/d/e/1FAIpQLSfa1nRQ0sKz-JEFnMMCi4Jseag\_yDssO\_3nV9qMfxfrkil-wA/viewform).  * Want to speak to a Feast contributor? We are more than happy to jump on a call. Please schedule a time using [Calendly](https://calendly.com/d/x2ry-g5bb/meet-with-feast-team).    * **Data Sources**    * [x] [Snowflake source](https://docs.feast.dev/reference/data-sources/snowflake)    * [x] [Redshift source](https://docs.feast.dev/reference/data-sources/redshift)    * [x] [BigQuery source](https://docs.feast.dev/reference/data-sources/bigquery)    * [x] [Parquet file source](https://docs.feast.dev/reference/data-sources/file)    * [x] [Synapse source (community plugin)](https://github.com/Azure/feast-azure)    * [x] [Hive (community plugin)](https://github.com/baineng/feast-hive)    * [x] [Postgres (community plugin)](https://github.com/nossrannug/feast-postgres)    * [x] [Spark (community plugin)](https://github.com/Adyen/feast-spark-offline-store)    * [x] Kafka source (with [push support into the online store](https://docs.feast.dev/reference/alpha-stream-ingestion))    * [ ] HTTP source  * **Offline Stores**    * [x] [Snowflake](https://docs.feast.dev/reference/offline-stores/snowflake)    * [x] [Redshift](https://docs.feast.dev/reference/offline-stores/redshift)    * [x] [BigQuery](https://docs.feast.dev/reference/offline-stores/bigquery)    * [x] [Synapse (community plugin)](https://github.com/Azure/feast-azure)    * [x] [Hive (community plugin)](https://github.com/baineng/feast-hive)    * [x] [Postgres (community plugin)](https://github.com/nossrannug/feast-postgres)    * [x] [Trino (community plugin)](https://github.com/Shopify/feast-trino)    * [x] [Spark (community plugin)](https://github.com/Adyen/feast-spark-offline-store)    * [x] [In-memory / Pandas](https://docs.feast.dev/reference/offline-stores/file)    * [x] [Custom offline store support](https://docs.feast.dev/how-to-guides/adding-a-new-offline-store)  * **Online Stores**    * [x] [DynamoDB](https://docs.feast.dev/reference/online-stores/dynamodb)    * [x] [Redis](https://docs.feast.dev/reference/online-stores/redis)    * [x] [Datastore](https://docs.feast.dev/reference/online-stores/datastore)    * [x] [SQLite](https://docs.feast.dev/reference/online-stores/sqlite)    * [x] [Azure Cache for Redis (community plugin)](https://github.com/Azure/feast-azure)    * [x] [Postgres (community plugin)](https://github.com/nossrannug/feast-postgres)    * [x] [Custom online store support](https://docs.feast.dev/how-to-guides/adding-support-for-a-new-online-store)    * [ ] Bigtable (in progress)    * [ ] Cassandra  * **Streaming**    * [x] [Custom streaming ingestion job support](https://docs.feast.dev/how-to-guides/creating-a-custom-provider)    * [x] [Push based streaming data ingestion](reference/alpha-stream-ingestion.md)    * [ ] Streaming ingestion on AWS    * [ ] Streaming ingestion on GCP  * **Feature Engineering**    * [x] On-demand Transformations (Alpha release. See [RFC](https://docs.google.com/document/d/1lgfIw0Drc65LpaxbUu49RCeJgMew547meSJttnUqz7c/edit#))    * [ ] Batch transformation (SQL. In progress. See [RFC](https://docs.google.com/document/d/1964OkzuBljifDvkV-0fakp2uaijnVzdwWNGdz7Vz50A/edit))    * [ ] Streaming transformation  * **Deployments**    * [x] AWS Lambda (Alpha release. See [RFC](https://docs.google.com/document/d/1eZWKWzfBif66LDN32IajpaG-j82LSHCCOzY6R7Ax7MI/edit))    * [x] Kubernetes (See [guide](https://docs.feast.dev/how-to-guides/running-feast-in-production#4.3.-java-based-feature-server-deployed-on-kubernetes))    * [ ] Cloud Run    * [ ] KNative  * **Feature Serving**    * [x] Python Client    * [x] REST Feature Server (Python) (Alpha release. See [RFC](https://docs.google.com/document/d/1iXvFhAsJ5jgAhPOpTdB3j-Wj1S9x3Ev\_Wr6ZpnLzER4/edit))    * [x] gRPC Feature Server (Java) (See [#1497](https://github.com/feast-dev/feast/issues/1497))    * [x] Push API    * [ ] Java Client    * [ ] Go Client    * [ ] Delete API    * [ ] Feature Logging (for training)  * **Data Quality Management (See [RFC](https://docs.google.com/document/d/110F72d4NTv80p35wDSONxhhPBqWRwbZXG4f9mNEMd98/edit))**    * [x] Data profiling and validation (Great Expectations)    * [ ] Training-serving skew detection (in progress)    * [ ] Metric production    * [ ] Drift detection  * **Feature Discovery and Governance**    * [x] Python SDK for browsing feature registry    * [x] CLI for browsing feature registry    * [x] Model-centric feature tracking (feature services)    * [x] Amundsen integration (see [Feast extractor](https://github.com/amundsen-io/amundsen/blob/main/databuilder/databuilder/extractor/feast_extractor.py))    * [ ] Feast Web UI (in progress)    * [ ] REST API for browsing feature registry    * [ ] Feature versioning      ## ðŸŽ“ Important Resources    Please refer to the official documentation at [Documentation](https://docs.feast.dev/)   * [Quickstart](https://docs.feast.dev/getting-started/quickstart)   * [Tutorials](https://docs.feast.dev/tutorials/tutorials-overview)   * [Running Feast with Snowflake/GCP/AWS](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws)   * [Change Log](https://github.com/feast-dev/feast/blob/master/CHANGELOG.md)   * [Slack (#Feast)](https://slack.feast.dev/)    ## ðŸ‘‹ Contributing  Feast is a community project and is still under active development. Please have a look at our contributing and development guides if you want to contribute to the project:  - [Contribution Process for Feast](https://docs.feast.dev/project/contributing)  - [Development Guide for Feast](https://docs.feast.dev/project/development-guide)  - [Development Guide for the Main Feast Repository](./CONTRIBUTING.md)    ## âœ¨ Contributors    Thanks goes to these incredible people:    <a href=""https://github.com/feast-dev/feast/graphs/contributors"">    <img src=""https://contrib.rocks/image?repo=feast-dev/feast"" />  </a>"""
Big data;https://github.com/gchq/Gaffer;"""Copyright 2016-2020 Crown Copyright    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License.    <img src=""logos/logoWithText.png"" width=""300"">    Gaffer  ======    Gaffer is a graph database framework. It allows the storage of very large graphs containing rich properties on the nodes and edges. Several storage options are available, including Accumulo, Hbase and Parquet.    It is designed to be as flexible, scalable and extensible as possible, allowing for rapid prototyping and transition to production systems.    Gaffer offers:     - Rapid query across very large numbers of nodes and edges;   - Continual ingest of data at very high data rates, and batch bulk ingest of data via MapReduce or Spark;   - Storage of arbitrary Java objects on the nodes and edges;   - Automatic, user-configurable in-database aggregation of rich statistical properties (e.g. counts, histograms, sketches) on the nodes and edges;   - Versatile query-time summarisation, filtering and transformation of data;   - Fine grained data access controls;   - Hooks to apply policy and compliance rules to queries;   - Automated, rule-based removal of data (typically used to age-off old data);   - Retrieval of graph data into Apache Spark for fast and flexible analysis;   - A fully-featured REST API.    To get going with Gaffer, visit our [getting started pages](https://gchq.github.io/gaffer-doc/summaries/getting-started.html).    Gaffer is under active development. Version 1.0 of Gaffer was released in October 2017.    License  -------    Gaffer is licensed under the Apache 2 license and is covered by [Crown Copyright](https://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/copyright-and-re-use/crown-copyright/).    Getting Started  ---------------    ### Try it out    We have a demo available to try that is based around a small uk road use dataset. See the example/road-traffic [README](https://github.com/gchq/Gaffer/blob/master/example/road-traffic/README.md) to try it out.    ### Building and Deploying    To build Gaffer run `mvn clean install -Pquick` in the top-level directory. This will build all of Gaffer's core libraries and some examples of how to load and query data.    See our [Store](https://gchq.github.io/gaffer-doc/summaries/stores.html) documentation page for a list of available Gaffer Stores to chose from and the relevant documentation for each.    ### Inclusion in other projects    Gaffer is hosted on [Maven Central](https://mvnrepository.com/search?q=uk.gov.gchq.gaffer) and can easily be incorporated into your own maven projects.    To use Gaffer from the Java API the only required dependencies are the Gaffer graph module and a store module for the specific database technology used to store the data, e.g. for the Accumulo store:    ```  <dependency>      <groupId>uk.gov.gchq.gaffer</groupId>      <artifactId>graph</artifactId>      <version>${gaffer.version}</version>  </dependency>  <dependency>      <groupId>uk.gov.gchq.gaffer</groupId>      <artifactId>accumulo-store</artifactId>      <version>${gaffer.version}</version>  </dependency>  ```    This will include all other mandatory dependencies. Other (optional) components can be added to your project as required.    ### Documentation    Our Javadoc can be found [here](http://gchq.github.io/Gaffer/).    We have some user guides in our [docs](https://gchq.github.io/gaffer-doc/getting-started/user-guide/contents.html).    Related repositories  --------------------    The [gaffer-tools](https://github.com/gchq/gaffer-tools) repository contains useful tools to help work with Gaffer. These include:    - `jar-shader` - Used to shade the version of Jackson to avoid incompatibility problems on CDH clusters;  - `mini-accumulo-cluster` - Allows a mini Accumulo cluster to be spun up for testing purposes;  - `performance-testing` - Methods of testing the performance of ingest and query operations against a graph;  - `python-shell` - Allows operations against a graph to be executed from a Python shell;  - `random-element-generation` - Code to generate large volumes of random graph data;  - `schema-builder` - A (beta) visual tool for writing schemas for a graph;  - `slider` - Code to deploy a Gaffer cluster to a YARN cluster using [Apache Slider](https://slider.incubator.apache.org/), including the ability to easily run Slider on an [AWS EMR cluster](https://aws.amazon.com/emr/);  - `ui` - A basic graph visualisation tool.    Contributing  ------------    We welcome contributions to the project. Detailed information on our ways of working can be found [here](https://gchq.github.io/gaffer-doc/other/ways-of-working.html). In brief:    - Sign the [GCHQ Contributor Licence Agreement](https://cla-assistant.io/gchq/Gaffer);  - Push your changes to a fork;  - Submit a pull request. """
Big data;https://github.com/bayandin/awesome-awesomeness;"""# Awesome Awesomeness    A curated list of amazingly awesome awesomeness.  - Programming Languages Package Manager      - [Package-Manager](https://github.com/damon-kwok/awesome-package-manager)    - Programming Languages  	- [Ada(Spark)](https://github.com/ohenley/awesome-ada)	  	- [Ansible](https://github.com/jdauphant/awesome-ansible)  	- [AutoHotkey](https://github.com/ahkscript/awesome-AutoHotkey)  	- [AutoIt](https://github.com/J2TeaM/awesome-AutoIt)  	- [C](https://notabug.org/koz.ross/awesome-c)  	- [C/C++](https://github.com/fffaraz/awesome-cpp)  	- [CMake](https://github.com/onqtam/awesome-cmake)  	- Clojure  		- [by @mbuczko](https://github.com/mbuczko/awesome-clojure)  		- [by @razum2um](https://github.com/razum2um/awesome-clojure)  	- [ColdFusion](https://github.com/seancoyne/awesome-coldfusion)  	- Common Lisp  		- [Common Lisp Libraries](https://github.com/CodyReichert/awesome-cl)  		- [Learning Common Lisp](https://github.com/GustavBertram/awesome-common-lisp-learning-list)  	- [Coronavirus](https://github.com/soroushchehresa/awesome-coronavirus)  	- [Crystal](https://github.com/veelenga/awesome-crystal)  	- [D](https://github.com/zhaopuming/awesome-d)  	- [Delphi](https://github.com/Fr0sT-Brutal/awesome-delphi)  	- [Elixir](https://github.com/h4cc/awesome-elixir)  	- [Elm](https://github.com/isRuslan/awesome-elm)  	- Erlang  		- [by @0xAX](https://github.com/0xAX/erlang-bookmarks)  		- [by @drobakowski](https://github.com/drobakowski/awesome-erlang)  		- [by @unbalancedparentheses](https://github.com/unbalancedparentheses/spawnedshelter)  	- [F#](https://github.com/fsprojects/awesome-fsharp)  	- [Fortran](https://github.com/rabbiabram/awesome-fortran)  	- [Go](https://github.com/avelino/awesome-go)  	- [Go Patterns](https://github.com/tmrts/go-patterns)  	- [Groovy](https://github.com/kdabir/awesome-groovy)  	- [Haskell](https://github.com/krispo/awesome-haskell)  	- [Idris](https://github.com/joaomilho/awesome-idris)  	- [Java](https://github.com/akullpp/awesome-java)  	- [JavaScript](https://github.com/sorrycc/awesome-javascript)  		- [Angular 2](https://github.com/AngularClass/awesome-angular)  		- [Ember.js](https://github.com/nmec/awesome-ember)  		- [JavaScript Learning Resources](https://github.com/micromata/awesome-javascript-learning)  		- [Koa](https://github.com/ellerbrock/awesome-koa)  		- [Node.js](https://github.com/sindresorhus/awesome-nodejs)  			- [Cross-platform Node.js](https://github.com/bcoe/awesome-cross-platform-nodejs)  			- [Node ESM](https://github.com/talentlessguy/awesome-node-esm)  		- [React](https://github.com/enaqx/awesome-react)  		- [Svelte](https://github.com/flagello/awesome-sveltejs)  		- [VueJS](https://github.com/vuejs/awesome-vue)  	- [Julia](https://github.com/svaksha/Julia.jl)  	- [Kotlin](https://github.com/KotlinBy/awesome-kotlin)  	- [Kotlin/Native](https://github.com/bipinvaylu/awesome-kotlin-native)  	- Lua  		- [by @forhappy](https://github.com/forhappy/awesome-lua)  		- [by @lewisjellis](https://github.com/LewisJEllis/awesome-lua)  	- [MongoDB](https://github.com/ramnes/awesome-mongodb)  	- [MySQL](https://github.com/shlomi-noach/awesome-mysql)  	- .NET  	        - [by @mehdihadeli](https://github.com/mehdihadeli/awesome-dotnet-core-education)  		- [by @quozd](https://github.com/quozd/awesome-dotnet)  		- [by @tallesl](https://github.com/tallesl/net-libraries-that-make-your-life-easier)  		- [by @thangchung](https://github.com/thangchung/awesome-dotnet-core)  	- [Nim](https://github.com/VPashkov/awesome-nim)  	- [OCaml](https://github.com/ocaml-community/awesome-ocaml)  	- [Perl](https://github.com/hachiojipm/awesome-perl)  	- [PHP](https://github.com/ziadoz/awesome-php)  		- [CakePHP](https://github.com/FriendsOfCake/awesome-cakephp)  	- [Postgres](https://github.com/dhamaniasad/awesome-postgres)  	- Python  		- [by @kirang89](https://github.com/kirang89/pycrumbs)  		- [by @svaksha](https://github.com/svaksha/pythonidae)  		- [by @trekhleb](https://github.com/trekhleb/learn-python)  		- [by @vinta](https://github.com/vinta/awesome-python)  		- [awesome-python-in-education](https://github.com/quobit/awesome-python-in-education)  	- [R](https://github.com/qinwf/awesome-R)  	- Ruby  		- [by @dreikanter](https://github.com/dreikanter/ruby-bookmarks)  		- [by @markets](https://github.com/markets/awesome-ruby)  		- [by @Sdogruyol](https://github.com/Sdogruyol/awesome-ruby)  		- [by @asyraffff](https://github.com/asyraffff/Open-Source-Ruby-and-Rails-Apps)  	- [Rust](https://github.com/rust-unofficial/awesome-rust)  	- [SAS](https://github.com/huyingjie/awesome-SAS)  	- [Scala](https://github.com/lauris/awesome-scala)  	- [Shell](https://github.com/alebcay/awesome-shell)  	- Swift  		- [by @matteocrippa](https://github.com/matteocrippa/awesome-swift)  		- [by @MaxChen](https://github.com/MaxChen/awesome-swift-and-tutorial-resources)  		- [by @Wolg](https://github.com/Wolg/awesome-swift)  		- [from ZEEF by @Edubits](https://swift.zeef.com/robin.eggenkamp)  	- TypeScript  		- [by @brookshi](https://github.com/brookshi/awesome-typescript-projects)  		- [by @dzharii](https://github.com/dzharii/awesome-typescript)  		- [by @ellerbrock](https://github.com/ellerbrock/awesome-typescript)  	- [V](https://github.com/vlang/awesome-v)    - General  	- [.htaccess](https://github.com/phanan/htaccess)  	- Accessibility  		- [by @a11yproject](https://github.com/a11yproject/a11yproject.com)  		- [by @brunopulis](https://github.com/brunopulis/awesome-a11y)  	- [Agile](https://github.com/lorabv/awesome-agile)  	- [Algolia](https://github.com/algolia/awesome-algolia)  	- [Algorithms](https://github.com/tayllan/awesome-algorithms)  		- [Algorithms Visualisation](https://github.com/enjalot/algovis)  		- [Big O Notation](https://github.com/okulbilisim/awesome-big-o)  	- [Amazon Web Services](https://github.com/donnemartin/awesome-aws)  	- [Analytics](https://github.com/onurakpolat/awesome-analytics)  	- [Android](https://github.com/JStumpp/awesome-android)  		- [Android Apps](https://github.com/LinuxCafeFederation/awesome-android)  		- [Android Release Notes](https://github.com/pedronveloso/awesome-android-release-notes)  		- [Android Security](https://github.com/ashishb/android-security-awesome)  		- [Android UI](https://github.com/wasabeef/awesome-android-ui)  	- [ARM Exploitation](https://github.com/HenryHoggard/awesome-arm-exploitation)  	- [Software Architecture](https://github.com/simskij/awesome-software-architecture)  	- [Arduino](https://github.com/Lembed/Awesome-arduino)  	- [Artificial intelligence](https://github.com/owainlewis/awesome-artificial-intelligence)  	- API  		- [by @Kikobeats](https://github.com/Kikobeats/awesome-api)  		- [by @toddmotto](https://github.com/toddmotto/public-apis)  	- [Apple](https://github.com/joeljfischer/awesome-apple)  		- [OS X](https://github.com/iCHAIT/awesome-macOS)  		- [OS X and iOS Security](https://github.com/ashishb/osx-and-ios-security-awesome)  	- [Beacons](https://github.com/beaconinside/awesome-beacon)  	- Big data  		- [by @onurakpolat](https://github.com/onurakpolat/awesome-bigdata)  		- [by @zenkay](https://github.com/zenkay/bigdata-ecosystem)  		- [Hadoop](https://github.com/youngwookim/awesome-hadoop)  	- [Blazor](https://github.com/AdrienTorris/awesome-blazor)  	- Blockchain  		- [by @0xtokens](https://github.com/0xtokens/awesome-blockchain)  		- [by @imbaniac](https://github.com/imbaniac/awesome-blockchain)  		- [by @coderplex](https://github.com/coderplex/awesome-blockchain)  		- [by @hitripod](https://github.com/hitripod/awesome-blockchain)  		- [by @iNiKe](https://github.com/iNiKe/awesome-blockchain)  		- [by @igorbarinov](https://github.com/igorbarinov/awesome-blockchain)  		- [by @istinspring](https://github.com/istinspring/awesome-blockchain)  		- [by @openblockchains](https://github.com/openblockchains/awesome-blockchains)  		- [by @kennethreitz](https://github.com/kennethreitz/awesome-coins)  		- [awesome-token-sale](https://github.com/holographicio/awesome-token-sale)  		- Bitcoin  			- [by @btcbrdev](https://github.com/btcbrdev/awesome-btcdev)  			- [by @igorbarinov](https://github.com/igorbarinov/awesome-bitcoin)  			- [Bitcoin Payment Processors](https://github.com/alexk111/awesome-bitcoin-payment-processors)  		- Ethereum  			- [by @vinsgo](https://github.com/vinsgo/awesome-ethereum)  			- [awesome-ethereum-virtual-machine](https://github.com/pirapira/awesome-ethereum-virtual-machine)  			- [by @Tom2718](https://github.com/Tom2718/Awesome-Ethereum)  		- [Ripple](https://github.com/vhpoet/awesome-ripple)  	- [Boilerplates](https://github.com/melvin0008/awesome-projects-boilerplates)  	- Books  		- [Free Programming Books](https://github.com/EbookFoundation/free-programming-books)  		- [Free Software Testing Books](https://github.com/ligurio/free-software-testing-books)  		- [Mind Expanding Books](https://github.com/hackerkid/Mind-Expanding-Books)  	- [Bootstrap](https://github.com/therebelrobot/awesome-bootstrap)  	- [BSD Software](https://github.com/SaintFenix/Awesome-BSD-Ports-Programs-And-Projects)  	- [Building Blocks for Web Apps](https://github.com/componently-com/awesome-building-blocks-for-web-apps)  	- [Web Effect](https://github.com/lindelof/awesome-web-effect)  	- [Landing Page](https://github.com/nordicgiant2/awesome-landing-page)  	- [Captcha](https://github.com/ZYSzys/awesome-captcha)  	- [Challenges](https://github.com/mauriciovieira/awesome-challenges)  	- [Code Formatters](https://github.com/rishirdua/awesome-code-formatters)  	- [Community Detection](https://github.com/benedekrozemberczki/awesome-community-detection)  	- [Competitive Programming](https://github.com/lnishan/awesome-competitive-programming)  	- [Computer Vision](https://github.com/jbhuang0604/awesome-computer-vision)  	- [Conferences](https://github.com/RichardLitt/awesome-conferences)  	- [Continuous Delivery](https://github.com/ciandcd/awesome-ciandcd)  	- [Conversational UI](https://github.com/mortenjust/awesome-conversational/)  	- [Cordova](https://github.com/busterc/awesome-cordova)  	- [Courses](https://github.com/prakhar1989/awesome-courses)  	- [Creative Commons Media](https://github.com/shime/creative-commons-media)  	- Cryptography  		- [by @MaciejCzyzewski](https://github.com/MaciejCzyzewski/retter)  		- [by @sobolevn](https://github.com/sobolevn/awesome-cryptography)  		- [by @coinpride](https://github.com/coinpride/CryptoList)  	- [Crypto Papers](https://github.com/pFarb/awesome-crypto-papers)  	- [CSS](https://github.com/sotayamashita/awesome-css)  		- [CSS Frameworks](https://github.com/troxler/awesome-css-frameworks)  	- [Data Science](https://github.com/bulutyazilim/awesome-datascience)  		- Data Science with Python  		  	- [by @r0f1](https://github.com/r0f1/datascience)  			- [by @krzjoa](https://github.com/krzjoa/awesome-python-data-science)  	- [Data Visualization](https://github.com/fasouto/awesome-dataviz)  	- [Database](https://github.com/numetriclabz/awesome-db)  		- [SQLAlchemy](https://github.com/dahlia/awesome-sqlalchemy)  	- Datasets  		- [by @caesar0301](https://github.com/caesar0301/awesome-public-datasets)  		- [by @leomaurodesenv](https://github.com/leomaurodesenv/game-datasets)  	- Deep Learning  		- [by @ChristosChristofidis](https://github.com/ChristosChristofidis/awesome-deep-learning)  		- [by @guillaume-chevalier](https://github.com/guillaume-chevalier/awesome-deep-learning-resources)  		- [by @tigerneil](https://github.com/tigerneil/awesome-deep-rl)  		- [by @nerox8664](https://github.com/nerox8664/awesome-computer-vision-models)  	- [Decision Tree Papers](https://github.com/benedekrozemberczki/awesome-decision-tree-papers)  	- [Design Patterns](https://github.com/DovAmir/awesome-design-patterns)  	- [Design Tools](https://github.com/LisaDziuba/Awesome-Design-Tools)  	- [Design](https://github.com/gztchan/awesome-design)  	- [Dev Env](https://github.com/jondot/awesome-devenv)  	- [DevOps](https://github.com/joubertredrat/awesome-devops)  	- [DevSecOps](https://github.com/TaptuIT/awesome-devsecops)  	- [Django](https://github.com/wsvincent/awesome-django)  	- [Docker](https://github.com/veggiemonk/awesome-docker)  	- [Documentation](https://github.com/PharkMillups/beautiful-docs)  	- [Dotfiles](https://github.com/webpro/awesome-dotfiles)  	- [Electron](https://github.com/sindresorhus/awesome-electron)  	- [Emacs](https://github.com/emacs-tw/awesome-emacs)  	- [Embedded](https://github.com/nhivp/Awesome-Embedded)  	- [Ethics](https://github.com/HussainAther/awesome-ethics)  	- [Falsehood](https://github.com/kdeldycke/awesome-falsehood)  	- [FastAPI](https://github.com/mjhea0/awesome-fastapi)  	- [FIRST Robotics Competition](https://github.com/andrewda/awesome-frc)  	- [Flask](https://github.com/mjhea0/awesome-flask)  	- [FluidApp Resources](https://github.com/lborgav/awesome-fluidapp)  	- [Flutter](https://github.com/Solido/awesome-flutter)  	- [Fonts](https://github.com/brabadu/awesome-fonts)  	- [Free Open Source Software (FOSS)](https://github.com/ishanvyas22/awesome-open-source-systems)  	- [Fraud Detection Papers](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers)  	- [Free Services](https://github.com/ripienaar/free-for-dev)  	- Frontend  		- [by @dypsilon](https://github.com/dypsilon/frontend-dev-bookmarks)  		- [by @moklick](https://github.com/moklick/frontend-stuff)  	- [Game Development](https://github.com/ellisonleao/magictools)  	- [Games](https://github.com/leereilly/games)  	- GIF  		- [by @Kikobeats](https://github.com/Kikobeats/awesome-gif)  	- [Gists](https://github.com/vsouza/awesome-gists)  	- [Git](https://github.com/dictcp/awesome-git)  	- [GitHub](https://github.com/Kikobeats/awesome-github)  		- [Browser extensions for GitHub](https://github.com/stefanbuck/awesome-browser-extensions-for-github)  		- [GitHub - Chinese](https://github.com/AntBranch/awesome-github)  	- [Gradient Boosting Papers](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers)  	- [Graph Classification](https://github.com/benedekrozemberczki/awesome-graph-classification)  	- [GraphQL](https://github.com/chentsulin/awesome-graphql)  	- [Growth Hacking](https://github.com/btomashvili/awesome-growth-hacking)  	- Guides  		- [by @narkoz](https://github.com/narkoz/guides)  		- [by @RichardLitt](https://github.com/RichardLitt/awesome-styleguides)  	- Hacking  		- [by @carpedm20](https://github.com/carpedm20/awesome-hacking)  		- [by @Hack-with-Github](https://github.com/Hack-with-Github/Awesome-Hacking)  	- [HTML5](https://github.com/diegocard/awesome-html5)  	- [Honeypots](https://github.com/paralax/awesome-honeypots)  	- [Hyper](https://github.com/bnb/awesome-hyper)  	- [Incident Response](https://github.com/meirwah/awesome-incident-response)  	- [Images](https://github.com/heyalexej/awesome-images)  	- [Image coloring](https://github.com/oskar-j/awesome-image-coloring)  	- [Internationalization](https://github.com/jpomykala/awesome-i18n)  	- [Internet of Things (IOT)](https://github.com/HQarroum/awesome-iot)  	- [iOS](https://github.com/vsouza/awesome-ios)  		- [Cocoa Controls](https://github.com/v-braun/awesome-cocoa)  		- [Open Source Apps](https://github.com/dkhamsing/open-source-ios-apps)  		- [UI](https://github.com/cjwirth/awesome-ios-ui)  	- [JSON](https://github.com/burningtree/awesome-json)  	- [Jupyter](https://github.com/markusschanta/awesome-jupyter)  	- [JVM](https://github.com/deephacks/awesome-jvm)  	- [Kafka](https://github.com/monksy/awesome-kafka)  	- [Koans](https://github.com/ahmdrefat/awesome-koans)  	- [Laravel](https://github.com/chiraggude/awesome-laravel)  	- [Leadership and Management](https://github.com/LappleApple/awesome-leading-and-managing)  	- [Lego](https://github.com/adius/awesome-lego)  	- [Linux Containers](https://github.com/Friz-zy/awesome-linux-containers)  	- [Linux resources](https://github.com/itech001/awesome-linux-resources)  	- Lists  		- [by @bayandin](https://github.com/bayandin/awesome-awesomeness)  		- [by @jnv](https://github.com/jnv/lists)  		- [by @sindresorhus](https://github.com/sindresorhus/awesome)  	- [Mac]  		- [by @xyNNN](https://github.com/xyNNN/awesome-mac)  		- [by @justin-j](https://github.com/justin-j/awesome-mac-apps)  	- [Machine Learning](https://github.com/josephmisiti/awesome-machine-learning)  	- [Malware Analysis](https://github.com/rshipp/awesome-malware-analysis)  	- [Material Design](https://github.com/sachin1092/awesome-material)  	- [Math](https://github.com/rossant/awesome-math)  	- [Matlab](https://github.com/mikecroucher/awesome-MATLAB)  	- [Mental Health](https://github.com/dreamingechoes/awesome-mental-health)  	- [micro:bit](https://github.com/carlosperate/awesome-microbit)  	- [MLOps](https://github.com/kelvins/awesome-mlops)  	- [Mobile marketing and development](https://github.com/alec-c4/awesome-mobile)  	- [Mobile Web Development](https://github.com/myshov/awesome-mobile-web-development)  	- [Monitoring](https://github.com/crazy-canux/awesome-monitoring)  		- [Prometheus](https://github.com/roaldnefs/awesome-prometheus)  		- [Prometheus alerting rules](https://github.com/samber/awesome-prometheus-alerts)          - [Monte Carlo Tree Search Papers](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers)  	- [Motion Design for Web](https://github.com/lucasmaiaesilva/awesome-motion-design-web)  	- [Nginx](https://github.com/fcambus/nginx-resources)  	- Newsletters  		- [by @vredniy](https://github.com/vredniy/awesome-newsletters)  		- [by @webpro](https://github.com/webpro/awesome-newsletters)  		- [by @mpron](https://github.com/mpron/awesome-newsletters)  	- [No Login Web Apps](https://github.com/aviaryan/awesome-no-login-web-apps)  	- [Open Science](https://github.com/silky/awesome-open-science)  	- [Open Source Photography](https://github.com/ibaaj/awesome-OpenSourcePhotography)  	- [Papers](https://github.com/papers-we-love/papers-we-love)  	- [Podcasts](https://github.com/Ghosh/awesome-podcasts)  	- [Philosophy](https://github.com/HussainAther/awesome-philosophy)  	- [Pipelines](https://github.com/pditommaso/awesome-pipeline)  	- [Product Manager](https://github.com/hugo53/awesome-ProductManager)  	- Protocols  		- [OSC](https://github.com/amir-arad/awesome-osc) (open sound control)  	- [Pentest Cheat Sheets](https://github.com/coreb1t/awesome-pentest-cheat-sheets)  	- [Quick Look Plugins](https://github.com/sindresorhus/quick-look-plugins)  	- [Random-Forest](https://github.com/kjw0612/awesome-random-forest)  	- Raspberry Pi  		- [by @blackout314](https://github.com/blackout314/awesome-raspberry-pi)  		- [by @thibmaek](https://github.com/thibmaek/awesome-raspberry-pi)  	- [React Native](https://github.com/jondot/awesome-react-native)  	- [README](https://github.com/matiassingers/awesome-readme)  	- [Regex](https://github.com/aloisdg/awesome-regex)  	- [Remote Job](https://github.com/lukasz-madon/awesome-remote-job)  	- [Remote Work](https://github.com/hugo53/awesome-RemoteWork)  	- [REST](https://github.com/marmelab/awesome-rest)  	- [Robotics](https://github.com/Kiloreux/awesome-robotics)  	- [Robotic Tooling](https://github.com/protontypes/awesome-robotic-tooling)  	- [RNN](https://github.com/kjw0612/awesome-rnn)  	- [Scalability](https://github.com/binhnguyennus/awesome-scalability)  	- [Science Fiction](https://github.com/sindresorhus/awesome-scifi)  	- Search Engine Optimization (SEO)  		- [by @marcobiedermann](https://github.com/marcobiedermann/search-engine-optimization)  		- [by @sneg55](https://github.com/sneg55/curatedseotools)  		- [by @teles](https://github.com/teles/awesome-seo)  	- [Security](https://github.com/sbilly/awesome-security)  	- [Selfhosted](https://github.com/Kickball/awesome-selfhosted)  	- [Serverless](https://github.com/anaibol/awesome-serverless)  	- [Serverless Security](https://github.com/puresec/awesome-serverless-security/)  	- [Service Fabric](https://github.com/lawrencegripper/awesome-servicefabric)  	- [Services Engineering](https://github.com/mmcgrana/services-engineering)  	- [Sheet Music](https://github.com/adius/awesome-sheet-music)  	- [Slack](https://github.com/matiassingers/awesome-slack)  	- [Sound](https://github.com/hwclass/awesome-sound)  	- [Space](https://github.com/elburz/awesome-space)  		- [Books and manuals](https://github.com/Hunter-Github/awesome-space-books)  	- [Speech and Natural Language Processing](https://github.com/edobashira/speech-language-processing)  		- [NLP with Ruby](https://github.com/arbox/nlp-with-ruby)  	- [Sphinx Documentation](https://github.com/yoloseem/awesome-sphinxdoc)  	- [Startup](https://github.com/KrishMunot/awesome-startup)  	- [Static Analysis](https://github.com/mre/awesome-static-analysis/)  	- [Styleguides](https://github.com/RichardLitt/awesome-styleguides)  	- [Sublime Text](https://github.com/dreikanter/sublime-bookmarks)  	- [Sustainable Technology](https://github.com/protontypes/awesome-sustainable-technology)  	- [SVG](https://github.com/willianjusten/awesome-svg)  	- [Swedish](https://github.com/gurre/awesome-swedish-opensource)  	- [Sysadmin](https://github.com/kahun/awesome-sysadmin)  	- [Taglines](https://github.com/miketheman/awesome-taglines)  	- [Tailwind CSS](https://github.com/aniftyco/awesome-tailwindcss)  	- [Talks](https://github.com/JanVanRyswyck/awesome-talks)  		- [Gaming](https://github.com/hzoo/awesome-gametalks)  	- [Telegram](https://github.com/ebertti/awesome-telegram)  	- [Terminals Are Sexy](https://github.com/k4m4/terminals-are-sexy)  	- [Test Automation](https://github.com/atinfo/awesome-test-automation)  	- [Testing](https://github.com/TheJambo/awesome-testing)  		- [JMeter](https://github.com/aliesbelik/awesome-jmeter)  	- [Threat Intelligence](https://github.com/hslatman/awesome-threat-intelligence)  	- [Tools](https://github.com/cjbarber/ToolsOfTheTrade)  	- [Twilio](https://github.com/Twilio-org/awesome-twilio)  	- [Unity](https://github.com/RyanNielson/awesome-unity)  	- [UI Styleguide](https://github.com/kevinwuhoo/ui-styleguides)  		- [UI Components for Styleguide](https://github.com/anubhavsrivastava/awesome-ui-component-library)  	- [UNIX](https://github.com/sirredbeard/Awesome-UNIX)  	- [Vagrant](https://github.com/iJackUA/awesome-vagrant)  	- [Vehicle Security](https://github.com/jaredthecoder/awesome-vehicle-security)  	- Vim  		- [by @akrawchyk](https://github.com/akrawchyk/awesome-vim)  		- [by @matteocrippa](https://github.com/matteocrippa/awesome-vim)  	- [Vulkan](https://github.com/vinjn/awesome-vulkan)  	- [Web Performance Optimization](https://github.com/davidsonfellipe/awesome-wpo)  	- [WebComponents](https://github.com/mateusortiz/webcomponents-the-right-way)  	- [Wordpress](https://github.com/miziomon/awesome-wordpress)  	- [Workshops](https://github.com/therebelrobot/awesome-workshopper)  	- [Xamarin](https://github.com/benoitjadinon/awesome-xamarin)  	- XMPP  		- [Ejabberd](https://github.com/shantanu-deshmukh/awesome-ejabberd)  	- [Typography](https://github.com/Jolg42/awesome-typography)    ## License    [![Creative Commons License](http://i.creativecommons.org/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0/)    This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/). """
Big data;https://github.com/vinta/awesome-python;"""# Awesome Python [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)    A curated list of awesome Python frameworks, libraries, software and resources.    Inspired by [awesome-php](https://github.com/ziadoz/awesome-php).    - [Awesome Python](#awesome-python)      - [Admin Panels](#admin-panels)      - [Algorithms and Design Patterns](#algorithms-and-design-patterns)      - [ASGI Servers](#asgi-servers)      - [Asynchronous Programming](#asynchronous-programming)      - [Audio](#audio)      - [Authentication](#authentication)      - [Build Tools](#build-tools)      - [Built-in Classes Enhancement](#built-in-classes-enhancement)      - [Caching](#caching)      - [ChatOps Tools](#chatops-tools)      - [CMS](#cms)      - [Code Analysis](#code-analysis)      - [Command-line Interface Development](#command-line-interface-development)      - [Command-line Tools](#command-line-tools)      - [Compatibility](#compatibility)      - [Computer Vision](#computer-vision)      - [Concurrency and Parallelism](#concurrency-and-parallelism)      - [Configuration](#configuration)      - [Cryptography](#cryptography)      - [Data Analysis](#data-analysis)      - [Data Validation](#data-validation)      - [Data Visualization](#data-visualization)      - [Database Drivers](#database-drivers)      - [Database](#database)      - [Date and Time](#date-and-time)      - [Debugging Tools](#debugging-tools)      - [Deep Learning](#deep-learning)      - [DevOps Tools](#devops-tools)      - [Distributed Computing](#distributed-computing)      - [Distribution](#distribution)      - [Documentation](#documentation)      - [Downloader](#downloader)      - [E-commerce](#e-commerce)      - [Editor Plugins and IDEs](#editor-plugins-and-ides)      - [Email](#email)      - [Enterprise Application Integrations](#enterprise-application-integrations)      - [Environment Management](#environment-management)      - [Files](#files)      - [Foreign Function Interface](#foreign-function-interface)      - [Forms](#forms)      - [Functional Programming](#functional-programming)      - [Game Development](#game-development)      - [Geolocation](#geolocation)      - [GUI Development](#gui-development)      - [Hardware](#hardware)      - [HTML Manipulation](#html-manipulation)      - [HTTP Clients](#http-clients)      - [Image Processing](#image-processing)      - [Implementations](#implementations)      - [Interactive Interpreter](#interactive-interpreter)      - [Internationalization](#internationalization)      - [Job Scheduler](#job-scheduler)      - [Logging](#logging)      - [Machine Learning](#machine-learning)      - [Miscellaneous](#miscellaneous)      - [Natural Language Processing](#natural-language-processing)      - [Network Virtualization](#network-virtualization)      - [News Feed](#news-feed)      - [ORM](#orm)      - [Package Management](#package-management)      - [Package Repositories](#package-repositories)      - [Penetration testing](#penetration-testing)      - [Permissions](#permissions)      - [Processes](#processes)      - [Recommender Systems](#recommender-systems)      - [Refactoring](#refactoring)      - [RESTful API](#restful-api)      - [Robotics](#robotics)      - [RPC Servers](#rpc-servers)      - [Science](#science)      - [Search](#search)      - [Serialization](#serialization)      - [Serverless Frameworks](#serverless-frameworks)      - [Shell](#shell)      - [Specific Formats Processing](#specific-formats-processing)      - [Static Site Generator](#static-site-generator)      - [Tagging](#tagging)      - [Task Queues](#task-queues)      - [Template Engine](#template-engine)      - [Testing](#testing)      - [Text Processing](#text-processing)      - [Third-party APIs](#third-party-apis)      - [URL Manipulation](#url-manipulation)      - [Video](#video)      - [Web Asset Management](#web-asset-management)      - [Web Content Extracting](#web-content-extracting)      - [Web Crawling](#web-crawling)      - [Web Frameworks](#web-frameworks)      - [WebSocket](#websocket)      - [WSGI Servers](#wsgi-servers)  - [Resources](#resources)      - [Books](#books)      - [Newsletters](#newsletters)      - [Podcasts](#podcasts)      - [Websites](#websites)  - [Contributing](#contributing)    ---    ## Admin Panels    *Libraries for administrative interfaces.*    * [ajenti](https://github.com/ajenti/ajenti) - The admin panel your servers deserve.  * [django-grappelli](https://grappelliproject.com/) - A jazzy skin for the Django Admin-Interface.  * [django-jet](https://github.com/geex-arts/django-jet) - Modern responsive template for the Django admin interface with improved functionality.  * [django-suit](https://djangosuit.com/) - Alternative Django Admin-Interface (free only for Non-commercial use).  * [django-xadmin](https://github.com/sshwsfc/xadmin) - Drop-in replacement of Django admin comes with lots of goodies.  * [flask-admin](https://github.com/flask-admin/flask-admin) - Simple and extensible administrative interface framework for Flask.  * [flower](https://github.com/mher/flower) - Real-time monitor and web admin for Celery.  * [jet-bridge](https://github.com/jet-admin/jet-bridge) - Admin panel framework for any application with nice UI (ex Jet Django).  * [wooey](https://github.com/wooey/wooey) - A Django app which creates automatic web UIs for Python scripts.    ## Algorithms and Design Patterns    *Python implementation of data structures, algorithms and design patterns. Also see [awesome-algorithms](https://github.com/tayllan/awesome-algorithms).*    * Algorithms      * [algorithms](https://github.com/keon/algorithms) - Minimal examples of data structures and algorithms.      * [python-ds](https://github.com/prabhupant/python-ds) - A collection of data structure and algorithms for coding interviews.      * [sortedcontainers](https://github.com/grantjenks/python-sortedcontainers) - Fast and pure-Python implementation of sorted collections.      * [TheAlgorithms](https://github.com/TheAlgorithms/Python) - All Algorithms implemented in Python.  * Design Patterns      * [PyPattyrn](https://github.com/tylerlaberge/PyPattyrn) - A simple yet effective library for implementing common design patterns.      * [python-patterns](https://github.com/faif/python-patterns) - A collection of design patterns in Python.      * [transitions](https://github.com/pytransitions/transitions) - A lightweight, object-oriented finite state machine implementation.    ## ASGI Servers    *[ASGI](https://asgi.readthedocs.io/en/latest/)-compatible web servers.*    * [daphne](https://github.com/django/daphne) - A HTTP, HTTP2 and WebSocket protocol server for ASGI and ASGI-HTTP.  * [uvicorn](https://github.com/encode/uvicorn) - A lightning-fast ASGI server implementation, using uvloop and httptools.    ## Asynchronous Programming    * [asyncio](https://docs.python.org/3/library/asyncio.html) - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks.      - [awesome-asyncio](https://github.com/timofurrer/awesome-asyncio)  * [trio](https://github.com/python-trio/trio) - A friendly library for async concurrency and I/O.  * [Twisted](https://twistedmatrix.com/trac/) - An event-driven networking engine.  * [uvloop](https://github.com/MagicStack/uvloop) - Ultra fast asyncio event loop.    ## Audio    *Libraries for manipulating audio and its metadata.*    * Audio      * [audioread](https://github.com/beetbox/audioread) - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding.      * [dejavu](https://github.com/worldveil/dejavu) - Audio fingerprinting and recognition.      * [kapre](https://github.com/keunwoochoi/kapre) - Keras Audio Preprocessors.      * [librosa](https://github.com/librosa/librosa) - Python library for audio and music analysis.      * [matchering](https://github.com/sergree/matchering) - A library for automated reference audio mastering.      * [mingus](http://bspaans.github.io/python-mingus/) - An advanced music theory and notation package with MIDI file and playback support.      * [pyAudioAnalysis](https://github.com/tyiannak/pyAudioAnalysis) - Audio feature extraction, classification, segmentation and applications.      * [pydub](https://github.com/jiaaro/pydub) - Manipulate audio with a simple and easy high level interface.      * [TimeSide](https://github.com/Parisson/TimeSide) - Open web audio processing framework.  * Metadata      * [beets](https://github.com/beetbox/beets) - A music library manager and [MusicBrainz](https://musicbrainz.org/) tagger.      * [eyeD3](https://github.com/nicfit/eyeD3) - A tool for working with audio files, specifically MP3 files containing ID3 metadata.      * [mutagen](https://github.com/quodlibet/mutagen) - A Python module to handle audio metadata.      * [tinytag](https://github.com/devsnd/tinytag) - A library for reading music meta data of MP3, OGG, FLAC and Wave files.    ## Authentication    *Libraries for implementing authentications schemes.*    * OAuth      * [authlib](https://github.com/lepture/authlib) - JavaScript Object Signing and Encryption draft implementation.      * [django-allauth](https://github.com/pennersr/django-allauth) - Authentication app for Django that ""just works.""      * [django-oauth-toolkit](https://github.com/evonove/django-oauth-toolkit) - OAuth 2 goodies for Django.      * [oauthlib](https://github.com/idan/oauthlib) - A generic and thorough implementation of the OAuth request-signing logic.      * [python-oauth2](https://github.com/joestump/python-oauth2) - A fully tested, abstract interface to creating OAuth clients and servers.      * [python-social-auth](https://github.com/omab/python-social-auth) - An easy-to-setup social authentication mechanism.  * JWT      * [pyjwt](https://github.com/jpadilla/pyjwt) - JSON Web Token implementation in Python.      * [python-jose](https://github.com/mpdavis/python-jose/) - A JOSE implementation in Python.      * [python-jwt](https://github.com/davedoesdev/python-jwt) - A module for generating and verifying JSON Web Tokens.    ## Build Tools    *Compile software from source code.*    * [BitBake](http://www.yoctoproject.org/docs/1.6/bitbake-user-manual/bitbake-user-manual.html) - A make-like build tool for embedded Linux.  * [buildout](http://www.buildout.org/en/latest/) - A build system for creating, assembling and deploying applications from multiple parts.  * [PlatformIO](https://github.com/platformio/platformio-core) - A console tool to build code with different development platforms.  * [pybuilder](https://github.com/pybuilder/pybuilder) - A continuous build tool written in pure Python.  * [SCons](http://www.scons.org/) - A software construction tool.    ## Built-in Classes Enhancement    *Libraries for enhancing Python built-in classes.*    * [attrs](https://github.com/python-attrs/attrs) - Replacement for `__init__`, `__eq__`, `__repr__`, etc. boilerplate in class definitions.  * [bidict](https://github.com/jab/bidict) - Efficient, Pythonic bidirectional map data structures and related functionality..  * [Box](https://github.com/cdgriffith/Box) - Python dictionaries with advanced dot notation access.  * [dataclasses](https://docs.python.org/3/library/dataclasses.html) - (Python standard library) Data classes.  * [DottedDict](https://github.com/carlosescri/DottedDict) - A library that provides a method of accessing lists and dicts with a dotted path notation.    ## CMS    *Content Management Systems.*    * [django-cms](https://www.django-cms.org/en/) - An Open source enterprise CMS based on the Django.  * [feincms](https://github.com/feincms/feincms) - One of the most advanced Content Management Systems built on Django.  * [indico](https://github.com/indico/indico) - A feature-rich event management system, made @ [CERN](https://en.wikipedia.org/wiki/CERN).  * [Kotti](https://github.com/Kotti/Kotti) - A high-level, Pythonic web application framework built on Pyramid.  * [mezzanine](https://github.com/stephenmcd/mezzanine) - A powerful, consistent, and flexible content management platform.  * [plone](https://plone.org/) - A CMS built on top of the open source application server Zope.  * [quokka](https://github.com/rochacbruno/quokka) - Flexible, extensible, small CMS powered by Flask and MongoDB.  * [wagtail](https://wagtail.io/) - A Django content management system.    ## Caching    *Libraries for caching data.*    * [beaker](https://github.com/bbangert/beaker) - A WSGI middleware for sessions and caching.  * [django-cache-machine](https://github.com/django-cache-machine/django-cache-machine) - Automatic caching and invalidation for Django models.  * [django-cacheops](https://github.com/Suor/django-cacheops) - A slick ORM cache with automatic granular event-driven invalidation.  * [dogpile.cache](http://dogpilecache.readthedocs.io/en/latest/) - dogpile.cache is next generation replacement for Beaker made by same authors.  * [HermesCache](https://pypi.org/project/HermesCache/) - Python caching library with tag-based invalidation and dogpile effect prevention.  * [pylibmc](https://github.com/lericson/pylibmc) - A Python wrapper around the [libmemcached](https://libmemcached.org/libMemcached.html) interface.  * [python-diskcache](http://www.grantjenks.com/docs/diskcache/) - SQLite and file backed cache backend with faster lookups than memcached and redis.    ## ChatOps Tools    *Libraries for chatbot development.*    * [errbot](https://github.com/errbotio/errbot/) - The easiest and most popular chatbot to implement ChatOps.    ## Code Analysis    *Tools of static analysis, linters and code quality checkers. Also see [awesome-static-analysis](https://github.com/mre/awesome-static-analysis).*    * Code Analysis      * [coala](https://github.com/coala/coala/) - Language independent and easily extendable code analysis application.      * [code2flow](https://github.com/scottrogowski/code2flow) - Turn your Python and JavaScript code into DOT flowcharts.      * [prospector](https://github.com/PyCQA/prospector) - A tool to analyse Python code.      * [pycallgraph](https://github.com/gak/pycallgraph) - A library that visualises the flow (call graph) of your Python application.      * [vulture](https://github.com/jendrikseipp/vulture) - A tool for finding and analysing dead Python code.  * Code Linters      * [flake8](https://pypi.org/project/flake8/) - A wrapper around `pycodestyle`, `pyflakes` and McCabe.          * [awesome-flake8-extensions](https://github.com/DmytroLitvinov/awesome-flake8-extensions)      * [pylama](https://github.com/klen/pylama) - A code audit tool for Python and JavaScript.      * [pylint](https://www.pylint.org/) - A fully customizable source code analyzer.      * [wemake-python-styleguide](https://github.com/wemake-services/wemake-python-styleguide) - The strictest and most opinionated python linter ever.  * Code Formatters      * [black](https://github.com/python/black) - The uncompromising Python code formatter.      * [isort](https://github.com/timothycrosley/isort) - A Python utility / library to sort imports.      * [yapf](https://github.com/google/yapf) - Yet another Python code formatter from Google.  * Static Type Checkers, also see [awesome-python-typing](https://github.com/typeddjango/awesome-python-typing)      * [mypy](http://mypy-lang.org/) - Check variable types during compile time.      * [pyre-check](https://github.com/facebook/pyre-check) - Performant type checking.      * [typeshed](https://github.com/python/typeshed) - Collection of library stubs for Python, with static types.  * Static Type Annotations Generators      * [MonkeyType](https://github.com/Instagram/MonkeyType) - A system for Python that generates static type annotations by collecting runtime types.      * [pyannotate](https://github.com/dropbox/pyannotate) - Auto-generate PEP-484 annotations.      * [pytype](https://github.com/google/pytype) - Pytype checks and infers types for Python code - without requiring type annotations.    ## Command-line Interface Development    *Libraries for building command-line applications.*    * Command-line Application Development      * [cement](http://builtoncement.com/) - CLI Application Framework for Python.      * [click](http://click.pocoo.org/dev/) - A package for creating beautiful command line interfaces in a composable way.      * [cliff](https://docs.openstack.org/developer/cliff/) - A framework for creating command-line programs with multi-level commands.      * [docopt](http://docopt.org/) - Pythonic command line arguments parser.      * [python-fire](https://github.com/google/python-fire) - A library for creating command line interfaces from absolutely any Python object.      * [python-prompt-toolkit](https://github.com/jonathanslenders/python-prompt-toolkit) - A library for building powerful interactive command lines.  * Terminal Rendering      * [alive-progress](https://github.com/rsalmei/alive-progress) - A new kind of Progress Bar, with real-time throughput, eta and very cool animations.      * [asciimatics](https://github.com/peterbrittain/asciimatics) - A package to create full-screen text UIs (from interactive forms to ASCII animations).      * [bashplotlib](https://github.com/glamp/bashplotlib) - Making basic plots in the terminal.      * [colorama](https://pypi.org/project/colorama/) - Cross-platform colored terminal text.      * [rich](https://github.com/willmcgugan/rich) - Python library for rich text and beautiful formatting in the terminal. Also provides a great `RichHandler` log handler.      * [tqdm](https://github.com/tqdm/tqdm) - Fast, extensible progress bar for loops and CLI.    ## Command-line Tools    *Useful CLI-based tools for productivity.*    * Productivity Tools      * [copier](https://github.com/pykong/copier) - A library and command-line utility for rendering projects templates.      * [cookiecutter](https://github.com/audreyr/cookiecutter) - A command-line utility that creates projects from cookiecutters (project templates).      * [doitlive](https://github.com/sloria/doitlive) - A tool for live presentations in the terminal.      * [howdoi](https://github.com/gleitz/howdoi) - Instant coding answers via the command line.      * [Invoke](https://github.com/pyinvoke/invoke#readme) - A tool for managing shell-oriented subprocesses and organizing executable Python code into CLI-invokable tasks.      * [PathPicker](https://github.com/facebook/PathPicker) - Select files out of bash output.      * [percol](https://github.com/mooz/percol) - Adds flavor of interactive selection to the traditional pipe concept on UNIX.      * [thefuck](https://github.com/nvbn/thefuck) - Correcting your previous console command.      * [tmuxp](https://github.com/tony/tmuxp) - A [tmux](https://github.com/tmux/tmux) session manager.      * [try](https://github.com/timofurrer/try) - A dead simple CLI to try out python packages - it's never been easier.  * CLI Enhancements      * [httpie](https://github.com/jakubroztocil/httpie) - A command line HTTP client, a user-friendly cURL replacement.      * [iredis](https://github.com/laixintao/iredis) - Redis CLI with autocompletion and syntax highlighting.      * [kube-shell](https://github.com/cloudnativelabs/kube-shell) - An integrated shell for working with the Kubernetes CLI.      * [litecli](https://github.com/dbcli/litecli) - SQLite CLI with autocompletion and syntax highlighting.      * [mycli](https://github.com/dbcli/mycli) - MySQL CLI with autocompletion and syntax highlighting.      * [pgcli](https://github.com/dbcli/pgcli) - PostgreSQL CLI with autocompletion and syntax highlighting.      * [saws](https://github.com/donnemartin/saws) - A Supercharged [aws-cli](https://github.com/aws/aws-cli).    ## Compatibility    *Libraries for migrating from Python 2 to 3.*    * [python-future](http://python-future.org/index.html) - The missing compatibility layer between Python 2 and Python 3.  * [modernize](https://github.com/PyCQA/modernize) - Modernizes Python code for eventual Python 3 migration.  * [six](https://pypi.org/project/six/) - Python 2 and 3 compatibility utilities.    ## Computer Vision    *Libraries for Computer Vision.*    * [EasyOCR](https://github.com/JaidedAI/EasyOCR) - Ready-to-use OCR with 40+ languages supported.  * [Face Recognition](https://github.com/ageitgey/face_recognition) - Simple facial recognition library.  * [Kornia](https://github.com/kornia/kornia/) - Open Source Differentiable Computer Vision Library for PyTorch.  * [OpenCV](https://opencv.org/) - Open Source Computer Vision Library.  * [pytesseract](https://github.com/madmaze/pytesseract) - A wrapper for [Google Tesseract OCR](https://github.com/tesseract-ocr).  * [SimpleCV](https://github.com/sightmachine/SimpleCV) - An open source framework for building computer vision applications.  * [tesserocr](https://github.com/sirfz/tesserocr) - Another simple, Pillow-friendly, wrapper around the `tesseract-ocr` API for OCR.    ## Concurrency and Parallelism    *Libraries for concurrent and parallel execution. Also see [awesome-asyncio](https://github.com/timofurrer/awesome-asyncio).*    * [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html) - (Python standard library) A high-level interface for asynchronously executing callables.  * [eventlet](http://eventlet.net/) - Asynchronous framework with WSGI support.  * [gevent](http://www.gevent.org/) - A coroutine-based Python networking library that uses [greenlet](https://github.com/python-greenlet/greenlet).  * [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) - (Python standard library) Process-based parallelism.  * [scoop](https://github.com/soravux/scoop) - Scalable Concurrent Operations in Python.  * [uvloop](https://github.com/MagicStack/uvloop) - Ultra fast implementation of `asyncio` event loop on top of `libuv`.    ## Configuration    *Libraries for storing and parsing configuration options.*    * [configobj](https://github.com/DiffSK/configobj) - INI file parser with validation.  * [configparser](https://docs.python.org/3/library/configparser.html) - (Python standard library) INI file parser.  * [hydra](https://github.com/facebookresearch/hydra) - Hydra is a framework for elegantly configuring complex applications.  * [profig](https://profig.readthedocs.io/en/latest/) - Config from multiple formats with value conversion.  * [python-decouple](https://github.com/henriquebastos/python-decouple) - Strict separation of settings from code.    ## Cryptography    * [cryptography](https://cryptography.io/en/latest/) - A package designed to expose cryptographic primitives and recipes to Python developers.  * [paramiko](https://github.com/paramiko/paramiko) - The leading native Python SSHv2 protocol library.  * [passlib](https://passlib.readthedocs.io/en/stable/) - Secure password storage/hashing library, very high level.  * [pynacl](https://github.com/pyca/pynacl) - Python binding to the Networking and Cryptography (NaCl) library.    ## Data Analysis    *Libraries for data analyzing.*    * [AWS Data Wrangler](https://github.com/awslabs/aws-data-wrangler) - Pandas on AWS.  * [Blaze](https://github.com/blaze/blaze) - NumPy and Pandas interface to Big Data.  * [Open Mining](https://github.com/mining/mining) - Business Intelligence (BI) in Pandas interface.  * [Optimus](https://github.com/ironmussa/Optimus) - Agile Data Science Workflows made easy with PySpark.  * [Orange](https://orange.biolab.si/) - Data mining, data visualization, analysis and machine learning through visual programming or scripts.  * [Pandas](http://pandas.pydata.org/) - A library providing high-performance, easy-to-use data structures and data analysis tools.    ## Data Validation    *Libraries for validating data. Used for forms in many cases.*    * [Cerberus](https://github.com/pyeve/cerberus) - A lightweight and extensible data validation library.  * [colander](https://docs.pylonsproject.org/projects/colander/en/latest/) - Validating and deserializing data obtained via XML, JSON, an HTML form post.  * [jsonschema](https://github.com/Julian/jsonschema) - An implementation of [JSON Schema](http://json-schema.org/) for Python.  * [schema](https://github.com/keleshev/schema) - A library for validating Python data structures.  * [Schematics](https://github.com/schematics/schematics) - Data Structure Validation.  * [valideer](https://github.com/podio/valideer) - Lightweight extensible data validation and adaptation library.  * [voluptuous](https://github.com/alecthomas/voluptuous) - A Python data validation library.    ## Data Visualization    *Libraries for visualizing data. Also see [awesome-javascript](https://github.com/sorrycc/awesome-javascript#data-visualization).*    * [Altair](https://github.com/altair-viz/altair) - Declarative statistical visualization library for Python.  * [Bokeh](https://github.com/bokeh/bokeh) - Interactive Web Plotting for Python.  * [bqplot](https://github.com/bloomberg/bqplot) - Interactive Plotting Library for the Jupyter Notebook.  * [Cartopy](https://github.com/SciTools/cartopy) - A cartographic python library with matplotlib support.  * [Dash](https://plot.ly/products/dash/) - Built on top of Flask, React and Plotly aimed at analytical web applications.      * [awesome-dash](https://github.com/Acrotrend/awesome-dash)  * [diagrams](https://github.com/mingrammer/diagrams) - Diagram as Code.  * [Matplotlib](http://matplotlib.org/) - A Python 2D plotting library.  * [plotnine](https://github.com/has2k1/plotnine) - A grammar of graphics for Python based on ggplot2.  * [Pygal](http://www.pygal.org/en/latest/) - A Python SVG Charts Creator.  * [PyGraphviz](https://pypi.org/project/pygraphviz/) - Python interface to [Graphviz](http://www.graphviz.org/).  * [PyQtGraph](http://www.pyqtgraph.org/) - Interactive and realtime 2D/3D/Image plotting and science/engineering widgets.  * [Seaborn](https://github.com/mwaskom/seaborn) - Statistical data visualization using Matplotlib.  * [VisPy](https://github.com/vispy/vispy) - High-performance scientific visualization based on OpenGL.    ## Database    *Databases implemented in Python.*    * [pickleDB](https://github.com/patx/pickledb) - A simple and lightweight key-value store for Python.  * [tinydb](https://github.com/msiemens/tinydb) - A tiny, document-oriented database.  * [ZODB](https://github.com/zopefoundation/ZODB) - A native object database for Python. A key-value and object graph database.    ## Database Drivers    *Libraries for connecting and operating databases.*    * MySQL - [awesome-mysql](http://shlomi-noach.github.io/awesome-mysql/)      * [mysqlclient](https://github.com/PyMySQL/mysqlclient-python) - MySQL connector with Python 3 support ([mysql-python](https://sourceforge.net/projects/mysql-python/) fork).      * [PyMySQL](https://github.com/PyMySQL/PyMySQL) - A pure Python MySQL driver compatible to mysql-python.  * PostgreSQL - [awesome-postgres](https://github.com/dhamaniasad/awesome-postgres)      * [psycopg2](http://initd.org/psycopg/) - The most popular PostgreSQL adapter for Python.      * [queries](https://github.com/gmr/queries) - A wrapper of the psycopg2 library for interacting with PostgreSQL.  * SQlite - [awesome-sqlite](https://github.com/planetopendata/awesome-sqlite)      * [sqlite3](https://docs.python.org/3/library/sqlite3.html) - (Python standard library) SQlite interface compliant with DB-API 2.0      * [SuperSQLite](https://github.com/plasticityai/supersqlite) - A supercharged SQLite library built on top of [apsw](https://github.com/rogerbinns/apsw).  * Other Relational Databases      * [pymssql](https://pymssql.readthedocs.io/en/latest/) - A simple database interface to Microsoft SQL Server.      * [clickhouse-driver](https://github.com/mymarilyn/clickhouse-driver) - Python driver with native interface for ClickHouse.  * NoSQL Databases      * [cassandra-driver](https://github.com/datastax/python-driver) - The Python Driver for Apache Cassandra.      * [happybase](https://github.com/wbolster/happybase) - A developer-friendly library for Apache HBase.      * [kafka-python](https://github.com/dpkp/kafka-python) - The Python client for Apache Kafka.      * [py2neo](https://py2neo.org/) - A client library and toolkit for working with Neo4j.      * [pymongo](https://github.com/mongodb/mongo-python-driver) - The official Python client for MongoDB.      * [redis-py](https://github.com/andymccurdy/redis-py) - The Python client for Redis.  * Asynchronous Clients      * [motor](https://github.com/mongodb/motor) - The async Python driver for MongoDB.    ## Date and Time    *Libraries for working with dates and times.*    * [Arrow](https://arrow.readthedocs.io/en/latest/) - A Python library that offers a sensible and human-friendly approach to creating, manipulating, formatting and converting dates, times and timestamps.  * [Chronyk](https://github.com/KoffeinFlummi/Chronyk) - A Python 3 library for parsing human-written times and dates.  * [dateutil](https://github.com/dateutil/dateutil) - Extensions to the standard Python [datetime](https://docs.python.org/3/library/datetime.html) module.  * [delorean](https://github.com/myusuf3/delorean/) - A library for clearing up the inconvenient truths that arise dealing with datetimes.  * [maya](https://github.com/timofurrer/maya) - Datetimes for Humans.  * [moment](https://github.com/zachwill/moment) - A Python library for dealing with dates/times. Inspired by [Moment.js](http://momentjs.com/).  * [Pendulum](https://github.com/sdispater/pendulum) - Python datetimes made easy.  * [PyTime](https://github.com/shinux/PyTime) - An easy-to-use Python module which aims to operate date/time/datetime by string.  * [pytz](https://launchpad.net/pytz) - World timezone definitions, modern and historical. Brings the [tz database](https://en.wikipedia.org/wiki/Tz_database) into Python.  * [when.py](https://github.com/dirn/When.py) - Providing user-friendly functions to help perform common date and time actions.    ## Debugging Tools    *Libraries for debugging code.*    * pdb-like Debugger      * [ipdb](https://github.com/gotcha/ipdb) - IPython-enabled [pdb](https://docs.python.org/3/library/pdb.html).      * [pdb++](https://github.com/antocuni/pdb) - Another drop-in replacement for pdb.      * [pudb](https://github.com/inducer/pudb) - A full-screen, console-based Python debugger.      * [wdb](https://github.com/Kozea/wdb) - An improbable web debugger through WebSockets.  * Tracing      * [lptrace](https://github.com/khamidou/lptrace) - [strace](http://man7.org/linux/man-pages/man1/strace.1.html) for Python programs.      * [manhole](https://github.com/ionelmc/python-manhole) - Debugging UNIX socket connections and present the stacktraces for all threads and an interactive prompt.      * [pyringe](https://github.com/google/pyringe) - Debugger capable of attaching to and injecting code into Python processes.      * [python-hunter](https://github.com/ionelmc/python-hunter) - A flexible code tracing toolkit.  * Profiler      * [line_profiler](https://github.com/rkern/line_profiler) - Line-by-line profiling.      * [memory_profiler](https://github.com/fabianp/memory_profiler) - Monitor Memory usage of Python code.      * [py-spy](https://github.com/benfred/py-spy) - A sampling profiler for Python programs. Written in Rust.      * [pyflame](https://github.com/uber/pyflame) - A ptracing profiler For Python.      * [vprof](https://github.com/nvdv/vprof) - Visual Python profiler.  * Others      * [django-debug-toolbar](https://github.com/jazzband/django-debug-toolbar) - Display various debug information for Django.      * [django-devserver](https://github.com/dcramer/django-devserver) - A drop-in replacement for Django's runserver.      * [flask-debugtoolbar](https://github.com/mgood/flask-debugtoolbar) - A port of the django-debug-toolbar to flask.      * [icecream](https://github.com/gruns/icecream) - Inspect variables, expressions, and program execution with a single, simple function call.      * [pyelftools](https://github.com/eliben/pyelftools) - Parsing and analyzing ELF files and DWARF debugging information.    ## Deep Learning    *Frameworks for Neural Networks and Deep Learning. Also see [awesome-deep-learning](https://github.com/ChristosChristofidis/awesome-deep-learning).*    * [caffe](https://github.com/BVLC/caffe) - A fast open framework for deep learning..  * [keras](https://github.com/keras-team/keras) - A high-level neural networks library and capable of running on top of either TensorFlow or Theano.  * [mxnet](https://github.com/dmlc/mxnet) - A deep learning framework designed for both efficiency and flexibility.  * [pytorch](https://github.com/pytorch/pytorch) - Tensors and Dynamic neural networks in Python with strong GPU acceleration.  * [SerpentAI](https://github.com/SerpentAI/SerpentAI) - Game agent framework. Use any video game as a deep learning sandbox.  * [tensorflow](https://github.com/tensorflow/tensorflow) - The most popular Deep Learning framework created by Google.  * [Theano](https://github.com/Theano/Theano) - A library for fast numerical computation.    ## DevOps Tools    *Software and libraries for DevOps.*    * Configuration Management      * [ansible](https://github.com/ansible/ansible) - A radically simple IT automation platform.      * [cloudinit](https://cloudinit.readthedocs.io/en/latest/) - A multi-distribution package that handles early initialization of a cloud instance.      * [OpenStack](https://www.openstack.org/) - Open source software for building private and public clouds.      * [pyinfra](https://github.com/Fizzadar/pyinfra) - A versatile CLI tools and python libraries to automate infrastructure.      * [saltstack](https://github.com/saltstack/salt) - Infrastructure automation and management system.  * SSH-style Deployment      * [cuisine](https://github.com/sebastien/cuisine) - Chef-like functionality for Fabric.      * [fabric](https://github.com/fabric/fabric) - A simple, Pythonic tool for remote execution and deployment.      * [fabtools](https://github.com/fabtools/fabtools) - Tools for writing awesome Fabric files.  * Process Management      * [honcho](https://github.com/nickstenning/honcho) - A Python clone of [Foreman](https://github.com/ddollar/foreman), for managing Procfile-based applications.      * [supervisor](https://github.com/Supervisor/supervisor) - Supervisor process control system for UNIX.  * Monitoring      * [psutil](https://github.com/giampaolo/psutil) - A cross-platform process and system utilities module.  * Backup      * [BorgBackup](https://www.borgbackup.org/) - A deduplicating archiver with compression and encryption.  * Others      * [docker-compose](https://docs.docker.com/compose/) - Fast, isolated development environments using [Docker](https://www.docker.com/).    ## Distributed Computing    *Frameworks and libraries for Distributed Computing.*    * Batch Processing      * [dask](https://github.com/dask/dask) - A flexible parallel computing library for analytic computing.      * [luigi](https://github.com/spotify/luigi) - A module that helps you build complex pipelines of batch jobs.      * [mrjob](https://github.com/Yelp/mrjob) - Run MapReduce jobs on Hadoop or Amazon Web Services.      * [PySpark](https://pypi.org/project/pyspark/) - [Apache Spark](https://spark.apache.org/) Python API.      * [Ray](https://github.com/ray-project/ray/) - A system for parallel and distributed Python that unifies the machine learning ecosystem.  * Stream Processing      * [faust](https://github.com/robinhood/faust) - A stream processing library, porting the ideas from [Kafka Streams](https://kafka.apache.org/documentation/streams/) to Python.      * [streamparse](https://github.com/Parsely/streamparse) - Run Python code against real-time streams of data via [Apache Storm](http://storm.apache.org/).    ## Distribution    *Libraries to create packaged executables for release distribution.*    * [dh-virtualenv](https://github.com/spotify/dh-virtualenv) - Build and distribute a virtualenv as a Debian package.  * [Nuitka](http://nuitka.net/) - Compile scripts, modules, packages to an executable or extension module.  * [py2app](http://pythonhosted.org/py2app/) - Freezes Python scripts (Mac OS X).  * [py2exe](http://www.py2exe.org/) - Freezes Python scripts (Windows).  * [pyarmor](https://github.com/dashingsoft/pyarmor) - A tool used to obfuscate python scripts, bind obfuscated scripts to fixed machine or expire obfuscated scripts.  * [PyInstaller](https://github.com/pyinstaller/pyinstaller) - Converts Python programs into stand-alone executables (cross-platform).  * [pynsist](http://pynsist.readthedocs.io/en/latest/) - A tool to build Windows installers, installers bundle Python itself.  * [shiv](https://github.com/linkedin/shiv) - A command line utility for building fully self-contained zipapps (PEP 441), but with all their dependencies included.    ## Documentation    *Libraries for generating project documentation.*    * [sphinx](https://github.com/sphinx-doc/sphinx/) - Python Documentation generator.      * [awesome-sphinxdoc](https://github.com/yoloseem/awesome-sphinxdoc)  * [pdoc](https://github.com/mitmproxy/pdoc) - Epydoc replacement to auto generate API documentation for Python libraries.  * [pycco](https://github.com/pycco-docs/pycco) - The literate-programming-style documentation generator.    ## Downloader    *Libraries for downloading.*    * [akshare](https://github.com/jindaxiang/akshare) - A financial data interface library, built for human beings!  * [s3cmd](https://github.com/s3tools/s3cmd) - A command line tool for managing Amazon S3 and CloudFront.  * [s4cmd](https://github.com/bloomreach/s4cmd) - Super S3 command line tool, good for higher performance.  * [you-get](https://you-get.org/) - A YouTube/Youku/Niconico video downloader written in Python 3.  * [youtube-dl](https://rg3.github.io/youtube-dl/) - A small command-line program to download videos from YouTube.    ## E-commerce    *Frameworks and libraries for e-commerce and payments.*    * [alipay](https://github.com/lxneng/alipay) - Unofficial Alipay API for Python.  * [Cartridge](https://github.com/stephenmcd/cartridge) - A shopping cart app built using the Mezzanine.  * [django-oscar](http://oscarcommerce.com/) - An open-source e-commerce framework for Django.  * [django-shop](https://github.com/awesto/django-shop) - A Django based shop system.  * [forex-python](https://github.com/MicroPyramid/forex-python) - Foreign exchange rates, Bitcoin price index and currency conversion.  * [merchant](https://github.com/agiliq/merchant) - A Django app to accept payments from various payment processors.  * [money](https://github.com/carlospalol/money) - `Money` class with optional CLDR-backed locale-aware formatting and an extensible currency exchange.  * [python-currencies](https://github.com/Alir3z4/python-currencies) - Display money format and its filthy currencies.  * [saleor](http://getsaleor.com/) - An e-commerce storefront for Django.  * [shoop](https://www.shuup.com/en/) - An open source E-Commerce platform based on Django.    ## Editor Plugins and IDEs    * Emacs      * [elpy](https://github.com/jorgenschaefer/elpy) - Emacs Python Development Environment.  * Sublime Text      * [anaconda](https://github.com/DamnWidget/anaconda) - Anaconda turns your Sublime Text 3 in a full featured Python development IDE.      * [SublimeJEDI](https://github.com/srusskih/SublimeJEDI) - A Sublime Text plugin to the awesome auto-complete library Jedi.  * Vim      * [jedi-vim](https://github.com/davidhalter/jedi-vim) - Vim bindings for the Jedi auto-completion library for Python.      * [python-mode](https://github.com/python-mode/python-mode) - An all in one plugin for turning Vim into a Python IDE.      * [YouCompleteMe](https://github.com/Valloric/YouCompleteMe) - Includes [Jedi](https://github.com/davidhalter/jedi)-based completion engine for Python.  * Visual Studio      * [PTVS](https://github.com/Microsoft/PTVS) - Python Tools for Visual Studio.  * Visual Studio Code      * [Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python) - The official VSCode extension with rich support for Python.  * IDE      * [PyCharm](https://www.jetbrains.com/pycharm/) - Commercial Python IDE by JetBrains. Has free community edition available.      * [spyder](https://github.com/spyder-ide/spyder) - Open Source Python IDE.    ## Email    *Libraries for sending and parsing email.*    * Mail Servers      * [modoboa](https://github.com/modoboa/modoboa) - A mail hosting and management platform including a modern Web UI.      * [salmon](https://github.com/moggers87/salmon) - A Python Mail Server.  * Clients      * [imbox](https://github.com/martinrusev/imbox) - Python IMAP for Humans.      * [yagmail](https://github.com/kootenpv/yagmail) - Yet another Gmail/SMTP client.  * Others      * [flanker](https://github.com/mailgun/flanker) - An email address and Mime parsing library.      * [mailer](https://github.com/marrow/mailer) - High-performance extensible mail delivery framework.    ## Enterprise Application Integrations    *Platforms and tools for systems integrations in enterprise environments*    * [Zato](https://zato.io) - ESB, SOA, REST, APIs and Cloud Integrations in Python.    ## Environment Management    *Libraries for Python version and virtual environment management.*    * [pyenv](https://github.com/pyenv/pyenv) - Simple Python version management.  * [virtualenv](https://github.com/pypa/virtualenv) - A tool to create isolated Python environments.    ## Files    *Libraries for file manipulation and MIME type detection.*    * [mimetypes](https://docs.python.org/3/library/mimetypes.html) - (Python standard library) Map filenames to MIME types.  * [path.py](https://github.com/jaraco/path.py) - A module wrapper for [os.path](https://docs.python.org/3/library/os.path.html).  * [pathlib](https://docs.python.org/3/library/pathlib.html) - (Python standard library) An cross-platform, object-oriented path library.  * [PyFilesystem2](https://github.com/pyfilesystem/pyfilesystem2) - Python's filesystem abstraction layer.  * [python-magic](https://github.com/ahupp/python-magic) - A Python interface to the libmagic file type identification library.  * [Unipath](https://github.com/mikeorr/Unipath) - An object-oriented approach to file/directory operations.  * [watchdog](https://github.com/gorakhargosh/watchdog) - API and shell utilities to monitor file system events.    ## Foreign Function Interface    *Libraries for providing foreign function interface.*    * [cffi](https://pypi.org/project/cffi/) - Foreign Function Interface for Python calling C code.  * [ctypes](https://docs.python.org/3/library/ctypes.html) - (Python standard library) Foreign Function Interface for Python calling C code.  * [PyCUDA](https://mathema.tician.de/software/pycuda/) - A Python wrapper for Nvidia's CUDA API.  * [SWIG](http://www.swig.org/Doc1.3/Python.html) - Simplified Wrapper and Interface Generator.    ## Forms    *Libraries for working with forms.*    * [Deform](https://github.com/Pylons/deform) - Python HTML form generation library influenced by the formish form generation library.  * [django-bootstrap3](https://github.com/dyve/django-bootstrap3) - Bootstrap 3 integration with Django.  * [django-bootstrap4](https://github.com/zostera/django-bootstrap4) - Bootstrap 4 integration with Django.  * [django-crispy-forms](https://github.com/django-crispy-forms/django-crispy-forms) - A Django app which lets you create beautiful forms in a very elegant and DRY way.  * [django-remote-forms](https://github.com/WiserTogether/django-remote-forms) - A platform independent Django form serializer.  * [WTForms](https://github.com/wtforms/wtforms) - A flexible forms validation and rendering library.    ## Functional Programming    *Functional Programming with Python.*    * [Coconut](https://github.com/evhub/coconut) - A variant of Python built for simple, elegant, Pythonic functional programming.  * [CyToolz](https://github.com/pytoolz/cytoolz/) - Cython implementation of `Toolz`: High performance functional utilities.  * [fn.py](https://github.com/kachayev/fn.py) - Functional programming in Python: implementation of missing features to enjoy FP.  * [funcy](https://github.com/Suor/funcy) - A fancy and practical functional tools.  * [more-itertools](https://github.com/erikrose/more-itertools) - More routines for operating on iterables, beyond `itertools`.  * [returns](https://github.com/dry-python/returns) - A set of type-safe monads, transformers, and composition utilities.  * [Toolz](https://github.com/pytoolz/toolz) - A collection of functional utilities for iterators, functions, and dictionaries.    ## GUI Development    *Libraries for working with graphical user interface applications.*    * [curses](https://docs.python.org/3/library/curses.html) - Built-in wrapper for [ncurses](http://www.gnu.org/software/ncurses/) used to create terminal GUI applications.  * [Eel](https://github.com/ChrisKnott/Eel) - A library for making simple Electron-like offline HTML/JS GUI apps.  * [enaml](https://github.com/nucleic/enaml) - Creating beautiful user-interfaces with Declarative Syntax like QML.  * [Flexx](https://github.com/zoofIO/flexx) - Flexx is a pure Python toolkit for creating GUI's, that uses web technology for its rendering.  * [Gooey](https://github.com/chriskiehl/Gooey) - Turn command line programs into a full GUI application with one line.  * [kivy](https://kivy.org/) - A library for creating NUI applications, running on Windows, Linux, Mac OS X, Android and iOS.  * [pyglet](https://github.com/pyglet/pyglet) - A cross-platform windowing and multimedia library for Python.  * [PyGObject](https://wiki.gnome.org/Projects/PyGObject) - Python Bindings for GLib/GObject/GIO/GTK+ (GTK+3).  * [PyQt](https://riverbankcomputing.com/software/pyqt/intro) - Python bindings for the [Qt](https://www.qt.io/) cross-platform application and UI framework.  * [PySimpleGUI](https://github.com/PySimpleGUI/PySimpleGUI) - Wrapper for tkinter, Qt, WxPython and Remi.  * [pywebview](https://github.com/r0x0r/pywebview/) - A lightweight cross-platform native wrapper around a webview component.  * [Tkinter](https://wiki.python.org/moin/TkInter) - Tkinter is Python's de-facto standard GUI package.  * [Toga](https://github.com/pybee/toga) - A Python native, OS native GUI toolkit.  * [urwid](http://urwid.org/) - A library for creating terminal GUI applications with strong support for widgets, events, rich colors, etc.  * [wxPython](https://wxpython.org/) - A blending of the wxWidgets C++ class library with the Python.  * [DearPyGui](https://github.com/RaylockLLC/DearPyGui/) - A Simple GPU accelerated Python GUI framework    ## GraphQL    *Libraries for working with GraphQL.*    * [graphene](https://github.com/graphql-python/graphene/) - GraphQL framework for Python.  * [tartiflette-aiohttp](https://github.com/tartiflette/tartiflette-aiohttp/) - An `aiohttp`-based wrapper for Tartiflette to expose GraphQL APIs over HTTP.  * [tartiflette-asgi](https://github.com/tartiflette/tartiflette-asgi/) - ASGI support for the Tartiflette GraphQL engine.  * [tartiflette](https://tartiflette.io) - SDL-first GraphQL engine implementation for Python 3.6+ and asyncio.    ## Game Development    *Awesome game development libraries.*    * [Arcade](https://api.arcade.academy/en/latest/) - Arcade is a modern Python framework for crafting games with compelling graphics and sound.  * [Cocos2d](http://cocos2d.org/) - cocos2d is a framework for building 2D games, demos, and other graphical/interactive applications.  * [Harfang3D](http://www.harfang3d.com) - Python framework for 3D, VR and game development.  * [Panda3D](https://www.panda3d.org/) - 3D game engine developed by Disney.  * [Pygame](http://www.pygame.org/news.html) - Pygame is a set of Python modules designed for writing games.  * [PyOgre](http://www.ogre3d.org/tikiwiki/PyOgre) - Python bindings for the Ogre 3D render engine, can be used for games, simulations, anything 3D.  * [PyOpenGL](http://pyopengl.sourceforge.net/) - Python ctypes bindings for OpenGL and it's related APIs.  * [PySDL2](https://pysdl2.readthedocs.io) - A ctypes based wrapper for the SDL2 library.  * [RenPy](https://www.renpy.org/) - A Visual Novel engine.    ## Geolocation    *Libraries for geocoding addresses and working with latitudes and longitudes.*    * [django-countries](https://github.com/SmileyChris/django-countries) - A Django app that provides a country field for models and forms.  * [GeoDjango](https://docs.djangoproject.com/en/dev/ref/contrib/gis/) - A world-class geographic web framework.  * [GeoIP](https://github.com/maxmind/geoip-api-python) - Python API for MaxMind GeoIP Legacy Database.  * [geojson](https://github.com/frewsxcv/python-geojson) - Python bindings and utilities for GeoJSON.  * [geopy](https://github.com/geopy/geopy) - Python Geocoding Toolbox.    ## HTML Manipulation    *Libraries for working with HTML and XML.*    * [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - Providing Pythonic idioms for iterating, searching, and modifying HTML or XML.  * [bleach](https://github.com/mozilla/bleach) - A whitelist-based HTML sanitization and text linkification library.  * [cssutils](https://pypi.org/project/cssutils/) - A CSS library for Python.  * [html5lib](https://github.com/html5lib/html5lib-python) - A standards-compliant library for parsing and serializing HTML documents and fragments.  * [lxml](http://lxml.de/) - A very fast, easy-to-use and versatile library for handling HTML and XML.  * [MarkupSafe](https://github.com/pallets/markupsafe) - Implements a XML/HTML/XHTML Markup safe string for Python.  * [pyquery](https://github.com/gawel/pyquery) - A jQuery-like library for parsing HTML.  * [untangle](https://github.com/stchris/untangle) - Converts XML documents to Python objects for easy access.  * [WeasyPrint](http://weasyprint.org) - A visual rendering engine for HTML and CSS that can export to PDF.  * [xmldataset](https://xmldataset.readthedocs.io/en/latest/) - Simple XML Parsing.  * [xmltodict](https://github.com/martinblech/xmltodict) - Working with XML feel like you are working with JSON.    ## HTTP Clients    *Libraries for working with HTTP.*    * [grequests](https://github.com/spyoungtech/grequests) - requests + gevent for asynchronous HTTP requests.  * [httplib2](https://github.com/httplib2/httplib2) - Comprehensive HTTP client library.  * [httpx](https://github.com/encode/httpx) - A next generation HTTP client for Python.  * [requests](https://github.com/psf/requests) - HTTP Requests for Humans.  * [treq](https://github.com/twisted/treq) - Python requests like API built on top of Twisted's HTTP client.  * [urllib3](https://github.com/shazow/urllib3) - A HTTP library with thread-safe connection pooling, file post support, sanity friendly.    ## Hardware    *Libraries for programming with hardware.*    * [ino](http://inotool.org/) - Command line toolkit for working with [Arduino](https://www.arduino.cc/).  * [keyboard](https://github.com/boppreh/keyboard) - Hook and simulate global keyboard events on Windows and Linux.  * [mouse](https://github.com/boppreh/mouse) - Hook and simulate global mouse events on Windows and Linux.  * [Pingo](http://www.pingo.io/) - Pingo provides a uniform API to program devices like the Raspberry Pi, pcDuino, Intel Galileo, etc.  * [PyUserInput](https://github.com/SavinaRoja/PyUserInput) - A module for cross-platform control of the mouse and keyboard.  * [scapy](https://github.com/secdev/scapy) - A brilliant packet manipulation library.    ## Image Processing    *Libraries for manipulating images.*    * [hmap](https://github.com/rossgoodwin/hmap) - Image histogram remapping.  * [imgSeek](https://sourceforge.net/projects/imgseek/) - A project for searching a collection of images using visual similarity.  * [nude.py](https://github.com/hhatto/nude.py) - Nudity detection.  * [pagan](https://github.com/daboth/pagan) - Retro identicon (Avatar) generation based on input string and hash.  * [pillow](https://github.com/python-pillow/Pillow) - Pillow is the friendly [PIL](http://www.pythonware.com/products/pil/) fork.  * [python-barcode](https://github.com/WhyNotHugo/python-barcode) - Create barcodes in Python with no extra dependencies.  * [pygram](https://github.com/ajkumar25/pygram) - Instagram-like image filters.  * [PyMatting](http://github.com/pymatting/pymatting) - A library for alpha matting.  * [python-qrcode](https://github.com/lincolnloop/python-qrcode) - A pure Python QR Code generator.  * [pywal](https://github.com/dylanaraps/pywal) - A tool that generates color schemes from images.  * [pyvips](https://github.com/libvips/pyvips) - A fast image processing library with low memory needs.  * [Quads](https://github.com/fogleman/Quads) - Computer art based on quadtrees.  * [scikit-image](http://scikit-image.org/) - A Python library for (scientific) image processing.  * [thumbor](https://github.com/thumbor/thumbor) - A smart imaging service. It enables on-demand crop, re-sizing and flipping of images.  * [wand](https://github.com/dahlia/wand) - Python bindings for [MagickWand](http://www.imagemagick.org/script/magick-wand.php), C API for ImageMagick.    ## Implementations    *Implementations of Python.*    * [CLPython](https://github.com/metawilm/cl-python) - Implementation of the Python programming language written in Common Lisp.  * [CPython](https://github.com/python/cpython) - **Default, most widely used implementation of the Python programming language written in C.**  * [Cython](http://cython.org/) - Optimizing Static Compiler for Python.  * [Grumpy](https://github.com/google/grumpy) - More compiler than interpreter as more powerful CPython2.7 replacement (alpha).  * [IronPython](https://github.com/IronLanguages/ironpython3) - Implementation of the Python programming language written in C#.  * [Jython](https://hg.python.org/jython) - Implementation of Python programming language written in Java for the JVM.  * [MicroPython](https://github.com/micropython/micropython) - A lean and efficient Python programming language implementation.  * [Numba](http://numba.pydata.org/) - Python JIT compiler to LLVM aimed at scientific Python.  * [PeachPy](https://github.com/Maratyszcza/PeachPy) - x86-64 assembler embedded in Python.  * [Pyjion](https://github.com/Microsoft/Pyjion) - A JIT for Python based upon CoreCLR.  * [PyPy](https://foss.heptapod.net/pypy/pypy) - A very fast and compliant implementation of the Python language.  * [Pyston](https://github.com/dropbox/pyston) - A Python implementation using JIT techniques.  * [Stackless Python](https://github.com/stackless-dev/stackless) - An enhanced version of the Python programming language.    ## Interactive Interpreter    *Interactive Python interpreters (REPL).*    * [bpython](https://github.com/bpython/bpython) - A fancy interface to the Python interpreter.  * [Jupyter Notebook (IPython)](https://jupyter.org) - A rich toolkit to help you make the most out of using Python interactively.      * [awesome-jupyter](https://github.com/markusschanta/awesome-jupyter)  * [ptpython](https://github.com/jonathanslenders/ptpython) - Advanced Python REPL built on top of the [python-prompt-toolkit](https://github.com/jonathanslenders/python-prompt-toolkit).    ## Internationalization    *Libraries for working with i18n.*    * [Babel](http://babel.pocoo.org/en/latest/) - An internationalization library for Python.  * [PyICU](https://github.com/ovalhub/pyicu) - A wrapper of International Components for Unicode C++ library ([ICU](http://site.icu-project.org/)).    ## Job Scheduler    *Libraries for scheduling jobs.*    * [Airflow](https://airflow.apache.org/) - Airflow is a platform to programmatically author, schedule and monitor workflows.  * [APScheduler](http://apscheduler.readthedocs.io/en/latest/) - A light but powerful in-process task scheduler that lets you schedule functions.  * [django-schedule](https://github.com/thauber/django-schedule) - A calendaring app for Django.  * [doit](http://pydoit.org/) - A task runner and build tool.  * [gunnery](https://github.com/gunnery/gunnery) - Multipurpose task execution tool for distributed systems with web-based interface.  * [Joblib](https://joblib.readthedocs.io/) - A set of tools to provide lightweight pipelining in Python.  * [Plan](https://github.com/fengsp/plan) - Writing crontab file in Python like a charm.  * [Prefect](https://github.com/PrefectHQ/prefect) - A modern workflow orchestration framework that makes it easy to build, schedule and monitor robust data pipelines.  * [schedule](https://github.com/dbader/schedule) - Python job scheduling for humans.  * [Spiff](https://github.com/knipknap/SpiffWorkflow) - A powerful workflow engine implemented in pure Python.  * [TaskFlow](https://docs.openstack.org/developer/taskflow/) - A Python library that helps to make task execution easy, consistent and reliable.    ## Logging    *Libraries for generating and working with logs.*    * [logbook](http://logbook.readthedocs.io/en/stable/) - Logging replacement for Python.  * [logging](https://docs.python.org/3/library/logging.html) - (Python standard library) Logging facility for Python.  * [loguru](https://github.com/Delgan/loguru) - Library which aims to bring enjoyable logging in Python.  * [sentry-python](https://github.com/getsentry/sentry-python) - Sentry SDK for Python.  * [structlog](https://www.structlog.org/en/stable/) - Structured logging made easy.    ## Machine Learning    *Libraries for Machine Learning. Also see [awesome-machine-learning](https://github.com/josephmisiti/awesome-machine-learning#python).*    * [gym](https://github.com/openai/gym) - A toolkit for developing and comparing reinforcement learning algorithms.  * [H2O](https://github.com/h2oai/h2o-3) - Open Source Fast Scalable Machine Learning Platform.  * [Metrics](https://github.com/benhamner/Metrics) - Machine learning evaluation metrics.  * [NuPIC](https://github.com/numenta/nupic) - Numenta Platform for Intelligent Computing.  * [scikit-learn](http://scikit-learn.org/) - The most popular Python library for Machine Learning.  * [Spark ML](http://spark.apache.org/docs/latest/ml-guide.html) - [Apache Spark](http://spark.apache.org/)'s scalable Machine Learning library.  * [vowpal_porpoise](https://github.com/josephreisinger/vowpal_porpoise) - A lightweight Python wrapper for [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/).  * [xgboost](https://github.com/dmlc/xgboost) - A scalable, portable, and distributed gradient boosting library.  * [MindsDB](https://github.com/mindsdb/mindsdb) - MindsDB is an open source AI layer for existing databases that allows you to effortlessly develop, train and deploy state-of-the-art machine learning models using standard queries.    ## Microsoft Windows    *Python programming on Microsoft Windows.*    * [Python(x,y)](http://python-xy.github.io/) - Scientific-applications-oriented Python Distribution based on Qt and Spyder.  * [pythonlibs](http://www.lfd.uci.edu/~gohlke/pythonlibs/) - Unofficial Windows binaries for Python extension packages.  * [PythonNet](https://github.com/pythonnet/pythonnet) - Python Integration with the .NET Common Language Runtime (CLR).  * [PyWin32](https://github.com/mhammond/pywin32) - Python Extensions for Windows.  * [WinPython](https://winpython.github.io/) - Portable development environment for Windows 7/8.    ## Miscellaneous    *Useful libraries or tools that don't fit in the categories above.*    * [blinker](https://github.com/jek/blinker) - A fast Python in-process signal/event dispatching system.  * [boltons](https://github.com/mahmoud/boltons) - A set of pure-Python utilities.  * [itsdangerous](https://github.com/pallets/itsdangerous) - Various helpers to pass trusted data to untrusted environments.  * [magenta](https://github.com/magenta/magenta) - A tool to generate music and art using artificial intelligence.  * [pluginbase](https://github.com/mitsuhiko/pluginbase) - A simple but flexible plugin system for Python.  * [tryton](http://www.tryton.org/) - A general purpose business framework.    ## Natural Language Processing    *Libraries for working with human languages.*    - General      * [gensim](https://github.com/RaRe-Technologies/gensim) - Topic Modeling for Humans.      * [langid.py](https://github.com/saffsd/langid.py) - Stand-alone language identification system.      * [nltk](http://www.nltk.org/) - A leading platform for building Python programs to work with human language data.      * [pattern](https://github.com/clips/pattern) - A web mining module.      * [polyglot](https://github.com/aboSamoor/polyglot) - Natural language pipeline supporting hundreds of languages.      * [pytext](https://github.com/facebookresearch/pytext) - A natural language modeling framework based on PyTorch.      * [PyTorch-NLP](https://github.com/PetrochukM/PyTorch-NLP) - A toolkit enabling rapid deep learning NLP prototyping for research.      * [spacy](https://spacy.io/) - A library for industrial-strength natural language processing in Python and Cython.      * [Stanza](https://github.com/stanfordnlp/stanza) - The Stanford NLP Group's official Python library, supporting 60+ languages.  - Chinese      * [funNLP](https://github.com/fighting41love/funNLP) - A collection of tools and datasets for Chinese NLP.      * [jieba](https://github.com/fxsjy/jieba) - The most popular Chinese text segmentation library.      * [pkuseg-python](https://github.com/lancopku/pkuseg-python) - A toolkit for Chinese word segmentation in various domains.      * [snownlp](https://github.com/isnowfy/snownlp) - A library for processing Chinese text.    ## Network Virtualization    *Tools and libraries for Virtual Networking and SDN (Software Defined Networking).*    * [mininet](https://github.com/mininet/mininet) - A popular network emulator and API written in Python.  * [napalm](https://github.com/napalm-automation/napalm) - Cross-vendor API to manipulate network devices.  * [pox](https://github.com/noxrepo/pox) - A Python-based SDN control applications, such as OpenFlow SDN controllers.    ## News Feed    *Libraries for building user's activities.*    * [django-activity-stream](https://github.com/justquick/django-activity-stream) - Generating generic activity streams from the actions on your site.  * [Stream Framework](https://github.com/tschellenbach/Stream-Framework) - Building news feed and notification systems using Cassandra and Redis.    ## ORM    *Libraries that implement Object-Relational Mapping or data mapping techniques.*    * Relational Databases      * [Django Models](https://docs.djangoproject.com/en/dev/topics/db/models/) - The Django ORM.      * [SQLAlchemy](https://www.sqlalchemy.org/) - The Python SQL Toolkit and Object Relational Mapper.          * [awesome-sqlalchemy](https://github.com/dahlia/awesome-sqlalchemy)      * [dataset](https://github.com/pudo/dataset) - Store Python dicts in a database - works with SQLite, MySQL, and PostgreSQL.      * [orator](https://github.com/sdispater/orator) -  The Orator ORM provides a simple yet beautiful ActiveRecord implementation.      * [orm](https://github.com/encode/orm) - An async ORM.      * [peewee](https://github.com/coleifer/peewee) - A small, expressive ORM.      * [pony](https://github.com/ponyorm/pony/) - ORM that provides a generator-oriented interface to SQL.      * [pydal](https://github.com/web2py/pydal/) - A pure Python Database Abstraction Layer.  * NoSQL Databases      * [hot-redis](https://github.com/stephenmcd/hot-redis) - Rich Python data types for Redis.      * [mongoengine](https://github.com/MongoEngine/mongoengine) - A Python Object-Document-Mapper for working with MongoDB.      * [PynamoDB](https://github.com/pynamodb/PynamoDB) - A Pythonic interface for [Amazon DynamoDB](https://aws.amazon.com/dynamodb/).      * [redisco](https://github.com/kiddouk/redisco) - A Python Library for Simple Models and Containers Persisted in Redis.    ## Package Management    *Libraries for package and dependency management.*    * [pip](https://pip.pypa.io/en/stable/) - The package installer for Python.      * [pip-tools](https://github.com/jazzband/pip-tools) - A set of tools to keep your pinned Python dependencies fresh.      * [PyPI](https://pypi.org/)  * [conda](https://github.com/conda/conda/) - Cross-platform, Python-agnostic binary package manager.  * [poetry](https://github.com/sdispater/poetry) - Python dependency management and packaging made easy.    ## Package Repositories    *Local PyPI repository server and proxies.*    * [bandersnatch](https://github.com/pypa/bandersnatch/) - PyPI mirroring tool provided by Python Packaging Authority (PyPA).  * [devpi](https://github.com/devpi/devpi) - PyPI server and packaging/testing/release tool.  * [localshop](https://github.com/jazzband/localshop) - Local PyPI server (custom packages and auto-mirroring of pypi).  * [warehouse](https://github.com/pypa/warehouse) - Next generation Python Package Repository (PyPI).    ## Penetration Testing    *Frameworks and tools for penetration testing.*    * [fsociety](https://github.com/Manisso/fsociety) - A Penetration testing framework.  * [setoolkit](https://github.com/trustedsec/social-engineer-toolkit) - A toolkit for social engineering.  * [sqlmap](https://github.com/sqlmapproject/sqlmap) - Automatic SQL injection and database takeover tool.    ## Permissions    *Libraries that allow or deny users access to data or functionality.*    * [django-guardian](https://github.com/django-guardian/django-guardian) - Implementation of per object permissions for Django 1.2+  * [django-rules](https://github.com/dfunckt/django-rules) - A tiny but powerful app providing object-level permissions to Django, without requiring a database.    ## Processes    *Libraries for starting and communicating with OS processes.*    * [delegator.py](https://github.com/amitt001/delegator.py) - [Subprocesses](https://docs.python.org/3/library/subprocess.html) for Humans 2.0.  * [sarge](https://sarge.readthedocs.io/en/latest/) - Yet another wrapper for subprocess.  * [sh](https://github.com/amoffat/sh) - A full-fledged subprocess replacement for Python.    ## Recommender Systems    *Libraries for building recommender systems.*    * [annoy](https://github.com/spotify/annoy) - Approximate Nearest Neighbors in C++/Python optimized for memory usage.  * [fastFM](https://github.com/ibayer/fastFM) - A library for Factorization Machines.  * [implicit](https://github.com/benfred/implicit) - A fast Python implementation of collaborative filtering for implicit datasets.  * [libffm](https://github.com/guestwalk/libffm) - A library for Field-aware Factorization Machine (FFM).  * [lightfm](https://github.com/lyst/lightfm) - A Python implementation of a number of popular recommendation algorithms.  * [spotlight](https://github.com/maciejkula/spotlight) - Deep recommender models using PyTorch.  * [Surprise](https://github.com/NicolasHug/Surprise) - A scikit for building and analyzing recommender systems.  * [tensorrec](https://github.com/jfkirk/tensorrec) - A Recommendation Engine Framework in TensorFlow.    ## Refactoring    *Refactoring tools and libraries for Python*     * [Bicycle Repair Man](http://bicyclerepair.sourceforge.net/) - Bicycle Repair Man, a refactoring tool for Python.   * [Bowler](https://pybowler.io/) - Safe code refactoring for modern Python.   * [Rope](https://github.com/python-rope/rope) -  Rope is a python refactoring library.    ## RESTful API    *Libraries for building RESTful APIs.*    * Django      * [django-rest-framework](http://www.django-rest-framework.org/) - A powerful and flexible toolkit to build web APIs.      * [django-tastypie](http://tastypieapi.org/) - Creating delicious APIs for Django apps.  * Flask      * [eve](https://github.com/pyeve/eve) - REST API framework powered by Flask, MongoDB and good intentions.      * [flask-api](https://github.com/flask-api/flask-api) - Browsable Web APIs for Flask.      * [flask-restful](https://github.com/flask-restful/flask-restful) - Quickly building REST APIs for Flask.  * Pyramid      * [cornice](https://github.com/Cornices/cornice) - A RESTful framework for Pyramid.  * Framework agnostic      * [apistar](https://github.com/encode/apistar) - A smart Web API framework, designed for Python 3.      * [falcon](https://github.com/falconry/falcon) - A high-performance framework for building cloud APIs and web app backends.      * [fastapi](https://github.com/tiangolo/fastapi) - A modern, fast, web framework for building APIs with Python 3.6+ based on standard Python type hints.      * [hug](https://github.com/hugapi/hug) - A Python 3 framework for cleanly exposing APIs.      * [sandman2](https://github.com/jeffknupp/sandman2) - Automated REST APIs for existing database-driven systems.      * [sanic](https://github.com/huge-success/sanic) - A Python 3.6+ web server and web framework that's written to go fast.      * [vibora](https://vibora.io/) - Fast, efficient and asynchronous Web framework inspired by Flask.    ## Robotics    *Libraries for robotics.*    * [PythonRobotics](https://github.com/AtsushiSakai/PythonRobotics) - This is a compilation of various robotics algorithms with visualizations.  * [rospy](http://wiki.ros.org/rospy) - This is a library for ROS (Robot Operating System).    ## RPC Servers    *RPC-compatible servers.*    * [RPyC](https://github.com/tomerfiliba/rpyc) (Remote Python Call) - A transparent and symmetric RPC library for Python  * [zeroRPC](https://github.com/0rpc/zerorpc-python) - zerorpc is a flexible RPC implementation based on [ZeroMQ](http://zeromq.org/) and [MessagePack](http://msgpack.org/).    ## Science    *Libraries for scientific computing. Also see [Python-for-Scientists](https://github.com/TomNicholas/Python-for-Scientists).*    * [astropy](http://www.astropy.org/) - A community Python library for Astronomy.  * [bcbio-nextgen](https://github.com/chapmanb/bcbio-nextgen) - Providing best-practice pipelines for fully automated high throughput sequencing analysis.  * [bccb](https://github.com/chapmanb/bcbb) - Collection of useful code related to biological analysis.  * [Biopython](http://biopython.org/wiki/Main_Page) - Biopython is a set of freely available tools for biological computation.  * [cclib](http://cclib.github.io/) - A library for parsing and interpreting the results of computational chemistry packages.  * [Colour](http://colour-science.org/) - Implementing a comprehensive number of colour theory transformations and algorithms.  * [Karate Club](https://github.com/benedekrozemberczki/karateclub) - Unsupervised machine learning toolbox for graph structured data.  * [NetworkX](https://networkx.github.io/) - A high-productivity software for complex networks.  * [NIPY](http://nipy.org) - A collection of neuroimaging toolkits.  * [NumPy](http://www.numpy.org/) - A fundamental package for scientific computing with Python.  * [ObsPy](https://github.com/obspy/obspy/wiki/) - A Python toolbox for seismology.  * [Open Babel](http://openbabel.org/wiki/Main_Page) - A chemical toolbox designed to speak the many languages of chemical data.  * [PyDy](http://www.pydy.org/) - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion.  * [PyMC](https://github.com/pymc-devs/pymc3) - Markov Chain Monte Carlo sampling toolkit.  * [QuTiP](http://qutip.org/) - Quantum Toolbox in Python.  * [RDKit](http://www.rdkit.org/) - Cheminformatics and Machine Learning Software.  * [SciPy](https://www.scipy.org/) - A Python-based ecosystem of open-source software for mathematics, science, and engineering.  * [SimPy](https://gitlab.com/team-simpy/simpy) -  A process-based discrete-event simulation framework.  * [statsmodels](https://github.com/statsmodels/statsmodels) - Statistical modeling and econometrics in Python.  * [SymPy](https://github.com/sympy/sympy) - A Python library for symbolic mathematics.  * [Zipline](https://github.com/quantopian/zipline) - A Pythonic algorithmic trading library.    ## Search    *Libraries and software for indexing and performing search queries on data.*    * [django-haystack](https://github.com/django-haystack/django-haystack) - Modular search for Django.  * [elasticsearch-dsl-py](https://github.com/elastic/elasticsearch-dsl-py) - The official high-level Python client for Elasticsearch.  * [elasticsearch-py](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/index.html) - The official low-level Python client for [Elasticsearch](https://www.elastic.co/products/elasticsearch).  * [pysolr](https://github.com/django-haystack/pysolr) - A lightweight Python wrapper for [Apache Solr](https://lucene.apache.org/solr/).  * [whoosh](http://whoosh.readthedocs.io/en/latest/) - A fast, pure Python search engine library.    ## Serialization    *Libraries for serializing complex data types*    * [marshmallow](https://github.com/marshmallow-code/marshmallow) - A lightweight library for converting complex objects to and from simple Python datatypes.  * [pysimdjson](https://github.com/TkTech/pysimdjson) - A Python bindings for [simdjson](https://github.com/lemire/simdjson).  * [python-rapidjson](https://github.com/python-rapidjson/python-rapidjson) - A Python wrapper around [RapidJSON](https://github.com/Tencent/rapidjson).  * [ultrajson](https://github.com/esnme/ultrajson) - A fast JSON decoder and encoder written in C with Python bindings.    ## Serverless Frameworks    *Frameworks for developing serverless Python code.*    * [python-lambda](https://github.com/nficano/python-lambda) - A toolkit for developing and deploying Python code in AWS Lambda.  * [Zappa](https://github.com/Miserlou/Zappa) - A tool for deploying WSGI applications on AWS Lambda and API Gateway.    ## Shell    *Shells based on Python.*    * [xonsh](https://github.com/xonsh/xonsh/) - A Python-powered, cross-platform, Unix-gazing shell language and command prompt.    ## Specific Formats Processing    *Libraries for parsing and manipulating specific text formats.*    * General      * [tablib](https://github.com/jazzband/tablib) - A module for Tabular Datasets in XLS, CSV, JSON, YAML.  * Office      * [docxtpl](https://github.com/elapouya/python-docx-template) - Editing a docx document by jinja2 template      * [openpyxl](https://openpyxl.readthedocs.io/en/stable/) - A library for reading and writing Excel 2010 xlsx/xlsm/xltx/xltm files.      * [pyexcel](https://github.com/pyexcel/pyexcel) - Providing one API for reading, manipulating and writing csv, ods, xls, xlsx and xlsm files.      * [python-docx](https://github.com/python-openxml/python-docx) - Reads, queries and modifies Microsoft Word 2007/2008 docx files.      * [python-pptx](https://github.com/scanny/python-pptx) - Python library for creating and updating PowerPoint (.pptx) files.      * [unoconv](https://github.com/unoconv/unoconv) - Convert between any document format supported by LibreOffice/OpenOffice.      * [XlsxWriter](https://github.com/jmcnamara/XlsxWriter) - A Python module for creating Excel .xlsx files.      * [xlwings](https://github.com/ZoomerAnalytics/xlwings) - A BSD-licensed library that makes it easy to call Python from Excel and vice versa.      * [xlwt](https://github.com/python-excel/xlwt) / [xlrd](https://github.com/python-excel/xlrd) - Writing and reading data and formatting information from Excel files.  * PDF      * [PDFMiner](https://github.com/euske/pdfminer) - A tool for extracting information from PDF documents.      * [PyPDF2](https://github.com/mstamy2/PyPDF2) - A library capable of splitting, merging and transforming PDF pages.      * [ReportLab](https://www.reportlab.com/opensource/) - Allowing Rapid creation of rich PDF documents.  * Markdown      * [Mistune](https://github.com/lepture/mistune) - Fastest and full featured pure Python parsers of Markdown.      * [Python-Markdown](https://github.com/waylan/Python-Markdown) - A Python implementation of John Gruberâ€™s Markdown.  * YAML      * [PyYAML](http://pyyaml.org/) - YAML implementations for Python.  * CSV      * [csvkit](https://github.com/wireservice/csvkit) - Utilities for converting to and working with CSV.  * Archive      * [unp](https://github.com/mitsuhiko/unp) - A command line tool that can unpack archives easily.    ## Static Site Generator    *Static site generator is a software that takes some text + templates as input and produces HTML files on the output.*    * [lektor](https://github.com/lektor/lektor) - An easy to use static CMS and blog engine.  * [mkdocs](https://github.com/mkdocs/mkdocs/) - Markdown friendly documentation generator.  * [makesite](https://github.com/sunainapai/makesite) - Simple, lightweight, and magic-free static site/blog generator (< 130 lines).  * [nikola](https://github.com/getnikola/nikola) - A static website and blog generator.  * [pelican](https://github.com/getpelican/pelican) - Static site generator that supports Markdown and reST syntax.    ## Tagging    *Libraries for tagging items.*    * [django-taggit](https://github.com/jazzband/django-taggit) - Simple tagging for Django.    ## Task Queues    *Libraries for working with task queues.*    * [celery](https://docs.celeryproject.org/en/stable/) - An asynchronous task queue/job queue based on distributed message passing.  * [dramatiq](https://github.com/Bogdanp/dramatiq) - A fast and reliable background task processing library for Python 3.  * [huey](https://github.com/coleifer/huey) - Little multi-threaded task queue.  * [mrq](https://github.com/pricingassistant/mrq) - A distributed worker task queue in Python using Redis & gevent.  * [rq](https://github.com/rq/rq) - Simple job queues for Python.    ## Template Engine    *Libraries and tools for templating and lexing.*    * [Genshi](https://genshi.edgewall.org/) - Python templating toolkit for generation of web-aware output.  * [Jinja2](https://github.com/pallets/jinja) - A modern and designer friendly templating language.  * [Mako](http://www.makotemplates.org/) - Hyperfast and lightweight templating for the Python platform.    ## Testing    *Libraries for testing codebases and generating test data.*    * Testing Frameworks      * [hypothesis](https://github.com/HypothesisWorks/hypothesis) - Hypothesis is an advanced Quickcheck style property based testing library.      * [nose2](https://github.com/nose-devs/nose2) - The successor to `nose`, based on `unittest2.      * [pytest](https://docs.pytest.org/en/latest/) - A mature full-featured Python testing tool.      * [Robot Framework](https://github.com/robotframework/robotframework) - A generic test automation framework.      * [unittest](https://docs.python.org/3/library/unittest.html) - (Python standard library) Unit testing framework.  * Test Runners      * [green](https://github.com/CleanCut/green) - A clean, colorful test runner.      * [mamba](http://nestorsalceda.github.io/mamba/) - The definitive testing tool for Python. Born under the banner of BDD.      * [tox](https://tox.readthedocs.io/en/latest/) - Auto builds and tests distributions in multiple Python versions  * GUI / Web Testing      * [locust](https://github.com/locustio/locust) - Scalable user load testing tool written in Python.      * [PyAutoGUI](https://github.com/asweigart/pyautogui) - PyAutoGUI is a cross-platform GUI automation Python module for human beings.      * [Schemathesis](https://github.com/kiwicom/schemathesis) - A tool for automatic property-based testing of web applications built with Open API / Swagger specifications.      * [Selenium](https://pypi.org/project/selenium/) - Python bindings for [Selenium](http://www.seleniumhq.org/) WebDriver.      * [sixpack](https://github.com/seatgeek/sixpack) - A language-agnostic A/B Testing framework.      * [splinter](https://github.com/cobrateam/splinter) - Open source tool for testing web applications.  * Mock      * [doublex](https://pypi.org/project/doublex/) - Powerful test doubles framework for Python.      * [freezegun](https://github.com/spulec/freezegun) - Travel through time by mocking the datetime module.      * [httmock](https://github.com/patrys/httmock) - A mocking library for requests for Python 2.6+ and 3.2+.      * [httpretty](https://github.com/gabrielfalcao/HTTPretty) - HTTP request mock tool for Python.      * [mock](https://docs.python.org/3/library/unittest.mock.html) - (Python standard library) A mocking and patching library.      * [mocket](https://github.com/mindflayer/python-mocket) - A socket mock framework with gevent/asyncio/SSL support.      * [responses](https://github.com/getsentry/responses) - A utility library for mocking out the requests Python library.      * [VCR.py](https://github.com/kevin1024/vcrpy) - Record and replay HTTP interactions on your tests.  * Object Factories      * [factory_boy](https://github.com/FactoryBoy/factory_boy) - A test fixtures replacement for Python.      * [mixer](https://github.com/klen/mixer) - Another fixtures replacement. Supports Django, Flask, SQLAlchemy, Peewee and etc.      * [model_mommy](https://github.com/vandersonmota/model_mommy) - Creating random fixtures for testing in Django.  * Code Coverage      * [coverage](https://pypi.org/project/coverage/) - Code coverage measurement.  * Fake Data      * [fake2db](https://github.com/emirozer/fake2db) - Fake database generator.      * [faker](https://github.com/joke2k/faker) - A Python package that generates fake data.      * [mimesis](https://github.com/lk-geimfari/mimesis) - is a Python library that help you generate fake data.      * [radar](https://pypi.org/project/radar/) - Generate random datetime / time.    ## Text Processing    *Libraries for parsing and manipulating plain texts.*    * General      * [chardet](https://github.com/chardet/chardet) - Python 2/3 compatible character encoding detector.      * [difflib](https://docs.python.org/3/library/difflib.html) - (Python standard library) Helpers for computing deltas.      * [ftfy](https://github.com/LuminosoInsight/python-ftfy) - Makes Unicode text less broken and more consistent automagically.      * [fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy) - Fuzzy String Matching.      * [Levenshtein](https://github.com/ztane/python-Levenshtein/) - Fast computation of Levenshtein distance and string similarity.      * [pangu.py](https://github.com/vinta/pangu.py) - Paranoid text spacing.      * [pyfiglet](https://github.com/pwaller/pyfiglet) - An implementation of figlet written in Python.      * [pypinyin](https://github.com/mozillazg/python-pinyin) - Convert Chinese hanzi (æ¼¢å­—) to pinyin (æ‹¼éŸ³).      * [textdistance](https://github.com/orsinium/textdistance) - Compute distance between sequences with 30+ algorithms.      * [unidecode](https://pypi.org/project/Unidecode/) - ASCII transliterations of Unicode text.  * Slugify      * [awesome-slugify](https://github.com/dimka665/awesome-slugify) - A Python slugify library that can preserve unicode.      * [python-slugify](https://github.com/un33k/python-slugify) - A Python slugify library that translates unicode to ASCII.      * [unicode-slugify](https://github.com/mozilla/unicode-slugify) - A slugifier that generates unicode slugs with Django as a dependency.  * Unique identifiers      * [hashids](https://github.com/davidaurelio/hashids-python) - Implementation of [hashids](http://hashids.org) in Python.      * [shortuuid](https://github.com/skorokithakis/shortuuid) - A generator library for concise, unambiguous and URL-safe UUIDs.  * Parser      * [ply](https://github.com/dabeaz/ply) - Implementation of lex and yacc parsing tools for Python.      * [pygments](http://pygments.org/) - A generic syntax highlighter.      * [pyparsing](https://github.com/pyparsing/pyparsing) - A general purpose framework for generating parsers.      * [python-nameparser](https://github.com/derek73/python-nameparser) - Parsing human names into their individual components.      * [python-phonenumbers](https://github.com/daviddrysdale/python-phonenumbers) - Parsing, formatting, storing and validating international phone numbers.      * [python-user-agents](https://github.com/selwin/python-user-agents) - Browser user agent parser.      * [sqlparse](https://github.com/andialbrecht/sqlparse) - A non-validating SQL parser.    ## Third-party APIs    *Libraries for accessing third party services APIs. Also see [List of Python API Wrappers and Libraries](https://github.com/realpython/list-of-python-api-wrappers).*    * [apache-libcloud](https://libcloud.apache.org/) - One Python library for all clouds.  * [boto3](https://github.com/boto/boto3) - Python interface to Amazon Web Services.  * [django-wordpress](https://github.com/istrategylabs/django-wordpress) - WordPress models and views for Django.  * [facebook-sdk](https://github.com/mobolic/facebook-sdk) - Facebook Platform Python SDK.  * [google-api-python-client](https://github.com/google/google-api-python-client) - Google APIs Client Library for Python.  * [gspread](https://github.com/burnash/gspread) - Google Spreadsheets Python API.  * [twython](https://github.com/ryanmcgrath/twython) - A Python wrapper for the Twitter API.    ## URL Manipulation    *Libraries for parsing URLs.*    * [furl](https://github.com/gruns/furl) - A small Python library that makes parsing and manipulating URLs easy.  * [purl](https://github.com/codeinthehole/purl) - A simple, immutable URL class with a clean API for interrogation and manipulation.  * [pyshorteners](https://github.com/ellisonleao/pyshorteners) - A pure Python URL shortening lib.  * [webargs](https://github.com/marshmallow-code/webargs) - A friendly library for parsing HTTP request arguments with built-in support for popular web frameworks.    ## Video    *Libraries for manipulating video and GIFs.*    * [moviepy](https://zulko.github.io/moviepy/) - A module for script-based movie editing with many formats, including animated GIFs.  * [scikit-video](https://github.com/aizvorski/scikit-video) - Video processing routines for SciPy.  * [vidgear](https://github.com/abhiTronix/vidgear) - Most Powerful multi-threaded Video Processing framework.    ## Web Asset Management    *Tools for managing, compressing and minifying website assets.*    * [django-compressor](https://github.com/django-compressor/django-compressor) - Compresses linked and inline JavaScript or CSS into a single cached file.  * [django-pipeline](https://github.com/jazzband/django-pipeline) - An asset packaging library for Django.  * [django-storages](https://github.com/jschneier/django-storages) - A collection of custom storage back ends for Django.  * [fanstatic](http://www.fanstatic.org/en/latest/) - Packages, optimizes, and serves static file dependencies as Python packages.  * [fileconveyor](http://wimleers.com/fileconveyor) - A daemon to detect and sync files to CDNs, S3 and FTP.  * [flask-assets](https://github.com/miracle2k/flask-assets) - Helps you integrate webassets into your Flask app.  * [webassets](https://github.com/miracle2k/webassets) - Bundles, optimizes, and manages unique cache-busting URLs for static resources.    ## Web Content Extracting    *Libraries for extracting web contents.*    * [html2text](https://github.com/Alir3z4/html2text) - Convert HTML to Markdown-formatted text.  * [lassie](https://github.com/michaelhelmick/lassie) - Web Content Retrieval for Humans.  * [micawber](https://github.com/coleifer/micawber) - A small library for extracting rich content from URLs.  * [newspaper](https://github.com/codelucas/newspaper) - News extraction, article extraction and content curation in Python.  * [python-readability](https://github.com/buriy/python-readability) - Fast Python port of arc90's readability tool.  * [requests-html](https://github.com/psf/requests-html) - Pythonic HTML Parsing for Humans.  * [sumy](https://github.com/miso-belica/sumy) - A module for automatic summarization of text documents and HTML pages.  * [textract](https://github.com/deanmalmgren/textract) - Extract text from any document, Word, PowerPoint, PDFs, etc.  * [toapi](https://github.com/gaojiuli/toapi) - Every web site provides APIs.    ## Web Crawling    *Libraries to automate web scraping.*    * [cola](https://github.com/chineking/cola) - A distributed crawling framework.  * [feedparser](https://pythonhosted.org/feedparser/) - Universal feed parser.  * [grab](https://github.com/lorien/grab) - Site scraping framework.  * [MechanicalSoup](https://github.com/MechanicalSoup/MechanicalSoup) - A Python library for automating interaction with websites.  * [portia](https://github.com/scrapinghub/portia) - Visual scraping for Scrapy.  * [pyspider](https://github.com/binux/pyspider) - A powerful spider system.  * [robobrowser](https://github.com/jmcarp/robobrowser) - A simple, Pythonic library for browsing the web without a standalone web browser.  * [scrapy](https://scrapy.org/) - A fast high-level screen scraping and web crawling framework.    ## Web Frameworks    *Traditional full stack web frameworks. Also see [RESTful API](https://github.com/vinta/awesome-python#restful-api).*    * Synchronous      * [Django](https://www.djangoproject.com/) - The most popular web framework in Python.          * [awesome-django](https://github.com/shahraizali/awesome-django)          * [awesome-django](https://github.com/wsvincent/awesome-django)      * [Flask](http://flask.pocoo.org/) - A microframework for Python.          * [awesome-flask](https://github.com/humiaozuzu/awesome-flask)      * [Pyramid](https://pylonsproject.org/) - A small, fast, down-to-earth, open source Python web framework.          * [awesome-pyramid](https://github.com/uralbash/awesome-pyramid)      * [Masonite](https://github.com/MasoniteFramework/masonite) - The modern and developer centric Python web framework.  * Asynchronous      * [Tornado](http://www.tornadoweb.org/en/latest/) - A web framework and asynchronous networking library.    ## WebSocket    *Libraries for working with WebSocket.*    * [autobahn-python](https://github.com/crossbario/autobahn-python) - WebSocket & WAMP for Python on Twisted and [asyncio](https://docs.python.org/3/library/asyncio.html).  * [channels](https://github.com/django/channels) - Developer-friendly asynchrony for Django.  * [websockets](https://github.com/aaugustin/websockets) - A library for building WebSocket servers and clients with a focus on correctness and simplicity.    ## WSGI Servers    *WSGI-compatible web servers.*    * [bjoern](https://github.com/jonashaag/bjoern) - Asynchronous, very fast and written in C.  * [gunicorn](https://github.com/benoitc/gunicorn) - Pre-forked, ported from Ruby's Unicorn project.  * [uWSGI](https://uwsgi-docs.readthedocs.io/en/latest/) - A project aims at developing a full stack for building hosting services, written in C.  * [waitress](https://github.com/Pylons/waitress) - Multi-threaded, powers Pyramid.  * [werkzeug](https://github.com/pallets/werkzeug) - A WSGI utility library for Python that powers Flask and can easily be embedded into your own projects.    # Resources    Where to discover learning resources or new Python libraries.    ## Books    - [Fluent Python](https://www.oreilly.com/library/view/fluent-python/9781491946237/)  - [Think Python](https://greenteapress.com/wp/think-python-2e/)    ## Websites    * Tutorials      * [Full Stack Python](https://www.fullstackpython.com/)      * [Python Cheatsheet](https://www.pythoncheatsheet.org/)      * [Real Python](https://realpython.com)      * [The Hitchhikerâ€™s Guide to Python](https://docs.python-guide.org/)      * [Ultimate Python study guide](https://github.com/huangsam/ultimate-python)  * Libraries      * [Awesome Python @LibHunt](https://python.libhunt.com/)  * Others      * [Python ZEEF](https://python.zeef.com/alan.richmond)      * [Pythonic News](https://news.python.sc/)      * [What the f*ck Python!](https://github.com/satwikkansal/wtfpython)    ## Newsletters    * [Awesome Python Newsletter](http://python.libhunt.com/newsletter)  * [Pycoder's Weekly](http://pycoders.com/)  * [Python Tricks](https://realpython.com/python-tricks/)  * [Python Weekly](http://www.pythonweekly.com/)    ## Podcasts    * [Django Chat](https://djangochat.com/)  * [Podcast.\_\_init__](https://podcastinit.com/)  * [Python Bytes](https://pythonbytes.fm)  * [Running in Production](https://runninginproduction.com/)  * [Talk Python To Me](https://talkpython.fm/)  * [Test and Code](https://testandcode.com/)  * [The Real Python Podcast](https://realpython.com/podcasts/rpp/)    # Contributing    Your contributions are always welcome! Please take a look at the [contribution guidelines](https://github.com/vinta/awesome-python/blob/master/CONTRIBUTING.md) first.    I will keep some pull requests open if I'm not sure whether those libraries are awesome, you could [vote for them](https://github.com/vinta/awesome-python/pulls) by adding :+1: to them. Pull requests will be merged when their votes reach **20**.    - - -    If you have any question about this opinionated list, do not hesitate to contact me [@VintaChen](https://twitter.com/VintaChen) on Twitter or open an issue on GitHub.   """
Big data;https://github.com/OryxProject/oryx;"""<img align=""right"" src=""http://oryx.io/img/OryxLogoMedium.png"" />    Oryx 2 is a realization of the lambda architecture built on [Apache Spark](http://spark.apache.org)   and [Apache Kafka](http://kafka.apache.org), but with specialization for real-time large scale machine   learning. It is a framework for building applications, but also includes packaged, end-to-end   applications for collaborative filtering, classification, regression and clustering.    Proceed to the [Oryx 2 site](http://oryx.io/) for full documentation.    Just looking to deploy a ready-made, end-to-end application for collaborative filtering, clustering or classification? Easy.  Proceed directly to:    - Prepare your Hadoop cluster with [Cluster Setup](http://oryx.io/docs/admin.html)  - Get a [Release](https://github.com/OryxProject/oryx/releases)  - Prepare a config file from the [Configuration Reference](http://oryx.io/docs/endusers.html#Configuration)  - Run the binaries with [Running Oryx](http://oryx.io/docs/endusers.html#Running)  - Learn about the REST API endpoints you can call in the [API Endpoint Reference](http://oryx.io/docs/endusers.html#API_Endpoint_Reference)    Developers can consume Oryx 2 as a framework for building custom applications as well.   Following the architecture overview below, proceed to   [Making an Oryx App](http://oryx.io/docs/developer.html#Making_an_Oryx_App)   to learn how to create a new application. You can review a [module diagram](https://sourcespy.com/github/oryx/)   as well to understand the project structure.    <img src=""http://oryx.io/img/Architecture.png""/>    ------    [![Build Status](https://travis-ci.org/OryxProject/oryx.svg?branch=master)](https://travis-ci.org/OryxProject/oryx)  [![Coverity](https://scan.coverity.com/projects/2697/badge.svg)](https://scan.coverity.com/projects/2697)  [![codecov.io](https://codecov.io/github/OryxProject/oryx/coverage.svg?branch=master)](https://codecov.io/github/OryxProject/oryx?branch=master) """
Big data;https://github.com/caskdata/tigon;"""# Tigon    ![Tigon Logo](/tigon-docs/developer-guide/source/_images/tigon.png)    **Introduction**    **Tigon** is an open-source, real-time, low-latency, high-throughput stream processing framework.    Tigon is a collaborative effort between Cask Data, Inc. and AT&T that combines   technologies from these companies to create a disruptive new framework to handle a diverse  set of real-time streaming requirements.    Cask Data has built technology that provides scalable, reliable, and persistent high-throughput  event processing with high-level Java APIs using Hadoop and HBase.    AT&T has built a streaming engine that provides massively scalable, flexible, and in-memory  low-latency stream processing with a SQL-like query Language.    Together, they have combined to create **Tigon**.    There are many applications that can take advantage of its features:    - Ability to handle extremely large data flows;  - Exactly-once event processing using an app-level Java API with consistency, reliability, and persistence;  - Streaming database using a SQL-like language to filter, group and join data streams in-memory;  - Runs collections of queries using pipelined query plans;  - Able to transparently handle complex record routing in large parallelized implementations;  - Runs and scales as a native Apache Hadoop YARN Application;  - Reads, writes, and tightly integrates with HDFS and HBase;  - Supports a significant amount of parallelization;  - Fault-tolerance and horizontal scalability without burdening the developer;  - Enterprise security features with debugging, logging, and monitoring tools; and  - Simpler programming model, tools and UI; and  - Open-source software and development process.    For more information, see our collection of   [Guides and other documentation](http://docs.cask.co/tigon/current/en/index.html).    ## Is It Building?    Builds                                                              ------------------------------------------------------------------  [Bamboo Build](https://builds.cask.co/browse/TIGON)                   [GitHub Version](https://github.com/caskdata/tigon/releases/latest)       ## Getting Started    ### Prerequisites    Tigon is supported on *NIX systems such as Linux and Macintosh OS X.  It is not supported on Microsoft Windows.    To install and use Tigon and its included examples, there are a few prerequisites:      1. JDK 6 or JDK 7 (required to run Tigon; note that $JAVA_HOME should be set)    2. GCC    3. G++    4. Apache Maven 3.0+ (required to build the example applications)      Note: To run the TigonSQL Stream Engine outside of Tigon, libz, Perl 5.x, and Python 3.x are required.    ### Download    Pre-compiled sources and related files can be downloaded in a zip file:   [tigon-developer-release-0.2.0.zip.](http://repository.cask.co/downloads/co/cask/tigon/tigon-developer-release/0.2.0/tigon-developer-release-0.2.0.zip)    ### Install     Once the download has completed, unzip the file in a suitable location.    ### Run Instructions    To run Tigon in standalone mode:        $ run_standalone.sh <path-to-flow-jar> <flow-class-name> <run-time-args>    To run Tigon in distributed mode:        $ run_distributed.sh <zookeeper-quorum> <hdfs-namespace>    ### Building from Source    You can also build Tigon directly from the latest source code:        git clone https://github.com/caskdata/tigon.git      cd tigon      mvn clean package -DskipTests -Pdist    After the build completes, you will have a distribution of Tigon under the  `tigon-distribution/target/` directory.      Take the `tigon-sdk-<version>.zip` file and unzip it into a suitable location.      ## Getting Started Guide    Visit our web site for a [Getting Started Guide](http://docs.cask.co/docs/tigon/current/en/getting-started.html)  that will guide you through installing Tigon and running an example.        ## Where to Go Next    Now that you've had a look at the Tigon SDK, take a look at:    - Examples, located in the `/tigon-examples` directory of Tigon  - [Online Examples](http://docs.cask.co/tigon/current/en/examples/index.html)     (demonstrating basic features of Tigon) are located on-line  - [Reference Applications:](https://github.com/caskdata/tigon-apps)    - [AdNetworkFlow:](https://github.com/caskdata/tigon-apps/tree/develop/AdNetworkFlow)      Demonstrates using Tigon to write a realtime bidding (RTB) advertisement framework  - Developer Guides, located in the source distribution in `/tigon-docs/developer-guide/source`    or [online](http://docs.cask.co/tigon/current/en/developer.html)      ## How to Contribute    Interested in helping to improve Tigon? We welcome all contributions, whether in filing detailed  bug reports, submitting pull requests for code changes and improvements, or by asking questions and  assisting others on the mailing list.    ### Bug Reports & Feature Requests    Bugs and tasks are tracked in a public JIRA issue tracker. Details on access will be forthcoming.    ### Pull Requests    We have a simple pull-based development model with a consensus-building phase, similar to Apache's  voting process. If youâ€™d like to help make Tigon better by adding new features, enhancing existing  features, or fixing bugs, here's how to do it:    1. If you are planning a large change or contribution, discuss your plans on the `cask-tigon-dev`     mailing list first.  This will help us understand your needs and best guide your solution in a     way that fits the project.  2. Fork Tigon into your own GitHub repository.  3. Create a topic branch with an appropriate name.  4. Work on the code to your heart's content.  5. Once youâ€™re satisfied, create a pull request from your GitHub repo (itâ€™s helpful if you fill in     all of the description fields).  6. After we review and accept your request, weâ€™ll commit your code to the cask/tigon     repository.    Thanks for helping to improve Tigon!    ### Mailing List    Tigon User Group:   [tigon-user@googlegroups.com](https://groups.google.com/d/forum/tigon-user)    Tigon Development Discussion:   [tigon-dev@googlegroups.com](https://groups.google.com/d/forum/tigon-dev)      ### IRC Channel    Tigon IRC Channel #tigon on irc.freenode.net      ## License and Trademarks    Copyright Â© 2014 Cask Data, Inc.    Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except  in compliance with the License. You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software distributed under the   License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,   either express or implied. See the License for the specific language governing permissions   and limitations under the License.    Cask is a trademark of Cask Data, Inc. All rights reserved.    Apache, Apache HBase, and HBase are trademarks of The Apache Software Foundation. Used with  permission. No endorsement by The Apache Software Foundation is implied by the use of these marks. """
Big data;https://github.com/etsy/411;"""![411](/docs/imgs/logo.png?raw=true)      What is 411?  ============    [![Join the chat at https://gitter.im/411/Lobby](https://badges.gitter.im/411/Lobby.svg)](https://gitter.im/411/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  [![Build Status](https://travis-ci.org/etsy/411.svg?branch=master)](https://travis-ci.org/etsy/411)  [![Code Climate](https://codeclimate.com/github/etsy/411/badges/gpa.svg)](https://codeclimate.com/github/etsy/411)  [![Test Coverage](https://codeclimate.com/github/etsy/411/badges/coverage.svg)](https://codeclimate.com/github/etsy/411/coverage)    Search scheduling  -----------------    Configure Searches to periodically run against a variety of data sources. You can define a custom pipeline of Filters to manipulate any generated Alerts and forward them to multiple Targets.      Alert management  ----------------    Review and manage Alerts through the web interface. You can apply Renderers to alerts to enrich them with additional metadata.      Use cases  =========    - You want to detect when certain log lines show up in ES.  - You want to detect when a Graphite metric changes.  - You want to detect when a server stops responding  - You want to manage alerts through a simple workflow.    And much more!      Setup  =====    - [Setup instructions](/docs/Setup.md)  - [Docker instructions](/docs/Docker.md)      Help  ====    See the [Table of Contents](/docs/README.md) for a list of documentation pages.    If you've any questions, feel free to ask on Gitter. You can also contact us on Twitter at [@sixhundredns](https://twitter.com/sixhundredns) and [@kennysan](https://twitter.com/Kennysan).      Links  =====    - [CaC post](https://codeascraft.com/2016/09/15/introducing-411-a-new-open-source-framework-for-handling-alerting/)  - [Defcon presentation](https://www.youtube.com/watch?v=LQyqhrDl7f8)  - [Slides](https://speakerdeck.com/kennysan/building-effective-security-alerting)  - [Demo](https://demo.fouroneone.io) (User: user, Pass: user)      Contribute  ==========    Check out the contribution [guidelines](/CONTRIBUTING.md).      License  =======    411 is released under the [MIT License](/LICENSE). """
Big data;https://github.com/skizzehq/skizze;"""<img src=""http://i.imgur.com/9z47NdA.png"" align=""center"" height=""190"" width=""600"">  <br>    [![Build Status](https://travis-ci.org/skizzehq/skizze.svg?branch=master)](https://travis-ci.org/skizzehq/skizze) [![license](http://img.shields.io/badge/license-Apache-blue.svg)](https://raw.githubusercontent.com/skizzehq/skizze/master/LICENSE)    Skizze ([ËˆskÉªtÍ¡sÉ™]: german for sketch) is a sketch data store to deal with all problems around counting and sketching using probabilistic data-structures.    Unlike a Key-Value store, Skizze does not store values, but rather appends values to defined sketches, allowing one to solve frequency and cardinality queries in near O(1) time, with minimal memory footprint.    <b> Current status ==> Alpha (tagged v0.0.2) </b>    ## Motivation    Statistical analysis and mining of huge multi-terabyte data sets is a common task nowadays, especially in areas like web analytics and Internet advertising. Analysis of such large data sets often requires powerful distributed data stores like Hadoop and heavy data processing with techniques like MapReduce. This approach often leads to heavyweight high-latency analytical processes and poor applicability to realtime use cases. On the other hand, when one is interested only in simple additive metrics like total page views or average price of conversion, it is obvious that raw data can be efficiently summarized, for example, on a daily basis or using simple in-stream counters.  Computation of more advanced metrics like a number of unique visitor or most frequent items is more challenging and requires a lot of resources if implemented straightforwardly.    Skizze is a (fire and forget) service that provides a probabilistic data structures (sketches) storage that allows estimation of these and many other metrics, with a trade off in precision of the estimations for the memory consumption. These data structures can be used both as temporary data accumulators in query processing procedures and, perhaps more important, as a compact â€“ sometimes astonishingly compact â€“ replacement of raw data in stream-based computing.    ## Example use cases (queries)  * How many distinct elements are in the data set (i.e. what is the cardinality of the data set)?  * What are the most frequent elements (the terms â€œheavy hittersâ€ and â€œtop-k elementsâ€ are also used)?  * What are the frequencies of the most frequent elements?  * How many elements belong to the specified range (range query, in SQL it looks like `SELECT count(v) WHERE v >= c1 AND v < c2)`?  * Does the data set contain a particular element (membership query)?    ## How to build and run  ```  make dist  ./bin/skizze  ```    ## Bindings    Two bindings are currently available:     * [Go](https://github.com/skizzehq/goskizze)      * `go get github.com/skizzehq/goskizze/skizze` [Documentation](https://godoc.org/github.com/skizzehq/goskizze/skizze)        * [Node.js](http://github.com/skizzehq/node-skizze)     * `npm install --save skizze` [Documentation](https://github.com/skizzehq/node-skizze#documentation)       ## Example usage:    Skizze comes with a CLI to help test and explore the server. It can be run via    ```  ./bin/skizze-cli  ```    ### Commands  **Create** a new Domain (Collection of Sketches):  ```{r, engine='bash', count_lines}  #CREATE DOM $name $estCardinality $topk  CREATE DOM demostream 10000000 100  ```    **Add** values to the domain:  ```{r, engine='bash', count_lines}  #ADD DOM $name $value1, $value2 ....  ADD DOM demostream zod joker grod zod zod grod  ```    **Get** the *cardinality* of the domain:  ```{r, engine='bash', count_lines}  # GET CARD $name  GET CARD demostream    # returns:  # Cardinality: 9  ```    **Get** the *rankings* of the domain:  ```{r, engine='bash', count_lines}  # GET RANK $name  GET RANK demostream    # returns:  # Rank: 1	  Value: zod	  Hits: 3  # Rank: 2	  Value: grod	  Hits: 2  # Rank: 3	  Value: joker	  Hits: 1  ```    **Get** the *frequencies* of values in the domain:  ```{r, engine='bash', count_lines}  # GET FREQ $name $value1 $value2 ...  GET FREQ demostream zod joker batman grod    # returns  # Value: zod	  Hits: 3  # Value: joker	  Hits: 1  # Value: batman	  Hits: 0  # Value: grod	  Hits: 2  ```    **Get** the *membership* of values in the domain:  ```{r, engine='bash', count_lines}  # GET MEMB $name $value1 $value2 ...  GET MEMB demostream zod joker batman grod    # returns  # Value: zod	  Member: true  # Value: joker	  Member: true  # Value: batman	  Member: false  # Value: grod	  Member: true  ```    **List** all available sketches (created by domains):  ```{r, engine='bash', count_lines}  LIST    # returns  # Name: demostream  Type: CARD  # Name: demostream  Type: FREQ  # Name: demostream  Type: MEMB  # Name: demostream  Type: RANK  ```    **Create** a new sketch of type $type (CARD, MEMB, FREQ or RANK):  ```{r, engine='bash', count_lines}  # CREATE CARD $name  CREATE CARD demosketch  ```    **Add** values to the sketch of type $type (CARD, MEMB, FREQ or RANK):  ```{r, engine='bash', count_lines}  #ADD $type $name $value1, $value2 ....  ADD CARD demostream zod joker grod zod zod grod  ```    ### License  Skizze is available under the Apache License, Version 2.0.      ### Authors  - [Seif Lotfy](https://twitter.com/seiflotfy)  - [Neil Jagdish Patel](https://twitter.com/njpatel) """
Big data;https://github.com/dagster-io/dagster;"""<p align=""center"">  <img src=""assets/dagster-logo.png"" />  <br /><br />  <a href=""https://badge.fury.io/py/dagster""><img src=""https://badge.fury.io/py/dagster.svg""></>  <a href=""https://coveralls.io/github/dagster-io/dagster?branch=master""><img src=""https://coveralls.io/repos/github/dagster-io/dagster/badge.svg?branch=master""></a>  <a href=""https://buildkite.com/dagster/dagster""><img src=""https://badge.buildkite.com/888545beab829e41e5d7303db15525a2bc3b0f0e33a72759ac.svg?branch=master""></a>  <a href=""https://dagster-slackin.herokuapp.com/""><img src=""https://dagster-slackin.herokuapp.com/badge.svg""></a>  </p>    # Dagster    An orchestration platform for the development, production, and observation of data assets.    Dagster lets you define jobs in terms of the data flow between reusable, logical components, then test locally and run anywhere. With a unified view of jobs and the assets they produce, Dagster can schedule and orchestrate Pandas, Spark, SQL, or anything else that Python can invoke.    Dagster is designed for data platform engineers, data engineers, and full-stack data scientists. Building a data platform with Dagster makes your stakeholders more independent and your systems more robust. Developing data pipelines with Dagster makes testing easier and deploying faster.    ### Develop and test locally, then deploy anywhere    With Dagsterâ€™s pluggable execution, the same computations can run in-process against your local file system, or on a distributed work queue against your production data lake. You can set up Dagsterâ€™s web interface in a minute on your laptop, deploy it on-premise, or in any cloud.    ### Model and type the data produced and consumed by each step    Dagster models data dependencies between steps in your orchestration graph and handles passing data between them. Optional typing on inputs and outputs helps catch bugs early.    ### Link data to computations    Dagsterâ€™s Asset Manager tracks the data sets and ML models produced by your jobs, so you can understand how they were generated and trace issues when they donâ€™t look how you expect.    ### Build a self-service data platform    Dagster helps platform teams build systems for data practitioners. Jobs are built from shared, reusable, configurable data processing and infrastructure components. Dagit, Dagsterâ€™s web interface, lets anyone inspect these objects and discover how to use them.    ### Avoid dependency nightmares    Dagsterâ€™s repository model lets you isolate codebases so that problems in one job donâ€™t bring down the rest. Each job can have its own package dependencies and Python version. Jobs are run in isolated processes so user code issues can't bring the system down.    ### Debug pipelines from a rich UI    Dagit, Dagsterâ€™s web interface, includes expansive facilities for understanding the jobs it orchestrates. When inspecting a run of your job, you can query over logs, discover the most time consuming tasks via a Gantt chart, re-execute subsets of steps, and more.    ## Getting Started    ### Installation    Dagster is available on PyPI, and officially supports Python 3.6+.    ```bash  $ pip install dagster dagit  ```    This installs two modules:    - **Dagster**: the core programming model and abstraction stack; stateless, single-node,    single-process and multi-process execution engines; and a CLI tool for driving those engines.  - **Dagit**: the UI for developing and operating Dagster pipelines, including a DAG browser, a    type-aware config editor, and a live execution interface.    ### Learn    Next, jump right into our [tutorial](https://docs.dagster.io/tutorial/), or read our [complete  documentation](https://docs.dagster.io). If you're actively using Dagster or have questions on  getting started, we'd love to hear from you:    <br />  <p align=""center"">  <a href=""https://join.slack.com/t/dagster/shared_invite/enQtNjEyNjkzNTA2OTkzLTI0MzdlNjU0ODVhZjQyOTMyMGM1ZDUwZDQ1YjJmYjI3YzExZGViMDI1ZDlkNTY5OThmYWVlOWM1MWVjN2I3NjU""><img src=""https://user-images.githubusercontent.com/609349/63558739-f60a7e00-c502-11e9-8434-c8a95b03ce62.png"" width=160px; /></a>  </p>    ## Contributing    For details on contributing or running the project for development, check out our [contributing  guide](https://docs.dagster.io/community/contributing/). <br />    ## Integrations    Dagster works with the tools and systems that you're already using with your data, including:    <table>  	<thead>  		<tr style=""background-color: #ddd"" align=""center"">  			<td colspan=2><b>Integration</b></td>  			<td><b>Dagster Library</b></td>  		</tr>  	</thead>  	<tbody>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle""  src=""https://user-images.githubusercontent.com/609349/57987547-a7e36b80-7a37-11e9-95ae-4c4de2618e87.png""></td>  			<td style=""border-left: 0px""> <b>Apache Airflow</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-airflow"" />dagster-airflow</a><br />Allows Dagster pipelines to be scheduled and executed, either containerized or uncontainerized, as <a href=""https://github.com/apache/airflow"">Apache Airflow DAGs</a>.</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle""  src=""https://user-images.githubusercontent.com/609349/57987976-5ccc5700-7a3d-11e9-9fa5-1a51299b1ccb.png""></td>  			<td style=""border-left: 0px""> <b>Apache Spark</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-spark"" />dagster-spark</a> &middot;Â <a href=""https://docs.dagster.io/_apidocs/libraries/dagster-pyspark"" />dagster-pyspark</a>  			<br />Libraries for interacting with Apache Spark and PySpark.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle""  src=""https://user-images.githubusercontent.com/609349/58348728-48f66b80-7e16-11e9-9e9f-1a0fea9a49b4.png""></td>  			<td style=""border-left: 0px""> <b>Dask</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-dask"" />dagster-dask</a>  			<br />Provides a Dagster integration with Dask / Dask.Distributed.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/58349731-f36f8e00-7e18-11e9-8a2e-86e086caab66.png""></td>  			<td style=""border-left: 0px""> <b>Datadog</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-datadog"" />dagster-datadog</a>  			<br />Provides a Dagster resource for publishing metrics to Datadog.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/57987809-bf245800-7a3b-11e9-8905-494ed99d0852.png"" />  			&nbsp;/&nbsp; <img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/57987827-fa268b80-7a3b-11e9-8a18-b675d76c19aa.png"">  			</td>  			<td style=""border-left: 0px""> <b>Jupyter / Papermill</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagstermill"" />dagstermill</a><br />Built on the <a href=""https://github.com/nteract/papermill"">papermill library</a>, dagstermill is meant for integrating productionized Jupyter notebooks into dagster pipelines.</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle""  src=""https://user-images.githubusercontent.com/609349/57988016-f431aa00-7a3d-11e9-8cb6-1309d4246b27.png""></td>  			<td style=""border-left: 0px""> <b>PagerDuty</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-pagerduty"" />dagster-pagerduty</a>  			<br />A library for creating PagerDuty alerts from Dagster workflows.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/58349397-fcac2b00-7e17-11e9-900c-9ab8cf7cb64a.png""></td>  			<td style=""border-left: 0px""> <b>Snowflake</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-snowflake"" />dagster-snowflake</a>  			<br />A library for interacting with the Snowflake Data Warehouse.  			</td>  		</tr>  		<tr style=""background-color: #ddd"">  			<td colspan=2 align=""center""><b>Cloud Providers</b></td>  			<td><b></b></td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/57987557-c2b5e000-7a37-11e9-9310-c274481a4682.png""> </td>  			<td style=""border-left: 0px""><b>AWS</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-aws"" />dagster-aws</a>  			<br />A library for interacting with Amazon Web Services. Provides integrations with Cloudwatch, S3, EMR, and Redshift.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/84176312-0bbb4680-aa36-11ea-9580-a70758b12161.png""> </td>  			<td style=""border-left: 0px""><b>Azure</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-azure"" />dagster-azure</a>  			<br />A library for interacting with Microsoft Azure.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/57987566-f98bf600-7a37-11e9-81fa-b8ca1ea6cc1e.png""> </td>  			<td style=""border-left: 0px""><b>GCP</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-gcp"" />dagster-gcp</a>  			<br />A library for interacting with Google Cloud Platform. Provides integrations with GCS, BigQuery, and Cloud Dataproc.  			</td>  		</tr>  	</tbody>  </table>    This list is growing as we are actively building more integrations, and we welcome contributions! """
Big data;https://github.com/snowplow/snowplow;"""# Snowplow    [![Release][release-badge]][release]  [![License][license-image]][license]  [![Discourse posts][discourse-image]][discourse]    [![Snowplow logo][logo-image]][website]    ## Overview    Snowplow is an enterprise-strength marketing and product analytics platform. It does three things:    1. Identifies your users, and tracks the way they engage with your website or application  2. Stores your users' behavioral data in a scalable ""event data warehouse"" you control: Amazon Redshift, Google BigQuery, Snowflake or Elasticsearch  3. Lets you leverage the biggest range of tools to analyze that data, including big data tools (e.g. Spark) via EMR or more traditional tools e.g. Looker, Mode, Superset, Re:dash to analyze that behavioral data    **To find out more, please check out the [Snowplow website][website] and the [docs website][docs].**    ### Version Compatibility Matrix    For compatibility assurance, the version compatibility matrix offers clarity on our recommended stack. It is strongly recommended when setting up a Snowplow pipeline to use the versions listed in the version compatibility matrix which can be found [within our docs][version-compatibility].    ### Public Roadmap    This repository also contains the [Snowplow Public Roadmap][roadmap]. The Public Roadmap lets you stay up to date and find out what's happening on the Snowplow Platform. Help us prioritize our cards: open the issue and leave a ðŸ‘ to vote for your favorites. Want us to build a feature or function? Tell us by heading to our [Discourse forum][discourse] ðŸ’¬.    ### Try Snowplow    Setting up a full open-source Snowplow pipeline requires a non-trivial amount of engineering expertise and time investment.  You might be interested in finding out what Snowplow can do first, by setting up [Try Snowplow][try-snowplow].    ### Open Source Quick Start    The [Open Source Quick Start][open-source-quick-start] will help you get up and running with a Snowplow open source pipeline. Snowplow publishes a [set of terraform modules][terraform-modules], which automate the setting up & deployment of the required infrastructure & applications for an operational Snowplow open source pipeline, with just a handful of input variables required on your side.    ### Join the Snowplow Research Panel and help shape the future of open source    As part of our ongoing efforts to improve the Snowplow Open Source experience, we're looking for users of our open-source software and  members of our community to take part in research studies. [Join here][research-survey].    ### Our Commercial Offering    If you wish to get everything setup and managed for you, you can consider [Snowplow BDP][snowplow-bdp]. You can also [request a demo][request-a-demo].    ## Snowplow technology 101    [![Snowplow architecture][architecture-image]][architecture]    The repository structure follows the conceptual architecture of Snowplow, which consists of six loosely-coupled sub-systems connected by five standardized data protocols/formats.    To briefly explain these six sub-systems:    * **[Trackers][trackers]** fire Snowplow events. Currently we have 15 trackers, covering web, mobile, desktop, server and IoT  * **[Collector][collector]** receives Snowplow events from trackers. Currently we have one official collector implementation with different sinks: Amazon Kinesis, Google PubSub, Amazon SQS, Apache Kafka and NSQ  * **[Enrich][enrich]** cleans up the raw Snowplow events, enriches them and puts them into storage. Currently we have several implementations, built for different environments (GCP, AWS, Apache Kafka) and one core library  * **[Storage][storage]** is where the Snowplow events live. Currently we store the Snowplow events in a flat file structure on S3, and in the Redshift, Postgres, Snowflake and BigQuery databases  * **[Data modeling][data-modeling]** is where event-level data is joined with other data sets and aggregated into smaller data sets, and business logic is applied. This produces a clean set of tables which make it easier to perform analysis on the data. We officially support data models for Redshift, Snowflake and BigQuery.  * **[Analytics][analytics-sdks]** are performed on the Snowplow events or on the aggregate tables.    **For more information on the current Snowplow architecture, please see the [Technical architecture][architecture]**.    ## About this repository    This repository is an umbrella repository for all loosely-coupled Snowplow components and is updated on each component release.    Since June 2020, all components have been extracted into their dedicated repositories (more info [here][split-blogpost])  and this repository serves as an entry point for Snowplow users, the home of our public roadmap and as a historical artifact.    Components that have been extracted to their own repository are still here as [git submodules][submodules].    ### Trackers    |                Web               |           Mobile           |         Gaming         |          TV          |       Desktop & Server        |  |:--------------------------------:|:--------------------------:|:----------------------:|:--------------------:|:-----------------------------:|  | [JavaScript][javascript-tracker] | [Android][android-tracker] | [Unity][unity-tracker] | [Roku][roku-tracker] | [Command line][tracking-cli]  |  | [AMP][amp-tracker]               | [iOS][ios-tracker]         |                        |                      | [.NET][dotnet-tracker]        |  |                                  | [React Native][rn-tracker] |                        |                      | [Go][golang-tracker]          |  |                                  | [Flutter][flutter-tracker] |                        |                      | [Java][java-tracker]          |  |                                  |                            |                        |                      | [Node.js][javascript-tracker] |  |                                  |                            |                        |                      | [PHP][php-tracker]            |  |                                  |                            |                        |                      | [Python][python-tracker]      |  |                                  |                            |                        |                      | [Ruby][ruby-tracker]          |  |                                  |                            |                        |                      | [Scala][scala-tracker]        |    ### [Collector](https://github.com/snowplow/stream-collector)    ### [Enrich](https://github.com/snowplow/enrich)    ### Loaders    * [BigQuery (streaming)](https://github.com/snowplow-incubator/snowplow-bigquery-loader)  * [Redshift (batch)](https://github.com/snowplow/snowplow-rdb-loader)  * [Snowflake (batch)](https://github.com/snowplow-incubator/snowplow-snowflake-loader)  * [Google Cloud Storage (streaming)](https://github.com/snowplow-incubator/snowplow-google-cloud-storage-loader)  * [Amazon S3 (streaming)](https://github.com/snowplow/snowplow-s3-loader)  * [Postgres (streaming)](https://github.com/snowplow-incubator/snowplow-postgres-loader)  * [Elasticsearch (streaming)](https://github.com/snowplow/snowplow-elasticsearch-loader)    ### Iglu    * [Iglu Server](https://github.com/snowplow-incubator/iglu-server/)  * [igluctl](https://github.com/snowplow-incubator/igluctl/)  * [Iglu Central](https://github.com/snowplow/iglu-central/)    ### Data modeling    #### Web    * [Web model: SQL-Runner version](https://github.com/snowplow/data-models/tree/master/web/v1)  * [Web model: dbt version](https://github.com/snowplow/dbt-snowplow-web)    #### Mobile    * [Mobile model: SQL-Runner version](https://github.com/snowplow/data-models/tree/master/mobile/v1)    ### Testing    * [Mini](https://github.com/snowplow/snowplow-mini)  * [Micro](https://github.com/snowplow-incubator/snowplow-micro)    ### Parsing enriched event    * [Analytics SDK Scala](https://github.com/snowplow/snowplow-scala-analytics-sdk)  * [Analytics SDK Python](https://github.com/snowplow/snowplow-python-analytics-sdk)  * [Analytics SDK .NET](https://github.com/snowplow/snowplow-dotnet-analytics-sdk)  * [Analytics SDK Javascript](https://github.com/snowplow-incubator/snowplow-js-analytics-sdk/)  * [Analytics SDK Golang](https://github.com/snowplow/snowplow-golang-analytics-sdk)    ### [Bad rows](https://github.com/snowplow-incubator/snowplow-badrows)    ### [Terraform Modules][terraform-modules]    ## Need help?    We want to make it super-easy for Snowplow users and contributors to talk to us and connect with each other, to share ideas, solve problems and help make Snowplow awesome. Here are the main channels we're running currently, we'd love to hear from you on one of them:    ### [Discourse][discourse]    This is for all Snowplow users: engineers setting up Snowplow, data modelers structuring the data and data consumers building insights. You can find guides, recipes, questions and answers from Snowplow users including the Snowplow team.    We welcome all questions and contributions!    ### Twitter    [@SnowplowData][snowplow-twitter] for official news or [@SnowplowLabs][snowplow-labs-twitter] for engineering-heavy conversations and release updates.    ### GitHub    If you spot a bug, then please raise an issue in the GitHub repository of the component in question.  Likewise if you have developed a cool new feature or an improvement, please open a pull request,  we'll be glad to integrate it in the codebase!    If you want to brainstorm a potential new feature, then [Discourse][discourse] is the best place to start.    ### Email    [community@snowplowanalytics.com][community-email]    If you want to talk directly to us (e.g. about a commercially sensitive issue), email is the easiest way.    ## Copyright and license    Snowplow is copyright 2012-2022 Snowplow Analytics Ltd.    Licensed under the **[Apache License, Version 2.0][license]** (the ""License"");  you may not use this software except in compliance with the License.    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License.    [license-image]: https://img.shields.io/badge/license-Apache--2-blue.svg?style=flat  [license]: https://www.apache.org/licenses/LICENSE-2.0    [logo-image]: media/snowplow_logo.png  [website]: https://snowplowanalytics.com  [docs]: https://docs.snowplowanalytics.com/open-source-docs/    [snowplow-bdp]: https://snowplowanalytics.com/products/snowplow-bdp/  [version-compatibility]: https://docs.snowplowanalytics.com/docs/pipeline-components-and-applications/version-compatibility-matrix/  [try-snowplow]: https://try.snowplowanalytics.com/?utm_source=github&utm_medium=post&utm_campaign=try-snowplow  [request-a-demo]: https://go.snowplowanalytics.com/l/571483/2021-05-04/3sv1pg8  [roadmap]: https://github.com/snowplow/snowplow/projects  [open-source-quick-start]: https://docs.snowplowanalytics.com/docs/open-source-quick-start/  [terraform-modules]: https://registry.terraform.io/modules/snowplow-devops  [research-survey]: https://forms.gle/pCtYx8naum7A8vvw5    [architecture-image]: media/snowplow_architecture.png  [architecture]: ./ARCHITECTURE.md    [trackers]: https://github.com/snowplow/snowplow/tree/master/1-trackers  [collector]: https://github.com/snowplow/snowplow/tree/master/2-collectors  [enrich]: https://github.com/snowplow/snowplow/tree/master/3-enrich  [storage]: https://github.com/snowplow/snowplow/tree/master/4-storage  [data-modeling]: https://github.com/snowplow/snowplow/tree/master/5-data-modeling  [analytics-sdks]: https://docs.snowplowanalytics.com/docs/modeling-your-data/analytics-sdk/    [split-blogpost]: https://snowplowanalytics.com/blog/2020/07/16/changing-releasing/  [submodules]: https://git-scm.com/book/en/v2/Git-Tools-Submodules    [discourse-image]: https://img.shields.io/discourse/posts?server=https%3A%2F%2Fdiscourse.snowplowanalytics.com%2F  [discourse]: http://discourse.snowplowanalytics.com/  [snowplow-twitter]: https://twitter.com/SnowplowData  [snowplow-labs-twitter]: https://twitter.com/SnowplowLabs  [community-email]: mailto:community@snowplowanalytics.com    [release]: https://github.com/snowplow/snowplow/releases/tag/22.01  [release-badge]: https://img.shields.io/badge/Snowplow-22.01%20Western%20Ghats-6638b8    [javascript-tracker]: https://github.com/snowplow/snowplow-javascript-tracker  [amp-tracker]: https://docs.snowplowanalytics.com/docs/collecting-data/collecting-from-own-applications/google-amp-tracker/  [android-tracker]: https://github.com/snowplow/snowplow-android-tracker  [ios-tracker]: https://github.com/snowplow/snowplow-objc-tracker  [rn-tracker]: https://github.com/snowplow-incubator/snowplow-react-native-tracker  [roku-tracker]: https://github.com/snowplow-incubator/snowplow-roku-tracker  [flutter-tracker]: https://github.com/snowplow-incubator/snowplow-flutter-tracker  [tracking-cli]: https://github.com/snowplow/snowplow-tracking-cli  [dotnet-tracker]: https://github.com/snowplow/snowplow-dotnet-tracker  [golang-tracker]: https://github.com/snowplow/snowplow-golang-tracker  [java-tracker]: https://github.com/snowplow/snowplow-java-tracker  [php-tracker]: https://github.com/snowplow/snowplow-php-tracker  [python-tracker]: https://github.com/snowplow/snowplow-python-tracker  [ruby-tracker]: https://github.com/snowplow/snowplow-ruby-tracker  [scala-tracker]: https://github.com/snowplow/snowplow-scala-tracker  [unity-tracker]: https://github.com/snowplow/snowplow-unity-tracker """
Big data;https://github.com/Netflix/suro;"""# Suro: Netflix's Data Pipeline    Suro is a data pipeline service for collecting, aggregating, and dispatching large volume of application events including log data. It has the following features:    - It is distributed and can be horizontally scaled.  - It supports streaming data flow, large number of connections, and high throughput.  - It allows dynamically dispatching events to different locations with flexible dispatching rules.  - It has a simple and flexible architecture to allow users to add additional data destinations.  - It fits well into NetflixOSS ecosystem  - It is a best-effort data pipeline with support of flexible retries and store-and-forward to minimize message loss    Learn more about Suro on the <a href=""https://github.com/Netflix/suro/wiki"">Suro Wiki</a> and the <a href=""http://techblog.netflix.com/2013/12/announcing-suro-backbone-of-netflixs.html"">Netflix TechBlog post</a> where Suro was introduced.    ## Master Build Status    <a href='https://netflixoss.ci.cloudbees.com/job/suro-master/'><img src='https://netflixoss.ci.cloudbees.com/job/suro-master/badge/icon'></a>    ## Pull Request Build Status    <a href='https://netflixoss.ci.cloudbees.com/job/suro-pull-requests/'><img src='https://netflixoss.ci.cloudbees.com/job/suro-pull-requests/badge/icon'></a></img></a></img></a>    Build  -----    NetflixGraph is built via Gradle (www.gradle.org). To build from the command line:        ./gradlew build    See the `build.gradle` file for other gradle targets, like `distTar`, `distZip`, `installApp`, and `runServer`.    Running the server  ------------------    You can run the server locally by just running `./gradlew runServer`.    More more advanced usage you may wish to run `./gradlew installApp` and then:    	cd suro-server  	java -cp ""build/install/suro-server/lib/*"" com.netflix.suro.SuroServer -m conf/routingmap.json -s conf/sink.json -i conf/input.json    To enable basic logging you can downloaded `slf4j-simple-1.7.7.jar` and copy it into `suro-server` then run:    	cd suro-server  	java -cp ""build/install/suro-server/lib/*:slf4j-simple-1.7.7.jar"" com.netflix.suro.SuroServer -m conf/routingmap.json -s conf/sink.json -i conf/input.json    Support  -----    We will use the Google Group, Suro Users, to discuss issues: https://groups.google.com/forum/#!forum/suro-users """
Big data;https://github.com/twitter/summingbird;"""## Summingbird [![Build Status](https://secure.travis-ci.org/twitter/summingbird.png)](http://travis-ci.org/twitter/summingbird)    Summingbird is a library that lets you write MapReduce programs that look like native Scala or Java collection transformations and execute them on a number of well-known distributed MapReduce platforms, including [Storm](https://github.com/nathanmarz/storm) and [Scalding](https://github.com/twitter/scalding).    ![Summingbird Logo](https://raw.github.com/twitter/summingbird/develop/logo/summingbird_logo.png)    While a word-counting aggregation in pure Scala might look like this:    ```scala    def wordCount(source: Iterable[String], store: MutableMap[String, Long]) =      source.flatMap { sentence =>        toWords(sentence).map(_ -> 1L)      }.foreach { case (k, v) => store.update(k, store.get(k) + v) }  ```    Counting words in Summingbird looks like this:    ```scala    def wordCount[P <: Platform[P]]      (source: Producer[P, String], store: P#Store[String, Long]) =        source.flatMap { sentence =>          toWords(sentence).map(_ -> 1L)        }.sumByKey(store)  ```    The logic is exactly the same, and the code is almost the same. The main difference is that you can execute the Summingbird program in ""batch mode"" (using [Scalding](https://github.com/twitter/scalding)), in ""realtime mode"" (using [Storm](https://github.com/nathanmarz/storm)), or on both Scalding and Storm in a hybrid batch/realtime mode that offers your application very attractive fault-tolerance properties.    Summingbird provides you with the primitives you need to build rock solid production systems.    ## Getting Started: Word Count with Twitter    The `summingbird-example` project allows you to run the wordcount program above on a sample of Twitter data using a local Storm topology and memcache instance. You can find the actual job definition in [ExampleJob.scala](https://github.com/twitter/summingbird/blob/develop/summingbird-example/src/main/scala/com/twitter/summingbird/example/ExampleJob.scala).    First, make sure you have `memcached` installed locally. If not, if you're on OS X, you can get it by installing [Homebrew](http://brew.sh/) and running this command in a shell:    ```bash  brew install memcached  ```    When this is finished, run the `memcached` command in a separate terminal.    Now you'll need to set up access to the Twitter Streaming API. [This blog post](http://tugdualgrall.blogspot.com/2012/11/couchbase-create-large-dataset-using.html) has a great walkthrough, so open that page, head over to https://dev.twitter.com/ and get your various keys and tokens. Once you have these, clone the Summingbird repository:    ```bash  git clone https://github.com/twitter/summingbird.git  cd summingbird  ```    And open [StormRunner.scala](https://github.com/twitter/summingbird/blob/develop/summingbird-example/src/main/scala/com/twitter/summingbird/example/StormRunner.scala) in your editor. Replace the dummy variables under `config` variable with your auth tokens:    ```scala  lazy val config = new ConfigurationBuilder()      .setOAuthConsumerKey(""mykey"")      .setOAuthConsumerSecret(""mysecret"")      .setOAuthAccessToken(""token"")      .setOAuthAccessTokenSecret(""tokensecret"")      .setJSONStoreEnabled(true) // required for JSON serialization      .build  ```    You're all ready to go! Now it's time to unleash Storm on your Twitter stream. Make sure the `memcached` terminal is still open, then start Storm from the `summingbird` directory:    ```bash  ./sbt ""summingbird-example/run --local""  ```    Storm should puke out a bunch of output, then stabilize and hang. This means that Storm is updating your local memcache instance with counts of every word that it sees in each tweet.    To query the aggregate results in Memcached, you'll need to open an SBT repl in a new terminal:    ```bash  ./sbt summingbird-example/console  ```    At the launched repl, run the following:    ```scala  scala> import com.twitter.summingbird.example._  import com.twitter.summingbird.example._    scala> StormRunner.lookup(""i"")  <memcache store loading elided>  res0: Option[Long] = Some(5)    scala> StormRunner.lookup(""i"")  res1: Option[Long] = Some(52)  ```    Boom. Counts for the word `""i""` are growing in realtime.    See the [wiki page](https://github.com/twitter/summingbird/wiki/Getting-started-with-summingbird-example) for a more detailed explanation of the configuration required to get this job up and running and some ideas for where to go next.    ## Community and Documentation    This, and all [github.com/twitter](https://github.com/twitter) projects, are under the [Twitter Open Source Code of Conduct](https://engineering.twitter.com/opensource/code-of-conduct). Additionally, see the [Typelevel Code of Conduct](http://typelevel.org/conduct) for specific examples of harassing behavior that are not tolerated.    To learn more and find links to tutorials and information around the web, check out the [Summingbird Wiki](https://github.com/twitter/summingbird/wiki).    The latest ScalaDocs are hosted on Summingbird's [Github Project Page](http://twitter.github.io/summingbird).    Discussion occurs primarily on the [Summingbird mailing list](https://groups.google.com/forum/#!forum/summingbird). Issues should be reported on the GitHub issue tracker. Simpler issues appropriate for first-time contributors looking to help out are tagged ""newbie"".    IRC: freenode channel #summingbird    Follow [@summingbird](https://twitter.com/summingbird) on Twitter for updates.    Please feel free to use the beautiful [Summingbird logo](https://drive.google.com/folderview?id=0B3i3pDi3yVgNMHV0TXVkTGZteWM&usp=sharing) artwork anywhere.    ## Maven    Summingbird modules are published on maven central. The current groupid and version for all modules is, respectively, `""com.twitter""` and  `0.9.1`.    Current published artifacts are    * `summingbird-core_2.11`  * `summingbird-core_2.10`  * `summingbird-batch_2.11`  * `summingbird-batch_2.10`  * `summingbird-client_2.11`  * `summingbird-client_2.10`  * `summingbird-storm_2.11`  * `summingbird-storm_2.10`  * `summingbird-scalding_2.11`  * `summingbird-scalding_2.10`  * `summingbird-builder_2.11`  * `summingbird-builder_2.10`    The suffix denotes the scala version.    ## Authors (alphabetically)    * Oscar Boykin <https://twitter.com/posco>  * Ian O'Connell <https://twitter.com/0x138>  * Sam Ritchie <https://twitter.com/sritchie>  * Ashutosh Singhal <https://twitter.com/daashu>    ## License    Copyright 2013 Twitter, Inc.    Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 """
Big data;https://github.com/Hydrospheredata/mist;"""[![Build Status](https://ci.hydrosphere.io/buildStatus/icon?job=hydrosphere.io/mist/master)](https://ci.hydrosphere.io/job/hydrosphere.io/job/mist/job/master/)  [![Build Status](https://travis-ci.org/Hydrospheredata/mist.svg?branch=master)](https://travis-ci.org/Hydrospheredata)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/io.hydrosphere/mist-lib_2.11/badge.svg)](https://maven-badges.herokuapp.com/maven-central/io.hydrosphere/mist-lib_2.11/)  [![Docker Hub Pulls](https://img.shields.io/docker/pulls/hydrosphere/mist.svg)](https://img.shields.io/docker/pulls/hydrosphere/mist.svg)  # Hydrosphere Mist    [![Join the chat at https://gitter.im/Hydrospheredata/mist](https://badges.gitter.im/Hydrospheredata/mist.svg)](https://gitter.im/Hydrospheredata/mist?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    [Hydrosphere](http://hydrosphere.io) Mist is a serverless proxy for Spark cluster.  Mist provides a new functional programming framework and deployment model for Spark applications.     Please see our [quick start guide](https://hydrosphere.io/mist-docs/quick_start.html) and [documentation](https://hydrosphere.io/mist-docs/)    Features:  * **Spark Function as a Service**. Deploy Spark functions rather than notebooks or scripts.  * Spark Cluster and Session management. Fully managed Spark sessions backed by on-demand EMR, Hortonworks, Cloudera, DC/OS and vanilla Spark clusters.  * **Typesafe** programming framework that clearly defines inputs and outputs of every Spark job.  * **REST** HTTP & Messaging (MQTT, Kafka) API for Scala & Python Spark jobs.  * Multi-cluster mode: Seamless Spark cluster on-demand provisioning, autoscaling and termination(**pending**)  ![Cluster of Spark Clusters](http://dv9c7babquml0.cloudfront.net/docs-images/mist-cluster-of-spark-clusters.gif)    It creates a unified API layer for building enterprise solutions and microservices on top of a Spark functions.    ![Mist use cases](http://dv9c7babquml0.cloudfront.net/docs-images/mist-use-case.png)    ## High Level Architecture    ![High Level Architecture](http://dv9c7babquml0.cloudfront.net/docs-images/mist-highlevel-architecture.png)    ## Contact    Please report bugs/problems to:   <https://github.com/Hydrospheredata/mist/issues>.    <http://hydrosphere.io/>    [LinkedIn](https://www.linkedin.com/company/hydrospherebigdata)    [Facebook](https://www.facebook.com/hydrosphere.io/)    [Twitter](https://twitter.com/hydrospheredata) """
Big data;https://github.com/benedekrozemberczki/awesome-fraud-detection-papers;"""# Awesome Fraud Detection Research Papers.  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)  [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)  [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/awesome-fraud-detection-papers.svg)](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers/archive/master.zip)  ![License](https://img.shields.io/github/license/benedekrozemberczki/awesome-fraud-detection-papers.svg?color=blue) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)  <p align=""center"">    <img width=""450"" src=""fraud.png"">  </p>  A curated list of fraud detection papers from the following conferences:    - Network Science     * [ASONAM](http://asonam.cpsc.ucalgary.ca/2019/)     * [COMPLEX NETWORKS](https://www.complexnetworks.org/)  - Data Science     * [DSAA](http://dsaa2019.dsaa.co/)  - Natural Language Processing     * [ACL](http://www.acl2019.org/EN/index.xhtml)  - Data Mining     * [KDD](https://www.kdd.org/)     * [ICDM](http://icdm2019.bigke.org/)     * [SIGIR](https://sigir.org/)     * [SDM](https://www.siam.org/conferences/cm/conference/sdm20)     * [WWW](https://www2019.thewebconf.org/)     * [CIKM](http://www.cikmconference.org/)  - Artificial Intelligence     * [AAAI](https://www.aaai.org/)     * [AISTATS](http://www.auai.org/)     * [IJCAI](https://www.ijcai.org/)     * [UAI](http://www.auai.org/)  - Databases     * [VLDB](http://www.vldb.org/)    Similar collections about [graph classification](https://github.com/benedekrozemberczki/awesome-graph-classification), [classification/regression tree](https://github.com/benedekrozemberczki/awesome-decision-tree-papers), [gradient boosting](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers), [Monte Carlo tree search](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers), and [community detection](https://github.com/benedekrozemberczki/awesome-community-detection) papers with implementations.      ## 2022  - **Active Learning for Human-in-the-loop Customs Inspection (TKDE 2022)**    - Sundong Kim, Tung-Duong Mai, Thi Nguyen Duc Khanh, Sungwon Han, Sungwon Park, Karandeep Singh, Meeyoung Cha    - [[Paper]](https://ieeexplore.ieee.org/document/9695316/)    - [[Code]](https://github.com/Seondong/Customs-Fraud-Detection)    - **Knowledge Sharing via Domain Adaptation in Customs Fraud Detection (AAAI 2022)**    - Sungwon Park, Sundong Kim, Meeyoung Cha    - [[Paper]](https://arxiv.org/abs/2201.06759)      ## 2021  - **Towards Consumer Loan Fraud Detection: Graph Neural Networks with Role-Constrained Conditional Random Field (AAAI 2021)**    - Bingbing Xu, Huawei Shen, Bing-Jie Sun, Rong An, Qi Cao, Xueqi Cheng    - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/16582)    - **Modeling the Field Value Variations and Field Interactions Simultaneously for Fraud Detection (AAAI 2021)**    - Dongbo Xi, Bowen Song, Fuzhen Zhuang, Yongchun Zhu, Shuai Chen, Tianyi Zhang, Yuan Qi, Qing He    - [[Paper]](https://arxiv.org/abs/2008.05600)    - **IFDDS: An Anti-fraud Outbound Robot (AAAI 2021)**    - Zihao Wang, Minghui Yang, Chunxiang Jin, Jia Liu, Zujie Wen, Saishuai Liu, Zhe Zhang    - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/18030)    - **Modeling Heterogeneous Graph Network on Fraud Detection: A Community-based Framework with Attention Mechanism (CIKM 2021)**    - Li Wang, Peipei Li, Kai Xiong, Jiashu Zhao, Rui Lin    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3459637.3482277)    - **Fraud Detection under Multi-Sourced Extremely Noisy Annotations (CIKM 2021)**    - Chuang Zhang, Qizhou Wang, Tengfei Liu, Xun Lu, Jin Hong, Bo Han, Chen Gong    - [[Paper]](https://gcatnjust.github.io/ChenGong/paper/zhang_cikm21.pdf)    - **Adversarial Reprogramming of Pretrained Neural Networks for Fraud Detection (CIKM 2021)**    - Lingwei Chen, Yujie Fan, Yanfang Ye    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3459637.3482053)    - **Fine-Grained Element Identification in Complaint Text of Internet Fraud (CIKM 2021)**    - Tong Liu, Siyuan Wang, Jingchao Fu, Lei Chen, Zhongyu Wei, Yaqi Liu, Heng Ye, Liaosa Xu, Weiqiang Wang, Xuanjing Huang    - [[Paper]](https://arxiv.org/abs/2108.08676)    - **Could You Describe the Reason for the Transfer: A Reinforcement Learning Based Voice-Enabled Bot Protecting Customers from Financial Frauds (CIKM 2021)**    - Zihao Wang, Fudong Wang, Haipeng Zhang, Minghui Yang, Shaosheng Cao, Zujie Wen, Zhe Zhang    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3459637.3481906)    - **Online Credit Payment Fraud Detection via Structure-Aware Hierarchical Recurrent Neural Network (IJCAI 2021)**    - Wangli Lin, Li Sun, Qiwei Zhong, Can Liu, Jinghua Feng, Xiang Ao, Hao Yang    - [[Paper]](https://www.ijcai.org/proceedings/2021/505)    - **Intention-aware Heterogeneous Graph Attention Networks for Fraud Transactions Detection (KDD 2021)**    - Can Liu, Li Sun, Xiang Ao, Jinghua Feng, Qing He, Hao Yang    - [[Paper]](https://dl.acm.org/doi/10.1145/3447548.3467142)    - **Live-Streaming Fraud Detection: A Heterogeneous Graph Neural Network Approach (KDD 2021)**    - Haishuai Wang, Zhao Li, Peng Zhang, Jiaming Huang, Pengrui Hui, Jian Liao, Ji Zhang, Jiajun Bu    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3447548.3467065)    - **Customs Fraud Detection in the Presence of Concept Drift (IncrLearn@ICDM 2021)**    - Tung-Duong Mai, Kien Hoang, Aitolkyn Baigutanova, Gaukhartas Alina, Sundong Kim    - [[Paper]](https://arxiv.org/abs/2109.14155)      - **Pick and Choose: A GNN-based Imbalanced Learning Approach for Fraud Detection (WWW 2021)**    - Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, Qing He    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3442381.3449989)    ## 2020      - **Spatio-Temporal Attention-Based Neural Network for Credit Card Fraud Detection (AAAI 2020)**    - Dawei Cheng, Sheng Xiang, Chencheng Shang, Yiyi Zhang, Fangzhou Yang, Liqing Zhang    - [[Paper]](https://aaai.org/Papers/AAAI/2020GB/AISI-ChengD.87.pdf)    - **FlowScope: Spotting Money Laundering Based on Graphs (AAAI 2020)**    - Xiangfeng Li, Shenghua Liu, Zifeng Li, Xiaotian Han, Chuan Shi, Bryan Hooi, He Huang, Xueqi Cheng    - [[Paper]](https://shenghua-liu.github.io/papers/aaai2020cr-flowscope.pdf)    - [[Code]](https://github.com/aplaceof/FlowScope)    - **Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters (CIKM 2020)**    - Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, Philip S. Yu    - [[Paper]](https://arxiv.org/abs/2008.08692)    - [[Code]](https://github.com/YingtongDou/CARE-GNN)    - **Loan Default Analysis with Multiplex Graph Learning (CIKM 2020)**    - Binbin Hu, Zhiqiang Zhang, Jun Zhou, Jingli Fang, Quanhui Jia, Yanming Fang, Quan Yu, Yuan Qi    - [[Paper]](https://www.researchgate.net/publication/343626706_Loan_Default_Analysis_with_Multiplex_Graph_Learning)    - **Error-Bounded Graph Anomaly Loss for GNNs (CIKM 2020)**    - Tong Zhao, Chuchen Deng, Kaifeng Yu, Tianwen Jiang, Daheng Wang, Meng Jiang    - [[Paper]](http://www.meng-jiang.com/pubs/gal-cikm20/gal-cikm20-paper.pdf)    - [[Code]](https://github.com/zhao-tong/Graph-Anomaly-Loss)      - **BotSpot: A Hybrid Learning Framework to Uncover Bot Install Fraud in Mobile Advertising (CIKM 2020)**    - Tianjun Yao, Qing Li, Shangsong Liang, Yadong Zhu    - [[Paper]](https://dl.acm.org/doi/pdf/10.1145/3340531.3412690)    - [[Code]](https://github.com/akakeigo2020/CIKM-Applied_Research-2150)    - **Early Fraud Detection with Augmented Graph Learning (DLG@KDD 2020)**    - Tong Zhao, Bo Ni, Wenhao Yu, Meng Jiang    - [[Paper]](http://www.meng-jiang.com/pubs/earlyfraud-dlg20/earlyfraud-dlg20-paper.pdf)    - **NAG: Neural Feature Aggregation Framework for Credit Card Fraud Detection (ICDM 2020)**    - Kanishka Ghosh Dastidar, Johannes Jurgovsky, Wissam Siblini, Liyun He-Guelton, Michael Granitzer    - [[Paper]](https://www.computer.org/csdl/proceedings-article/icdm/2020/831600a092/1r54A3Sb2yk)    - **Heterogeneous Mini-Graph Neural Network and Its Application to Fraud Invitation Detection (ICDM 2020)**    - Yong-Nan Zhu, Xiaotian Luo, Yu-Feng Li, Bin Bu, Kaibo Zhou, Wenbin Zhang, Mingfan Lu    - [[Paper]](https://cs.nju.edu.cn/liyf/paper/icdm20-hmgnn.pdf)      - **Collaboration Based Multi-Label Propagation for Fraud Detection (IJCAI 2020)**    - Haobo Wang, Zhao Li, Jiaming Huang, Pengrui Hui, Weiwei Liu, Tianlei Hu, Gang Chen    - [[Paper]](https://www.ijcai.org/Proceedings/2020/343)    - **The Behavioral Sign of Account Theft: Realizing Online Payment Fraud Alert (IJCAI 2020)**    - Cheng Wang    - [[Paper]](https://www.ijcai.org/Proceedings/2020/0636.pdf)    - **Federated Meta-Learning for Fraudulent Credit Card Detection (IJCAI 2020)**    - Wenbo Zheng, Lan Yan, Chao Gou, Fei-Yue Wang    - [[Paper]](https://www.ijcai.org/Proceedings/2020/642)    - **Robust Spammer Detection by Nash Reinforcement Learning (KDD 2020)**    - Yingtong Dou, Guixiang Ma, Philip S. Yu, Sihong Xie    - [[Paper]](https://arxiv.org/abs/2006.06069)    - [[Code]](https://github.com/YingtongDou/Nash-Detect)      - **DATE: Dual Attentive Tree-aware Embedding for Customs Fraud Detection (KDD 2020)**    - Sundong Kim, Yu-Che Tsai, Karandeep Singh, Yeonsoo Choi, Etim Ibok, Cheng-Te Li, Meeyoung Cha    - [[Paper]](https://seondong.github.io/assets/papers/2020_KDD_DATE.pdf)    - [[Code]](https://github.com/Roytsai27/Dual-Attentive-Tree-aware-Embedding)    - **Fraud Transactions Detection via Behavior Tree with Local Intention Calibration (KDD 2020)**    - Can Liu, Qiwei Zhong, Xiang Ao, Li Sun, Wangli Lin, Jinghua Feng, Qing He, Jiayu Tang    - [[Paper]](https://dl.acm.org/doi/pdf/10.1145/3394486.3403354)    - **Interleaved Sequence RNNs for Fraud Detection (KDD 2020)**    - Bernardo Branco, Pedro Abreu, Ana Sofia Gomes, Mariana S. C. Almeida, JoÃ£o Tiago AscensÃ£o, Pedro Bizarro    - [[Paper]](https://arxiv.org/abs/2002.05988)    - **GCN-Based User Representation Learning for Unifying Robust Recommendation and Fraudster Detection (SIGIR 2020)**    - Shijie Zhang, Hongzhi Yin, Tong Chen, Quoc Viet Nguyen Hung, Zi Huang, Lizhen Cui    - [[Paper]](https://arxiv.org/abs/2005.10150)     - **Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection (SIGIR 2020)**    - Zhiwei Liu, Yingtong Dou, Philip S. Yu, Yutong Deng, Hao Peng    - [[Paper]](https://arxiv.org/abs/2005.00625)    - [[Code]](https://github.com/safe-graph/DGFraud)      - **Friend or Faux: Graph-Based Early Detection of Fake Accounts on Social Networks (WWW 2020)**    - Adam Breuer, Roee Eilat, Udi Weinsberg    - [[Paper]](https://arxiv.org/abs/2004.04834)    - **Financial Defaulter Detection on Online Credit Payment via Multi-view Attributed Heterogeneous Information Network (WWW 2020)**    - Qiwei Zhong, Yang Liu, Xiang Ao, Binbin Hu, Jinghua Feng, Jiayu Tang, Qing He    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3366423.3380159)    - **ASA: Adversary Situation Awareness via Heterogeneous Graph Convolutional Networks (WWW 2020)**    - Rui Wen, Jianyu Wang, Chunming Wu, Jian Xiong    - [[Paper]](https://dl.acm.org/doi/10.1145/3366424.3391266)      - **Modeling Users' Behavior Sequences with Hierarchical Explainable Network for Cross-domain Fraud Detection (WWW 2020)**    - Yongchun Zhu, Dongbo Xi, Bowen Song, Fuzhen Zhuang, Shuai Chen, Xi Gu, Qing He    - [[Paper]](https://dl.acm.org/doi/fullHtml/10.1145/3366423.3380172)      ## 2019  - **SliceNDice: Mining Suspicious Multi-attribute Entity Groups with Multi-view Graphs (DSAA 2019)**    - Hamed Nilforoshan, Neil Shah    - [[Paper]](https://arxiv.org/abs/1908.07087)    - [[Code]](https://github.com/hamedn/SliceNDice)    - **FARE: Schema-Agnostic Anomaly Detection in Social Event Logs (DSAA 2019)**    - Neil Shah    - [[Paper]](http://nshah.net/publications/FARE.DSAA.19.pdf)      - **Cash-Out User Detection Based on Attributed Heterogeneous Information Network with a Hierarchical Attention Mechanism (AAAI 2019)**    - Binbin Hu, Zhiqiang Zhang, Chuan Shi, Jun Zhou, Xiaolong Li, Yuan Qi    - [[Paper]](https://aaai.org/ojs/index.php/AAAI/article/view/3884)    - [[Code]](https://github.com/safe-graph/DGFraud)      - **GeniePath: Graph Neural Networks with Adaptive Receptive Paths (AAAI 2019)**    - Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, Yuan Qi    - [[Paper]](https://arxiv.org/abs/1802.00910)    - [[Code]](https://github.com/safe-graph/DGFraud)      - **SAFE: A Neural Survival Analysis Model for Fraud Early Detection (AAAI 2019)**    - Panpan Zheng, Shuhan Yuan, Xintao Wu    - [[Paper]](https://arxiv.org/abs/1809.04683v2)    - [[Code]](https://github.com/PanpanZheng/SAFE)    - **One-Class Adversarial Nets for Fraud Detection (AAAI 2019)**    - Panpan Zheng, Shuhan Yuan, Xintao Wu, Jun Li, Aidong Lu    - [[Paper]](https://arxiv.org/abs/1803.01798)    - [[Code]](https://github.com/ILoveAI2019/OCAN)      - **Uncovering Download Fraud Activities in Mobile App Markets (ASONAM 2019)**    - Yingtong Dou, Weijian Li, Zhirong Liu, Zhenhua Dong, Jiebo Luo, Philip S. Yu    - [[Paper]](https://arxiv.org/pdf/1907.03048.pdf)    - **Spam Review Detection with Graph Convolutional Networks (CIKM 2019)**    - Ao Li, Zhou Qin, Runshi Liu, Yiqun Yang, Dong Li    - [[Paper]](https://arxiv.org/abs/1908.10679)    - [[Code]](https://github.com/safe-graph/DGFraud)    - **Key Player Identification in Underground Forums Over Attributed Heterogeneous Information Network Embedding Framework (CIKM 2019)**     - Yiming Zhang, Yujie Fan, Yanfang Ye, Liang Zhao, Chuan Shi     - [[Paper]](http://mason.gmu.edu/~lzhao9/materials/papers/lp0110-zhangA.pdf)     - [[Code]](https://github.com/safe-graph/DGFraud)       - **CatchCore: Catching Hierarchical Dense Subtensor (ECML-PKDD 2019)**    -  Wenjie Feng, Shenghua Liu, Huawei Shen, and Xueqi Cheng    - [[Paper]](https://shenghua-liu.github.io/papers/pkdd2019-catchcore.pdf)    - [[Code]](https://github.com/wenchieh/catchcore)      - **Spotting Collective Behaviour of Online Frauds in Customer Reviews (IJCAI 2019)**    - Sarthika Dhawan, Siva Charan Reddy Gangireddy, Shiv Kumar, Tanmoy Chakraborty    - [[Paper]](https://arxiv.org/abs/1905.13649)    - [[Code]](https://github.com/LCS2-IIITD/DeFrauder)      - **A Semi-Supervised Graph Attentive Network for Fraud Detection (ICDM 2019)**    - Daixin Wang, Jianbin Lin, Peng Cui, Quanhui Jia, Zhen Wang, Yanming Fang, Quan Yu, Jun Zhou, Shuang Yang, and Qi Yuan     - [[Paper]](https://arxiv.org/abs/2003.01171)    - [[Code]](https://github.com/safe-graph/DGFraud)     - **EigenPulse: Detecting Surges in Large Streaming Graphs with Row Augmentation (PAKDD 2019)**    - Jiabao Zhang, Shenghua Liu, Wenjian Yu, Wenjie Feng, Xueqi Cheng    - [[Paper]](https://shenghua-liu.github.io/papers/pakdd2019-eigenpulse.pdf)    - **Uncovering Insurance Fraud Conspiracy with Network Learning (SIGIR 2019)**    - Chen Liang, Ziqi Liu, Bin Liu, Jun Zhou, Xiaolong Li, Shuang Yang, Yuan Qi    - [[Paper]](https://dl.acm.org/citation.cfm?id=3331372)      - **A Contrast Metric for Fraud Detection in Rich Graphs (TKDE 2019)**    - Shenghua Liu, Bryan Hooi, Christos Faloutsos    - [[Paper]](https://shenghua-liu.github.io/papers/tkde2019-constrastsusp_holoscope.pdf)    - **Think Outside the Dataset: Finding Fraudulent Reviews using Cross-Dataset Analysis (WWW 2019)**    - Shirin Nilizadeh, Hojjat Aghakhani, Eric Gustafson, Christopher Kruegel, Giovanni Vigna    - [[Paper]](https://www.researchgate.net/publication/333060486_Think_Outside_the_Dataset_Finding_Fraudulent_Reviews_using_Cross-Dataset_Analysis)    - **Securing the Deep Fraud Detector in Large-Scale E-Commerce Platform via Adversarial Machine Learning Approach (WWW 2019)**    - Qingyu Guo, Zhao Li, Bo An, Pengrui Hui, Jiaming Huang, Long Zhang, Mengchen Zhao    - [[Paper]](https://www.ntu.edu.sg/home/boan/papers/WWW19.pdf)    - **No Place to Hide: Catching Fraudulent Entities in Tensors (WWW 2019)**    - Yikun Ban, Xin Liu, Ling Huang, Yitao Duan, Xue Liu, Wei Xu    - [[Paper]](https://arxiv.org/pdf/1810.06230.pdf)      - **FdGars: Fraudster Detection via Graph Convolutional Networks in Online App Review System (WWW 2019)**    - Rui Wen, Jianyu Wang and Yu Huang    - [[Paper]](https://dl.acm.org/citation.cfm?id=3316586)    - [[Code]](https://github.com/safe-graph/DGFraud)    ## 2018    - **Heterogeneous Graph Neural Networks for Malicious Account Detection (CIKM 2018)**    - Ziqi Liu, Chaochao Chen, Xinxing Yang, Jun Zhou, Xiaolong Li, and Le Song    - [[Paper]](https://dl.acm.org/doi/10.1145/3269206.3272010)    - [[Code]](https://github.com/safe-graph/DGFraud)      - **Reinforcement Mechanism Design for Fraudulent Behaviour in e-Commerce (AAAI 2018)**    - Qingpeng Cai, Aris Filos-Ratsikas, Pingzhong Tang, Yiwei Zhang    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16650)      - **Adapting to Concept Drift in Credit Card Transaction Data Streams Using Contextual Bandits and Decision Trees (AAAI 2018)**    - Dennis J. N. J. Soemers, Tim Brys, Kurt Driessens, Mark H. M. Winands, Ann NowÃ©    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16183/16394)    - **Nextgen AML: Distributed Deep Learning Based Language Technologies to Augment Anti Money Laundering Investigation(ACL 2018)**    - Jingguang Han, Utsab Barman, Jeremiah Hayes, Jinhua Du, Edward Burgin, Dadong Wan    - [[Paper]](https://www.aclweb.org/anthology/P18-4007)    - **Preserving Privacy of Fraud Detection Rule Sharing Using Intel's SGX (CIKM 2018)**    - Daniel Deutch, Yehonatan Ginzberg, Tova Milo    - [[Paper]](https://www.researchgate.net/publication/328439345_Preserving_Privacy_of_Fraud_Detection_Rule_Sharing_Using_Intel%27s_SGX)    - **Deep Structure Learning for Fraud Detection (ICDM 2018)**    - Haibo Wang, Chuan Zhou, Jia Wu, Weizhen Dang, Xingquan Zhu, Jilong Wang    - [[Paper]](https://www.researchgate.net/publication/330030140_Deep_Structure_Learning_for_Fraud_Detection)    - **Learning Sequential Behavior Representations for Fraud Detection (ICDM 2018)**    - Jia Guo, Guannan Liu, Yuan Zuo, Junjie Wu    - [[Paper]](https://www.researchgate.net/publication/330028902_Learning_Sequential_Behavior_Representations_for_Fraud_Detection)    - **Impression Allocation for Combating Fraud in E-commerce Via Deep Reinforcement Learning with Action Norm Penalty (IJCAI 2018)**    - Mengchen Zhao, Zhao Li, Bo An, Haifeng Lu, Yifan Yang, Chen Chu    - [[Paper]](https://www.ijcai.org/proceedings/2018/0548.pdf)    - **Tax Fraud Detection for Under-Reporting Declarations Using an Unsupervised Machine Learning Approach (KDD 2018)**    - Daniel de Roux, Boris Perez, AndrÃ©s Moreno, MarÃ­a-Del-Pilar Villamil, CÃ©sar Figueroa    - [[Paper]](https://www.kdd.org/kdd2018/accepted-papers/view/tax-fraud-detection-for-under-reporting-declarations-using-an-unsupervised-)      - **Collective Fraud Detection Capturing Inter-Transaction Dependency (KDD 2018)**    - Bokai Cao, Mia Mao, Siim Viidu, Philip Yu    - [[Paper]](http://proceedings.mlr.press/v71/cao18a.html)      - **Fraud Detection with Density Estimation Trees (KDD 2018)**    - Fraud Detection with Density Estimation Trees    - [[Paper]](http://proceedings.mlr.press/v71/ram18a/ram18a.pdf)    - **Real-time Constrained Cycle Detection in Large Dynamic Graphs (VLDB 2018)**    - Xiafei Qiu, Wubin Cen, Zhengping Qian, You Peng, Ying Zhang, Xuemin Lin, Jingren Zhou    - [[Paper]](http://www.vldb.org/pvldb/vol11/p1876-qiu.pdf)      - **REV2: Fraudulent User Prediction in Rating Platforms (WSDM 2018)**    - Srijan Kumar, Bryan Hooi, Disha Makhija, Mohit Kumar, Christos Faloutsos, V. S. Subrahmanian    - [[Paper]](https://cs.stanford.edu/~srijan/pubs/rev2-wsdm18.pdf)    - [[Code]](https://cs.stanford.edu/~srijan/rev2/)      - **Exposing Search and Advertisement Abuse Tactics and Infrastructure of Technical Support Scammers (WWW 2018)**    - Bharat Srinivasan, Athanasios Kountouras, Najmeh Miramirkhani, Monjur Alam, Nick Nikiforakis, Manos Antonakakis, Mustaque Ahamad    - [[Paper]](https://www.securitee.org/files/tss_www2018.pdf)    ## 2017  - **ZooBP: Belief Propagation for Heterogeneous Networks (VLDB 2017)**    - Dhivya Eswaran, Stephan Gunnemann, Christos Faloutsos, Disha Makhija, Mohit Kumar    - [[Paper]](http://www.vldb.org/pvldb/vol10/p625-eswaran.pdf)    - [[Code]](https://github.com/safe-graph/UGFraud)    - **Behavioral Analysis of Review Fraud: Linking Malicious Crowdsourcing to Amazon and Beyond (AAAI 2017)**    - Parisa Kaghazgaran, James Caverlee, Majid Alfifi    - [[Paper]](https://aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/view/15659)      - **Detection of Money Laundering Groups: Supervised Learning on Small Networks (AAAI 2017)**    - David Savage, Qingmai Wang, Xiuzhen Zhang, Pauline Chou, Xinghuo Yu    - [[Paper]](https://arxiv.org/pdf/1608.00708.pdf)      - **Spectrum-based Deep Neural Networks for Fraud Detection (CIKM 2017)**    - Shuhan Yuan, Xintao Wu, Jun Li, Aidong Lu    - [[Paper]](https://arxiv.org/abs/1706.00891)    - **HoloScope: Topology-and-Spike Aware Fraud Detection (CIKM 2017)**    - Shenghua Liu, Bryan Hooi, Christos Faloutsos    - [[Paper]](https://arxiv.org/abs/1705.02505)    - **The Many Faces of Link Fraud (ICDM 2017)**    - Neil Shah, Hemank Lamba, Alex Beutel, Christos Faloutsos    - [[Paper]](https://arxiv.org/abs/1704.01420)    - **HitFraud: A Broad Learning Approach for Collective Fraud Detection in Heterogeneous Information Networks (ICDM 2017)**    - Bokai Cao, Mia Mao, Siim Viidu, Philip S. Yu    - [[Paper]](https://arxiv.org/abs/1709.04129)      - **GANG: Detecting Fraudulent Users in Online Social Networks via Guilt-by-Association on Directed Graphs (ICDM 2017)**    - Binghui Wang, Neil Zhenqiang Gong, Hao Fu    - [[Paper]](https://ieeexplore.ieee.org/document/8215519)    - [[Code]](https://github.com/safe-graph/UGFraud)      - **Improving Card Fraud Detection Through Suspicious Pattern Discovery (IEA/AIE 2017)**    - Fabian Braun, Olivier Caelen, Evgueni N. Smirnov, Steven Kelk, Bertrand Lebichot:    - [[Paper]](http://www.oliviercaelen.be/doc/GBSSCCFDS.pdf)    - **Online Reputation Fraud Campaign Detection in User Ratings (IJCAI 2017)**    - Chang Xu, Jie Zhang, Zhu Sun    - [[Paper]](https://www.ijcai.org/proceedings/2017/0541.pdf)      - **Uncovering Unknown Unknowns in Financial Services Big Data by Unsupervised Methodologies: Present and Future trends (KDD 2017)**    - Gil Shabat, David Segev, Amir Averbuch    - [[Paper]](http://proceedings.mlr.press/v71/shabat18a.html)      - **PD-FDS: Purchase Density based Online Credit Card Fraud Detection System (KDD 2017)**    - Youngjoon Ki, Ji Won Yoon     - [[Paper]](http://proceedings.mlr.press/v71/ki18a/ki18a.pdf)      - **HiDDen: Hierarchical Dense Subgraph Detection with Application to Financial Fraud Detection (SDM 2017)**    - Si Zhang, Dawei Zhou, Mehmet Yigit Yildirim, Scott Alcorn, Jingrui He, Hasan Davulcu, Hanghang Tong    - [[Paper]](http://www.public.asu.edu/~hdavulcu/SDM17.pdf)    ## 2016  - **A Fraud Resilient Medical Insurance Claim System (AAAI 2016)**    - Yuliang Shi, Chenfei Sun, Qingzhong Li, Lizhen Cui, Han Yu, Chunyan Miao    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11813)    - **A Graph-Based, Semi-Supervised, Credit Card Fraud Detection System (COMPLEX NETWORKS 2016)**    - Bertrand Lebichot, Fabian Braun, Olivier Caelen, Marco Saerens    - [[Paper]](http://www.oliviercaelen.be/doc/IEAAIE_2017_Finalversion-PDF_39.pdf)    - **FRAUDAR: Bounding Graph Fraud in the Face of Camouflage (KDD 2016)**    - Bryan Hooi, Hyun Ah Song, Alex Beutel, Neil Shah, Kijung Shin, Christos Faloutsos    - [[Paper]](https://www.andrew.cmu.edu/user/bhooi/papers/fraudar_kdd16.pdf)    - [[Code]](https://github.com/safe-graph/UGFraud)      - **Identifying Anomalies in Graph Streams Using Change Detection (KDD 2016)**    - William Eberle and Lawrence Holde    - [[Paper]](http://www.mlgworkshop.org/2016/paper/MLG2016_paper_12.pdf)    - **FairPlay: Fraud and Malware Detection in Google Play (SDM 2016)**    - Mahmudur Rahman, Mizanur Rahman, Bogdan Carbunar, Duen Horng Chau    - [[Paper]](https://arxiv.org/abs/1703.02002)    - **BIRDNEST: Bayesian Inference for Ratings-Fraud Detection (SDM 2016)**    - Bryan Hooi, Neil Shah, Alex Beutel, Stephan GÃ¼nnemann, Leman Akoglu, Mohit Kumar, Disha Makhija, Christos Faloutsos    - [[Paper]](https://www.andrew.cmu.edu/user/bhooi/papers/birdnest_sdm16.pdf)    - **Understanding the Detection of View Fraud in Video Content Portals (WWW 2016)**    - Miriam Marciel, RubÃ©n Cuevas, Albert Banchs, Roberto Gonzalez, Stefano Traverso, Mohamed Ahmed, Arturo Azcorra    - [[Paper]](https://dl.acm.org/citation.cfm?id=2882980)    ## 2015  - **Toward An Intelligent Agent for Fraud Detection â€” The CFE Agent (AAAI 2015)**    - Joe Johnson    - [[Paper]](https://www.aaai.org/ocs/index.php/FSS/FSS15/paper/download/11664/11485)      - **Graph Analysis for Detecting Fraud, Waste, and Abuse in Healthcare Data (AAAI 2015)**    - Juan Liu, Eric Bier, Aaron Wilson, Tomonori Honda, Kumar Sricharan, Leilani Gilpin, John Alexis Guerra GÃ³mez, Daniel Davies    - [[Paper]](https://pdfs.semanticscholar.org/1ea7/125b789ef938bffe10c7588e6b071c4ff73c.pdf)    - **Robust System for Identifying Procurement Fraud (AAAI 2015)**    - Amit Dhurandhar, Rajesh Kumar Ravi, Bruce Graves, Gopikrishnan Maniachari, Markus Ettl    - [[Paper]](https://pdfs.semanticscholar.org/27af/c9ec453ae0cf9e55f4032ff688cb70c2a61e.pdf)    - **Fraud Transaction Recognition: A Money Flow Network Approach (CIKM 2015)**    - Renxin Mao, Zhao Li, Jinhua Fu    - [[Paper]](https://dl.acm.org/citation.cfm?id=2806647)    - **Towards Collusive Fraud Detection in Online Reviews (ICDM 2015)**    - Chang Xu, Jie Zhang    - [[Paper]](https://ieeexplore.ieee.org/document/7373434)    - **Catch the Black Sheep: Unified Framework for Shilling Attack Detection Based on Fraudulent Action Propagation (IJCAI 2015)**    - Yongfeng Zhang, Yunzhi Tan, Min Zhang, Yiqun Liu, Tat-Seng Chua, Shaoping Ma    - [[Paper]](https://www.ijcai.org/Proceedings/15/Papers/341.pdf)    - **Collective Opinion Spam Detection: Bridging Review Networks and Metadata (KDD 2015)**    - Shebuti Rayana, Leman Akoglu    - [[Paper]](https://www.andrew.cmu.edu/user/lakoglu/pubs/15-kdd-collectiveopinionspam.pdf)    - [[Code]](https://github.com/safe-graph/UGFraud)    - **Graph-Based User Behavior Modeling: From Prediction to Fraud Detection (KDD 2015)**    - Alex Beutel, Leman Akoglu, Christos Faloutsos    - [[Paper]](https://www.cs.cmu.edu/~abeutel/kdd2015_tutorial/tutorial.pdf)    - **FrauDetector: A Graph-Mining-based Framework for Fraudulent Phone Call Detection (KDD 2015)**    - Vincent S. Tseng, Jia-Ching Ying, Che-Wei Huang, Yimin Kao, Kuan-Ta Chen    - [[Paper]](http://repository.ncku.edu.tw/bitstream/987654321/166322/1/4010204000-000004_1.pdf)      - **A Framework for Intrusion Detection Based on Frequent Subgraph Mining (SDM 2015)**    - Vitali Herrera-Semenets, Niusvel Acosta-Mendoza, Andres Gago-Alonso    - [[Paper]](https://www.researchgate.net/publication/271839253_A_Framework_for_Intrusion_Detection_based_on_Frequent_Subgraph_Mining)    - **Crowd Fraud Detection in Internet Advertising (WWW 2015)**    - Tian Tian, Jun Zhu, Fen Xia, Xin Zhuang, Tong Zhang    - [[Paper]](http://www.www2015.it/documents/proceedings/proceedings/p1100.pdf)    ## 2014  - **Spotting Suspicious Link Behavior with fBox: An Adversarial Perspective (ICDM 2014)**    - Neil Shah, Alex Beutel, Brian Gallagher, Christos Faloutsos    - [[Paper]](https://arxiv.org/pdf/1410.3915.pdf)    - [[Code]](https://github.com/safe-graph/UGFraud)    - **Fraudulent Support Telephone Number Identification Based on Co-Occurrence Information on the Web (AAAI 2014)**    - Xin Li, Yiqun Liu, Min Zhang, Shaoping Ma    - [[Paper]](https://pdfs.semanticscholar.org/2733/1f48c87736ea12b9edec062e384d3bd58f88.pdf)    - **Corporate Residence Fraud Detection (KDD 2014)**    - Enric JunquÃ© de Fortuny, Marija Stankova, Julie Moeyersoms, Bart Minnaert, Foster J. Provost, David Martens    - [[Paper]](http://delivery.acm.org/10.1145/2630000/2623333/p1650-fortuny.pdf?ip=129.215.164.203&id=2623333&acc=ACTIVE%20SERVICE&key=C2D842D97AC95F7A%2EEB9E991028F4E1F1%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1559048806_f1a6f763ef7088a4fb4b1a4ff94856f8)    - **Graphical Models for Identifying Fraud and Waste in Healthcare Claims (SDM 2014)**    - Peder A. Olsen, Ramesh Natarajan, Sholom M. Weiss    - [[Paper]](https://epubs.siam.org/doi/abs/10.1137/1.9781611973440.66)    - **Improving Credit Card Fraud Detection with Calibrated Probabilities (SDM 2014)**    - Alejandro Correa Bahnsen, Aleksandar Stojanovic, Djamila Aouada, BjÃ¶rn E. Ottersten    - [[Paper]](https://pdfs.semanticscholar.org/9241/ef2a2f6638eafeffd0056736c0f46f9aa083.pdf)    - **Large Graph Mining: Patterns, Cascades, Fraud Detection, and Algorithms (WWW 2014)**    - Christos Faloutsos    - [[Paper]](http://wwwconference.org/proceedings/www2014/proceedings/p1.pdf)      ## 2013    - **Opinion Fraud Detection in Online Reviews by Network Effects (AAAI 2013)**    - Leman Akoglu, Rishi Chandy, Christos Faloutsos    - [[Paper]](https://www.researchgate.net/publication/279905898_Opinion_fraud_detection_in_online_reviews_by_network_effects)      - **Using Social Network Knowledge for Detecting Spider Constructions in Social Security Fraud (ASONAM 2013)**    - VÃ©ronique Van Vlasselaer, Jan Meskens, Dries Van Dromme, Bart Baesens     - [[Paper]](https://ieeexplore.ieee.org/document/6785796)      - **Ranking Fraud Detection for Mobile Apps: a Holistic View (CIKM 2013)**    - Hengshu Zhu, Hui Xiong, Yong Ge, Enhong Chen    - [[Paper]](http://dm.ustc.edu.cn/zhu-cikm13.pdf)    - **Using Co-Visitation Networks for Detecting Large Scale Online Display Advertising Exchange Fraud (KDD 2013)**    - Ori Stitelman, Claudia Perlich, Brian Dalessandro, Rod Hook, Troy Raeder, Foster J. Provost    - [[Paper]](http://chbrown.github.io/kdd-2013-usb/kdd/p1240.pdf)    - **Adaptive Adversaries: Building Systems to Fight Fraud and Cyber Intruders (KDD 2013)**    - Ari Gesher    - [[Paper]](https://dl.acm.org/citation.cfm?id=2491134)    - **Anomaly, Event, and Fraud Detection in Large Network Datasets (WSDM 2013)**    - Leman Akoglu, Christos Faloutsos    - [[Paper]](https://www.andrew.cmu.edu/user/lakoglu/wsdm13/13-wsdm-tutorial.pdf)    ## 2012    - **Fraud Detection: Methods of Analysis for Hypergraph Data (ASONAM 2012)**    - Anna Leontjeva, Konstantin Tretyakov, Jaak Vilo, and Taavi Tamkivi    - [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6425618)    - **Online Modeling of Proactive Moderation System for Auction Fraud Detection (WWW 2012)**    - Liang Zhang, Jie Yang, Belle L. Tseng    - [[Paper]](http://www.chennaisunday.com/Java%202012%20Base%20Paper/Online%20Modeling%20of%20Proactive%20Moderation%20System%20for%20Auction%20Fraud%20Detection.pdf)    ## 2011  - **A Machine-Learned Proactive Moderation System for Auction Fraud Detection (CIKM 2011)**    - Liang Zhang, Jie Yang, Wei Chu, Belle L. Tseng    - [[Paper]](http://www.gatsby.ucl.ac.uk/~chuwei/paper/p2501-zhang.pdf)    - **A Taxi Driving Fraud Detection System (ICDM 2011)**    - Yong Ge, Hui Xiong, Chuanren Liu, Zhi-Hua Zhou    - [[Paper]](https://ieeexplore.ieee.org/document/6137222)    - **Utility-Based Fraud Detection (IJCAI 2011)**    - LuÃ­s Torgo, Elsa Lopes    - [[Paper]](https://www.ijcai.org/Proceedings/11/Papers/255.pdf)    - **A Pattern Discovery Approach to Retail Fraud Detection (KDD 2011)**    - Prasad Gabbur, Sharath Pankanti, Quanfu Fan, Hoang Trinh    - [[Paper]](http://www2.engr.arizona.edu/~pgsangam/gabbur_kdd_11.pdf)    ## 2010    - **Hunting for the Black Swan: Risk Mining from Text (ACL 2010)**    - JL Leidner, F Schilder    - [[Paper]](https://www.aclweb.org/anthology/P10-4010)      - **Fraud Detection by Generating Positive Samples for Classification from Unlabeled Data (ACL 2010)**    - Levente Kocsis, Andras George    - [[Paper]](http://www.szit.bme.hu/~gya/publications/KocsisGyorgy.pdf)      ## 2009  - **SVM-based Credit Card Fraud Detection with Reject Cost and Class-Dependent Error Cost (PAKDD 2009)**    - En-hui Zheng,Chao Zou,Jian Sun, Le Chen    - [[Paper]](https://www.semanticscholar.org/paper/SVM-Based-Cost-sensitive-Classification-Algorithm-Zheng-Zou/bcae06626ccd453925ef040a1edb5cbb10b862ef)      - **An Approach for Automatic Fraud Detection in the Insurance Domain (AAAI 2009)**    - Alexander Widder, Rainer v. Ammon, Gerit Hagemann, Dirk SchÃ¶nfeld    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.325.3231&rep=rep1&type=pdf)      ## 2007  - **Relational Data Pre-Processing Techniques for Improved Securities Fraud Detection (KDD 2007)**    - Andrew S. Fast, Lisa Friedland, Marc E. Maier, Brian J. Taylor, David D. Jensen, Henry G. Goldberg, John Komoroske    - [[Paper]](https://dl.acm.org/citation.cfm?id=1281192.1281293)    - **Uncovering Fraud in Direct Marketing Data with a Fraud Auditing Case Builder (PKDD 2007)**    - Fletcher Lu    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-74976-9_56)    - **Netprobe: A Fast and Scalable System for Fraud Detection in Online Auction Networks (WWW 2007)**    - Shashank Pandit, Duen Horng Chau, Samuel Wang, Christos Faloutsos    - [[Paper]](http://www.cs.cmu.edu/~christos/PUBLICATIONS/netprobe-www07.pdf)    ## 2006    - **Data Mining Approaches to Criminal Career Analysis (ICDM 2006)**    - Jeroen S. De Bruin, Tim K. Cocx, Walter A. Kosters, Jeroen F. J. Laros, Joost N. Kok     - [[Paper]](https://ieeexplore.ieee.org/document/4053045)      - **Large Scale Detection of Irregularities in Accounting Data (ICDM 2006)**    - Stephen Bay, Krishna Kumaraswamy, Markus G. Anderle, Rohit Kumar, David M. Steier     - [[Paper]](https://ieeexplore.ieee.org/document/4053036)      - **Camouflaged Fraud Detection in Domains with Complex Relationships (KDD 2006)**    - Sankar Virdhagriswaran, Gordon Dakin    - [[Paper]](https://dl.acm.org/citation.cfm?id=1150532)    - **Detecting Fraudulent Personalities in Networks of Online Auctioneers (PKDD 2006)**    - Duen Horng Chau, Shashank Pandit, Christos Faloutsos    - [[Paper]](http://www.cs.cmu.edu/~dchau/papers/auction_fraud_pkdd06.pdf)    ## 2005    - **Technologies to Defeat Fraudulent Schemes Related to Email Requests (AAAI 2005)**    - Edoardo Airoldi, Bradley Malin, and Latanya Sweeney    - [[Paper]](http://www.aaai.org/Library/Symposia/Spring/2005/ss05-01-023.php)      - **AI Technologies to Defeat Identity Theft Vulnerabilities (AAAI 2005)**    - Latanya Sweeney    - [[Paper]](https://dataprivacylab.org/dataprivacy/projects/idangel/paper1.pdf)      - **Detecting Fraud in Health Insurance Data: Learning to Model Incomplete Benford's Law Distributions (ECML 2005)**    - Fletcher Lu, J. Efrim Boritz    - [[Paper]](https://faculty.uoit.ca/fletcherlu/LuECML05.pdf)    - **Using Relational Knowledge Discovery to Prevent Securities Fraud (KDD 2005)**    - Jennifer Neville, Ã–zgÃ¼r Simsek, David D. Jensen, John Komoroske, Kelly Palmer, Henry G. Goldberg    - [[Paper]](https://www.cs.purdue.edu/homes/neville/papers/neville-et-al-kdd2005.pdf)    ## 2003  - **Applying Data Mining in Investigating Money Laundering Crimes (KDD 2003)**    - Zhongfei (Mark) Zhang, John J. Salerno, Philip S. Yu    - [[Paper]](https://pdfs.semanticscholar.org/9124/b61d48b7e52008c7fd5fac1b7eac38474581.pdf)    ## 2000  - **Document Classification and Visualisation to Support the Investigation of Suspected Fraud (PKDD 2000)**    - Johan Hagman, Domenico Perrotta, Ralf Steinberger, and Aristi de Varfis    - [[Paper]](https://pdfs.semanticscholar.org/9124/b61d48b7e52008c7fd5fac1b7eac38474581.pdf)      ## 1999  - **Statistical Challenges to Inductive Inference in Linked Data. (AISTATS 1999)**    - David Jensen    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.589.1445&rep=rep1&type=pdf)    ## 1998    - **Toward Scalable Learning with Non-Uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection (KDD 1998)**    - Phillip K Chan, Salvatore J Stolfo     - [[Paper]](https://pdfs.semanticscholar.org/6e19/3366945bf3bd72d5ba906e3982ac4d8ae874.pdf)      - **Call-Based Fraud Detection in Mobile Communication Networks Using a Hierarchical Regime-Switching Model (NIPS 1998)**    - Jaakko HollmÃ©n, Volker Tresp    - [[Paper]](https://papers.nips.cc/paper/1505-call-based-fraud-detection-in-mobile-communication-networks-using-a-hierarchical-regime-switching-model.pdf)    ## 1997    - **Detection of Mobile Phone Fraud Using Supervised Neural Networks: A First Prototype (ICANN 1997)**    - Yves Moreau, Herman Verrelst, Joos Vandewalle    - [[Paper]](https://link.springer.com/content/pdf/10.1007%2FBFb0020294.pdf)      - **Prospective Assessment of AI Technologies for Fraud Detection: A Case Study (AAAI 1997)**    - David Jensen    - [[Paper]](https://pdfs.semanticscholar.org/0efe/8a145cc4d52e8769bb1d13142326a154624f.pdf)      - **Credit Card Fraud Detection Using Meta-Learning: Issues and Initial Results (AAAI 1997)**    - Salvatore J. Stolfo, David W. Fan, Wenke Lee and Andreas L. Prodromidis    - [[Paper]](https://pdfs.semanticscholar.org/29b3/e330e0045e5da71cc1d333bed24b7a4670f8.pdf)    ## 1995  - **Fraud: Uncollectible Debt Detection Using a Bayesian Network Based Learning System: A Rare Binary Outcome with Mixed Data Structures (UAI 1995)**    - Kazuo J. Ezawa, Til Schuermann    - [[Paper]](https://arxiv.org/abs/1302.4945)      --------------------------------------------------------------------------------    **License**    - [CC0 Universal](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers/blob/master/LICENSE)   """
Big data;https://github.com/tidwall/summitdb;"""<p align=""center"">  <img       src=""resources/logo.png""       width=""350"" height=""85"" border=""0"" alt=""SummitDB"">  </p>    SummitDB is an in-memory, [NoSQL](https://en.wikipedia.org/wiki/NoSQL) key/value database. It persists to disk, uses the [Raft](https://raft.github.io/) consensus algorithm, is [ACID](https://en.wikipedia.org/wiki/ACID) compliant, and built on a transactional and strongly-consistent model. It supports [custom indexes](https://github.com/tidwall/summitdb/wiki/SETINDEX), [geospatial data](https://github.com/tidwall/summitdb/wiki/SETINDEX#spatial), [JSON documents](#json-documents), and [user-defined JS scripting](https://github.com/tidwall/summitdb/wiki/EVAL).    Under the hood it utilizes [Finn](https://github.com/tidwall/finn), [Redcon](https://github.com/tidwall/redcon), [BuntDB](https://github.com/tidwall/buntdb), [GJSON](https://github.com/tidwall/gjson), and [Otto](https://github.com/robertkrimen/otto).    Features  --------  - [In-memory with disk persistence](#in-memory-disk-persistence)  - [Strong-consistency and durability](#consistency-and-durability)  - [High-availability](#consistency-and-durability)  - [Ordered key space](#differences-between-summitdb-and-redis)  - [Hot backups](#hot-backups)  - [Simplified Redis-style APIs](#commands)  - [Indexing on values](https://github.com/tidwall/summitdb/wiki/SETINDEX)  - [JSON documents](#json-documents)  - [Spatial indexing](https://github.com/tidwall/summitdb/wiki/SETINDEX#spatial)  - [Fencing tokens](#fencing-tokens)    Getting started  ---------------    ### Getting SummitDB    The easiest way to get SummitDB is to use one of the pre-built release binaries which are available for OSX, Linux, and Windows.   Instructions for using these binaries are on the GitHub [releases page](https://github.com/tidwall/summitdb/releases).    If you want to try the latest version, you can build SummitDB from the master branch.    ### Building SummitDB    SummitDB can be compiled and used on Linux, OSX, Windows, FreeBSD, ARM (Raspberry PI) and probably others since the codebase is 100% Go. We support both 32 bit and 64 bit systems. Go must be installed on the build machine.    To build simply:    ```  $ make  ```    It's a good idea to install the [redis-cli](http://redis.io/topics/rediscli).    ```  $ make redis-cli  ```    To run tests:    ```  $ make test  ```    ## Docker    Check out the SummitDB images in [Docker Hub](https://hub.docker.com/search?q=summitdb&type=image).    ### Running    First start a single-member cluster:  ```  $ ./summitdb-server  ```    This will start the server listening on port 7481 for client and server-to-server communication.    Next, let's set a single key, and then retrieve it:    ```  $ ./redis-cli -p 7481 SET mykey ""my value""  OK  $ ./redis-cli -p 7481 GET mykey  ""my value""  ```    Adding members:  ```  $ ./summitdb-server -p 7482 -dir data2 -join localhost:7481  $ ./summitdb-server -p 7483 -dir data3 -join localhost:7481  ```    That's it. Now if node1 goes down, node2 and node3 will continue to operate.    ## Differences between SummitDB and Redis    It may be worth noting that while SummitDB supports many Redis features, it is not a strict Redis clone. Redis has a lot of commands and data types that are not available in SummitDB such as Sets, Hashes, Sorted Sets, and PubSub. SummitDB also has many features that are not available in Redis such as:    - **Ordered key space** - SummitDB provides one key space that is a large B-tree. An ordered key space allows for stable paging through keys using the [KEYS](https://github.com/tidwall/summitdb/wiki/KEYS) command. Redis uses an unordered dictionary structure and provides a specialized [SCAN](http://redis.io/commands/scan) command for iterating through keys.  - **Everything a string** - SummitDB stores only strings which are exact binary representations of what the user stores. Redis has many [internal data types](http://redis.io/topics/data-types-intro), such as strings, hashes, floats, sets, etc.   - **Raft clusters** - SummitDB uses the Raft consensus algorithm to provide high-availablity. Redis provides [Master/Slave replication](http://redis.io/topics/replication).   - **Javascript** - SummitDB uses Javascript for user-defined scripts. Redis uses Lua.  - **Indexes** - SummitDB provides an API for indexing the key space. Indexes allow for quickly querying and iterating on values. Redis has specialized data types like Sorted Sets and Hashes which can provide [secondary indexing](http://redis.io/topics/indexes).  - **Spatial indexes** - SummitDB provides the ability to create spatial indexes. A spatial index uses an R-tree under the hood, and each index can be up to 20 dimensions. This is useful for geospatial, statistical, time, and range data. Redis has the [GEO API](http://redis.io/commands/geoadd) which allows for using storing and querying geospatial data using the [Geohashes](https://en.wikipedia.org/wiki/Geohash).  - **JSON documents** - SummitDB allows for storing JSON documents and indexing fields directly. Redis has Hashes and a JSON parser via Lua.    <a name=""in-memory-disk-persistence""></a>  ## In-memory with disk persistence  SummitDB store all data in memory. Yet each writable command is appended to a file that is used to rebuild the database if the database needs to be restarted.     This is similar to [Redis AOF persistence](http://redis.io/topics/persistence).    ## JSON Documents    SummitDB provides the commands  [JSET](https://github.com/tidwall/summitdb/wiki/JSET),  [JGET](https://github.com/tidwall/summitdb/wiki/JGET),  [JDEL](https://github.com/tidwall/summitdb/wiki/JDEL)  for working with json documents.    `JSET` and `JDEL` uses the   [sjson path syntax](https://github.com/tidwall/sjson#path-syntax)   and `JGET` uses the   [gjson path syntax](https://github.com/tidwall/gjson#path-syntax).    Here are some examples:    ```  > JSET user:101 name Tom  OK  > JSET user:101 age 46  OK  > GET user:101  ""{\""age\"":46,\""name\"":\""Tom\""}""  > JGET user:101 age  ""46""  > JSET user:101 name.first Tom  OK  > JSET user:101 name.last Anderson  OK  > GET user:101  ""{\""age\"":46,\""name\"":{\""last\"":\""Anderson\"",\""first\"":\""Tom\""}}""  > JDEL user:101 name.last  (integer) 1  > GET user:101  ""{\""age\"":46,\""name\"":{\""first\"":\""Tom\""}}""  > JSET user:101 friends.0 Carol  OK  > JSET user:101 friends.1 Andy  OK  > JSET user:101 friends.3 Frank  OK  > GET user:101  ""{\""friends\"":[\""Carol\"",\""Andy\"",null,\""Frank\""],\""age\"":46,\""name\"":{\""first\"":\""Tom\""}}""  > JGET user:101 friends.1  ""Andy""  ```    ## JSON Indexes    Indexes can be created on individual fields inside JSON documents.    For example, let's say you have the following documents:    ```json  {""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}  {""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}  {""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}  {""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}  ```    Create an index:    ```  > SETINDEX last_name user:* JSON name.last  ```    Then add some JSON:  ```  > SET user:1 '{""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}'  > SET user:2 '{""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}'  > SET user:3 '{""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}'  > SET user:4 '{""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}'  ```    Query with the ITER command:    ```  > ITER last_name  1) ""user:3""  2) ""{\""name\"":{\""first\"":\""Carol\"",\""last\"":\""Anderson\""},\""age\"":52}""  3) ""user:4""  4) ""{\""name\"":{\""first\"":\""Alan\"",\""last\"":\""Cooper\""},\""age\"":28}""  5) ""user:1""  6) ""{\""name\"":{\""first\"":\""Tom\"",\""last\"":\""Johnson\""},\""age\"":38}""  7) ""user:2""  8) ""{\""name\"":{\""first\"":\""Janet\"",\""last\"":\""Prichard\""},\""age\"":47}""  ```    Or perhaps you want to index on age:    ```  > SETINDEX age user:* JSON age  > ITER age  1) ""user:4""  2) ""{\""name\"":{\""first\"":\""Alan\"",\""last\"":\""Cooper\""},\""age\"":28}""  3) ""user:1""  4) ""{\""name\"":{\""first\"":\""Tom\"",\""last\"":\""Johnson\""},\""age\"":38}""  5) ""user:2""  6) ""{\""name\"":{\""first\"":\""Janet\"",\""last\"":\""Prichard\""},\""age\"":47}""  7) ""user:3""  8) ""{\""name\"":{\""first\"":\""Carol\"",\""last\"":\""Anderson\""},\""age\"":52}""  ```    It's also possible to multi-index on two fields:    ```  > SETINDEX last_name_age user:* JSON name.last JSON age  ```    For full JSON indexing syntax check out the [SETINDEX](https://github.com/tidwall/summitdb/wiki/SETINDEX#json) and [ITER](https://github.com/tidwall/summitdb/wiki/ITER) commands.    Fencing Tokens  --------------  A fencing token is simply a number that increases.   It's guaranteed to be consistent across the cluster and can never be deleted or decreased.   The value is a 64-bit unsigned integer. The first FENCE call will return ""1"".  This can be useful in applications that need things like distributed locking and preventing race conditions. FENCEGET will read the token without incrementing it.    ```  > FENCE mytoken  ""1""  > FENCE mytoken  ""2""  > FENCE mytoken  ""3""  > FENCEGET mytoken  ""3""  > FENCE mytoken  ""4""  ```      <a href=""raft-commands""></a>  Built-in Raft Commands  ----------------------  Here are a few commands for monitoring and managing the cluster:    - **RAFTADDPEER addr**    Adds a new member to the Raft cluster  - **RAFTREMOVEPEER addr**    Removes an existing member  - **RAFTPEERS**    Lists known peers and their status  - **RAFTLEADER**    Returns the Raft leader, if known  - **RAFTSNAPSHOT**    Triggers a snapshot operation  - **RAFTSTATE**    Returns the state of the node  - **RAFTSTATS**    Returns information and statistics for the node and cluster    Consistency and Durability  --------------------------    SummitDB is tuned by design for strong consistency and durability. A server shutdown, power event, or `kill -9` will not corrupt the state of the cluster or lose data.     All data persists to disk. SummitDB uses an append-only file format that stores for each command in exact order of execution.   Each command consists of a one write and one fsync. This provides excellent durability.    ### Read Consistency    The `--consistency` param has the following options:    - `low` - all nodes accept reads, small risk of [stale](http://stackoverflow.com/questions/1563319/what-is-stale-state) data  - `medium` - only the leader accepts reads, itty-bitty risk of stale data during a leadership change  - `high` - only the leader accepts reads, the raft log index is incremented to guarantee no stale data. **this is the default**    For example, setting the following options:    ```  $ summitdb --consistency high  ```    Provides the highest level of consistency. The default is **high**.      Leadership Changes  ------------------    In a Raft cluster only the leader can apply commands. If a command is attempted on a follower you will be presented with the response:    ```  > SET x y  -TRY 127.0.0.1:7481  ```    This means you should try the same command at the specified address.      Hot Backups  -----------    SummitDB supports hot-backing up a node.   You can retrieve and restore a snapshot of the database to a file using the [BACKUP](https://github.com/tidwall/summitdb/wiki/BACKUP) command.    ```  > BACKUP  BULK REPLY OF DATA  ```    Or using an HTTP connection like such:    ```  curl localhost:7481/backup -o backup.db  ```    The backup file format is a series of commands which are stored as [RESP Arrays](http://redis.io/topics/protocol#resp-arrays).  The command:  ```  SET mykey 123  ```  Is stored on disk as:  ```go  ""*3\r\n$3\r\nSET\r\n$5\r\nmykey\r\n$3\r\n123\r\n""  ```      To restore a system from a backup, issue each command to the leader. For example, using the `nc` command you could execute:  ```sh  $ cat backup.db | nc localhost 7481  ```    Commands  --------    Below is the complete list of commands.    **Keys and values**    [APPEND](https://github.com/tidwall/summitdb/wiki/APPEND),   [BITCOUNT](https://github.com/tidwall/summitdb/wiki/BITCOUNT),   [BITOP](https://github.com/tidwall/summitdb/wiki/BITOP),   [BITPOS](https://github.com/tidwall/summitdb/wiki/BITPOS),   [DBSIZE](https://github.com/tidwall/summitdb/wiki/DBSIZE),  [DECR](https://github.com/tidwall/summitdb/wiki/DECR),   [DECRBY](https://github.com/tidwall/summitdb/wiki/DECRBY),   [DEL](https://github.com/tidwall/summitdb/wiki/DEL),  [EXISTS](https://github.com/tidwall/summitdb/wiki/EXISTS),  [EXPIRE](https://github.com/tidwall/summitdb/wiki/EXPIRE),  [EXPIREAT](https://github.com/tidwall/summitdb/wiki/EXPIREAT),  [FENCE](https://github.com/tidwall/summitdb/wiki/FENCE),  [FENCEGET](https://github.com/tidwall/summitdb/wiki/FENCEGET),  [FLUSHDB](https://github.com/tidwall/summitdb/wiki/FLUSHDB),  [GET](https://github.com/tidwall/summitdb/wiki/GET),   [GETBIT](https://github.com/tidwall/summitdb/wiki/GETBIT),   [GETRANGE](https://github.com/tidwall/summitdb/wiki/GETRANGE),   [GETSET](https://github.com/tidwall/summitdb/wiki/GETSET),   [INCR](https://github.com/tidwall/summitdb/wiki/INCR),   [INCRBY](https://github.com/tidwall/summitdb/wiki/INCRBY),   [INCRBYFLOAT](https://github.com/tidwall/summitdb/wiki/INCRBYFLOAT),   [KEYS](https://github.com/tidwall/summitdb/wiki/KEYS),  [MGET](https://github.com/tidwall/summitdb/wiki/MGET),   [MSET](https://github.com/tidwall/summitdb/wiki/MSET),   [MSETNX](https://github.com/tidwall/summitdb/wiki/MSETNX),   [PDEL](https://github.com/tidwall/summitdb/wiki/PDEL),  [PERSIST](https://github.com/tidwall/summitdb/wiki/PERSIST),  [PEXPIRE](https://github.com/tidwall/summitdb/wiki/PEXPIRE),  [PEXPIREAT](https://github.com/tidwall/summitdb/wiki/PEXPIREAT),  [PTTL](https://github.com/tidwall/summitdb/wiki/PTTL),  [RENAME](https://github.com/tidwall/summitdb/wiki/RENAME),  [RENAMENX](https://github.com/tidwall/summitdb/wiki/RENAMENX),  [SET](https://github.com/tidwall/summitdb/wiki/SET),   [SETBIT](https://github.com/tidwall/summitdb/wiki/SETBIT),   [SETRANGE](https://github.com/tidwall/summitdb/wiki/SETRANGE),   [STRLEN](https://github.com/tidwall/summitdb/wiki/STRLEN),  [TTL](https://github.com/tidwall/summitdb/wiki/TTL)    **JSON**  [JSET](https://github.com/tidwall/summitdb/wiki/JSET),  [JGET](https://github.com/tidwall/summitdb/wiki/JGET),  [JDEL](https://github.com/tidwall/summitdb/wiki/JDEL)    **Indexes and iteration**    [DELINDEX](https://github.com/tidwall/summitdb/wiki/DELINDEX),  [INDEXES](https://github.com/tidwall/summitdb/wiki/INDEXES),  [ITER](https://github.com/tidwall/summitdb/wiki/ITER),  [RECT](https://github.com/tidwall/summitdb/wiki/RECT),  [SETINDEX](https://github.com/tidwall/summitdb/wiki/SETINDEX)    **Transactions**    [MULTI](https://github.com/tidwall/summitdb/wiki/MULTI),  [EXEC](https://github.com/tidwall/summitdb/wiki/EXEC),  [DISCARD](https://github.com/tidwall/summitdb/wiki/DISCARD)    **Scripts**    [EVAL](https://github.com/tidwall/summitdb/wiki/EVAL),  [EVALRO](https://github.com/tidwall/summitdb/wiki/EVALRO),  [EVALSHA](https://github.com/tidwall/summitdb/wiki/EVALSHA),  [EVALSHARO](https://github.com/tidwall/summitdb/wiki/EVALSHARO),  [SCRIPT LOAD](https://github.com/tidwall/summitdb/wiki/SCRIPT-LOAD),  [SCRIPT FLUSH](https://github.com/tidwall/summitdb/wiki/SCRIPT-FLUSH)    **Raft management**    [RAFTADDPEER](https://github.com/tidwall/summitdb/wiki/RAFTADDPEER),  [RAFTREMOVEPEER](https://github.com/tidwall/summitdb/wiki/RAFTREMOVEPEER),  [RAFTLEADER](https://github.com/tidwall/summitdb/wiki/RAFTLEADER),  [RAFTSNAPSHOT](https://github.com/tidwall/summitdb/wiki/RAFTSNAPSHOT),  [RAFTSTATE](https://github.com/tidwall/summitdb/wiki/RAFTSTATE),  [RAFTSTATS](https://github.com/tidwall/summitdb/wiki/RAFTSTATS)    **Server**    [BACKUP](https://github.com/tidwall/summitdb/wiki/BACKUP)    ## Contact  Josh Baker [@tidwall](http://twitter.com/tidwall)    ## License    SummitDB source code is available under the MIT [License](/LICENSE).       """
Big data;https://github.com/deroproject/graviton;"""  # Graviton Database: ZFS for key-value stores.    Graviton Database is simple, fast, versioned, authenticated, embeddable key-value store database in pure GOLANG.    Graviton Database in short is like ""ZFS for key-value stores"" in which every write is tracked, versioned and authenticated with cryptographic proofs. Additionally it is possible to take snapshots of database. Also it is possible to use simple copy,rsync commands for database backup even during live updates without any possibilities of database corruption.    ![Graviton: ZFS for key-value stores](images/GRAVITON.png?raw=true ""Graviton: ZFS for key-value stores"")    ## Project Status  Graviton is currently alpha software. Almost full unit test coverage and randomized black box testing are used to ensure database consistency and thread safety. The project already has 100% code coverage. A number of decisions such as change,rename APIs, handling errors, hashing algorithms etc. are being evaluated and open for improvements and suggestions.    ## Features  Graviton Database in short is  ""ZFS for key-value stores"".    * Authenticated data store (All keys, values are backed by blake 256 bit checksum).  * Append only data store.  * Support of 2^64 trees (Theoretically) within a single data store. Trees can be named and thus used as buckets.  * Support of values version tracking. All committed changes are versioned with ability to visit them at any point in time.   * Snapshots (Multi tree commits in a single version causing multi bucket sync, each snapshot can be visited, appended and further modified, keys deleted, values modified etc., new keys, values stored.)  * Ability to iterate over all key-value pairs in a tree.  * Ability to diff between 2 trees in linear time and report all changes of Insertions, Deletions, Modifications.)  * Minimal and simplified API.  * Theoretically support Exabyte data store, Multi TeraByte tested internally.  * Decoupled storage layer, allowing use of object stores such as Ceph, AWS etc.  * Ability to generate cryptographic proofs which can prove key existance or non-existance (Cryptographic Proofs are around 1 KB.)  * Superfast proof generation time of around 1000 proofs per second per core.  * Support for disk based filesystem based persistant stores.  * Support for memory based non-persistant stores.  * 100% code coverage        ## Table of Contents  1. [Getting Started](#getting-started)   1. [Installing](#installing)   1. [Opening and Using the Database](#opening-and-using-the-database)   1. [Graviton Tree](#graviton-tree)   1. [Using key,value pairs](#using-keyvalue-pairs)   1. [Iterating over keys](#iterating-over-keys)   1. [Snapshots](#snapshots)   1. [Diffing](#diffing) (Diffing of 2 trees to detect changes between versions or compare 2 arbitrary trees in linear time.)  1. [GravitonDB Backups](#gravitondb-backups)   1. [Stress testing](#stress-testing)   1. [Graviton Internals](#graviton-internals)   1. [Lines of Code](#lines-of-Code)   1. [TODO](#todo)   1. [Comparison with other databases](#comparison-with-other-databases) (Mysql, Postgres, LevelDB, RocksDB, LMDB, Bolt etc.)  1. [License](#license)       GNU General Public License v3.0    ## Getting Started  ### Installing  To start using Graviton DB, install Go and run go get:    ```go get github.com/deroproject/graviton/...```    This will retrieve the library and build the library      ### Opening and Using the Database    The top-level object in Graviton is a Store. It is represented as a directory with multiple files on server's disk and represents a consistent snapshot of your data at all times.    Example code to open database:        package main        import ""fmt""      import ""github.com/deroproject/graviton""        func main() {  	   //store, _ := graviton.NewDiskStore(""/tmp/testdb"")   // create a new testdb in ""/tmp/testdb""          store, _ := graviton.NewMemStore()            // create a new  DB in RAM          ss, _ := store.LoadSnapshot(0)           // load most recent snapshot          tree, _ := ss.GetTree(""root"")            // use or create tree named ""root""          tree.Put([]byte(""key""), []byte(""value"")) // insert a value          graviton.Commit(tree)                  // commit the tree          value, _ := tree.Get([]byte(""key""))          fmt.Printf(""value retrived from DB \""%s\""\n"", string(value))      }        //NOTE: Linux (or other platforms) have open file limit for 1024.       //    Default limits allows upto 2TB of Graviton databases.    ### Graviton Tree  A Tree in Graviton DB acts like a bucket in BoltDB or a ZFS dataset. It is named and can contain upto 128 byte names. Any store can contain infinite trees. Each tree can also contain infinite key-value pairs. However, practically being limited by the server or system storage space.    Each tree can be accessed with its merkle root hash using ""*GetTreeWithRootHash*"" API. Also each tree maintains its own separate version number and any specific version can be used *GetTreeWithVersion*. Note that each tree can also have arbitrary tags and any tagged tree can be accessed using the tag *GetTreeWithTag*. Also, 2 arbitrary trees can diffed in linear time and relevant changes detected.        NOTE: Tree tags or names cannot start with ':' .    ### Using key,value pairs    To save a key/value pair to a tree ( or bucket), use the `tree.Put()` function:    ```go          tree, _ := ss.GetTree(""root"")           tree.Put([]byte(""answer""), []byte(""44"")) // insert a value          graviton.Commit(tree)  // make the tree persistant by storing it in backend disk  ```    This will set the value of the `""answer""` key to `""44""` in the `root`  tree. To retrieve this value, we can use the `tree.Get()` function:    ```go  	tree, _ := ss.GetTree(""root"")   	v,_ := tree.Get([]byte(""answer""))  	fmt.Printf(""The answer is: %s\n"", v)  ```    The `Get()` function returns an error because its operation is guaranteed to work (unless there is some kind of system failure which we try to report). If the key exists then it will return its byte slice value. If it doesn't exist then it  will return  an error.     ### Iterating over keys    Graviton stores its keys in hash byte-sorted order within a tree. This makes sequential  iteration over these keys extremely fast. To iterate over keys GravitonDB uses a  `Cursor`:    ```go  	// Assume ""root"" tree exists and has keys      tree, _ := store.GetTree(""root"")   	c := tree.Cursor()    	for k, v, err := c.First(); err == nil; k, v, err = c.Next() {   		fmt.Printf(""key=%s, value=%s\n"", k, v)  	}  ```    The cursor allows you to move to a specific point in the list of keys and move  forward or backward through the keys one at a time.    The following functions are available on the cursor:    ```  First()  Move to the first key.  Last()   Move to the last key.  Next()   Move to the next key.  Prev()   Move to the previous key.  ```    Each of those functions has a return signature of `(key []byte, value []byte, err error)`.  When you have iterated to the end of the cursor then `Next()` will return an error `ErrNoMoreKeys`.  You must seek to a position using `First()`, `Last()`  before calling `Next()` or `Prev()`. If you do not seek to a position then these functions will return an error.      ### Snapshots  Snapshot refers to collective state of all buckets + data + history. Each commit( tree.Commit() or Commit(tree1, tree2 .....)) creates a new snapshot in the store.Each snapshot is represented by an incremental uint64 number, 0 represents most recent snapshot.  Snapshots can be used to access any arbitrary state of entire database at any point in time.    Example code for snapshots:        package main        import ""fmt""      import ""github.com/deroproject/graviton""        func main() {      	   key := []byte(""key1"")  	   //store, _ := graviton.NewDiskStore(""/tmp/testdb"")   // create a new testdb in ""/tmp/testdb""  	   store, _ := graviton.NewMemStore()          // create a new  DB in RAM  	   ss, _ := store.LoadSnapshot(0)         // load most recent snapshot  	   tree, _ := ss.GetTree(""root"")          // use or create tree named ""root""  	   tree.Put(key, []byte(""commit_value1"")) // insert a value  	   commit1, _ := graviton.Commit(tree)         // commit the tree  	   tree.Put(key, []byte(""commit_value2"")) // overwrite existing value  	   commit2, _ := graviton.Commit(tree)         // commit the tree again    	   // at this point, you have done 2 commits  	   // at first commit or snapshot,  ""root"" tree contains  ""key1 : commit_value1""  	   // at second commit or snapshot,  ""root"" tree contains  ""key1 : commit_value2""    	   // we will traverse now commit1 snapshot  	   ss, _ = store.LoadSnapshot(commit1)  	   tree, _ = ss.GetTree(""root"")  	   value, err := tree.Get(key)  	   fmt.Printf("" snapshot%d  key %s value %s err %s\n"", ss.GetVersion(), string(key), string(value), err)    	   // we will traverse now commit2 snapshot  	   ss, _ = store.LoadSnapshot(commit2)  	   tree, _ = ss.GetTree(""root"")  	   value, err = tree.Get(key)  	   fmt.Printf("" snapshot%d  key %s value %s err %s\n"", ss.GetVersion(), string(key), string(value), err)      }    ### Diffing  #### Diffing of 2 trees to detect changes between versions or compare 2 arbitrary trees in linear time.  Two arbitrary trees can be diffed in linear time to detect changes. Changes are of 3 types insertions, deletions and modifications (Same key but value changed). If the reported changes are applied to base tree, it will be equivalent to the head tree being compared.        func Diff(base_tree, head_tree *Tree, deleted, modified, inserted DiffHandler) (err error)    Diffhandler is a callback function of the following type having k,v as arguments        type DiffHandler func(k, v []byte)    The algorithm is linear time in the number of changes. Eg. a tree with billion KVs can be diffed with parent almost instantaneously.        ### GravitonDB Backups  Use simple commands like cp, copy or rsync to sync a Graviton database even while the database is being updated. However, as the database might be continuously appending, backup will always lag a bit. And note that the database or backups will NEVER get corrupted during copying while commits are being done.    ### Stress Testing  A mini tool to do single thread testing is provided which can be used to perform various tests on memory or disk backend.        go run github.com/deroproject/graviton/cmd/stress    See help using `--help` argument. To use disk backend, use `--memory=false`      ### Graviton Internals  Internally, all trees are stored within a base-2 merkle with collapsing path. This means if tree has 4 billion key-value pairs, it will only be 32 level deep.This leads to tremendous savings in storage space.This also means when you modify an existing key-value, only limited amount of nodes are touched.      ### Lines of Code      ~/tools/gocloc   --by-file  node_inner.go tree.go snapshot.go proof.go node_leaf.go  store.go node.go  hash.go  const.go doc.go  diff_tree.go cursor.go       -----------------------------------------------------------------      File           files          blank        comment           code      -----------------------------------------------------------------      node_inner.go                    76             33            364      store.go                         69             22            250      tree.go                          75             71            250      proof.go                         30             16            171      snapshot.go                      36             18            155      node_leaf.go                     29              3            150      diff_tree.go                     34             33            133      cursor.go                        21             15            106      node.go                           5              3             35      const.go                          4              0             21      hash.go                           7              2             19      doc.go                           16             42              1      -----------------------------------------------------------------      TOTAL             12            402            258           1655      -----------------------------------------------------------------    ## TODO   * Currently it is not optimized for speed and GC (Garbage collection).  * Expose/build metrics.  * Currently, we have error reportingapi to reports rot bits, but nothing about disks corruption, should we discard such error design and make the API simpler (except snapshots, tree loading, commiting, no more errors ). More discussion required on this hard-disk failures,errors etc. required.      ### Comparison with other databases  None of the following databases provides ability to traverse back-in-time for each and every commit. GravitonDB is the only DB which provides back-in-time. Also presently GravitonDB is the only database which can diff between 2 trees in linear time. Let's compare between other features of some databases.    #### Postgres, MySQL, & other relational databases    Relational databases structure data into rows and are only accessible through  the use of SQL. This approach provides flexibility in how you store and query  your data but also incurs overhead in parsing and planning SQL statements. GravitonDB  accesses all data by a byte slice key. This makes GravitonDB fast to read and write  data by key but provides no built-in support for joining values together.    Most relational databases (with the exception of SQLite) are standalone servers  that run separately from the application. This gives systems  flexibility to connect multiple application servers to a single database  server but also adds overhead in serializing and transporting data over the  network. Graviton runs as a library included in your application so all data access  has to go through your application's process. This brings data closer to your  application but limits multi-process access to the data.        #### LevelDB, RocksDB    LevelDB and its derivatives (RocksDB, HyperLevelDB) are similar to Graviton in that  they are libraries bundled into the application, However, their underlying  structure is a log-structured merge-tree (LSM tree). An LSM tree optimizes  random writes by using a write ahead log and multi-tiered, sorted files called  SSTables. Graviton uses a base 2 merkle tree internally. Both approaches  have trade-offs.    If you require a high random write throughput or you need to use  spinning disks then LevelDB could be a good choice unless there are requirements of versioning, authenticated proofs or other features of Graviton database.    #### LMDB, BoltDB    LMDB, Bolt are architecturally similar. Both use a B+ tree, have ACID semantics with fully serializable transactions, and support lock-free MVCC using a single writer and multiple readers.    In-addition LMDB heavily focuses on raw performance while BoltDB focus on simplicity and ease of use. For example, LMDB allows several unsafe actions such as direct writes for the sake of performance. Bolt opts to disallow actions which can leave the database in a corrupted state. The only exception to this in Bolt is `DB.NoSync`.GravitonDB does not leave the database in corrupted state at any point in time.      In-addition LMDB, BoltDB doesn't support versioning, snapshots, linear diffing etc. features only Graviton provides such features for now.      ### License   [GNU General Public License v3.0](https://github.com/deroproject/graviton/blob/master/LICENSE) """
Big data;https://github.com/rakam-io/rakam;"""[![Build Status](https://travis-ci.org/rakam-io/rakam.svg?branch=master)](https://travis-ci.org/rakam-io/rakam)  [<img alt=""Deploy"" src=""https://www.herokucdn.com/deploy/button.png"" height=""21"">](https://dashboard.heroku.com/new?button-url=https%3A%2F%2Fgithub.com%2Frakam-io%2Frakam&template=https%3A%2F%2Fgithub.com%2Frakam-io%2Frakam)       Rakam  =======    Rakam is an analytics platform that allows you to create your analytics services.    Features / Goals  ------------  Rakam is a modular analytics platform that gives you a set of features to create your own analytics service.    Typical workflow of using Rakam:  * Collect data from multiple sources with **[trackers, client libraries, webhooks, tasks etc.](https://docs.rakam.io/docs/collect-data)**  * Enrich and sanitize your event data with **[event mappers](https://docs.rakam.io/docs/event-enrichment)**  * Store data in a data warehouse to analyze it later. (Postgresql, Snowflake, S3 etc.)  * Analyze your event data with your SQL queries and integrated rich analytics APIs with Rakam Cloud (**[funnel, retention, segmentation reports](https://docs.rakam.io/docs/core-cencept)**  * **[Develop your own modules](https://docs.rakam.io/docs/developing-modules)** for Rakam to customize it for your needs.    We also provide user interface for Rakam as a separate product called [Rakam UI](https://app.rakam.io). You can create custom reports with SQL, dashboards, funnel and retention reports via [Rakam UI](https://beta.rakam.io).    All these features come with a single box, you just need to specify which modules you want to use using a configuration file (config.properties) and Rakam will do the rest for you.  We also provide cloud deployment tools for scaling your Rakam cluster easily.    Deployment  ----------    If your event data-set can fit in a single server, we recommend using Postgresql backend. Rakam will collect all your events in row-oriented format in a Postgresql node. All the features provided by Rakam are supported in Postgresql deployment type. Please note that we support Postgresql 11 because we're using new features such as partitioning and BRIN indexes for performance.    However Rakam is designed to be highly scalable in order to provide a solution for high work-loads. You can configure Rakam to send events to a distributed commit-log such as Apache Kafka or Amazon Kinesis in serialized Apache Avro format and process data in PrestoDB workers and store them in a distributed filesystem in a columnar format.    ### Heroku    You can deploy Rakam to Heroku using Heroku button, it uses Heroku Postgresql add-on for your app and uses Postgresql deployment type.    [![Deploy](https://www.herokucdn.com/deploy/button.png)](https://dashboard.heroku.com/new?button-url=https%3A%2F%2Fgithub.com%2Frakam-io%2Frakam&template=https%3A%2F%2Fgithub.com%2Frakam-io%2Frakam)    ### Docker    Run the following command to start a Postgresql server in docker container and Rakam API in your local environment.        docker run -d --name rakam-db -e POSTGRES_PASSWORD=dummy -e POSTGRES_USER=rakam postgres:10.1 && docker run --link rakam-db --name rakam -p 9999:9999 -e RAKAM_CONFIG_LOCK__KEY=mylockKey -e RAKAM_CONFIG_STORE_ADAPTER_POSTGRESQL_URL=postgres://rakam:dummy@rakam-db:5432/rakam buremba/rakam    After docker container is started, visit [http://127.0.0.1:9999](http://127.0.0.1:9999) and follow the instructions. You can also register your local Rakam API to Rakam BI at  [http://app.rakam.io](http://app.rakam.io)  or directly use Rakam API. You may also consult to [API documentation](https://api.rakam.io) for details of the API.    We also provide a docker-compose definition for a Postgresql backend. Create a `docker-compose.yml` with this definition and run the command `docker-compose -f docker-compose.yml up -d`.        version: '2.1'      services:        rakam-db:          image: postgres:11.4          environment:            - POSTGRES_PASSWORD=dummy            - POSTGRES_USER=rakam          healthcheck:            test: [""CMD-SHELL"", ""pg_isready""]            interval: 5s            timeout: 5s            retries: 3        rakam-api:          image: buremba/rakam          environment:            - RAKAM_CONFIG_STORE_ADAPTER_POSTGRESQL_URL=postgres://rakam:dummy@rakam-db:5432/rakam            - RAKAM_CONFIG_LOCK__KEY=mylockKey          ports:            - ""9999:9999""          depends_on:            rakam-db:              condition: service_healthy    You can set config variables for Rakam instance using environment variables. All properties in config.properties file can be set via environment variable `RAKAM_CONFIG_property_name_dots_replaced_by_underscore`.  For example, if you want to set `store.adapter=postgresql` you need to set environment variable `RAKAM_CONFIG_STORE_ADAPTER=postgresql`. Also the dash `-` is replaced by double underscore character `__`.   Therefore the environment variable `RAKAM_CONFIG_LOCK__KEY` corresponds to `lock-key` config property.     Dockerfile will generate `config.properties` file from environment variables in docker container that start with `RAKAM_CONFIG` prefix.    In order to set environment variables for container, you may use `-e` flag for for `docker run` but we advice you to set all environment variables in a file and use  `--env-file` flag when starting your container.    Then you can share same file among the Rakam containers. If Dockerfile can't find any environment variable starts with `RAKAM_CONFIG`, it tries to connect Postgresql instance created with docker-compose.    ### AWS (Terraform)    See [https://github.com/rakam-io/rakam-api-terraform-aws](https://github.com/rakam-io/rakam-api-terraform-aws).    Terraform installer is the recommended way to deploy Rakam in production because it automatically handles most of the complexity like fail over and load-balancing.    ### Custom  - Download Java 1.8 for your operating system.  - Download latest version from [Bintray](https://dl.bintray.com/buremba/maven/org/rakam/rakam) ([VERSION]/rakam-[VERSION]-.bundle.tar.gz) extract package.  - Modify `etc/config.properties` [(sample for Postgresql deployment type)](https://gist.github.com/buremba/ada247b0ce837cfd3a81a92a98629f1d) file and run `bin/launcher start`.  - The launcher script can take the following arguments: `start|restart|stop|status|run`.   `bin/launcher run` will start Rakam in foreground.    ### Building Rakam  You can try the master branch by pulling the source code from Github and building Rakam using Maven:    ##### Requirements  - Java 8  - Maven 3.2.3+ (for building)    ```sh  git clone https://github.com/rakam-io/rakam.git  cd rakam  mvn clean install package -DskipTests  ```    ##### Running the application locally  ```sh  rakam/target/rakam-*-bundle/rakam-*/bin/launcher.py run --config rakam/target/rakam-*-bundle/rakam-*/etc/config.properties  ```    Note that you need to modify `config.properties` file in order to be able to start Rakam. [(sample for Postgresql deployment type)](https://gist.github.com/buremba/ada247b0ce837cfd3a81a92a98629f1d)    ##### Running Rakam in your IDE    Since we already use Maven, you can import Rakam to your IDE using the root pom.xml file. We recommend using Intellij IDEA since the core team uses it when developing Rakam. Here is a sample configuration for executing Rakam in your IDE:    ```  Main Class: org.rakam.ServiceStarter  VM Options: -ea -Xmx2G -Dconfig=YOUR_CONFIG_DIRECTORY/config.properties  Working directory: $MODULE_DIR$  Use classpath of module: rakam  ```    ### Managed    We're also working for managed Rakam cluster, we will deploy Rakam to our AWS accounts and manage it for you so that you don't need to worry about scaling, managing and software updates. We will do it for you.  Please shoot us an email to `emre@rakam.io` if you want to test our managed Rakam service.    Web application  ------------  This repository contains Rakam API server that allows you to interact with Rakam using a REST interface. If you already have a frontend and developed a custom analytics service based on Rakam, it's all you need.    However, we also developed Rakam Web Application that allows you to analyze your user and event data-set but performing SQL queries, visualising your data in various charts, creating (real-time) dashboards and custom reports. You can turn Rakam into a analytics web service similar to [Mixpanel](https://mixpanel.com), [Kissmetrics](https://kissmetrics.com) and [Localytics](https://localytics.com) using the web application. Otherwise, Rakam server is similar to [Keen.io](https://keen.io) with SQL as query language and some extra features.    Another nice property of Rakam web application is being BI `(Business Intelligence)` tool. If you can disable collect APIs and connect Rakam to your SQL database with JDBC adapter and use Rakam application to query your data in your database. Rakam Web Application has various charting formats, supports parameterized SQL queries, custom pages that allows you to design pages with internal components.    Contribution  ------------  Currently I'm actively working on Rakam. If you want to contribute the project or suggest an idea feel free to fork it or create a ticket for your suggestions. I promise to respond you ASAP.  The purpose of Rakam is being generic data analysis tool which can be a solution for many use cases. Rakam still needs too much work and will be evolved based on people's needs so your thoughts are important.    Acknowledgment  --------------  [![YourKit](https://www.yourkit.com/images/yklogo.png)](https://www.yourkit.com/java/profiler/index.jsp)    We use YourKit Java Profiler in order to monitor the JVM instances for identifing the bugs and potential bottlenecks. Kudos to YourKit for supporting Rakam with your full-featured Java Profile! """
Big data;https://github.com/jnv/lists;"""# Lists    List of useful, silly and [awesome](#awesome-) lists curated on GitHub. Contributions welcome!    âœ¨ Now also available [in CSV](https://github.com/jnv/lists/blob/gh-pages/lists.csv)! âœ¨    - [Lists](#lists)    - [Non-technical](#non-technical)    - [Technical](#technical)      - [awesome-*](#awesome-)    - [Lists of lists](#lists-of-lists)      - [Lists of lists of lists](#lists-of-lists-of-lists)        - [Lists of lists of lists of lists](#lists-of-lists-of-lists-of-lists)          - [Lists of lists of lists of lists of lists](#lists-of-lists-of-lists-of-lists-of-lists)            - [Lists of lists of lists of lists of lists of lists](#lists-of-lists-of-lists-of-lists-of-lists-of-lists)            - [Lists of lists of lists of lists of lists of lists of lists](#lists-of-lists-of-lists-of-lists-of-lists-of-lists-of-lists)    - [License](#license)    <!-- lists-start -->    ## Non-technical    * [aksh](https://github.com/svaksha/aksh) â€“ Bibliography of STEM (Science, Technology, Engineering & Mathematics) resources and grey literature.  * [amas](https://github.com/sindresorhus/amas) â€“ Awesome & Marvelous Amas (Ask Me Anything) on GitHub  * [Annual-Reading-List](https://github.com/davidskeck/Annual-Reading-List) â€“ Things to read every year.  * [awesomebandnames](https://github.com/jnv/awesomebandnames) â€“ The open-source list of awesome band names.  * [awesome-belarus-online](https://github.com/Friz-zy/awesome-belarus-online) â€“ Useful belarusian online resources.  * [awesome-biology](https://github.com/raivivek/awesome-biology) â€“ Learning resources, research papers, tools and other resources related to Biology.  * [awesome-bitclout](https://github.com/barrymode/awesome-bitclout) â€“ BitClout is social media on a blockchain. Everyone gets their own coin.  * [awesome-board-games](https://github.com/edm00se/awesome-board-games) â€“ Awesome and exceptional board games    - https://awesomeboard.games  * [awesome-ethics](https://github.com/HussainAther/awesome-ethics)  * [awesome-fantasy](https://github.com/RichardLitt/awesome-fantasy) â€“ Fantasy literature worth reading.  * [awesome-gif](https://github.com/Kikobeats/awesome-gif) â€“ GIF /dÊ’/ links and resources.  * [awesome-glasgow](https://github.com/allyjweir/awesome-glasgow) â€“ Some highlights around Glasgow, Scotland.  * [awesome-hacking-locations](https://github.com/daviddias/awesome-hacking-locations) â€“ Hacking places, organised by Country and City, listing if it features power and wifi.  * [awesome-health](https://github.com/prabhic/awesome-health) â€“ Useful health resources.  * [awesome-images](https://github.com/heyalexej/awesome-images) â€“ Free (stock) photo resources for your projects.  * [awesome-kimchi](https://github.com/jeyraof/awesome-kimchi) â€“ Kimchi of the people, by the people, for the people.  * [awesome-lego](https://github.com/ad-si/awesome-lego)  * [awesome-lockpicking](https://github.com/fabacab/awesome-lockpicking) â€“ Guides, tools, and other resources related to the security and compromise of locks, safes, and keys.  * [awesome-maps](https://github.com/simsieg/awesome-maps) â€“ Various Online Maps  * [awesome-mental-health](https://github.com/dreamingechoes/awesome-mental-health) â€“ Articles, websites and resources about mental health in the software industry.    - https://dreamingechoes.github.io/awesome-mental-health  * [awesome-parasite](https://github.com/ecohealthalliance/awesome-parasite) â€“ Parasites and host-pathogen interactions.  * [awesome-philosophy](https://github.com/HussainAther/awesome-philosophy) â€“ Philosophy  * [awesome-reddit-channels](https://github.com/MadhuNimmo/awesome-reddit-channels) â€“ Reddit Channels every programmer must follow.  * [awesome-scifi](https://github.com/sindresorhus/awesome-scifi) â€“ Sci-Fi worth consuming.  * [awesome-speaking](https://github.com/matteofigus/awesome-speaking) â€“ Resources about public speaking  * [awesome-stock-resources](https://github.com/neutraltone/awesome-stock-resources) â€“ Stock photography, video and illustration websites.  * [awesome-theravada](https://github.com/johnjago/awesome-theravada) â€“ Theravada Buddhist teachings  * [awesome-uncopyright](https://github.com/johnjago/awesome-uncopyright) â€“ All things public domain  * [awesome-webcomics](https://github.com/dhamaniasad/awesome-webcomics)  * [baby-sleep](https://github.com/simple10/baby-sleep) â€“ Baby sleep guides curated from the best of the Internet.  * [bailfunds.github.io](https://github.com/bailfunds/bailfunds.github.io) â€“ Bail Funds for Protestors across the USA.    - https://bailfunds.github.io/  * [boardgames](https://gitlab.com/gamearians/boardgames) â€“ Boardgames and boardgame-related projects that can be found on GitHub.  * [chinese-poetry](https://github.com/chinese-poetry/chinese-poetry) _In Chinese_ â€“ The most comprehensive database of Chinese poetry    - http://shici.store  * [cocktails](https://github.com/balevine/cocktails) â€“ Cocktail Recipes  * [corporate-logos](https://github.com/marketreef/corporate-logos) â€“ Curated repo of publicly listed co. logos, identified by ticker. *Almost 1500 logos*  * [creative-commons-media](https://github.com/shime/creative-commons-media) â€“ Audio, graphics and other resources that provide media licensed under Creative Commons licenses.  * [discord-listings](https://github.com/angrymouse/discord-listings) â€“ Places where to promote Discord servers.  * [dissertation-tips](https://github.com/katychuang/dissertation-tips) â€“ Resources to help PhD students complete their dissertation successfully.  * [diversity-index](https://github.com/svaksha/diversity-index) â€“ Grants, scholarships and FA that encourages diversity in STEM fields aimed at half the world's population, Women!    - http://svaksha.github.io/diversity-index  * [diversity-twitter](https://github.com/gregorycoleman/diversity-twitter) â€“ Twitter feeds of interesting people to follow for Diversity & Inclusion  * [food](https://notabug.org/themusicgod1/food)  * [food-recipes](https://github.com/obfuscurity/food-recipes) â€“ Honest-to-goodness ""real food"" recipes  * [frequent-transit-maps](https://github.com/wwcline/list-of-frequent-transit-maps) â€“ Transit maps highlighting frequent all-day service  * [global-reports](https://github.com/andressoop/global-reports) â€“ Major global reports published by international organisations  * [guitarspecs](https://github.com/gitfrage/guitarspecs) â€“ Electric guitar's parts specs    - https://gitfrage.github.io/guitarspecs/  * [isaacs/reading-list](https://github.com/isaacs/reading-list) â€“ [isaac](https://github.com/isaacs)'s reading list.  * [lawrence-veggie](https://github.com/codysoyland/lawrence-veggie) â€“ Vegetarian/vegan restaurants in Lawrence, KS.  * [lawyersongithub](https://github.com/dpp/lawyersongithub) â€“ A club full of lawyers who also have GitHub accounts.  * [low-resource-languages](https://github.com/RichardLitt/low-resource-languages) â€“ Conservation, development, and documentation of endangered, minority, and low or under-resourced human languages.  * [Mind-Expanding-Books](https://github.com/hackerkid/Mind-Expanding-Books) â€“ :books: Books that will blow your mind    - http://books.vishnuks.com  * [mining-resources](https://github.com/Mining-Resources/mining-resources) â€“ Natural resources mining.  * [no-free-basics](https://github.com/net-neutrality/no-free-basics) â€“ Those who have spoken up against Facebook's â€œFree Basicsâ€    - https://net-neutrality.github.io/no-free-basics/  * [open-sustainable-technology](https://github.com/protontypes/open-sustainable-technology) â€“ Worldwide open technology projects preserving a stable climate, energy supply and vital natural resources.  * [plastic-free](https://github.com/IrosTheBeggar/plastic-free) â€“ Plastic-free products.  * [ProjectSoundtracks](https://github.com/sarthology/ProjectSoundtracks) â€“ Soundtracks to boost your Productivity and Focus.  * [PublicMedia](https://github.com/melodykramer/PublicMedia) â€“ Everything about public (broadcast) media.    - Also [an introduction to working with GitHub](https://melodykramer.github.io/2015/04/06/learning-github-without-one-line-of-code) for non-programmers.  * [recipes](https://github.com/bzimmerman/recipes) by @bzimmerman â€“ This repository contains tasty open-source recipes.  * [recipes](https://github.com/csswizardry/recipes) by @csswizardy â€“ Collection of things I like cooking  * [recipes](https://github.com/LarryMad/recipes) by @LarryMad  * [recipes](https://github.com/nofunsir/recipes) by @nofunsir  * [recipes](https://github.com/schacon/recipes) by @schacon  * [recipes](https://github.com/silizuo/recipes) _In Chinese and English_ by @silizuo  * [sf-vegetarian-restaurants](https://github.com/mojombo/sf-vegetarian-restaurants) â€“ Awesome vegetarian-friendly restaurants in SF  * [shelfies](https://github.com/kyro/shelfies) â€“ Bookshelves of awesome people, community-transcribed.  * [SiliconValleyThingsToDo](https://github.com/cjbarber/SiliconValleyThingsToDo) â€“ Things to do and activities within Silicon Valley.  * [stayinghomeclub](https://github.com/phildini/stayinghomeclub) â€“ All the companies working from home or events changed because of covid-19.    - https://stayinghome.club  * [Sustainable-Earth](https://github.com/bizz84/Sustainable-Earth) â€“ All things sustainable  * [tacofancy](https://github.com/sinker/tacofancy) â€“ community-driven taco repo. stars stars stars.  * [teesites](https://github.com/elder-cb/teesites) â€“ Great sites to buy awesome t-shirts and other cool stuff.      ## Technical    * [101](https://github.com/ojas/101) â€“ Resources on running a software biz.  * [10PL](https://github.com/nuprl/10PL) â€“ 10 papers that all PhD students in programming languages ought to know, for some value of 10.  * [1on1-questions](https://github.com/VGraupera/1on1-questions) â€“ 1 on 1 meeting questions.  * [30-seconds-of-code](https://github.com/30-seconds/30-seconds-of-code) â€“ JavaScript snippets you can understand in 30 seconds or less.    - https://30secondsofcode.org/  * [30-seconds-of-interviews](https://github.com/30-seconds/30-seconds-of-interviews) â€“ Common interview questions to help you prepare for your next interview.  * [a11yproject.com](https://github.com/a11yproject/a11yproject.com) â€“ A communityâ€“driven effort to make web accessibility easier.    - https://a11yproject.com  * [addinslist](https://github.com/daattali/addinslist) â€“ Useful [RStudio](https://www.rstudio.com/) addins  * [admesh-projects](https://github.com/admesh/admesh-projects) â€“ Projects using [ADMesh](https://github.com/admesh/admesh) (a triangulated solid meshes processor).  * [AI-reading-list](https://github.com/m0nologuer/AI-reading-list) â€“ Papers about Artificial Intelligence.  * [alexandria](https://github.com/alxgcrz/alexandria) _In English and Spanish_ â€“ Various resources by [@alxgcrz](https://github.com/alxgcrz)  * [algovis](https://github.com/enjalot/algovis) â€“ Algorithm Visualization.  * [alternative-front-ends](https://github.com/mendel5/alternative-front-ends) â€“ Alternative open source front-ends for popular internet platforms (e.g. YouTube, Twitter, etc.).  * [alternative-internet](https://github.com/redecentralize/alternative-internet) â€“ A collection of interesting new networks and tech aiming at decentralisation (in some form).  * [amazing-deployment](https://github.com/delirehberi/amazing-deployment)  * [android-awesome-libraries](https://github.com/kaiinui/android-awesome-libraries) â€“ Useful Android development libraries with usage examples.  * [android-dev-readme](https://github.com/anirudh24seven/android-dev-readme) â€“ Links for every Android developer.  * [AndroidDevTools](https://github.com/inferjay/AndroidDevTools) _In Chinese_ â€“ SDK, development tools, libraries, and resources.    - http://www.androiddevtools.cn/  * [android-jobs](https://github.com/android-cn/android-jobs) _In Chinese_ â€“ Android positions in China.  * [Android-Learning-Resources](https://github.com/zhujun2730/Android-Learning-Resources) _In Chinese_ â€“ Learning resources for Android.  * [android-open-project](https://github.com/Trinea/android-open-project) _In Chinese_ â€“ Collect and classify android open source projects.  * [android-security-awesome](https://github.com/ashishb/android-security-awesome) â€“ â€œA lot of work is happening in academia and industry on tools to perform dynamic analysis, static analysis and reverse engineering of android apps.â€  * [android-tech-frontier](https://github.com/hehonghui/android-tech-frontier) _In Chinese_ â€“ Translation of articles about Android development.  * [angular-education](https://github.com/timjacobi/angular-education) â€“ Helpful material to develop using Angular  * [AngularJS-Learning](https://github.com/jmcunningham/AngularJS-Learning)  * [ansible-gentoo-roles](https://github.com/jirutka/ansible-gentoo-roles) â€“ Ansible roles for Gentoo Linux.  * [apis-list](https://github.com/apis-list/apis-list) â€“ Community maintained, human and machine readable list of Public APIs  * [app-ideas](https://github.com/florinpop17/app-ideas) â€“ Application ideas which can be used to improve your coding skills.  * [app-launch-guide](https://github.com/adamwulf/app-launch-guide) â€“ Indie dev's definitive guide to building and launching your app, including pre-launch, marketing, building, QA, buzz building, and launch.  * [applied-ml](https://github.com/eugeneyan/applied-ml) â€“ Data science & machine learning in production.  * [APTnotes](https://github.com/kbandla/APTnotes) â€“ Various public documents, whitepapers and articles about APT [Advanced persistent threat] campaigns.  * [architect-awesome](https://github.com/xingshaocheng/architect-awesome) _In Chinese_ â€“ åŽç«¯æž¶æž„å¸ˆæŠ€æœ¯å›¾è°±  * [asynchronous-php](https://github.com/elazar/asynchronous-php) â€“ Asynchronous programming in PHP.  * [Automated-SPA-Testing](https://github.com/webpro/Automated-SPA-Testing) â€“ Automated unit & functional testing for web applications [JavaScript et al.].  * [awful-ai](https://github.com/daviddao/awful-ai) â€“ Current scary usages of AI, hoping to raise awareness to its misuses in society.  * [awmy](https://github.com/potch/awmy) â€“ Are We Meta Yet?    - http://arewemetayet.com/  * [b1fipl](https://github.com/marcpaq/b1fipl) â€“ A Bestiary of Single-File Implementations of Programming Languages.  * [Backpack](https://github.com/sevab/Backpack) â€“ Various learning resources, organized by technology/topic.  * [badass-dev-resources](https://github.com/sodevious/badass-dev-resources) â€“ #bada55 front-end developer resources.  * [Badges4-README.md-Profile](https://github.com/alexandresanlim/Badges4-README.md-Profile) â€“ Badges for GitHub profiles.  * [bangalore-startups](https://github.com/hemanth/bangalore-startups) â€“ Startups in Bangalore.  * [beautiful-docs](https://github.com/PharkMillups/beautiful-docs) â€“ Pointers to useful, well-written, and otherwise beautiful documentation.  * [BEM-resources](https://github.com/sturobson/BEM-resources)  * [Best-App](https://github.com/hzlzh/Best-App) _In Chinese_ â€“ Recommendations for best desktop and mobile apps.  * [best-of-awesomeness-and-usefulness-for-webdev](https://github.com/Pestov/best-of-awesomeness-and-usefulness-for-webdev) â€“ Digest of the most useful tools and resources for the last year.    - [Russian version](https://github.com/Pestov/best-of-awesomeness-and-usefulness-for-webdev/tree/master/ru)  * [best-practices-checklist](https://github.com/palash25/best-practices-checklist) â€“ Language-specific resources to look up the best practices followed by that particular language's community.  * [Best-websites-a-programmer-should-visit](https://github.com/sdmg15/Best-websites-a-programmer-should-visit) â€“ Some useful websites for programmers.  * [Best-websites-a-programmer-should-visit-zh](https://github.com/tuteng/Best-websites-a-programmer-should-visit-zh) _In Chinese_ â€“ ç¨‹åºå‘˜åº”è¯¥è®¿é—®çš„æœ€ä½³ç½‘ç«™ä¸­æ–‡ç‰ˆ  * [bigdata-ecosystem](https://github.com/zenkay/bigdata-ecosystem) â€“ Big-data related projects packed into a JSON dataset.    - http://bigdata.andreamostosi.name/  * [Big-List-of-ActivityPub](https://github.com/shleeable/Big-List-of-ActivityPub) â€“ ActivityPub Projects  * [big-list-of-naughty-strings](https://github.com/minimaxir/big-list-of-naughty-strings) â€“ Strings which have a high probability of causing issues when used as user-input data.  * [bioinformatics-compbio-tools](https://github.com/lancelafontaine/bioinformatics-compbio-tools) â€“ Bioinformatics and computational biology tools.  * [bitcoin-reading-list](https://github.com/jashmenn/bitcoin-reading-list) â€“ Learn to program Bitcoin transactions.  * [BNN-ANN-papers](https://github.com/takyamamoto/BNN-ANN-papers) â€“ Papers about Biological and Artificial Neural Networks related to (Computational) Neuroscience  * [bookmarklets](https://github.com/RadLikeWhoa/bookmarklets) â€“ Bookmarklets that are useful on the web    - https://sacha.me/bookmarklets/  * [bookshelf](https://github.com/OpenTechSchool/bookshelf) â€“ Reading lists for learners.  * [bots](https://github.com/hackerkid/bots) â€“ Tools for building bots  * [breakfast-repo](https://github.com/ashleygwilliams/breakfast-repo) â€“ Videos, recordings, and podcasts to accompany our morning coffee.  * [browser-resources](https://github.com/azu/browser-resources) â€“ Latest JavaScript information by browser.  * [build-your-own-x](https://github.com/danistefanovic/build-your-own-x) â€“ Build your own (insert technology here)  * [channels](https://github.com/andrew--r/channels) _In Russian_ â€“ YouTube channels for web developers.  * [citizen-science](https://github.com/dylanrees/citizen-science) â€“ Scientific tools to empower communities and/or practice various forms of non-institutional science  * [classics](https://github.com/eyy/classics) â€“ Classical studies (Latin and Ancient Greek) resources: software, code and raw data.  * [Clone-Wars](https://github.com/GorvGoyl/Clone-Wars) â€“ Open-source clones of popular sites.  * [cloud-conferences](https://github.com/stefan-kolb/cloud-conferences) â€“ A collection of scientific and industry conferences focused on cloud computing.    - http://stefan-kolb.github.io/cloud-conferences/  * [code-canon](https://github.com/darius/code-canon) â€“ Code worth reading.  * [codeface](https://github.com/chrissimpkins/codeface) â€“ Typefaces for source code / text editors.  * [Colorful](https://github.com/Siddharth11/Colorful) â€“ Choose your next color scheme  * [CompilerJobs](https://github.com/mgaudet/CompilerJobs) â€“ Compiler, language, and runtime teams for people looking for jobs in this area.  * [compilers-targeting-c](https://github.com/dbohdan/compilers-targeting-c) â€“ Compilers that can generate C code.  * [computer-science](https://github.com/ossu/computer-science) â€“ Path to a free self-taught graduation in Computer Science.  * [content-management-systems](https://github.com/ahadb/content-management-systems) â€“ Open source & proprietary content management systems.  * [critical-path-css-tools](https://github.com/addyosmani/critical-path-css-tools) â€“ Tools to help prioritize above-the-fold CSS.  * [CryptoList](https://github.com/coinpride/CryptoList) â€“ Blockchain & cryptocurrency resources.  * [crypto-might-not-suck](https://github.com/sweis/crypto-might-not-suck) â€“ Crypto Projects that Might not Suck.  * [cscs](https://github.com/SalGnt/cscs) â€“ Coding Style Conventions and Standards.  * [css-in-js](https://github.com/MicheleBertoli/css-in-js) â€“ CSS in JS techniques comparison for React et al.  * [css-protips](https://github.com/AllThingsSmitty/css-protips) â€“ Take your CSS skills pro  * [cto](https://github.com/92bondstreet/cto) â€“ Chief Technology Officers resources.  * [curated-list-espresso-sugar-plugins](https://github.com/GioSensation/curated-list-espresso-sugar-plugins) â€“ Sugar plugins for Espresso, the code editor by MacRabbit.  * [curated-programming-resources](https://github.com/Michael0x2a/curated-programming-resources) â€“ Resources for learning programming and computer science.  * [curatedseotools](https://github.com/sneg55/curatedseotools) â€“ Best SEO Tools Stash    - https://curatedseotools.com  * [cycle-ecosystem](https://github.com/Widdershin/cycle-ecosystem) â€“ What are the most popular and trending libraries for [Cycle.js](http://cycle.js.org/)?  * [dad-jokes](https://github.com/wesbos/dad-jokes) â€“ Dad style programming jokes.  * [datajournalists-toolbox](https://github.com/basilesimon/datajournalists-toolbox) â€“ Tools for datajournalists, with examples and gists.  * [datascience](https://github.com/r0f1/datascience) â€“ Python resources for data science.  * [data-science-blogs](https://github.com/rushter/data-science-blogs)  * [data-science-must-watch](https://github.com/kmonsoor/data-must-watch)  * [datasciencemasters](https://github.com/datasciencemasters/go) â€“ The Curriculum for learning Data Science, Open Source and at your fingertips.    - http://datasciencemasters.org/  * [datascience-pizza](https://github.com/PizzaDeDados/datascience-pizza) _In Portugese_ â€“ Materiais de estudo em anÃ¡lise de dados e Ã¡reas afins, empresas que trabalham com dados e dicionÃ¡rio de conceitos.  * [DataSciencePython](https://github.com/ujjwalkarn/DataSciencePython) â€“ Python tutorials for Data Science, NLP and Machine Learning  * [debugging-stories](https://github.com/danluu/debugging-stories) â€“ Collection of links to various debugging stories.  * [Deep-NLP-Resources](https://github.com/pawangeek/Deep-NLP-Resources) â€“ Deep Natural Language Processing  * [degoogle](https://github.com/tycrek/degoogle) â€“ Alternatives to Google's products.  * [Developer-Conferences](https://github.com/MurtzaM/Developer-Conferences) â€“ Upcoming developer conferences.  * [dev-movies](https://github.com/aryaminus/dev-movies) â€“ Recommended movies for people working in the Software and IT Industry.  * [devopsbookmarks.com](https://github.com/devopsbookmarks/devopsbookmarks.com) â€“ To discover tools in the devops landscape.    - http://www.devopsbookmarks.com/  * [devops_resources](https://github.com/dustinmm80/devops_resources)  * [DevopsWiki](https://github.com/Leo-G/DevopsWiki) â€“ Devops Tools, Tutorials and Scripts.  * [dev-resource](https://github.com/Ibrahim-Islam/dev-resource) â€“ Resources for devs online and offline.  * [digital-gardeners](https://github.com/MaggieAppleton/digital-gardeners) â€“ Resources for gardeners tending their digital notes on the public interwebs.  * [discord-resources](https://github.com/DTinker/discord-resources) â€“ Discord modding resources.  * [discount-for-student-dev](https://github.com/AchoArnold/discount-for-student-dev) â€“ Discounts on software (SaaS, PaaS, IaaS, etc.) and other offerings for developers who are students  * [dive-into-machine-learning](https://github.com/hangtwenty/dive-into-machine-learning) â€“ Dive into Machine Learning with Python Jupyter notebook and scikit-learn    - http://hangtwenty.github.io/dive-into-machine-learning/  * [django-must-watch](https://gitlab.com/rosarior/django-must-watch) â€“ Must-watch videos bout Django web framework + Python.  * [DL4NLP](https://github.com/andrewt3000/DL4NLP) â€“ Deep Learning for Natural Language Processing resources.  * [dumb-password-rules](https://github.com/dumb-password-rules/dumb-password-rules) â€“ Shaming sites with dumb password rules.  * [easy-application](https://github.com/j-delaney/easy-application) â€“ Software engineering companies that are easy to apply to.  * [effects-bibliography](https://github.com/yallop/effects-bibliography) â€“ A collaborative bibliography of work related to the theory and practice of computational effects  * [ElixirBooks](https://github.com/sger/ElixirBooks) â€“ Elixir programming language books  * [elm-companies](https://github.com/jah2488/elm-companies) â€“ Companies using Elm  * [embedded-scripting-languages](https://github.com/dbohdan/embedded-scripting-languages)  * [ember-links/list](https://github.com/ember-links/list) â€“ Ember.js web framework  * [empathy-in-engineering](https://github.com/KimberlyMunoz/empathy-in-engineering) â€“ Building and promoting more compassionate engineering cultures  * [engineering-blogs](https://github.com/kilimchoi/engineering-blogs)  * [engine.so](https://github.com/pmwkaa/engine.so) â€“ Tracking, Benchmarking and Sharing Information about an open source embedded data storage engines, internals, architectures, data storage and transaction processing.  * [erlang-bookmarks](https://github.com/0xAX/erlang-bookmarks) â€“ All about erlang programming language.  * [erlang-watchlist](https://github.com/gabrielelana/erlang-watchlist) â€“ Where to find good code to master Erlang idioms  * [ES6-Learning](https://github.com/ericdouglas/ES6-Learning) â€“ Resources to learn ECMAScript 6!  * [es6-tools](https://github.com/addyosmani/es6-tools) â€“ An aggregation of tooling for ES6  * [Essential-JavaScript-Links](https://github.com/starandtina/Essential-JavaScript-Links)    - http://starandtina.github.io/Essential-JavaScript-Links/  * [every-programmer-should-know](https://github.com/mtdvio/every-programmer-should-know) â€“ (Mostly) technical things every software developer should know.  * [Facets](https://github.com/O-I/Facets) â€“ One-liners in Ruby  * [fks](https://github.com/JacksonTian/fks) _In Chinese_ â€“ Frontend Knowledge Structure.  * [flat-file-cms](https://github.com/ahadb/flat-file-cms) â€“ Stictly flat-file cms systems.  * [FOSS-for-Dev](https://github.com/tvvocold/FOSS-for-Dev) â€“ Free and open-source software for developers  * [freeCodeCamp](https://github.com/freeCodeCamp/freeCodeCamp) â€“ Open Source, Free Full Stack Training with hours of coding challenges, projects, and certifications.    - https://www.freecodecamp.org/  * [free-for-dev](https://github.com/ripienaar/free-for-dev) â€“ Software, SaaS, PaaS etc offerings that have free tiers for devs.    - https://free-for.dev/  * [free-programming-books](https://github.com/EbookFoundation/free-programming-books)    - http://resrc.io/list/10/list-of-free-programming-books/  * [free-programming-books-zh_CN](https://github.com/justjavac/free-programming-books-zh_CN) _In Chinese_  * [frontdesk](https://github.com/miripiruni/frontdesk) â€“ Useful things for Front End Developers  * [Front-end-Developer-Interview-Questions](https://github.com/h5bp/Front-end-Developer-Interview-Questions) â€“ Helpful front-end related questions you can use to interview potential candidates, test yourself or completely ignore.    - Available in [various translations](https://github.com/darcyclarke/Front-end-Developer-Interview-Questions/tree/master/Translations)  * [Front-end-Web-Development-Interview-Question](https://github.com/paddingme/Front-end-Web-Development-Interview-Question) _In Chinese_  * [Front-End-Web-Development-Resources](https://github.com/RitikPatni/Front-End-Web-Development-Resources)    - https://resources.ritikpatni.me/  * [frontend-case-studies](https://github.com/andrew--r/frontend-case-studies) â€“ Technical talks and articles about real world enterprise frontend development.  * [frontend-challenges](https://github.com/felipefialho/frontend-challenges) â€“ Playful challenges for job applicants to test your knowledge.  * [frontend-dev-bookmarks](https://github.com/dypsilon/frontend-dev-bookmarks) â€“ Frontend development resources I collected over time.  * [frontend-dev-resources](https://github.com/dmytroyarmak/frontend-dev-resources) â€“ Frontend resources [conferences].  * [frontend-developer-resources](https://github.com/mrcodedev/frontend-developer-resources) _In Spanish._ â€“ El camino del Frontend Developer.  * [frontend-development](https://github.com/mojpm/frontend-development)  * [frontend-resources](https://github.com/JonathanZWhite/frontend-resources) by @JonathanZWhite  * [frontend-resources](https://github.com/zedix/frontend-resources) by @zedix  * [frontend-stuff](https://github.com/moklick/frontend-stuff) â€“ Framework/libraries/tools to use when building things on the web. Mostly Javascript stuff.  * [frontend-tools](https://github.com/codylindley/frontend-tools) â€“ Tools for frontend (i.e. html, js, css) desktop/laptop (i.e. does not include tablet or phone yet) web development  * [fsharp-companies](https://github.com/Kavignon/fsharp-companies) â€“ Companies that use F#  * [game-datasets](https://github.com/leomaurodesenv/game-datasets) â€“ Game datasets, tools for artificial intelligence in games  * [Game-Networking-Resources](https://github.com/MFatihMAR/Game-Networking-Resources) â€“ Game Network Programming  * [games](https://github.com/leereilly/games) â€“ Popular/awesome videos games, add-on, maps, etc. hosted on GitHub.  * [generated-awesomeness](https://github.com/orsinium-labs/generated-awesomeness) â€“ Awesome list autogenerated from GitHub API.  * [git-cheat-sheet](https://github.com/arslanbilal/git-cheat-sheet) â€“ git and git flow cheat sheet    - http://bilalarslan.me/git-cheat-sheet/  * [github-cheat-sheet](https://github.com/tiimgreen/github-cheat-sheet) â€“ Cool features of Git and GitHub.  * [github-drama](https://github.com/nikolas/github-drama)  * [github-hall-of-fame](https://github.com/mehulkar/github-hall-of-fame) â€“ Hall of Fame for spectacular things on Github.  * [GoBooks](https://github.com/dariubs/GoBooks) â€“ Golang books.  * [go-is-not-good](https://github.com/ksimka/go-is-not-good) â€“ Articles that complain about Golang's imperfection.  * [go-must-watch](https://github.com/sauravtom/go-must-watch) â€“ Must-watch videos about Golang.  * [go-patterns](https://github.com/tmrts/go-patterns) â€“ Go design patterns, recipes and idioms    - http://tmrts.com/go-patterns  * [graph-adversarial-learning-literature](https://github.com/YingtongDou/graph-adversarial-learning-literature) â€“ Adversarial learning papers on graph-structured data.  * [graphics-resources](https://github.com/mattdesl/graphics-resources) â€“ Game development and realtime graphics programming.  * [guides](https://github.com/NARKOZ/guides) by @NARKOZ â€“ Design and development guides  * [guides](https://github.com/taniarascia/guides) by @taniarascia â€“ Web Development Guides, Tutorials and Snippets.  * [Hackathon-Resources](https://github.com/xasos/Hackathon-Resources) by @xasos â€“ Hackathon Resources for organizers.  * [hack-chat/3rd-party-software-list](https://github.com/hack-chat/3rd-party-software-list) â€“ Bots, clients, and other software people have made for [hack.chat](https://hack.chat).  * [hacker-laws](https://github.com/dwmkerr/hacker-laws) â€“ Laws, Theories, Principles and Patterns that developers will find useful.  * [hacktoberfest-swag](https://github.com/benbarth/hacktoberfest-swag) â€“ Looking for [Hacktoberfest](https://hacktoberfest.digitalocean.com/) swag? You've come to the right place.  * [hacktoberfest-swag-list](https://github.com/crweiner/hacktoberfest-swag-list) â€“ Companies giving out swag for participation in [Hacktoberfest](https://hacktoberfest.digitalocean.com/).    - https://hacktoberfestswaglist.com  * [HarmonyOS](https://github.com/Awesome-HarmonyOS/HarmonyOS) â€“ [HarmonyOS](https://www.harmonyos.com/en/) by Huawei  * [haskell-companies](https://github.com/erkmos/haskell-companies) â€“ Companies using Haskel  * [haskell-must-watch](https://github.com/hzlmn/haskell-must-watch)  * [HeadlessBrowsers](https://github.com/dhamaniasad/HeadlessBrowsers)  * [hipchat-alternatives](https://github.com/cjbarber/hipchat-alternatives)  * [hiring-without-whiteboards](https://github.com/poteto/hiring-without-whiteboards) â€“ Companies that don't have a broken hiring process.  * [htaccess](https://github.com/phanan/htaccess) â€“ Useful .htaccess snippets.  * [hyperawesome](https://github.com/jorgebucaran/hyperawesome) â€“ Hyperapp JavaScript framework  * [idaplugins-list](https://github.com/onethawt/idaplugins-list) â€“ Plugins for [IDA disassembler](https://www.hex-rays.com/products/ida/).  * [ideas](https://github.com/samsquire/ideas) â€“ One Hundred Ideas for Computing  * [InfoSec-Black-Friday](https://github.com/0x90n/InfoSec-Black-Friday) â€“ Deals for InfoSec related software/tools this Black Friday  * [Inspire](https://github.com/NoahBuscher/Inspire) â€“ Links to assist you in web design and development  * [interviews](https://github.com/kdn251/interviews) â€“ Your personal guide to Software Engineering technical interviews.  * [InterviewThis](https://github.com/Twipped/InterviewThis) â€“ Developer questions to ask prospective employers  * [ios-awesome-libraries](https://github.com/kaiinui/ios-awesome-libraries) â€“ Useful iOS development libraries with usage examples.  * [iOS-Developer-and-Designer-Interview-Questions](https://github.com/9magnets/iOS-Developer-and-Designer-Interview-Questions)  * [iOSDevResource](https://github.com/objcc/iOSDevResource)  * [iptv](https://github.com/iptv-org/iptv) â€“ 5000+ publicly available IPTV channels from all over the world.  * [javacard-curated-list](https://github.com/EnigmaBridge/javacard-curated-list) â€“ Java Card applets and related applications for cryptographic smartcards.  * [javascript-dev-bookmarks](https://github.com/didicodes/javascript-dev-bookmarks) â€“ Articles that will help you get better at JavaScript.  * [javascript-patterns](https://github.com/shichuan/javascript-patterns) â€“ JavaScript Patterns    - http://shichuan.github.io/javascript-patterns/  * [javascript-resources](https://github.com/ztsu/javascript-resources)  * [javascript-sdk-design](https://github.com/hueitan/javascript-sdk-design)  * [jquery-tips-everyone-should-know](https://github.com/AllThingsSmitty/jquery-tips-everyone-should-know)  * [jsemu](https://github.com/fcambus/jsemu) â€“ Emulators written in JavaScript.  * [jshomes/learning-resources](https://github.com/jshomes/learning-resources) â€“ Web Platform/SaaS Learning Resources.  * [jslibs](https://github.com/esamattis/jslibs) â€“ My picks of promising/useful Javascript libraries.    - *See also [JSwiki](http://jswiki.org/)*  * [js-must-watch](https://github.com/bolshchikov/js-must-watch) â€“ Must-watch videos about javascript.  * [jsonauts](https://github.com/jsonauts/jsonauts.github.com) â€“ The ultimate reference for JSON tooling and specs.    - http://jsonauts.github.io/  * [jstips](https://github.com/loverajoel/jstips) â€“ JavaScript tips    - http://jstips.co  * [jstools](https://github.com/codefellows/jstools) â€“ Foundational JavaScript Tools  * [js-type-master](https://github.com/yumyo/js-type-master) â€“ JavaScript resources about web typography.    - https://www.codefellows.org/blog/a-list-of-foundational-javascript-tools  * [Julia.jl](https://github.com/svaksha/Julia.jl) â€“ Curated decibans of Julia language.    - https://github.com/svaksha/Julia.jl  * [killer-talks](https://github.com/PharkMillups/killer-talks) â€“ Talks that are worth watching.  * [kubernetes-failure-stories](https://github.com/hjacobs/kubernetes-failure-stories) â€“ Public failure/horror stories related to Kubernetes    - https://k8s.af  * [langs-in-rust](https://github.com/alilleybrinker/langs-in-rust) â€“ Programming languages implemented in Rust.  * [language-list](https://github.com/thomasfoster96/language-list) â€“ Programming languages being developed on GitHub.  * [Laravel-Resources](https://github.com/abhimanyu003/Laravel-Resources) â€“ Laravel Framework Resources and Blogs.  * [learn-drupal](https://github.com/rocketeerbkw/learn-drupal) â€“ Stuff to help you learn Drupal.  * [learn-for-free](https://github.com/aviaryan/learn-for-free) â€“ Free learning resources for all topics you can think of.  * [learnhaskell](https://github.com/bitemyapp/learnhaskell) â€“ A curated guide for learning Haskell.  * [learning-code-through-github-repos](https://github.com/muchirijane/learning-code-through-github-repos) â€“ Github repositories that you can use in your coding journey.  * [learn-python](https://github.com/adrianmoisey/learn-python) by @adrianmoisey â€“ Links that teach Python.  * [learn-python](https://github.com/trekhleb/learn-python) by @trekhleb â€“ Python scripts that are split by topics and contain code examples with explanations.  * [learn-to-program](https://github.com/karlhorky/learn-to-program) â€“ Foundation in Web Development.  * [learn-tt](https://github.com/jozefg/learn-tt) â€“ Resources for learning type theory.  * [learnxinyminutes-docs](https://github.com/adambard/learnxinyminutes-docs) â€“ Code documentation written as code!    - https://learnxinyminutes.com/  * [libertr](https://github.com/gaapt/libertr) â€“ Resources for liberty seekers.  * [lifeofjs](https://github.com/abhijeetkpawar/lifeofjs) â€“ Curated source for all types of awesome resources available for JavaScript.  * [Linux_websites](https://github.com/hduffddybz/Linux_websites) _In Chinese_ â€“ Websites related to Linux kernel development.  * [lua-languages](https://github.com/hengestone/lua-languages) â€“ Languages that compile to Lua.  * [machine-learning-algorithms](https://github.com/Sahith02/machine-learning-algorithms) â€“ Conceptual understanding of all machine learning algorithms.  * [Machine-Learning-Tutorials](https://github.com/ujjwalkarn/Machine-Learning-Tutorials) â€“ Machine Learning and Deep Learning Tutorials  * [machine-learning-with-ruby](https://github.com/arbox/machine-learning-with-ruby) â€“ Machine learning in Ruby  * [macos-apps](https://github.com/learn-anything/macos-apps)  * [magictools](https://github.com/ellisonleao/magictools) â€“ Game Development resources to make magic happen.  * [maintenance-modules](https://github.com/maxogden/maintenance-modules) â€“ NPM / Node.js modules useful for maintaining or developing modules  * [manong](https://github.com/nemoTyrant/manong) _In Chinese_ â€“ Weekly digest of technology  * [markdown-resources](https://github.com/rhythmus/markdown-resources) â€“ Markdown resources: apps, dialects, parsers, people, â€¦  * [Marketing-for-Engineers](https://github.com/goabstract/Marketing-for-Engineers) â€“ Marketing articles & tools to grow your product.  * [mind-bicycles](https://github.com/pel-daniel/mind-bicycles) â€“ Future of programming projects  * [motion-ui-design](https://github.com/fliptheweb/motion-ui-design) â€“ Motion UI design, animations and transitions.  * [movies-for-hackers](https://github.com/k4m4/movies-for-hackers)    - https://hackermovie.club/  * [must-watch-css](https://github.com/AllThingsSmitty/must-watch-css) â€“ Must-watch videos about CSS.  * [must-watch-javascript](https://github.com/AllThingsSmitty/must-watch-javascript) â€“ Must-watch videos about JavaScript.  * [my-arsenal-of-aws-security-tools](https://github.com/toniblyx/my-arsenal-of-aws-security-tools) â€“ Open source tools for AWS security: defensive, offensive, auditing, DFIR, etc.  * [my_tech_resources](https://github.com/JamesLavin/my_tech_resources) by @JamesLavin  * [nashville-lispers/resources](https://github.com/nashville-lispers/resources) â€“ Lisp Resources: exercises, great books, videos, etc.  * [net-libraries-that-make-your-life-easier](https://github.com/tallesl/net-libraries-that-make-your-life-easier) â€“ Open Source .NET libraries that make your life easier.  * [neural-network-papers](https://github.com/robertsdionne/neural-network-papers)  * [nginx-resources](https://github.com/fcambus/nginx-resources) â€“ Nginx web server (+ Lua), OpenResty and Tengine.  * [nlp_thai_resources](https://github.com/kobkrit/nlp_thai_resources) â€“ Natural Language Processing for Thai  * [nlp-with-ruby](https://github.com/arbox/nlp-with-ruby) â€“ Practical Natural Language Processing done in Ruby    - http://rubynlp.org  * [node-daily](https://github.com/dailyNode/node-daily) _In Chinese_ â€“ Daily article about Node.js.  * [node-frameworks](https://github.com/pillarjs/node-frameworks) â€“ Comparison of server-side Node frameworks.  * [nodejs-conference-cfps](https://github.com/rosskukulinski/nodejs-conference-cfps) â€“ NodeJS and Javascript Conference Call for Presentations.  * [NodeJS-Learning](https://github.com/sergtitov/NodeJS-Learning) â€“ Resources to help you learn Node.js and keep up to date.  * [NotesIndex](https://github.com/Wilbeibi/NotesIndex)  * [not-yet-awesome-rust](https://github.com/not-yet-awesome-rust/not-yet-awesome-rust) â€“ Rust code and resources that do NOT exist yet, but would be beneficial to the Rust community.  * [offline-first](https://github.com/pazguille/offline-first) â€“ Everything you need to know to create offline-first web apps.  * [open-source-android-apps](https://github.com/pcqpcq/open-source-android-apps) â€“ Collection of Android Apps which are open source.  * [open-source-ios-apps](https://github.com/dkhamsing/open-source-ios-apps) â€“ Open-source iOS apps.  * [open-source-mac-os-apps](https://github.com/serhii-londar/open-source-mac-os-apps) â€“ macOS open source applications.  * [open-source-meetup-alternatives](https://github.com/coderbyheart/open-source-meetup-alternatives)  * [opensource-discordbots](https://github.com/gillesheinesch/opensource-discordbots) â€“ Open-source bots for Discord.  * [ops-books](https://github.com/stack72/ops-books) â€“ Book recommendations related to Continuous Delivery, DevOps, Operations and Systems Thinking.  * [osx-and-ios-security-awesome](https://github.com/ashishb/osx-and-ios-security-awesome) â€“ OSX and iOS related security tools  * [papers](https://github.com/NicolasT/papers) â€“ A collection of papers found across the web.  * [papers-we-love](https://github.com/papers-we-love/papers-we-love) â€“ Papers from the computer science community to read and discuss. (Contains actual papers)  * [ParseAlternatives](https://github.com/relatedcode/ParseAlternatives) â€“ Alternative backend service providers ala [Parse](http://parse.com/).  * [pattern_classification](https://github.com/rasbt/pattern_classification) â€“ A collection of tutorials and examples for solving and understanding machine learning and pattern classification tasks.  * [PayloadsAllTheThings](https://github.com/swisskyrepo/PayloadsAllTheThings) â€“ Useful payloads and bypasses for Web Application Security and Pentest/CTF  * [personal-security-checklist](https://github.com/Lissy93/personal-security-checklist) â€“ 100+ tips for protecting digital security and privacy  * [php-must-watch](https://github.com/phptodayorg/php-must-watch) â€“ Must-watch videos about PHP.  * [phpvietnam/bookmarks](https://github.com/phpvietnam/bookmarks) â€“ PHP resources for Vietnamese.  * [PlacesToPostYourStartup](https://github.com/mmccaff/PlacesToPostYourStartup) â€“ â€œWhere can I post my startup to get beta users?â€  * [planetruby/calendar](https://github.com/planetruby/calendar) â€“ Ruby events (meetups, conferences, camps, etc.) from around the world.    - https://planetruby.github.io/calendar/  * [post-mortems](https://github.com/danluu/post-mortems)  * [programmers-proverbs](https://github.com/AntJanus/programmers-proverbs) â€“ Proverbs from the programmer  * [programming-talks](https://github.com/hellerve/programming-talks) â€“ Awesome & Interesting Talks concerning Programming  * [progressive-enhancement-resources](https://github.com/jbmoelker/progressive-enhancement-resources) â€“ (code) examples.  * [project-based-learning](https://github.com/tuvtran/project-based-learning) â€“ Programming tutorials to build an application from scratch.  * [Projects](https://github.com/karan/Projects) â€“ Practical projects that anyone can solve in any programming language.  * [public-apis](https://github.com/public-apis/public-apis) â€“ JSON APIs for use in web development.  * [purescript-companies](https://github.com/ajnsit/purescript-companies) â€“ Companies that use Purescript  * [pycrumbs](https://github.com/kirang89/pycrumbs) â€“ Bits and Bytes of Python from the Internet.  * [py-must-watch](https://github.com/s16h/py-must-watch) by @s16h â€“ Must-watch videos about Python.  * [python-github-projects](https://github.com/checkcheckzz/python-github-projects) â€“ Collect and classify python projects on Github.    - http://itgeekworkhard.com/python-github-projects/  * [pythonidae](https://github.com/svaksha/pythonidae) â€“ Curated decibans of Python scientific programming resources.    - http://svaksha.github.io/pythonidae/  * [python-must-watch](https://github.com/primalpop/python-must-watch) by @primalpop â€“ Must-watch videos about Python.  * [python_reference](https://github.com/rasbt/python_reference) â€“ Useful functions, tutorials, and other Python-related things.  * [Qix](https://github.com/ty4z2008/Qix) _In Chinese_ â€“ Node, Golang, Machine Learning, PostgreSQL.  * [queues.io](https://github.com/lukaszx0/queues.io) â€“ Job queues, message queues and other queues.    - http://queues.io/  * [quick-look-plugins](https://github.com/sindresorhus/quick-look-plugins) â€“ macOS Quick Look plugins for developers  * [rails-must-watch](https://github.com/gerricchaplin/rails-must-watch) â€“ Must-watch videos about Ruby on Rails.  * [rbooks](https://github.com/RomanTsegelskyi/rbooks) â€“ R programming language books  * [remote-in-japan](https://github.com/remote-jp/remote-in-japan) â€“ Tech companies in Japan that hire remote workers.  * [remote-jobs](https://github.com/remoteintech/remote-jobs) â€“ Semi to fully remote-friendly companies in tech.  * [remote-jobs-brazil](https://github.com/lerrua/remote-jobs-brazil) â€“ Remote-friendly Brazilian companies.  * [remote-software-companies](https://github.com/RemoteByDefault/remote-software-companies) â€“ Remote companies with information about tech stack and salary.  * [resource-list](https://github.com/kyasui/resource-list) â€“ Design & Development Resources.  * [resources](https://github.com/jbranchaud/resources) by @jbranchaud â€“ Free, online resources for various technologies, languages, and tools.  * [Resources](https://github.com/tevko/Resources) by @tevko â€“ Tools for front end devs.  * [Resources-for-Beginner-Bug-Bounty-Hunters](https://github.com/nahamsec/Resources-for-Beginner-Bug-Bounty-Hunters) â€“ Getting started with bug bounties.  * [Resources-for-Writing-Shaders-in-Unity](https://github.com/VoxelBoy/Resources-for-Writing-Shaders-in-Unity)  * [retter](https://github.com/MaciejCzyzewski/retter) â€“ Hash functions, ciphers, tools, libraries, and materials related to cryptography & security.  * [reverse-interview](https://github.com/viraptor/reverse-interview) â€“ Questions to ask the company during your interview  * [Rich-Hickey-fanclub](https://github.com/tallesl/Rich-Hickey-fanclub) â€“ Rich Hickey's works on the internet.  * [rss-readers-list](https://github.com/smithbr/rss-readers-list) â€“ Reader replacements megalist    - http://smithbr.github.io/rss-readers-list  * [rubybib.org](https://github.com/rubybib/rubybib.org) â€“ The Ruby Bibliography    - http://rubybib.org/  * [ruby-bookmarks](https://github.com/dreikanter/ruby-bookmarks) â€“ Ruby and Ruby on Rails bookmarks collection.  * [ruby-dev-bookmarks](https://github.com/saberma/ruby-dev-bookmarks) â€“ Ruby development resources I've collected.  * [ruby-nlp](https://github.com/diasks2/ruby-nlp) â€“ Ruby Natural Language Processing (NLP) libraries, tools and software.  * [rust-lang-resources](https://github.com/dschenkelman/rust-lang-resources) â€“ Links related to the Rust programming language.  * [rxjs-ecosystem](https://github.com/Widdershin/rxjs-ecosystem) â€“ What are the most popular libraries in the RxJS ecosystem?  * [rx-react-flux](https://github.com/christianramsey/rx-react-flux) â€“ RxJS + React/Flux implementations.  * [scalable-css-reading-list](https://github.com/davidtheclark/scalable-css-reading-list) â€“ Collected dispatches from The Quest for Scalable CSS.  * [search-engine-optimization](https://github.com/marcobiedermann/search-engine-optimization) â€“ Checklist / collection of Search Engine Optimization (SEO) tips and technics.  * [SecLists](https://github.com/danielmiessler/SecLists) â€“ Lists used during security assessments: usernames, passwords, URLs, sensitive data patterns, fuzzing payloads, web shells, etc.  * [secure-email](https://github.com/OpenTechFund/secure-email) â€“ Overview of projects working on next-generation secure email.  * [Security_list](https://github.com/zbetcheckin/Security_list)  * [selfhosted-music-overview](https://github.com/basings/selfhosted-music-overview) â€“ Software network services which can be hosted on your own servers.  * [services-engineering](https://github.com/mmcgrana/services-engineering) â€“ A reading list for services engineering, with a focus on cloud infrastructure services.  * [shareable-links](https://github.com/vinkla/shareable-links) â€“ URLs for sharing on social media.  * [shellshocker-pocs](https://github.com/mubix/shellshocker-pocs) â€“ Proof of concepts and potential targets for Shellshock.  * [slack-groups](https://github.com/learn-anything/slack-groups) â€“ Public Slack communities.  * [spark-joy](https://github.com/sw-yx/spark-joy) â€“ Add design flair, user delight, and whimsy to your product.  * [spawnedshelter](https://github.com/unbalancedparentheses/spawnedshelter) â€“ Erlang Spawned Shelter â€“ the best articles, videos and presentations related to Erlang.  * [speech-language-processing](https://github.com/edobashira/speech-language-processing)  * [stack-on-a-budget](https://github.com/255kb/stack-on-a-budget) â€“ Services with great free tiers for developers on a budget  * [startup-must-watch](https://github.com/gerricchaplin/startup-must-watch) â€“ Must-watch videos devoted to Entrepreneurship and Startups.  * [startupreadings](https://github.com/dennybritz/startupreadings) â€“ Reading list for all things startup-related.  * [startup-resources](https://github.com/JonathanZWhite/startup-resources)  * [state-machines](https://github.com/achou11/state-machines)  * [static-analysis](https://github.com/analysis-tools-dev/static-analysis) â€“ Static analysis tools, linters and code quality checkers  * [Static-Site-Generators](https://github.com/pinceladasdaweb/Static-Site-Generators)  * [staticsitegenerators-list](https://github.com/bevry/staticsitegenerators-list)    - https://staticsitegenerators.net/  * [streaming-papers](https://github.com/sorenmacbeth/streaming-papers) â€“ Papers on streaming algorithms.  * [structured-text-tools](https://github.com/dbohdan/structured-text-tools) â€“ Command line tools for manipulating structured text data  * [styleguide-generators](https://github.com/davidhund/styleguide-generators) â€“ Automatic living styleguide generators.  * [sublime](https://github.com/JaredCubilla/sublime) â€“ Some of the best Sublime Text packages, themes, and goodies.  * [sublime-bookmarks](https://github.com/dreikanter/sublime-bookmarks) â€“ Sublime Text essential plugins and resources.  * [svelte/integrations](https://github.com/sveltejs/integrations) â€“ Ways to incorporate [Svelte](https://svelte.dev/) framework into your stack  * [SwiftInFlux](https://github.com/ksm/SwiftInFlux) â€“ An attempt to gather all that is in flux in Swift.  * [tech-weekly](https://github.com/adrianmoisey/tech-weekly) â€“ Weekly technical newsletters.  * [terminals-are-sexy](https://github.com/k4m4/terminals-are-sexy) â€“ Terminal frameworks, plugins & resources for CLI lovers.    - https://terminalsare.sexy/  * [the-book-of-secret-knowledge](https://github.com/trimstray/the-book-of-secret-knowledge) â€“ Inspiring lists, manuals, cheatsheets, blogs, hacks, one-liners, cli/web tools and more.  * [The-Documentation-Compendium](https://github.com/kylelobo/The-Documentation-Compendium) â€“ Templates & tips on writing high-quality documentation  * [think-awesome](https://github.com/thinkjs/think-awesome) â€“ [ThinkJS](https://thinkjs.org/) Node.js framework  * [til](https://github.com/jbranchaud/til) â€“ Today I Learned.  * [tips](https://github.com/git-tips/tips) â€“ Most commonly used git tips and tricks.    - http://git.io/git-tips  * [Toolbox](https://github.com/Dillion/Toolbox) â€“ Open source iOS stuff.  * [tool_lists](https://github.com/johnyf/tool_lists) â€“ Links to tools by theme. *Verification, synthesis, and static analysis.*  * [tools](https://github.com/lvwzhen/tools) â€“ Tools for web.  * [toolsforactivism](https://github.com/drewrwilson/toolsforactivism) â€“ Digital tools for activism  * [tools-list](https://github.com/everestpipkin/tools-list) â€“ Open source, experimental, and tiny tools for building game/website/interactive project.    - https://tinytools.directory/  * [ToolsOfTheTrade](https://github.com/cjbarber/ToolsOfTheTrade) â€“ Tools of The Trade, from Hacker News.  * [top-starred-devs-and-repos-to-follow](https://github.com/StijnMiroslav/top-starred-devs-and-repos-to-follow) â€“ Top-Starred Python GitHub Devs, Orgs, and Repos to Follow (All-Time and Trending).  * [trending-repositories](https://github.com/Semigradsky/trending-repositories) â€“ Repositories that were trending for a day.  * [trip-to-iOS](https://github.com/Aufree/trip-to-iOS) _In Chinese_ â€“ Delightful iOS resources.  * [twofactorauth](https://github.com/2factorauth/twofactorauth) â€“ Sites with two factor auth support which includes SMS, email, phone calls, hardware, and software.    - https://twofactorauth.org/  * [type-findings](https://github.com/charliewilco/type-findings) â€“ Posts about web typography.  * [typography](https://github.com/deanhume/typography) â€“ Web typography    - https://deanhume.github.io/typography/  * [ui-styleguides](https://github.com/kevinwuhoo/ui-styleguides)    - http://kevinformatics.com/ui-styleguides/  * [universities-on-github](https://github.com/filler/universities-on-github) â€“ Universities which have a public organization on GitHub.  * [upcoming-conferences](https://github.com/svenanders/upcoming-conferences) â€“ Upcoming web developer conferences.  * [vertx-awesome](https://github.com/vert-x3/vertx-awesome) â€“ [Vert.x](http://vertx.io/) toolkit  * [vim-galore](https://github.com/mhinz/vim-galore) â€“ All things Vim!  * [visual-programming-codex](https://github.com/ivanreese/visual-programming-codex) â€“ Resources and references for the past and future of visual programming.  * [we-are-twtxt](https://github.com/mdom/we-are-twtxt) â€“ [twtxt](https://twtxt.readthedocs.io/) users and bots  * [web-audio-resources](https://github.com/alemangui/web-audio-resources) â€“ A list of curated resources related to the Web audio API.  * [WebComponents-Polymer-Resources](https://github.com/matthiasn/WebComponents-Polymer-Resources)  * [webcomponents-the-right-way](https://github.com/mateusortiz/webcomponents-the-right-way) â€“ Introduction to Web Components.  * [web-dev-resources](https://github.com/ericandrewlewis/web-dev-resources) â€“ A table of contents for web developer resources across the internet.  * [web-development-resources](https://github.com/MasonONeal/web-development-resources)  * [webdev-jokes](https://github.com/jerstew/webdev-jokes) â€“ Web development jokes.  * [webdevresourcecuration](https://github.com/lwakefield/webdevresourcecuration)  * [weekly](https://github.com/zenany/weekly) _In Chinese_ â€“ Weekly summary of articles and resources.  * [what-next](https://github.com/messa/what-next) _In Czech_ â€“ Co dÄ›lat, kdyÅ¾ se chci nauÄit programovat jeÅ¡tÄ› vÃ­c.  * [Women-Made-It](https://github.com/LisaDziuba/Women-Made-It) â€“ Design & development tools, books, podcasts, and blogs made by women.  * [Worth-Reading-the-Android-technical-articles](https://github.com/zmywly8866/Worth-Reading-the-Android-technical-articles) _In Chinese_  * [You-Dont-Need](https://github.com/you-dont-need/You-Dont-Need) â€“ People choose popular projects, often not because it applies to their problems.      ### awesome-*    * [awesome-2048-and-beyond](https://github.com/cstrap/awesome-2048-and-beyond) â€“ Waste and lose at least 8 hours of your lifeâ€¦ then **multiply** itâ€¦  * [awesome4girls](https://github.com/cristianoliveira/awesome4girls) â€“ Inclusive events/projects/initiatives for women in the tech area.  * [awesome-a11y](https://github.com/brunopulis/awesome-a11y) â€“ Accesibility tools, articles and resources.  * [awesome-accessibility](https://github.com/GonzagaAccess/awesome-accessibility) â€“ Utilities for accessibility-based web development  * [awesome-acf](https://github.com/navidkashani/awesome-acf) â€“ Add-ons for the Advanced Custom Field plugin for WordPress.  * [awesome-actions](https://github.com/sdras/awesome-actions) â€“ [GitHub Actions](https://github.com/features/actions)  * [awesome-actionscript3](https://github.com/robinrodricks/awesome-actionscript3) â€“ ActionScript 3 and Adobe AIR.  * [awesome-activeadmin](https://github.com/serradura/awesome-activeadmin) â€“ Active Admin resources, extensions, posts and utilities. *For Rails.*  * [awesome-activitypub](https://github.com/BasixKOR/awesome-activitypub) â€“ ActivityPub based projects  * [awesome-ad-free](https://github.com/johnjago/awesome-ad-free) â€“ Ad-free alternatives to popular services on the web  * [awesome-ada](https://github.com/ohenley/awesome-ada) â€“ Ada and SPARK programming language  * [awesome-adafruitio](https://github.com/adafruit/awesome-adafruitio) â€“ [Adafruit IO](https://io.adafruit.com/) Internet of Things platform  * [awesome-advent-of-code](https://github.com/Bogdanp/awesome-advent-of-code) â€“ [Advent of Code](https://adventofcode.com/)  * [awesome-agile](https://github.com/lorabv/awesome-agile) â€“ Agile Software Development.    - https://lorabv.github.io/awesome-agile  * [awesome-agriculture](https://github.com/brycejohnston/awesome-agriculture) â€“ Open source technology for agriculture, farming, and gardening  * [awesome-alfred-workflows](https://github.com/alfred-workflows/awesome-alfred-workflows) â€“ [Alfred](https://www.alfredapp.com/) macOS app workflows  * [awesome-algolia](https://github.com/algolia/awesome-algolia) â€“ [Algolia](https://www.algolia.com/) web search service  * [awesome-algorithms](https://github.com/tayllan/awesome-algorithms) â€“ Places to learn and/or practice algorithms.  * [awesome-algorithms-education](https://github.com/gaerae/awesome-algorithms-education) â€“ Learning and practicing algorithms    - https://gaerae.com/awesome-algorithms  * [awesome-alternatives](https://gitlab.com/linuxcafefederation/awesome-alternatives) â€“ Mostly free and open source alternatives to proprietary software and services.  * [awesome-ama-answers](https://github.com/stoeffel/awesome-ama-answers) â€“ @stoeffel's AMA answers  * [awesome-amazon-alexa](https://github.com/miguelmota/awesome-amazon-alexa) â€“ Resources for the Amazon Alexa platform.  * [awesome-amazon-seller](https://github.com/ScaleLeap/awesome-amazon-seller) â€“ Tools and resources for Amazon sellers.  * [awesome-analytics](https://github.com/onurakpolat/awesome-analytics) â€“ Analytics services, frameworks, software and other tools.  * [awesome-android](https://github.com/Jackgris/awesome-android) _In Spanish._ by @Jackgris  * [awesome-android](https://github.com/JStumpp/awesome-android) by @JStumpp  * [awesome-android](https://github.com/snowdream/awesome-android) _Partially in Chinese_ by @snowdream  * [awesome-android-awesomeness](https://github.com/yongjhih/awesome-android-awesomeness)  * [awesome-android-kotlin-apps](https://github.com/androiddevnotes/awesome-android-kotlin-apps) â€“ Open-source Android apps written in Kotlin with particular tech stack and libraries.  * [awesome-android-learner](https://github.com/MakinGiants/awesome-android-learner) â€“ A â€œstudy guideâ€ for mobile development.  * [awesome-android-learning-resources](https://github.com/androiddevnotes/awesome-android-learning-resources)  * [awesome-android-libraries](https://github.com/wasabeef/awesome-android-libraries) â€“ General Android libraries.  * [awesome-android-performance](https://github.com/Juude/awesome-android-performance) â€“ Performance optimization on Android.  * [awesome-android-release-notes](https://github.com/pedronveloso/awesome-android-release-notes) â€“ Keep up-to-date with all the things related with Android software development.  * [awesome-android-tips](https://github.com/jiang111/awesome-android-tips) _In Chinese_  * [awesome-android-ui](https://github.com/wasabeef/awesome-android-ui) â€“ UI/UX libraries for Android.  * [awesome-androidstudio-plugins](https://github.com/jiang111/awesome-androidstudio-plugins) _In Chinese_  * [awesome-angular](https://github.com/hugoleodev/awesome-angular) by @hugoleodev  * [awesome-angular](https://github.com/PatrickJS/awesome-angular) by @PatrickJS  * [awesome-angularjs](https://github.com/gianarb/awesome-angularjs) by @gianarb  * [awesome-animation](https://github.com/Animatious/awesome-animation) â€“ Open-source UI animations by Animatious Group.  * [awesome-ansible](https://github.com/jdauphant/awesome-ansible) â€“ [Ansible](https://www.ansible.com/) configuration management  * [awesome-answers](https://github.com/cyberglot/awesome-answers) â€“ Inspiring and thoughtful answers given at stackoverflow, quora, etc.  * [awesome-ant-design](https://github.com/websemantics/awesome-ant-design) â€“ [Ant Design](https://ant.design/) system  * [awesome-api](https://github.com/Kikobeats/awesome-api) â€“ Design and implement RESTful API's  * [awesome-app-ideas](https://github.com/tastejs/awesome-app-ideas) â€“ Ideas for apps to demonstrate how framework or library approach specific problems.  * [awesome-appium](https://github.com/SrinivasanTarget/awesome-appium) â€“ [Appium](http://appium.io/) test automation frmework  * [awesome-apple](https://github.com/joeljfischer/awesome-apple) â€“ 3rd party libraries and tools for Apple platforms development.  * [awesome-appsec](https://github.com/paragonie/awesome-appsec) â€“ Resources for developers to learn application security.  * [awesome-arabic](https://github.com/OthmanAba/awesome-arabic) â€“ Arabic supporting tools, fonts, and development resources.  * [Awesome-arduino](https://github.com/Lembed/Awesome-arduino) â€“ Arduino hardwares, libraries and softwares with update script  * [awesome-arm-exploitation](https://github.com/HenryHoggard/awesome-arm-exploitation) â€“ ARM processors security and exploitation.  * [awesome-artificial-intelligence](https://github.com/owainlewis/awesome-artificial-intelligence)  * [awesome-asciidoc](https://github.com/bodiam/awesome-asciidoc) â€“ Collection of AsciiDoc tools, guides, tutorials and examples of usage.  * [awesome-asciidoctor](https://github.com/dongwq/awesome-asciidoctor) â€“ Collection of asciidoctorâ€™s intros, examples and usages.  * [awesome-ast](https://github.com/chadbrewbaker/awesome-ast) by @chadbrewbaker â€“ Tools for Abstract Syntax Tree processing.  * [awesome-ast](https://github.com/cowchimp/awesome-ast) by @cowchimp â€“ Abstract Syntax Trees.  * [awesome-asyncio](https://github.com/timofurrer/awesome-asyncio) â€“ [asyncio](https://docs.python.org/3/library/asyncio.html) Python library  * [awesome-asyncio-cn](https://github.com/chenjiandongx/awesome-asyncio-cn) _In Chinese_ â€“ [asyncio](https://docs.python.org/3/library/asyncio.html) Python library    - https://awesome-asyncio-cn.chenjiandongx.com/  * [awesome-atom](https://github.com/mehcode/awesome-atom) â€“ [Atom](https://atom.io/) text editor  * [awesome-audio-visualization](https://github.com/willianjusten/awesome-audio-visualization)  * [awesome-aurelia](https://github.com/aurelia-contrib/awesome-aurelia) â€“ [Aurelia](https://aurelia.io/) JavaScript framework  * [awesome-authentication](https://github.com/gitcommitshow/awesome-authentication)  * [awesome-AutoHotkey](https://github.com/ahkscript/awesome-AutoHotkey) â€“ AutoHotkey libraries, library distributions, scripts, tools and resources.  * [awesome-AutoIt](https://github.com/J2TeaM/awesome-AutoIt) â€“ UDFs, example scripts, tools and useful resources for AutoIt.    - https://j2team.github.io/awesome-AutoIt/  * [awesome-automotive](https://github.com/Marcin214/awesome-automotive) â€“ Automotive engineering.  * [awesome-ava](https://github.com/avajs/awesome-ava) â€“ [AVA](https://github.com/avajs/ava) JavaScript test runner.  * [awesome-avr](https://github.com/fffaraz/awesome-avr)  * [awesome-aws](https://github.com/donnemartin/awesome-aws) â€“ Amazon Web Services (AWS)  * [awesome-backbone](https://github.com/sadcitizen/awesome-backbone) â€“ Resources for [Backbone.js](http://backbonejs.org/)  * [awesome-bash](https://github.com/awesome-lists/awesome-bash)  * [awesome-bci](https://github.com/NeuroTechX/awesome-bci) â€“ Brain-Computer Interface.  * [awesome-beacon](https://github.com/rabschi/awesome-beacon) â€“ Bluetooth beacon (iBeacon, Eddystone)  * [awesome-beancount](https://github.com/wzyboy/awesome-beancount) â€“ [Beancount](http://furius.ca/beancount/), a double-entry bookkeeping with text files.  * [awesome-bem](https://github.com/getbem/awesome-bem) â€“ Tools, sites, articles about BEM (frontend development method).  * [awesome-big-o](https://github.com/okulbilisim/awesome-big-o) â€“ Big O notation  * [awesome-bigdata](https://github.com/onurakpolat/awesome-bigdata) â€“ Big data frameworks, resources and other awesomeness.  * [Awesome-Bioinformatics](https://github.com/danielecook/Awesome-Bioinformatics) â€“ Open-source bioinformatics software and libraries.  * [awesome-bitcoin](https://github.com/igorbarinov/awesome-bitcoin) â€“ Bitcoin services and tools for software developers.  * [awesome-bitcoin-payment-processors](https://github.com/alexk111/awesome-bitcoin-payment-processors) â€“ Bitcoin payment processors and stories from merchants using them.  * [awesome-blazor](https://github.com/AdrienTorris/awesome-blazor) â€“ [Blazor](https://blazor.net/), a .NET web framework using C#/Razor and HTML that runs in the browser with WebAssembly.  * [awesome-blender](https://github.com/agmmnn/awesome-blender) â€“ [Blender](https://www.blender.org/) add-ons, tools, tutorials and 3D resources.  * [awesome-blockchain](https://github.com/0xtokens/awesome-blockchain) by @0xtokens â€“ Blockchain and Crytocurrency Resources  * [awesome-blockchain](https://github.com/coderplex-org/awesome-blockchain) by @coderplex-org â€“ Blockchain, Bitcoin and Ethereum related resources  * [awesome-blockchain](https://github.com/cyberFund/awesome-blockchain) _In Russian_ by @cyberFund â€“ Digest of knowledge about crypto networks (including cryptocurrencies).  * [awesome-blockchain](https://github.com/hitripod/awesome-blockchain) by @hitripod  * [awesome-blockchain](https://github.com/igorbarinov/awesome-blockchain) by @igorbarinov â€“ The bitcoin blockchain services  * [awesome-blockchain](https://github.com/imbaniac/awesome-blockchain) by @imbaniac â€“ Blockchain services and exchanges  * [awesome-blockchain](https://github.com/iNiKe/awesome-blockchain) by @iNiKe â€“ Blockchain, ICO, â‚¿itcoin, Cryptocurrencies  * [awesome-blockchain](https://github.com/oiwn/awesome-blockchain) by @oiwn â€“ Projects and services based on blockchain technology  * [awesome-blockchain-ai](https://github.com/steven2358/awesome-blockchain-ai) â€“ Blockchain projects for Artificial Intelligence and Machine Learning  * [awesome-blockchains](https://github.com/openblockchains/awesome-blockchains) â€“ Blockchains - open distributed databases w/ crypto hashes incl. git  * [awesome-blockstack](https://github.com/jackzampolin/awesome-blockstack) â€“ [Blockstack](https://blockstack.org/) decentralized computing platform  * [awesome-book-authoring](https://github.com/TalAter/awesome-book-authoring) â€“ Resources for technical book authors  * [awesome-bootstrap](https://github.com/therebelrobot/awesome-bootstrap) â€“ Free Bootstrap themes I think are cool.  * [awesome-bpm](https://github.com/ungerts/awesome-bpm) â€“ Business Process Management (BPM) awesomeness.  * [awesome-broadcasting](https://github.com/ebu/awesome-broadcasting) â€“ Open source resources related to broadcast technologies    - http://ebu.io/opensource  * [awesome-browser-extensions-for-github](https://github.com/stefanbuck/awesome-browser-extensions-for-github) â€“ Browser extensions for GitHub.  * [awesome-browserify](https://github.com/browserify/awesome-browserify) â€“ [Browserify](http://browserify.org/) bundler  * [awesome-btcdev](https://github.com/btcbrdev/awesome-btcdev) â€“ Bitcoin development  * [awesome-bugs](https://github.com/criswell/awesome-bugs) â€“ Funny and interesting bugs  * [awesome-building-blocks-for-web-apps](https://github.com/componently-com/awesome-building-blocks-for-web-apps) â€“ Standalone features (services, components, libraries) to be integrated into web applications.    - https://www.componently.com/  * [awesome-c](https://github.com/aleksandar-todorovic/awesome-c) by @aleksandar-todorovic â€“ Continuing the development of awesome-c on GitHub  * [awesome-c](https://github.com/kozross/awesome-c) by @kozross â€“ C frameworks, libraries, resources etc.    - [mirror](https://notabug.org/koz.ross/awesome-c)  * [awesome-cakephp](https://github.com/FriendsOfCake/awesome-cakephp) â€“ [CakePHP](https://cakephp.org/) web framework  * [awesome-calculators](https://github.com/xxczaki/awesome-calculators)  * [awesome-canvas](https://github.com/raphamorim/awesome-canvas) â€“ HTML5 Canvas  * [awesome-captcha](https://github.com/ZYSzys/awesome-captcha) â€“ Captcha libraries and crack tools.    - http://zyszys.github.io/awesome-captcha/  * [awesome-cassandra](https://github.com/yikebocai/awesome-cassandra)  * [awesome-ccxt](https://github.com/suenot/awesome-ccxt) â€“ [CryptoCurrency eXchange Trading Library](https://github.com/ccxt/ccxt)  * [awesome_challenge_list](https://github.com/AwesomeRubyist/awesome_challenge_list) â€“ Sites with challenges to improve your programming skills.  * [awesome-challenges](https://github.com/mauriciovieira/awesome-challenges) â€“ Algorithmic challenges  * [awesome-charting](https://github.com/zingchart/awesome-charting) â€“ Charts and dataviz.  * [awesome-chatops](https://github.com/exAspArk/awesome-chatops) â€“ ChatOps â€“ managing operations through a chat  * [awesome-chef](https://github.com/obazoud/awesome-chef) â€“ Cookbooks, handlers, add-ons and other resources for Chef, a configuration management tool.  * [awesome-cheminformatics](https://github.com/hsiaoyi0504/awesome-cheminformatics) â€“ Chemical informatics  * [awesome-chess](https://github.com/hkirat/awesome-chess) â€“ Chess software, libraries, and resources  * [awesome-choo](https://github.com/choojs/awesome-choo) â€“ [choo](https://choo.io/) web framework  * [awesome-chrome-devtools](https://github.com/ChromeDevTools/awesome-chrome-devtools) â€“ Chrome DevTools ecosystem tooling and resources.  * [awesome-ci](https://github.com/ligurio/awesome-ci) by @ligurio â€“ Comparison of cloud based CI services.  * [awesome-ci](https://github.com/pditommaso/awesome-ci) by @pditommaso â€“ Continuous integation services.  * [awesome-ciandcd](https://github.com/cicdops/awesome-ciandcd) â€“ Continuous Integration and Continuous Delivery    - http://www.ciandcd.com/  * [awesome-circuitpython](https://github.com/adafruit/awesome-circuitpython) â€“ [CircuitPython](https://circuitpython.org/) microcontrollers programming language  * [awesome-cl](https://github.com/CodyReichert/awesome-cl) â€“ Common Lisp  * [awesome-cl-software](https://github.com/azzamsa/awesome-cl-software) â€“ Applications built with Common Lisp  * [awesome-cli-apps](https://github.com/agarrharr/awesome-cli-apps) â€“ Command line apps  * [awesome-clojure](https://github.com/mbuczko/awesome-clojure) by @mbuczko â€“ Useful links for clojurians  * [awesome-clojure](https://github.com/razum2um/awesome-clojure) by @razum2um  * [awesome-clojurescript](https://github.com/hantuzun/awesome-clojurescript)  * [awesome-cloud](https://github.com/JStumpp/awesome-cloud) â€“ Delightful cloud services.  * [awesome-cloud-certifications](https://gitlab.com/edzob/awesome-cloud-certifications) â€“ Certifications for cloud platforms  * [awesome-cloudflare](https://github.com/irazasyed/awesome-cloudflare) â€“ [Cloudflare](https://www.cloudflare.com/) tools and recipes.  * [awesome-cmake](https://github.com/onqtam/awesome-cmake) â€“ CMake  * [awesome-cms](https://github.com/postlight/awesome-cms) â€“ Open and closed source Content Management Systems (CMS)  * [Awesome-CobaltStrike-Defence](https://github.com/MichaelKoczwara/Awesome-CobaltStrike-Defence) â€“ Defences against [Cobalt Strike](https://www.cobaltstrike.com/), Adversary Simulations and Red Team Operations software.  * [awesome-cobol](https://github.com/mickaelandrieu/awesome-cobol) â€“ COBOL programming language  * [awesome-cocoa](https://github.com/v-braun/awesome-cocoa) â€“ Cocoa controls for iOS, watchOS and macOS    - http://cocoa.rocks  * [awesome-code-formatters](https://github.com/rishirdua/awesome-code-formatters)  * [awesome-code-review](https://github.com/joho/awesome-code-review)  * [awesome-codepoints](https://github.com/Codepoints/awesome-codepoints) â€“ Interesting Unicode characters  * [awesome-coins](https://github.com/Zheaoli/awesome-coins) â€“ Guide to cryto-currencies and their algos.  * [awesome-cold-showers](https://github.com/hwayne/awesome-cold-showers) â€“ For when people get too hyped up about things.  * [awesome-coldfusion](https://github.com/seancoyne/awesome-coldfusion)  * [awesome-common-lisp-learning](https://github.com/GustavBertram/awesome-common-lisp-learning)  * [awesome-community](https://github.com/phpearth/awesome-community) â€“ development, support and discussion channels, groups and communities.  * [awesome-community-building](https://github.com/CrowdDevHQ/awesome-community-building) â€“ Building developer communities.  * [awesome-community-detection](https://github.com/benedekrozemberczki/awesome-community-detection) â€“ Community detection papers with implementations.  * [awesome-comparisons](https://github.com/dhamaniasad/awesome-comparisons) â€“ Framework and code comparison projects, like TodoMVC and Notejam.  * [awesome-competitive-programming](https://github.com/lnishan/awesome-competitive-programming) â€“ Competitive Programming, Algorithm and Data Structure resources    - http://codeforces.com/blog/entry/23054  * [awesome-composer](https://github.com/jakoch/awesome-composer) â€“ Composer, Packagist, Satis PHP ecosystem  * [awesome-computational-neuroscience](https://github.com/eselkin/awesome-computational-neuroscience) â€“ Schools and researchers in computational neuroscience  * [awesome-computer-history](https://github.com/watson/awesome-computer-history) â€“ Computer history videos, documentaries and related folklore.  * [awesome-computer-vision](https://github.com/AGV-IIT-KGP/awesome-computer-vision) by @AGV-IIT-KGP  * [awesome-computer-vision](https://github.com/jbhuang0604/awesome-computer-vision) by @jbhuang0604  * [awesome-computer-vision-models](https://github.com/nerox8664/awesome-computer-vision-models) â€“ Popular deep learning models related to classification and segmentation task  * [awesome-conference-playlists](https://github.com/chentsulin/awesome-conference-playlists) â€“ Video playlists for conferences.  * [awesome-conferences](https://github.com/RichardLitt/awesome-conferences)  * [awesome-connectivity-info](https://github.com/stevesong/awesome-connectivity-info) â€“ Connectivity indexes and reports to help you better under who has access to communication infrastructure and on what terms.  * [awesome-conservation-tech](https://github.com/anselmbradford/awesome-conservation-tech) â€“ Intersection of tech and environmental conservation.  * [awesome-console-services](https://github.com/chubin/awesome-console-services) â€“ Console services (reachable via HTTP, HTTPS and other network protocols).  * [awesome-construct](https://github.com/WebCreationClub/awesome-construct) â€“ [Construct](https://www.construct.net/) game development toolkit  * [awesome-container](https://github.com/tcnksm/awesome-container) â€“ Container technologies and services.  * [awesome-conversational](https://github.com/mortenjust/awesome-conversational) â€“ Conversational UI  * [awesome-cordova](https://github.com/busterc/awesome-cordova) _Apache Cordova / PhoneGap_  * [Awesome-CoreML-Models](https://github.com/likedan/Awesome-CoreML-Models) â€“ Models for Core ML (for iOS 11+)  * [awesome-coronavirus](https://github.com/soroushchehresa/awesome-coronavirus) â€“ Projects and resources related to SARS-CoV-2 and COVID-19.  * [awesome-couchdb](https://github.com/quangv/awesome-couchdb) â€“ CouchDB resource list.  * [awesome-courses](https://github.com/fffaraz/awesome-courses) by @fffaraz â€“ Online programming/CS courses.  * [awesome-courses](https://github.com/prakhar1989/awesome-courses) by @prakhar1989 â€“ University Computer Science courses across the web.  * [awesome-cpp](https://github.com/fffaraz/awesome-cpp) â€“ C/C++  * [awesome-crdt](https://github.com/alangibson/awesome-crdt) â€“ Conflict-free replicated data types  * [awesome-creative-coding](https://github.com/terkelg/awesome-creative-coding) â€“ Creative Coding: Generative Art, Data visualization, Interaction Design  * [awesome-critical-tech-reading-list](https://github.com/chobeat/awesome-critical-tech-reading-list) â€“ Reading list for the modern critical programmer.  * [Awesome-Cross-Platform-Apps](https://github.com/Juude/Awesome-Cross-Platform-Apps) â€“ Solutions for building cross-platform apps.  * [awesome-cross-platform-nodejs](https://github.com/bcoe/awesome-cross-platform-nodejs) â€“ Tools for writing cross-platform Node.js code.  * [awesome-crypto-papers](https://github.com/pFarb/awesome-crypto-papers) â€“ Cryptography papers, articles, tutorials and howtos.  * [awesome-cryptocurrencies](https://github.com/kasketis/awesome-cryptocurrencies)  * [awesome-cryptography](https://github.com/sobolevn/awesome-cryptography) â€“ Cryptography and encryption resources.  * [awesome-crystal](https://github.com/veelenga/awesome-crystal) â€“ Crystal Language  * [awesome-css](https://github.com/awesome-css-group/awesome-css) by @awesome-css-group  * [awesome-css](https://github.com/bring2dip/awesome-css) by @deepakbhattarai  * [awesome-css-frameworks](https://github.com/troxler/awesome-css-frameworks) â€“ CSS frameworks  * [awesome-css-learning](https://github.com/micromata/awesome-css-learning) â€“ A tiny list limited to the best CSS Learning Resources  * [awesomeCSV](https://github.com/secretGeek/awesomeCSV) â€“ CSV, Comma Separated Values format  * [awesome-ctf](https://github.com/apsdehal/awesome-ctf) â€“ [Capture the Flag](https://en.wikipedia.org/wiki/Capture_the_flag#Computer_security)    - https://apsdehal.in/awesome-ctf/  * [awesome-cto](https://github.com/kuchin/awesome-cto) â€“ Resources for Chief Technology Officers, with the emphasis on startups  * [awesome-cto-resources](https://github.com/mateusz-brainhub/awesome-cto-resources) â€“ Grow as a Chief Technology Officer.  * [awesome-cybersecurity-blueteam](https://github.com/fabacab/awesome-cybersecurity-blueteam) â€“ [Cybersecurity blue teams](https://en.wikipedia.org/wiki/Blue_team_(computer_security)) resources  * [awesome-cyclejs](https://github.com/cyclejs-community/awesome-cyclejs) â€“ Cycle.js framework  * [awesome-d](https://github.com/zhaopuming/awesome-d) â€“ D programming language.  * [awesome-d3](https://github.com/wbkd/awesome-d3) â€“ [D3js](http://d3js.org/) libraries, plugins and utilities.  * [awesome-dart](https://github.com/yissachar/awesome-dart)  * [awesome-dash](https://github.com/ucg8j/awesome-dash) â€“ [Dash (plotly)](https://plot.ly/dash/) framework for analytical web applications  * [awesome-dashboard](https://github.com/obazoud/awesome-dashboard) â€“ Dashboards/visualization resources.  * [awesome-data-engineering](https://github.com/igorbarinov/awesome-data-engineering) â€“ Data engineering tools for software developers.  * [awesome-datascience](https://github.com/academic/awesome-datascience) â€“ An open source DataScience repository to learn and apply for real world problems.  * [awesome-datasets](https://github.com/viisar/awesome-datasets) â€“ Datasets for papers/experiments/validation.  * [awesome-dataviz](https://github.com/fasouto/awesome-dataviz) â€“ Data visualizations frameworks, libraries and software.  * [awesome-db](https://github.com/numetriclabz/awesome-db) â€“ Database libraries and resources.  * [awesome-ddd](https://github.com/heynickc/awesome-ddd) by @heynickc â€“ Domain-Driven Design (DDD), Command Query Responsibility Segregation (CQRS), Event Sourcing, and Event Storming  * [awesome-ddd](https://github.com/wkjagt/awesome-ddd) by @wkjagt â€“ Domain-Driven Design  * [awesome-decentralized-web](https://github.com/gdamdam/awesome-decentralized-web) â€“ Decentralized services and technologies  * [awesome-decision-tree-papers](https://github.com/benedekrozemberczki/awesome-decision-tree-papers) â€“ Decision Tree Research Papers  * [awesome-deep-learning](https://github.com/ChristosChristofidis/awesome-deep-learning) â€“ Deep Learning tutorials, projects and communities.  * [awesome-deep-learning-papers](https://github.com/terryum/awesome-deep-learning-papers) â€“ The most cited deep learning papers  * [awesome-deep-learning-resources](https://github.com/guillaume-chevalier/awesome-deep-learning-resources) â€“ Rough list of resources about deep learning.  * [awesome-deep-rl](https://github.com/tigerneil/awesome-deep-rl) â€“ Deep Reinforcement Learning  * [awesome-deep-vision](https://github.com/kjw0612/awesome-deep-vision) â€“ Computer vision / deep learning.  * [awesome-deku](https://github.com/lambtron/awesome-deku) â€“ Resources for the Deku library.  * [awesome-delphi](https://github.com/Fr0sT-Brutal/awesome-delphi)  * [awesome-deno](https://github.com/denolib/awesome-deno) â€“ [Deno](https://deno.land/), a secure runtime for JavaScript and TypeScript.  * [awesome-derby](https://github.com/russll/awesome-derby) â€“ Components for DerbyJS.  * [awesome-design](https://github.com/troyericg/awesome-design) â€“ Resources for digital designers.  * [awesome-design-patterns](https://github.com/DovAmir/awesome-design-patterns) â€“ Resources on software design patterns.  * [awesome-design-principles](https://github.com/robinstickel/awesome-design-principles)  * [awesome-design-systems](https://github.com/alexpate/awesome-design-systems)  * [Awesome-Design-Tools](https://github.com/goabstract/Awesome-Design-Tools)    - https://flawlessapp.io/designtools  * [awesome-desktop-js](https://github.com/styfle/awesome-desktop-js) â€“ Implementing desktop apps with JavaScript  * [awesome-dev-discord](https://github.com/ljosberinn/awesome-dev-discord) â€“ Official, development-related Discord servers.    - https://dev-discords.now.sh/  * [awesome-dev-fun](https://github.com/mislavcimpersak/awesome-dev-fun) â€“ Fun libs/packages/languages that have no real purpose but to make a developer chuckle.  * [awesome-developer-blogs](https://github.com/endymion1818/awesome-developer-blogs)  * [awesome-developer-experience](https://github.com/prokopsimek/awesome-developer-experience)  * [awesome-devenv](https://github.com/jondot/awesome-devenv) â€“ Tools, resources and workflow tips making an awesome development environment.  * [awesome-devops](https://github.com/joubertredrat/awesome-devops)  * [awesome-devrel](https://github.com/devrelcollective/awesome-devrel) â€“ Developer Relations  * [awesome-devtools](https://github.com/moimikey/awesome-devtools) â€“ In-browser bookmarklets, tools, and resources for front-end devs.  * [awesome-digital-nomads](https://github.com/cbovis/awesome-digital-nomads) â€“ Resources for Digital Nomads.  * [awesome-digitalocean](https://github.com/jonleibowitz/awesome-digitalocean) â€“ DigitalOcean cloud infrastructure provider  * [awesome-discord](https://github.com/alfg/awesome-discord) by @alfg  * [awesome-discord](https://github.com/jacc/awesome-discord) by @jacc â€“ Discord chat and VoIP application.  * [awesome-discord-communities](https://github.com/mhxion/awesome-discord-communities) â€“ Discord communities for programmers.  * [awesome-diversity](https://github.com/folkswhocode/awesome-diversity) â€“ Diversity in technology.  * [awesome-django](https://github.com/wsvincent/awesome-django) â€“ [Django](https://www.djangoproject.com/) Python web framework  * [awesome-django-cms](https://github.com/mishbahr/awesome-django-cms) â€“ django CMS add-ons.  * [awesome-docker](https://github.com/veggiemonk/awesome-docker) by @veggiemonk  * [awesome-docsify](https://github.com/docsifyjs/awesome-docsify) â€“ [docsify](https://docsify.js.org/) documentation site generator.  * [awesome-doctrine](https://github.com/biberlabs/awesome-doctrine) â€“ Doctrine ORM libraries and resources.  * [awesome-document-understanding](https://github.com/tstanislawek/awesome-document-understanding) â€“ Automated data extraction from documents.  * [awesome-dojo](https://github.com/petk/awesome-dojo) â€“ Dojo JavaScript Toolkit resources and libraries.  * [awesome-dot-dev](https://github.com/orbit-love/awesome-dot-dev) â€“ Developer resources on the .dev TLD.  * [awesome-dotfiles](https://github.com/webpro/awesome-dotfiles)  * [awesome-dotnet](https://github.com/quozd/awesome-dotnet) â€“ .NET libraries, tools, frameworks and software.  * [awesome-dotnet-architecture](https://github.com/mehdihadeli/awesome-dotnet-architecture) â€“ Software architecture, patterns, and principles in .NET platform.  * [awesome-dotnet-async](https://github.com/mehdihadeli/awesome-dotnet-async) â€“ Async, threading, and channels in .NET platform,  * [awesome-dotnet-core](https://github.com/thangchung/awesome-dotnet-core) â€“ .NET core libraries, tools, frameworks and software  * [awesome-dotnet-core-education](https://github.com/mehdihadeli/awesome-dotnet-core-education) â€“ .NET Core education resources.  * [awesome-draft-js](https://github.com/nikgraf/awesome-draft-js) â€“ [Draft.js](https://draftjs.org/) text editor framework  * [awesome-dropwizard](https://github.com/stve/awesome-dropwizard) â€“ [Dropwizard](https://www.dropwizard.io/) Java web framework  * [awesome-drupal](https://github.com/emincansumer/awesome-drupal) by @emincansumer  * [awesome-drupal](https://github.com/mrsinguyen/awesome-drupal) by @mrsinguyen  * [awesome-drupal](https://github.com/nirgn975/awesome-drupal) by @nirgn975 â€“ Useful resources for Drupal CMS :droplet:  * [awesome-dtrace](https://github.com/xen0l/awesome-dtrace) â€“ DTrace books, articles, videos, tools and resources.    - https://awesome-dtrace.com  * [awesome-ebpf](https://github.com/zoidbergwill/awesome-ebpf) â€“ eBPF Linux packet filter  * [awesome-economics](https://github.com/antontarasenko/awesome-economics) â€“ Economics related projects, software, people  * [awesome-ecs](https://github.com/nathanpeck/awesome-ecs) â€“ AWS Elastic Container Service and Fargate.  * [awesome-edtech-tools](https://github.com/hkalant/awesome-edtech-tools) â€“ Tools and resources for educators and virtual teachers.  * [awesome-educate](https://github.com/mercer/awesome-educate) â€“ Education resources online.  * [awesome-educational-games](https://github.com/yrgo/awesome-educational-games) â€“ Educational games to learn editors, languages, programming  * [awesome-ejabberd](https://github.com/shantanu-deshmukh/awesome-ejabberd) â€“ All awesome stuff of the ejabberd ecosystem.    - https://ejabberd.shantanudeshmukh.com  * [awesome-electron](https://github.com/sindresorhus/awesome-electron) â€“ Resources for creating apps with [Electron](http://electron.atom.io/) (formerly atom-shell).  * [awesome-electronics](https://github.com/kitspace/awesome-electronics) â€“ Electronic engineering  * [awesome-elixir](https://github.com/h4cc/awesome-elixir)  * [awesome-elm](https://github.com/sporto/awesome-elm) â€“ [Elm](https://elm-lang.org/), a functional reactive language  * [awesome-emacs](https://github.com/emacs-tw/awesome-emacs) by @emacs-tw  * [awesome-emacs](https://github.com/sefakilic/awesome-emacs) by @sefakilic  * [awesome-emacs](https://github.com/tacticiankerala/awesome-emacs) by @tacticiankerala  * [awesome-emails](https://github.com/jonathandion/awesome-emails) â€“ Build better emails.  * [awesome-embedded-rust](https://github.com/rust-embedded/awesome-embedded-rust) â€“ Embedded and Low-level development in the Rust programming language  * [awesome-ember](https://github.com/ember-community-russia/awesome-ember) by @ember-community-russia â€“ [Ember.js](https://emberjs.com/) JavaScript framework  * [awesome-ember](https://github.com/nmec/awesome-ember) by @nmec â€“ Ember.js things.  * [awesome-endless-codeforall-list](https://github.com/RobTranquillo/awesome-endless-codeforall-list) â€“ Every tool that civic hackers worldwide use to work.  * [awesome-engineer-onboarding](https://github.com/posquit0/awesome-engineer-onboarding)  * [awesome-engineering-ladders](https://github.com/posquit0/awesome-engineering-ladders)  * [awesome-engineering-team-principles](https://github.com/posquit0/awesome-engineering-team-principles)  * [awesome-eosio](https://github.com/DanailMinchev/awesome-eosio) â€“ [EOS.IO](https://eos.io/) blockchain protocol  * [awesome-erlang](https://github.com/drobakowski/awesome-erlang)  * [awesome-eslint](https://github.com/dustinspecker/awesome-eslint) â€“ [ESLint](https://eslint.org/) JavaScript linter  * [awesome-esolangs](https://github.com/angrykoala/awesome-esolangs) â€“ Esoteric languages  * [awesome-eta](https://github.com/sfischer13/awesome-eta) â€“ [Eta](https://eta-lang.org/) programming language  * [awesome-ethereum](https://github.com/bekatom/awesome-ethereum) by @bekatom â€“ [Ethereum](https://ethereum.org/) decentralized software platform & Dapps.  * [Awesome-Ethereum](https://github.com/ttumiel/Awesome-Ethereum) by @ttumiel  * [awesome-ethereum](https://github.com/vinsgo/awesome-ethereum) by @vinsgo    - http://awesome-ethereum.com/  * [awesome-ethereum-virtual-machine](https://github.com/pirapira/awesome-ethereum-virtual-machine)  * [awesome-falsehood](https://github.com/kdeldycke/awesome-falsehood) â€“ Falsehoods programmers believe in.  * [awesome-fantasy](https://github.com/r7kamura/awesome-fantasy) â€“ FinalFantasy-ish metaphors in software.  * [awesome-fast-check](https://github.com/dubzzz/awesome-fast-check) â€“ [fast-check](https://github.com/dubzzz/fast-check/) property based testing framework for JavaScript/TypeScript  * [awesome-fastapi](https://github.com/mjhea0/awesome-fastapi) â€“ [FastAPI](https://fastapi.tiangolo.com/) Python web framework  * [awesome-feathersjs](https://github.com/feathersjs/awesome-feathersjs) â€“ [Feathers](https://feathersjs.com/) Node.js framework for real-time applications REST APIs.  * [awesome-fediverse](https://github.com/emilebosch/awesome-fediverse) â€“ [Fediverse](https://en.wikipedia.org/wiki/Fediverse) resources.  * [awesome-ffmpeg](https://github.com/transitive-bullshit/awesome-ffmpeg) â€“ FFmpeg resources.  * [awesome-firebase](https://github.com/jthegedus/awesome-firebase) â€“ Firebase mobile development platform  * [awesome.fish](https://github.com/jorgebucaran/awesome.fish) â€“ Fish shell    - https://git.io/awesome-fish  * [awesome-flask](https://github.com/humiaozuzu/awesome-flask) â€“ Flask Python web framework resources and plugins.  * [awesome-flexbox](https://github.com/afonsopacifer/awesome-flexbox) â€“ CSS Flexible Box Layout Module.  * [awesome-fluidapp](https://github.com/lborgav/awesome-fluidapp) â€“ Icons, Userstyles and Userscripts for Fluid Apps  * [awesome-flutter](https://github.com/Solido/awesome-flutter) â€“ An awesome list that curates the best Flutter libraries, tools, tutorials, articles and more.  * [awesome-fonts](https://github.com/brabadu/awesome-fonts) â€“ Fonts and everything  * [awesome-food](https://github.com/jzarca01/awesome-food) â€“ Food related software projects  * [awesome-for-beginners](https://github.com/MunGell/awesome-for-beginners) â€“ Beginner-friendly projects to start contributing.  * [awesome-fortran](https://github.com/rabbiabram/awesome-fortran)  * [awesome-foss-apps](https://github.com/DataDaoDe/awesome-foss-apps) â€“ Production grade free and open source software  * [awesome-fp-js](https://github.com/stoeffel/awesome-fp-js) â€“ Functional programming stuff in JavaScript.  * [awesome-framer](https://github.com/podo/awesome-framer) â€“ Framer prototyping tool  * [awesome-fraud-detection-papers](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers) â€“ Fraud detection research papers.  * [awesome-frc](https://github.com/andrewda/awesome-frc) â€“ First Robotics Competition  * [awesome-free-software](https://github.com/johnjago/awesome-free-software) â€“ Free as in freedom software  * [awesome-frege](https://github.com/sfischer13/awesome-frege) â€“ [Frege](https://github.com/Frege/frege) programming language  * [awesome-fsharp](https://github.com/fsprojects/awesome-fsharp) â€“ F# programming language  * [awesome-fsm](https://github.com/leonardomso/awesome-fsm) by @leonardomso â€“ Finite State Machines and Statecharts  * [awesome-fsm](https://github.com/soixantecircuits/awesome-fsm) by @soixantecircuits â€“ Finite State Machines  * [awesome-functional-programming](https://github.com/lucasviola/awesome-functional-programming) by @lucasviola  * [awesome-functional-programming](https://github.com/xgrommx/awesome-functional-programming) by @xgrommx  * [awesome-funny-markov](https://github.com/sublimino/awesome-funny-markov) â€“ Delightfully amusing and facetious Markov chain output.  * [awesome-fuse](https://github.com/fuse-compound/awesome-fuse) â€“ [Fuse](https://fuseopen.com/) mobile development framework  * [awesome-fuzzing](https://github.com/cpuu/awesome-fuzzing) â€“ Fuzzing (or Fuzz Testing) for software security  * [awesome-gametalks](https://github.com/hzoo/awesome-gametalks) â€“ Gaming talks (development, design, etc)  * [awesome-gbdev](https://github.com/gbdev/awesome-gbdev) â€“ Game Boy development resources such as tools, docs, emulators, related projects and open-source ROMs    - https://gbdev.github.io/list  * [awesome-geek-podcasts](https://github.com/ayr-ton/awesome-geek-podcasts) â€“ Podcasts we like to listen to.    - http://ayr-ton.github.io/awesome-geek-podcasts  * [awesome-gemini](https://github.com/kr1sp1n/awesome-gemini) â€“ [Gemini protocol](https://gemini.circumlunar.space/)  * [awesome-geojson](https://github.com/tmcw/awesome-geojson) â€“ GeoJSON  * [awesome-ggplot2](https://github.com/erikgahner/awesome-ggplot2) â€“ [ggplot2](https://ggplot2.tidyverse.org/) data visualization for R.  * [awesome-gideros](https://github.com/stetso/awesome-gideros) â€“ [Gideros](http://giderosmobile.com/) game development framework  * [awesome-gif](https://github.com/davisonio/awesome-gif) â€“ GIF software resources    - https://davison.io/awesome-gif  * [awesome-gists](https://github.com/vsouza/awesome-gists) â€“ Amazing gists  * [awesome-git](https://github.com/dictcp/awesome-git) â€“ Git tools, resources and shiny things.  * [awesome-git-addons](https://github.com/stevemao/awesome-git-addons) â€“ Add-ons that extend/enhance the git CLI.  * [awesome-git-hooks](https://github.com/CompSciLauren/awesome-git-hooks) â€“ Easy-to-use git hooks for automating tasks during git workflows.  * [awesome-github](https://github.com/AntBranch/awesome-github) _In Chinese_ by @AntBranch â€“ GitHub guides, articles, sites, tools, projects and resources.  æ”¶é›†è¿™ä¸ªåˆ—è¡¨ï¼Œåªæ˜¯ä¸ºäº†æ›´å¥½åœ°ä½¿ç”¨äº²çˆ±çš„GitHub,æ¬¢è¿Žæäº¤prå’Œissueã€‚    - https://github.com/AntBranch/awesome-github  * [awesome-github](https://github.com/fffaraz/awesome-github) by @fffaraz â€“ Git and GitHub references.  * [awesome-github](https://github.com/Kikobeats/awesome-github) by @Kikobeats â€“ GitHub secrets and goodies.  * [awesome-github](https://github.com/phillipadsmith/awesome-github) by @phillipadsmith â€“ GitHub's awesomeness  * [awesome-github-repo](https://github.com/flyhigher139/awesome-github-repo) â€“ GitHub repositories; various topics like study materials, Raspberry Pi etc.  * [awesome-gnome](https://github.com/Kazhnuz/awesome-gnome) â€“ Gnome Desktop Environment.  * [awesome-go](https://github.com/avelino/awesome-go) by @avelino â€“ Golang    - http://awesome-go.com/  * [awesome-go-books](https://github.com/heatroom/awesome-go-books) â€“ Online and free golang books.  * [awesome-godot](https://github.com/godotengine/awesome-godot) â€“ [Godot](https://godotengine.org/) game engine  * [awesome-gradient-boosting-papers](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers) â€“ Gradient boosting research papers with implementations.  * [awesome-grails](https://github.com/hitenpratap/awesome-grails)  * [awesome-graph-classification](https://github.com/benedekrozemberczki/awesome-graph-classification) â€“ Graph embedding papers with implementations.  * [awesome-graphql](https://github.com/chentsulin/awesome-graphql) â€“ GraphQL & Relay Resources.  * [awesome-groovy](https://github.com/kdabir/awesome-groovy)  * [awesome-growth-hacking](https://github.com/bekatom/awesome-growth-hacking)  * [awesome-gulp](https://github.com/alferov/awesome-gulp) â€“ [Gulp](http://gulpjs.com/) build system resources and plugins.  * [awesome-gyazo](https://github.com/gyazo/awesome-gyazo) â€“ Tools for [Gyazo](https://gyazo.com/) screen capture application.  * [awesome-h2o](https://github.com/h2oai/awesome-h2o) â€“ H2O Machine Learning  * [awesome-hacking](https://github.com/carpedm20/awesome-hacking)  * [awesome-hacktoberfest-2020](https://github.com/Piyushhbhutoria/awesome-hacktoberfest-2020) â€“ [Hacktoberfest](https://hacktoberfest.digitalocean.com/)-friendly repositories and resources.  * [awesome-hadoop](https://github.com/youngwookim/awesome-hadoop) â€“ Hadoop and Hadoop ecosystem resources.  * [awesome-hardware-tools](https://github.com/aolofsson/awesome-hardware-tools) â€“ Open-source hardware tools.  * [awesome-haskell](https://github.com/krispo/awesome-haskell)  * [awesome-hasura](https://github.com/aaronhayes/awesome-hasura) â€“ [Hasura](https://hasura.io/) is an instant realtime GraphQL engine for PostgreSQL.  * [awesome-haxe-gamedev](https://github.com/dvergar/awesome-haxe-gamedev) â€“ Game development in [Haxe](https://haxe.org/) cross-platform programming language  * [awesome-hbase](https://github.com/rayokota/awesome-hbase) â€“ Apache HBase  * [awesome-hdl](https://github.com/drom/awesome-hdl) â€“ Hardware Description Languages  * [awesome-healthcare](https://github.com/kakoni/awesome-healthcare) â€“ Open source healthcare software, libraries, tools and resources.  * [awesome-heroku](https://github.com/ianstormtaylor/awesome-heroku) â€“ Heroku resources.  * [awesome_hierarchical_matrices](https://github.com/gchavez2/awesome_hierarchical_matrices) â€“ Hierarchical matrices frameworks, libraries, and software.  * [awesome-home-assistant](https://github.com/frenck/awesome-home-assistant) â€“ [Home Assistant](https://www.home-assistant.io/) home automation    - https://awesome-ha.com  * [awesome-homematic](https://github.com/homematic-community/awesome-homematic) â€“ [HomeMatic](https://www.homematic.com/) home automation  * [awesome-honeypots](https://github.com/paralax/awesome-honeypots) â€“ Honeypot resources  * [awesome-html5](https://github.com/diegocard/awesome-html5)  * [awesome-humane-tech](https://github.com/humanetech-community/awesome-humane-tech) â€“ Promoting Solutions that Improve Wellbeing, Freedom and Society  * [awesome-hyper](https://github.com/bnb/awesome-hyper) â€“ [Hyper](https://hyper.is/) terminal  * [awesome-ibmcloud](https://github.com/victorshinya/awesome-ibmcloud) â€“ IBM Cloud    - https://awesome-ibmcloud.mybluemix.net  * [awesome-icons](https://github.com/notlmn/awesome-icons) â€“ Downloadable SVG/PNG/Font icon projects  * [awesome-idris](https://github.com/joaomilho/awesome-idris) â€“ ð›Œ [Idris](https://www.idris-lang.org/), functional programming language with dependent types  * [awesome-incident-response](https://github.com/meirwah/awesome-incident-response) â€“ Resources useful for incident responders.  * [awesome-indie](https://github.com/mezod/awesome-indie) â€“ Resources for independent developers to make money  * [awesome-infinidash](https://github.com/joenash/awesome-infinidash)  * [awesome-influxdb](https://github.com/mark-rushakoff/awesome-influxdb) â€“ Resources for the time series database InfluxDB  * [awesome-information-retrieval](https://github.com/harpribot/awesome-information-retrieval) â€“ Information retrieval resources  * [awesome-inspectit](https://github.com/inspectit-labs/awesome-inspectit) â€“ InspectIT documentations and resources.  * [AwesomeInterpreter](https://github.com/BaseMax/AwesomeInterpreter) â€“ Open-source code interpreters on GitHub.  * [awesome-interview-questions](https://github.com/MaximAbramchuck/awesome-interview-questions) â€“ Interview questions.  * [awesome-ionic](https://github.com/candelibas/awesome-ionic) â€“ [Ionic](https://ionicframework.com/) mobile development framework  * [awesome-ios](https://github.com/vsouza/awesome-ios)  * [awesome-ios-cn](https://github.com/jobbole/awesome-ios-cn) _In Chinese_ â€“ iOS èµ„æºå¤§å…¨ä¸­æ–‡ç‰ˆï¼Œå†…å®¹åŒ…æ‹¬ï¼šæ¡†æž¶ã€ç»„ä»¶ã€æµ‹è¯•ã€Apple Storeã€SDKã€XCodeã€ç½‘ç«™ã€ä¹¦ç±ç­‰  * [awesome-ios-ui](https://github.com/cjwirth/awesome-ios-ui) â€“ UI/UX libraries for iOS.  * [awesome-IoT](https://github.com/dharmeshkakadia/awesome-IoT) by @dharmeshkakadia â€“ Internet of Things  * [awesome-iot](https://github.com/HQarroum/awesome-iot) by @HQarroum â€“ Internet of Things  * [awesome-IoT-hybrid](https://github.com/weblancaster/awesome-IoT-hybrid) â€“ Internet of Things and Hybrid Applications  * [awesome-ipfs](https://github.com/ipfs/awesome-ipfs) â€“ [IPFS](https://ipfs.io/) distributed web    - https://awesome.ipfs.io/  * [awesome-irc](https://github.com/davisonio/awesome-irc) â€“ Internet Relay Chat protocol.  * [awesome-it-quotes](https://github.com/victorlaerte/awesome-it-quotes) â€“ Collect all relevant quotes said over the history of IT  * [awesome-jamstack](https://github.com/automata/awesome-jamstack) â€“ [JAMstack](https://jamstack.org) (JavaScript, APIs, Markup)  * [awesome-java](https://github.com/akullpp/awesome-java)  * [awesome-javascript](https://github.com/sorrycc/awesome-javascript)  * [awesome-javascript-books](https://github.com/heatroom/awesome-javascript-books) â€“ Online and free JavaScript books.  * [awesome-javascript-learning](https://github.com/micromata/awesome-javascript-learning) â€“ Tiny list limited to the best JavaScript Learning Resources  * [awesome-jitsi](https://github.com/easyjitsi/awesome-jitsi) â€“ [Jitsi](https://jitsi.org/) open-source video conferencing.  * [awesome-jmeter](https://github.com/aliesbelik/awesome-jmeter) â€“ Apache JMeter load testing  * [awesome-job-boards](https://github.com/emredurukn/awesome-job-boards) by @emredurukn  * [awesome-job-boards](https://github.com/tramcar/awesome-job-boards) by @tramcar  * [awesome-jquery](https://github.com/petk/awesome-jquery)  * [awesome-js-drama](https://github.com/scottcorgan/awesome-js-drama) â€“ JavaScript topics the just might spark the next revolt!  * [awesome-json](https://github.com/burningtree/awesome-json)  * [awesome-json-datasets](https://github.com/jdorfman/awesome-json-datasets) â€“ JSON datasets that don't require authentication  * [awesome-json-next](https://github.com/json-next/awesome-json-next) â€“ What's Next for JSON for Structured (Meta) Data in Text.  * [awesome-jsonschema](https://github.com/jviotti/awesome-jsonschema) â€“ [JSON Schema](http://json-schema.org/).  * [awesome-julia](https://github.com/melvin0008/awesome-julia)  * [awesome-jupyter](https://github.com/markusschanta/awesome-jupyter) â€“ [Jupyter](https://jupyter.org/)  * [awesome-jvm](https://github.com/deephacks/awesome-jvm)  * [awesome-kafka](https://github.com/monksy/awesome-kafka) â€“ [Apache Kafka](http://kafka.apache.org/), distributed streaming platform  * [awesome-katas](https://github.com/gamontal/awesome-katas) â€“ Code katas  * [awesome-kde](https://github.com/francoism90/awesome-kde) â€“ KDE Desktop Environment.  * [awesome-knockout](https://github.com/dnbard/awesome-knockout) â€“ Plugins for Knockout MVVM framework.  * [awesome-koa](https://github.com/ellerbrock/awesome-koa) â€“ [Koa.js](https://koajs.com/) Web Framework    - https://ellerbrock.github.io/awesome-koa  * [awesome-koans](https://github.com/ahmdrefat/awesome-koans) â€“ Programming kÅans in various languages.  * [awesome-kotlin](https://github.com/KotlinBy/awesome-kotlin) â€“ [Kotlin](https://kotlinlang.org/) programming language    - https://kotlin.link/  * [awesome-kotlin-native](https://github.com/bipinvaylu/awesome-kotlin-native) â€“ Kotlin Multiplatform libraries & resources.  * [awesome-kr-foss](https://github.com/darjeeling/awesome-kr-foss) â€“ Korean open source projects.  * [awesome-kubernetes](https://github.com/ramitsurana/awesome-kubernetes)    - https://ramitsurana.github.io/awesome-kubernetes  * [awesome-landing-page](https://github.com/nordicgiant2/awesome-landing-page) â€“ Landing pages templates  * [awesome-languages](https://github.com/perfaram/awesome-languages) â€“ Open-source programming languages.  * [awesome-laravel](https://github.com/chiraggude/awesome-laravel) by @chiraggude  * [awesome-laravel](https://github.com/TimothyDJones/awesome-laravel) by @TimothyDJones  * [Awesome-Laravel-Education](https://github.com/fukuball/Awesome-Laravel-Education) _In English and Chinese_ â€“ Laravel PHP framework learning resources.  * [awesome-latam](https://github.com/gophers-latam/awesome-latam) _In Spanish_ â€“ Recursos en EspaÃ±ol para desarrolladores de Golang.    - https://gophers-latam.github.io/  * [awesome-LaTeX](https://github.com/egeerardyn/awesome-LaTeX)  * [awesome-ld-preload](https://github.com/gaul/awesome-ld-preload) â€“ LD_PRELOAD, a mechanism for changing application behavior at run-time.  * [awesome-leading-and-managing](https://github.com/LappleApple/awesome-leading-and-managing) â€“ Leading people and being a manager. Geared toward tech, but potentially useful to anyone.  * [awesome-learn-datascience](https://github.com/siboehm/awesome-learn-datascience) â€“ Resources to help you get started with Data Science  * [awesome-ledger](https://github.com/sfischer13/awesome-ledger) â€“ Ledger command-line accounting system  * [awesome-legacy-code](https://github.com/legacycoderocks/awesome-legacy-code) â€“ Legacy systems with publicly available source code  * [awesome-less](https://github.com/LucasBassetti/awesome-less) â€“ Less CSS preprocessor  * [awesome-lesscode](https://github.com/dream2023/awesome-lesscode) _In Chinese_ â€“ Low code / no code projects  * [awesome-libgdx](https://github.com/rafaskb/awesome-libgdx) â€“ [libGDX](https://libgdx.badlogicgames.com/) cross-platform games development framework  * [awesome-libgen](https://github.com/freereadorg/awesome-libgen) â€“ Library Genesis, the world's largest free library.  * [awesome-libra](https://github.com/learndapp/awesome-libra) by @learndapp â€“ [Libra](https://libra.org/) cryptocurrency by Facebook  * [awesome-libra](https://github.com/reed-hong/awesome-libra) by @reed-hong â€“ [Facebook Diem](https://www.diem.com/) (nÃ©e Libra) digital currency.  * [awesome-librehosters](https://github.com/libresh/awesome-librehosters) â€“ Nice hosting providers  * [awesome-linguistics](https://github.com/theimpossibleastronaut/awesome-linguistics) â€“ Tools, theory and platforms for linguistics.  * [awesome-links](https://github.com/rbk/awesome-links) â€“ Web Development Links by @richardbenjamin.  * [awesome-linters](https://github.com/caramelomartins/awesome-linters) â€“ Resources for a more literate programming.  * [awesome-linux](https://github.com/aleksandar-todorovic/awesome-linux) â€“ Linux software.  * [awesome-linux-containers](https://github.com/Friz-zy/awesome-linux-containers) â€“ Linux Containers frameworks, libraries and software  * [awesome-linux-resources](https://github.com/itech001/awesome-linux-resources)    - http://www.linux6.com  * [Awesome-Linux-Software](https://github.com/luong-komorebi/Awesome-Linux-Software) â€“ Linux applications for all users and developers.  * [awesome-linuxaudio](https://github.com/nodiscc/awesome-linuxaudio) â€“ Professional audio/video/live events production on Linux.  * [awesome-lit-html](https://github.com/web-padawan/awesome-lit-html) â€“ [lit-html](https://lit-html.polymer-project.org/) HTML templating library  * [awesome-livecoding](https://github.com/toplap/awesome-livecoding) â€“ All things Livecoding.  * [awesome-logging](https://github.com/roundrobin/awesome-logging)  * [awesome-loginless](https://github.com/fiatjaf/awesome-loginless) â€“ Internet services that don't require logins or registrations.  * [awesome-love2d](https://github.com/love2d-community/awesome-love2d) â€“ [LÃ–VE](http://love2d.org/) Lua game framework  * [awesome-lowcode](https://github.com/taowen/awesome-lowcode) _In Chinese_ â€“ Chinese low code platforms.  * [awesome-lua](https://github.com/forhappy/awesome-lua) by @forhappy  * [awesome-lua](https://github.com/LewisJEllis/awesome-lua) by @LewisJEllis  * [awesome-lumen](https://github.com/unicodeveloper/awesome-lumen) â€“ [Lumen](https://lumen.laravel.com/), PHP Microframework by Laravel  * [awesome-luvit](https://github.com/luvit/awesome-luvit) â€“ [Luvit](https://luvit.io/), asynchronous I/O for Lua  * [awesome-mac](https://github.com/jaywcjlove/awesome-mac) by @jaywcjlove â€“ Premium macOS software in various categories    - https://git.io/macx  * [awesome-mac](https://github.com/xyNNN/awesome-mac) by @xyNNN â€“ macOS tools, applications and games.  * [awesome-mac-apps](https://github.com/justin-j/awesome-mac-apps) â€“ macOS apps  * [awesome-machine-learning](https://github.com/josephmisiti/awesome-machine-learning)  * [awesome-macOS](https://github.com/iCHAIT/awesome-macOS) â€“ OS X applications, tools and communities.  * [awesome-macos-command-line](https://github.com/herrbischoff/awesome-macos-command-line) â€“ Shell commands and tools specific to OS X.  * [awesome-macos-screensavers](https://github.com/agarrharr/awesome-macos-screensavers) â€“ Screensavers for Mac OS X  * [awesome-mad-science](https://github.com/feross/awesome-mad-science) â€“ npm packages that make you say ""wow, didn't know that was possible!""  * [awesome-magento2](https://github.com/DavidLambauer/awesome-magento2) â€“ [Magento 2](https://magento.com/) PHP eCommerce platform    - https://davidlambauer.github.io/awesome-magento2/  * [awesome-maintainers](https://github.com/nayafia/awesome-maintainers) â€“ Talks, blog posts, and interviews about the experience of being an open source maintainer  * [awesome-malware-analysis](https://github.com/rshipp/awesome-malware-analysis)  * [awesome-manifestos](https://github.com/imsky/awesome-manifestos) â€“ Interesting software manifestos and principles  * [awesome-marionette](https://github.com/sadcitizen/awesome-marionette) â€“ [marionette.js](https://marionettejs.com/) framework  * [awesome-markdown](https://github.com/BubuAnabelas/awesome-markdown)  * [awesome-markdown-alternatives](https://github.com/mundimark/awesome-markdown-alternatives) â€“ Light-weight markup markdown alternatives.  * [awesome-masonite](https://github.com/vaibhavmule/awesome-masonite) â€“ [Masonite](https://docs.masoniteproject.com/) Python web framework  * [awesome-mastodon](https://github.com/tleb/awesome-mastodon) â€“ [Mastodon](https://joinmastodon.org/) decentralized microblogging network  * [awesome-material](https://github.com/sachin1092/awesome-material) â€“ Google's material design  * [Awesome-MaterialDesign](https://github.com/lightSky/Awesome-MaterialDesign) _In Chinese_ â€“ Resources and libraries for [Material Design](http://www.google.com/design/spec/material-design/introduction.html).  * [awesome-math](https://github.com/rossant/awesome-math) â€“ Mathematics  * [awesome-MATLAB](https://github.com/mikecroucher/awesome-MATLAB)  * [awesome-mechanical-keyboard](https://github.com/BenRoe/awesome-mechanical-keyboard) â€“ Mechanical Keyboards    - https://keebfol.io  * [awesome-mesos](https://github.com/dharmeshkakadia/awesome-mesos) by @dharmeshkakadia  * [awesome-mesos](https://github.com/parolkar/awesome-mesos) by @parolkar  * [awesome-meteor](https://github.com/Urigo/awesome-meteor)  * [awesome-meteor-developers](https://github.com/harryadel/awesome-meteor-developers) â€“ Ways to support Meteor developers and packages.  * [awesome-mews](https://github.com/MewsSystems/awesome-mews) â€“ Resources Mews developers like and aligns with their vision.  * [awesome-micro-npm-packages](https://github.com/parro-it/awesome-micro-npm-packages) â€“ Small, focused npm packages.  * [awesome-microbit](https://github.com/carlosperate/awesome-microbit) â€“ BBC micro:bit  * [awesome-microfrontends](https://github.com/ChristianUlbrich/awesome-microfrontends)  * [awesome-microservices](https://github.com/mfornos/awesome-microservices) â€“ Microservice Architecture related principles and technologies.  * [awesome-minecraft](https://github.com/bs-community/awesome-minecraft)  * [awesome-minimalist](https://github.com/neiesc/awesome-minimalist) â€“ Minimalist frameworks (simple and lightweight).  * [awesome-mobile](https://github.com/alec-c4/awesome-mobile) â€“ Instruments for mobile marketing and development  * [awesome-mobile-web-development](https://github.com/myshov/awesome-mobile-web-development) â€“ All that you need to create a great mobile web experience  * [awesome-mongodb](https://github.com/ramnes/awesome-mongodb)  * [awesome-monitoring](https://github.com/crazy-canux/awesome-monitoring) â€“ INFRASTRUCTUREã€OPERATION SYSTEM and APPLICATION monitoring tools for Operations.    - http://canuxcheng.com/awesome-monitoring/  * [awesome-monte-carlo-tree-search-papers](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers) â€“ Monte Carlo tree search, a heuristic search algorithm frequently used in games.  * [awesome-motion-design-web](https://github.com/lucasmaiaesilva/awesome-motion-design-web)  * [awesome-motion-planning](https://github.com/AGV-IIT-KGP/awesome-motion-planning) â€“ Papers, books and tools for motion planning.  * [awesome-mqtt](https://github.com/hobbyquaker/awesome-mqtt) â€“ MQTT related stuff.  * [awesome-msr](https://github.com/dspinellis/awesome-msr) â€“ Empirical Software Engineering: evidence-based, data-driven research on software systems  * [awesome-music](https://github.com/ciconia/awesome-music) â€“ Music, audio, MIDI  * [awesome-mysql](https://github.com/shlomi-noach/awesome-mysql) â€“ MySQL software, libraries, tools and resources  * [awesome-naming](https://github.com/gruhn/awesome-naming) â€“ When naming things is done right.  * [awesome-neo4j](https://github.com/neueda/awesome-neo4j) â€“ Neo4j graph database  * [awesome-netherlands-events](https://github.com/awkward/awesome-netherlands-events) â€“ Dutch (tech related) events  * [awesome-network-analysis](https://github.com/briatte/awesome-network-analysis)    - http://f.briatte.org/r/awesome-network-analysis-list  * [awesome-network-embedding](https://github.com/chihming/awesome-network-embedding) â€“ Papers on node embedding techniques.  * [awesome-network-js](https://github.com/Kikobeats/awesome-network-js) â€“ Network layer resources in pure JavaScript  * [Awesome-Networking](https://github.com/clowwindy/Awesome-Networking)  * [awesome-neuroscience](https://github.com/analyticalmonk/awesome-neuroscience) â€“ Neuroscience libraries, software and resources    - http://akashtandon.com/awesome-neuroscience/  * [awesome-newsletters](https://github.com/mpron/awesome-newsletters) by @mpron â€“ Developer newsletters  * [awesome-newsletters](https://github.com/webpro/awesome-newsletters) by @webpro â€“ The best (weekly) newsletters  * [awesome-newsletters](https://github.com/zudochkin/awesome-newsletters) by @zudochkin  * [awesome-nextjs](https://github.com/unicodeveloper/awesome-nextjs) â€“ [Next.js](https://nextjs.org/) React-based JavaScript framework  * [awesome-nim](https://github.com/VPashkov/awesome-nim) â€“ [Nim](https://nim-lang.org/) programming language  * [awesome-nlp](https://github.com/keon/awesome-nlp) â€“ Natural Language Processing.  * [Awesome-no-code-tools](https://github.com/ElijT/Awesome-no-code-tools)  * [awesome-no-login-web-apps](https://github.com/aviaryan/awesome-no-login-web-apps) â€“ Web apps that work without login  * [awesome-nocode](https://github.com/nslindtner/awesome-nocode)  * [awesome-node-esm](https://github.com/talentlessguy/awesome-node-esm) â€“ ES modules for Node.js  * [awesome-nodejs](https://github.com/sindresorhus/awesome-nodejs) by @sindresorhus  * [awesome-non-financial-blockchain](https://github.com/machinomy/awesome-non-financial-blockchain) â€“ Non-financial applications of blockchain  * [awesome-nosql-guides](https://github.com/erictleung/awesome-nosql-guides) â€“ NoSQL databases    - https://erictleung.com/awesome-nosql-guides/  * [awesome-npm](https://github.com/sindresorhus/awesome-npm)  * [awesome-npm-scripts](https://github.com/RyanZim/awesome-npm-scripts) â€“ using npm as a build tool  * [awesome-ntnu](https://github.com/michaelmcmillan/awesome-ntnu) â€“ Projects by NTNU students.  * [awesome-nuxt](https://github.com/nuxt-community/awesome-nuxt) â€“ Resources for [Nuxt.js](https://nuxtjs.org/), framework for universal Vue.js applications.  * [awesome-objc-frameworks](https://github.com/follyxing/awesome-objc-frameworks)  * [awesome-observables](https://github.com/sindresorhus/awesome-observables) â€“ An Observable is a collection that arrives over time.  * [awesome-obsidian](https://github.com/kmaasrud/awesome-obsidian) â€“ [Obsidian](https://obsidian.md/) knowledge base app.  * [awesome-ocaml](https://github.com/ocaml-community/awesome-ocaml)  * [awesome-ocap](https://github.com/dckc/awesome-ocap) â€“ Capability-based security enables the concise composition of powerful patterns of cooperation without vulnerability.  * [awesome-okr](https://github.com/domenicosolazzo/awesome-okr) â€“ Objective - Key Results, the best practice of setting and communicating company, team and employee objectives and measuring their progress based on achieved results  * [awesome-online-ide](https://github.com/styfle/awesome-online-ide) â€“ Online development environments    - https://ide.ceriously.com  * [awesome-online-machine-learning](https://github.com/MaxHalford/awesome-online-machine-learning) â€“ [Online machine learning](https://en.wikipedia.org/wiki/Online_machine_learning)  * [awesome-open-company](https://github.com/opencompany/awesome-open-company) â€“ Open companies: Share as much as possible, charge as little as possible.  * [awesome-open-science](https://github.com/silky/awesome-open-science)  * [awesome-open-source-supporters](https://github.com/zachflower/awesome-open-source-supporters) â€“ Companies that offer their services for free to Open Source projects  * [awesome-opengl](https://github.com/eug/awesome-opengl) â€“ OpenGL libraries, debuggers and resources.  * [awesome-opensource-data-engineering](https://github.com/gunnarmorling/awesome-opensource-data-engineering)  * [awesome-opensource-documents](https://github.com/44bits/awesome-opensource-documents) â€“ Open source or open source licensed documents, guides, books.  * [awesome-OpenSourcePhotography](https://github.com/ibaaj/awesome-OpenSourcePhotography) â€“ Free open source software & libraries for photography. Also tools for video.  * [awesome-osc](https://github.com/amir-arad/awesome-osc) â€“ [Open Sound Control](http://opensoundcontrol.org/)  * [awesome-oss-alternatives](https://github.com/RunaCapital/awesome-oss-alternatives) â€“ Open-source alternatives to established SaaS products.  * [awesome-pascal](https://github.com/Fr0sT-Brutal/awesome-pascal) â€“ Delphi/FreePascal/(any)Pascal frameworks, libraries, resources, and shiny things.  * [awesome-pcaptools](https://github.com/caesar0301/awesome-pcaptools) â€“ Tools to process network traces.  * [awesome-pentest](https://github.com/enaqx/awesome-pentest) â€“ Penetration testing resources and tools.  * [awesome-pentest-cheat-sheets](https://github.com/coreb1t/awesome-pentest-cheat-sheets) â€“ Penetration testing  * [Awesome-People-in-Computer-Vision](https://github.com/solarlee/Awesome-People-in-Computer-Vision)  * [awesome-perfocards](https://github.com/Wolg/awesome-perfocards) _See [perfokaart](https://et.wikipedia.org/wiki/Perfokaart)._  * [awesome-perl](https://github.com/hachiojipm/awesome-perl)  * [awesome-persian](https://github.com/fffaraz/awesome-persian) â€“ Persian/Farsi supporting tools, fonts, and development resources.  * [awesome-phalcon](https://github.com/phalcon/awesome-phalcon) â€“ [Phalcon](https://phalconphp.com/en/) PHP framework libraries and resources.  * [awesome-pharo](https://github.com/pharo-open-documentation/awesome-pharo) â€“ [Pharo](https://pharo.org/) Smalltalk  * [awesome-pharo-ml](https://github.com/pharo-ai/awesome-pharo-ml) â€“ Machine learning, AI, data science in Pharo.  * [awesome-php](https://github.com/ziadoz/awesome-php)  * [awesome-PICO-8](https://github.com/pico-8/awesome-PICO-8) â€“ [PICO-8](https://www.lexaloffle.com/pico-8.php) fantasy console for making, sharing and playing tiny games    - https://pico-8.github.io/awesome-PICO-8/  * [awesome-pinned-gists](https://github.com/matchai/awesome-pinned-gists) â€“ Dynamic pinned gists for GitHub.  * [awesome-pipeline](https://github.com/pditommaso/awesome-pipeline) â€“ Pipeline toolkits.  * [awesome-piracy](https://github.com/Igglybuff/awesome-piracy) â€“ Warez and piracy links  * [awesome-pixel-art](https://github.com/Siilwyn/awesome-pixel-art)  * [awesome-play1](https://github.com/PerfectCarl/awesome-play1) â€“ Play Framework 1.x modules, tools, and resources.  * [awesome-plotters](https://github.com/beardicus/awesome-plotters) â€“ Computer-controlled drawing machines and other visual art robots.  * [awesome-podcasts](https://github.com/Ghosh/awesome-podcasts) by @Ghosh â€“ Podcasts for designers, developers, product managers, entrepreneurs and hustlers    - http://podcasts.surge.sh/  * [awesome-podcasts](https://github.com/rShetty/awesome-podcasts) by @rShetty â€“ Important Podcasts for software engineers.  * [awesome-pokemon](https://github.com/tobiasbueschel/awesome-pokemon) â€“ PokÃ©mon & PokÃ©mon Go  * [awesome-polymer](https://github.com/Granze/awesome-polymer) â€“ [Polymer Project](https://www.polymer-project.org/)  * [awesome-postcss](https://github.com/jdrgomes/awesome-postcss) â€“ [PostCSS](https://postcss.org/) CSS processor  * [awesome-postgres](https://github.com/dhamaniasad/awesome-postgres)  * [awesome-power-mode](https://github.com/codeinthedark/awesome-power-mode)  * [awesome-powershell](https://github.com/janikvonrotz/awesome-powershell)  * [awesome-preact](https://github.com/preactjs/awesome-preact) â€“ [Preact](https://github.com/preactjs/preact) JavaScript framework  * [awesome-prisma](https://github.com/catalinmiron/awesome-prisma) â€“ [Prisma](https://www.prisma.io/) GraphQL library  * [awesome-privacy](https://github.com/pluja/awesome-privacy) â€“ Services and alternatives that respect your privacy because PRIVACY MATTERS.  * [awesome-product-design](https://github.com/teoga/awesome-product-design) by @teoga â€“ Bookmarks, resources, articles for product designers.  * [awesome-product-design](https://github.com/ttt30ga/awesome-product-design) by @ttt30ga â€“ Resources for product designers.  * [awesome-product-management](https://github.com/dend/awesome-product-management) â€“ Resources for product/program managers to learn and grow.  * [awesome-productivity](https://github.com/jyguyomarch/awesome-productivity) â€“ Delightful productivity resources.  * [awesome-ProductManager](https://github.com/hugo53/awesome-ProductManager) â€“ Books and tools for Product Managers.  * [awesome-programming-for-kids](https://github.com/HollyAdele/awesome-programming-for-kids) â€“ Teaching kids programming  * [awesome-progressive-web-apps](https://github.com/TalAter/awesome-progressive-web-apps) â€“ Progressive Web Apps (PWA)  * [awesome-projects-boilerplates](https://github.com/melvin0008/awesome-projects-boilerplates)  * [awesome-prolog](https://github.com/klaussinani/awesome-prolog) â€“ Prolog logic programming language  * [awesome-prometheus](https://github.com/roaldnefs/awesome-prometheus) â€“ [Prometheus](https://prometheus.io/) monitoring system  * [awesome-prometheus-alerts](https://github.com/samber/awesome-prometheus-alerts) â€“ Prometheus alerting rules    - https://awesome-prometheus-alerts.grep.to  * [awesome-promises](https://github.com/wbinnssmith/awesome-promises) â€“ JavaScript Promises.  * [awesome-public-datasets](https://github.com/awesomedata/awesome-public-datasets) by @awesomedata â€“ (Large-scale) public datasets on the Internet.    - [source data](https://github.com/awesomedata/apd-core)  * [awesome-puppet](https://github.com/rnelson0/awesome-puppet)  * [awesome-pure-css-no-javascript](https://github.com/Zhangjd/awesome-pure-css-no-javascript) _In Chinese_  * [awesome-purescript](https://github.com/passy/awesome-purescript)  * [awesome-pyramid](https://github.com/uralbash/awesome-pyramid) â€“ Resources for Pyramid Python web framework.  * [awesome-python](https://github.com/kevmo/awesome-python) by @kevmo  * [awesome-python](https://github.com/vinta/awesome-python) by @vinta  * [awesome-python-cn](https://github.com/jobbole/awesome-python-cn) _In Chinese_  * [awesome-python-data-science](https://github.com/krzjoa/awesome-python-data-science)  * [awesome-python-in-education](https://github.com/quobit/awesome-python-in-education)  * [awesome-python-models](https://github.com/grundic/awesome-python-models) â€“ List of ORMs, models, schemas, serializers, etc. libraries  for python.  * [awesome-python-scientific-audio](https://github.com/faroit/awesome-python-scientific-audio) â€“ Python software and packages related to scientific research in audio  * [awesome-python-talks](https://github.com/jhermann/awesome-python-talks) â€“ Videos related to Python, with a focus on training and gaining hands-on experience.  * [awesome-python-typing](https://github.com/typeddjango/awesome-python-typing) â€“ Python types, stubs, plugins, and tools to work with them.  * [Awesome-pytorch-list](https://github.com/bharathgs/Awesome-pytorch-list) â€“ [PyTorch](https://pytorch.org/) Python machine learning framework.  * [awesome-qa](https://github.com/seriousran/awesome-qa) â€“ [Question Answering](https://en.wikipedia.org/wiki/Question_answering) systems automatically answer questions asked in a natural language  * [awesome-qsharp](https://github.com/ebraminio/awesome-qsharp) â€“ [Q#](https://docs.microsoft.com/en-us/quantum/) quantum programming language  * [awesome-qt](https://github.com/JesseTG/awesome-qt) by @JesseTG â€“ Qt framework  * [awesome-qt](https://github.com/skhaz/awesome-qt) by @skhaz â€“ Qt framework  * [awesome-quantified-self](https://github.com/woop/awesome-quantified-self) â€“ Devices, Wearables, Applications, and Platforms for Self Tracking  * [awesome-quantum-computing](https://github.com/desireevl/awesome-quantum-computing) â€“ Quantum computing learning and developing resources.  * [awesome-R](https://github.com/qinwf/awesome-R)  * [awesome-radio](https://github.com/kyleterry/awesome-radio) â€“ Radio and citizens band (CB) radio resources.  * [awesome-rails](https://github.com/dpaluy/awesome-rails) by @dpaluy  * [awesome-rails](https://github.com/gramantin/awesome-rails) by @gramantin â€“ Projects and sites made with Rails.  * [awesome-rails](https://github.com/ruby-vietnam/awesome-rails) by @ruby-vietnam â€“ Rails libraries/app examples/ebooks/tutorials/screencasts/magazines/news.  * [awesome-rails-gem](https://github.com/hothero/awesome-rails-gem) â€“ Ruby Gems for Rails development.  * [awesome-random-forest](https://github.com/kjw0612/awesome-random-forest) â€“ Decision forest, tree-based methods, including random forest, bagging, and boosting.  * [awesome-raspberry-pi](https://github.com/blackout314/awesome-raspberry-pi) by @blackout314    - http://blackout314.github.io/awesome-raspberry-pi/  * [awesome-raspberry-pi](https://github.com/thibmaek/awesome-raspberry-pi) by @thibmaek â€“ Raspberry Pi tools, projects, images and resources  * [awesome-react](https://github.com/enaqx/awesome-react) â€“ ReactJS tools, resources, videos.  * [awesome-react-components](https://github.com/brillout/awesome-react-components) â€“ React Components & Libraries.  * [awesome-react-hooks](https://github.com/glauberfc/awesome-react-hooks) â€“ React Hooks  * [awesome-react-native](https://github.com/jondot/awesome-react-native)    - http://www.awesome-react-native.com  * [awesome-react-state-management](https://github.com/olegrjumin/awesome-react-state-management)  * [awesome-react-state-management-tools](https://github.com/cs01/awesome-react-state-management-tools)  * [awesome-readme](https://github.com/matiassingers/awesome-readme) â€“ READMEs examples and best practices  * [awesome-reasonml](https://github.com/vramana/awesome-reasonml) â€“ [ReasonML](https://reasonml.github.io/), [BuckleScript](https://bucklescript.github.io/) and [OCaml](https://ocaml.org/) programming languages.  * [awesome-recommender-system](https://github.com/Geek4IT/awesome-recommender-system) â€“ Recommender System frameworks, libraries and software.  * [awesome-recursion-schemes](https://github.com/passy/awesome-recursion-schemes)  * [awesome-redux](https://github.com/brillout/awesome-redux) by @brillout â€“ Redux Libraries & Learning Material    - https://devarchy.com/redux  * [awesome-redux](https://github.com/xgrommx/awesome-redux) by @xgrommx â€“ [Redux](https://github.com/rackt/redux) web application state container  * [awesome-refinerycms](https://github.com/refinerycms-contrib/awesome-refinerycms) â€“ [Refinery](https://www.refinerycms.com/) Ruby on Rails CMS  * [awesome-regex](https://github.com/aloisdg/awesome-regex) â€“ Regular expressions  * [awesome-regression-testing](https://github.com/mojoaxel/awesome-regression-testing) â€“ Visual regression testing  * [awesome-relay](https://github.com/expede/awesome-relay) â€“ [Relay](https://relay.dev/) JavaScript framework for React and GraphQL  * [awesome-reMarkable](https://github.com/reHackable/awesome-reMarkable) â€“ [reMarkable](https://remarkable.com/) e-ink tablet.  * [awesome-remote-job](https://github.com/lukasz-madon/awesome-remote-job) â€“ Remote companies and other resources.  * [awesome-RemoteWork](https://github.com/hugo53/awesome-RemoteWork) â€“ Books and links about and for remote work.  * [awesome-research](https://github.com/emptymalei/awesome-research) â€“ Tools to help you with research/life    - http://openmetric.org/tool/  * [awesome-rest](https://github.com/marmelab/awesome-rest) â€“ Great resources about RESTful API architecture, development, test, and performance  * [awesome-rethinkdb](https://github.com/d3viant0ne/awesome-rethinkdb) â€“ [RethinkDB](https://rethinkdb.com/) realtime database  * [awesome-retrospectives](https://github.com/josephearl/awesome-retrospectives) â€“ Facilitating and learning about retrospectives.  * [awesome-ripple](https://github.com/vhpoet/awesome-ripple) â€“ [Ripple](https://ripple.com/) cryptocurrency  * [awesome-rl](https://github.com/aikorea/awesome-rl) â€“ Reinforcement Learning.  * [awesome-rl-for-cybersecurity](https://github.com/Limmen/awesome-rl-for-cybersecurity) â€“ Reinforcement learning applied to cyber security.  * [awesome-rnn](https://github.com/kjw0612/awesome-rnn) â€“ Recurrent Neural Networks.  * [awesome-roadmaps](https://github.com/liuchong/awesome-roadmaps) â€“ Skills roadmaps for software development  * [awesome-roam](https://github.com/roam-unofficial/awesome-roam) â€“ Roam Research networked note-taking  * [awesome-robotics](https://github.com/Kiloreux/awesome-robotics)  * [awesome-ros2](https://github.com/fkromer/awesome-ros2) â€“ [Robot Operating System](http://www.ros.org/)    - https://fkromer.github.io/awesome-ros2  * [awesome-roslyn](https://github.com/ironcev/awesome-roslyn) â€“ Roslyn .NET Compiler Platform  * [awesome-rshiny](https://github.com/grabear/awesome-rshiny) â€“ A curated list of resources for the R shiny package.    - https://grabear.github.io/awesome-rshiny/  * [awesome-ruby](https://github.com/markets/awesome-ruby) by @markets    - http://awesome-ruby.com/  * [awesome-ruby](https://github.com/Sdogruyol/awesome-ruby) by @Sdogruyol  * [awesome-ruby-ast](https://github.com/rajasegar/awesome-ruby-ast) â€“ Abstract Syntax Trees (AST) in Ruby  * [AwesomeRubyist/awesome_podcast_list](https://github.com/AwesomeRubyist/awesome_podcast_list) â€“ Podcasts about Ruby and development, also in Russian.  * [AwesomeRubyist/awesome_reading_list](https://github.com/AwesomeRubyist/awesome_reading_list) â€“ Books about Ruby and Rails.  * [AwesomeRubyist/awesome_resource_list](https://github.com/AwesomeRubyist/awesome_resource_list) â€“ Resources for Ruby and Rails.  * [awesome-rubymotion](https://github.com/motion-open-source/awesome-rubymotion) â€“ [RubyMotion](http://www.rubymotion.com/), cross-platform development in Ruby    - http://motion-open-source.github.io/awesome-rubymotion/  * [awesome-rust](https://github.com/rust-unofficial/awesome-rust)  * [awesome-rxjava](https://github.com/eleventigers/awesome-rxjava) â€“ RxJava, reactive programming library  * [awesome-salesforce](https://github.com/mailtoharshit/awesome-salesforce) â€“ Salesforce Platform Resources  * [awesome-saltstack](https://github.com/hbokh/awesome-saltstack) â€“ [SaltStack](https://www.saltstack.com/) configuration management  * [awesome-sarl](https://github.com/sarl/awesome-sarl) â€“ Resources for [SARL](http://www.sarl.io/) Agent-Oriented Programming Language.  * [awesome-SAS](https://github.com/huyingjie/awesome-SAS) â€“ [SAS](https://www.sas.com/) analysis system  * [awesome-sass](https://github.com/Famolus/awesome-sass) by @Famolus â€“ Sass and SCSS CSS preprocessor  * [awesome-sass](https://github.com/HugoGiraudel/awesome-sass) by @HugoGiraudel â€“ Sass and SCSS CSS preprocessor  * [awesome-satellite-imagery-datasets](https://github.com/chrieke/awesome-satellite-imagery-datasets) â€“ Satellite imagery datasets with annotations for computer vision and deep learning.  * [awesome-scala](https://github.com/lauris/awesome-scala) â€“ Scala programming language  * [awesome-scala-native](https://github.com/tindzk/awesome-scala-native) â€“ [Scala Native](http://www.scala-native.org) compiler  * [awesome-scalability](https://github.com/binhnguyennus/awesome-scalability) â€“ The Patterns of Scalable, Reliable, and Performant Large-Scale Systems  * [awesome-scientific-computing](https://github.com/nschloe/awesome-scientific-computing) â€“ Software for numerical analysis  * [awesome-scientific-writing](https://github.com/writing-resources/awesome-scientific-writing) â€“ Tools, demos and resources to go beyond LaTeX.  * [awesome-sdn](https://github.com/sdnds-tw/awesome-sdn) â€“ Software Defined Network (SDN)  * [awesome-sec-talks](https://github.com/PaulSec/awesome-sec-talks) â€“ Security talks.  * [awesome-security](https://github.com/sbilly/awesome-security) â€“ Software, libraries, documents, books, resources and cool stuff about security.  * [awesome-selenium](https://github.com/christian-bromann/awesome-selenium)  * [awesome-selfhosted](https://github.com/awesome-selfhosted/awesome-selfhosted) â€“ Network services and web applications which can be hosted locally.  * [awesome-semantic-web](https://github.com/semantalytics/awesome-semantic-web) â€“ Semantic web and linked data  * [awesome-seo](https://github.com/teles/awesome-seo) â€“ SEO (Search Engine Optimization) links.    - http://jotateles.com.br/awesome-seo/  * [awesome-serverless](https://github.com/anaibol/awesome-serverless) by @anaibol â€“ Services, solutions and resources for serverless / nobackend applications.  * [awesome-serverless](https://github.com/pmuens/awesome-serverless) by @pmuens â€“ Resources related to serverless computing and serverless architectures.  * [awesome-serverless-security](https://github.com/puresec/awesome-serverless-security) â€“ Serverless security resources  * [awesome-service-workers](https://github.com/TalAter/awesome-service-workers) â€“ Service Workers for Progressive Web Applications  * [awesome-servicefabric](https://github.com/lawrencegripper/awesome-servicefabric) â€“ Azure [Service Fabric](https://docs.microsoft.com/en-us/azure/service-fabric/) distributed services platform  * [awesome-services](https://github.com/indrasantosa/awesome-services) â€“ Services that make a painful programmer's life easier.  * [awesome-sharepoint](https://github.com/BSUG/awesome-sharepoint) by @BSUG  * [awesome-SharePoint](https://github.com/siaf/awesome-SharePoint) by @siaf  * [awesome-sheet-music](https://github.com/ad-si/awesome-sheet-music) â€“ Sheet music software, libraries and resources.  * [awesome-shell](https://github.com/alebcay/awesome-shell) â€“ Command-line frameworks, toolkits, guides and gizmos.  * [awesome-sites](https://github.com/Gherciu/awesome-sites) â€“ Various websites with resources for development, graphics, and learning  * [awesome-sketch](https://github.com/diessica/awesome-sketch) â€“ Guides, articles, videos about [Sketch 3](http://www.sketchapp.com/).  * [awesome-slack](https://github.com/filipelinhares/awesome-slack) by @filipelinhares â€“ Communities powered by Slack.  * [awesome-slack](https://github.com/matiassingers/awesome-slack) by @matiassingers  * [awesome-slack-communities](https://github.com/radermacher/awesome-slack-communities) â€“ Public Slack Communities.  * [awesome-smart-tv](https://github.com/vitalets/awesome-smart-tv) â€“ Smart TV apps  * [awesome-software-architecture](https://github.com/mehdihadeli/awesome-software-architecture) by @mehdihadeli â€“ Software architecture, patterns, and principles.  * [awesome-software-architecture](https://github.com/simskij/awesome-software-architecture) by @simskij â€“ Design, reason around and build software using architectural patterns and methods  * [awesome-software-craftsmanship](https://github.com/benas/awesome-software-craftsmanship) â€“ [Software craftsmanship](http://manifesto.softwarecraftsmanship.org/) resources to help learn the craft.  * [awesome-software-patreons](https://github.com/uraimo/awesome-software-patreons) â€“ Programmers and software-related Patreon accounts.  * [awesome-software-quality](https://github.com/ligurio/awesome-software-quality) â€“ Free software testing books.  * [awesome-solid](https://github.com/kustomzone/awesome-solid) â€“ [Solid](https://solidproject.org/) (social linked data) project.  * [awesome-sound](https://github.com/hwclass/awesome-sound) â€“ Sound & audio libraries and resources.  * [awesome-space](https://github.com/elburz/awesome-space) â€“ Outer Space  * [awesome-space-books](https://github.com/Hunter-Github/awesome-space-books) â€“ Space exploration related book  * [awesome-spanish-nlp](https://github.com/dav009/awesome-spanish-nlp) â€“ Linguistic Resources for doing NLP & CL on Spanish  * [awesome-spark](https://github.com/awesome-spark/awesome-spark) â€“ Apache Spark packages and resources.  * [awesome-speakers](https://github.com/karlhorky/awesome-speakers) â€“ Speakers in the programming and design communities  * [awesome-sphinxdoc](https://github.com/yoloseem/awesome-sphinxdoc) â€“ Tools for Sphinx Python Documentation Generator.  * [awesome-split-keyboards](https://github.com/diimdeep/awesome-split-keyboards) â€“ Ergonomic split keyboards.  * [awesome-sqlalchemy](https://github.com/dahlia/awesome-sqlalchemy) â€“ Extra libraries for SQLAlchemy, a Python ORM.  * [awesome-sre](https://github.com/dastergon/awesome-sre) â€“ Site Reliability and Production Engineering    - https://sre.xyz  * [awesome-ssh](https://github.com/moul/awesome-ssh)    - https://manfred.life/awesome-ssh  * [awesome-stacks](https://github.com/stackshareio/awesome-stacks) â€“ Tech stacks for building different applications & features    - https://awesomestacks.dev  * [awesome-standard](https://github.com/standard/awesome-standard) â€“ Documenting the explosion of packages in the [standard](http://standardjs.com/) (JavaScript code style) ecosystem.  * [awesome-stars](https://github.com/lichunqiang/awesome-stars) _In Chinese_ â€“ Useful libraries with personal remarks.  * [awesome-startup](https://github.com/KrishMunot/awesome-startup) â€“ Resources to build your own startup  * [awesome-static-generators](https://github.com/myles/awesome-static-generators) â€“ Static web site generators.  * [awesome-static-website-services](https://github.com/agarrharr/awesome-static-website-services)  * [awesome-steam](https://github.com/scholtzm/awesome-steam) â€“ Steam video games distribution platform development  * [awesome-storybook](https://github.com/lauthieb/awesome-storybook) â€“ [Storybook](https://storybook.js.org/) UI web development  * [awesome-streaming](https://github.com/manuzhang/awesome-streaming) â€“ Streaming frameworks, applications, etc  * [awesome-styleguides](https://github.com/RichardLitt/awesome-styleguides)  * [awesome-stylelint](https://github.com/stylelint/awesome-stylelint) â€“ [Stylelint](https://stylelint.io/) CSS linter.  * [awesome-sustainable-technology](https://github.com/protontypes/awesome-sustainable-technology) â€“ Open technology projects sustaining stable climate, energy supply and vital natural resources.    - https://opensustain.tech/  * [awesome-svelte](https://github.com/CalvinWalzel/awesome-svelte) â€“ [Svelte](https://svelte.dev/) framework  * [awesome-svelte-resources](https://github.com/ryanatkn/awesome-svelte-resources) â€“ [Svelte](https://svelte.dev/) framework  * [awesome-svg](https://github.com/willianjusten/awesome-svg)  * [awesome-swedish-opensource](https://github.com/gurre/awesome-swedish-opensource) â€“ Open-source projects from Swedes  * [awesome-swift](https://github.com/matteocrippa/awesome-swift) by @matteocrippa  * [awesome-swift](https://github.com/Wolg/awesome-swift) by @Wolg  * [awesome-swift-and-tutorial-resources](https://github.com/MaxChen/awesome-swift-and-tutorial-resources) â€“ Swift programming language  * [Awesome-Swift-Education](https://github.com/hsavit1/Awesome-Swift-Education) â€“ Learn some Swift  * [Awesome-Swift-Playgrounds](https://github.com/uraimo/Awesome-Swift-Playgrounds) â€“ Swift Playgrounds  * [awesome-symfony](https://github.com/sitepoint-editors/awesome-symfony) â€“ [Symfony PHP framework](http://symfony.com/) bundles, utilities and resources.  * [awesome-symfony-education](https://github.com/pehapkari/awesome-symfony-education) â€“ Symfony PHP framework learning resources  * [awesome-sysadmin](https://github.com/kahun/awesome-sysadmin) by @kahun â€“ Open source sysadmin resources.  * [awesome-sysadmin](https://github.com/n1trux/awesome-sysadmin) by @n1trux â€“ Open source sysadmin resources.  * [awesome-system-design](https://github.com/madd86/awesome-system-design) â€“ Distributed systems design  * [awesome-system-fonts](https://github.com/mrmrs/awesome-system-fonts) â€“ Websites that use system fonts.  * [awesome-taglines](https://github.com/miketheman/awesome-taglines) â€“ Software taglines  * [awesome-tailwindcss](https://github.com/aniftyco/awesome-tailwindcss) â€“ [Tailwind CSS](https://tailwindcss.com/)    - https://git.io/awesome-tailwindcss  * [awesome-talks](https://github.com/JanVanRyswyck/awesome-talks)  * [awesome-tap](https://github.com/sindresorhus/awesome-tap) â€“ Test Anything Protocol  * [awesome-tech-blogs](https://github.com/markodenic/awesome-tech-blogs) â€“ Technical blogs    - https://tech-blogs.dev/  * [awesome-tech-conferences](https://github.com/trstringer/awesome-tech-conferences) â€“ Upcoming technical conferences.  * [awesome-tech-videos](https://github.com/lucasviola/awesome-tech-videos) â€“ Tech conferences from youtube, vimeo, etc for us to get inspired  * [awesome-technical-writing](https://github.com/BolajiAyodeji/awesome-technical-writing)  * [awesome-telegram](https://github.com/ebertti/awesome-telegram) â€“ Telegram messaging service  * [awesome-tensorflow](https://github.com/jtoy/awesome-tensorflow) â€“ [TensorFlow](https://www.tensorflow.org/) machine intelligence library.  * [awesome-terraform](https://github.com/shuaibiyy/awesome-terraform) â€“ HashiCorp Terraform  * [awesome-test-automation](https://github.com/atinfo/awesome-test-automation)    - http://automated-testing.info  * [awesome-testing](https://github.com/TheJambo/awesome-testing) â€“ Testing resources    - https://git.io/v1hSm  * [awesome-text-editing](https://github.com/dok/awesome-text-editing) â€“ Text editing resources and libraries for the web  * [awesome-textpattern](https://github.com/drmonkeyninja/awesome-textpattern) â€“ Textpattern plugins and resources  * [awesome-themes](https://github.com/AdrienTorris/awesome-themes) â€“ Web themes and templates  * [awesome-tikz](https://github.com/xiaohanyu/awesome-tikz) â€“ [TikZ](https://pgf-tikz.github.io/) graph drawing package for TeX/LaTeX/ConTeXt  * [awesome-tinkerpop](https://github.com/mohataher/awesome-tinkerpop) â€“ [Apache TinkerPop](http://tinkerpop.apache.org/) graph computing framework  * [awesome-token-sale](https://github.com/holographicio/awesome-token-sale) â€“ Token sale / ICO resources  * [awesome-torch](https://github.com/carpedm20/awesome-torch) â€“ Tutorials, projects and communities for [Torch](http://torch.ch/), a scientific computing framework for LuaJIT.  * [awesome-transit](https://github.com/CUTR-at-USF/awesome-transit) â€“ Transit APIs, apps, datasets, research, and software  * [awesome-tunneling](https://github.com/anderspitman/awesome-tunneling) â€“ [Ngrok](https://ngrok.com/) alternatives and other ngrok-like tunneling software and services. Focus on self-hosting.  * [awesome-twilio](https://github.com/Twilio-org/awesome-twilio) â€“ Curated repository of useful and generally awesome Twilio tools and technologies  * [AwesomeTwitterAccounts](https://github.com/yask123/AwesomeTwitterAccounts) â€“ Twitter accounts, organised by programming communities.  * [awesome-typescript](https://github.com/dzharii/awesome-typescript) by @dzharii â€“ TypeScript programming language  * [awesome-typescript](https://github.com/ellerbrock/awesome-typescript) by @ellerbrock    - https://ellerbrock.github.io/awesome-typescript  * [awesome-typescript-projects](https://github.com/brookshi/awesome-typescript-projects) â€“ TypeScript open-source projects  * [awesome-typography](https://github.com/Jolg42/awesome-typography) â€“ Resources on OpenType & TrueType.  * [awesome-ui-component-library](https://github.com/anubhavsrivastava/awesome-ui-component-library) â€“ Framework component libraries for UI styles/toolkit    - https://anubhavsrivastava.github.io/awesome-ui-component-library/  * [awesome-umbraco](https://github.com/umbraco-community/awesome-umbraco) â€“ Resources for Umbraco 7, a .NET CMS.  * [Awesome-Unicode](https://github.com/Wisdom/Awesome-Unicode) â€“ Unicode tidbits, packages and resources.    - https://git.io/Awesome-Unicode  * [awesome-unity](https://github.com/RyanNielson/awesome-unity) â€“ Assets and resources for [Unity](http://unity3d.com/) game engine.  * [awesome-unix](https://github.com/sirredbeard/Awesome-UNIX)  * [awesome-userscripts](https://github.com/brunocvcunha/awesome-userscripts)  * [awesome-uses](https://github.com/wesbos/awesome-uses) â€“ `/uses` pages detailing developer setups, gear, software and configs.    - https://uses.tech  * [awesome-uxn](https://github.com/hundredrabbits/awesome-uxn) â€“ The [Uxn](https://100r.co/site/uxn.html) ecosystem is a personal computing playground, created to host small tools and games, programmable in its own unique assembly language.  * [awesome-v](https://github.com/vlang/awesome-v) â€“ [V](https://vlang.io/) programming language  * [awesome-vagrant](https://github.com/iJackUA/awesome-vagrant)  * [awesome-vanilla-js](https://github.com/davidhund/awesome-vanilla-js) â€“ Plainâ€”â€˜Vanillaâ€™â€”JavaScript  * [awesome-vapor](https://github.com/Cellane/awesome-vapor) â€“ [Vapor](https://vapor.codes/) Swift web framework  * [awesome-vector-tiles](https://github.com/mapbox/awesome-vector-tiles) â€“ Implementations of the [Mapbox Vector Tile](https://www.mapbox.com/developers/vector-tiles/) specification.  * [awesome-vehicle-security](https://github.com/jaredthecoder/awesome-vehicle-security) â€“ Vehicle security and car hacking  * [awesome-vhdl](https://github.com/VHDL/awesome-vhdl) â€“ VHDL hardware description language  * [awesome-vim](https://github.com/akrawchyk/awesome-vim) by @akrawchyk  * [awesome-vim](https://github.com/matteocrippa/awesome-vim) by @matteocrippa  * [awesome-vite](https://github.com/vitejs/awesome-vite) â€“ [Vite](https://vitejs.dev/) front-end build tooling.  * [awesome-vlc](https://github.com/mfkl/awesome-vlc) â€“ [VideoLAN VLC](https://www.videolan.org/) multimedia player and framework.  * [awesome-volt](https://github.com/heri/awesome-volt) â€“ [Volt](http://voltframework.com/) Ruby web framework.  * [awesome-vorpal](https://github.com/vorpaljs/awesome-vorpal) â€“ [Vorpal](http://vorpal.js.org/) Node.js interactive CLI framework  * [awesome-vscode](https://github.com/viatsko/awesome-vscode) â€“ Visual Studio Code    - https://viatsko.github.io/awesome-vscode/  * [awesome-vue](https://github.com/vuejs/awesome-vue) â€“ Resources for [Vue.js](http://vuejs.org/) JavaScript UI library.  * [awesome-vulkan](https://github.com/vinjn/awesome-vulkan) â€“ [3D graphics and compute API](https://www.khronos.org/vulkan/)  * [awesome-wagtail](https://github.com/springload/awesome-wagtail) â€“ [Wagtail](https://wagtail.io/) Python CMS  * [awesome-wasm](https://github.com/mbasso/awesome-wasm) â€“ WebAssembly  * [awesome-watchos](https://github.com/yenchenlin/awesome-watchos) â€“ Apple watchOS  * [awesome-web-animation](https://github.com/sergey-pimenov/awesome-web-animation) â€“ Web animation libraries, books, apps etc.    - https://awesome-web-animation.netlify.com  * [awesome-web-archiving](https://github.com/iipc/awesome-web-archiving) â€“ Getting started with web archiving  * [awesome-web-design](https://github.com/nicolesaidy/awesome-web-design) â€“ Resources for digital designers.  * [awesome-web-effect](https://github.com/lindelof/awesome-web-effect) â€“ Exquisite and compact web page effects.  * [awesome-web-scraping](https://github.com/lorien/awesome-web-scraping) â€“ tools and programming libraries related to web scraping and data processing  * [awesome-web-security](https://github.com/qazbnm456/awesome-web-security)    - https://awesomelists.top/#/repos/qazbnm456/awesome-web-security  * [awesome-webaudio](https://github.com/notthetup/awesome-webaudio) â€“ WebAudio packages and resources.  * [awesome-webauthn](https://github.com/herrjemand/awesome-webauthn) â€“ WebAuthn/FIDO2  * [awesome-webcomponents](https://github.com/obetomuniz/awesome-webcomponents)  * [Awesome-WebExtensions](https://github.com/fregante/Awesome-WebExtensions) â€“ WebExtensions development.  * [awesome-webgl](https://github.com/sjfricke/awesome-webgl) â€“ WebGL libraries, resources and much more  * [awesome-webpack](https://github.com/webpack-contrib/awesome-webpack) â€“ Webpack resources, libraries and tools  * [awesome-webpack-perf](https://github.com/iamakulov/awesome-webpack-perf) â€“ Webpack tools for web performance  * [awesome-webservice](https://github.com/wapmorgan/awesome-webservice) â€“ Web and cloud services, SaaS.  * [awesome-websockets](https://github.com/facundofarias/awesome-websockets) â€“ Websocket libraries and resources.  * [awesome-webvis](https://github.com/rajsite/awesome-webvis) â€“ [WebVI](http://www.webvi.io/) examples made using [LabVIEW](http://www.ni.com/en-us/support/software-technology-preview.html) systems engineering software.  * [awesome-weekly](https://github.com/jondot/awesome-weekly) â€“ Quality weekly subscription newsletters from the software world.  * [awesome-wicket](https://github.com/PhantomYdn/awesome-wicket) â€“ [Apache Wicket](http://wicket.apache.org/) Java web framework  * [awesome-wikipedia](https://github.com/emijrp/awesome-wikipedia) â€“ Wikipedia-related frameworks, libraries, software, datasets and references.  * [Awesome-Windows/Awesome](https://github.com/Awesome-Windows/Awesome) â€“ Applications and tools for Windows.  * [awesome-wordpress](https://github.com/dropndot/awesome-wordpress) by @dropndot  * [awesome-wordpress](https://github.com/endel/awesome-wordpress) by @endel  * [awesome-wordpress](https://github.com/miziomon/awesome-wordpress) by @miziomon  * [awesome-workflow-engines](https://github.com/meirwah/awesome-workflow-engines) â€“ Open source workflow engines  * [awesome-workshopper](https://github.com/therebelrobot/awesome-workshopper)  * [awesome-wpo](https://github.com/davidsonfellipe/awesome-wpo) â€“ Web Performance Optimization  * [awesome-xamarin](https://github.com/XamSome/awesome-xamarin) by @XamSome â€“ [Xamarin](https://visualstudio.microsoft.com/xamarin/) mobile application framework  * [awesome-xamarin](https://github.com/XamSome/awesome-xamarin) by @XamSome â€“ Interesting libraries/tools for Xamarin mobile projects  * [awesome-xcode-plugin](https://github.com/aashishtamsya/awesome-xcode-scripts) â€“ XCode IDE scripts  * [awesome-xmpp](https://github.com/bluszcz/awesome-xmpp) â€“ Curated list of awesome XMPP protocol resources.  * [awesome-yamada](https://github.com/supermomonga/awesome-yamada) â€“ Dancing yamada  * [awesome-yii](https://github.com/iJackUA/awesome-yii) â€“ Yii PHP framework extensions, tutorials and other nice things.  * [awesome-zig](https://github.com/nrdmn/awesome-zig) â€“ [Zig](https://ziglang.org/) programming language.  * [awesome-zsh-plugins](https://github.com/unixorn/awesome-zsh-plugins)  * [awesomo](https://github.com/lk-geimfari/awesomo) â€“ Open source projects in various languages.  * [craftcms/awesome](https://github.com/craftcms/awesome) â€“ [Craft CMS](https://craftcms.com/)  * [not-awesome-es6-classes](https://github.com/petsel/not-awesome-es6-classes) â€“ Why ES6 (aka ES2015) classes are NOT awesome    - https://matthias-endler.de/awesome-static-analysis/      ## Lists of lists    * [awesome](https://github.com/sindresorhus/awesome) â€“ A curated list of awesome lists.  * [awesome-all](https://github.com/bradoyler/awesome-all) â€“ A curated list of awesome lists of awesome frameworks, libraries and software  * [awesome-android-awesomeness](https://github.com/yongjhih/awesome-android-awesomeness#awesomeness)  * [awesome-awesome](https://github.com/aligoren/awesome-awesome) by @aligoren â€“ List of GitHub Lists  * [awesome-awesome](https://github.com/emijrp/awesome-awesome) by @emijrp â€“ A curated list of awesome curated lists of many topics.  * [awesome-awesome](https://github.com/erichs/awesome-awesome) by @erichs â€“ A curated list of awesome curated lists! Inspired by inspiration.  * [awesome-awesome](https://github.com/oyvindrobertsen/awesome-awesome) by @oyvindrobertsen â€“ A curated list of curated lists of libraries, resources and shiny things for various languages.  * [awesome-awesomeness](https://github.com/bayandin/awesome-awesomeness) â€“ A curated list of awesome awesomeness  * [awesome-awesomeness-zh_CN](https://github.com/justjavac/awesome-awesomeness-zh_CN) _In Chinese_ â€“ ä¸­æ–‡ç‰ˆawesome list ç³»åˆ—æ–‡ç«   * [awesome-awesomes](https://github.com/fleveque/awesome-awesomes) â€“ Awesome collection of awesome lists of libraries, tools, frameworks and software for any programming language  * [awesome-collection](https://github.com/flyhigher139/awesome-collection) â€“ A list of awesome repos.  * [Awesome-Hacking](https://github.com/Hack-with-Github/Awesome-Hacking) â€“ Lists for hackers, pentesters and security researchers.  * [awesome-lists](https://github.com/pshah123/awesome-lists) â€“ A curated list for your curated lists, including other curated lists of curated lists that may or may not contain other curated lists.  * [curated-lists](https://github.com/learn-anything/curated-lists)  * [delightful](https://codeberg.org/teaserbot-labs/delightful) â€“ Home of delightful curated lists of free software, open science and information sources.  * [getAwesomeness](https://github.com/panzhangwang/getAwesomeness) â€“ Explorer designed for curated awesome list hosted on Github    - https://getawesomeness.herokuapp.com/  * [list-of-lists](https://github.com/cyrusstoller/list-of-lists) â€“ A meta list of lists of useful open source projects and developer tools.  * [ListOfGithubLists](https://github.com/asciimoo/ListOfGithubLists) â€“ List of github lists  * [must-watch-list](https://github.com/adrianmoisey/must-watch-list) â€“ List of must-watch lists.  * [this one](https://github.com/jnv/lists)  * [wiki](https://github.com/huguangju/wiki) _In Chinese_ â€“ A curated list of awesome lists.      ### Lists of lists of lists    * [awesome-awesome-awesome](https://github.com/geekan/awesome-awesome-awesome) by @geekan â€“ An awesome-awesome list.  * [awesome-awesome-awesome](https://github.com/t3chnoboy/awesome-awesome-awesome) by @t3chnoboy â€“ A a curated list of curated lists of awesome lists.  * [awesomecubed](https://github.com/hunterboerner/awesomecubed) â€“ A curated list of awesome awesomeness awesomenesses.  * [lologl](https://github.com/yaph/lologl) â€“ List of Lists of Github Lists.  * [meta-awesome](https://github.com/PatrickMcDonald/meta-awesome)  * [the one above](#lists-of-lists)      #### Lists of lists of lists of lists    * [awesome-awesome-awesome-awesome](https://github.com/sindresorhus/awesome-awesome-awesome-awesome)  * [the one above](#lists-of-lists-of-lists)      ##### Lists of lists of lists of lists of lists    * [awesome-power-of-5](https://github.com/therebelbeta/awesome-power-of-5)  * [the one above](#lists-of-lists-of-lists-of-lists)      ###### Lists of lists of lists of lists of lists of lists    * [the one above](#lists-of-lists-of-lists-of-lists-of-lists)      ###### Lists of lists of lists of lists of lists of lists of lists    * [awesome-awesome-awesome-awesome-awesome-awesome-awesome](https://github.com/sparanoid/awesome-awesome-awesome-awesome-awesome-awesome-awesome)  * [the one above](#lists-of-lists-of-lists-of-lists-of-lists-of-lists)    <!-- lists-end -->    ## License    [![CC0 Public Domain](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)    Social preview photo by [Eli Francis](https://unsplash.com/@elifrancis?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/books-clutter?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText). """
Big data;https://github.com/rudderlabs/rudder-server;"""<p align=""center"">    <a href=""https://rudderstack.com/"">      <img src=""resources/logo.png"">    </a>  </p>    <p align=""center""><b>The Customer Data Platform for Developers</b></p>    <p align=""center"">    <a href=""https://rudderstack.com/"">      <img src=""https://codebuild.us-east-1.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiT01EQkVPc0NBbDJLV2txTURidkRTMTNmWFRZWUY2dEtia3FRVmFXdXhWeUwzaC9aV3dsWWNNT0NwaVZKd1hKTFVMazB2cDQ5UHlaZTgvbFRER3R5SXRvPSIsIml2UGFyYW1ldGVyU3BlYyI6IktJQVMveHIzQnExZVE5b0YiLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=master"">    </a>    <a href=""https://github.com/rudderlabs/rudder-server/releases"">      <img src=""https://img.shields.io/github/v/release/rudderlabs/rudder-server?color=blue&sort=semver"">    </a>    <a href=""https://rudderstack.com/docs/get-started/installing-and-setting-up-rudderstack/docker/"">      <img src=""https://img.shields.io/docker/pulls/rudderlabs/rudder-server"">    </a>    <a href=""https://github.com/rudderlabs/rudder-server/blob/master/LICENSE"">      <img src=""https://img.shields.io/github/license/rudderlabs/rudder-server"">    </a>  </p>    <p align=""center"">    <b>      <a href=""https://rudderstack.com"">Website</a>      Â·      <a href=""https://rudderstack.com/docs"">Documentation</a>      Â·      <a href=""https://rudderstack.com/blog"">Blog</a>      Â·      <a href=""https://rudderstack.com/join-rudderstack-slack-community"">Slack</a>      Â·      <a href=""https://twitter.com/rudderstack"">Twitter</a>    </b>  </p>    ---    As the leading open source Customer Data Platform (CDP), [**RudderStack**](https://rudderstack.com/) provides data pipelines that make it easy to collect data from every application, website and SaaS platform, then activate it in your warehouse and business tools.    With RudderStack, you can build customer data pipelines that connect your whole customer data stack and then make them smarter by triggering enrichment and activation in customer tools based on analysis in your data warehouse. It's easy-to-use SDKs and event source integrations, Cloud Extract integrations, transformations, and expansive library of destination and warehouse integrations makes building customer data pipelines for both event streaming and cloud-to-warehouse ELT simple.    <p align=""center"">    <a href=""https://rudderstack.com"">      <img src=""https://user-images.githubusercontent.com/59817155/121468374-4ef91e00-c9d8-11eb-8611-28bea18f609d.gif"" alt=""RudderStack"">    </a>  </p>    | Try **RudderStack Cloud Free** - a free tier of [**RudderStack Cloud**](https://rudderstack.com/cloud). Click [**here**](https://app.rudderlabs.com/signup?type=freetrial) to start building a smarter customer data pipeline today, with RudderStack Cloud. |  |:------|    ## Key features    - **Warehouse-first**: RudderStack treats your data warehouse as a first class citizen among destinations, with advanced features and configurable, near real-time sync.    - **Developer-focused**: RudderStack is built API-first. It integrates seamlessly with the tools that the developers already use and love.    - **High Availability**: RudderStack comes with at least 99.99% uptime. We have built a sophisticated error handling and retry system that ensures that your data will be delivered even in the event of network partitions or destinations downtime.    - **Privacy and Security**: You can collect and store your customer data without sending everything to a third-party vendor. With RudderStack, you get fine-grained control over what data to forward to which analytical tool.     - **Unlimited Events**: Event volume-based pricing of most of the commercial systems is broken. With RudderStack, you are be able to collect as much data as possible without worrying about overrunning your event budgets.    - **Segment API-compatible**: RudderStack is fully compatible with the Segment API. So you don't need to change your app if you are using Segment; just integrate the RudderStack SDKs into your app and your events will keep flowing to the destinations (including data warehouses) as before.    - **Production-ready**: Companies like Mattermost, IFTTT, Torpedo, Grofers, 1mg, Nana, OnceHub, and dozens of large companies use RudderStack for collecting their events.    - **Seamless Integration**: RudderStack currently supports integration with over 90 popular [**tool**](https://rudderstack.com/docs/destinations/) and [**warehouse**](https://rudderstack.com/docs/data-warehouse-integrations/) destinations.    - **User-specified Transformation**: RudderStack offers a powerful JavaScript-based event transformation framework which lets you enhance or transform your event data by combining it with your other internal data. Furthermore, as RudderStack runs inside your cloud or on-premise environment, you can easily access your production data to join with the event data.    ## Get started    The easiest way to experience RudderStack is to [**sign up**](https://app.rudderlabs.com/signup?type=freetrial) for **RudderStack Cloud Free** - a completely free tier of [**RudderStack Cloud**](https://rudderstack.com/cloud).    You can also set up RudderStack on your platform of choice with these two easy steps:    ### Step 1: Set up RudderStack    - [**Docker**](https://www.rudderstack.com/docs/rudderstack-open-source/installing-and-setting-up-rudderstack/docker/)  - [**Kubernetes**](https://www.rudderstack.com/docs/rudderstack-open-source/installing-and-setting-up-rudderstack/kubernetes/)  - [**Developer machine setup**](https://www.rudderstack.com/docs/rudderstack-open-source/installing-and-setting-up-rudderstack/developer-machine-setup/)    > **Note**: If you are planning to use RudderStack in production, we STRONGLY recommend using our Kubernetes Helm charts. We update our Docker images with bug fixes much more frequently than our GitHub repo.    ### Step 2: Verify the installation    Once you have installed RudderStack, [**send test events**](https://rudderstack.com/docs/get-started/installing-and-setting-up-rudderstack/sending-test-events/) to verify the setup.    ## Architecture    RudderStack is an independent, stand-alone system with a dependency only on the database (PostgreSQL). Its backend is written in **Go** with a rich UI written in **React.js**.    A high-level view of RudderStackâ€™s architecture is shown below:    ![Architecture](resources/rudder-server-architecture.png)    For more details on the various architectural components, refer to our [**documentation**](https://rudderstack.com/docs/get-started/rudderstack-architecture/).    ## Contribute    We would love to see you contribute to RudderStack. Get more information on how to contribute [**here**](https://github.com/rudderlabs/rudder-server/blob/master/CONTRIBUTING.md).    ## License    RudderStack server is released under the [**AGPLv3 License**](https://github.com/rudderlabs/rudder-server/blob/master/LICENSE).    Read [**our blog**](https://rudderstack.com/blog/rudderstacks-licensing-explained) to know more about how our software is licensed. """
Big data;https://github.com/ziadoz/awesome-php;"""# Awesome PHP ![](https://github.com/ziadoz/awesome-php/workflows/Awesome%20Bot/badge.svg)    A curated list of amazingly awesome PHP libraries, resources and shiny things.    ## Contributing and Collaborating  Please see [CONTRIBUTING](https://github.com/ziadoz/awesome-php/blob/master/CONTRIBUTING.md), [CODE-OF-CONDUCT](https://github.com/ziadoz/awesome-php/blob/master/CODE-OF-CONDUCT.md) and [COLLABORATING](https://github.com/ziadoz/awesome-php/blob/master/COLLABORATING.md) for details.    ## Table of Contents  - [Awesome PHP](#awesome-php)      - [Composer Repositories](#composer-repositories)      - [Dependency Management](#dependency-management)      - [Dependency Management Extras](#dependency-management-extras)      - [Frameworks](#frameworks)      - [Framework Extras](#framework-extras)      - [Content Management Systems](#content-management-systems-cms)      - [Components](#components)      - [Micro Frameworks](#micro-frameworks)      - [Micro Framework Extras](#micro-framework-extras)      - [Routers](#routers)      - [Templating](#templating)      - [Static Site Generators](#static-site-generators)      - [HTTP](#http)      - [Scraping](#scraping)      - [Middlewares](#middlewares)      - [URL](#url)      - [Email](#email)      - [Files](#Files)      - [Streams](#streams)      - [Dependency Injection](#dependency-injection)      - [Imagery](#imagery)      - [Testing](#testing)      - [Continuous Integration](#continuous-integration)      - [Documentation](#documentation)      - [Security](#security)      - [Passwords](#passwords)      - [Code Analysis](#code-analysis)      - [Code Quality](#code-quality)      - [Static Analysis](#static-analysis)      - [Architectural](#architectural)      - [Debugging and Profiling](#debugging-and-profiling)      - [Build Tools](#build-tools)      - [Task Runners](#task-runners)      - [Navigation](#navigation)      - [Asset Management](#asset-management)      - [Geolocation](#geolocation)      - [Date and Time](#date-and-time)      - [Event](#event)      - [Logging](#logging)      - [E-commerce](#e-commerce)      - [PDF](#pdf)      - [Office](#office)      - [Database](#database)      - [Migrations](#migrations)      - [NoSQL](#nosql)      - [Queue](#queue)      - [Search](#search)      - [Command Line](#command-line)      - [Authentication and Authorization](#authentication-and-authorization)      - [Markup and CSS](#markup-and-css)      - [JSON](#json)      - [Strings](#strings)      - [Numbers](#numbers)      - [Filtering and Validation](#filtering-and-validation)      - [API](#api)      - [Caching and Locking](#caching-and-locking)      - [Data Structure and Storage](#data-structure-and-storage)      - [Notifications](#notifications)      - [Deployment](#deployment)      - [Internationalisation and Localisation](#internationalisation-and-localisation)      - [Serverless](#serverless)      - [Configuration](#configuration)      - [Third Party APIs](#third-party-apis)      - [Extensions](#extensions)      - [Miscellaneous](#miscellaneous)  - [Software](#software)      - [PHP Installation](#php-installation)      - [Development Environment](#development-environment)      - [Virtual Machines](#virtual-machines)      - [Text Editors and IDEs](#text-editors-and-ides)      - [Web Applications](#web-applications)      - [Infrastructure](#infrastructure)  - [Resources](#resources)      - [PHP Websites](#php-websites)      - [PHP Books](#php-books)      - [PHP Videos](#php-videos)      - [PHP Podcasts](#php-podcasts)      - [PHP Newsletters](#php-newsletters)      - [PHP Reading](#php-reading)      - [PHP Internals Reading](#php-internals-reading)    ### Composer Repositories  *Composer Repositories.*    * [Firegento](https://packages.firegento.com/) - Magento Module Composer Repository.  * [Packagist](https://packagist.org/) - The PHP Package Repository.  * [Private Packagist](https://packagist.com/) - Composer package archive as a service for PHP.  * [WordPress Packagist](https://wpackagist.org/) - Manage your plugins with Composer.    ### Dependency Management  *Libraries for dependency and package management.*    * [Composer Installers](https://github.com/composer/installers) - A  multi framework Composer library installer.  * [Composer](https://getcomposer.org/) - A package and dependency manager.  * [Phive](https://phar.io/) - A PHAR manager.  * [Pickle](https://github.com/FriendsOfPHP/pickle) - A PHP extension installer.    ### Dependency Management Extras  *Extras related to dependency management.*    * [Composed](https://github.com/joshdifabio/composed) - A library to parse your project's Composer environment at runtime.  * [Composer Merge Plugin](https://github.com/wikimedia/composer-merge-plugin) - A composer plugin to merge several `composer.json` files.  * [Composer Normalize](https://github.com/ergebnis/composer-normalize) - A plugin for normalising `composer.json` files.   * [Composer Patches](https://github.com/cweagans/composer-patches) - A plugin for Composer to apply patches.  * [Composer Require Checker](https://github.com/maglnet/ComposerRequireChecker) - CLI tool to analyze composer dependencies and verify that no unknown symbols are used in the sources of a package.  * [Composer Unused](https://github.com/composer-unused/composer-unused) - A CLI Tool to scan for unused composer packages.  * [Prestissimo](https://github.com/hirak/prestissimo) - A composer plugin which enables parallel install process.  * [Repman](https://repman.io) - A private PHP package repository manager and Packagist proxy.  * [Satis](https://github.com/composer/satis) - A static Composer repository generator.  * [Tooly](https://github.com/tommy-muehle/tooly-composer-script) - A library to manage PHAR files in project using Composer.  * [Toran Proxy](https://toranproxy.com) - A static Composer repository and proxy.    ### Frameworks  *Web development frameworks.*    * [CakePHP](https://cakephp.org/) - A rapid application development framework.  * [Laminas](https://getlaminas.org/) - A framework comprised of individual components (previously Zend Framework).  * [Laravel](https://laravel.com/) - A web application framework with expressive, elegant syntax.  * [Nette](https://nette.org) - A web framework comprised of mature components.  * [Phalcon](https://phalcon.io/en-us) - A framework implemented as a C extension.  * [Spiral](https://spiral.dev/) - A high performance PHP/Go framework.  * [Symfony](https://symfony.com/) - A set of reuseable components and a web framework.  * [Yii2](https://github.com/yiisoft/yii2/) - A fast, secure, and efficient web framework.    ### Framework Extras  *Extras related to web development frameworks.*    * [CakePHP CRUD](https://github.com/friendsofcake/crud) - A Rapid Application Development (RAD) plugin for CakePHP.  * [Knp RAD Components](https://rad.knplabs.com/) - A set of Rapid Application Development (RAD) components for Symfony.  * [LaravelS](https://github.com/hhxsv5/laravel-s) - Glue for using Swoole in Laravel or Lumen.  * [Symfony CMF](https://github.com/symfony-cmf/symfony-cmf) - A Content Management Framework to create custom CMS.  * [Livewire](https://laravel-livewire.com/) - A full-stack framework for Laravel that takes the pain out of building dynamic UIs.    ### Content Management Systems (CMS)  *Tools for managing digital content.*    * [Backdrop](https://backdropcms.org) - A CMS targeting small-to-medium sized business and non-profits (a fork of Drupal).  * [Concrete5](https://www.concrete5.org/) - A CMS targeting users with a minimum of technical skills.  * [CraftCMS](https://github.com/craftcms/cms) - A flexible, user-friendly CMS for creating custom digital experiences on the web and beyond.  * [Drupal](https://www.drupal.org) - An enterprise level CMS.  * [Grav](https://github.com/getgrav/grav) - A modern flat-file CMS.  * [Joomla](https://www.joomla.org/) - Another leading CMS.  * [Kirby](https://getkirby.com/) - A flat-file CMS that adapts to any project.  * [Magento](https://magento.com/) - The most popular ecommerce platform.  * [Moodle](https://moodle.org/) - An open-source learning platform.  * [Pico CMS](http://picocms.org/) - A stupidly simple, blazing fast, flat file CMS.  * [Statamic](https://statamic.com/) - Build beautiful, easy to manage websites.  * [WordPress](https://wordpress.org/) - A blogging platform and CMS.    ### Components  *Standalone components from web development frameworks and development groups.*    * [Aura](http://auraphp.com/) - Independent components, fully decoupled from each other and from any framework.  * [CakePHP Plugins](https://plugins.cakephp.org/) - A directory of CakePHP plugins.  * [Hoa Project](https://hoa-project.net/En/) - Another package of PHP components.  * [Laravel Components](https://github.com/illuminate) - The Laravel Framework components.  * [League of Extraordinary Packages](https://thephpleague.com/) - A PHP package development group.  * [Spatie Open Source](https://spatie.be/open-source) - A collection of open source PHP and Laravel packages.  * [Symfony Components](https://symfony.com/components) - The components that make Symfony.  * [Laminas Components](https://docs.laminas.dev/components/) - The components that make the Laminas Framework.    ### Micro Frameworks  *Micro frameworks and routers.*    * [Laravel-Zero](https://laravel-zero.com) - A micro-framework for console applications.  * [Lumen](https://lumen.laravel.com) - A micro-framework by Laravel.  * [Mezzio](https://getexpressive.org/) - A micro-framework by Laminas.  * [Radar](https://github.com/radarphp/Radar.Adr) - An Action-Domain-Responder implementation for PHP.  * [Silly](https://github.com/mnapoli/silly) - A micro-framework for CLI applications.  * [Slim](https://www.slimframework.com/) - Another simple micro framework.    ### Micro Framework Extras  *Extras related to micro frameworks and routers.*    * [Slim Skeleton](https://github.com/slimphp/Slim-Skeleton) - A skeleton for Slim.  * [Slim Twig View](https://github.com/slimphp/Slim-Views) - Integrate Twig into Slim.  * [Slim PHP View](https://github.com/slimphp/PHP-View) - A simple PHP renderer for Slim.    ### Routers  *Libraries for handling application routing.*    * [Aura.Router](https://github.com/auraphp/Aura.Router) - A full-featured routing library.  * [Fast Route](https://github.com/nikic/FastRoute) - A fast routing library.  * [Klein](https://github.com/klein/klein.php) - A flexible router.  * [Pux](https://github.com/c9s/Pux) - Another fast routing library.  * [Route](https://github.com/thephpleague/route) - A routing library built on top of Fast Route.    ### Templating  *Libraries and tools for templating and lexing.*    * [MtHaml](https://github.com/arnaud-lb/MtHaml) - A PHP implementation of the HAML template language.  * [Mustache](https://github.com/bobthecow/mustache.php) - A PHP implementation of the Mustache template language.  * [PHPTAL](https://phptal.org/) - A PHP implementation of the [TAL](https://en.wikipedia.org/wiki/Template_Attribute_Language) templating language.  * [Plates](http://platesphp.com/) - A native PHP templating library.  * [Smarty](https://www.smarty.net/) - A template engine to complement PHP.  * [Twig](https://twig.symfony.com/) - A comprehensive templating language.    ### Static Site Generators  *Tools for pre-processing content to generate web pages.*    * [Couscous](http://couscous.io) - Couscous turns Markdown documentation into beautiful websites. It's GitHub Pages on steroids.  * [Jigsaw](http://jigsaw.tighten.co/) - Simple static sites with Laravel's Blade.  * [Sculpin](https://sculpin.io) - A tool that converts Markdown and Twig into static HTML.  * [Spress](http://spress.yosymfony.com) - An extensible tool that converts Markdown and Twig into HTML.    ### HTTP  *Libraries for working with HTTP.*    * [Buzz](https://github.com/kriswallsmith/Buzz) - Another HTTP client.  * [Guzzle]( https://github.com/guzzle/guzzle) - A comprehensive HTTP client.  * [HTTPlug](http://httplug.io) - An HTTP client abstraction without binding to a specific implementation.  * [Nyholm PSR-7](https://github.com/Nyholm/psr7) - A super lightweight PSR-7 implementation. Very strict and very fast.  * [PHP VCR](https://php-vcr.github.io/) - A library for recording and replaying HTTP requests.  * [Requests](https://github.com/rmccue/Requests) - A simple HTTP library.  * [Retrofit](https://github.com/tebru/retrofit-php) - A library to ease creation of REST API clients.  * [Symfony HTTP Client](https://github.com/symfony/http-client) - A component to fetch HTTP resources synchronously or asynchronously.  * [Laminas Diactoros](https://github.com/laminas/laminas-diactoros) - PSR-7 HTTP Message implementation.    ### Scraping  *Libraries for scraping websites.*    * [Chrome PHP](https://github.com/chrome-php/chrome) - Instrument headless Chrome/Chromium instances from PHP.   * [DiDOM](https://github.com/Imangazaliev/DiDOM) - A super fast HTML scrapper and parser.  * [Embed](https://github.com/oscarotero/Embed) - An information extractor from any web service or page.  * [Goutte](https://github.com/FriendsOfPHP/Goutte) - A simple web scraper.  * [Symfony Panther](https://github.com/symfony/panther) - A browser testing and web crawling library for PHP and Symfony.  * [PHP Spider](https://github.com/mvdbos/php-spider) - A configurable and extensible PHP web spider.    ### Middlewares  *Libraries for building application using middlewares.*    * [PSR-7 Middlewares](https://github.com/oscarotero/psr7-middlewares) - Inspiring collection of handy middlewares.  * [Relay](https://github.com/relayphp/Relay.Relay) - A PHP 5.5 PSR-7 middleware dispatcher.  * [Stack](https://github.com/stackphp) - A library of stackable middleware for Symfony.  * [Laminas Stratigility](https://github.com/laminas/laminas-stratigility) - Middleware for PHP built on top of PSR-7.    ### URL  *Libraries for parsing URLs.*    * [PHP Domain Parser](https://github.com/jeremykendall/php-domain-parser) - A domain suffix parser library.  * [Purl](https://github.com/jwage/purl) - A URL manipulation library.  * [sabre/uri](https://github.com/sabre-io/uri) - A functional URI manipulation library.  * [Uri](https://github.com/thephpleague/uri) - Another URL manipulation library.    ### Email  *Libraries for sending and parsing email.*    * [CssToInlineStyles](https://github.com/tijsverkoyen/CssToInlineStyles) - A library to inline CSS in email templates.  * [Email Reply Parser](https://github.com/willdurand/EmailReplyParser) - An email reply parser library.  * [Email Validator](https://github.com/nojacko/email-validator) - A small email address validation library.  * [Fetch](https://github.com/tedious/Fetch) - An IMAP library.  * [Mautic](https://github.com/mautic/mautic) - Email marketing automation  * [PHPMailer](https://github.com/PHPMailer/PHPMailer) - Another mailer solution.  * [PHP IMAP](https://github.com/barbushin/php-imap) - A library to access mailboxes via POP3, IMAP and NNTP.  * [Stampie](https://github.com/Stampie/Stampie) - A library for email services such as [SendGrid](https://sendgrid.com/), [PostMark](https://postmarkapp.com), [MailGun](https://www.mailgun.com/) and [Mandrill](https://mailchimp.com/features/transactional-email/).  * [SwiftMailer](https://swiftmailer.symfony.com) - A mailer solution.  * [Symfony Mailer](https://github.com/symfony/mailer) - A powerful library for creating and sending emails.    ### Files  *Libraries for file manipulation and MIME type detection.*    * [CSV](https://github.com/thephpleague/csv) - A CSV data manipulation library.  * [Flysystem](https://github.com/thephpleague/Flysystem) - Abstraction for local and remote filesystems.  * [Gaufrette](https://github.com/KnpLabs/Gaufrette) - A filesystem abstraction layer.  * [Hoa Mime](https://github.com/hoaproject/Mime) - Another MIME detection library.  * [PHP FFmpeg](https://github.com/PHP-FFmpeg/PHP-FFmpeg/) - A wrapper for the [FFmpeg](https://www.ffmpeg.org/) video library.  * [UnifiedArchive](https://github.com/wapmorgan/UnifiedArchive) - A unified reader and writer of compressed archives.    ### Streams  *Libraries for working with streams.*    * [ByteStream](https://amphp.org/byte-stream/) - An asynchronous stream abstraction.  * [Streamer](https://github.com/fzaninotto/Streamer) - A simple object-orientated stream wrapper library.    ### Dependency Injection  *Libraries that implement the dependency injection design pattern.*    * [Aura.Di](https://github.com/auraphp/Aura.Di) - A serializable dependency injection container with constructor and setter injection, interface and trait awareness, configuration inheritance, and much more.  * [Acclimate](https://github.com/AcclimateContainer/acclimate-container) - A common interface to dependency injection containers and service locators.  * [Auryn](https://github.com/rdlowrey/Auryn) - A recursive dependency injector.  * [Container](https://github.com/thephpleague/container) - Another flexible dependency injection container.  * [Disco](https://github.com/bitExpert/disco) - A PSR-11 compatible, annotation-based dependency injection container.  * [PHP-DI](https://php-di.org/) - A dependency injection container that supports autowiring.  * [Pimple](https://pimple.symfony.com/) - A tiny dependency injection container.  * [Symfony DI](https://github.com/symfony/dependency-injection) - A dependency injection container component.    ### Imagery  *Libraries for manipulating images.*    * [Color Extractor](https://github.com/thephpleague/color-extractor) - A library for extracting colours from images.  * [Glide](https://github.com/thephpleague/glide) - An on-demand image manipulation library.  * [Image Hash](https://github.com/jenssegers/imagehash) - A library for generating perceptual image hashes.  * [Image Optimizer](https://github.com/psliwa/image-optimizer) - A library for optimizing images.  * [Imagine](https://imagine.readthedocs.io/en/latest/index.html) - An image manipulation library.  * [Intervention Image](https://github.com/Intervention/image) - Another image manipulation library.  * [PHP Image Workshop](https://github.com/Sybio/ImageWorkshop) - Another image manipulation library.    ### Testing  *Libraries for testing codebases and generating test data.*    * [Alice](https://github.com/nelmio/alice) - An expressive fixture generation library.  * [AspectMock](https://github.com/Codeception/AspectMock) - A mocking framework for PHPUnit/Codeception.  * [Atoum](https://github.com/atoum/atoum) - A simple testing library.  * [Behat](https://docs.behat.org/en/latest/) - A behaviour driven development (BDD) testing framework.  * [Codeception](https://github.com/Codeception/Codeception) - A full stack testing framework.  * [Faker](https://github.com/fakerphp/faker) - A fake data generator library.  * [HTTP Mock](https://github.com/InterNations/http-mock) - A library for mocking HTTP requests in unit tests.  * [Infection](https://github.com/infection/infection) - An AST-based PHP Mutation testing framework.  * [Kahlan](https://github.com/kahlan/kahlan) - Full stack Unit/BDD testing framework with built-in stub, mock and code-coverage support.  * [Mink](http://mink.behat.org/en/latest/) - Web acceptance testing.  * [Mockery](https://github.com/mockery/mockery) - A mock object library for testing.  * [ParaTest](https://github.com/paratestphp/paratest) - A parallel testing library for PHPUnit.  * [Pest](https://pestphp.com/) - A testing framework with a focus on simplicity.  * [Peridot](https://github.com/peridot-php/peridot) - An event driven test framework.  * [Phake](https://github.com/mlively/Phake) - Another mock object library for testing.  * [Pho](https://github.com/danielstjules/pho) - Another behaviour driven development testing framework.  * [PHP-Mock](https://github.com/php-mock/php-mock) - A mock library for built-in PHP functions (e.g. time()).  * [PHP MySQL Engine](https://github.com/vimeo/php-mysql-engine) -  A MySQL engine written in pure PHP.   * [PHPSpec](https://github.com/phpspec/phpspec) - A design by specification unit testing library.  * [PHPT](https://qa.php.net/write-test.php) - A test tool used by PHP itself.  * [PHPUnit](https://github.com/sebastianbergmann/phpunit) - A unit testing framework.  * [Prophecy](https://github.com/phpspec/prophecy) - A highly opinionated mocking framework.  * [VFS Stream](https://github.com/bovigo/vfsStream) - A virtual filesystem stream wrapper for testing.    ### Continuous Integration  *Libraries and applications for continuous integration.*    * [CircleCI](https://circleci.com) - A continuous integration platform.  * [GitlabCi](https://about.gitlab.com/stages-devops-lifecycle/continuous-integration/) - Let GitLab CI test, build, deploy your code. TravisCi like.  * [Jenkins](https://www.jenkins.io/) - A continuous integration platform with [PHP support](https://www.jenkins.io/solutions/php/).  * [JoliCi](https://github.com/jolicode/JoliCi) - A continuous integration client written in PHP and powered by Docker.  * [PHPCI](https://github.com/dancryer/phpci) - An open source continuous integration platform for PHP.  * [SemaphoreCI](https://semaphoreci.com/) - A continuous integration platform for open source and private projects.  * [Shippable](https://www.shippable.com/) - A Docker based continious integration platform for open source and private projects.  * [Travis CI](https://travis-ci.org/) - A continuous integration platform.  * [Setup PHP](https://github.com/shivammathur/setup-php) - A GitHub Action for PHP.    ### Documentation  *Libraries for generating project documentation.*    * [APIGen](https://github.com/apigen/apigen) - Another API documentation generator.  * [daux.io](https://github.com/dauxio/daux.io) - A documentation generator which uses Markdown files.  * [PHP Documentor 2](https://github.com/phpDocumentor/phpDocumentor) - A documentation generator.  * [phpDox](http://phpdox.de/) - A documentation generator for PHP projects (that is not limited to API documentation).    ### Security  *Libraries for generating secure random numbers, encrypting data and scanning and testing for vulnerabilities.*    * [Halite](https://paragonie.com/project/halite) - A simple library for encryption using [libsodium](https://github.com/jedisct1/libsodium).  * [HTML Purifier](https://github.com/ezyang/htmlpurifier) - A standards compliant HTML filter.  * [IniScan](https://github.com/psecio/iniscan) - A tool that scans PHP INI files for security.  * [Optimus](https://github.com/jenssegers/optimus) - Id obfuscation based on Knuth's multiplicative hashing method.  * [PHPGGC](https://github.com/ambionics/phpggc) - A library of PHP unserializeable payloads along with a tool to generate them.  * [PHP Encryption](https://github.com/defuse/php-encryption) - Secure PHP Encryption Library.  * [PHP SSH](https://github.com/Herzult/php-ssh) - An experimental object orientated SSH wrapper library.  * [PHPSecLib](http://phpseclib.sourceforge.net/) - A pure PHP secure communications library.  * [random_compat](https://github.com/paragonie/random_compat) - PHP 5.x support for `random_bytes()` and `random_int()`  * [RandomLib](https://github.com/ircmaxell/RandomLib) - A library for generating random numbers and strings.  * [Symfony Security Monitoring](https://security.symfony.com/) - A web tool to check your Composer dependencies for security advisories, previously known as ""SensioLabs Security Check"".  * [SQLMap](https://github.com/sqlmapproject/sqlmap) - An automatic SQL injection and database takeover tool.   * [TCrypto](https://github.com/timoh6/TCrypto) - A simple encrypted key-value storage library.  * [VAddy](https://vaddy.net/) - A continuous security testing platform for web applications.  * [Zap](https://owasp.org/www-project-zap/) - An integrated penetration testing tool for web applications.    ### Passwords  *Libraries and tools for working with and storing passwords.*    * [GenPhrase](https://github.com/timoh6/GenPhrase) - A library for generating secure random passphrases.  * [Password Compat](https://github.com/ircmaxell/password_compat) - A compatibility library for the new PHP 5.5 password functions.  * [Password Policy](https://github.com/ircmaxell/password-policy) - A password policy library for PHP and JavaScript.  * [Password Validator](https://github.com/jeremykendall/password-validator) - A library for validating and upgrading password hashes.  * [Password-Generator](https://github.com/hackzilla/password-generator) - PHP library to generate random passwords.  * [PHP Password Lib](https://github.com/ircmaxell/PHP-PasswordLib) - A library for generating and validating passwords.  * [phpass](https://www.openwall.com/phpass/) - A portable password hashing framework.  * [Zxcvbn PHP](https://github.com/bjeavons/zxcvbn-php) - A realistic PHP password strength estimate library based on Zxcvbn JS.    ### Code Analysis  *Libraries and tools for analysing, parsing and manipulating codebases.*    * [Better Reflection](https://github.com/Roave/BetterReflection) - AST-based reflection library that allows analysis and manipulation of code  * [Code Climate](https://codeclimate.com) - An automated code review.  * [GrumPHP](https://github.com/phpro/grumphp) - A PHP code-quality tool.  * [PHP Parser](https://github.com/nikic/PHP-Parser) - A PHP parser written in PHP.  * [PHP Semantic Versioning Checker](https://github.com/tomzx/php-semver-checker) - A command line utility that compares two source sets and determines the appropriate semantic versioning to apply.  * [Phpactor](https://github.com/phpactor/phpactor) - PHP completion, refactoring and introspection tool.  * [PHPLOC](https://github.com/sebastianbergmann/phploc) - A tool for quickly measuring the size of a PHP project.  * [PHPQA](https://github.com/EdgedesignCZ/phpqa) - A tool for running QA tools (phploc, phpcpd, phpcs, pdepend, phpmd, phpmetrics).  * [Qafoo Quality Analyzer](https://github.com/Qafoo/QualityAnalyzer) - A tool to visualize metrics and source code.  * [Rector](https://github.com/rectorphp/rector) - A tool to upgrade and refactor code.  * [Scrutinizer](https://scrutinizer-ci.com/) - A web tool to [scrutinise PHP code](https://github.com/scrutinizer-ci/php-analyzer).  * [UBench](https://github.com/devster/ubench) - A simple micro benchmark library.    ### Code Quality  *Libraries for managing code quality, formatting and linting.*    * [CaptainHook](https://github.com/captainhookphp/captainhook) - An easy-to-use and flexible Git hook library.   * [PHP CodeSniffer](https://github.com/squizlabs/PHP_CodeSniffer) - A library that detects PHP, CSS and JS coding standard violations.  * [PHP CS Fixer](https://github.com/FriendsOfPHP/PHP-CS-Fixer) - A coding standards fixer library.  * [PHP Mess Detector](https://github.com/phpmd/phpmd) - A library that scans code for bugs, sub-optimal code, unused parameters and more.  * [PHPCheckstyle](https://github.com/PHPCheckstyle/phpcheckstyle) - A tool to help adhere to certain coding conventions.  * [PHPCPD](https://github.com/sebastianbergmann/phpcpd) - A library that detects copied and pasted code.    ### Static Analysis  *Libraries for performing static analysis of PHP code.*    * [Exakat](https://github.com/exakat/exakat) - A static analysis engine for PHP.  * [Deptrac](https://github.com/sensiolabs-de/deptrac) - A static code analysis tool that helps to enforce rules for dependencies between software layers.  * [Mondrian](https://github.com/Trismegiste/Mondrian) - A code analysis tool using Graph Theory.  * [phan](https://github.com/phan/phan) - A static analyzer based on PHP 7+ and the php-ast extension.  * [PHP Architecture Tester](https://github.com/carlosas/phpat) - Easy to use architecture testing tool for PHP.  * [PHPCompatibility](https://github.com/PHPCompatibility/PHPCompatibility) - A PHP compatibility checker for PHP CodeSniffer.  * [PhpDependencyAnalysis](https://github.com/mamuz/PhpDependencyAnalysis) - A tool to create customisable dependency graphs.  * [PHP Metrics](https://github.com/phpmetrics/PhpMetrics) - A static metric library.  * [PHP Migration](https://github.com/monque/PHP-Migration) - A static analyzer for PHP version migration.  * [PHPStan](https://github.com/phpstan/phpstan) - A PHP Static Analysis Tool.  * [Psalm](https://github.com/vimeo/psalm) - A static analysis tool for finding errors in PHP applications.    ### Architectural  *Libraries related to design patterns, programming approaches and ways to organize code.*    * [Design Patterns PHP](https://github.com/domnikl/DesignPatternsPHP) - A repository of software patterns implemented in PHP.  * [Finite](https://yohan.giarel.li/Finite/) - A simple PHP finite state machine.  * [Functional PHP](https://github.com/lstrojny/functional-php) - A functional programming library.  * [Iter](https://github.com/nikic/iter) - A library that provides iteration primitives using generators.  * [Patchwork](http://patchwork2.org/) - A library for redefining userland functions.  * [Pipeline](https://github.com/thephpleague/pipeline) - A pipeline pattern implementation.  * [Porter](https://github.com/ScriptFUSION/Porter) - Data import abstraction library for consuming Web APIs and other data sources.  * [Ruler](https://github.com/bobthecow/Ruler) - A simple stateless production rules engine.  * [RulerZ](https://github.com/K-Phoen/rulerz) - A powerful rule engine and implementation of the Specification pattern.    ### Debugging and Profiling  *Libraries and tools for debugging errors and profiling code.*    * [APM](https://pecl.php.net/package/APM) - Monitoring extension collecting errors and statistics into SQLite/MySQL/StatsD.  * [Barbushin PHP Console](https://github.com/barbushin/php-console) - Another web debugging console using Google Chrome.  * [Blackfire.io](https://blackfire.io) - A low-overhead code profiler.  * [Kint](https://github.com/kint-php/kint) - A debugging and profiling tool.  * [Metrics](https://github.com/beberlei/metrics) - A simple metrics API library.  * [PCOV](https://github.com/krakjoe/pcov) - A self contained code coverage compatible driver.  * [PHP Console](https://github.com/Seldaek/php-console) - A web debugging console.  * [PHP Debug Bar](http://phpdebugbar.com/) - A debugging toolbar.  * [PHPBench](https://github.com/phpbench/phpbench) - A benchmarking Framework.  * [PHPSpy](https://github.com/adsr/phpspy) - A low-overhead sampling profiler.  * [Symfony VarDumper](https://github.com/symfony/var-dumper) - A variable dumper component.  * [Tideways.io](https://tideways.com/) - Monitoring and profiling tool.  * [Tracy](https://github.com/nette/tracy) - A simple error detection, logging and time measuring library.  * [Whoops](https://github.com/filp/whoops) - A pretty error handling library.  * [xDebug](https://github.com/xdebug/xdebug) - A debug and profile tool for PHP.  * [XHProf](https://github.com/phacility/xhprof) - A profiling tool originally developed by Facebook.  * [Z-Ray](https://www.zend.com/products/z-ray) - A debug and profile tool for Zend Server.    ### Build Tools  *Project build and automation tools.*    * [Box](https://github.com/box-project/box) - A utility to build PHAR files.  * [Construct](https://github.com/jonathantorres/construct) - A PHP project/micro-package generator.  * [Phing](https://www.phing.info/) - A PHP project build system inspired by Apache Ant.  * [RMT](https://github.com/liip/RMT) - A library for versioning and releasing software.    ### Task Runners  *Libraries for automating and running tasks.*    * [Bldr](https://bldr.io/) - A PHP Task runner built on Symfony components.  * [Jobby](https://github.com/jobbyphp/jobby) - A PHP cron job manager without modifying crontab.  * [Robo](https://github.com/consolidation/Robo) - A PHP Task runner with object-orientated configurations.  * [Task](https://taskphp.github.io/) - A pure PHP task runner inspired by Grunt and Gulp.    ### Navigation  *Tools for building navigation structures.*    * [KnpMenu](https://github.com/KnpLabs/KnpMenu) - A menu library.  * [Menu](https://github.com/spatie/menu) - A flexible menu library with a fluent interface.    ### Asset Management  *Tools for managing, compressing and minifying website assets.*    * [JShrink](https://github.com/tedious/JShrink) - A JavaScript minifier library.  * [Laravel Mix](https://github.com/JeffreyWay/laravel-mix) - An elegant wrapper around Webpack for the 80% use case.  * [Symfony Asset](https://github.com/symfony/asset) - Manages URL generation and versioning of web assets.  * [Symfony Encore](https://github.com/symfony/webpack-encore) - A simple but powerful API for processing and compiling assets built around Webpack.    ### Geolocation  *Libraries for geocoding addresses and working with latitudes and longitudes.*    * [Country List](https://github.com/umpirsky/country-list) - A list of all countries with names and ISO 3166-1 codes.  * [GeoCoder](https://geocoder-php.org/) - A geocoding library.  * [GeoJSON](https://github.com/jmikola/geojson) - A GeoJSON implementation.  * [GeoTools](https://github.com/thephpleague/geotools) - A library of geo-related tools.  * [PHPGeo](https://github.com/mjaschen/phpgeo) - A simple geo library.    ### Date and Time  *Libraries for working with dates and times.*    * [CalendR](https://yohan.giarel.li/CalendR/) - A calendar management library.  * [Carbon](https://github.com/briannesbitt/Carbon) - A simple DateTime API extension.  * [Chronos](https://github.com/cakephp/chronos) - A DateTime API extension supporting both mutable and immutable date/time.  * [Moment.php](https://github.com/fightbulc/moment.php) - Moment.js inspired PHP DateTime handler with i18n support.  * [Yasumi](https://github.com/azuyalabs/yasumi) - An library to help you calculate the dates and names of holidays.    ### Event  *Libraries that are event-driven or implement non-blocking event loops.*  * [Amp](https://github.com/amphp/amp) - An event driven non-blocking I/O library.  * [Broadway](https://github.com/broadway/broadway) - An event source and CQRS library.  * [CakePHP Event](https://github.com/cakephp/event) - An event dispatcher library.  * [Elephant.io](https://github.com/Wisembly/Elephant.io) - Yet another web socket library.  * [Evenement](https://github.com/igorw/evenement) - An event dispatcher library.  * [Event](https://github.com/thephpleague/event) - An event library with a focus on domain events.  * [Hoa EventSource](https://github.com/hoaproject/Eventsource) - An event source library.  * [Hoa WebSocket](https://github.com/hoaproject/Websocket) - Another web socket library.  * [Pawl](https://github.com/ratchetphp/Pawl) - An asynchronous web socket client.  * [Prooph Event Store](https://github.com/prooph/event-store) - An event source component to persist event messages  * [PHP Defer](https://github.com/php-defer/php-defer) - Golang's defer statement for PHP.  * [Ratchet](https://github.com/ratchetphp/Ratchet) - A web socket library.  * [ReactPHP](https://github.com/reactphp/reactphp) - An event driven non-blocking I/O library.  * [RxPHP](https://github.com/ReactiveX/RxPHP) - A reactive extension library.  * [Swoole](https://github.com/swoole/swoole-src) - An event-driven asynchronous and concurrent networking communication framework with high performance for PHP written in C.  * [Workerman](https://github.com/walkor/Workerman) - An event driven non-blocking I/O library.    ### Logging  *Libraries for generating and working with log files.*    * [Monolog](https://github.com/Seldaek/monolog) - A comprehensive logger.    ### E-commerce  *Libraries and applications for taking payments and building online e-commerce stores.*    * [Money](https://github.com/moneyphp/money) - A PHP implementation of Fowler's money pattern.  * [Brick\Money](https://github.com/brick/money) - A money library for PHP, with support for contexts, cash roundings, currency conversion.  * [OmniPay](https://github.com/thephpleague/omnipay) - A framework agnostic multi-gateway payment processing library.  * [Payum](https://github.com/payum/payum) - A payment abstraction library.  * [Shopware](https://github.com/shopware/shopware) - Highly customizable e-commerce software  * [Swap](https://github.com/florianv/swap) - An exchange rates library.  * [Sylius](https://sylius.com/) - An open source e-commerce solution.    ### PDF  *Libraries and software for working with PDF files.*    * [Dompdf](https://github.com/dompdf/dompdf) - A HTML to PDF converter.  * [PHPPdf](https://github.com/psliwa/PHPPdf) - A library for generating PDFs and images from XML.  * [Snappy](https://github.com/KnpLabs/snappy) - A PDF and image generation library.  * [WKHTMLToPDF](https://github.com/wkhtmltopdf/wkhtmltopdf) - A tool to convert HTML to PDF.    ### Office  *Libraries for working with office suite documents.*    * [PHPPowerPoint](https://github.com/PHPOffice/PHPPresentation) - A library for working with Microsoft PowerPoint Presentations.  * [PHPWord](https://github.com/PHPOffice/PHPWord) - A library for working with Microsoft Word documents.  * [PHPSpreadsheet](https://github.com/PHPOffice/PhpSpreadsheet) - A pure PHP library for reading and writing spreadsheet files (successor of PHPExcel).  * [Spout](https://github.com/box/spout) - Read and write spreadsheet files (CSV, XLSX and ODS), in a fast and scalable way .    ### Database  *Libraries for interacting with databases using object-relational mapping (ORM) or datamapping techniques.*    * [Atlas.Orm](https://github.com/atlasphp/Atlas.Orm) - A data mapper implementation for your persistence model in PHP.  * [Aura.Sql](https://github.com/auraphp/Aura.Sql) - Provides an extension to the native PDO along with a profiler and connection locator.  * [Aura.SqlQuery](https://github.com/auraphp/Aura.SqlQuery) - Independent query builders for MySQL, PostgreSQL, SQLite, and Microsoft SQL Server.  * [Baum](https://github.com/etrepat/baum) - A nested set implementation for Eloquent.  * [CakePHP ORM](https://github.com/cakephp/orm) - Object-Relational Mapper, implemented using the DataMapper pattern.  * [Cycle ORM](https://github.com/cycle/orm) - PHP DataMapper, ORM.  * [Doctrine Extensions](https://github.com/Atlantic18/DoctrineExtensions) - A collection of Doctrine behavioural extensions.  * [Doctrine](https://www.doctrine-project.org/) - A comprehensive DBAL and ORM.  * [Laravel Eloquent](https://github.com/illuminate/database) - A simple ORM.  * [Pomm](https://github.com/chanmix51/Pomm) - An Object Model Manager for PostgreSQL.  * [ProxyManager](https://github.com/Ocramius/ProxyManager) - A set of utilities to generate proxy objects for data mappers.  * [RedBean](https://redbeanphp.com/index.php) - A lightweight, configuration-less ORM.  * [Slimdump](https://github.com/webfactory/slimdump) - An easy dumper tool for MySQL.  * [Spot2](https://github.com/spotorm/spot2) - A MySQL datamapper ORM.    ### Migrations  Libraries to help manage database schemas and migrations.    * [Doctrine Migrations](https://www.doctrine-project.org/projects/migrations.html) - A migration library for Doctrine.  * [Migrations](https://github.com/icomefromthenet/Migrations) - A migration management library.  * [Phinx](https://github.com/cakephp/phinx) - Another database migration library.  * [PHPMig](https://github.com/davedevelopment/phpmig) - Another migration management library.  * [Ruckusing](https://github.com/ruckus/ruckusing-migrations) - Database migrations for PHP ala ActiveRecord Migrations with support for MySQL, Postgres, SQLite.    ### NoSQL  *Libraries for working with ""NoSQL"" backends.*    * [PHPMongo](https://github.com/sokil/php-mongo) - A MongoDB ORM.  * [Predis](https://github.com/predis/predis) - A feature complete Redis library.    ### Queue  *Libraries for working with event and task queues.*    * [Bernard](https://github.com/bernardphp/bernard) - A multibackend abstraction library.  * [BunnyPHP](https://github.com/jakubkulhan/bunny) - A performant pure-PHP AMQP (RabbitMQ) sync and also async (ReactPHP) library.  * [Pheanstalk](https://github.com/pheanstalk/pheanstalk) - A Beanstalkd client library.  * [PHP AMQP](https://github.com/php-amqplib/php-amqplib) - A pure PHP AMQP library.  * [Tarantool Queue](https://github.com/tarantool-php/queue) - PHP bindings for Tarantool Queue.  * [Thumper](https://github.com/php-amqplib/Thumper) - A RabbitMQ pattern library.  * [Enqueue](https://github.com/php-enqueue/enqueue-dev) - A message queue packages for PHP that supports RabbitMQ, AMQP, STOMP, Amazon SQS, Redis and Doctrine transports.     ### Search  *Libraries and software for indexing and performing search queries on data.*    * [Elastica](https://github.com/ruflin/Elastica) - A client library for ElasticSearch.  * [ElasticSearch PHP](https://github.com/elastic/elasticsearch-php) - The official client library for [ElasticSearch](https://www.elastic.co/).  * [Solarium](https://www.solarium-project.org/) - A client library for [Solr](https://lucene.apache.org/solr/).  * [SphinxQL Query Builder](https://foolcode.github.io/SphinxQL-Query-Builder/) - A query library for the [Sphinx](https://sphinxsearch.com/) and [Manticore](https://manticoresearch.com/) search engines.    ### Command Line  *Libraries related to the command line.*    * [Aura.Cli](https://github.com/auraphp/Aura.Cli) - Provides the equivalent of request ( Context ) and response ( Stdio ) objects for the command line interface, including Getopt support, and an independent Help object for describing commands.  * [Boris](https://github.com/borisrepl/boris) - A tiny PHP REPL.  * [Cilex](https://github.com/Cilex/Cilex) - A micro framework for building command line tools.  * [CLI Menu](https://github.com/php-school/cli-menu) - A library for building CLI menus.  * [CLIFramework](https://github.com/c9s/CLIFramework) - A command-line framework supports zsh/bash completion generation, subcommands and option constraints. It also powers phpbrew.  * [CLImate](https://github.com/thephpleague/climate) - A library for outputting colours and special formatting.  * [Commando](https://github.com/nategood/commando) - Another simple command line opt parser.  * [Cron Expression](https://github.com/mtdowling/cron-expression) - A library to calculate cron run dates.  * [GetOpt](https://github.com/getopt-php/getopt-php) - A command line opt parser.  * [GetOptionKit](https://github.com/c9s/GetOptionKit) - Another command line opt parser.  * [Hoa Console](https://github.com/hoaproject/Console) - Another command line library.  * [PsySH](https://github.com/bobthecow/psysh) - Another PHP REPL.  * [ShellWrap](https://github.com/MrRio/shellwrap) - A simple command line wrapper library.    ### Authentication and Authorization  *Libraries for implementing user authentication and authorization.*    * [Aura.Auth](https://github.com/auraphp/Aura.Auth) - Provides authentication functionality and session tracking using various adapters.  * [SocialConnect Auth](https://github.com/socialConnect/auth) - An open source social sign (OAuth1\OAuth2\OpenID\OpenIDConnect).  * [Json Web Token](https://github.com/lcobucci/jwt) - Json Tokens to authenticate and transmit information.  * [OAuth 1.0 Client](https://github.com/thephpleague/oauth1-client) - An OAuth 1.0 client library.  * [OAuth 2.0 Client](https://github.com/thephpleague/oauth2-client) - An OAuth 2.0 client library.  * [OAuth2 Server](https://bshaffer.github.io/oauth2-server-php-docs/) - Another OAuth2 server implementation.  * [OAuth2 Server](https://oauth2.thephpleague.com/) - An OAuth2 authentication server, resource server and client library.  * [Opauth](https://github.com/opauth/opauth) - A multi-provider authentication framework.  * [Paseto](https://github.com/paragonie/paseto) - Platform-Agnostic Security Tokens.  * [PHP oAuthLib](https://github.com/Lusitanian/PHPoAuthLib) - Another OAuth library.  * [Sentinel Social](https://cartalyst.com/manual/sentinel-social/2.0) - A library for social network authentication.  * [Sentinel](https://cartalyst.com/manual/sentinel/2.0) - A framework agnostic authentication & authorisation library.  * [TwitterOAuth](https://github.com/abraham/twitteroauth) - A Twitter OAuth library.    ### Markup and CSS  *Libraries for working with markup and CSS formats.    * [Cebe Markdown](https://github.com/cebe/markdown) - An fast and extensible Markdown parser.  * [CommonMark PHP](https://github.com/thephpleague/commonmark) - Highly-extensible Markdown parser which fully supports the [CommonMark spec](https://spec.commonmark.org/).  * [Decoda](https://github.com/milesj/decoda) - A lightweight markup parser library.  * [Essence](https://github.com/essence/essence) - A library for extracting web media.  * [Embera](https://github.com/mpratt/Embera) - An Oembed consumer library.  * [HTML to Markdown](https://github.com/thephpleague/html-to-markdown) - Converts HTML into Markdown.  * [HTML5 PHP](https://github.com/Masterminds/html5-php) - An HTML5 parser and serializer library.  * [Parsedown](https://github.com/erusev/parsedown) - Another Markdown parser.  * [PHP CSS Parser](https://github.com/sabberworm/PHP-CSS-Parser) - A Parser for CSS Files written in PHP.  * [PHP Markdown](https://github.com/michelf/php-markdown) - A Markdown parser.  * [Shiki PHP](https://github.com/spatie/shiki-php) - A [Shiki](https://github.com/shikijs/shiki) code highlighting package in PHP.  * [VObject](https://github.com/sabre-io/vobject) - A library for parsing VCard and iCalendar objects.    ### JSON  *Libraries for working with JSON.*    * [JSON Lint](https://github.com/Seldaek/jsonlint) - A JSON lint utility.  * [JSONMapper](https://github.com/JsonMapper/JsonMapper) - A library for mapping JSON to PHP objects.    ### Strings  *Libraries for parsing and manipulating strings.*    * [Agent](https://github.com/jenssegers/agent) - A PHP desktop/mobile user agent parser, based on Mobiledetect.  * [ANSI to HTML5](https://github.com/sensiolabs/ansi-to-html) - An ANSI to HTML5 converter library.  * [Color Jizz](https://github.com/mikeemoo/ColorJizz-PHP) - A library for manipulating and converting colours.  * [Device Detector](https://github.com/matomo-org/device-detector) - Another library for parsing user agent strings.  * [Hoa String](https://github.com/hoaproject/Ustring) - Another UTF-8 string library.  * [Jieba-PHP](https://github.com/fukuball/jieba-php) - A PHP port of Python's jieba. Chinese text segmentation for natural language processing.  * [Mobile-Detect](https://github.com/serbanghita/Mobile-Detect) - A lightweight PHP class for detecting mobile devices (including tablets).  * [Patchwork UTF-8](https://github.com/nicolas-grekas/Patchwork-UTF8) - A portable library for working with UTF-8 strings.  * [Portable UTF-8](https://github.com/voku/portable-utf8) - A string manipulation library with UTF-8 safe replacement methods.  * [Slugify](https://github.com/cocur/slugify) - A library to convert strings to slugs.  * [SQL Formatter](https://github.com/jdorn/sql-formatter/) - A library for formatting SQL statements.  * [Stringy](https://github.com/voku/Stringy) - A string manipulation library with multibyte support.  * [UA Parser](https://github.com/tobie/ua-parser/tree/master/php) - A library for parsing user agent strings.  * [URLify](https://github.com/jbroadway/urlify) - A PHP port of Django's URLify.js.  * [UUID](https://github.com/ramsey/uuid) - A library for generating UUIDs.    ### Numbers  *Libraries for working with numbers.*    * [Brick\Math](https://github.com/brick/math) - A library providing large number support: `BigInteger`, `BigDecimal` and `BigRational`.  * [ByteUnits](https://github.com/gabrielelana/byte-units) - A library to parse, format and convert byte units in binary and metric systems.  * [DecimalObject](https://github.com/spryker/decimal-object) - A value object to handle decimals/floats easily and more precisely.  * [IP](https://github.com/darsyn/ip) - An immutable value object for working with IPv4 and IPv6 addresses.  * [LibPhoneNumber for PHP](https://github.com/giggsey/libphonenumber-for-php) - A PHP implementation of Google's phone number handling library.  * [PHP Conversion](https://github.com/Crisu83/php-conversion) - Another library for converting between units of measure.  * [PHP Units of Measure](https://github.com/triplepoint/php-units-of-measure) - A library for converting between units of measure.  * [MathPHP](https://github.com/markrogoyski/math-php) - A math library for PHP.     ### Filtering and Validation  *Libraries for filtering and validating data.*    * [Assert](https://github.com/beberlei/assert) - A validation library with a rich set of assertions. Supports assertion chaining and lazy assertions.  * [Aura.Filter](https://github.com/auraphp/Aura.Filter) - Provides tools to validate and sanitize objects and arrays.  * [CakePHP Validation](https://github.com/cakephp/validation) - Another validation library.  * [Filterus](https://github.com/ircmaxell/filterus) - A simple PHP filtering library.  * [ISO-codes](https://github.com/ronanguilloux/IsoCodes) - A library for validating inputs according standards from ISO, International Finance, Public Administrations, GS1, Book Industry, Phone numbers & Zipcodes for many countries.  * [JSON Schema](https://github.com/justinrainbow/json-schema) - A [JSON Schema](https://json-schema.org/) validation library.  * [MetaYaml](https://github.com/romaricdrigon/MetaYaml) - A schema validation library that supports YAML, JSON and XML.  * [Respect Validation](https://github.com/Respect/Validation) - A simple validation library.  * [Upload](https://github.com/brandonsavage/Upload) - A library for handling file uploads and validation.  * [Valitron](https://github.com/vlucas/valitron) - Another validation library.  * [Volan](https://github.com/serkin/Volan) - Another simplified validation library.    ### API  *Libraries and web tools for developing APIs.*    * [API Platform](https://api-platform.com ) - Expose in minutes an hypermedia REST API that embraces JSON-LD, Hydra format.  * [Laminas API Tool Skeleton](https://github.com/laminas-api-tools/api-tools-skeleton) - An API builder built with the Laminas Framework.  * [Drest](https://github.com/leedavis81/drest) - A library for exposing Doctrine entities as REST resource endpoints.  * [HAL](https://github.com/blongden/hal) - A Hypertext Application Language (HAL) builder library.  * [Hateoas](https://github.com/willdurand/Hateoas) - A HATEOAS REST web service library.  * [Negotiation](https://github.com/willdurand/Negotiation) - A content negotiation library.  * [Restler](https://github.com/Luracast/Restler) - A lightweight framework to expose PHP methods as RESTful web API.  * [wsdl2phpgenerator](https://github.com/wsdl2phpgenerator/wsdl2phpgenerator) - A tool to generate PHP classes from SOAP WSDL files.    ### Caching and Locking  *Libraries for caching data and acquiring locks.*    * [APIx Cache](https://github.com/apix/cache) - A thin PSR-6 cache wrapper to various caching backends emphasising cache tagging and indexing.  * [CacheTool](https://github.com/gordalina/cachetool) - A tool to clear APC/opcode caches from the command line.  * [CakePHP Cache](https://github.com/cakephp/cache) - A caching library.  * [Doctrine Cache](https://github.com/doctrine/cache) - A caching library.  * [Metaphore](https://github.com/sobstel/metaphore) - Cache slam defense using a semaphore to prevent dogpile effect.  * [Stash](https://github.com/tedious/Stash) - Another library for caching.  * [Laminas Cache](https://github.com/laminas/laminas-cache) - Another caching library.  * [Lock](https://github.com/php-lock/lock) - A lock library to provide exclusive execution.    ### Data Structure and Storage  *Libraries that implement data structure or storage techniques.*    * [CakePHP Collection](https://github.com/cakephp/collection) - A simple collections library.  * [Fractal](https://github.com/thephpleague/fractal) - A library for converting complex data structures to JSON output.  * [Ginq](https://github.com/akanehara/ginq) - Another PHP library based on .NET's LINQ.  * [JsonMapper](https://github.com/cweiske/jsonmapper) - A library that maps nested JSON structures onto PHP classes.  * [JSON Machine](https://github.com/halaxa/json-machine) - Provides iteration over huge JSONs using simple `foreach`  * [Knapsack](https://github.com/DusanKasan/Knapsack) - Collection library inspired by Clojure's sequences.  * [msgpack.php](https://github.com/rybakit/msgpack.php) - A pure PHP implementation of the [MessagePack](https://msgpack.org/) serialization format.  * [PINQ](https://github.com/TimeToogo/Pinq) - A PHP library based on .NET's LINQ (Language Integrated Query).  * [Serializer](https://github.com/schmittjoh/serializer) - A library for serialising and de-serialising data.  * [YaLinqo](https://github.com/Athari/YaLinqo) - Yet Another LINQ to Objects for PHP.  * [Laminas Serializer](https://github.com/laminas/laminas-serializer) - Another library for serialising and de-serialising data.    ### Notifications  *Libraries for working with notification software.*    * [JoliNotif](https://github.com/jolicode/JoliNotif) - A cross-platform library for desktop notification (support for Growl, notify-send, toaster, etc)  * [Notification Pusher](https://github.com/Ph3nol/NotificationPusher) - A standalone library for device push notifications.  * [Notificato](https://github.com/mac-cain13/notificato) - A library for handling push notifications.  * [Notificator](https://github.com/namshi/notificator) - A lightweight notification library.  * [Php-pushwoosh](https://github.com/gomoob/php-pushwoosh) - A PHP Library to easily send push notifications with the Pushwoosh REST Web Services.    ### Deployment  *Libraries for project deployment.*    * [Deployer](https://github.com/deployphp/deployer) - A deployment tool.  * [Envoy](https://github.com/laravel/envoy) - A tool to run SSH tasks with PHP.  * [Rocketeer](https://github.com/rocketeers/rocketeer) - A fast and easy deployer for the PHP world.    ### Internationalisation and Localisation  *Libraries for Internationalization (I18n) and Localization (L10n).*    * [Aura.Intl](https://github.com/auraphp/Aura.Intl) - Provides internationalization (I18N) tools, specifically package-oriented per-locale message translation.  * [CakePHP I18n](https://github.com/cakephp/i18n) - Message translation and localization for dates and numbers.    ### Serverless  *Libraries and tools to help build serverless web applications.*    * [Bref](https://bref.sh/) - Serverless PHP on AWS Lambda.  * [OpenWhisk](http://openwhisk.apache.org/) - An open-source serverless cloud platform.  * [Serverless Framework](https://www.serverless.com/open-source/) - An open-source framework for building serverless applications.  * [Laravel Vapor](https://vapor.laravel.com/) - A serverless deployment platform for Laravel, powered by AWS.    ## Configuration  *Libraries and tools for configuration.*    * [PHP Dotenv](https://github.com/vlucas/phpdotenv) - Parse and load environment variables from `.env` files.  * [Symfony Dotenv](https://github.com/symfony/dotenv)- Parse and load environment variables from `.env` files.  * [Yo! Symfony TOML](https://github.com/yosymfony/toml) - A PHP parser for [TOML](https://github.com/toml-lang/toml).     ### Third Party APIs  *Libraries for accessing third party APIs.*    * [Amazon Web Service SDK](https://github.com/aws/aws-sdk-php) - The official PHP AWS SDK library.  * [AsyncAWS](https://async-aws.com/) - An unofficial asynchronous PHP AWS SDK.  * [Campaign Monitor](https://campaignmonitor.github.io/createsend-php/) - The official Campaign Monitor PHP library.  * [Github](https://github.com/KnpLabs/php-github-api) - A library to interface with the Github API.  * [Mailgun](https://github.com/mailgun/mailgun-php) The official Mailgun PHP API.  * [Square](https://github.com/square/connect-php-sdk) - The official Square PHP SDK for payments and other Square APIs.  * [Stripe](https://github.com/stripe/stripe-php) - The official Stripe PHP library.  * [Twilio](https://github.com/twilio/twilio-php) - The official Twilio PHP REST API.    ### Extensions  *Libraries to help build PHP extensions.*    * [PHP CPP](https://www.php-cpp.com/) - A C++ library for developing PHP extensions.  * [Zephir](https://github.com/phalcon/zephir) - A compiled language between PHP and C++ for developing PHP extensions.    ### Miscellaneous  *Useful libraries or utilities that don't fit into the categories above.*    * [Annotations](https://github.com/doctrine/annotations) - An annotation library (part of Doctrine).  * [BotMan](https://github.com/botman/botman) - A framework agnostic PHP library to build cross-platform chat bots.  * [ClassPreloader](https://github.com/ClassPreloader/ClassPreloader) - A library for optimising autoloading.  * [Hprose-PHP](https://github.com/hprose/hprose-php) - A cross-language RPC.  * [noCAPTCHA](https://github.com/ARCANEDEV/noCAPTCHA) - Helper for Google's noCAPTCHA (reCAPTCHA).  * [Pagerfanta](https://github.com/whiteoctober/Pagerfanta) - A pagination library.  * [Safe](https://github.com/thecodingmachine/safe) - All PHP functions, rewritten to throw exceptions instead of returning false.  * [SuperClosure](https://github.com/jeremeamia/super_closure) - A library that allows Closures to be serialized.    # Software  *Software for creating a development environment.*    ### PHP Installation  *Tools to help install and manage PHP on your computer.*    * [Brew PHP Switcher](https://github.com/philcook/brew-php-switcher) - Brew PHP switcher.  * [HomeBrew](https://brew.sh/) - A package manager for OSX.  * [Laravel Valet](https://laravel.com/docs/master/valet) - A development environment for macOS.  * [PHP Brew](https://github.com/phpbrew/phpbrew) - A PHP version manager and installer.  * [PHP Build](https://github.com/php-build/php-build) - Another PHP version installer.  * [PHP OSX](https://php-osx.liip.ch/) - A PHP installer for OSX.    ### Development Environment  *Software and tools for creating and sharing a development environment.*    * [Ansible](https://www.ansible.com/) - A radically simple orchestration framework.  * [Docker](https://www.docker.com/) - A containerization platform.  * [Docker PHP Extension Installer](https://github.com/mlocati/docker-php-extension-installer) - Easily install PHP extensions in Docker containers.  * [Expose](https://github.com/beyondcode/expose) - An open source PHP tunneling service.  * [Lando](https://lando.dev/) - Push-button development environments.  * [Laravel Homestead](https://laravel.com/docs/master/homestead) - A local development environment for Laravel.   * [Laradock](http://laradock.io/) - A full PHP development environment based on Docker.  * [Puppet](https://puppet.com/) - A server automation framework and application.  * [Takeout](https://github.com/tighten/takeout) - A Docker-based development-only dependency manager.  * [Vagrant](https://www.vagrantup.com/) - A portable development environment utility.    ### Virtual Machines  *Alternative PHP virtual machines.*    * [Hack](https://hacklang.org/) - A programming language for HHVM.  * [HHVM](https://github.com/facebook/hhvm) - A Virtual Machine, Runtime and JIT for PHP by Facebook.  * [PeachPie](https://github.com/peachpiecompiler/peachpie) - PHP compiler and runtime for .NET and .NET Core.    ### Text Editors and IDEs  *Text Editors and Integrated Development Environments (IDE) with support for PHP.*    * [Eclipse for PHP Developers](https://www.eclipse.org/downloads/) - A PHP IDE based on the Eclipse platform.  * [Apache NetBeans](https://netbeans.apache.org/) - An IDE with support for PHP and HTML5.  * [PhpStorm](https://www.jetbrains.com/phpstorm/) - A commercial PHP IDE.  * [VS Code](https://code.visualstudio.com/) - An open source code editor.    ### Web Applications  *Web-based applications and tools.*    * [3V4L](https://3v4l.org/) - An online PHP & HHVM shell.  * [Adminer](https://www.adminer.org/) - Database management in a single PHP file.  * [Cachet](https://github.com/cachethq/cachet) - The open source status page system.  * [DBV](https://github.com/victorstanciu/dbv) - A database version control application.  * [Lychee](https://github.com/electerious/Lychee) - An easy to use and great looking photo-management-system.  * [MailCatcher](https://github.com/sj26/mailcatcher) - A web tool for capturing and viewing emails.  * [phpMyAdmin](https://github.com/phpmyadmin/phpmyadmin) - A web interface for MySQL/MariaDB.  * [PHP Queue](https://github.com/CoderKungfu/php-queue) - An application for managing queueing backends.  * [phpRedisAdmin](https://github.com/ErikDubbelboer/phpRedisAdmin) - A simple web interface to manage [Redis](https://redis.io/) databases.  * [PHPSandbox](https://phpsandbox.io) - An online IDE for PHP in the browser.    ### Infrastructure  *Infrastructure for providing PHP applications and services.*    * [appserver.io](https://github.com/appserver-io/appserver) - A multithreaded application server for PHP, written in PHP.  * [php-pm](https://github.com/php-pm/php-pm) - A process manager, supercharger and load balancer for PHP applications.  * [RoadRunner](https://github.com/spiral/roadrunner) - High-performance PHP application server, load-balancer and process manager.    # Resources  Various resources, such as books, websites and articles, for improving your PHP development skills and knowledge.    ### PHP Websites  *Useful PHP-related websites.*    * [libs.garden: PHP](https://libs.garden/php) - An overview of fastest growing PHP libraries.  * [Nomad PHP](https://nomadphp.com/) - A online PHP learning resource.  * [Laravel News](https://laravel-news.com/) - The official Laravel blog.  * [PHP Annotated Monthly](https://blog.jetbrains.com/phpstorm/category/php-annotated-monthly/) - A monthly digest of PHP news.  * [PHP Best Practices](https://phpbestpractices.org/) - A PHP best practice guide.  * [PHP FIG](https://www.php-fig.org/) - The PHP Framework Interoperability Group.  * [PHP Package Development Standards](http://php-pds.com) - Package development standards for PHP.  * [PHP School](https://www.phpschool.io/) - Open Source Learning for PHP.  * [PHP Security](https://phpsecurity.readthedocs.io/en/latest/index.html) - A guide to PHP security.  * [PHP The Right Way](https://phptherightway.com/) - A PHP best practice quick reference guide.  * [PHP UG](https://php.ug) - A website to help people locate their nearest PHP user group (UG).  * [PHP Versions](http://phpversions.info/) - Lists which versions of PHP are available on several popular web hosts.  * [PHP Watch](https://php.watch/) - PHP articles, news, upcoming changes, RFCs and more.  * [PHP Weekly](http://www.phpweekly.com/archive.html) - A weekly PHP newsletter.  * [Securing PHP](https://www.securingphp.com/) - A newsletter about PHP security and library recommendations.  * [Seven PHP](https://7php.com/) - A website that interviews members of the PHP community.    ### PHP Books  *Fantastic PHP-related books.*    * [Domain-Driven Design in PHP](https://leanpub.com/ddd-in-php) - Real examples written in PHP showcasing DDD Architectural Styles.  * [Functional Programming in PHP](https://www.functionalphp.com/) - This book will show you how to leverage these new PHP5.3+ features by understanding functional programming principles  * [Grumpy PHPUnit](https://leanpub.com/grumpy-phpunit) - A book about unit testing with PHPUnit by Chris Hartjes.  * [Mastering Object-Orientated PHP](https://www.brandonsavage.net/) - A book about object-orientated PHP by Brandon Savage.  * [Modern PHP New Features and Good Practices](https://www.oreilly.com/library/view/~/9781491905173/) - A book about new PHP features and best practices by Josh Lockhart.  * [Modernizing Legacy Applications in PHP](https://leanpub.com/mlaphp) - A book about modernizing legacy PHP applications by Paul M. Jones.  * [PHP 7 Upgrade Guide](https://leanpub.com/php7) - An ebook covering all of the features and changes in PHP 7 by Colin O'Dell.  * [PHP Pandas](https://daylerees.com/php-pandas/) - A book about learning to write PHP by Dayle Rees.  * [Scaling PHP Applications](https://www.scalingphpbook.com) - An ebook about scaling PHP applications by Steve Corona.  * [Securing PHP: Core Concepts](https://leanpub.com/securingphp-coreconcepts) - A book about common security terms and practices for PHP by Chris Cornutt.  * [Signaling PHP](https://leanpub.com/signalingphp) - A book about catching PCNTL signals in CLI scripts by Cal Evans.  * [The Grumpy Programmer's Guide to Building Testable PHP Applications](https://leanpub.com/grumpy-testing) - A book about building testing PHP applications by Chris Hartjes.  * [XML Parsing with PHP](https://www.phparch.com/books/xml-parsing-with-php/) - This book covers parsing and validating XML documents, leveraging XPath expressions, and working with namespaces as well as how to create and modify XML files programmatically.    ### PHP Videos  *Fantastic PHP-related videos.*    * [Nomad PHP Lightning Talks](https://www.youtube.com/c/nomadphp) - 10 to 15 minute Lightning Talks by PHP community members.  * [PHP UK Conference](https://www.youtube.com/user/phpukconference/videos) - A collection of videos from the PHP UK Conference.  * [Programming with Anthony](https://www.youtube.com/playlist?list=PLM-218uGSX3DQ3KsB5NJnuOqPqc5CW2kW) - A video series by Anthony Ferrara.  * [Taking PHP Seriously](https://www.infoq.com/presentations/php-history/) - A talk outlining PHP's strengths by Keith Adams of Facebook.  * [Laracasts](https://laracasts.com) - Screencasts about Laravel, Vue JS and more.  * [Laravel YouTube Channel](https://www.youtube.com/channel/UCfO2GiQwb-cwJTb1CuRSkwg) - The official Laravel YouTube channel.  * [SymfonyCasts](https://symfonycasts.com/) - Screencasts and tutorials about PHP and Symfony.    ### PHP Podcasts  *Podcasts with a focus on PHP topics.*    * [Laravel Podcast](https://laravelpodcast.com/) - Laravel and PHP development news and discussion.  * [PHP Internals News](https://phpinternals.news) - A podcast about PHP internals.  * [PHP Roundtable](https://www.phproundtable.com/) - The PHP Roundtable is a casual gathering of developers discussing topics that PHP nerds care about.  * [PHP Town Hall](https://phptownhall.com/) - A casual PHP podcast by Ben Edmunds and Phil Sturgeon.  * [Voices of the ElePHPant](https://voicesoftheelephpant.com/) Interviews with the people that make the PHP community special.    ### PHP Newsletters  *PHP-related news directly to your inbox.*    * [PHP Weekly](http://www.phpweekly.com/) - A weekly newsletter about PHP.    ### PHP Reading  *PHP-releated reading materials.*    * [php[architect]](https://www.phparch.com/magazine/) - A monthly magazine dedicated to PHP.    ### PHP Internals Reading  *Reading materials related to the PHP internals or performance.*    * [PHP RFCs](https://wiki.php.net/rfc) - The home of PHP RFCs (Request for Comments).  * [Externals](https://externals.io/) - PHP internal discussions.   * [PHP RFC Watch](https://php-rfc-watch.beberlei.de/) - Watch the latest PHP [RFCs](https://wiki.php.net/rfc).  * [PHP Internals Book](http://www.phpinternalsbook.com) - An online book about PHP internals, written by three core developers. """
Big data;https://github.com/dagster-io/dagster;"""<p align=""center"">  <img src=""assets/dagster-logo.png"" />  <br /><br />  <a href=""https://badge.fury.io/py/dagster""><img src=""https://badge.fury.io/py/dagster.svg""></>  <a href=""https://coveralls.io/github/dagster-io/dagster?branch=master""><img src=""https://coveralls.io/repos/github/dagster-io/dagster/badge.svg?branch=master""></a>  <a href=""https://buildkite.com/dagster/dagster""><img src=""https://badge.buildkite.com/888545beab829e41e5d7303db15525a2bc3b0f0e33a72759ac.svg?branch=master""></a>  <a href=""https://dagster-slackin.herokuapp.com/""><img src=""https://dagster-slackin.herokuapp.com/badge.svg""></a>  </p>    # Dagster    An orchestration platform for the development, production, and observation of data assets.    Dagster lets you define jobs in terms of the data flow between reusable, logical components, then test locally and run anywhere. With a unified view of jobs and the assets they produce, Dagster can schedule and orchestrate Pandas, Spark, SQL, or anything else that Python can invoke.    Dagster is designed for data platform engineers, data engineers, and full-stack data scientists. Building a data platform with Dagster makes your stakeholders more independent and your systems more robust. Developing data pipelines with Dagster makes testing easier and deploying faster.    ### Develop and test locally, then deploy anywhere    With Dagsterâ€™s pluggable execution, the same computations can run in-process against your local file system, or on a distributed work queue against your production data lake. You can set up Dagsterâ€™s web interface in a minute on your laptop, deploy it on-premise, or in any cloud.    ### Model and type the data produced and consumed by each step    Dagster models data dependencies between steps in your orchestration graph and handles passing data between them. Optional typing on inputs and outputs helps catch bugs early.    ### Link data to computations    Dagsterâ€™s Asset Manager tracks the data sets and ML models produced by your jobs, so you can understand how they were generated and trace issues when they donâ€™t look how you expect.    ### Build a self-service data platform    Dagster helps platform teams build systems for data practitioners. Jobs are built from shared, reusable, configurable data processing and infrastructure components. Dagit, Dagsterâ€™s web interface, lets anyone inspect these objects and discover how to use them.    ### Avoid dependency nightmares    Dagsterâ€™s repository model lets you isolate codebases so that problems in one job donâ€™t bring down the rest. Each job can have its own package dependencies and Python version. Jobs are run in isolated processes so user code issues can't bring the system down.    ### Debug pipelines from a rich UI    Dagit, Dagsterâ€™s web interface, includes expansive facilities for understanding the jobs it orchestrates. When inspecting a run of your job, you can query over logs, discover the most time consuming tasks via a Gantt chart, re-execute subsets of steps, and more.    ## Getting Started    ### Installation    Dagster is available on PyPI, and officially supports Python 3.6+.    ```bash  $ pip install dagster dagit  ```    This installs two modules:    - **Dagster**: the core programming model and abstraction stack; stateless, single-node,    single-process and multi-process execution engines; and a CLI tool for driving those engines.  - **Dagit**: the UI for developing and operating Dagster pipelines, including a DAG browser, a    type-aware config editor, and a live execution interface.    ### Learn    Next, jump right into our [tutorial](https://docs.dagster.io/tutorial/), read our [complete documentation](https://docs.dagster.io), or check out our [GitHub Discussions](https://github.com/dagster-io/dagster/discussions). If you're actively using Dagster or have questions on  getting started, we'd love to hear from you:    <br />  <p align=""center"">  <a href=""https://dagster.io/slack""><img src=""https://user-images.githubusercontent.com/609349/63558739-f60a7e00-c502-11e9-8434-c8a95b03ce62.png"" width=160px; /></a>  </p>      ## Contributing    For details on contributing or running the project for development, check out our [contributing  guide](https://docs.dagster.io/community/contributing/). <br />    ## Integrations    Dagster works with the tools and systems that you're already using with your data, including:    <table>  	<thead>  		<tr style=""background-color: #ddd"" align=""center"">  			<td colspan=2><b>Integration</b></td>  			<td><b>Dagster Library</b></td>  		</tr>  	</thead>  	<tbody>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle""  src=""https://user-images.githubusercontent.com/609349/57987547-a7e36b80-7a37-11e9-95ae-4c4de2618e87.png""></td>  			<td style=""border-left: 0px""> <b>Apache Airflow</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-airflow"" />dagster-airflow</a><br />Allows Dagster pipelines to be scheduled and executed, either containerized or uncontainerized, as <a href=""https://github.com/apache/airflow"">Apache Airflow DAGs</a>.</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle""  src=""https://user-images.githubusercontent.com/609349/57987976-5ccc5700-7a3d-11e9-9fa5-1a51299b1ccb.png""></td>  			<td style=""border-left: 0px""> <b>Apache Spark</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-spark"" />dagster-spark</a> &middot;Â <a href=""https://docs.dagster.io/_apidocs/libraries/dagster-pyspark"" />dagster-pyspark</a>  			<br />Libraries for interacting with Apache Spark and PySpark.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle""  src=""https://user-images.githubusercontent.com/609349/58348728-48f66b80-7e16-11e9-9e9f-1a0fea9a49b4.png""></td>  			<td style=""border-left: 0px""> <b>Dask</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-dask"" />dagster-dask</a>  			<br />Provides a Dagster integration with Dask / Dask.Distributed.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/58349731-f36f8e00-7e18-11e9-8a2e-86e086caab66.png""></td>  			<td style=""border-left: 0px""> <b>Datadog</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-datadog"" />dagster-datadog</a>  			<br />Provides a Dagster resource for publishing metrics to Datadog.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/57987809-bf245800-7a3b-11e9-8905-494ed99d0852.png"" />  			&nbsp;/&nbsp; <img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/57987827-fa268b80-7a3b-11e9-8a18-b675d76c19aa.png"">  			</td>  			<td style=""border-left: 0px""> <b>Jupyter / Papermill</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagstermill"" />dagstermill</a><br />Built on the <a href=""https://github.com/nteract/papermill"">papermill library</a>, dagstermill is meant for integrating productionized Jupyter notebooks into dagster pipelines.</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle""  src=""https://user-images.githubusercontent.com/609349/57988016-f431aa00-7a3d-11e9-8cb6-1309d4246b27.png""></td>  			<td style=""border-left: 0px""> <b>PagerDuty</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-pagerduty"" />dagster-pagerduty</a>  			<br />A library for creating PagerDuty alerts from Dagster workflows.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/58349397-fcac2b00-7e17-11e9-900c-9ab8cf7cb64a.png""></td>  			<td style=""border-left: 0px""> <b>Snowflake</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-snowflake"" />dagster-snowflake</a>  			<br />A library for interacting with the Snowflake Data Warehouse.  			</td>  		</tr>  		<tr style=""background-color: #ddd"">  			<td colspan=2 align=""center""><b>Cloud Providers</b></td>  			<td><b></b></td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/57987557-c2b5e000-7a37-11e9-9310-c274481a4682.png""> </td>  			<td style=""border-left: 0px""><b>AWS</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-aws"" />dagster-aws</a>  			<br />A library for interacting with Amazon Web Services. Provides integrations with Cloudwatch, S3, EMR, and Redshift.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/84176312-0bbb4680-aa36-11ea-9580-a70758b12161.png""> </td>  			<td style=""border-left: 0px""><b>Azure</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-azure"" />dagster-azure</a>  			<br />A library for interacting with Microsoft Azure.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/57987566-f98bf600-7a37-11e9-81fa-b8ca1ea6cc1e.png""> </td>  			<td style=""border-left: 0px""><b>GCP</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-gcp"" />dagster-gcp</a>  			<br />A library for interacting with Google Cloud Platform. Provides integrations with GCS, BigQuery, and Cloud Dataproc.  			</td>  		</tr>  	</tbody>  </table>    This list is growing as we are actively building more integrations, and we welcome contributions! """
Big data;https://github.com/apache/incubator-superset;"""<!--  Licensed to the Apache Software Foundation (ASF) under one  or more contributor license agreements.  See the NOTICE file  distributed with this work for additional information  regarding copyright ownership.  The ASF licenses this file  to you under the Apache License, Version 2.0 (the  ""License""); you may not use this file except in compliance  with the License.  You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing,  software distributed under the License is distributed on an  ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY  KIND, either express or implied.  See the License for the  specific language governing permissions and limitations  under the License.  -->    # Superset    [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)  [![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/apache/superset?sort=semver)](https://github.com/apache/superset/tree/latest)  [![Build Status](https://github.com/apache/superset/workflows/Python/badge.svg)](https://github.com/apache/superset/actions)  [![PyPI version](https://badge.fury.io/py/apache-superset.svg)](https://badge.fury.io/py/apache-superset)  [![Coverage Status](https://codecov.io/github/apache/superset/coverage.svg?branch=master)](https://codecov.io/github/apache/superset)  [![PyPI](https://img.shields.io/pypi/pyversions/apache-superset.svg?maxAge=2592000)](https://pypi.python.org/pypi/apache-superset)  [![Get on Slack](https://img.shields.io/badge/slack-join-orange.svg)](https://join.slack.com/t/apache-superset/shared_invite/zt-16jvzmoi8-sI7jKWp~xc2zYRe~NqiY9Q)  [![Documentation](https://img.shields.io/badge/docs-apache.org-blue.svg)](https://superset.apache.org)    <img    src=""https://github.com/apache/superset/raw/master/superset-frontend/src/assets/branding/superset-logo-horiz-apache.png""    alt=""Superset""    width=""500""  />    A modern, enterprise-ready business intelligence web application.    [**Why Superset?**](#why-superset) |  [**Supported Databases**](#supported-databases) |  [**Installation and Configuration**](#installation-and-configuration) |  [**Release Notes**](RELEASING/README.md#release-notes-for-recent-releases) |  [**Get Involved**](#get-involved) |  [**Contributor Guide**](#contributor-guide) |  [**Resources**](#resources) |  [**Organizations Using Superset**](RESOURCES/INTHEWILD.md)    ## Why Superset?    Superset is a modern data exploration and data visualization platform. Superset can replace or augment proprietary business intelligence tools for many teams. Superset integrates well with a variety of data sources.    Superset provides:    - A **no-code interface** for building charts quickly  - A powerful, web-based **SQL Editor** for advanced querying  - A **lightweight semantic layer** for quickly defining custom dimensions and metrics  - Out of the box support for **nearly any SQL** database or data engine  - A wide array of **beautiful visualizations** to showcase your data, ranging from simple bar charts to geospatial visualizations  - Lightweight, configurable **caching layer** to help ease database load  - Highly extensible **security roles and authentication** options  - An **API** for programmatic customization  - A **cloud-native architecture** designed from the ground up for scale    ## Screenshots & Gifs    **Large Gallery of Visualizations**    <kbd><a href=""https://superset.apache.org/gallery""><img title=""Gallery"" src=""superset-frontend/src/assets/images/screenshots/gallery.jpg""/></a></kbd><br/>    **Craft Beautiful, Dynamic Dashboards**    <kbd><img title=""View Dashboards"" src=""superset-frontend/src/assets/images/screenshots/slack_dash.jpg""/></kbd><br/>    **No-Code Chart Builder**    <kbd><img title=""Slice & dice your data"" src=""superset-frontend/src/assets/images/screenshots/explore.jpg""/></kbd><br/>    **Powerful SQL Editor**    <kbd><img title=""SQL Lab"" src=""superset-frontend/src/assets/images/screenshots/sql_lab.jpg""/></kbd><br/>    ## Supported Databases    Superset can query data from any SQL-speaking datastore or data engine (Presto, Trino, Athena, [and more](https://superset.apache.org/docs/databases/installing-database-drivers/)) that has a Python DB-API driver and a SQLAlchemy dialect.    Here are some of the major database solutions that are supported:    <p align=""center"">    <img src=""superset-frontend/src/assets/images/redshift.png"" alt=""redshift"" border=""0"" width=""106"" height=""41""/>    <img src=""superset-frontend/src/assets/images/google-biquery.png"" alt=""google-biquery"" border=""0"" width=""114"" height=""43""/>    <img src=""superset-frontend/src/assets/images/snowflake.png"" alt=""snowflake"" border=""0"" width=""152"" height=""46""/>    <img src=""superset-frontend/src/assets/images/trino.png"" alt=""trino"" border=""0"" width=""46"" height=""46""/>    <img src=""superset-frontend/src/assets/images/presto.png"" alt=""presto"" border=""0"" width=""152"" height=""46""/>    <img src=""superset-frontend/src/assets/images/druid.png"" alt=""druid"" border=""0"" width=""135"" height=""37"" />    <img src=""superset-frontend/src/assets/images/firebolt.png"" alt=""firebolt"" border=""0"" width=""133"" height=""21.5"" />    <img src=""superset-frontend/src/assets/images/timescale.png"" alt=""timescale"" border=""0"" width=""102"" height=""26.8"" />      <img src=""superset-frontend/src/assets/images/rockset.png"" alt=""rockset"" border=""0"" width=""125"" height=""51"" />    <img src=""superset-frontend/src/assets/images/postgresql.png"" alt=""postgresql"" border=""0"" width=""132"" height=""81"" />    <img src=""superset-frontend/src/assets/images/mysql.png"" alt=""mysql"" border=""0"" width=""119"" height=""62"" />    <img src=""superset-frontend/src/assets/images/mssql-server.png"" alt=""mssql-server"" border=""0"" width=""93"" height=""74"" />    <img src=""superset-frontend/src/assets/images/db2.png"" alt=""db2"" border=""0"" width=""62"" height=""62"" />    <img src=""superset-frontend/src/assets/images/sqlite.png"" alt=""sqlite"" border=""0"" width=""102"" height=""45"" />    <img src=""superset-frontend/src/assets/images/sybase.png"" alt=""sybase"" border=""0"" width=""128"" height=""47"" />    <img src=""superset-frontend/src/assets/images/mariadb.png"" alt=""mariadb"" border=""0"" width=""83"" height=""63"" />    <img src=""superset-frontend/src/assets/images/vertica.png"" alt=""vertica"" border=""0"" width=""128"" height=""40"" />    <img src=""superset-frontend/src/assets/images/oracle.png"" alt=""oracle"" border=""0"" width=""121"" height=""66"" />    <img src=""superset-frontend/src/assets/images/firebird.png"" alt=""firebird"" border=""0"" width=""86"" height=""56"" />    <img src=""superset-frontend/src/assets/images/greenplum.png"" alt=""greenplum"" border=""0"" width=""140"" height=""45"" />    <img src=""superset-frontend/src/assets/images/clickhouse.png"" alt=""clickhouse"" border=""0"" width=""133"" height=""34"" />    <img src=""superset-frontend/src/assets/images/exasol.png"" alt=""exasol"" border=""0"" width=""106"" height=""59"" />    <img src=""superset-frontend/src/assets/images/monet-db.png"" alt=""monet-db"" border=""0"" width=""106"" height=""46"" />    <img src=""superset-frontend/src/assets/images/apache-kylin.png"" alt=""apache-kylin"" border=""0"" width=""56"" height=""64""/>    <img src=""superset-frontend/src/assets/images/hologres.png"" alt=""hologres"" border=""0"" width=""71"" height=""64""/>    <img src=""superset-frontend/src/assets/images/netezza.png"" alt=""netezza"" border=""0"" width=""64"" height=""64""/>    <img src=""superset-frontend/src/assets/images/pinot.png"" alt=""pinot"" border=""0"" width=""165"" height=""64""/>    <img src=""superset-frontend/src/assets/images/teradata.png"" alt=""teradata"" border=""0"" width=""165"" height=""64""/>    <img src=""superset-frontend/src/assets/images/yugabyte.png"" alt=""yugabyte"" border=""0"" width=""180"" height=""31""/>  </p>    **A more comprehensive list of supported databases** along with the configuration instructions can be found  [here](https://superset.apache.org/docs/databases/installing-database-drivers).    Want to add support for your datastore or data engine? Read more [here](https://superset.apache.org/docs/frequently-asked-questions#does-superset-work-with-insert-database-engine-here) about the technical requirements.    ## Installation and Configuration    [Extended documentation for Superset](https://superset.apache.org/docs/installation/installing-superset-using-docker-compose)    ## Get Involved    - Ask and answer questions on [StackOverflow](https://stackoverflow.com/questions/tagged/apache-superset) using the **apache-superset** tag  - [Join our community's Slack](https://join.slack.com/t/apache-superset/shared_invite/zt-16jvzmoi8-sI7jKWp~xc2zYRe~NqiY9Q)    and please read our [Slack Community Guidelines](https://github.com/apache/superset/blob/master/CODE_OF_CONDUCT.md#slack-community-guidelines)  - [Join our dev@superset.apache.org Mailing list](https://lists.apache.org/list.html?dev@superset.apache.org)    ## Contributor Guide    Interested in contributing? Check out our  [CONTRIBUTING.md](https://github.com/apache/superset/blob/master/CONTRIBUTING.md)  to find resources around contributing along with a detailed guide on  how to set up a development environment.    ## Resources    - Getting Started with Superset    - [Superset in 2 Minutes using Docker Compose](https://superset.apache.org/docs/installation/installing-superset-using-docker-compose#installing-superset-locally-using-docker-compose)    - [Installing Database Drivers](https://superset.apache.org/docs/databases/docker-add-drivers/)    - [Building New Database Connectors](https://preset.io/blog/building-database-connector/)    - [Create Your First Dashboard](https://superset.apache.org/docs/creating-charts-dashboards/first-dashboard)    - [Comprehensive Tutorial for Contributing Code to Apache Superset  ](https://preset.io/blog/tutorial-contributing-code-to-apache-superset/)  - [Documentation for Superset End-Users (by Preset)](https://docs.preset.io/docs/terminology)  - Deploying Superset    - [Official Docker image](https://hub.docker.com/r/apache/superset)    - [Helm Chart](https://github.com/apache/superset/tree/master/helm/superset)  - Recordings of Past [Superset Community Events](https://preset.io/events)    - [Live Demo: Interactive Time-series Analysis with Druid and Superset](https://preset.io/events/2021-03-02-interactive-time-series-analysis-with-druid-and-superset/)    - [Live Demo: Visualizing MongoDB and Pinot Data using Trino](https://preset.io/events/2021-04-13-visualizing-mongodb-and-pinot-data-using-trino/)  	- [Superset Contributor Bootcamp](https://preset.io/events/superset-contributor-bootcamp-dec-21/)  	- [Introduction to the Superset API](https://preset.io/events/introduction-to-the-superset-api/)  	- [Apache Superset 1.3 Meetup](https://preset.io/events/apache-superset-1-3/)  	- [Building a Database Connector for Superset](https://preset.io/events/2021-02-16-building-a-database-connector-for-superset/)  - Visualizations    - [Building Custom Viz Plugins](https://superset.apache.org/docs/installation/building-custom-viz-plugins)    - [Managing and Deploying Custom Viz Plugins](https://medium.com/nmc-techblog/apache-superset-manage-custom-viz-plugins-in-production-9fde1a708e55)    - [Why Apache Superset is Betting on Apache ECharts](https://preset.io/blog/2021-4-1-why-echarts/)    - [Superset API](https://superset.apache.org/docs/rest-api) """
Big data;https://github.com/WeBankFinTech/Linkis;"""Linkis  ==========    [![License](https://img.shields.io/badge/license-Apache%202-4EB1BA.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)    [English](README.md) | [ä¸­æ–‡](README_CN.md)    # Introduction     Linkis builds a layer of computation middleware between upper applications and underlying engines. By using standard interfaces such as REST/WS/JDBC provided by Linkis, the upper applications can easily access the underlying engines such as MySQL/Spark/Hive/Presto/Flink, etc., and achieve the intercommunication of user resources like unified variables, scripts, UDFs, functions and resource files at the same time.    As a computation middleware, Linkis provides powerful connectivity, reuse, orchestration, expansion, and governance capabilities. By decoupling the application layer and the engine layer, it simplifies the complex network call relationship, and thus reduces the overall complexity and saves the development and maintenance costs as well.    Since the first release of Linkis in 2019, it has accumulated more than **700** trial companies and **1000+** sandbox trial users, which involving diverse industries, from finance, banking, tele-communication, to manufactory, internet companies and so on. Lots of companies have already used Linkis as a unified entrance for the underlying computation and storage engines of the big data platform.      ![linkis-intro-01](https://user-images.githubusercontent.com/7869972/148767375-aeb11b93-16ca-46d7-a30e-92fbefe2bd5e.png)    ![linkis-intro-03](https://user-images.githubusercontent.com/7869972/148767380-c34f44b2-9320-4633-9ec8-662701f41d15.png)    # Features    - **Support for diverse underlying computation storage engines**.        Currently supported computation/storage engines: Spark, Hive, Python, Presto, ElasticSearch, MLSQL, TiSpark, JDBC, Shell, etc;            Computation/storage engines to be supported: Flink(Supported in version >=1.0.2), Impala, etc;            Supported scripting languages: SparkSQL, HiveQL, Python, Shell, Pyspark, R, Scala and JDBC, etc.        - **Powerful task/request governance capabilities**. With services such as Orchestrator, Label Manager and customized Spring Cloud Gateway, Linkis is able to provide multi-level labels based, cross-cluster/cross-IDC fine-grained routing, load balance, multi-tenancy, traffic control, resource control, and orchestration strategies like dual-active, active-standby, etc.      - **Support full stack computation/storage engine**. As a computation middleware, it will receive, execute and manage tasks and requests for various computation storage engines, including batch tasks, interactive query tasks, real-time streaming tasks and storage tasks;    - **Resource management capabilities**.  ResourceManager is not only capable of managing resources for Yarn and Linkis EngineManger as in Linkis 0.X, but also able to provide label-based multi-level resource allocation and recycling, allowing itself to have powerful resource management capabilities across mutiple Yarn clusters and mutiple computation resource types;    - **Unified Context Service**. Generate Context ID for each task/request,  associate and manage user and system resource files (JAR, ZIP, Properties, etc.), result set, parameter variable, function, etc., across user, system, and computing engine. Set in one place, automatic reference everywhere;    - **Unified materials**. System and user-level unified material management, which can be shared and transferred across users and systems.    # Supported engine types    | **Engine** | **Supported Version** | **Linkis 0.X version requirement**| **Linkis 1.X version requirement** | **Description** |  |:---- |:---- |:---- |:---- |:---- |  |Flink |1.12.2|\>=dev-0.12.0, PR #703 not merged yet.|>=1.0.2|	Flink EngineConn. Supports FlinkSQL code, and also supports Flink Jar to Linkis Manager to start a new Yarn application.|  |Impala|\>=3.2.0, CDH >=6.3.0""|\>=dev-0.12.0, PR #703 not merged yet.|ongoing|Impala EngineConn. Supports Impala SQL.|  |Presto|\>= 0.180|\>=0.11.0|ongoing|Presto EngineConn. Supports Presto SQL.|  |ElasticSearch|\>=6.0|\>=0.11.0|ongoing|ElasticSearch EngineConn. Supports SQL and DSL code.|  |Shell|Bash >=2.0|\>=0.9.3|\>=1.0.0_rc1|Shell EngineConn. Supports shell code.|  |MLSQL|\>=1.1.0|\>=0.9.1|ongoing|MLSQL EngineConn. Supports MLSQL code.|  |JDBC|MySQL >=5.0, Hive >=1.2.1|\>=0.9.0|\>=1.0.0_rc1|JDBC EngineConn. Supports MySQL and HiveQL code.|  |Spark|Apache 2.0.0~2.4.7, CDH >=5.4.0|\>=0.5.0|\>=1.0.0_rc1|Spark EngineConn. Supports SQL, Scala, Pyspark and R code.|  |Hive|Apache >=1.0.0, CDH >=5.4.0|\>=0.5.0|\>=1.0.0_rc1|Hive EngineConn. Supports HiveQL code.|  |Hadoop|Apache >=2.6.0, CDH >=5.4.0|\>=0.5.0|ongoing|Hadoop EngineConn. Supports Hadoop MR/YARN application.|  |Python|\>=2.6|\>=0.5.0|\>=1.0.0_rc1|Python EngineConn. Supports python code.|  |TiSpark|1.1|\>=0.5.0|ongoing|TiSpark EngineConn. Support querying TiDB data by SparkSQL.|    # Ecosystem    | Component | Description | Linkis 0.x(recommend 0.11.0) Compatible | Linkis 1.x(recommend 1.0.3) Compatible |  | --------------- | -------------------------------------------------------------------- | --------- | --------- |  | [**DataSphereStudio**](https://github.com/WeBankFinTech/DataSphereStudio/blob/master/README.md) | DataSphere Studio (DSS for short) is WeDataSphere, a one-stop data application development management portal. | DSS 0.9.1[released] | **DSS 1.0.1[released]** |  | [**Scriptis**](https://github.com/WeBankFinTech/Scriptis) | Support online script writing such as SQL, Pyspark, HiveQL, etc., submit to [Linkis](https://github.com/apache/incubator-linkis) to perform data analysis web tools. | Scriptis merged in DSSï¼ˆDSS 0.9.1[released]ï¼‰ | **In DSS 1.0.1[released]** |  | [**Schedulis**](https://github.com/WeBankFinTech/Schedulis) | Workflow task scheduling system based on Azkaban secondary development, with financial-grade features such as high performance, high availability and multi-tenant resource isolation. | Schedulis 0.6.1[released] |  **Schedulis0.6.2 [released]** |  | [**Qualitis**](https://github.com/WeBankFinTech/Qualitis) | Data quality verification tool, providing data verification capabilities such as data integrity and correctness  | Qualitis 0.8.0[released] | **Qualitis 0.9.1 [released]** |  | [**Streamis**](https://github.com/WeBankFinTech/Streamis) | Streaming application development management tool. It supports the release of Flink Jar and Flink SQL, and provides the development, debugging and production management capabilities of streaming applications, such as: start-stop, status monitoring, checkpoint, etc. | **No support** | **Streamis 0.1.0 [released]** |  | [**Exchangis**](https://github.com/WeBankFinTech/Exchangis) | A data exchange platform that supports data transmission between structured and unstructured heterogeneous data sources, the upcoming Exchangis1. 0, will be connected with DSS workflow | **No support** | **Exchangis 1.0.0 [developing]**|  | [**Visualis**](https://github.com/WeBankFinTech/Visualis) | A data visualization BI tool based on the second development of Davinci, an open source project of CreditEase, provides users with financial-level data visualization capabilities in terms of data security. | Visualis 0.5.0[released]| **Visualis 1.0.0[developing]**|  | [**Prophecis**](https://github.com/WeBankFinTech/Prophecis) | A one-stop machine learning platform that integrates multiple open source machine learning frameworks. Prophecis' MLFlow can be connected to DSS workflow through AppConn. | Prophecis 0.2.2[released] | **Prophecis 0.3.0 [developing]** |    # Download    Please go to the [Linkis Releases Page](https://github.com/apache/incubator-linkis/releases) to download a compiled distribution or a source code package of Linkis.    # Compile and deploy  Please follow [Compile Guide](https://linkis.apache.org/docs/latest/development/linkis_compile_and_package) to compile Linkis from source code.    Please refer to [Deployment Documents](https://linkis.apache.org/docs/latest/deployment/quick_deploy) to do the deployment.      # Examples and Guidance  You can find examples and guidance for how to use and manage Linkis in [User Manual](https://linkis.apache.org/docs/latest/user_guide/overview), [Engine Usage Documents](https://linkis.apache.org/docs/latest/engine_usage/overview) and [API Documents](https://linkis.apache.org/docs/latest/api/overview).    # Documentation    The documentation of linkis is in [Linkis-Website Git Repository](https://github.com/apache/incubator-linkis-website).    # Architecture  Linkis services could be divided into three categories: computation governance services, public enhancement services and microservice governance services.  - The computation governance services, support the 3 major stages of processing a task/request: submission -> preparation -> execution;  - The public enhancement services, including the material library service, context service, and data source service;  - The microservice governance services, including Spring Cloud Gateway, Eureka and Open Feign.    Below is the Linkis architecture diagram. You can find more detailed architecture docs in [Linkis-Doc/Architecture](https://linkis.apache.org/docs/latest/architecture/overview).  ![architecture](https://user-images.githubusercontent.com/7869972/148767383-f87e84ba-5baa-4125-8b6e-d0aa4f7d3a66.png)    Based on Linkis the computation middleware, we've built a lot of applications and tools on top of it in the big data platform suite [WeDataSphere](https://github.com/WeBankFinTech/WeDataSphere). Below are the currently available open-source projects. More projects upcoming, please stay tuned.    ![wedatasphere_stack_Linkis](https://user-images.githubusercontent.com/7869972/148767389-049361df-3609-4c2f-a4e2-c904c273300e.png)    # Contributing    Contributions are always welcomed, we need more contributors to build Linkis together. either code, or doc, or other supports that could help the community.    For code and documentation contributions, please follow the [contribution guide](https://linkis.apache.org/community/how-to-contribute).    # Contact Us    Any questions or suggestions please kindly submit an issue.    You can scan the QR code below to join our WeChat and QQ group to get more immediate response.    ![introduction05](https://user-images.githubusercontent.com/7869972/148767386-0663f833-547d-4c30-8876-081bb966ffb8.png)    Meetup videos on [Bilibili](https://space.bilibili.com/598542776?from=search&seid=14344213924133040656).    # Who is Using Linkis    We opened [an issue](https://github.com/apache/incubator-linkis/issues/23) for users to feedback and record who is using Linkis.    Since the first release of Linkis in 2019, it has accumulated more than **700** trial companies and **1000+** sandbox trial users, which involving diverse industries, from finance, banking, tele-communication, to manufactory, internet companies and so on. """
Big data;https://github.com/SnappyDataInc/snappydata;"""<span style=""background-color:yellow"">  This repository is provided for legacy users and informational purposes only. It may contain security vulnerabilities in the code itself or its dependencies. TIBCO provides no updates, including security updates, to this code. Consistent with the terms of the Apache License 2.0 that apply to the TIBCO code in this repository, the code is provided on an ""as is"" basis, without any warranties or conditions of any kind and in no event and under no legal theory shall TIBCO be liable to you for damages arising as a result of the use or inability to use the code.  </span>      ## Introduction   SnappyData (aka TIBCO ComputeDB) is a distributed, in-memory optimized analytics database. SnappyData delivers high throughput, low latency, and high concurrency for unified analytics workload. By fusing an in-memory hybrid database inside Apache Spark, it provides analytic query processing, mutability/transactions, access to virtually all big data sources and stream processing all in one unified cluster.    One common use case for SnappyData is to provide analytics at interactive speeds over large volumes of data with minimal or no pre-processing of the dataset. For instance, there is no need to often pre-aggregate/reduce or generate cubes over your large data sets for ad-hoc visual analytics. This is made possible by smartly managing data in-memory, dynamically generating code using vectorization optimizations and maximizing the potential of modern multi-core CPUs.  SnappyData enables complex processing on large data sets in sub-second timeframes.     ![SnappyData Positioning](docs/Images/Snappy_intro.1.png)    !!!Note  	*SnappyData is not another Enterprise Data Warehouse (EDW) platform, but rather a high performance computational and caching cluster that augments traditional EDWs and data lakes.*    ### Important Capabilities    *	**Easily discover and catalog big data sets**</br>  	You can connect and discover datasets in SQL DBs, Hadoop, NoSQL stores, file systems, or even cloud data stores such as S3 by using SQL, infer schemas automatically and register them in a secure catalog. A wide variety of data formats are supported out of the box such as JSON, CSV, text, Objects, Parquet, ORC, SQL, XML, and more.    *	**Rich connectivity**</br>  	SnappyData is built with Apache Spark inside. Therefore, any data store that has an Apache Spark connector can be accessed using SQL or by using the Apache Spark RDD/Dataset API. Virtually all modern data stores do have Apache Spark connector. See [Apache Spark Packages](https://spark-packages.org/). You can also dynamically deploy connectors to a running SnappyData cluster.    *	**Virtual or in-memory data**</br>  	You can decide which datasets need to be provisioned into distributed memory or left at the source. When the data is left at source, after being modeled as a virtual/external tables, the analytic query processing is parallelized, and the query fragments are pushed down wherever possible and executed at high speed.  When speed is essential, applications can selectively copy the external data into memory using a single SQL command.    *	**In-memory Columnar + Row store** </br>  	You can choose in-memory data to be stored in any of the following forms:      *	**Columnar**: The form that is compressed and designed for scanning/aggregating large data sets.      *	**Row store**: The form that has an extremely fast key access or highly selective access.  	The columnar store is automatically indexed using a skipping index. Applications can explicitly add indexes for the row store.    *	**High performance** </br>  	When data is loaded, the engine parallelizes all the accesses by carefully taking into account the available distributed cores, the available memory, and whether the source data can be partitioned to deliver extremely high-speed loading. Therefore, unlike a traditional warehouse, you can bring up SnappyData whenever required, load, process, and tear it down. Query processing uses code generation and vectorization techniques to shift the processing to the modern-day multi-core processor and L1/L2/L3 caches to the possible extent.    *	**Flexible rich data transformations** </br>  	External data sets when discovered automatically through schema inference will have the schema of the source. Users can cleanse, blend, reshape data using a SQL function library (Apache Spark SQL+) or even submit Apache Spark jobs and use custom logic. The entire rich Apache Spark API is at your disposal. This logic can be written in SQL, Java, Scala, or even Python.*    *	**Prepares data for data science**</br>   	Through the use of apache Apache Spark API for statistics and machine learning, raw or curated datasets can be easily prepared for machine learning. You can understand the statistical characteristics such as correlation, independence of different variables and so on. You can generate distributed feature vectors from your data that is by using processes such as one-hot encoder, binarizer, and a range of functions built into the Apache Spark ML library. These features can be stored back into column tables and shared across a group of users with security and avoid dumping copies to disk, which is slow and error-prone.     *	**Stream ingestion and liveness** </br>  	While it is common to see query service engines today, most resort to periodic refreshing of data sets from the source as the managed data cannot be mutated â€” for example query engines such as Presto, HDFS formats like parquet, etc. Moreover, when updates can be applied pre-processing, re-shaping of the data is not necessarily simple.      In SnappyData, operational systems can feed data updates through Kafka to SnappyData. The incoming data can be CDC(Change-data-capture) events (insert, updates, or deletes) and can be easily ingested into in-memory tables with ease, consistency, and exactly-once semantics. The Application can apply custom logic to do sophisticated transformations and get the data ready for analytics. This incremental and continuous process is far more efficient than batch refreshes. Refer [Stream Processing with SnappyData](docs/howto/use_stream_processing_with_snappydata.md) </br>      *	**Approximate Query Processing(AQP)** </br>  	When dealing with huge data sets, for example, IoT sensor streaming time-series data, it may not be possible to provision the data in-memory, and if left at the source (say Hadoop or S3) your analytic query processing can take too long. In SnappyData, you can create one or more stratified data samples on the full data set. The query engine automatically uses these samples for aggregation queries, and a nearly accurate answer returned to clients. This can be immensely valuable when visualizing a trend, plotting a graph or bar chart. Refer [AQP](docs/sde/index.md).    *	**Access from anywhere** </br>  	You can use JDBC, ODBC, REST, or any of the Apache Spark APIs. The product is fully compatible with Apache Spark 2.1.1. SnappyData natively supports modern visualization tools such as [TIBCO Spotfire](docs/howto/connecttibcospotfire.md), [Tableau](docs/howto/tableauconnect.md), and [Qlikview](docs/setting_up_jdbc_driver_qlikview.md). Refer       ## Downloading and Installing SnappyData  You can download and install the latest version of SnappyData from [github](https://github.com/TIBCOSoftware/snappydata/releases).  Refer to the [documentation](docs/install/index.md) for installation steps.    ## Getting Started  Multiple options are provided to get started with SnappyData. Easiest way to get going with SnappyData is on your laptop. You can also use any of the following options:    *	On-premise clusters    *	AWS    *	Docker  *	Kubernetes    You can find more information on options for running SnappyData [here](docs/quickstart/index.md).    ## Quick Test to Measure Performance of SnappyData vs Apache Spark    If you are already using Apache Spark, you can experience upto 20x speedup for your query performance with SnappyData. Try this [test](https://github.com/TIBCOSoftware/snappydata/blob/master/examples/quickstart/scripts/Quickstart.scala) using the Spark Shell.    ## Documentation  To understand SnappyData and its features refer to the [documentation](http://tibcosoftware.github.io/snappydata/).    ### Other Relevant content  - [Paper](http://cidrdb.org/cidr2017/papers/p28-mozafari-cidr17.pdf) on Snappydata at Conference on Innovative Data Systems Research (CIDR) - Info on key concepts and motivating problems.  - [Another early Paper](https://www.snappydata.io/snappy-industrial) that focuses on overall architecture, use cases, and benchmarks. ACM Sigmod 2016.  - [TPC-H benchmark](https://www.snappydata.io/whitepapers/snappydata-tpch) comparing Apache Spark with SnappyData  - Checkout the [SnappyData blog](https://www.snappydata.io/blog) for developer content  -	[TIBCO community page](https://community.tibco.com/products/tibco-computedb) for the latest info.    ## Community Support    We monitor the following channels comments/questions:    *	[Stackoverflow](http://stackoverflow.com/questions/tagged/snappydata) ![Stackoverflow](http://i.imgur.com/LPIdp12.png)    *	[Slack](http://snappydata-slackin.herokuapp.com/) ![Slack](http://i.imgur.com/h3sc6GM.png)    *	[Gitter](https://gitter.im/SnappyDataInc/snappydata) ![Gitter](http://i.imgur.com/jNAJeOn.jpg)    *	[Mailing List](https://groups.google.com/forum/#!forum/snappydata-user) ![Mailing List](http://i.imgur.com/YomdH4s.png)    *	[Reddit](https://www.reddit.com/r/snappydata) ![Reddit](http://i.imgur.com/AB3cVtj.png)              *	[JIRA](https://jira.snappydata.io/projects/SNAP/issues) ![JIRA](http://i.imgur.com/E92zntA.png)    ## Link with SnappyData Distribution    ### Using Maven Dependency    SnappyData artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates:    ```  groupId: io.snappydata  artifactId: snappydata-cluster_2.11  version: 1.3.0  ```    ### Using SBT Dependency    If you are using SBT, add this line to your **build.sbt** for core SnappyData artifacts:    ```  libraryDependencies += ""io.snappydata"" % ""snappydata-core_2.11"" % ""1.3.0""  ```    For additions related to SnappyData cluster, use:    ```  libraryDependencies += ""io.snappydata"" % ""snappydata-cluster_2.11"" % ""1.3.0""  ```    You can find more specific SnappyData artifacts [here](http://mvnrepository.com/artifact/io.snappydata)    !!!Note  	If your project fails when resolving the above dependency (that is, it fails to download `javax.ws.rs#javax.ws.rs-api;2.1`), it may be due an issue with its pom file. </br> As a workaround, you can add the below code to your **build.sbt**:    ```  val workaround = {    sys.props += ""packaging.type"" -> ""jar""    ()  }  ```    For more details, refer [https://github.com/sbt/sbt/issues/3618](https://github.com/sbt/sbt/issues/3618).      ## Building from Source  If you would like to build SnappyData from source, refer to the [documentation on building from source](docs/install/building_from_source.md).      ## How is SnappyData Different than Apache Spark?    Apache Spark is a general purpose parallel computational engine for analytics at scale. At its core, it has a batch design center and is capable of working with disparate data sources. While this provides rich unified access to data, this can also be quite inefficient and expensive. Analytic processing requires massive data sets to be repeatedly copied and data to be reformatted to suit Apache Spark. In many cases, it ultimately fails to deliver the promise of interactive analytic performance.  For instance, each time an aggregation is run on a large Cassandra table, it necessitates streaming the entire table into Apache Spark to do the aggregation. Caching within Apache Spark is immutable and results in stale insight.    ### The SnappyData Approach    ##### Snappy Architecture    ![SnappyData Architecture](docs/Images/SnappyArchitecture.png)    SnappyData takes a different approach. SnappyData fuses a low latency, highly available in-memory transactional database ((Pivotal GemFire/Apache Geode) into Apache Spark with shared memory management and optimizations. Data can be managed in columnar form similar to Apache Spark caching or in a row oriented manner commonly used in popular relational databases like postgres). But, many query engine operators are significantly more optimized through better vectorization, code generation and indexing. </br>  The net effect is, an order of magnitude performance improvement when compared to native Apache Spark caching, and more than two orders of magnitude better performance when Apache Spark is used in conjunction with external data sources.  Apache Spark is turned into an in-memory operational database capable of transactions, point reads, writes, working with Streams (Apache Spark) and running analytic SQL queries without losing the computational richness in Apache Spark.      ## Streaming Example - Ad Analytics  Here is a stream + Transactions + Analytics use case example to illustrate the SQL as well as the Apache Spark programming approaches in SnappyData - [Ad Analytics code example](https://github.com/TIBCOSoftware/snappy-examples). Here is a [screencast](https://www.youtube.com/watch?v=bXofwFtmHjE) that showcases many useful features of SnappyData. The example also goes through a benchmark comparing SnappyData to a Hybrid in-memory database and Cassandra.    ## Contributing to SnappyData    If you are interested in contributing, please visit the [community page](http://www.snappydata.io/community) for ways in which you can help.   """
Big data;https://github.com/apache/incubator-airflow;"""<!--   Licensed to the Apache Software Foundation (ASF) under one   or more contributor license agreements.  See the NOTICE file   distributed with this work for additional information   regarding copyright ownership.  The ASF licenses this file   to you under the Apache License, Version 2.0 (the   ""License""); you may not use this file except in compliance   with the License.  You may obtain a copy of the License at       http://www.apache.org/licenses/LICENSE-2.0     Unless required by applicable law or agreed to in writing,   software distributed under the License is distributed on an   ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY   KIND, either express or implied.  See the License for the   specific language governing permissions and limitations   under the License.  -->    # Apache Airflow    [![PyPI version](https://badge.fury.io/py/apache-airflow.svg)](https://badge.fury.io/py/apache-airflow)  [![GitHub Build](https://github.com/apache/airflow/workflows/CI%20Build/badge.svg)](https://github.com/apache/airflow/actions)  [![Coverage Status](https://img.shields.io/codecov/c/github/apache/airflow/main.svg)](https://codecov.io/github/apache/airflow?branch=main)  [![License](https://img.shields.io/:license-Apache%202-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0.txt)  [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/apache-airflow.svg)](https://pypi.org/project/apache-airflow/)  [![Docker Pulls](https://img.shields.io/docker/pulls/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow)  [![Docker Stars](https://img.shields.io/docker/stars/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow)  [![PyPI - Downloads](https://img.shields.io/pypi/dm/apache-airflow)](https://pypi.org/project/apache-airflow/)  [![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow)](https://artifacthub.io/packages/search?repo=apache-airflow)  [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)  [![Twitter Follow](https://img.shields.io/twitter/follow/ApacheAirflow.svg?style=social&label=Follow)](https://twitter.com/ApacheAirflow)  [![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://s.apache.org/airflow-slack)    [Apache Airflow](https://airflow.apache.org/docs/apache-airflow/stable/) (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.    When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.    Use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.    <!-- START doctoc generated TOC please keep comment here to allow auto update -->  <!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->  **Table of contents**    - [Project Focus](#project-focus)  - [Principles](#principles)  - [Requirements](#requirements)  - [Getting started](#getting-started)  - [Installing from PyPI](#installing-from-pypi)  - [Official source code](#official-source-code)  - [Convenience packages](#convenience-packages)  - [User Interface](#user-interface)  - [Semantic versioning](#semantic-versioning)  - [Version Life Cycle](#version-life-cycle)  - [Support for Python and Kubernetes versions](#support-for-python-and-kubernetes-versions)  - [Base OS support for reference Airflow images](#base-os-support-for-reference-airflow-images)  - [Approach to dependencies of Airflow](#approach-to-dependencies-of-airflow)  - [Support for providers](#support-for-providers)  - [Contributing](#contributing)  - [Who uses Apache Airflow?](#who-uses-apache-airflow)  - [Who Maintains Apache Airflow?](#who-maintains-apache-airflow)  - [Can I use the Apache Airflow logo in my presentation?](#can-i-use-the-apache-airflow-logo-in-my-presentation)  - [Airflow merchandise](#airflow-merchandise)  - [Links](#links)  - [Sponsors](#sponsors)    <!-- END doctoc generated TOC please keep comment here to allow auto update -->    ## Project Focus    Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include [Luigi](https://github.com/spotify/luigi), [Oozie](https://oozie.apache.org/) and [Azkaban](https://azkaban.github.io/).    Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's [Xcom feature](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#xcoms)). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.    Airflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.    ## Principles    - **Dynamic**: Airflow pipelines are configuration as code (Python), allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically.  - **Extensible**: Easily define your own operators, executors and extend the library so that it fits the level of abstraction that suits your environment.  - **Elegant**: Airflow pipelines are lean and explicit. Parameterizing your scripts is built into the core of Airflow using the powerful **Jinja** templating engine.  - **Scalable**: Airflow has a modular architecture and uses a message queue to orchestrate an arbitrary number of workers.    ## Requirements    Apache Airflow is tested with:    |                     | Main version (dev)      | Stable version (2.2.5)   |  |---------------------|-------------------------|--------------------------|  | Python              | 3.7, 3.8, 3.9, 3.10     | 3.6, 3.7, 3.8, 3.9       |  | Platform            | AMD64/ARM64(\*)         | AMD64                    |  | Kubernetes          | 1.20, 1.21, 1.22, 1.23  | 1.18, 1.19, 1.20         |  | PostgreSQL          | 10, 11, 12, 13          | 9.6, 10, 11, 12, 13      |  | MySQL               | 5.7, 8                  | 5.7, 8                   |  | SQLite              | 3.15.0+                 | 3.15.0+                  |  | MSSQL               | 2017(\*), 2019 (\*)     |                          |    \* Experimental    **Note**: MySQL 5.x versions are unable to or have limitations with  running multiple schedulers -- please see the [Scheduler docs](https://airflow.apache.org/docs/apache-airflow/stable/scheduler.html).  MariaDB is not tested/recommended.    **Note**: SQLite is used in Airflow tests. Do not use it in production. We recommend  using the latest stable version of SQLite for local development.    **Note**: Support for Python v3.10 will be available from Airflow 2.3.0. The `main` (development) branch  already supports Python 3.10.    **Note**: Airflow currently can be run on POSIX-compliant Operating Systems. For development it is regularly  tested on fairly modern Linux Distros and recent versions of MacOS.  On Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers.  The work to add Windows support is tracked via [#10388](https://github.com/apache/airflow/issues/10388) but  it is not a high priority. You should only use Linux-based distros as ""Production"" execution environment  as this is the only environment that is supported. The only distro that is used in our CI tests and that  is used in the [Community managed DockerHub image](https://hub.docker.com/p/apache/airflow) is  `Debian Bullseye`.    ## Getting started    Visit the official Airflow website documentation (latest **stable** release) for help with  [installing Airflow](https://airflow.apache.org/docs/apache-airflow/stable/installation.html),  [getting started](https://airflow.apache.org/docs/apache-airflow/stable/start/index.html), or walking  through a more complete [tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html).    > Note: If you're looking for documentation for the main branch (latest development branch): you can find it on [s.apache.org/airflow-docs](https://s.apache.org/airflow-docs/).    For more information on Airflow Improvement Proposals (AIPs), visit  the [Airflow Wiki](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals).    Documentation for dependent projects like provider packages, Docker image, Helm Chart, you'll find it in [the documentation index](https://airflow.apache.org/docs/).    ## Installing from PyPI    We publish Apache Airflow as `apache-airflow` package in PyPI. Installing it however might be sometimes tricky  because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and  applications usually pin them, but we should do neither and both simultaneously. We decided to keep  our dependencies as open as possible (in `setup.py`) so users can install different versions of libraries  if needed. This means that `pip install apache-airflow` will not work from time to time or will  produce unusable Airflow installation.    To have repeatable installation, however, we keep a set of ""known-to-be-working"" constraint  files in the orphan `constraints-main` and `constraints-2-0` branches. We keep those ""known-to-be-working""  constraints files separately per major/minor Python version.  You can use them as constraint files when installing Airflow from PyPI. Note that you have to specify  correct Airflow tag/version/branch and Python versions in the URL.      1. Installing just Airflow:    > Note: Only `pip` installation is currently officially supported.    While it is possible to install Airflow with tools like [Poetry](https://python-poetry.org) or  [pip-tools](https://pypi.org/project/pip-tools), they do not share the same workflow as  `pip` - especially when it comes to constraint vs. requirements management.  Installing via `Poetry` or `pip-tools` is not currently supported.    If you wish to install Airflow using those tools, you should use the constraint files and convert  them to the appropriate format and workflow that your tool requires.      ```bash  pip install 'apache-airflow==2.2.5' \   --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.2.5/constraints-3.7.txt""  ```    2. Installing with extras (i.e., postgres, google)    ```bash  pip install 'apache-airflow[postgres,google]==2.2.5' \   --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.2.5/constraints-3.7.txt""  ```    For information on installing provider packages, check  [providers](http://airflow.apache.org/docs/apache-airflow-providers/index.html).    ## Official source code    Apache Airflow is an [Apache Software Foundation](https://www.apache.org) (ASF) project,  and our official source code releases:    - Follow the [ASF Release Policy](https://www.apache.org/legal/release-policy.html)  - Can be downloaded from [the ASF Distribution Directory](https://downloads.apache.org/airflow)  - Are cryptographically signed by the release manager  - Are officially voted on by the PMC members during the    [Release Approval Process](https://www.apache.org/legal/release-policy.html#release-approval)    Following the ASF rules, the source packages released must be sufficient for a user to build and test the  release provided they have access to the appropriate platform and tools.    ## Convenience packages    There are other ways of installing and using Airflow. Those are ""convenience"" methods - they are  not ""official releases"" as stated by the `ASF Release Policy`, but they can be used by the users  who do not want to build the software themselves.    Those are - in the order of most common ways people install Airflow:    - [PyPI releases](https://pypi.org/project/apache-airflow/) to install Airflow using standard `pip` tool  - [Docker Images](https://hub.docker.com/r/apache/airflow) to install airflow via    `docker` tool, use them in Kubernetes, Helm Charts, `docker-compose`, `docker swarm`, etc. You can    read more about using, customising, and extending the images in the    [Latest docs](https://airflow.apache.org/docs/docker-stack/index.html), and    learn details on the internals in the [IMAGES.rst](https://github.com/apache/airflow/blob/main/IMAGES.rst) document.  - [Tags in GitHub](https://github.com/apache/airflow/tags) to retrieve the git project sources that    were used to generate official source packages via git    All those artifacts are not official releases, but they are prepared using officially released sources.  Some of those artifacts are ""development"" or ""pre-release"" ones, and they are clearly marked as such  following the ASF Policy.    ## User Interface    - **DAGs**: Overview of all DAGs in your environment.      ![DAGs](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/dags.png)    - **Grid**: Grid representation of a DAG that spans across time.      ![Grid](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/grid.png)    - **Graph**: Visualization of a DAG's dependencies and their current status for a specific run.      ![Graph](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/graph.png)    - **Task Duration**: Total time spent on different tasks over time.      ![Task Duration](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/duration.png)    - **Gantt**: Duration and overlap of a DAG.      ![Gantt](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/gantt.png)    - **Code**: Quick way to view source code of a DAG.      ![Code](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/code.png)    ## Semantic versioning    As of Airflow 2.0.0, we support a strict [SemVer](https://semver.org/) approach for all packages released.    There are few specific rules that we agreed to that define details of versioning of the different  packages:    * **Airflow**: SemVer rules apply to core airflow only (excludes any changes to providers).    Changing limits for versions of Airflow dependencies is not a breaking change on its own.  * **Airflow Providers**: SemVer rules apply to changes in the particular provider's code only.    SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version.    For example, `google 4.1.0` and `amazon 3.0.3` providers can happily be installed    with `Airflow 2.1.2`. If there are limits of cross-dependencies between providers and Airflow packages,    they are present in providers as `install_requires` limitations. We aim to keep backwards    compatibility of providers with all previously released Airflow 2 versions but    there will sometimes be breaking changes that might make some, or all    providers, have minimum Airflow version specified. Change of that minimum supported Airflow version    is a breaking change for provider because installing the new provider might automatically    upgrade Airflow (which might be an undesired side effect of upgrading provider).  * **Airflow Helm Chart**: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR    versions for the chart are independent from the Airflow version. We aim to keep backwards    compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might    only work starting from specific Airflow releases. We might however limit the Helm    Chart to depend on minimal Airflow version.  * **Airflow API clients**: SemVer MAJOR and MINOR versions follow MAJOR and MINOR versions of Airflow.    The first MAJOR or MINOR X.Y.0 release of Airflow should always be followed by X.Y.0 release of    all clients. The clients then can release their own PATCH releases with bugfixes,    independently of Airflow PATCH releases.    ## Version Life Cycle    Apache Airflow version life cycle:    <!-- This table is automatically updated by pre-commit scripts/ci/pre-commit/supported_versions.py -->  <!-- Beginning of auto-generated table -->    | Version   | Current Patch/Minor   | State     | First Release   | Limited Support   | EOL/Terminated   |  |-----------|-----------------------|-----------|-----------------|-------------------|------------------|  | 2         | 2.2.5                 | Supported | Dec 17, 2020    | TBD               | TBD              |  | 1.10      | 1.10.15               | EOL       | Aug 27, 2018    | Dec 17, 2020      | June 17, 2021    |  | 1.9       | 1.9.0                 | EOL       | Jan 03, 2018    | Aug 27, 2018      | Aug 27, 2018     |  | 1.8       | 1.8.2                 | EOL       | Mar 19, 2017    | Jan 03, 2018      | Jan 03, 2018     |  | 1.7       | 1.7.1.2               | EOL       | Mar 28, 2016    | Mar 19, 2017      | Mar 19, 2017     |    <!-- End of auto-generated table -->    Limited support versions will be supported with security and critical bug fix only.  EOL versions will not get any fixes nor support.  We always recommend that all users run the latest available minor release for whatever major version is in use.  We **highly** recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.    ## Support for Python and Kubernetes versions    As of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support.  They are based on the official release schedule of Python and Kubernetes, nicely summarized in the  [Python Developer's Guide](https://devguide.python.org/#status-of-python-branches) and  [Kubernetes version skew policy](https://kubernetes.io/docs/setup/release/version-skew-policy/).    1. We drop support for Python and Kubernetes versions when they reach EOL. Except for kubernetes, a     version stay supported by Airflow if two major cloud provider still provide support for it. We drop     support for those EOL versions in main right after EOL date, and it is effectively removed when we release     the first new MINOR (Or MAJOR if there is no new MINOR version) of Airflow For example, for Python 3.7 it     means that we will drop support in main right after 27.06.2023, and the first MAJOR or MINOR version of     Airflow released after will not have it.    2. The ""oldest"" supported version of Python/Kubernetes is the default one until we decide to switch to     later version. ""Default"" is only meaningful in terms of ""smoke tests"" in CI PRs, which are run using this     default version and the default reference image available. Currently `apache/airflow:latest`     and `apache/airflow:2.2.5` images are Python 3.7 images. This means that default reference image will     become the default at the time when we start preparing for dropping 3.7 support which is few months     before the end of life for Python 3.7.    3. We support a new version of Python/Kubernetes in main after they are officially released, as soon as we     make them work in our CI pipeline (which might not be immediate due to dependencies catching up with     new versions of Python mostly) we release new images/support in Airflow based on the working CI setup.    ## Base OS support for reference Airflow images    The Airflow Community provides conveniently packaged container images that are published whenever  we publish an Apache Airflow release. Those images contain:    * Base OS with necessary packages to install Airflow (stable Debian OS)  * Base Python installation in versions supported at the time of release for the MINOR version of    Airflow released (so there could be different versions for 2.3 and 2.2 line for example)  * Libraries required to connect to suppoerted Databases (again the set of databases supported depends    on the MINOR version of Airflow.  * Predefined set of popular providers (for details see the [Dockerfile](Dockerfile)).  * Possibility of building your own, custom image where the user can choose their own set of providers    and libraries (see [Building the image](https://airflow.apache.org/docs/docker-stack/build.html))  * In the future Airflow might also support a ""slim"" version without providers nor database clients installed    The version of the base OS image is the stable version of Debian. Airflow supports using all currently active  stable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline for  building and testing the OS version. Approximately 6 months before the end-of-life of a previous stable  version of the OS, Airflow switches the images released to use the latest supported version of the OS.  For example since Debian Buster end-of-life is August 2022, Airflow switches the images in `main` branch  to use Debian Bullseye in February/March 2022. The version will be used in the next MINOR release after  the switch happens. In case of the Bullseye switch - 2.3.0 version will use Bullseye. The images released  in the previous MINOR version continue to use the version that all other releases for the MINOR version  used.    Users will continue to be able to build their images using stable Debian releases until the end of life and  building and verifying of the images happens in our CI but no unit tests are executed using this image in  the `main` branch.    ## Approach to dependencies of Airflow    Airflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application,  therefore our policies to dependencies has to include both - stability of installation of application,  but also ability to install newer version of dependencies for those users who develop DAGs. We developed  the approach where `constraints` are used to make sure airflow can be installed in a repeatable way, while  we do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-bound  version of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them is  needed because of importance of the dependency as well as risk it involves to upgrade specific dependency.  We also upper-bound the dependencies that we know cause problems.    The constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependencies  automatically (providing that all the tests pass). Our `main` build failures will indicate in case there  are versions of dependencies that break our tests - indicating that we should either upper-bind them or  that we should fix our code/tests to account for the upstream changes from those dependencies.    Whenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should have  a good reason why dependency is upper-bound. And we should also mention what is the condition to remove the  binding.    ### Approach for dependencies for Airflow Core    Those `extras` and `providers` dependencies are maintained in `setup.cfg`.    There are few dependencies that we decided are important enough to upper-bound them by default, as they are  known to follow predictable versioning scheme, and we know that new versions of those are very likely to  bring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions of  the dependencies as they are released, but this is manual process.    The important dependencies are:    * `SQLAlchemy`: upper-bound to specific MINOR version (SQLAlchemy is known to remove deprecations and     introduce breaking changes especially that support for different Databases varies and changes at     various speed (example: SQLAlchemy 1.4 broke MSSQL integration for Airflow)  * `Alembic`: it is important to handle our migrations in predictable and performant way. It is developed     together with SQLAlchemy. Our experience with Alembic is that it very stable in MINOR version  * `Flask`: We are using Flask as the back-bone of our web UI and API. We know major version of Flask     are very likely to introduce breaking changes across those so limiting it to MAJOR version makes sense  * `werkzeug`: the library is known to cause problems in new versions. It is tightly coupled with Flask     libraries, and we should update them together  * `celery`: Celery is crucial component of Airflow as it used for CeleryExecutor (and similar). Celery     [follows SemVer](https://docs.celeryq.dev/en/stable/contributing.html?highlight=semver#versions), so     we should upper-bound it to the next MAJOR version. Also when we bump the upper version of the library,     we should make sure Celery Provider minimum Airflow version is updated).  * `kubernetes`: Kubernetes is a crucial component of Airflow as it is used for the KubernetesExecutor     (and similar). Kubernetes Python library [follows SemVer](https://github.com/kubernetes-client/python#compatibility),     so we should upper-bound it to the next MAJOR version. Also when we bump the upper version of the library,     we should make sure Kubernetes Provider minimum Airflow version is updated.    ### Approach for dependencies in Airflow Providers and extras    Those `extras` and `providers` dependencies are maintained in `setup.py`.    By default, we should not upper-bound dependencies for providers, however each provider's maintainer  might decide to add additional limits (and justify them with comment)    ## Support for providers    Providers released by the community have limitation of a minimum supported version of Airflow. The minimum  version of Airflow is the `MINOR` version (2.1, 2.2 etc.) indicating that the providers might use features  that appeared in this release. The default support timespan for the minimum version of Airflow  (there could be justified exceptions) is that we increase the minimum Airflow version, when 12 months passed  since the first release for the MINOR version of Airflow.    For example this means that by default we upgrade the minimum version of Airflow supported by providers  to 2.2.0 in the first Provider's release after 21st of May 2022 (21st of May 2021 is the date when the  first `PATCHLEVEL` of 2.1 (2.1.0) has been released.    ## Contributing    Want to help build Apache Airflow? Check out our [contributing documentation](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst).    Official Docker (container) images for Apache Airflow are described in [IMAGES.rst](https://github.com/apache/airflow/blob/main/IMAGES.rst).    ## Who uses Apache Airflow?    More than 400 organizations are using Apache Airflow  [in the wild](https://github.com/apache/airflow/blob/main/INTHEWILD.md).    ## Who Maintains Apache Airflow?    Airflow is the work of the [community](https://github.com/apache/airflow/graphs/contributors),  but the [core committers/maintainers](https://people.apache.org/committers-by-project.html#airflow)  are responsible for reviewing and merging PRs as well as steering conversations around new feature requests.  If you would like to become a maintainer, please review the Apache Airflow  [committer requirements](https://github.com/apache/airflow/blob/main/COMMITTERS.rst#guidelines-to-become-an-airflow-committer).    ## Can I use the Apache Airflow logo in my presentation?    Yes! Be sure to abide by the Apache Foundation [trademark policies](https://www.apache.org/foundation/marks/#books) and the Apache Airflow [Brandbook](https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook). The most up to date logos are found in [this repo](/docs/apache-airflow/img/logos) and on the Apache Software Foundation [website](https://www.apache.org/logos/about.html).    ## Airflow merchandise    If you would love to have Apache Airflow stickers, t-shirt, etc. then check out  [Redbubble Shop](https://www.redbubble.com/i/sticker/Apache-Airflow-by-comdev/40497530.EJUG5).    ## Links    - [Documentation](https://airflow.apache.org/docs/apache-airflow/stable/)  - [Chat](https://s.apache.org/airflow-slack)    ## Sponsors    The CI infrastructure for Apache Airflow has been sponsored by:    <!-- Ordered by most recently ""funded"" -->    <a href=""https://astronomer.io""><img src=""https://assets2.astronomer.io/logos/logoForLIGHTbackground.png"" alt=""astronomer.io"" width=""250px""></a>  <a href=""https://aws.amazon.com/opensource/""><img src=""docs/integration-logos/aws/AWS-Cloud-alt_light-bg@4x.png"" alt=""AWS OpenSource"" width=""130px""></a> """
Big data;https://github.com/VictoriaMetrics/VictoriaMetrics;"""# VictoriaMetrics    [![Latest Release](https://img.shields.io/github/release/VictoriaMetrics/VictoriaMetrics.svg?style=flat-square)](https://github.com/VictoriaMetrics/VictoriaMetrics/releases/latest)  [![Docker Pulls](https://img.shields.io/docker/pulls/victoriametrics/victoria-metrics.svg?maxAge=604800)](https://hub.docker.com/r/victoriametrics/victoria-metrics)  [![Slack](https://img.shields.io/badge/join%20slack-%23victoriametrics-brightgreen.svg)](https://slack.victoriametrics.com/)  [![GitHub license](https://img.shields.io/github/license/VictoriaMetrics/VictoriaMetrics.svg)](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/LICENSE)  [![Go Report](https://goreportcard.com/badge/github.com/VictoriaMetrics/VictoriaMetrics)](https://goreportcard.com/report/github.com/VictoriaMetrics/VictoriaMetrics)  [![Build Status](https://github.com/VictoriaMetrics/VictoriaMetrics/workflows/main/badge.svg)](https://github.com/VictoriaMetrics/VictoriaMetrics/actions)  [![codecov](https://codecov.io/gh/VictoriaMetrics/VictoriaMetrics/branch/master/graph/badge.svg)](https://codecov.io/gh/VictoriaMetrics/VictoriaMetrics)    <img src=""logo.png"" width=""300"" alt=""VictoriaMetrics logo"">    VictoriaMetrics is a fast, cost-effective and scalable monitoring solution and time series database.    VictoriaMetrics is available in [binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases),  [Docker images](https://hub.docker.com/r/victoriametrics/victoria-metrics/), [Snap packages](https://snapcraft.io/victoriametrics)  and [source code](https://github.com/VictoriaMetrics/VictoriaMetrics). Just download VictoriaMetrics and follow [these instructions](#how-to-start-victoriametrics).  Then read [Prometheus setup](#prometheus-setup) and [Grafana setup](#grafana-setup) docs.    Cluster version of VictoriaMetrics is available [here](https://docs.victoriametrics.com/Cluster-VictoriaMetrics.html).    [Contact us](mailto:info@victoriametrics.com) if you need enterprise support for VictoriaMetrics. See [features available in enterprise package](https://victoriametrics.com/products/enterprise/). Enterprise binaries can be downloaded and evaluated for free from [the releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases).    ## Prominent features    VictoriaMetrics has the following prominent features:    * It can be used as long-term storage for Prometheus. See [these docs](#prometheus-setup) for details.  * It can be used as drop-in replacement for Prometheus in Grafana, because it supports [Prometheus querying API](#prometheus-querying-api-usage).  * It can be used as drop-in replacement for Graphite in Grafana, because it supports [Graphite API](#graphite-api-usage).  * It features easy setup and operation:    * VictoriaMetrics consists of a single [small executable](https://medium.com/@valyala/stripping-dependency-bloat-in-victoriametrics-docker-image-983fb5912b0d) without external dependencies.    * All the configuration is done via explicit command-line flags with reasonable defaults.    * All the data is stored in a single directory pointed by `-storageDataPath` command-line flag.    * Easy and fast backups from [instant snapshots](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282) to S3 or GCS can be done with [vmbackup](https://docs.victoriametrics.com/vmbackup.html) / [vmrestore](https://docs.victoriametrics.com/vmrestore.html) tools. See [this article](https://medium.com/@valyala/speeding-up-backups-for-big-time-series-databases-533c1a927883) for more details.  * It implements PromQL-based query language - [MetricsQL](https://docs.victoriametrics.com/MetricsQL.html), which provides improved functionality on top of PromQL.  * It provides global query view. Multiple Prometheus instances or any other data sources may ingest data into VictoriaMetrics. Later this data may be queried via a single query.  * It provides high performance and good vertical and horizontal scalability for both [data ingestion](https://medium.com/@valyala/high-cardinality-tsdb-benchmarks-victoriametrics-vs-timescaledb-vs-influxdb-13e6ee64dd6b) and [data querying](https://medium.com/@valyala/when-size-matters-benchmarking-victoriametrics-vs-timescale-and-influxdb-6035811952d4). It [outperforms InfluxDB and TimescaleDB by up to 20x](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae).  * It [uses 10x less RAM than InfluxDB](https://medium.com/@valyala/insert-benchmarks-with-inch-influxdb-vs-victoriametrics-e31a41ae2893) and [up to 7x less RAM than Prometheus, Thanos or Cortex](https://valyala.medium.com/prometheus-vs-victoriametrics-benchmark-on-node-exporter-metrics-4ca29c75590f) when dealing with millions of unique time series (aka [high cardinality](https://docs.victoriametrics.com/FAQ.html#what-is-high-cardinality)).  * It is optimized for time series with [high churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate).  * It provides high data compression, so [up to 70x more data points](https://medium.com/@valyala/when-size-matters-benchmarking-victoriametrics-vs-timescale-and-influxdb-6035811952d4) may be crammed into limited storage comparing to TimescaleDB and [up to 7x less storage space is required compared to Prometheus, Thanos or Cortex](https://valyala.medium.com/prometheus-vs-victoriametrics-benchmark-on-node-exporter-metrics-4ca29c75590f).  * It is optimized for storage with high-latency IO and low IOPS (HDD and network storage in AWS, Google Cloud, Microsoft Azure, etc). See [disk IO graphs from these benchmarks](https://medium.com/@valyala/high-cardinality-tsdb-benchmarks-victoriametrics-vs-timescaledb-vs-influxdb-13e6ee64dd6b).  * A single-node VictoriaMetrics may substitute moderately sized clusters built with competing solutions such as Thanos, M3DB, Cortex, InfluxDB or TimescaleDB. See [vertical scalability benchmarks](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae), [comparing Thanos to VictoriaMetrics cluster](https://medium.com/@valyala/comparing-thanos-to-victoriametrics-cluster-b193bea1683) and [Remote Write Storage Wars](https://promcon.io/2019-munich/talks/remote-write-storage-wars/) talk from [PromCon 2019](https://promcon.io/2019-munich/talks/remote-write-storage-wars/).  * It protects the storage from data corruption on unclean shutdown (i.e. OOM, hardware reset or `kill -9`) thanks to [the storage architecture](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).  * It supports metrics' scraping, ingestion and [backfilling](#backfilling) via the following protocols:    * [Metrics scraping from Prometheus exporters](#how-to-scrape-prometheus-exporters-such-as-node-exporter).    * [Prometheus remote write API](#prometheus-setup).    * [Prometheus exposition format](#how-to-import-data-in-prometheus-exposition-format).    * [InfluxDB line protocol](#how-to-send-data-from-influxdb-compatible-agents-such-as-telegraf) over HTTP, TCP and UDP.    * [Graphite plaintext protocol](#how-to-send-data-from-graphite-compatible-agents-such-as-statsd) with [tags](https://graphite.readthedocs.io/en/latest/tags.html#carbon).    * [OpenTSDB put message](#sending-data-via-telnet-put-protocol).    * [HTTP OpenTSDB /api/put requests](#sending-opentsdb-data-via-http-apiput-requests).    * [JSON line format](#how-to-import-data-in-json-line-format).    * [Arbitrary CSV data](#how-to-import-csv-data).    * [Native binary format](#how-to-import-data-in-native-format).  * It supports metrics' relabeling. See [these docs](#relabeling) for details.  * It can deal with [high cardinality issues](https://docs.victoriametrics.com/FAQ.html#what-is-high-cardinality) and [high churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate) issues via [series limiter](#cardinality-limiter).  * It ideally works with big amounts of time series data from APM, Kubernetes, IoT sensors, connected cars, industrial telemetry, financial data and various [Enterprise workloads](https://victoriametrics.com/products/enterprise/).  * It has open source [cluster version](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster).    See also [various Articles about VictoriaMetrics](https://docs.victoriametrics.com/Articles.html).    ## Case studies and talks    Case studies:    * [AbiosGaming](https://docs.victoriametrics.com/CaseStudies.html#abiosgaming)  * [adidas](https://docs.victoriametrics.com/CaseStudies.html#adidas)  * [Adsterra](https://docs.victoriametrics.com/CaseStudies.html#adsterra)  * [ARNES](https://docs.victoriametrics.com/CaseStudies.html#arnes)  * [Brandwatch](https://docs.victoriametrics.com/CaseStudies.html#brandwatch)  * [CERN](https://docs.victoriametrics.com/CaseStudies.html#cern)  * [COLOPL](https://docs.victoriametrics.com/CaseStudies.html#colopl)  * [Dreamteam](https://docs.victoriametrics.com/CaseStudies.html#dreamteam)  * [Fly.io](https://docs.victoriametrics.com/CaseStudies.html#flyio)  * [German Research Center for Artificial Intelligence](https://docs.victoriametrics.com/CaseStudies.html#german-research-center-for-artificial-intelligence)  * [Grammarly](https://docs.victoriametrics.com/CaseStudies.html#grammarly)  * [Groove X](https://docs.victoriametrics.com/CaseStudies.html#groove-x)  * [Idealo.de](https://docs.victoriametrics.com/CaseStudies.html#idealode)  * [MHI Vestas Offshore Wind](https://docs.victoriametrics.com/CaseStudies.html#mhi-vestas-offshore-wind)  * [Razorpay](https://docs.victoriametrics.com/CaseStudies.html#razorpay)  * [Percona](https://docs.victoriametrics.com/CaseStudies.html#percona)  * [Sensedia](https://docs.victoriametrics.com/CaseStudies.html#sensedia)  * [Smarkets](https://docs.victoriametrics.com/CaseStudies.html#smarkets)  * [Synthesio](https://docs.victoriametrics.com/CaseStudies.html#synthesio)  * [Wedos.com](https://docs.victoriametrics.com/CaseStudies.html#wedoscom)  * [Wix.com](https://docs.victoriametrics.com/CaseStudies.html#wixcom)  * [Zerodha](https://docs.victoriametrics.com/CaseStudies.html#zerodha)  * [zhihu](https://docs.victoriametrics.com/CaseStudies.html#zhihu)    See also [articles and slides about VictoriaMetrics from our users](https://docs.victoriametrics.com/Articles.html#third-party-articles-and-slides-about-victoriametrics)    ## Operation    ## How to start VictoriaMetrics    Just download [VictoriaMetrics executable](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) or [Docker image](https://hub.docker.com/r/victoriametrics/victoria-metrics/) and start it with the desired command-line flags.    The following command-line flags are used the most:    * `-storageDataPath` - VictoriaMetrics stores all the data in this directory. Default path is `victoria-metrics-data` in the current working directory.  * `-retentionPeriod` - retention for stored data. Older data is automatically deleted. Default retention is 1 month. See [the Retention section](#retention) for more details.    Other flags have good enough default values, so set them only if you really need this. Pass `-help` to see [all the available flags with description and default values](#list-of-command-line-flags).    See how to [ingest data to VictoriaMetrics](#how-to-import-time-series-data), how to [query VictoriaMetrics via Grafana](#grafana-setup), how to [query VictoriaMetrics via Graphite API](#graphite-api-usage) and how to [handle alerts](#alerting).    VictoriaMetrics accepts [Prometheus querying API requests](#prometheus-querying-api-usage) on port `8428` by default.    It is recommended setting up [monitoring](#monitoring) for VictoriaMetrics.    ### Environment variables    Each flag value can be set via environment variables according to these rules:    * The `-envflag.enable` flag must be set.  * Each `.` char in flag name must be substituted with `_` (for example `-insert.maxQueueDuration <duration>` will translate to `insert_maxQueueDuration=<duration>`).  * For repeating flags an alternative syntax can be used by joining the different values into one using `,` char as separator (for example `-storageNode <nodeA> -storageNode <nodeB>` will translate to `storageNode=<nodeA>,<nodeB>`).  * Environment var prefix can be set via `-envflag.prefix` flag. For instance, if `-envflag.prefix=VM_`, then env vars must be prepended with `VM_`.    ### Configuration with snap package    Snap package for VictoriaMetrics is available [here](https://snapcraft.io/victoriametrics).    Command-line flags for Snap package can be set with following command:    ```text  echo 'FLAGS=""-selfScrapeInterval=10s -search.logSlowQueryDuration=20s""' > $SNAP_DATA/var/snap/victoriametrics/current/extra_flags  snap restart victoriametrics  ```    Do not change value for `-storageDataPath` flag, because snap package has limited access to host filesystem.    Changing scrape configuration is possible with text editor:    ```text  vi $SNAP_DATA/var/snap/victoriametrics/current/etc/victoriametrics-scrape-config.yaml  ```    After changes were made, trigger config re-read with the command `curl 127.0.0.1:8248/-/reload`.    ## Prometheus setup    Add the following lines to Prometheus config file (it is usually located at `/etc/prometheus/prometheus.yml`) in order to send data to VictoriaMetrics:    ```yml  remote_write:    - url: http://<victoriametrics-addr>:8428/api/v1/write  ```    Substitute `<victoriametrics-addr>` with hostname or IP address of VictoriaMetrics.  Then apply new config via the following command:    ```bash  kill -HUP `pidof prometheus`  ```    Prometheus writes incoming data to local storage and replicates it to remote storage in parallel.  This means that data remains available in local storage for `--storage.tsdb.retention.time` duration  even if remote storage is unavailable.    If you plan sending data to VictoriaMetrics from multiple Prometheus instances, then add the following lines into `global` section  of [Prometheus config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file):    ```yml  global:    external_labels:      datacenter: dc-123  ```    This instructs Prometheus to add `datacenter=dc-123` label to each sample before sending it to remote storage.  The label name can be arbitrary - `datacenter` is just an example. The label value must be unique  across Prometheus instances, so time series could be filtered and grouped by this label.    For highly loaded Prometheus instances (200k+ samples per second) the following tuning may be applied:    ```yaml  remote_write:    - url: http://<victoriametrics-addr>:8428/api/v1/write      queue_config:        max_samples_per_send: 10000        capacity: 20000        max_shards: 30  ```    Using remote write increases memory usage for Prometheus by up to ~25%. If you are experiencing issues with  too high memory consumption of Prometheus, then try to lower `max_samples_per_send` and `capacity` params. Keep in mind that these two params are tightly connected.  Read more about tuning remote write for Prometheus [here](https://prometheus.io/docs/practices/remote_write).    It is recommended upgrading Prometheus to [v2.12.0](https://github.com/prometheus/prometheus/releases) or newer, since previous versions may have issues with `remote_write`.    Take a look also at [vmagent](https://docs.victoriametrics.com/vmagent.html) and [vmalert](https://docs.victoriametrics.com/vmalert.html),  which can be used as faster and less resource-hungry alternative to Prometheus.    ## Grafana setup    Create [Prometheus datasource](http://docs.grafana.org/features/datasources/prometheus/) in Grafana with the following url:    ```url  http://<victoriametrics-addr>:8428  ```    Substitute `<victoriametrics-addr>` with the hostname or IP address of VictoriaMetrics.    Then build graphs and dashboards for the created datasource using [PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/) or [MetricsQL](https://docs.victoriametrics.com/MetricsQL.html).    ## How to upgrade VictoriaMetrics    It is safe upgrading VictoriaMetrics to new versions unless [release notes](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) say otherwise. It is safe skipping multiple versions during the upgrade unless [release notes](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) say otherwise. It is recommended performing regular upgrades to the latest version, since it may contain important bug fixes, performance optimizations or new features.    It is also safe downgrading to older versions unless [release notes](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) say otherwise.    The following steps must be performed during the upgrade / downgrade procedure:    * Send `SIGINT` signal to VictoriaMetrics process in order to gracefully stop it.  * Wait until the process stops. This can take a few seconds.  * Start the upgraded VictoriaMetrics.    Prometheus doesn't drop data during VictoriaMetrics restart. See [this article](https://grafana.com/blog/2019/03/25/whats-new-in-prometheus-2.8-wal-based-remote-write/) for details. The same applies also to [vmagent](https://docs.victoriametrics.com/vmagent.html).    ## How to apply new config to VictoriaMetrics    VictoriaMetrics is configured via command-line flags, so it must be restarted when new command-line flags should be applied:    * Send `SIGINT` signal to VictoriaMetrics process in order to gracefully stop it.  * Wait until the process stops. This can take a few seconds.  * Start VictoriaMetrics with the new command-line flags.    Prometheus doesn't drop data during VictoriaMetrics restart. See [this article](https://grafana.com/blog/2019/03/25/whats-new-in-prometheus-2.8-wal-based-remote-write/) for details. The same applies alos to [vmagent](https://docs.victoriametrics.com/vmagent.html).    ## How to scrape Prometheus exporters such as [node-exporter](https://github.com/prometheus/node_exporter)    VictoriaMetrics can be used as drop-in replacement for Prometheus for scraping targets configured in `prometheus.yml` config file according to [the specification](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file). Just set `-promscrape.config` command-line flag to the path to `prometheus.yml` config - and VictoriaMetrics should start scraping the configured targets. Currently the following [scrape_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config) types are supported:    * [static_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#static_config)  * [file_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#file_sd_config)  * [kubernetes_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config)  * [ec2_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config)  * [gce_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#gce_sd_config)  * [consul_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config)  * [dns_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dns_sd_config)  * [openstack_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#openstack_sd_config)  * [docker_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#docker_sd_config)  * [dockerswarm_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dockerswarm_sd_config)  * [eureka_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#eureka_sd_config)  * [digitalocean_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#digitalocean_sd_config)  * [http_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#http_sd_config)    File a [feature request](https://github.com/VictoriaMetrics/VictoriaMetrics/issues) if you need support for other `*_sd_config` types.    The file pointed by `-promscrape.config` may contain `%{ENV_VAR}` placeholders, which are substituted by the corresponding `ENV_VAR` environment variable values.    VictoriaMetrics also supports [importing data in Prometheus exposition format](#how-to-import-data-in-prometheus-exposition-format).    See also [vmagent](https://docs.victoriametrics.com/vmagent.html), which can be used as drop-in replacement for Prometheus.    ## How to send data from DataDog agent    VictoriaMetrics accepts data from [DataDog agent](https://docs.datadoghq.com/agent/) or [DogStatsD]() via [""submit metrics"" API](https://docs.datadoghq.com/api/latest/metrics/#submit-metrics) at `/datadog/api/v1/series` path.    Run DataDog agent with `DD_DD_URL=http://victoriametrics-host:8428/datadog` environment variable in order to write data to VictoriaMetrics at `victoriametrics-host` host. Another option is to set `dd_url` param at [DataDog agent configuration file](https://docs.datadoghq.com/agent/guide/agent-configuration-files/) to `http://victoriametrics-host:8428/datadog`.    VictoriaMetrics doesn't check `DD_API_KEY` param, so it can be set to arbitrary value.    Example on how to send data to VictoriaMetrics via DataDog ""submit metrics"" API from command line:    ```bash  echo '  {    ""series"": [      {        ""host"": ""test.example.com"",        ""interval"": 20,        ""metric"": ""system.load.1"",        ""points"": [[          0,          0.5        ]],        ""tags"": [          ""environment:test""        ],        ""type"": ""rate""      }    ]  }  ' | curl -X POST --data-binary @- http://localhost:8428/datadog/api/v1/series  ```    The imported data can be read via [export API](https://docs.victoriametrics.com/#how-to-export-data-in-json-line-format):    ```bash  curl http://localhost:8428/api/v1/export -d 'match[]=system.load.1'  ```    This command should return the following output if everything is OK:    ```  {""metric"":{""__name__"":""system.load.1"",""environment"":""test"",""host"":""test.example.com""},""values"":[0.5],""timestamps"":[1632833641000]}  ```    Extra labels may be added to all the written time series by passing `extra_label=name=value` query args.  For example, `/datadog/api/v1/series?extra_label=foo=bar` would add `{foo=""bar""}` label to all the ingested metrics.    ## How to send data from InfluxDB-compatible agents such as [Telegraf](https://www.influxdata.com/time-series-platform/telegraf/)    Use `http://<victoriametric-addr>:8428` url instead of InfluxDB url in agents' configs.  For instance, put the following lines into `Telegraf` config, so it sends data to VictoriaMetrics instead of InfluxDB:    ```toml  [[outputs.influxdb]]    urls = [""http://<victoriametrics-addr>:8428""]  ```    Another option is to enable TCP and UDP receiver for InfluxDB line protocol via `-influxListenAddr` command-line flag  and stream plain InfluxDB line protocol data to the configured TCP and/or UDP addresses.    VictoriaMetrics performs the following transformations to the ingested InfluxDB data:    * [`db` query arg](https://docs.influxdata.com/influxdb/v1.7/tools/api/#write-http-endpoint) is mapped into `db` label value    unless `db` tag exists in the InfluxDB line. The `db` label name can be overriden via `-influxDBLabel` command-line flag.  * Field names are mapped to time series names prefixed with `{measurement}{separator}` value, where `{separator}` equals to `_` by default. It can be changed with `-influxMeasurementFieldSeparator` command-line flag. See also `-influxSkipSingleField` command-line flag. If `{measurement}` is empty or if `-influxSkipMeasurement` command-line flag is set, then time series names correspond to field names.  * Field values are mapped to time series values.  * Tags are mapped to Prometheus labels as-is.    For example, the following InfluxDB line:    ```raw  foo,tag1=value1,tag2=value2 field1=12,field2=40  ```    is converted into the following Prometheus data points:    ```raw  foo_field1{tag1=""value1"", tag2=""value2""} 12  foo_field2{tag1=""value1"", tag2=""value2""} 40  ```    Example for writing data with [InfluxDB line protocol](https://docs.influxdata.com/influxdb/v1.7/write_protocols/line_protocol_tutorial/)  to local VictoriaMetrics using `curl`:    ```bash  curl -d 'measurement,tag1=value1,tag2=value2 field1=123,field2=1.23' -X POST 'http://localhost:8428/write'  ```    An arbitrary number of lines delimited by '\n' (aka newline char) can be sent in a single request.  After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match={__name__=~""measurement_.*""}'  ```    The `/api/v1/export` endpoint should return the following response:    ```jsonl  {""metric"":{""__name__"":""measurement_field1"",""tag1"":""value1"",""tag2"":""value2""},""values"":[123],""timestamps"":[1560272508147]}  {""metric"":{""__name__"":""measurement_field2"",""tag1"":""value1"",""tag2"":""value2""},""values"":[1.23],""timestamps"":[1560272508147]}  ```    Note that InfluxDB line protocol expects [timestamps in *nanoseconds* by default](https://docs.influxdata.com/influxdb/v1.7/write_protocols/line_protocol_tutorial/#timestamp),  while VictoriaMetrics stores them with *milliseconds* precision.    Extra labels may be added to all the written time series by passing `extra_label=name=value` query args.  For example, `/write?extra_label=foo=bar` would add `{foo=""bar""}` label to all the ingested metrics.    Some plugins for Telegraf such as [fluentd](https://github.com/fangli/fluent-plugin-influxdb), [Juniper/open-nti](https://github.com/Juniper/open-nti)  or [Juniper/jitmon](https://github.com/Juniper/jtimon) send `SHOW DATABASES` query to `/query` and expect a particular database name in the response.  Comma-separated list of expected databases can be passed to VictoriaMetrics via `-influx.databaseNames` command-line flag.    ## How to send data from Graphite-compatible agents such as [StatsD](https://github.com/etsy/statsd)    Enable Graphite receiver in VictoriaMetrics by setting `-graphiteListenAddr` command line flag. For instance,  the following command will enable Graphite receiver in VictoriaMetrics on TCP and UDP port `2003`:    ```bash  /path/to/victoria-metrics-prod -graphiteListenAddr=:2003  ```    Use the configured address in Graphite-compatible agents. For instance, set `graphiteHost`  to the VictoriaMetrics host in `StatsD` configs.    Example for writing data with Graphite plaintext protocol to local VictoriaMetrics using `nc`:    ```bash  echo ""foo.bar.baz;tag1=value1;tag2=value2 123 `date +%s`"" | nc -N localhost 2003  ```    VictoriaMetrics sets the current time if the timestamp is omitted.  An arbitrary number of lines delimited by `\n` (aka newline char) can be sent in one go.  After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match=foo.bar.baz'  ```    The `/api/v1/export` endpoint should return the following response:    ```bash  {""metric"":{""__name__"":""foo.bar.baz"",""tag1"":""value1"",""tag2"":""value2""},""values"":[123],""timestamps"":[1560277406000]}  ```    ## Querying Graphite data    Data sent to VictoriaMetrics via `Graphite plaintext protocol` may be read via the following APIs:    * [Graphite API](#graphite-api-usage)  * [Prometheus querying API](#prometheus-querying-api-usage). See also [selecting Graphite metrics](#selecting-graphite-metrics).  * [go-graphite/carbonapi](https://github.com/go-graphite/carbonapi/blob/main/cmd/carbonapi/carbonapi.example.victoriametrics.yaml)    ## Selecting Graphite metrics    VictoriaMetrics supports `__graphite__` pseudo-label for selecting time series with Graphite-compatible filters in [MetricsQL](https://docs.victoriametrics.com/MetricsQL.html). For example, `{__graphite__=""foo.*.bar""}` is equivalent to `{__name__=~""foo[.][^.]*[.]bar""}`, but it works faster and it is easier to use when migrating from Graphite to VictoriaMetrics. See [docs for Graphite paths and wildcards](https://graphite.readthedocs.io/en/latest/render_api.html#paths-and-wildcards). VictoriaMetrics also supports [label_graphite_group](https://docs.victoriametrics.com/MetricsQL.html#label_graphite_group) function for extracting the given groups from Graphite metric name.    The `__graphite__` pseudo-label supports e.g. alternate regexp filters such as `(value1|...|valueN)`. They are transparently converted to `{value1,...,valueN}` syntax [used in Graphite](https://graphite.readthedocs.io/en/latest/render_api.html#paths-and-wildcards). This allows using [multi-value template variables in Grafana](https://grafana.com/docs/grafana/latest/variables/formatting-multi-value-variables/) inside `__graphite__` pseudo-label. For example, Grafana expands `{__graphite__=~""foo.($bar).baz""}` into `{__graphite__=~""foo.(x|y).baz""}` if `$bar` template variable contains `x` and `y` values. In this case the query is automatically converted into `{__graphite__=~""foo.{x,y}.baz""}` before execution.    ## How to send data from OpenTSDB-compatible agents    VictoriaMetrics supports [telnet put protocol](http://opentsdb.net/docs/build/html/api_telnet/put.html)  and [HTTP /api/put requests](http://opentsdb.net/docs/build/html/api_http/put.html) for ingesting OpenTSDB data.  The same protocol is used for [ingesting data in KairosDB](https://kairosdb.github.io/docs/PushingData.html).    ### Sending data via `telnet put` protocol    Enable OpenTSDB receiver in VictoriaMetrics by setting `-opentsdbListenAddr` command line flag. For instance,  the following command enables OpenTSDB receiver in VictoriaMetrics on TCP and UDP port `4242`:    ```bash  /path/to/victoria-metrics-prod -opentsdbListenAddr=:4242  ```    Send data to the given address from OpenTSDB-compatible agents.    Example for writing data with OpenTSDB protocol to local VictoriaMetrics using `nc`:    ```bash  echo ""put foo.bar.baz `date +%s` 123 tag1=value1 tag2=value2"" | nc -N localhost 4242  ```    An arbitrary number of lines delimited by `\n` (aka newline char) can be sent in one go.  After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match=foo.bar.baz'  ```    The `/api/v1/export` endpoint should return the following response:    ```bash  {""metric"":{""__name__"":""foo.bar.baz"",""tag1"":""value1"",""tag2"":""value2""},""values"":[123],""timestamps"":[1560277292000]}  ```    ### Sending OpenTSDB data via HTTP `/api/put` requests    Enable HTTP server for OpenTSDB `/api/put` requests by setting `-opentsdbHTTPListenAddr` command line flag. For instance,  the following command enables OpenTSDB HTTP server on port `4242`:    ```bash  /path/to/victoria-metrics-prod -opentsdbHTTPListenAddr=:4242  ```    Send data to the given address from OpenTSDB-compatible agents.    Example for writing a single data point:    ```bash  curl -H 'Content-Type: application/json' -d '{""metric"":""x.y.z"",""value"":45.34,""tags"":{""t1"":""v1"",""t2"":""v2""}}' http://localhost:4242/api/put  ```    Example for writing multiple data points in a single request:    ```bash  curl -H 'Content-Type: application/json' -d '[{""metric"":""foo"",""value"":45.34},{""metric"":""bar"",""value"":43}]' http://localhost:4242/api/put  ```    After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match[]=x.y.z' -d 'match[]=foo' -d 'match[]=bar'  ```    The `/api/v1/export` endpoint should return the following response:    ```bash  {""metric"":{""__name__"":""foo""},""values"":[45.34],""timestamps"":[1566464846000]}  {""metric"":{""__name__"":""bar""},""values"":[43],""timestamps"":[1566464846000]}  {""metric"":{""__name__"":""x.y.z"",""t1"":""v1"",""t2"":""v2""},""values"":[45.34],""timestamps"":[1566464763000]}  ```    Extra labels may be added to all the imported time series by passing `extra_label=name=value` query args.  For example, `/api/put?extra_label=foo=bar` would add `{foo=""bar""}` label to all the ingested metrics.    ## Prometheus querying API usage    VictoriaMetrics supports the following handlers from [Prometheus querying API](https://prometheus.io/docs/prometheus/latest/querying/api/):    * [/api/v1/query](https://prometheus.io/docs/prometheus/latest/querying/api/#instant-queries)  * [/api/v1/query_range](https://prometheus.io/docs/prometheus/latest/querying/api/#range-queries)  * [/api/v1/series](https://prometheus.io/docs/prometheus/latest/querying/api/#finding-series-by-label-matchers)  * [/api/v1/labels](https://prometheus.io/docs/prometheus/latest/querying/api/#getting-label-names)  * [/api/v1/label/.../values](https://prometheus.io/docs/prometheus/latest/querying/api/#querying-label-values)  * [/api/v1/status/tsdb](https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-stats). See [these docs](#tsdb-stats) for details.  * [/api/v1/targets](https://prometheus.io/docs/prometheus/latest/querying/api/#targets) - see [these docs](#how-to-scrape-prometheus-exporters-such-as-node-exporter) for more details.  * [/federate](https://prometheus.io/docs/prometheus/latest/federation/) - see [these docs](#federation) for more details.    These handlers can be queried from Prometheus-compatible clients such as Grafana or curl.  All the Prometheus querying API handlers can be prepended with `/prometheus` prefix. For example, both `/prometheus/api/v1/query` and `/api/v1/query` should work.    ### Prometheus querying API enhancements    VictoriaMetrics accepts optional `extra_label=<label_name>=<label_value>` query arg, which can be used for enforcing additional label filters for queries. For example,  `/api/v1/query_range?extra_label=user_id=123&extra_label=group_id=456&query=<query>` would automatically add `{user_id=""123"",group_id=""456""}` label filters to the given `<query>`. This functionality can be used for limiting the scope of time series visible to the given tenant. It is expected that the `extra_label` query args are automatically set by auth proxy sitting in front of VictoriaMetrics. See [vmauth](https://docs.victoriametrics.com/vmauth.html) and [vmgateway](https://docs.victoriametrics.com/vmgateway.html) as examples of such proxies.    VictoriaMetrics accepts optional `extra_filters[]=series_selector` query arg, which can be used for enforcing arbitrary label filters for queries. For example,  `/api/v1/query_range?extra_filters[]={env=~""prod|staging"",user=""xyz""}&query=<query>` would automatically add `{env=~""prod|staging"",user=""xyz""}` label filters to the given `<query>`. This functionality can be used for limiting the scope of time series visible to the given tenant. It is expected that the `extra_filters[]` query args are automatically set by auth proxy sitting in front of VictoriaMetrics. See [vmauth](https://docs.victoriametrics.com/vmauth.html) and [vmgateway](https://docs.victoriametrics.com/vmgateway.html) as examples of such proxies.    VictoriaMetrics accepts relative times in `time`, `start` and `end` query args additionally to unix timestamps and [RFC3339](https://www.ietf.org/rfc/rfc3339.txt).  For example, the following query would return data for the last 30 minutes: `/api/v1/query_range?start=-30m&query=...`.    VictoriaMetrics accepts `round_digits` query arg for `/api/v1/query` and `/api/v1/query_range` handlers. It can be used for rounding response values to the given number of digits after the decimal point. For example, `/api/v1/query?query=avg_over_time(temperature[1h])&round_digits=2` would round response values to up to two digits after the decimal point.    By default, VictoriaMetrics returns time series for the last 5 minutes from `/api/v1/series`, while the Prometheus API defaults to all time.  Use `start` and `end` to select a different time range.    Additionally VictoriaMetrics provides the following handlers:    * `/vmui` - Basic Web UI. See [these docs](#vmui).  * `/api/v1/series/count` - returns the total number of time series in the database. Some notes:    * the handler scans all the inverted index, so it can be slow if the database contains tens of millions of time series;    * the handler may count [deleted time series](#how-to-delete-time-series) additionally to normal time series due to internal implementation restrictions;  * `/api/v1/labels/count` - returns a list of `label: values_count` entries. It can be used for determining labels with the maximum number of values.  * `/api/v1/status/active_queries` - returns a list of currently running queries.  * `/api/v1/status/top_queries` - returns the following query lists:    * the most frequently executed queries - `topByCount`    * queries with the biggest average execution duration - `topByAvgDuration`    * queries that took the most time for execution - `topBySumDuration`      The number of returned queries can be limited via `topN` query arg. Old queries can be filtered out with `maxLifetime` query arg.    For example, request to `/api/v1/status/top_queries?topN=5&maxLifetime=30s` would return up to 5 queries per list, which were executed during the last 30 seconds.    VictoriaMetrics tracks the last `-search.queryStats.lastQueriesCount` queries with durations at least `-search.queryStats.minQueryDuration`.    ## Graphite API usage    VictoriaMetrics supports data ingestion in Graphite protocol - see [these docs](#how-to-send-data-from-graphite-compatible-agents-such-as-statsd) for details.  VictoriaMetrics supports the following Graphite querying APIs, which are needed for [Graphite datasource in Grafana](https://grafana.com/docs/grafana/latest/datasources/graphite/):    * Render API - see [these docs](#graphite-render-api-usage).  * Metrics API - see [these docs](#graphite-metrics-api-usage).  * Tags API - see [these docs](#graphite-tags-api-usage).    All the Graphite handlers can be pre-pended with `/graphite` prefix. For example, both `/graphite/metrics/find` and `/metrics/find` should work.    VictoriaMetrics accepts optional query args: `extra_label=<label_name>=<label_value>` and `extra_filters[]=series_selector` query args for all the Graphite APIs. These args can be used for limiting the scope of time series visible to the given tenant. It is expected that the `extra_label` query arg is automatically set by auth proxy sitting in front of VictoriaMetrics. See [vmauth](https://docs.victoriametrics.com/vmauth.html) and [vmgateway](https://docs.victoriametrics.com/vmgateway.html) as examples of such proxies.    [Contact us](mailto:sales@victoriametrics.com) if you need assistance with such a proxy.    VictoriaMetrics supports `__graphite__` pseudo-label for filtering time series with Graphite-compatible filters in [MetricsQL](https://docs.victoriametrics.com/MetricsQL.html). See [these docs](#selecting-graphite-metrics).    ### Graphite Render API usage    [VictoriaMetrics Enterprise](https://victoriametrics.com/products/enterprise/) supports [Graphite Render API](https://graphite.readthedocs.io/en/stable/render_api.html) subset  at `/render` endpoint, which is used by [Graphite datasource in Grafana](https://grafana.com/docs/grafana/latest/datasources/graphite/).  When configuring Graphite datasource in Grafana, the `Storage-Step` http request header must be set to a step between Graphite data points stored in VictoriaMetrics. For example, `Storage-Step: 10s` would mean 10 seconds distance between Graphite datapoints stored in VictoriaMetrics.  Enterprise binaries can be downloaded and evaluated for free from [the releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases).    ### Graphite Metrics API usage    VictoriaMetrics supports the following handlers from [Graphite Metrics API](https://graphite-api.readthedocs.io/en/latest/api.html#the-metrics-api):    * [/metrics/find](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-find)  * [/metrics/expand](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-expand)  * [/metrics/index.json](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-index-json)    VictoriaMetrics accepts the following additional query args at `/metrics/find` and `/metrics/expand`:    * `label` - for selecting arbitrary label values. By default `label=__name__`, i.e. metric names are selected.  * `delimiter` - for using different delimiters in metric name hierachy. For example, `/metrics/find?delimiter=_&query=node_*` would return all the metric name prefixes      that start with `node_`. By default `delimiter=.`.    ### Graphite Tags API usage    VictoriaMetrics supports the following handlers from [Graphite Tags API](https://graphite.readthedocs.io/en/stable/tags.html):    * [/tags/tagSeries](https://graphite.readthedocs.io/en/stable/tags.html#adding-series-to-the-tagdb)  * [/tags/tagMultiSeries](https://graphite.readthedocs.io/en/stable/tags.html#adding-series-to-the-tagdb)  * [/tags](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags)  * [/tags/{tag_name}](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags)  * [/tags/findSeries](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags)  * [/tags/autoComplete/tags](https://graphite.readthedocs.io/en/stable/tags.html#auto-complete-support)  * [/tags/autoComplete/values](https://graphite.readthedocs.io/en/stable/tags.html#auto-complete-support)  * [/tags/delSeries](https://graphite.readthedocs.io/en/stable/tags.html#removing-series-from-the-tagdb)    ## vmui    VictoriaMetrics provides UI for query troubleshooting and exploration. The UI is available at `http://victoriametrics:8428/vmui`.  The UI allows exploring query results via graphs and tables. Graphs support scrolling and zooming:    * Drag the graph to the left / right in order to move the displayed time range into the past / future.  * Hold `Ctrl` (or `Cmd` on MacOS) and scroll up / down in order to zoom in / out the graph.    Query history can be navigated by holding `Ctrl` (or `Cmd` on MacOS) and pressing `up` or `down` arrows on the keyboard while the cursor is located in the query input field.    When querying the [backfilled data](https://docs.victoriametrics.com/#backfilling), it may be useful disabling response cache by clicking `Enable cache` checkbox.    VMUI automatically adjusts the interval between datapoints on the graph depending on the horizontal resolution and on the selected time range. The step value can be customized by clickhing `Override step value` checkbox.    VMUI allows investigating correlations between two queries on the same graph. Just click `+Query` button, enter the second query in the newly appeared input field and press `Ctrl+Enter`. Results for both queries should be displayed simultaneously on the same graph. Every query has its own vertical scale, which is displayed on the left and the right side of the graph. Lines for the second query are dashed.    See the [example VMUI at VictoriaMetrics playground](https://play.victoriametrics.com/select/accounting/1/6a716b0f-38bc-4856-90ce-448fd713e3fe/prometheus/graph/?g0.expr=100%20*%20sum(rate(process_cpu_seconds_total))%20by%20(job)&g0.range_input=1d).    ## How to build from sources    We recommend using either [binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) or  [docker images](https://hub.docker.com/r/victoriametrics/victoria-metrics/) instead of building VictoriaMetrics  from sources. Building from sources is reasonable when developing additional features specific  to your needs or when testing bugfixes.    ### Development build    1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.17.  2. Run `make victoria-metrics` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics` binary and puts it into the `bin` folder.    ### Production build    1. [Install docker](https://docs.docker.com/install/).  2. Run `make victoria-metrics-prod` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics-prod` binary and puts it into the `bin` folder.    ### ARM build    ARM build may run on Raspberry Pi or on [energy-efficient ARM servers](https://blog.cloudflare.com/arm-takes-wing/).    ### Development ARM build    1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.17.  2. Run `make victoria-metrics-arm` or `make victoria-metrics-arm64` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics-arm` or `victoria-metrics-arm64` binary respectively and puts it into the `bin` folder.    ### Production ARM build    1. [Install docker](https://docs.docker.com/install/).  2. Run `make victoria-metrics-arm-prod` or `make victoria-metrics-arm64-prod` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics-arm-prod` or `victoria-metrics-arm64-prod` binary respectively and puts it into the `bin` folder.    ### Pure Go build (CGO_ENABLED=0)    `Pure Go` mode builds only Go code without [cgo](https://golang.org/cmd/cgo/) dependencies.    1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.17.  2. Run `make victoria-metrics-pure` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics-pure` binary and puts it into the `bin` folder.    ### Building docker images    Run `make package-victoria-metrics`. It builds `victoriametrics/victoria-metrics:<PKG_TAG>` docker image locally.  `<PKG_TAG>` is auto-generated image tag, which depends on source code in the repository.  The `<PKG_TAG>` may be manually set via `PKG_TAG=foobar make package-victoria-metrics`.    The base docker image is [alpine](https://hub.docker.com/_/alpine) but it is possible to use any other base image  by setting it via `<ROOT_IMAGE>` environment variable.  For example, the following command builds the image on top of [scratch](https://hub.docker.com/_/scratch) image:    ```bash  ROOT_IMAGE=scratch make package-victoria-metrics  ```    ## Start with docker-compose    [Docker-compose](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/deployment/docker/docker-compose.yml)  helps to spin up VictoriaMetrics, [vmagent](https://docs.victoriametrics.com/vmagent.html) and Grafana with one command.  More details may be found [here](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/master/deployment/docker#folder-contains-basic-images-and-tools-for-building-and-running-victoria-metrics-in-docker).    ## Setting up service    Read [these instructions](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/43) on how to set up VictoriaMetrics as a service in your OS.  There is also [snap package for Ubuntu](https://snapcraft.io/victoriametrics).    ## How to work with snapshots    VictoriaMetrics can create [instant snapshots](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282)  for all the data stored under `-storageDataPath` directory.  Navigate to `http://<victoriametrics-addr>:8428/snapshot/create` in order to create an instant snapshot.  The page will return the following JSON response:    ```json  {""status"":""ok"",""snapshot"":""<snapshot-name>""}  ```    Snapshots are created under `<-storageDataPath>/snapshots` directory, where `<-storageDataPath>`  is the command-line flag value. Snapshots can be archived to backup storage at any time  with [vmbackup](https://docs.victoriametrics.com/vmbackup.html).    The `http://<victoriametrics-addr>:8428/snapshot/list` page contains the list of available snapshots.    Navigate to `http://<victoriametrics-addr>:8428/snapshot/delete?snapshot=<snapshot-name>` in order  to delete `<snapshot-name>` snapshot.    Navigate to `http://<victoriametrics-addr>:8428/snapshot/delete_all` in order to delete all the snapshots.    Steps for restoring from a snapshot:    1. Stop VictoriaMetrics with `kill -INT`.  2. Restore snapshot contents from backup with [vmrestore](https://docs.victoriametrics.com/vmrestore.html)     to the directory pointed by `-storageDataPath`.  3. Start VictoriaMetrics.    ## How to delete time series    Send a request to `http://<victoriametrics-addr>:8428/api/v1/admin/tsdb/delete_series?match[]=<timeseries_selector_for_delete>`,  where `<timeseries_selector_for_delete>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)  for metrics to delete. After that all the time series matching the given selector are deleted. Storage space for  the deleted time series isn't freed instantly - it is freed during subsequent [background merges of data files](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).  Note that background merges may never occur for data from previous months, so storage space won't be freed for historical data.  In this case [forced merge](#forced-merge) may help freeing up storage space.    It is recommended verifying which metrics will be deleted with the call to `http://<victoria-metrics-addr>:8428/api/v1/series?match[]=<timeseries_selector_for_delete>`  before actually deleting the metrics.  By default this query will only scan series in the past 5 minutes, so you may need to  adjust `start` and `end` to a suitable range to achieve match hits.    The `/api/v1/admin/tsdb/delete_series` handler may be protected with `authKey` if `-deleteAuthKey` command-line flag is set.    The delete API is intended mainly for the following cases:    * One-off deleting of accidentally written invalid (or undesired) time series.  * One-off deleting of user data due to [GDPR](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation).    Using the delete API is not recommended in the following cases, since it brings a non-zero overhead:    * Regular cleanups for unneeded data. Just prevent writing unneeded data into VictoriaMetrics.    This can be done with [relabeling](#relabeling).    See [this article](https://www.robustperception.io/relabelling-can-discard-targets-timeseries-and-alerts) for details.  * Reducing disk space usage by deleting unneeded time series. This doesn't work as expected, since the deleted    time series occupy disk space until the next merge operation, which can never occur when deleting too old data.    [Forced merge](#forced-merge) may be used for freeing up disk space occupied by old data.    It's better to use the `-retentionPeriod` command-line flag for efficient pruning of old data.    ## Forced merge    VictoriaMetrics performs [data compactions in background](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282)  in order to keep good performance characteristics when accepting new data. These compactions (merges) are performed independently on per-month partitions.  This means that compactions are stopped for per-month partitions if no new data is ingested into these partitions.  Sometimes it is necessary to trigger compactions for old partitions. For instance, in order to free up disk space occupied by [deleted time series](#how-to-delete-time-series).  In this case forced compaction may be initiated on the specified per-month partition by sending request to `/internal/force_merge?partition_prefix=YYYY_MM`,  where `YYYY_MM` is per-month partition name. For example, `http://victoriametrics:8428/internal/force_merge?partition_prefix=2020_08` would initiate forced  merge for August 2020 partition. The call to `/internal/force_merge` returns immediately, while the corresponding forced merge continues running in background.    Forced merges may require additional CPU, disk IO and storage space resources. It is unnecessary to run forced merge under normal conditions,  since VictoriaMetrics automatically performs [optimal merges in background](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282)  when new data is ingested into it.    ## How to export time series    VictoriaMetrics provides the following handlers for exporting data:    * `/api/v1/export` for exporing data in JSON line format. See [these docs](#how-to-export-data-in-json-line-format) for details.  * `/api/v1/export/csv` for exporting data in CSV. See [these docs](#how-to-export-csv-data) for details.  * `/api/v1/export/native` for exporting data in native binary format. This is the most efficient format for data export.    See [these docs](#how-to-export-data-in-native-format) for details.    ### How to export data in JSON line format    Send a request to `http://<victoriametrics-addr>:8428/api/v1/export?match[]=<timeseries_selector_for_export>`,  where `<timeseries_selector_for_export>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)  for metrics to export. Use `{__name__!=""""}` selector for fetching all the time series.  The response would contain all the data for the selected time series in [JSON streaming format](https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON).  Each JSON line contains samples for a single time series. An example output:    ```jsonl  {""metric"":{""__name__"":""up"",""job"":""node_exporter"",""instance"":""localhost:9100""},""values"":[0,0,0],""timestamps"":[1549891472010,1549891487724,1549891503438]}  {""metric"":{""__name__"":""up"",""job"":""prometheus"",""instance"":""localhost:9090""},""values"":[1,1,1],""timestamps"":[1549891461511,1549891476511,1549891491511]}  ```    Optional `start` and `end` args may be added to the request in order to limit the time frame for the exported data. These args may contain either  unix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values.    Optional `max_rows_per_line` arg may be added to the request for limiting the maximum number of rows exported per each JSON line.  Optional `reduce_mem_usage=1` arg may be added to the request for reducing memory usage when exporting big number of time series.  In this case the output may contain multiple lines with samples for the same time series.    Pass `Accept-Encoding: gzip` HTTP header in the request to `/api/v1/export` in order to reduce network bandwidth during exporing big amounts  of time series data. This enables gzip compression for the exported data. Example for exporting gzipped data:    ```bash  curl -H 'Accept-Encoding: gzip' http://localhost:8428/api/v1/export -d 'match[]={__name__!=""""}' > data.jsonl.gz  ```    The maximum duration for each request to `/api/v1/export` is limited by `-search.maxExportDuration` command-line flag.    Exported data can be imported via POST'ing it to [/api/v1/import](#how-to-import-data-in-json-line-format).    The [deduplication](#deduplication) is applied to the data exported via `/api/v1/export` by default. The deduplication  isn't applied if `reduce_mem_usage=1` query arg is passed to the request.    ### How to export CSV data    Send a request to `http://<victoriametrics-addr>:8428/api/v1/export/csv?format=<format>&match=<timeseries_selector_for_export>`,  where:    * `<format>` must contain comma-delimited label names for the exported CSV. The following special label names are supported:    * `__name__` - metric name    * `__value__` - sample value    * `__timestamp__:<ts_format>` - sample timestamp. `<ts_format>` can have the following values:      * `unix_s` - unix seconds      * `unix_ms` - unix milliseconds      * `unix_ns` - unix nanoseconds      * `rfc3339` - [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) time      * `custom:<layout>` - custom layout for time that is supported by [time.Format](https://golang.org/pkg/time/#Time.Format) function from Go.    * `<timeseries_selector_for_export>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)  for metrics to export.    Optional `start` and `end` args may be added to the request in order to limit the time frame for the exported data. These args may contain either  unix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values.    The exported CSV data can be imported to VictoriaMetrics via [/api/v1/import/csv](#how-to-import-csv-data).    The [deduplication](#deduplication) is applied for the data exported in CSV by default. It is possible to export raw data without de-duplication by passing `reduce_mem_usage=1` query arg to `/api/v1/export/csv`.    ### How to export data in native format    Send a request to `http://<victoriametrics-addr>:8428/api/v1/export/native?match[]=<timeseries_selector_for_export>`,  where `<timeseries_selector_for_export>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)  for metrics to export. Use `{__name__=~"".*""}` selector for fetching all the time series.    On large databases you may experience problems with limit on the number of time series, which can be exported. In this case you need to adjust `-search.maxExportSeries` command-line flag:    ```bash  # count unique timeseries in database  wget -O- -q 'http://your_victoriametrics_instance:8428/api/v1/series/count' | jq '.data[0]'    # relaunch victoriametrics with search.maxExportSeries more than value from previous command  ```    Optional `start` and `end` args may be added to the request in order to limit the time frame for the exported data. These args may contain either  unix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values.    The exported data can be imported to VictoriaMetrics via [/api/v1/import/native](#how-to-import-data-in-native-format).  The native export format may change in incompatible way between VictoriaMetrics releases, so the data exported from the release X  can fail to be imported into VictoriaMetrics release Y.    The [deduplication](#deduplication) isn't applied for the data exported in native format. It is expected that the de-duplication is performed during data import.    ## How to import time series data    Time series data can be imported into VictoriaMetrics via any supported data ingestion protocol:    * [Prometheus remote_write API](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write). See [these docs](#prometheus-setup) for details.  * DataDog `submit metrics` API. See [these docs](#how-to-send-data-from-datadog-agent) for details.  * InfluxDB line protocol. See [these docs](#how-to-send-data-from-influxdb-compatible-agents-such-as-telegraf) for details.  * Graphite plaintext protocol. See [these docs](#how-to-send-data-from-graphite-compatible-agents-such-as-statsd) for details.  * OpenTSDB telnet put protocol. See [these docs](#sending-data-via-telnet-put-protocol) for details.  * OpenTSDB http `/api/put` protocol. See [these docs](#sending-opentsdb-data-via-http-apiput-requests) for details.  * `/api/v1/import` for importing data obtained from [/api/v1/export](#how-to-export-data-in-json-line-format).    See [these docs](#how-to-import-data-in-json-line-format) for details.  * `/api/v1/import/native` for importing data obtained from [/api/v1/export/native](#how-to-export-data-in-native-format).    See [these docs](#how-to-import-data-in-native-format) for details.  * `/api/v1/import/csv` for importing arbitrary CSV data. See [these docs](#how-to-import-csv-data) for details.  * `/api/v1/import/prometheus` for importing data in Prometheus exposition format. See [these docs](#how-to-import-data-in-prometheus-exposition-format) for details.    ### How to import data in JSON line format    Example for importing data obtained via [/api/v1/export](#how-to-export-data-in-json-line-format):    ```bash  # Export the data from <source-victoriametrics>:  curl http://source-victoriametrics:8428/api/v1/export -d 'match={__name__!=""""}' > exported_data.jsonl    # Import the data to <destination-victoriametrics>:  curl -X POST http://destination-victoriametrics:8428/api/v1/import -T exported_data.jsonl  ```    Pass `Content-Encoding: gzip` HTTP request header to `/api/v1/import` for importing gzipped data:    ```bash  # Export gzipped data from <source-victoriametrics>:  curl -H 'Accept-Encoding: gzip' http://source-victoriametrics:8428/api/v1/export -d 'match={__name__!=""""}' > exported_data.jsonl.gz    # Import gzipped data to <destination-victoriametrics>:  curl -X POST -H 'Content-Encoding: gzip' http://destination-victoriametrics:8428/api/v1/import -T exported_data.jsonl.gz  ```    Extra labels may be added to all the imported time series by passing `extra_label=name=value` query args.  For example, `/api/v1/import?extra_label=foo=bar` would add `""foo"":""bar""` label to all the imported time series.    Note that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.    VictoriaMetrics parses input JSON lines one-by-one. It loads the whole JSON line in memory, then parses it and then saves the parsed samples into persistent storage. This means that VictoriaMetrics can occupy big amounts of RAM when importing too long JSON lines. The solution is to split too long JSON lines into smaller lines. It is OK if samples for a single time series are split among multiple JSON lines.    ### How to import data in native format    The specification of VictoriaMetrics' native format may yet change and is not formally documented yet. So currently we do not recommend that external clients attempt to pack their own metrics in native format file.    If you have a native format file obtained via [/api/v1/export/native](#how-to-export-data-in-native-format) however this is the most efficient protocol for importing data in.    ```bash  # Export the data from <source-victoriametrics>:  curl http://source-victoriametrics:8428/api/v1/export/native -d 'match={__name__!=""""}' > exported_data.bin    # Import the data to <destination-victoriametrics>:  curl -X POST http://destination-victoriametrics:8428/api/v1/import/native -T exported_data.bin  ```    Extra labels may be added to all the imported time series by passing `extra_label=name=value` query args.  For example, `/api/v1/import/native?extra_label=foo=bar` would add `""foo"":""bar""` label to all the imported time series.    Note that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.    ### How to import CSV data    Arbitrary CSV data can be imported via `/api/v1/import/csv`. The CSV data is imported according to the provided `format` query arg.  The `format` query arg must contain comma-separated list of parsing rules for CSV fields. Each rule consists of three parts delimited by a colon:    ```  <column_pos>:<type>:<context>  ```    * `<column_pos>` is the position of the CSV column (field). Column numbering starts from 1. The order of parsing rules may be arbitrary.  * `<type>` describes the column type. Supported types are:    * `metric` - the corresponding CSV column at `<column_pos>` contains metric value, which must be integer or floating-point number.      The metric name is read from the `<context>`. CSV line must have at least a single metric field. Multiple metric fields per CSV line is OK.    * `label` - the corresponding CSV column at `<column_pos>` contains label value. The label name is read from the `<context>`.      CSV line may have arbitrary number of label fields. All these labels are attached to all the configured metrics.    * `time` - the corresponding CSV column at `<column_pos>` contains metric time. CSV line may contain either one or zero columns with time.      If CSV line has no time, then the current time is used. The time is applied to all the configured metrics.      The format of the time is configured via `<context>`. Supported time formats are:      * `unix_s` - unix timestamp in seconds.      * `unix_ms` - unix timestamp in milliseconds.      * `unix_ns` - unix timestamp in nanoseconds. Note that VictoriaMetrics rounds the timestamp to milliseconds.      * `rfc3339` - timestamp in [RFC3339](https://tools.ietf.org/html/rfc3339) format, i.e. `2006-01-02T15:04:05Z`.      * `custom:<layout>` - custom layout for the timestamp. The `<layout>` may contain arbitrary time layout according to [time.Parse rules in Go](https://golang.org/pkg/time/#Parse).    Each request to `/api/v1/import/csv` may contain arbitrary number of CSV lines.    Example for importing CSV data via `/api/v1/import/csv`:    ```bash  curl -d ""GOOG,1.23,4.56,NYSE"" 'http://localhost:8428/api/v1/import/csv?format=2:metric:ask,3:metric:bid,1:label:ticker,4:label:market'  curl -d ""MSFT,3.21,1.67,NASDAQ"" 'http://localhost:8428/api/v1/import/csv?format=2:metric:ask,3:metric:bid,1:label:ticker,4:label:market'  ```    After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match[]={ticker!=""""}'  ```    The following response should be returned:    ```bash  {""metric"":{""__name__"":""bid"",""market"":""NASDAQ"",""ticker"":""MSFT""},""values"":[1.67],""timestamps"":[1583865146520]}  {""metric"":{""__name__"":""bid"",""market"":""NYSE"",""ticker"":""GOOG""},""values"":[4.56],""timestamps"":[1583865146495]}  {""metric"":{""__name__"":""ask"",""market"":""NASDAQ"",""ticker"":""MSFT""},""values"":[3.21],""timestamps"":[1583865146520]}  {""metric"":{""__name__"":""ask"",""market"":""NYSE"",""ticker"":""GOOG""},""values"":[1.23],""timestamps"":[1583865146495]}  ```    Extra labels may be added to all the imported lines by passing `extra_label=name=value` query args.  For example, `/api/v1/import/csv?extra_label=foo=bar` would add `""foo"":""bar""` label to all the imported lines.    Note that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.    ### How to import data in Prometheus exposition format    VictoriaMetrics accepts data in [Prometheus exposition format](https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format)  and in [OpenMetrics format](https://github.com/OpenObservability/OpenMetrics/blob/master/specification/OpenMetrics.md)  via `/api/v1/import/prometheus` path. For example, the following line imports a single line in Prometheus exposition format into VictoriaMetrics:    ```bash  curl -d 'foo{bar=""baz""} 123' -X POST 'http://localhost:8428/api/v1/import/prometheus'  ```    The following command may be used for verifying the imported data:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match={__name__=~""foo""}'  ```    It should return something like the following:    ```  {""metric"":{""__name__"":""foo"",""bar"":""baz""},""values"":[123],""timestamps"":[1594370496905]}  ```    Pass `Content-Encoding: gzip` HTTP request header to `/api/v1/import/prometheus` for importing gzipped data:    ```bash  # Import gzipped data to <destination-victoriametrics>:  curl -X POST -H 'Content-Encoding: gzip' http://destination-victoriametrics:8428/api/v1/import/prometheus -T prometheus_data.gz  ```    Extra labels may be added to all the imported metrics by passing `extra_label=name=value` query args.  For example, `/api/v1/import/prometheus?extra_label=foo=bar` would add `{foo=""bar""}` label to all the imported metrics.    If timestamp is missing in `<metric> <value> <timestamp>` Prometheus exposition format line, then the current timestamp is used during data ingestion.  It can be overriden by passing unix timestamp in *milliseconds* via `timestamp` query arg. For example, `/api/v1/import/prometheus?timestamp=1594370496905`.    VictoriaMetrics accepts arbitrary number of lines in a single request to `/api/v1/import/prometheus`, i.e. it supports data streaming.    Note that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.    VictoriaMetrics also may scrape Prometheus targets - see [these docs](#how-to-scrape-prometheus-exporters-such-as-node-exporter).    ## Relabeling    VictoriaMetrics supports Prometheus-compatible relabeling for all the ingested metrics if `-relabelConfig` command-line flag points  to a file containing a list of [relabel_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config) entries.  The `-relabelConfig` also can point to http or https url. For example, `-relabelConfig=https://config-server/relabel_config.yml`.  See [this article with relabeling tips and tricks](https://valyala.medium.com/how-to-use-relabeling-in-prometheus-and-victoriametrics-8b90fc22c4b2).    Example contents for `-relabelConfig` file:    ```yml  # Add {cluster=""dev""} label.  - target_label: cluster    replacement: dev    # Drop the metric (or scrape target) with `{__meta_kubernetes_pod_container_init=""true""}` label.  - action: drop    source_labels: [__meta_kubernetes_pod_container_init]    regex: true  ```    See [these docs](https://docs.victoriametrics.com/vmagent.html#relabeling) for more details about relabeling in VictoriaMetrics.    ## Federation    VictoriaMetrics exports [Prometheus-compatible federation data](https://prometheus.io/docs/prometheus/latest/federation/)  at `http://<victoriametrics-addr>:8428/federate?match[]=<timeseries_selector_for_federation>`.    Optional `start` and `end` args may be added to the request in order to scrape the last point for each selected time series on the `[start ... end]` interval.  `start` and `end` may contain either unix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values. By default, the last point  on the interval `[now - max_lookback ... now]` is scraped for each time series. The default value for `max_lookback` is `5m` (5 minutes), but it can be overridden.  For instance, `/federate?match[]=up&max_lookback=1h` would return last points on the `[now - 1h ... now]` interval. This may be useful for time series federation  with scrape intervals exceeding `5m`.    ## Capacity planning    VictoriaMetrics uses lower amounts of CPU, RAM and storage space on production workloads compared to competing solutions (Prometheus, Thanos, Cortex, TimescaleDB, InfluxDB, QuestDB, M3DB) according to [our case studies](https://docs.victoriametrics.com/CaseStudies.html).    VictoriaMetrics capacity scales linearly with the available resources. The needed amounts of CPU and RAM highly depends on the workload - the number of [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series), series [churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate), query types, query qps, etc. It is recommended setting up a test VictoriaMetrics for your production workload and iteratively scaling CPU and RAM resources until it becomes stable according to [troubleshooting docs](#troubleshooting). A single-node VictoriaMetrics works perfectly with the following production workload according to [our case studies](https://docs.victoriametrics.com/CaseStudies.html):    * Ingestion rate: 1.5+ million samples per second  * Active time series: 50+ million  * Total time series: 5+ billion  * Time series churn rate: 150+ million of new series per day  * Total number of samples: 10+ trillion  * Queries: 200+ qps  * Query latency (99th percentile): 1 second    The needed storage space for the given retention (the retention is set via `-retentionPeriod` command-line flag) can be extrapolated from disk space usage in a test run. For example, if `-storageDataPath` directory size becomes 10GB after a day-long test run on a production workload, then it will need at least `10GB*100=1TB` of disk space for `-retentionPeriod=100d` (100-days retention period).    It is recommended leaving the following amounts of spare resources:    * 50% of free RAM for reducing the probability of OOM (out of memory) crashes and slowdowns during temporary spikes in workload.  * 50% of spare CPU for reducing the probability of slowdowns during temporary spikes in workload.  * At least 30% of free storage space at the directory pointed by `-storageDataPath` command-line flag. See also `-storage.minFreeDiskSpaceBytes` command-line flag description [here](#list-of-command-line-flags).    ## High availability    * Install multiple VictoriaMetrics instances in distinct datacenters (availability zones).  * Pass addresses of these instances to [vmagent](https://docs.victoriametrics.com/vmagent.html) via `-remoteWrite.url` command-line flag:    ```bash  /path/to/vmagent -remoteWrite.url=http://<victoriametrics-addr-1>:8428/api/v1/write -remoteWrite.url=http://<victoriametrics-addr-2>:8428/api/v1/write  ```    Alternatively these addresses may be passed to `remote_write` section in Prometheus config:    ```yml  remote_write:    - url: http://<victoriametrics-addr-1>:8428/api/v1/write      queue_config:        max_samples_per_send: 10000    # ...    - url: http://<victoriametrics-addr-N>:8428/api/v1/write      queue_config:        max_samples_per_send: 10000  ```    * Apply the updated config:    ```bash  kill -HUP `pidof prometheus`  ```    It is recommended to use [vmagent](https://docs.victoriametrics.com/vmagent.html) instead of Prometheus for highly loaded setups.    * Now Prometheus should write data into all the configured `remote_write` urls in parallel.  * Set up [Promxy](https://github.com/jacksontj/promxy) in front of all the VictoriaMetrics replicas.  * Set up Prometheus datasource in Grafana that points to Promxy.    If you have Prometheus HA pairs with replicas `r1` and `r2` in each pair, then configure each `r1`  to write data to `victoriametrics-addr-1`, while each `r2` should write data to `victoriametrics-addr-2`.    Another option is to write data simultaneously from Prometheus HA pair to a pair of VictoriaMetrics instances  with the enabled de-duplication. See [this section](#deduplication) for details.    ## Deduplication    VictoriaMetrics de-duplicates data points if `-dedup.minScrapeInterval` command-line flag is set to positive duration. For example, `-dedup.minScrapeInterval=60s` would de-duplicate data points on the same time series if they fall within the same discrete 60s bucket.  The earliest data point will be kept. In the case of equal timestamps, an arbitrary data point will be kept. See [this comment](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/2112#issuecomment-1032587618) for more details on how downsampling works.    The `-dedup.minScrapeInterval=D` is equivalent to `-downsampling.period=0s:D` if [downsampling](#downsampling) is enabled. It is safe to use deduplication and downsampling simultaneously.    The recommended value for `-dedup.minScrapeInterval` must equal to `scrape_interval` config from Prometheus configs. It is recommended to have a single `scrape_interval` across all the scrape targets. See [this article](https://www.robustperception.io/keep-it-simple-scrape_interval-id) for details.    The de-duplication reduces disk space usage if multiple identically configured [vmagent](https://docs.victoriametrics.com/vmagent.html) or Prometheus instances in HA pair  write data to the same VictoriaMetrics instance. These vmagent or Prometheus instances must have identical  `external_labels` section in their configs, so they write data to the same time series.    ## Storage    VictoriaMetrics stores time series data in [MergeTree](https://en.wikipedia.org/wiki/Log-structured_merge-tree)-like  data structures. On insert, VictoriaMetrics accumulates up to 1s of data and dumps it on disk to  `<-storageDataPath>/data/small/YYYY_MM/` subdirectory forming a `part` with the following  name pattern: `rowsCount_blocksCount_minTimestamp_maxTimestamp`. Each part consists of two ""columns"":  values and timestamps. These are sorted and compressed raw time series values. Additionally, part contains  index files for searching for specific series in the values and timestamps files.    `Parts` are periodically merged into the bigger parts. The resulting `part` is constructed  under `<-storageDataPath>/data/{small,big}/YYYY_MM/tmp` subdirectory. When the resulting `part` is complete, it is atomically moved from the `tmp`  to its own subdirectory, while the source parts are atomically removed. The end result is that the source  parts are substituted by a single resulting bigger `part` in the `<-storageDataPath>/data/{small,big}/YYYY_MM/` directory.  Information about merging process is available in [single-node VictoriaMetrics](https://grafana.com/dashboards/10229)  and [clustered VictoriaMetrics](https://grafana.com/grafana/dashboards/11176) Grafana dashboards.  See more details in [monitoring docs](#monitoring).    The `merge` process is usually named ""compaction"", because the resulting `part` size is usually smaller than  the sum of the source `parts`. There are following benefits of doing the merge process:    * it improves query performance, since lower number of `parts` are inspected with each query;  * it reduces the number of data files, since each `part`contains fixed number of files;  * better compression rate for the resulting part.    Newly added `parts` either appear in the storage or fail to appear.  Storage never contains partially created parts. The same applies to merge process â€” `parts` are either fully  merged into a new `part` or fail to merge. There are no partially merged `parts` in MergeTree.  `Part` contents in MergeTree never change. Parts are immutable. They may be only deleted after the merge  to a bigger `part` or when the `part` contents goes outside the configured `-retentionPeriod`.    See [this article](https://valyala.medium.com/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282) for more details.    See also [how to work with snapshots](#how-to-work-with-snapshots).    ## Retention    Retention is configured with the `-retentionPeriod` command-line flag, which takes a number followed by a time unit character - `h(ours)`, `d(ays)`, `w(eeks)`, `y(ears)`. If the time unit is not specified, a month is assumed. For instance, `-retentionPeriod=3` means that the data will be stored for 3 months and then deleted. The default retention period is one month.    Data is split in per-month partitions inside `<-storageDataPath>/data/{small,big}` folders.  Data partitions outside the configured retention are deleted on the first day of the new month.  Each partition consists of one or more data parts with the following name pattern `rowsCount_blocksCount_minTimestamp_maxTimestamp`.  Data parts outside of the configured retention are eventually deleted during  [background merge](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).    The maximum disk space usage for a given `-retentionPeriod` is going to be (`-retentionPeriod` + 1) months.  For example, if `-retentionPeriod` is set to 1, data for January is deleted on March 1st.    Please note, the time range covered by data part is not limited by retention period unit. Hence, data part may contain data  for multiple days and will be deleted only when fully outside of the configured retention.    It is safe to extend `-retentionPeriod` on existing data. If `-retentionPeriod` is set to a lower  value than before, then data outside the configured period will be eventually deleted.    VictoriaMetrics does not support indefinite retention, but you can specify an arbitrarily high duration, e.g. `-retentionPeriod=100y`.    ## Multiple retentions    A single instance of VictoriaMetrics supports only a single retention, which can be configured via `-retentionPeriod` command-line flag. If you need multiple retentions, then you may start multiple VictoriaMetrics instances with distinct values for the following flags:    * `-retentionPeriod`  * `-storageDataPath`, so the data for each retention period is saved in a separate directory  * `-httpListenAddr`, so clients may reach VictoriaMetrics instance with proper retention    Then set up [vmauth](https://docs.victoriametrics.com/vmauth.html) in front of VictoriaMetrics instances,  so it could route requests from particular user to VictoriaMetrics with the desired retention.  The same scheme could be implemented for multiple tenants in [VictoriaMetrics cluster](https://docs.victoriametrics.com/Cluster-VictoriaMetrics.html).  See [these docs](https://docs.victoriametrics.com/guides/guide-vmcluster-multiple-retention-setup.html) for multi-retention setup details.    ## Downsampling    [VictoriaMetrics Enterprise](https://victoriametrics.com/products/enterprise/) supports multi-level downsampling with `-downsampling.period` command-line flag. For example:    * `-downsampling.period=30d:5m` instructs VictoriaMetrics to [deduplicate](#deduplication) samples older than 30 days with 5 minutes interval.    * `-downsampling.period=30d:5m,180d:1h` instructs VictoriaMetrics to deduplicate samples older than 30 days with 5 minutes interval and to deduplicate samples older than 180 days with 1 hour interval.    Downsampling is applied independently per each time series. It can reduce disk space usage and improve query performance if it is applied to time series with big number of samples per each series. The downsampling doesn't improve query performance if the database contains big number of time series with small number of samples per each series (aka [high churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate)), since downsampling doesn't reduce the number of time series. So the majority of time is spent on searching for the matching time series. It is possible to use recording rules in [vmalert](https://docs.victoriametrics.com/vmalert.html) in order to reduce the number of time series. See [these docs](https://docs.victoriametrics.com/vmalert.html#downsampling-and-aggregation-via-vmalert).    The downsampling can be evaluated for free by downloading and using enterprise binaries from [the releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases).    ## Multi-tenancy    Single-node VictoriaMetrics doesn't support multi-tenancy. Use [cluster version](https://docs.victoriametrics.com/Cluster-VictoriaMetrics.html#multitenancy) instead.    ## Scalability and cluster version    Though single-node VictoriaMetrics cannot scale to multiple nodes, it is optimized for resource usage - storage size / bandwidth / IOPS, RAM, CPU.  This means that a single-node VictoriaMetrics may scale vertically and substitute a moderately sized cluster built with competing solutions  such as Thanos, Uber M3, InfluxDB or TimescaleDB. See [vertical scalability benchmarks](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae).    So try single-node VictoriaMetrics at first and then [switch to cluster version](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster) if you still need  horizontally scalable long-term remote storage for really large Prometheus deployments.  [Contact us](mailto:info@victoriametrics.com) for enterprise support.    ## Alerting    It is recommended using [vmalert](https://docs.victoriametrics.com/vmalert.html) for alerting.    Additionally, alerting can be set up with the following tools:    * With Prometheus - see [the corresponding docs](https://prometheus.io/docs/alerting/overview/).  * With Promxy - see [the corresponding docs](https://github.com/jacksontj/promxy/blob/master/README.md#how-do-i-use-alertingrecording-rules-in-promxy).  * With Grafana - see [the corresponding docs](https://grafana.com/docs/alerting/rules/).    ## Security    Do not forget protecting sensitive endpoints in VictoriaMetrics when exposing it to untrusted networks such as the internet.  Consider setting the following command-line flags:    * `-tls`, `-tlsCertFile` and `-tlsKeyFile` for switching from HTTP to HTTPS.  * `-httpAuth.username` and `-httpAuth.password` for protecting all the HTTP endpoints    with [HTTP Basic Authentication](https://en.wikipedia.org/wiki/Basic_access_authentication).  * `-deleteAuthKey` for protecting `/api/v1/admin/tsdb/delete_series` endpoint. See [how to delete time series](#how-to-delete-time-series).  * `-snapshotAuthKey` for protecting `/snapshot*` endpoints. See [how to work with snapshots](#how-to-work-with-snapshots).  * `-forceMergeAuthKey` for protecting `/internal/force_merge` endpoint. See [force merge docs](#forced-merge).  * `-search.resetCacheAuthKey` for protecting `/internal/resetRollupResultCache` endpoint. See [backfilling](#backfilling) for more details.  * `-configAuthKey` for protecting `/config` endpoint, since it may contain sensitive information such as passwords.    - `-pprofAuthKey` for protecting `/debug/pprof/*` endpoints, which can be used for [profiling](#profiling).    Explicitly set internal network interface for TCP and UDP ports for data ingestion with Graphite and OpenTSDB formats.  For example, substitute `-graphiteListenAddr=:2003` with `-graphiteListenAddr=<internal_iface_ip>:2003`.    Prefer authorizing all the incoming requests from untrusted networks with [vmauth](https://docs.victoriametrics.com/vmauth.html)  or similar auth proxy.    ## Tuning    * There is no need for VictoriaMetrics tuning since it uses reasonable defaults for command-line flags,    which are automatically adjusted for the available CPU and RAM resources.  * There is no need for Operating System tuning since VictoriaMetrics is optimized for default OS settings.    The only option is increasing the limit on [the number of open files in the OS](https://medium.com/@muhammadtriwibowo/set-permanently-ulimit-n-open-files-in-ubuntu-4d61064429a).    The recommendation is not specific for VictoriaMetrics only but also for any service which handles many HTTP connections and stores data on disk.  * VictoriaMetrics is a write-heavy application and its performance depends on disk performance. So be careful with other    applications or utilities (like [fstrim](http://manpages.ubuntu.com/manpages/bionic/man8/fstrim.8.html))    which could [exhaust disk resources](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/1521).  * The recommended filesystem is `ext4`, the recommended persistent storage is [persistent HDD-based disk on GCP](https://cloud.google.com/compute/docs/disks/#pdspecs),    since it is protected from hardware failures via internal replication and it can be [resized on the fly](https://cloud.google.com/compute/docs/disks/add-persistent-disk#resize_pd).    If you plan to store more than 1TB of data on `ext4` partition or plan extending it to more than 16TB,    then the following options are recommended to pass to `mkfs.ext4`:    ```bash  mkfs.ext4 ... -O 64bit,huge_file,extent -T huge  ```    ## Monitoring    VictoriaMetrics exports internal metrics in Prometheus format at `/metrics` page.  These metrics may be collected by [vmagent](https://docs.victoriametrics.com/vmagent.html)  or Prometheus by adding the corresponding scrape config to it.  Alternatively they can be self-scraped by setting `-selfScrapeInterval` command-line flag to duration greater than 0.  For example, `-selfScrapeInterval=10s` would enable self-scraping of `/metrics` page with 10 seconds interval.    There are officials Grafana dashboards for [single-node VictoriaMetrics](https://grafana.com/dashboards/10229) and [clustered VictoriaMetrics](https://grafana.com/grafana/dashboards/11176). There is also an [alternative dashboard for clustered VictoriaMetrics](https://grafana.com/grafana/dashboards/11831).    Graphs on these dashboard contain useful hints - hover the `i` icon at the top left corner of each graph in order to read it.    It is recommended setting up alerts in [vmalert](https://docs.victoriametrics.com/vmalert.html) or in Prometheus from [this config](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/deployment/docker/alerts.yml).    The most interesting metrics are:    * `vm_cache_entries{type=""storage/hour_metric_ids""}` - the number of time series with new data points during the last hour    aka [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series).  * `increase(vm_new_timeseries_created_total[1h])` - time series [churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate) during the previous hour.  * `sum(vm_rows{type=~""storage/.*""})` - total number of `(timestamp, value)` data points in the database.  * `sum(rate(vm_rows_inserted_total[5m]))` - ingestion rate, i.e. how many samples are inserted int the database per second.  * `vm_free_disk_space_bytes` - free space left at `-storageDataPath`.  * `sum(vm_data_size_bytes)` - the total size of data on disk.  * `increase(vm_slow_row_inserts_total[5m])` - the number of slow inserts during the last 5 minutes.    If this number remains high during extended periods of time, then it is likely more RAM is needed for optimal handling    of the current number of [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series).  * `increase(vm_slow_metric_name_loads_total[5m])` - the number of slow loads of metric names during the last 5 minutes.    If this number remains high during extended periods of time, then it is likely more RAM is needed for optimal handling    of the current number of [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series).    VictoriaMetrics also exposes currently running queries with their execution times at `/api/v1/status/active_queries` page.    See the example of alerting rules for VM components [here](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/deployment/docker/alerts.yml).    ## TSDB stats    VictoriaMetrics returns TSDB stats at `/api/v1/status/tsdb` page in the way similar to Prometheus - see [these Prometheus docs](https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-stats). VictoriaMetrics accepts the following optional query args at `/api/v1/status/tsdb` page:    * `topN=N` where `N` is the number of top entries to return in the response. By default top 10 entries are returned.  * `date=YYYY-MM-DD` where `YYYY-MM-DD` is the date for collecting the stats. By default the stats is collected for the current day.  * `match[]=SELECTOR` where `SELECTOR` is an arbitrary [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors) for series to take into account during stats calculation. By default all the series are taken into account.  * `extra_label=LABEL=VALUE`. See [these docs](#prometheus-querying-api-enhancements) for more details.    ## Cardinality limiter    By default VictoriaMetrics doesn't limit the number of stored time series. The limit can be enforced by setting the following command-line flags:    * `-storage.maxHourlySeries` - limits the number of time series that can be added during the last hour. Useful for limiting the number of [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series).  * `-storage.maxDailySeries` - limits the number of time series that can be added during the last day. Useful for limiting daily [churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate).    Both limits can be set simultaneously. If any of these limits is reached, then incoming samples for new time series are dropped. A sample of dropped series is put in the log with `WARNING` level.    The exceeded limits can be [monitored](#monitoring) with the following metrics:    * `vm_hourly_series_limit_rows_dropped_total` - the number of metrics dropped due to exceeded hourly limit on the number of unique time series.  * `vm_daily_series_limit_rows_dropped_total` - the number of metrics dropped due to exceeded daily limit on the number of unique time series.    These limits are approximate, so VictoriaMetrics can underflow/overflow the limit by a small percentage (usually less than 1%).    See also more advanced [cardinality limiter in vmagent](https://docs.victoriametrics.com/vmagent.html#cardinality-limiter).    ## Troubleshooting    * It is recommended to use default command-line flag values (i.e. don't set them explicitly) until the need    of tweaking these flag values arises.    * It is recommended inspecting logs during troubleshooting, since they may contain useful information.    * It is recommended upgrading to the latest available release from [this page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases),    since the encountered issue could be already fixed there.    * It is recommended to have at least 50% of spare resources for CPU, disk IO and RAM, so VictoriaMetrics could handle short spikes in the workload without performance issues.    * VictoriaMetrics requires free disk space for [merging data files to bigger ones](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).    It may slow down when there is no enough free space left. So make sure `-storageDataPath` directory    has at least 20% of free space. The remaining amount of free space    can be [monitored](#monitoring) via `vm_free_disk_space_bytes` metric. The total size of data    stored on the disk can be monitored via sum of `vm_data_size_bytes` metrics.    See also `vm_merge_need_free_disk_space` metrics, which are set to values higher than 0    if background merge cannot be initiated due to free disk space shortage. The value shows the number of per-month partitions,    which would start background merge if they had more free disk space.    * VictoriaMetrics buffers incoming data in memory for up to a few seconds before flushing it to persistent storage.    This may lead to the following ""issues"":    * Data becomes available for querying in a few seconds after inserting. It is possible to flush in-memory buffers to persistent storage      by requesting `/internal/force_flush` http handler. This handler is mostly needed for testing and debugging purposes.    * The last few seconds of inserted data may be lost on unclean shutdown (i.e. OOM, `kill -9` or hardware reset).      See [this article for technical details](https://valyala.medium.com/wal-usage-looks-broken-in-modern-time-series-databases-b62a627ab704).    * If VictoriaMetrics works slowly and eats more than a CPU core per 100K ingested data points per second,    then it is likely you have too many [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series) for the current amount of RAM.    VictoriaMetrics [exposes](#monitoring) `vm_slow_*` metrics such as `vm_slow_row_inserts_total` and `vm_slow_metric_name_loads_total`, which could be used    as an indicator of low amounts of RAM. It is recommended increasing the amount of RAM on the node with VictoriaMetrics in order to improve    ingestion and query performance in this case.    * If the order of labels for the same metrics can change over time (e.g. if `metric{k1=""v1"",k2=""v2""}` may become `metric{k2=""v2"",k1=""v1""}`),    then it is recommended running VictoriaMetrics with `-sortLabels` command-line flag in order to reduce memory usage and CPU usage.    * VictoriaMetrics prioritizes data ingestion over data querying. So if it has no enough resources for data ingestion,    then data querying may slow down significantly.    * If VictoriaMetrics doesn't work because of certain parts are corrupted due to disk errors,    then just remove directories with broken parts. It is safe removing subdirectories under `<-storageDataPath>/data/{big,small}/YYYY_MM` directories    when VictoriaMetrics isn't running. This recovers VictoriaMetrics at the cost of data loss stored in the deleted broken parts.    In the future, `vmrecover` tool will be created for automatic recovering from such errors.    * If you see gaps on the graphs, try resetting the cache by sending request to `/internal/resetRollupResultCache`.    If this removes gaps on the graphs, then it is likely data with timestamps older than `-search.cacheTimestampOffset`    is ingested into VictoriaMetrics. Make sure that data sources have synchronized time with VictoriaMetrics.      If the gaps are related to irregular intervals between samples, then try adjusting `-search.minStalenessInterval` command-line flag    to value close to the maximum interval between samples.    * If you are switching from InfluxDB or TimescaleDB, then take a look at `-search.maxStalenessInterval` command-line flag.    It may be needed in order to suppress default gap filling algorithm used by VictoriaMetrics - by default it assumes    each time series is continuous instead of discrete, so it fills gaps between real samples with regular intervals.    * Metrics and labels leading to [high cardinality](https://docs.victoriametrics.com/FAQ.html#what-is-high-cardinality) or [high churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate) can be determined at `/api/v1/status/tsdb` page. See [these docs](#tsdb-stats) for details.    * New time series can be logged if `-logNewSeries` command-line flag is passed to VictoriaMetrics.    * VictoriaMetrics limits the number of labels per each metric with `-maxLabelsPerTimeseries` command-line flag.    This prevents from ingesting metrics with too many labels. It is recommended [monitoring](#monitoring) `vm_metrics_with_dropped_labels_total`    metric in order to determine whether `-maxLabelsPerTimeseries` must be adjusted for your workload.    * If you store Graphite metrics like `foo.bar.baz` in VictoriaMetrics, then `{__graphite__=""foo.*.baz""}` filter can be used for selecting such metrics. See [these docs](#selecting-graphite-metrics) for details.    * VictoriaMetrics ignores `NaN` values during data ingestion.    ## Cache removal    VictoriaMetrics uses various internal caches. These caches are stored to `<-storageDataPath>/cache` directory during graceful shutdown (e.g. when VictoriaMetrics is stopped by sending `SIGINT` signal). The caches are read on the next VictoriaMetrics startup. Sometimes it is needed to remove such caches on the next startup. This can be performed by placing `reset_cache_on_startup` file inside the `<-storageDataPath>/cache` directory before the restart of VictoriaMetrics. See [this issue](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/1447) for details.    ## Cache tuning    VictoriaMetrics uses various in-memory caches for faster data ingestion and query performance.  The following metrics for each type of cache are exported at [`/metrics` page](#monitoring):  * `vm_cache_size_bytes` - the actual cache size  * `vm_cache_size_max_bytes` - cache size limit  * `vm_cache_requests_total` - the number of requests to the cache  * `vm_cache_misses_total` - the number of cache misses  * `vm_cache_entries` - the number of entries in the cache    Both Grafana dashboards for [single-node VictoriaMetrics](https://grafana.com/dashboards/10229)  and [clustered VictoriaMetrics](https://grafana.com/grafana/dashboards/11176)  contain `Caches` section with cache metrics visualized. The panels show the current  memory usage by each type of cache, and also a cache hit rate. If hit rate is close to 100%  then cache efficiency is already very high and does not need any tuning.  The panel `Cache usage %` in `Troubleshooting` section shows the percentage of used cache size  from the allowed size by type. If the percentage is below 100%, then no further tuning needed.    Please note, default cache sizes were carefully adjusted accordingly to the most  practical scenarios and workloads. Change the defaults only if you understand the implications.    To override the default values see command-line flags with `-storage.cacheSize` prefix.  See the full description of flags [here](#list-of-command-line-flags).    ## Data migration    ### From VictoriaMetrics    The simplest way to migrate data from one single-node (source) to another (destination), or from one vmstorage node  to another do the following:    1. Stop the VictoriaMetrics (source) with `kill -INT`;  2. Copy (via [rsync](https://en.wikipedia.org/wiki/Rsync) or any other tool) the entire folder specified  via `-storageDataPath` from the source node to the empty folder at the destination node.  3. Once copy is done, stop the VictoriaMetrics (destination) with `kill -INT` and verify that  its `-storageDataPath` points to the copied folder from p.2;  4. Start the VictoriaMetrics (destination). The copied data should be now available.    Things to consider when copying data:    1. Data formats between single-node and vmstorage node aren't compatible and can't be copied.  2. Copying data folder means complete replacement of the previous data on destination VictoriaMetrics.    For more complex scenarios like single-to-cluster, cluster-to-single, re-sharding or migrating only a fraction  of data - see [vmctl. Migrating data from VictoriaMetrics](https://docs.victoriametrics.com/vmctl.html#migrating-data-from-victoriametrics).    ### From other systems    Use [vmctl](https://docs.victoriametrics.com/vmctl.html) for data migration. It supports the following data migration types:    * From Prometheus to VictoriaMetrics  * From InfluxDB to VictoriaMetrics  * From VictoriaMetrics to VictoriaMetrics  * From OpenTSDB to VictoriaMetrics    See [vmctl docs](https://docs.victoriametrics.com/vmctl.html) for more details.    ## Backfilling    VictoriaMetrics accepts historical data in arbitrary order of time via [any supported ingestion method](#how-to-import-time-series-data).  See [how to backfill data with recording rules in vmalert](https://docs.victoriametrics.com/vmalert.html#rules-backfilling).  Make sure that configured `-retentionPeriod` covers timestamps for the backfilled data.    It is recommended disabling query cache with `-search.disableCache` command-line flag when writing  historical data with timestamps from the past, since the cache assumes that the data is written with  the current timestamps. Query cache can be enabled after the backfilling is complete.    An alternative solution is to query `/internal/resetRollupResultCache` url after backfilling is complete. This will reset  the query cache, which could contain incomplete data cached during the backfilling.    Yet another solution is to increase `-search.cacheTimestampOffset` flag value in order to disable caching  for data with timestamps close to the current time. Single-node VictoriaMetrics automatically resets response  cache when samples with timestamps older than `now - search.cacheTimestampOffset` are ingested to it.    ## Data updates    VictoriaMetrics doesn't support updating already existing sample values to new ones. It stores all the ingested data points  for the same time series with identical timestamps. While it is possible substituting old time series with new time series via  [removal of old time series](#how-to-delete-time-series) and then [writing new time series](#backfilling), this approach  should be used only for one-off updates. It shouldn't be used for frequent updates because of non-zero overhead related to data removal.    ## Replication    Single-node VictoriaMetrics doesn't support application-level replication. Use cluster version instead.  See [these docs](https://docs.victoriametrics.com/Cluster-VictoriaMetrics.html#replication-and-data-safety) for details.    Storage-level replication may be offloaded to durable persistent storage such as [Google Cloud disks](https://cloud.google.com/compute/docs/disks#pdspecs).    See also [high availability docs](#high-availability) and [backup docs](#backups).    ## Backups    VictoriaMetrics supports backups via [vmbackup](https://docs.victoriametrics.com/vmbackup.html)  and [vmrestore](https://docs.victoriametrics.com/vmrestore.html) tools.  We also provide [vmbackupmanager](https://docs.victoriametrics.com/vmbackupmanager.html) tool for enterprise subscribers.  Enterprise binaries can be downloaded and evaluated for free from [the releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases).    ## Benchmarks    Note, that vendors (including VictoriaMetrics) are often biased when doing such tests. E.g. they try highlighting  the best parts of their product, while highlighting the worst parts of competing products.  So we encourage users and all independent third parties to conduct their becnhmarks for various products  they are evaluating in production and publish the results.    As a reference, please see [benchmarks](https://docs.victoriametrics.com/Articles.html#benchmarks) conducted by  VictoriaMetrics team. Please also see the [helm chart](https://github.com/VictoriaMetrics/benchmark)  for running ingestion benchmarks based on node_exporter metrics.    ## Profiling    VictoriaMetrics provides handlers for collecting the following [Go profiles](https://blog.golang.org/profiling-go-programs):    * Memory profile. It can be collected with the following command (replace `0.0.0.0` with hostname if needed):    <div class=""with-copy"" markdown=""1"">    ```bash  curl http://0.0.0.0:8428/debug/pprof/heap > mem.pprof  ```    </div>    * CPU profile. It can be collected with the following command (replace `0.0.0.0` with hostname if needed):    <div class=""with-copy"" markdown=""1"">    ```bash  curl http://0.0.0.0:8428/debug/pprof/profile > cpu.pprof  ```    </div>    The command for collecting CPU profile waits for 30 seconds before returning.    The collected profiles may be analyzed with [go tool pprof](https://github.com/google/pprof).    ## Integrations    * [Helm charts for single-node and cluster versions of VictoriaMetrics](https://github.com/VictoriaMetrics/helm-charts).  * [Kubernetes operator for VictoriaMetrics](https://github.com/VictoriaMetrics/operator).  * [netdata](https://github.com/netdata/netdata) can push data into VictoriaMetrics via `Prometheus remote_write API`.    See [these docs](https://github.com/netdata/netdata#integrations).  * [go-graphite/carbonapi](https://github.com/go-graphite/carbonapi) can use VictoriaMetrics as time series backend.    See [this example](https://github.com/go-graphite/carbonapi/blob/main/cmd/carbonapi/carbonapi.example.victoriametrics.yaml).  * [Ansible role for installing single-node VictoriaMetrics](https://github.com/dreamteam-gg/ansible-victoriametrics-role).  * [Ansible role for installing cluster VictoriaMetrics](https://github.com/Slapper/ansible-victoriametrics-cluster-role).  * [Snap package for VictoriaMetrics](https://snapcraft.io/victoriametrics).  * [vmalert-cli](https://github.com/aorfanos/vmalert-cli) - a CLI application for managing [vmalert](https://docs.victoriametrics.com/vmalert.html).    ## Third-party contributions    * [Unofficial yum repository](https://copr.fedorainfracloud.org/coprs/antonpatsev/VictoriaMetrics/) ([source code](https://github.com/patsevanton/victoriametrics-rpm))  * [Prometheus -> VictoriaMetrics exporter #1](https://github.com/ryotarai/prometheus-tsdb-dump)  * [Prometheus -> VictoriaMetrics exporter #2](https://github.com/AnchorFree/tsdb-remote-write)  * [Prometheus Oauth proxy](https://gitlab.com/optima_public/prometheus_oauth_proxy) - see [this article](https://medium.com/@richard.holly/powerful-saas-solution-for-detection-metrics-c67b9208d362) for details.    ## Contacts    Contact us with any questions regarding VictoriaMetrics at [info@victoriametrics.com](mailto:info@victoriametrics.com).    ## Community and contributions    Feel free asking any questions regarding VictoriaMetrics:    * [slack](https://slack.victoriametrics.com/)  * [linkedin](https://www.linkedin.com/company/victoriametrics/)  * [reddit](https://www.reddit.com/r/VictoriaMetrics/)  * [telegram-en](https://t.me/VictoriaMetrics_en)  * [telegram-ru](https://t.me/VictoriaMetrics_ru1)  * [articles and talks about VictoriaMetrics in Russian](https://github.com/denisgolius/victoriametrics-ru-links)  * [google groups](https://groups.google.com/forum/#!forum/victorametrics-users)    If you like VictoriaMetrics and want to contribute, then we need the following:    * Filing issues and feature requests [here](https://github.com/VictoriaMetrics/VictoriaMetrics/issues).  * Spreading a word about VictoriaMetrics: conference talks, articles, comments, experience sharing with colleagues.  * Updating documentation.    We are open to third-party pull requests provided they follow [KISS design principle](https://en.wikipedia.org/wiki/KISS_principle):    * Prefer simple code and architecture.  * Avoid complex abstractions.  * Avoid magic code and fancy algorithms.  * Avoid [big external dependencies](https://medium.com/@valyala/stripping-dependency-bloat-in-victoriametrics-docker-image-983fb5912b0d).  * Minimize the number of moving parts in the distributed system.  * Avoid automated decisions, which may hurt cluster availability, consistency or performance.    Adhering `KISS` principle simplifies the resulting code and architecture, so it can be reviewed, understood and verified by many people.    ## Reporting bugs    Report bugs and propose new features [here](https://github.com/VictoriaMetrics/VictoriaMetrics/issues).    ## VictoriaMetrics Logo    [Zip](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/VM_logo.zip) contains three folders with different image orientations (main color and inverted version).    Files included in each folder:    * 2 JPEG Preview files  * 2 PNG Preview files with transparent background  * 2 EPS Adobe Illustrator EPS10 files    ### Logo Usage Guidelines    #### Font used    * Lato Black  * Lato Regular    #### Color Palette    * HEX [#110f0f](https://www.color-hex.com/color/110f0f)  * HEX [#ffffff](https://www.color-hex.com/color/ffffff)    ### We kindly ask    * Please don't use any other font instead of suggested.  * There should be sufficient clear space around the logo.  * Do not change spacing, alignment, or relative locations of the design elements.  * Do not change the proportions of any of the design elements or the design itself. You    may resize as needed but must retain all proportions.    ## List of command-line flags    Pass `-help` to VictoriaMetrics in order to see the list of supported command-line flags with their description:    ```    -bigMergeConcurrency int       The maximum number of CPU cores to use for big merges. Default value is used if set to 0    -configAuthKey string       Authorization key for accessing /config page. It must be passed via authKey query arg    -csvTrimTimestamp duration       Trim timestamps when importing csv data to this duration. Minimum practical duration is 1ms. Higher duration (i.e. 1s) may be used for reducing disk space usage for timestamp data (default 1ms)    -datadog.maxInsertRequestSize size       The maximum size in bytes of a single DataDog POST request to /api/v1/series       Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 67108864)    -dedup.minScrapeInterval duration       Leave only the first sample in every time series per each discrete interval equal to -dedup.minScrapeInterval > 0. See https://docs.victoriametrics.com/#deduplication and https://docs.victoriametrics.com/#downsampling    -deleteAuthKey string       authKey for metrics' deletion via /api/v1/admin/tsdb/delete_series and /tags/delSeries    -denyQueriesOutsideRetention       Whether to deny queries outside of the configured -retentionPeriod. When set, then /api/v1/query_range would return '503 Service Unavailable' error for queries with 'from' value outside -retentionPeriod. This may be useful when multiple data sources with distinct retentions are hidden behind query-tee    -downsampling.period array       Comma-separated downsampling periods in the format 'offset:period'. For example, '30d:10m' instructs to leave a single sample per 10 minutes for samples older than 30 days. See https://docs.victoriametrics.com/#downsampling for details       Supports an array of values separated by comma or specified via multiple flags.    -dryRun       Whether to check only -promscrape.config and then exit. Unknown config entries aren't allowed in -promscrape.config by default. This can be changed with -promscrape.config.strictParse=false command-line flag    -enableTCP6       Whether to enable IPv6 for listening and dialing. By default only IPv4 TCP and UDP is used    -envflag.enable       Whether to enable reading flags from environment variables additionally to command line. Command line flag values have priority over values from environment vars. Flags are read only from command line if this flag isn't set. See https://docs.victoriametrics.com/#environment-variables for more details    -envflag.prefix string       Prefix for environment variables if -envflag.enable is set    -eula       By specifying this flag, you confirm that you have an enterprise license and accept the EULA https://victoriametrics.com/assets/VM_EULA.pdf    -finalMergeDelay duration       The delay before starting final merge for per-month partition after no new data is ingested into it. Final merge may require additional disk IO and CPU resources. Final merge may increase query speed and reduce disk space usage in some cases. Zero value disables final merge    -forceFlushAuthKey string       authKey, which must be passed in query string to /internal/force_flush pages    -forceMergeAuthKey string       authKey, which must be passed in query string to /internal/force_merge pages    -fs.disableMmap       Whether to use pread() instead of mmap() for reading data files. By default mmap() is used for 64-bit arches and pread() is used for 32-bit arches, since they cannot read data files bigger than 2^32 bytes in memory. mmap() is usually faster for reading small data chunks than pread()    -graphiteListenAddr string       TCP and UDP address to listen for Graphite plaintext data. Usually :2003 must be set. Doesn't work if empty    -graphiteTrimTimestamp duration       Trim timestamps for Graphite data to this duration. Minimum practical duration is 1s. Higher duration (i.e. 1m) may be used for reducing disk space usage for timestamp data (default 1s)    -http.connTimeout duration       Incoming http connections are closed after the configured timeout. This may help to spread the incoming load among a cluster of services behind a load balancer. Please note that the real timeout may be bigger by up to 10% as a protection against the thundering herd problem (default 2m0s)    -http.disableResponseCompression       Disable compression of HTTP responses to save CPU resources. By default compression is enabled to save network bandwidth    -http.idleConnTimeout duration       Timeout for incoming idle http connections (default 1m0s)    -http.maxGracefulShutdownDuration duration       The maximum duration for a graceful shutdown of the HTTP server. A highly loaded server may require increased value for a graceful shutdown (default 7s)    -http.pathPrefix string       An optional prefix to add to all the paths handled by http server. For example, if '-http.pathPrefix=/foo/bar' is set, then all the http requests will be handled on '/foo/bar/*' paths. This may be useful for proxied requests. See https://www.robustperception.io/using-external-urls-and-proxies-with-prometheus    -http.shutdownDelay duration       Optional delay before http server shutdown. During this delay, the server returns non-OK responses from /health page, so load balancers can route new requests to other servers    -httpAuth.password string       Password for HTTP Basic Auth. The authentication is disabled if -httpAuth.username is empty    -httpAuth.username string       Username for HTTP Basic Auth. The authentication is disabled if empty. See also -httpAuth.password    -httpListenAddr string       TCP address to listen for http connections (default "":8428"")    -import.maxLineLen size       The maximum length in bytes of a single line accepted by /api/v1/import; the line length can be limited with 'max_rows_per_line' query arg passed to /api/v1/export       Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 104857600)    -influx.databaseNames array       Comma-separated list of database names to return from /query and /influx/query API. This can be needed for accepting data from Telegraf plugins such as https://github.com/fangli/fluent-plugin-influxdb       Supports an array of values separated by comma or specified via multiple flags.    -influx.maxLineSize size       The maximum size in bytes for a single InfluxDB line during parsing       Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 262144)    -influxDBLabel string       Default label for the DB name sent over '?db={db_name}' query parameter (default ""db"")    -influxListenAddr string       TCP and UDP address to listen for InfluxDB line protocol data. Usually :8189 must be set. Doesn't work if empty. This flag isn't needed when ingesting data over HTTP - just send it to http://<victoriametrics>:8428/write    -influxMeasurementFieldSeparator string       Separator for '{measurement}{separator}{field_name}' metric name when inserted via InfluxDB line protocol (default ""_"")    -influxSkipMeasurement       Uses '{field_name}' as a metric name while ignoring '{measurement}' and '-influxMeasurementFieldSeparator'    -influxSkipSingleField       Uses '{measurement}' instead of '{measurement}{separator}{field_name}' for metic name if InfluxDB line contains only a single field    -influxTrimTimestamp duration       Trim timestamps for InfluxDB line protocol data to this duration. Minimum practical duration is 1ms. Higher duration (i.e. 1s) may be used for reducing disk space usage for timestamp data (default 1ms)    -insert.maxQueueDuration duration       The maximum duration for waiting in the queue for insert requests due to -maxConcurrentInserts (default 1m0s)    -logNewSeries       Whether to log new series. This option is for debug purposes only. It can lead to performance issues when big number of new series are ingested into VictoriaMetrics    -loggerDisableTimestamps       Whether to disable writing timestamps in logs    -loggerErrorsPerSecondLimit int       Per-second limit on the number of ERROR messages. If more than the given number of errors are emitted per second, the remaining errors are suppressed. Zero values disable the rate limit    -loggerFormat string       Format for logs. Possible values: default, json (default ""default"")    -loggerLevel string       Minimum level of errors to log. Possible values: INFO, WARN, ERROR, FATAL, PANIC (default ""INFO"")    -loggerOutput string       Output for the logs. Supported values: stderr, stdout (default ""stderr"")    -loggerTimezone string       Timezone to use for timestamps in logs. Timezone must be a valid IANA Time Zone. For example: America/New_York, Europe/Berlin, Etc/GMT+3 or Local (default ""UTC"")    -loggerWarnsPerSecondLimit int       Per-second limit on the number of WARN messages. If more than the given number of warns are emitted per second, then the remaining warns are suppressed. Zero values disable the rate limit    -maxConcurrentInserts int       The maximum number of concurrent inserts. Default value should work for most cases, since it minimizes the overhead for concurrent inserts. This option is tigthly coupled with -insert.maxQueueDuration (default 16)    -maxInsertRequestSize size       The maximum size in bytes of a single Prometheus remote_write API request       Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 33554432)    -maxLabelValueLen int       The maximum length of label values in the accepted time series. Longer label values are truncated. In this case the vm_too_long_label_values_total metric at /metrics page is incremented (default 16384)    -maxLabelsPerTimeseries int       The maximum number of labels accepted per time series. Superfluous labels are dropped. In this case the vm_metrics_with_dropped_labels_total metric at /metrics page is incremented (default 30)    -memory.allowedBytes size       Allowed size of system memory VictoriaMetrics caches may occupy. This option overrides -memory.allowedPercent if set to a non-zero value. Too low a value may increase the cache miss rate usually resulting in higher CPU and disk IO usage. Too high a value may evict too much data from OS page cache resulting in higher disk IO usage       Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 0)    -memory.allowedPercent float       Allowed percent of system memory VictoriaMetrics caches may occupy. See also -memory.allowedBytes. Too low a value may increase cache miss rate usually resulting in higher CPU and disk IO usage. Too high a value may evict too much data from OS page cache which will result in higher disk IO usage (default 60)    -metricsAuthKey string       Auth key for /metrics. It must be passed via authKey query arg. It overrides httpAuth.* settings    -opentsdbHTTPListenAddr string       TCP address to listen for OpentTSDB HTTP put requests. Usually :4242 must be set. Doesn't work if empty    -opentsdbListenAddr string       TCP and UDP address to listen for OpentTSDB metrics. Telnet put messages and HTTP /api/put messages are simultaneously served on TCP port. Usually :4242 must be set. Doesn't work if empty    -opentsdbTrimTimestamp duration       Trim timestamps for OpenTSDB 'telnet put' data to this duration. Minimum practical duration is 1s. Higher duration (i.e. 1m) may be used for reducing disk space usage for timestamp data (default 1s)    -opentsdbhttp.maxInsertRequestSize size       The maximum size of OpenTSDB HTTP put request       Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 33554432)    -opentsdbhttpTrimTimestamp duration       Trim timestamps for OpenTSDB HTTP data to this duration. Minimum practical duration is 1ms. Higher duration (i.e. 1s) may be used for reducing disk space usage for timestamp data (default 1ms)    -pprofAuthKey string       Auth key for /debug/pprof. It must be passed via authKey query arg. It overrides httpAuth.* settings    -precisionBits int       The number of precision bits to store per each value. Lower precision bits improves data compression at the cost of precision loss (default 64)    -promscrape.cluster.memberNum string       The number of number in the cluster of scrapers. It must be an unique value in the range 0 ... promscrape.cluster.membersCount-1 across scrapers in the cluster. Can be specified as pod name of Kubernetes StatefulSet - pod-name-Num, where Num is a numeric part of pod name (default ""0"")    -promscrape.cluster.membersCount int       The number of members in a cluster of scrapers. Each member must have an unique -promscrape.cluster.memberNum in the range 0 ... promscrape.cluster.membersCount-1 . Each member then scrapes roughly 1/N of all the targets. By default cluster scraping is disabled, i.e. a single scraper scrapes all the targets    -promscrape.cluster.replicationFactor int       The number of members in the cluster, which scrape the same targets. If the replication factor is greater than 2, then the deduplication must be enabled at remote storage side. See https://docs.victoriametrics.com/#deduplication (default 1)    -promscrape.config string       Optional path to Prometheus config file with 'scrape_configs' section containing targets to scrape. The path can point to local file and to http url. See https://docs.victoriametrics.com/#how-to-scrape-prometheus-exporters-such-as-node-exporter for details    -promscrape.config.dryRun       Checks -promscrape.config file for errors and unsupported fields and then exits. Returns non-zero exit code on parsing errors and emits these errors to stderr. See also -promscrape.config.strictParse command-line flag. Pass -loggerLevel=ERROR if you don't need to see info messages in the output.    -promscrape.config.strictParse       Whether to deny unsupported fields in -promscrape.config . Set to false in order to silently skip unsupported fields (default true)    -promscrape.configCheckInterval duration       Interval for checking for changes in '-promscrape.config' file. By default the checking is disabled. Send SIGHUP signal in order to force config check for changes    -promscrape.consul.waitTime duration       Wait time used by Consul service discovery. Default value is used if not set    -promscrape.consulSDCheckInterval duration       Interval for checking for changes in Consul. This works only if consul_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config for details (default 30s)    -promscrape.digitaloceanSDCheckInterval duration       Interval for checking for changes in digital ocean. This works only if digitalocean_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#digitalocean_sd_config for details (default 1m0s)    -promscrape.disableCompression       Whether to disable sending 'Accept-Encoding: gzip' request headers to all the scrape targets. This may reduce CPU usage on scrape targets at the cost of higher network bandwidth utilization. It is possible to set 'disable_compression: true' individually per each 'scrape_config' section in '-promscrape.config' for fine grained control    -promscrape.disableKeepAlive       Whether to disable HTTP keep-alive connections when scraping all the targets. This may be useful when targets has no support for HTTP keep-alive connection. It is possible to set 'disable_keepalive: true' individually per each 'scrape_config' section in '-promscrape.config' for fine grained control. Note that disabling HTTP keep-alive may increase load on both vmagent and scrape targets    -promscrape.discovery.concurrency int       The maximum number of concurrent requests to Prometheus autodiscovery API (Consul, Kubernetes, etc.) (default 100)    -promscrape.discovery.concurrentWaitTime duration       The maximum duration for waiting to perform API requests if more than -promscrape.discovery.concurrency requests are simultaneously performed (default 1m0s)    -promscrape.dnsSDCheckInterval duration       Interval for checking for changes in dns. This works only if dns_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dns_sd_config for details (default 30s)    -promscrape.dockerSDCheckInterval duration       Interval for checking for changes in docker. This works only if docker_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#docker_sd_config for details (default 30s)    -promscrape.dockerswarmSDCheckInterval duration       Interval for checking for changes in dockerswarm. This works only if dockerswarm_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dockerswarm_sd_config for details (default 30s)    -promscrape.dropOriginalLabels       Whether to drop original labels for scrape targets at /targets and /api/v1/targets pages. This may be needed for reducing memory usage when original labels for big number of scrape targets occupy big amounts of memory. Note that this reduces debuggability for improper per-target relabeling configs    -promscrape.ec2SDCheckInterval duration       Interval for checking for changes in ec2. This works only if ec2_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config for details (default 1m0s)    -promscrape.eurekaSDCheckInterval duration       Interval for checking for changes in eureka. This works only if eureka_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#eureka_sd_config for details (default 30s)    -promscrape.fileSDCheckInterval duration       Interval for checking for changes in 'file_sd_config'. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#file_sd_config for details (default 5m0s)    -promscrape.gceSDCheckInterval duration       Interval for checking for changes in gce. This works only if gce_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#gce_sd_config for details (default 1m0s)    -promscrape.httpSDCheckInterval duration       Interval for checking for changes in http endpoint service discovery. This works only if http_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#http_sd_config for details (default 1m0s)    -promscrape.kubernetes.apiServerTimeout duration       How frequently to reload the full state from Kuberntes API server (default 30m0s)    -promscrape.kubernetesSDCheckInterval duration       Interval for checking for changes in Kubernetes API server. This works only if kubernetes_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config for details (default 30s)    -promscrape.maxDroppedTargets int       The maximum number of droppedTargets to show at /api/v1/targets page. Increase this value if your setup drops more scrape targets during relabeling and you need investigating labels for all the dropped targets. Note that the increased number of tracked dropped targets may result in increased memory usage (default 1000)    -promscrape.maxResponseHeadersSize size       The maximum size of http response headers from Prometheus scrape targets       Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 4096)    -promscrape.maxScrapeSize size       The maximum size of scrape response in bytes to process from Prometheus targets. Bigger responses are rejected       Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 16777216)    -promscrape.minResponseSizeForStreamParse size       The minimum target response size for automatic switching to stream parsing mode, which can reduce memory usage. See https://docs.victoriametrics.com/vmagent.html#stream-parsing-mode       Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 1000000)    -promscrape.noStaleMarkers       Whether to disable sending Prometheus stale markers for metrics when scrape target disappears. This option may reduce memory usage if stale markers aren't needed for your setup. This option also disables populating the scrape_series_added metric. See https://prometheus.io/docs/concepts/jobs_instances/#automatically-generated-labels-and-time-series    -promscrape.openstackSDCheckInterval duration       Interval for checking for changes in openstack API server. This works only if openstack_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#openstack_sd_config for details (default 30s)    -promscrape.seriesLimitPerTarget int       Optional limit on the number of unique time series a single scrape target can expose. See https://docs.victoriametrics.com/vmagent.html#cardinality-limiter for more info    -promscrape.streamParse       Whether to enable stream parsing for metrics obtained from scrape targets. This may be useful for reducing memory usage when millions of metrics are exposed per each scrape target. It is posible to set 'stream_parse: true' individually per each 'scrape_config' section in '-promscrape.config' for fine grained control    -promscrape.suppressDuplicateScrapeTargetErrors       Whether to suppress 'duplicate scrape target' errors; see https://docs.victoriametrics.com/vmagent.html#troubleshooting for details    -promscrape.suppressScrapeErrors       Whether to suppress scrape errors logging. The last error for each target is always available at '/targets' page even if scrape errors logging is suppressed    -relabelConfig string       Optional path to a file with relabeling rules, which are applied to all the ingested metrics. The path can point either to local file or to http url. See https://docs.victoriametrics.com/#relabeling for details. The config is reloaded on SIGHUP signal    -relabelDebug       Whether to log metrics before and after relabeling with -relabelConfig. If the -relabelDebug is enabled, then the metrics aren't sent to storage. This is useful for debugging the relabeling configs    -retentionPeriod value       Data with timestamps outside the retentionPeriod is automatically deleted       The following optional suffixes are supported: h (hour), d (day), w (week), y (year). If suffix isn't set, then the duration is counted in months (default 1)    -search.cacheTimestampOffset duration       The maximum duration since the current time for response data, which is always queried from the original raw data, without using the response cache. Increase this value if you see gaps in responses due to time synchronization issues between VictoriaMetrics and data sources. See also -search.disableAutoCacheReset (default 5m0s)    -search.disableAutoCacheReset       Whether to disable automatic response cache reset if a sample with timestamp outside -search.cacheTimestampOffset is inserted into VictoriaMetrics    -search.disableCache       Whether to disable response caching. This may be useful during data backfilling    -search.graphiteMaxPointsPerSeries int       The maximum number of points per series Graphite render API can return (default 1000000)    -search.graphiteStorageStep duration       The interval between datapoints stored in the database. It is used at Graphite Render API handler for normalizing the interval between datapoints in case it isn't normalized. It can be overriden by sending 'storage_step' query arg to /render API or by sending the desired interval via 'Storage-Step' http header during querying /render API (default 10s)    -search.latencyOffset duration       The time when data points become visible in query results after the collection. Too small value can result in incomplete last points for query results (default 30s)    -search.logSlowQueryDuration duration       Log queries with execution time exceeding this value. Zero disables slow query logging (default 5s)    -search.maxConcurrentRequests int       The maximum number of concurrent search requests. It shouldn't be high, since a single request can saturate all the CPU cores. See also -search.maxQueueDuration (default 8)    -search.maxExportDuration duration       The maximum duration for /api/v1/export call (default 720h0m0s)    -search.maxExportSeries int       The maximum number of time series, which can be returned from /api/v1/export* APIs. This option allows limiting memory usage (default 1000000)    -search.maxFederateSeries int       The maximum number of time series, which can be returned from /federate. This option allows limiting memory usage (default 300000)    -search.maxGraphiteSeries int       The maximum number of time series, which can be scanned during queries to Graphite Render API. See https://docs.victoriametrics.com/#graphite-render-api-usage (default 300000)    -search.maxLookback duration       Synonym to -search.lookback-delta from Prometheus. The value is dynamically detected from interval between time series datapoints if not set. It can be overridden on per-query basis via max_lookback arg. See also '-search.maxStalenessInterval' flag, which has the same meaining due to historical reasons    -search.maxPointsPerTimeseries int       The maximum points per a single timeseries returned from /api/v1/query_range. This option doesn't limit the number of scanned raw samples in the database. The main purpose of this option is to limit the number of per-series points returned to graphing UI such as Grafana. There is no sense in setting this limit to values bigger than the horizontal resolution of the graph (default 30000)    -search.maxQueryDuration duration       The maximum duration for query execution (default 30s)    -search.maxQueryLen size       The maximum search query length in bytes       Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 16384)    -search.maxQueueDuration duration       The maximum time the request waits for execution when -search.maxConcurrentRequests limit is reached; see also -search.maxQueryDuration (default 10s)    -search.maxSamplesPerQuery int       The maximum number of raw samples a single query can process across all time series. This protects from heavy queries, which select unexpectedly high number of raw samples. See also -search.maxSamplesPerSeries (default 1000000000)    -search.maxSamplesPerSeries int       The maximum number of raw samples a single query can scan per each time series. This option allows limiting memory usage (default 30000000)    -search.maxSeries int       The maximum number of time series, which can be returned from /api/v1/series. This option allows limiting memory usage (default 10000)    -search.maxStalenessInterval duration       The maximum interval for staleness calculations. By default it is automatically calculated from the median interval between samples. This flag could be useful for tuning Prometheus data model closer to Influx-style data model. See https://prometheus.io/docs/prometheus/latest/querying/basics/#staleness for details. See also '-search.maxLookback' flag, which has the same meaning due to historical reasons    -search.maxStatusRequestDuration duration       The maximum duration for /api/v1/status/* requests (default 5m0s)    -search.maxStepForPointsAdjustment duration       The maximum step when /api/v1/query_range handler adjusts points with timestamps closer than -search.latencyOffset to the current time. The adjustment is needed because such points may contain incomplete data (default 1m0s)    -search.maxTSDBStatusSeries int       The maximum number of time series, which can be processed during the call to /api/v1/status/tsdb. This option allows limiting memory usage (default 1000000)    -search.maxTagKeys int       The maximum number of tag keys returned from /api/v1/labels (default 100000)    -search.maxTagValueSuffixesPerSearch int       The maximum number of tag value suffixes returned from /metrics/find (default 100000)    -search.maxTagValues int       The maximum number of tag values returned from /api/v1/label/<label_name>/values (default 100000)    -search.maxUniqueTimeseries int       The maximum number of unique time series, which can be selected during /api/v1/query and /api/v1/query_range queries. This option allows limiting memory usage (default 300000)    -search.minStalenessInterval duration       The minimum interval for staleness calculations. This flag could be useful for removing gaps on graphs generated from time series with irregular intervals between samples. See also '-search.maxStalenessInterval'    -search.noStaleMarkers       Set this flag to true if the database doesn't contain Prometheus stale markers, so there is no need in spending additional CPU time on its handling. Staleness markers may exist only in data obtained from Prometheus scrape targets    -search.queryStats.lastQueriesCount int       Query stats for /api/v1/status/top_queries is tracked on this number of last queries. Zero value disables query stats tracking (default 20000)    -search.queryStats.minQueryDuration duration       The minimum duration for queries to track in query stats at /api/v1/status/top_queries. Queries with lower duration are ignored in query stats (default 1ms)    -search.resetCacheAuthKey string       Optional authKey for resetting rollup cache via /internal/resetRollupResultCache call    -search.treatDotsAsIsInRegexps       Whether to treat dots as is in regexp label filters used in queries. For example, foo{bar=~""a.b.c""} will be automatically converted to foo{bar=~""a\\.b\\.c""}, i.e. all the dots in regexp filters will be automatically escaped in order to match only dot char instead of matching any char. Dots in "".+"", "".*"" and "".{n}"" regexps aren't escaped. This option is DEPRECATED in favor of {__graphite__=""a.*.c""} syntax for selecting metrics matching the given Graphite metrics filter    -selfScrapeInstance string       Value for 'instance' label, which is added to self-scraped metrics (default ""self"")    -selfScrapeInterval duration       Interval for self-scraping own metrics at /metrics page    -selfScrapeJob string       Value for 'job' label, which is added to self-scraped metrics (default ""victoria-metrics"")    -smallMergeConcurrency int       The maximum number of CPU cores to use for small merges. Default value is used if set to 0    -snapshotAuthKey string       authKey, which must be passed in query string to /snapshot* pages    -sortLabels       Whether to sort labels for incoming samples before writing them to storage. This may be needed for reducing memory usage at storage when the order of labels in incoming samples is random. For example, if m{k1=""v1"",k2=""v2""} may be sent as m{k2=""v2"",k1=""v1""}. Enabled sorting for labels can slow down ingestion performance a bit    -storage.cacheSizeIndexDBDataBlocks size       Overrides max size for indexdb/dataBlocks cache. See https://docs.victoriametrics.com/Single-server-VictoriaMetrics.html#cache-tuning       Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 0)    -storage.cacheSizeIndexDBIndexBlocks size       Overrides max size for indexdb/indexBlocks cache. See https://docs.victoriametrics.com/Single-server-VictoriaMetrics.html#cache-tuning       Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 0)    -storage.cacheSizeStorageTSID size       Overrides max size for storage/tsid cache. See https://docs.victoriametrics.com/Single-server-VictoriaMetrics.html#cache-tuning       Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 0)    -storage.maxDailySeries int       The maximum number of unique series can be added to the storage during the last 24 hours. Excess series are logged and dropped. This can be useful for limiting series churn rate. See also -storage.maxHourlySeries    -storage.maxHourlySeries int       The maximum number of unique series can be added to the storage during the last hour. Excess series are logged and dropped. This can be useful for limiting series cardinality. See also -storage.maxDailySeries    -storage.minFreeDiskSpaceBytes size       The minimum free disk space at -storageDataPath after which the storage stops accepting new data       Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 10000000)    -storageDataPath string       Path to storage data (default ""victoria-metrics-data"")    -tls       Whether to enable TLS for incoming HTTP requests at -httpListenAddr (aka https). -tlsCertFile and -tlsKeyFile must be set if -tls is set    -tlsCertFile string       Path to file with TLS certificate if -tls is set. Prefer ECDSA certs instead of RSA certs as RSA certs are slower. The provided certificate file is automatically re-read every second, so it can be dynamically updated    -tlsCipherSuites array       Optional list of TLS cipher suites for incoming requests over HTTPS if -tls is set. See the list of supported cipher suites at https://pkg.go.dev/crypto/tls#pkg-constants       Supports an array of values separated by comma or specified via multiple flags.    -tlsKeyFile string       Path to file with TLS key if -tls is set. The provided key file is automatically re-read every second, so it can be dynamically updated    -version       Show VictoriaMetrics version  ``` """
Big data;https://github.com/benedekrozemberczki/pytorch_geometric_temporal;"""[pypi-image]: https://badge.fury.io/py/torch-geometric-temporal.svg  [pypi-url]: https://pypi.python.org/pypi/torch-geometric-temporal  [size-image]: https://img.shields.io/github/repo-size/benedekrozemberczki/pytorch_geometric_temporal.svg  [size-url]: https://github.com/benedekrozemberczki/pytorch_geometric_temporal/archive/master.zip  [build-image]: https://github.com/benedekrozemberczki/pytorch_geometric_temporal/workflows/CI/badge.svg  [build-url]: https://github.com/benedekrozemberczki/pytorch_geometric_temporal/actions?query=workflow%3ACI  [docs-image]: https://readthedocs.org/projects/pytorch-geometric-temporal/badge/?version=latest  [docs-url]: https://pytorch-geometric-temporal.readthedocs.io/en/latest/?badge=latest  [coverage-image]: https://codecov.io/gh/benedekrozemberczki/pytorch_geometric_temporal/branch/master/graph/badge.svg  [coverage-url]: https://codecov.io/github/benedekrozemberczki/pytorch_geometric_temporal?branch=master        <p align=""center"">    <img width=""90%"" src=""https://raw.githubusercontent.com/benedekrozemberczki/pytorch_geometric_temporal/master/docs/source/_static/img/text_logo.jpg?sanitize=true"" />  </p>    -----------------------------------------------------    [![PyPI Version][pypi-image]][pypi-url]  [![Docs Status][docs-image]][docs-url]  [![Code Coverage][coverage-image]][coverage-url]  [![Build Status][build-image]][build-url]  [![Arxiv](https://img.shields.io/badge/ArXiv-2104.07788-orange.svg)](https://arxiv.org/abs/2104.07788)  [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)    **[Documentation](https://pytorch-geometric-temporal.readthedocs.io)** | **[External Resources](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/resources.html)** | **[Datasets](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#discrete-time-datasets)**    *PyTorch Geometric Temporal* is a temporal (dynamic) extension library for [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric).    <p align=""justify"">The library consists of various dynamic and temporal geometric deep learning, embedding, and spatio-temporal regression methods from a variety of published research papers. Moreover, it comes with an easy-to-use dataset loader, train-test splitter and temporal snaphot iterator for dynamic and temporal graphs. The framework naturally provides GPU support. It also comes with a number of benchmark datasets from the epidemological forecasting, sharing economy, energy production and web traffic management domains. Finally, you can also create your own datasets.</p>    The package interfaces well with [Pytorch Lightning](https://pytorch-lightning.readthedocs.io) which allows training on CPUs, single and multiple GPUs out-of-the-box. Take a look at this [introductory example](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/blob/master/examples/recurrent/lightning_example.py) of using PyTorch Geometric Temporal with Pytorch Lighning.    We also provide detailed examples for each of the [recurrent](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/tree/master/examples/recurrent) models and [notebooks](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/tree/master/notebooks) for the attention based ones.      --------------------------------------------------------------------------------    **Case Study Tutorials**    We provide in-depth case study tutorials in theÂ [Documentation](https://pytorch-geometric-temporal.readthedocs.io/en/latest/), each covers an aspect of PyTorch Geometric Temporalâ€™s functionality.    **Incremental Training**:Â [Epidemiological Forecasting Case Study](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#epidemiological-forecasting)    **Cumulative Training**:Â [Web Traffic Management Case Study](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#web-traffic-prediction)    --------------------------------------------------------------------------------    **Citing**      If you find *PyTorch Geometric Temporal* and the new datasets useful in your research, please consider adding the following citation:    ```bibtex  @inproceedings{rozemberczki2021pytorch,                 author = {Benedek Rozemberczki and Paul Scherer and Yixuan He and George Panagopoulos and Alexander Riedel and Maria Astefanoaei and Oliver Kiss and Ferenc Beres and Guzman Lopez and Nicolas Collignon and Rik Sarkar},                 title = {{PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models}},                 year = {2021},                 booktitle={Proceedings of the 30th ACM International Conference on Information and Knowledge Management},                 pages = {4564â€“4573},  }  ```    --------------------------------------------------------------------------------    **A simple example**    PyTorch Geometric Temporal makes implementing Dynamic and Temporal Graph Neural Networks quite easy - see the accompanying [tutorial](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#applications). For example, this is all it takes to implement a recurrent graph convolutional network with two consecutive [graph convolutional GRU](https://arxiv.org/abs/1612.07659) cells and a linear layer:    ```python  import torch  import torch.nn.functional as F  from torch_geometric_temporal.nn.recurrent import GConvGRU    class RecurrentGCN(torch.nn.Module):        def __init__(self, node_features, num_classes):          super(RecurrentGCN, self).__init__()          self.recurrent_1 = GConvGRU(node_features, 32, 5)          self.recurrent_2 = GConvGRU(32, 16, 5)          self.linear = torch.nn.Linear(16, num_classes)        def forward(self, x, edge_index, edge_weight):          x = self.recurrent_1(x, edge_index, edge_weight)          x = F.relu(x)          x = F.dropout(x, training=self.training)          x = self.recurrent_2(x, edge_index, edge_weight)          x = F.relu(x)          x = F.dropout(x, training=self.training)          x = self.linear(x)          return F.log_softmax(x, dim=1)  ```  --------------------------------------------------------------------------------    **Methods Included**    In detail, the following temporal graph neural networks were implemented.      **Recurrent Graph Convolutions**    * **[DCRNN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.dcrnn.DCRNN)** from Li *et al.*: [Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting](https://arxiv.org/abs/1707.01926) (ICLR 2018)    * **[GConvGRU](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.gconv_gru.GConvGRU)** from Seo *et al.*: [Structured Sequence Modeling with Graph  Convolutional Recurrent Networks](https://arxiv.org/abs/1612.07659) (ICONIP 2018)    * **[GConvLSTM](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.gconv_lstm.GConvLSTM)** from Seo *et al.*: [Structured Sequence Modeling with Graph  Convolutional Recurrent Networks](https://arxiv.org/abs/1612.07659) (ICONIP 2018)    * **[GC-LSTM](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.gc_lstm.GCLSTM)** from Chen *et al.*: [GC-LSTM: Graph Convolution Embedded LSTM for Dynamic Link Prediction](https://arxiv.org/abs/1812.04206) (CoRR 2018)    * **[LRGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.lrgcn.LRGCN)** from Li *et al.*: [Predicting Path Failure In Time-Evolving Graphs](https://arxiv.org/abs/1905.03994) (KDD 2019)    * **[DyGrEncoder](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.dygrae.DyGrEncoder)** from Taheri *et al.*: [Learning to Represent the Evolution of Dynamic Graphs with Recurrent Models](https://dl.acm.org/doi/10.1145/3308560.3316581)    * **[EvolveGCNH](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.evolvegcnh.EvolveGCNH)** from Pareja *et al.*: [EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs](https://arxiv.org/abs/1902.10191)    * **[EvolveGCNO](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.evolvegcno.EvolveGCNO)** from Pareja *et al.*: [EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs](https://arxiv.org/abs/1902.10191)    * **[T-GCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.temporalgcn.TGCN)** from Zhao *et al.*: [T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction](https://arxiv.org/abs/1811.05320)    * **[A3T-GCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.attentiontemporalgcn.A3TGCN)** from Zhu *et al.*: [A3T-GCN: Attention Temporal Graph Convolutional Network for Traffic Forecasting](https://arxiv.org/abs/2006.11583)     * **[AGCRN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.agcrn.AGCRN)** from Bai *et al.*: [Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting](https://arxiv.org/abs/2007.02842) (NeurIPS 2020)    * **[MPNN LSTM](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.mpnn_lstm.MPNNLSTM)** from Panagopoulos *et al.*: [Transfer Graph Neural Networks for Pandemic Forecasting](https://arxiv.org/abs/2009.08388) (AAAI 2021)      **Attention Aggregated Temporal Graph Convolutions**    * **[STGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.stgcn.STConv)** from Yu *et al.*: [Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting](https://arxiv.org/abs/1709.04875) (IJCAI 2018)    * **[ASTGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.astgcn.ASTGCN)** from Guo *et al.*: [Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting](https://ojs.aaai.org/index.php/AAAI/article/view/3881) (AAAI 2019)    * **[MSTGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.mstgcn.MSTGCN)** from Guo *et al.*: [Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting](https://ojs.aaai.org/index.php/AAAI/article/view/3881) (AAAI 2019)    * **[GMAN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.gman.GMAN)** from Zheng *et al.*: [GMAN: A Graph Multi-Attention Network for Traffic Prediction](https://arxiv.org/pdf/1911.08415.pdf) (AAAI 2020)    * **[MTGNN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.mtgnn.MTGNN)** from Wu *et al.*: [Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks](https://arxiv.org/abs/2005.11650) (KDD 2020)    * **[2S-AGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.tsagcn.AAGCN)** from Shi *et al.*: [Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition](https://arxiv.org/abs/1805.07694) (CVPR 2019)    * **[DNNTSP](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.dnntsp.DNNTSP)** from Yu *et al.*: [Predicting Temporal Sets with Deep Neural Networks](https://dl.acm.org/doi/abs/10.1145/3394486.3403152) (KDD 2020)    **Auxiliary Graph Convolutions**    * **[TemporalConv](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.stgcn.TemporalConv)** from Yu *et al.*: [Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting](https://arxiv.org/abs/1709.04875) (IJCAI 2018)    * **[DConv](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.dcrnn.DConv)** from Li *et al.*: [Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting](https://arxiv.org/abs/1707.01926) (ICLR 2018)    * **[ChebConvAttention](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.astgcn.ChebConvAttention)** from Guo *et al.*: [Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting](https://ojs.aaai.org/index.php/AAAI/article/view/3881) (AAAI 2019)    * **[AVWGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.agcrn.AVWGCN)** from Bai *et al.*: [Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting](https://arxiv.org/abs/2007.02842) (NeurIPS 2020)      --------------------------------------------------------------------------------      Head over to our [documentation](https://pytorch-geometric-temporal.readthedocs.io) to find out more about installation, creation of datasets and a full list of implemented methods and available datasets.  For a quick start, check out the [examples](https://pytorch-geometric-temporal.readthedocs.io) in the `examples/` directory.    If you notice anything unexpected, please open an [issue](https://benedekrozemberczki/pytorch_geometric_temporal/issues). If you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/issues).      --------------------------------------------------------------------------------    **Installation**    Binaries are provided for Python version <= 3.9.    **PyTorch 1.10.0**    To install the binaries for PyTorch 1.10.0, simply run    ```sh  pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+${CUDA}.html  pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+${CUDA}.html  pip install torch-geometric  pip install torch-geometric-temporal  ```    where `${CUDA}` should be replaced by either `cpu`, `cu102`, or `cu113` depending on your PyTorch installation.    |             | `cpu` | `cu102` | `cu113` |  |-------------|-------|---------|---------|  | **Linux**   | âœ…    | âœ…      | âœ…      |  | **Windows** | âœ…    | âœ…      | âœ…      |  | **macOS**   | âœ…    |         |         |    --------------------------------------------------------------------------------    **Running tests**    ```  $ python setup.py test  ```  --------------------------------------------------------------------------------    **License**    - [MIT License](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/blob/master/LICENSE) """
Big data;https://github.com/linkedin/gobblin;"""# Apache Gobblin   [![Build Status](https://github.com/apache/gobblin/actions/workflows/build_and_test.yaml/badge.svg?branch=master)](https://travis-ci.org/apache/gobblin)  [![Documentation Status](https://readthedocs.org/projects/gobblin/badge/?version=latest)](https://gobblin.readthedocs.org/en/latest/?badge=latest)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.apache.gobblin/gobblin-api/badge.svg)](https://search.maven.org/search?q=g:org.apache.gobblin)  [![Stack Overflow](http://img.shields.io/:stack%20overflow-gobblin-brightgreen.svg)](http://stackoverflow.com/questions/tagged/gobblin)  [![Join us on Slack](https://img.shields.io/badge/slack-apache--gobblin-brightgreen.svg)]( https://join.slack.com/t/apache-gobblin/shared_invite/zt-vqgdztup-UUq8S6gGJqE6L5~9~JelNg)  [![codecov.io](https://codecov.io/github/apache/gobblin/branch/master/graph/badge.svg)](https://codecov.io/github/apache/gobblin)    Apache Gobblin is a highly scalable data management solution for structured and byte-oriented data in heterogeneous data ecosystems.     ### Capabilities  - Ingestion and export of data from a variety of sources and sinks into and out of the data lake. Gobblin is optimized and designed for ELT patterns with inline transformations on ingest (small t).  - Data Organization within the lake (e.g. compaction, partitioning, deduplication)  - Lifecycle Management of data within the lake (e.g. data retention)  - Compliance Management of data across the ecosystem (e.g. fine-grain data deletions)    ### Highlights  - Battle tested at scale: Runs in production at petabyte-scale at companies like LinkedIn, PayPal, Verizon etc.  - Feature rich: Supports task partitioning, state management for incremental processing, atomic data publishing, data quality checking, job scheduling, fault tolerance etc.  - Supports stream and batch execution modes   - Control Plane (Gobblin-as-a-service) supports programmatic triggering and orchestration of data plane operations.     ### Common Patterns used in production  - Stream / Batch ingestion of Kafka to Data Lake (HDFS, S3, ADLS)  - Bulk-loading serving stores from the Data Lake (e.g. HDFS -> Couchbase)  - Support for data sync across Federated Data Lake (HDFS <-> HDFS, HDFS <-> S3, S3 <-> ADLS)  - Integrate external vendor API-s (e.g. Salesforce, Dynamics etc.) with data store (HDFS, Couchbase etc)  - Enforcing Data retention policies and GDPR deletion on HDFS / ADLS      ### Apache Gobblin is NOT  - A general purpose data transformation engine like Spark or Flink. Gobblin can delegate complex-data processing tasks to Spark, Hive etc.   - A data storage system like Apache Kafka or HDFS. Gobblin integrates with these systems as sources or sinks.   - A general-purpose workflow execution system like Airflow, Azkaban, Dagster, Luigi.       # Requirements  * Java >= 1.8    If building the distribution with tests turned on:  * Maven version 3.5.3     # Instructions to run Apache RAT (Release Audit Tool)  1. Extract the archive file to your local directory.  2. Run `./gradlew rat`. Report will be generated under build/rat/rat-report.html    # Instructions to build the distribution  1. Extract the archive file to your local directory.  2. Skip tests and build the distribution:   Run `./gradlew build -x findbugsMain -x test -x rat -x checkstyleMain`   The distribution will be created in build/gobblin-distribution/distributions directory.  (or)  3. Run tests and build the distribution (requires Maven):   Run `./gradlew build`   The distribution will be created in build/gobblin-distribution/distributions directory.    # Quick Links      * [Gobblin documentation](https://gobblin.apache.org/docs/)      * [Running Gobblin on Docker from your laptop](https://github.com/apache/gobblin/blob/master/gobblin-docs/user-guide/Docker-Integration.md)      * [Getting started guide](https://gobblin.apache.org/docs/Getting-Started/)      * [Gobblin architecture](https://gobblin.apache.org/docs/Gobblin-Architecture/)    * Community Slack: [Get your invite](https://join.slack.com/t/apache-gobblin/shared_invite/zt-1723tsdhd-ITcAEsaQNpQvuQUFOgfHbQ)    * [List of companies known to use Gobblin](https://gobblin.apache.org/docs/Powered-By/)     * [Sample project](https://github.com/apache/gobblin/tree/master/gobblin-example)    * [How to build Gobblin from source code](https://gobblin.apache.org/docs/user-guide/Building-Gobblin/)    * [Issue tracker - Apache Jira](https://issues.apache.org/jira/projects/GOBBLIN/issues/) """
Big data;https://github.com/spring-projects/spring-xd;"""# spring-xd is no longer actively maintained by VMware, Inc.    Spring XD  =========    *Spring XD* makes it easy to solve common big data problems such as data ingestion and export, real-time analytics, and batch workflow orchestration.  By building on mature, open source projects such as Spring Integration, Data and Batch, Spring XD will simplify the process of creating real-word big data solutions.  XD stands for 'eXtreme Data' or 'x' as in y=mx+b :)    While it is possible today to build such solutions using Spring (see the [Spring Data Book][] for details and examples), Spring XD will move well beyond the framework API level by providing an out-of-the-box executable server, a pluggable module system, a high level configuration DSL, a simple model for distributing data processing instances on or off the Hadoop cluster, and more.    You can fork the repository and/or monitor JIRA to see what is going on. As always, we consider the feedback from our broad and passionate community to be one of our greatest assets.    ## Documentation    Look for it on the [XD wiki](https://github.com/springsource/spring-xd/wiki). [API Documentation](http://static.springsource.org/spring-xd/docs/current-SNAPSHOT/api/) (JavaDoc) is available as well. Please also visit the SpringSource.org [project website](http://www.springsource.org/spring-xd) for more information.    ## How to build     Check the documentation on how to build Spring XD [here](http://docs.spring.io/spring-xd/docs/current-SNAPSHOT/reference/html/#building-spring-xd).    ## Getting Help    * Get involved with the community on StackOverflow using the tag spring-xd.    ## License    *Spring XD* is released under version 2.0 of the [Apache License][].    ## Contributing to Spring XD    Here are some ways for you to get involved     * Create [JIRA](https://jira.springsource.org/browse/XD) tickets for bugs and new features and comment and vote on the ones that you are interested in.  * Follow the flow of developing on the [work board](https://jira.springsource.org/secure/RapidBoard.jspa?rapidView=6).  * Github is for social coding: if you want to write code, we encourage contributions through pull requests from [forks of this repository](http://help.github.com/forking/).  If you want to contribute code this way, please familiarize yourself with the process outlined for contributing to Spring projects here: [Contributor Guidelines](https://github.com/SpringSource/spring-integration/wiki/Contributor-Guidelines).    Before we accept a non-trivial patch or pull request we will need you to sign the [contributor's agreement](https://support.springsource.com/spring_committer_signup).  Signing the contributor's agreement does not grant anyone commit rights to the main repository, but it does mean that we can accept your contributions, and you will get an author credit if we do.  Active contributors might be asked to join the core team, and given the ability to merge pull requests.    ## Issue Tracking    Report issues via the [Spring XD JIRA][].    ## Continuous Integration    * **Master**: https://build.spring.io/browse/XD-MASTER  * **Sonar**: https://build.spring.io/browse/XD-SONAR    ## Metrics    Source Metrics are available via Sonar at:    * https://sonar.springsource.org/dashboard/index/org.springframework.xd:spring-xd      [Spring XD JIRA]: https://jira.springsource.org/browse/XD  [Apache License]: http://www.apache.org/licenses/LICENSE-2.0  [Spring Data Book]: http://bit.ly/sd-book  """
Big data;https://github.com/ecomfe/echarts;"""# Apache ECharts    <a href=""https://echarts.apache.org/"">      <img style=""vertical-align: top;"" src=""./asset/logo.png?raw=true"" alt=""logo"" height=""50px"">  </a>    Apache ECharts is a free, powerful charting and visualization library offering an easy way of adding intuitive, interactive, and highly customizable charts to your commercial products. It is written in pure JavaScript and based on <a href=""https://github.com/ecomfe/zrender"">zrender</a>, which is a whole new lightweight canvas library.    **[ä¸­æ–‡å®˜ç½‘](https://echarts.apache.org/zh/index.html)** | **[ENGLISH HOMEPAGE](https://echarts.apache.org/en/index.html)**    [![License](https://img.shields.io/npm/l/echarts?color=5470c6)](https://github.com/apache/echarts/blob/master/LICENSE) [![Latest npm release](https://img.shields.io/npm/v/echarts?color=91cc75)](https://www.npmjs.com/package/echarts) [![NPM downloads](https://img.shields.io/npm/dm/echarts.svg?label=npm%20downloads&style=flat&color=fac858)](https://www.npmjs.com/package/echarts) [![Contributors](https://img.shields.io/github/contributors/apache/echarts?color=3ba272)](https://github.com/apache/echarts/graphs/contributors)    [![Build Status](https://github.com/apache/echarts/actions/workflows/ci.yml/badge.svg)](https://github.com/apache/echarts/actions/workflows/ci.yml)    ## Get Apache ECharts    You may choose one of the following methods:    + Download from the [official website](https://echarts.apache.org/download.html)  + `npm install echarts --save`  + CDN: [jsDelivr CDN](https://www.jsdelivr.com/package/npm/echarts?path=dist)    ## Docs    + [Get Started](https://echarts.apache.org/handbook)  + [API](https://echarts.apache.org/api.html)  + [Option Manual](https://echarts.apache.org/option.html)  + [Examples](https://echarts.apache.org/examples)    ## Get Help    + [GitHub Issues](https://github.com/apache/echarts/issues) for bug report and feature requests  + Email [dev@echarts.apache.org](mailto:dev@echarts.apache.org) for general questions  + Subscribe to the [mailing list](https://echarts.apache.org/maillist.html) to get updated with the project    ## Build    Build echarts source code:    Execute the instructions in the root directory of the echarts:  ([Node.js](https://nodejs.org) is required)    ```shell  # Install the dependencies from NPM:  npm install    # Rebuild source code immediately in watch mode when changing the source code.  npm run dev    # Check correctness of TypeScript code.  npm run checktype    # If intending to build and get all types of the ""production"" files:  npm run release  ```    Then the ""production"" files are generated in the `dist` directory.    ## Contribution    If you wish to debug locally or make pull requests, please refer to the [contributing](https://github.com/apache/echarts/blob/master/CONTRIBUTING.md) document.    ## Resources    ### Awesome ECharts    [https://github.com/ecomfe/awesome-echarts](https://github.com/ecomfe/awesome-echarts)    ### Extensions    + [ECharts GL](https://github.com/ecomfe/echarts-gl) An extension pack of ECharts, which provides 3D plots, globe visualization, and WebGL acceleration.    + [Liquidfill æ°´çƒå›¾](https://github.com/ecomfe/echarts-liquidfill)    + [Wordcloud å­—ç¬¦äº‘](https://github.com/ecomfe/echarts-wordcloud)    + [Extension for Baidu Map ç™¾åº¦åœ°å›¾æ‰©å±•](https://github.com/apache/echarts/tree/master/extension-src/bmap) An extension provides a wrapper of Baidu Map Service SDK.    + [vue-echarts](https://github.com/ecomfe/vue-echarts) ECharts component for Vue.js    + [echarts-stat](https://github.com/ecomfe/echarts-stat) Statistics tool for ECharts    ## License    ECharts is available under the Apache License V2.    ## Code of Conduct    Please refer to [Apache Code of Conduct](https://www.apache.org/foundation/policies/conduct.html).    ## Paper    Deqing Li, Honghui Mei, Yi Shen, Shuang Su, Wenli Zhang, Junting Wang, Ming Zu, Wei Chen.  [ECharts: A Declarative Framework for Rapid Construction of Web-based Visualization](https://www.sciencedirect.com/science/article/pii/S2468502X18300068).  Visual Informatics, 2018. """
Big data;https://github.com/Codecademy/EventHub;"""# EventHub  EventHub enables companies to do cross device event tracking. Events are joined by their associated user on EventHub and can be visualized by the built-in dashboard to answer the following common business questions  * what is my funnel conversion rate?  * what is my cohorted KPI retention?  * which variant in my A/B test has a higher conversion rate?    Most important of all, EventHub is free and open source.    **Table of Contents**  - [Quick Start](#quick-start)  - [Server](#server)  - [Dashboard](#dashboard)  - [Javascript Library](#javascript-library)  - [Ruby Library](#ruby-library)    ## Quick Start  ### Playground  A [demo server](http://codecademy:codecademy@floating-mesa-9408.herokuapp.com/) is available on Heroku and the username/password to access the dashboard is `codecademy/codecademy`.    - [Example funnel query](http://codecademy:codecademy@54.193.159.140/?start_date=20130101&end_date=20130107&num_days_to_complete_funnel=7&funnel_steps%5B%5D=receive_email&funnel_steps%5B%5D=view_track_page&funnel_steps%5B%5D=finish_course&type=funnel)  - [Example cohort query](http://codecademy:codecademy@54.193.159.140/?start_date=20130101&end_date=20130107&row_event_type=receive_email&column_event_type=start_track&num_days_per_row=1&num_columns=11&type=cohort)    ### Screenshots  ![Funnel screenshot](https://raw.githubusercontent.com/Codecademy/EventHub/master/funnel-screenshot.png)  ![Cohort screenshot](https://raw.githubusercontent.com/Codecademy/EventHub/master/cohort-screenshot.png)    ### Deploy with Heroku  Developers who want to try EventHub can quickly set the server up on Heroku with the following commands. However, please be aware that Heroku's file system is ephemeral and your data will be wiped after the instance is closed.  ```bash  git clone https://github.com/Codecademy/EventHub.git    cd EventHub  heroku create  git push heroku master    heroku open  ```    ### Required dependencies  * [java sdk7](http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html)  * [maven](http://maven.apache.org)    ### Compile and run  ```bash  # set up proper JAVA_HOME for mac  export JAVA_HOME=$(/usr/libexec/java_home)    git clone https://github.com/Codecademy/EventHub.git  cd EventHub  export EVENT_HUB_DIR=`pwd`  mvn -am -pl web clean package  java -jar web/target/web-1.0-SNAPSHOT.jar  ```    ### How to run all the tests  #### Unit/Integration/Functional testing  ```bash  mvn -am -pl web clean test  ```    #### Manual testing with curl  Comprehensive examples can be found in `script.sh`.  ```bash  cd ${EVENT_HUB_DIR}; ./script.sh  ```    Test all event related endpoints  * Add new event      ```bash      curl -X POST http://localhost:8080/events/track --data ""event_type=signup&external_user_id=foobar&event_property_1=1""      ```    * Batch add new event      ```bash      curl -X POST http://localhost:8080/events/batch_track --data ""events=[{event_type: signup, external_user_id: foobar, date: 20130101, event_property_1: 1}]""      ```    * Show all event types      ```bash      curl http://localhost:8080/events/types      ```    * Show events for a given user      ```bash      curl http://localhost:8080/users/timeline\?external_user_id\=chengtao@codecademy.com\&offset\=0\&num_records\=1      ```    * Show all property keys for the given event type      ```bash      curl 'http://localhost:8080/events/keys?event_type=signup'      ```    * Show all property values for the given event type and property key      ```bash      curl 'http://localhost:8080/events/values?event_type=signup&event_key=treatment'      ```    * Show all property values for the given event type, property key and value prefix      ```bash      curl 'http://localhost:8080/events/values?event_type=signup&event_key=treatment&prefix=fa'      ```    * Show server stats      ```bash      curl http://localhost:8080/varz      ```    * Funnel query      ```bash      today=`date +'%Y%m%d'`      end_date=`(date -d '+7day' +'%Y%m%d' || date -v '+7d' +'%Y%m%d') 2> /dev/null`        curl -X POST ""http://localhost:8080/events/funnel"" --data ""start_date=${today}&end_date=${end_date}&funnel_steps[]=signup&funnel_steps[]=view_shopping_cart&funnel_steps[]=checkout&num_days_to_complete_funnel=7&eck=event_property_1&ecv=1""      ```    * Retention query      ```bash      today=`date +'%Y%m%d'`      end_date=`(date -d '+7day' +'%Y%m%d' || date -v '+7d' +'%Y%m%d') 2> /dev/null`        curl -X POST ""http://localhost:8080/events/cohort"" --data ""start_date=${today}&end_date=${end_date}&row_event_type=signup&column_event_type=view_shopping_cart&num_days_per_row=1&num_columns=2""      ```    Test all user related endpoints  * show paginated events for a given user      ```bash      curl http://localhost:8080/users/timeline\?external_user_id\=chengtao@codecademy.com\&offset\=0\&num_records\=5      ```    * show information of users who have matched property keys & values      ```bash      curl -X POST http://localhost:8080/users/find --data ""ufk[]=external_user_id&ufv[]=chengtao1@codecademy.com""      ```    * add or update user information      ```bash      curl -X POST http://localhost:8080/users/add_or_update --data ""external_user_id=chengtao@codecademy.com&foo=bar&hello=world""      ```    * Show all property keys for users      ```bash      curl 'http://localhost:8080/users/keys      ```    * Show all property values for users given property key and (optional) value prefix      ```bash      curl 'http://localhost:8080/users/values?user_key=hello&prefix=w'      ```    #### Load testing with Jmeter  We use [Apache Jmeter](http://jmeter.apache.org) for load testing, and the load testing script can be found in `${EVENT_HUB_DIR}/jmeter.jmx`.  ```bash  export JMETER_DIR=~/Downloads/apache-jmeter-2.11/  java -jar ${JMETER_DIR}/bin/ApacheJMeter.jar -JnumThreads=1 -n -t jmeter.jmx -p jmeter.properties  java -jar ${JMETER_DIR}/bin/ApacheJMeter.jar -JnumThreads=5 -n -t jmeter.jmx -p jmeter.properties  java -jar ${JMETER_DIR}/bin/ApacheJMeter.jar -JnumThreads=10 -n -t jmeter.jmx -p jmeter.properties    # generate graph (require matplotlib)  ./plot_jmeter_performance.py 1-jmeter-performance.csv 5-jmeter-performance.csv 10-jmeter-performance.csv    # open ""Track Event.png""  ```    ## Server    ### Key observations & design decisions  Our goal is to build something usable on a single machine with a reasonably large SSD drive. Let's say, hypothetically, the server receives 100M events monthly (might cost you few thousand dollars per month to use SAAS provider), and each event is 500 bytes without compression. In this situation, storing all the events likely only takes you few hundreds GB with compression, and chances are, only the data in recent months are of interest.    Also, to efficiently run basic funnel and cohort queries without filtering, only two forward indices are needed, event index sharded by event types and event index sharded by users. Therefore, our strategy is to make those two indices as small as possible to fit in memory, and if the client wants to do filtering for events, we build a bloomfilter to reject most of the non exact-match. Imagine we are running another hypothetical query while assuming both indices and the bloomfilters can be fit in memory. Say there are 1M events that cannot be rejected and need to hit the disk, assuming each SSD disk read is 16 microseconds, we are talking about sub-minute query time, while assuming none of the data are in memory. In practice, this situation is likely much better as we cache all the recently hit records, and most of the queries likely only care the most recent data.    To simplify the design of the server and store indices compactly so that they fit in memory, we made the following two assumptions.    1. Times are associated to events when the server receives the an event  2. Date is the finest level of granularity    With the above two assumptions, we can rely on the server generated monotonically increasing id to maintain the total order for the events. In addition, as long as we track the id of the first event in any given date, we do not need to store the time information in the indices (which greatly reduces the size of the indices). The direct implication for those assumptions are, first, if the client chose to cache some events locally and sent them later, the timing for those events will be recorded as the server receives them, not when the user made those actions; second, though the server maintains the total ordering of all events, it cannot answer questions like what is the conversion rate for the given funnel between 2pm and 3pm on a given date.    Lastly, for both indices, since they are sharded by event types or users, we can expect the size of the indices to reduce significantly with proper compression.    ### Architecture  At the highest level, `com.codecademy.evenhub.web.EventHubHandler` is the main entry point. It runs a [Jetty](http://www.eclipse.org/jetty) server, reflectively collects supported commands under `com.codecademy.evenhub.web.commands`, handles JSONP request transparently, handles requests to static resources like the dashboard, and most importantly, act as a proxy which translates http request and respones to and from method calls to `com.codecademy.evenhub.EventHub`.    `com.codecademy.evenhub.EventHub` can be thought of as a facade to the key components of `UserStorage`, `EventStorage`, `ShardedEventIndex`, `DatedEventIndex`, `UserEventIndex` and `PropertiesIndex`.    For `UserStorage` and `EventStorage`, at the lowest level, we implemented `Journal{User,Event}Storage` backed by [HawtJournal](https://github.com/fusesource/hawtjournal/) to store underlying records reliably. In addition, when clients are quering records which cannot be filtered by the supported indices, the server will loop through all the potential hits, look up the properties from the `Journal` and then filter accordingly. For better performance, there are also decorators for each storage like `Cached{User,Event}Storage` to support caching and `BloomFiltered{User,Event}Storage` to support fast rejection for filters like `ExactMatch`. Please also beware that each `Storage` maintains a monotonically increasing counter as the internal id generator for each event and user received.    To make the funnel and cohort queries fast, `EventHub` also maintains three indices, `ShardedEventIndex`, `UserEventIndex`, and `DatedEventIndex` behind the scene. `DatedEventIndex` simply tracks the mapping from a given date, the id of the first event received in that day. `ShardedEventIndex` can be thought of as sorted event ids sharded by event type. `UserEventIndex` can be thought of as sorted event ids sharded by users.    Lastly, `EventHub` maintains a `PropertiesIndex` backed by [LevelDB Jni](https://github.com/fusesource/leveldbjni) to track what properties keys are available for a given event type and what properties values are available for a given event type and a property key.    ### Horizontal scalabiltiy  While EventHub does not need any information from different users, with a broker in front of EventHub servers, EventHub can be easily sharded by users and scale horizontally.    ### Performance  In the following three experiments, the spec of the computer used can be found in the following table    | Component      | Spec                                    |  |----------------|-----------------------------------------|  | Computer Model | Mac Book Pro, Retina 15-inch, Late 2013 |  | Processor      | 2GHz Intel Core i7                      |  | Memory         | 8GB 1600 MHz DDR3                       |  | Software       | OS X 10.9.2                             |  | Jvm            | Oracle JDK 1.7                          |    #### Write performance  The following graph is generated as described in [Load testing with Jmeter](#load-testing-with-jmeter). The graph shows both the throughput and latency of adding the first one million events (without batching) with different number of threads (1, 5, 10, 15).  ![Throughput and latency by threads](http://i60.tinypic.com/16ad66b.png)    #### Query performance  While it is difficult to come up with a generic benchmark, we would rather show something rather than show nothing. After generating about one million events with the load testing script as described in [Load testing with Jmeter](#load-testing-with-jmeter), we ran the four types of queries twice, once after the server starts cleanly and another time while the cache is still warm.    | Query                   | 1st execution | 2nd execution | command |  |-------------------------|---------------|---------------|---------|  | Funnel without filters  | 1.15s         | 0.19s         | curl -X POST ""http://localhost:8080/events/funnel"" --data ""start_date=20130101&end_date=20130130&funnel_steps[]=receive_email&funnel_steps[]=view_track_page&funnel_steps[]=start_track&num_days_to_complete_funnel=30"" |  | Funnel with filters     | 1.31s         | 0.43s         | curl -X POST ""http://localhost:8080/events/funnel"" --data ""start_date=20130101&end_date=20130130&funnel_steps[]=receive_email&funnel_steps[]=view_track_page&funnel_steps[]=start_track&num_days_to_complete_funnel=30&efk0[]=event_property_1&efv0[]=1"" |  | Cohort without filters  | 0.63s         | 0.13s         | curl -X POST ""http://localhost:8080/events/cohort"" --data ""start_date=20130101&end_date=20130130&row_event_type=receive_email&column_event_type=start_track&num_days_per_row=1&num_columns=7"" |  | Cohort with filters     | 1.20s         | 0.32s         | curl -X POST ""http://localhost:8080/events/cohort"" --data ""start_date=20130101&end_date=20130130&row_event_type=receive_email&column_event_type=start_track&num_days_per_row=1&num_columns=7&refk[]=event_property_1&refv[]=1"" |    #### Memory footprint  In the experiment, the server was bootstrapped differently. Instead of using the load testing script, we used subset of data from Codecademy, which has around 53M events and 2.4M users. Please be aware that the current storage format on disk is fairly inefficient and has serious internal fragmentation. However, when the data are loaded to memory, it will be much more efficient as we would never load those ""hole"" pages into memory.    | Key Component             | Size in memory  | Note |  |---------------------------|-----------------|------|  | ShardedEventIndex         | 424Mb           | (data size) + (index size) <br>= (event id size * number of events) + negligible<br>= (8 * 53M) |  | UserEventIndex            | 722Mb           | (data size) + (index size) <br>= (event id size * number of events) + (index entry size * number of users)<br>= (8 * 53M) + ((numPointersPerIndexEntry * 2 + 1) * 8 + 4) * 2.4M)<br>= (8 * 53M) + (124 * 2.4M) |  | BloomFilteredEventStorage | 848Mb           | (bloomfilter size) * (number of events) <br>= 16 * 53M |    ## Dashboard  The server comes with a built-in dashboard which is simply some static resources stored in `/web/src/main/resources/frontend` and gets compiled into the server jar file. After running the server, the dashboard can be accessed at [http://localhost:8080](http://localhost:8080). Through the dashboard, you can access the server for your funnel and cohort analysis.    #### Password protection  The dashboard comes with insecure basic authentication which send unencrypted information without SSL. Please use it at your own discretion. The default username/password is codecademy/codecademy and you can change it by modifying your web.properties file or use the following command to start your server  ```bash  USERNAME=foo  PASSWORD=bar  java -Deventhubhandler.username=${USERNAME} -Deventhubhandler.password=${PASSWORD} -jar web/target/web-1.0-SNAPSHOT.jar  ```    ## Javascript Library  The project comes with a javascript library which can be integrated with your website as a way to send events to your EventHub server.     ### How to run JS tests  #### install [karma](http://karma-runner.github.io/0.12/index.html)  ```bash  cd ${EVENT_HUB_DIR}    npm install -g karma  npm install -g karma-jasmine@2_0  npm install -g karma-chrome-launcher    karma start karma.conf.js  ```    ### API  The javascript library is extremely simple and heavily inspired by mixpanel. There are only five methods that a developer needs to understand. Beware that behind the scenes, the library maintains a queue backed by localStorage, buffers the events in the queue, and has a timer reguarly clear the queue. If the browser doesn't support localStorage, a in-memory queue will be created as EventHub is created. Also, our implementation relies on the server to track the timestamp of each event. Therefore, in the case of a browser session disconnected before all the events are sent, the remaining events will be sent in the next browser session and thus have the timestamp recorded as the next session starts.    #### window.newEventHub()  The method will create an EventHub and start the timer which clears out the event queue in every second (default)  ```javascript  var name = ""EventHub"";  var options = {    url: 'http://example.com',    flushInterval: 10 /* in seconds */  };  var eventHub = window.newEventHub(name, options);  ```    #### eventHub.track()  This method enqueues the given event which will be cleared in batch at every flushInterval. Beware that if there is no identify method called before the track method is called, the library will automatically generate an user id which remain the same for the entire session (clears after the browser tab is closed), and send the generated user id along with the queued event. On the other hand, if `eventhub.identify()` is called before the track method is called, the user information passed along with the identify method call will be merged to the queued event.  ```javascript  eventHub.track(""signup"", {    property_1: 'value1',    property_2: 'value2'  });  ```    #### eventHub.alias()  This method links the given user to the automatically generated user. Typically, you only want to call this method once -- right after the user successfully signs up.  ```javascript  eventHub.alias('chengtao@codecademy.com');  ```    #### eventHub.identify()  This method tells the library instead of using the automatically generated user information, use the given information instead.  ```javascript  eventHub.identify('chengtao@codecademy.com', {    user_property_1: 'value1',    user_property_2: 'value2'  });  ```    #### eventHub.register()  This method allows the developer to add additional information to the generated user.  ```javascript  eventHub.register({    user_property_1: 'value1',    user_property_2: 'value2'  });  ```    ### Scenario and Receipes  #### Link the events sent before and after an user sign up  The following code  ```javascript  var eventHub = window.newEventHub('EventHub', { url: 'http://example.com' });  eventHub.track('pageview', { page: 'home' });  eventHub.register({    ip: '10.0.0.1'  });    // after user signup  eventHub.alias('chengtao@codecademy.com');  eventHub.identify('chengtao@codecademy.com', {    gender: 'male'  });  eventHub.track('pageview', { page: 'learn' });  ```   will result in a funnel like  ```javascript  {    user: 'something generated',    event: 'pageview',    page: 'home',    ip: '10.0.0.1'  }  link 'chengtao@codecademy.com' to 'something generated'  {    user: 'chengtao@codecademy.com',    event: 'pageview',    page: 'learn',    gender: 'male'  }  ```    #### A/B testing  The following code  ```javascript  var eventHub = window.newEventHub('EventHub', { url: 'http://example.com' });  eventHub.identify('chengtao@codecademy.com', {});  eventHub.track('pageview', {    page: 'javascript exercise 1',    experiment: 'fancy feature',    treatment: 'new'  });  eventHub.track('submit', {    page: 'javascript exercise 1'  });  ```  and  ```javascript  var eventHub = window.newEventHub('EventHub', { url: 'http://example.com' });  eventHub.identify('bob@codecademy.com', {});  eventHub.track('pageview', {    page: 'javascript exercise 1',    experiment: 'fancy feature',    treatment: 'control'  });  eventHub.track('skip', {    page: 'javascript exercise 1'  });  ```  will result in two funnels like  ```javascript  {    user: 'chengtao@codecademy.com',    event: 'pageview',    page: 'javascript exercise 1',    experiment: 'fancy feature',    treatment: 'new'  }  {    user: 'chengtao@codecademy.com',    event: 'submit',    page: 'javascript exercise 1'  }  ```  and  ```javascript  {    user: 'bob@codecademy.com',    event: 'pageview',    page: 'javascript exercise 1',    experiment: 'fancy feature',    treatment: 'control'  }  {    user: 'bob@codecademy.com',    event: 'skip',    page: 'javascript exercise 1'  }  ```    ## Ruby Library  Separate ruby gem is also available at [https://github.com/Codecademy/EventHubClient](https://github.com/Codecademy/EventHubClient)    ## License  MIT License.    Copyright (c) 2022 Codecademy LLC   """
Big data;https://github.com/gojek/feast;"""<!--Do not modify this file. It is auto-generated from a template (infra/templates/README.md.jinja2)-->    <p align=""center"">      <a href=""https://feast.dev/"">        <img src=""docs/assets/feast_logo.png"" width=""550"">      </a>  </p>  <br />    [![unit-tests](https://github.com/feast-dev/feast/actions/workflows/unit_tests.yml/badge.svg?branch=master&event=push)](https://github.com/feast-dev/feast/actions/workflows/unit_tests.yml)  [![integration-tests-and-build](https://github.com/feast-dev/feast/actions/workflows/master_only.yml/badge.svg?branch=master&event=push)](https://github.com/feast-dev/feast/actions/workflows/master_only.yml)  [![java-integration-tests](https://github.com/feast-dev/feast/actions/workflows/java_master_only.yml/badge.svg?branch=master&event=push)](https://github.com/feast-dev/feast/actions/workflows/java_master_only.yml)  [![linter](https://github.com/feast-dev/feast/actions/workflows/linter.yml/badge.svg?branch=master&event=push)](https://github.com/feast-dev/feast/actions/workflows/linter.yml)  [![Docs Latest](https://img.shields.io/badge/docs-latest-blue.svg)](https://docs.feast.dev/)  [![Python API](https://img.shields.io/readthedocs/feast/master?label=Python%20API)](http://rtd.feast.dev/)  [![License](https://img.shields.io/badge/License-Apache%202.0-blue)](https://github.com/feast-dev/feast/blob/master/LICENSE)  [![GitHub Release](https://img.shields.io/github/v/release/feast-dev/feast.svg?style=flat&sort=semver&color=blue)](https://github.com/feast-dev/feast/releases)    ## Overview    Feast is an open source feature store for machine learning. Feast is the fastest path to productionizing analytic data for model training and online inference.    Please see our [documentation](https://docs.feast.dev/) for more information about the project.    ## ðŸ“ Architecture  ![](docs/assets/feast-marchitecture.png)    The above architecture is the minimal Feast deployment. Want to run the full Feast on Snowflake/GCP/AWS? Click [here](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws).    ## ðŸ£ Getting Started    ### 1. Install Feast  ```commandline  pip install feast  ```    ### 2. Create a feature repository  ```commandline  feast init my_feature_repo  cd my_feature_repo  ```    ### 3. Register your feature definitions and set up your feature store  ```commandline  feast apply  ```    ### 4. Explore your data in the web UI (experimental)    ![Web UI](ui/sample.png)    ### 5. Build a training dataset  ```python  from feast import FeatureStore  import pandas as pd  from datetime import datetime    entity_df = pd.DataFrame.from_dict({      ""driver_id"": [1001, 1002, 1003, 1004],      ""event_timestamp"": [          datetime(2021, 4, 12, 10, 59, 42),          datetime(2021, 4, 12, 8,  12, 10),          datetime(2021, 4, 12, 16, 40, 26),          datetime(2021, 4, 12, 15, 1 , 12)      ]  })    store = FeatureStore(repo_path=""."")    training_df = store.get_historical_features(      entity_df=entity_df,      features = [          'driver_hourly_stats:conv_rate',          'driver_hourly_stats:acc_rate',          'driver_hourly_stats:avg_daily_trips'      ],  ).to_df()    print(training_df.head())    # Train model  # model = ml.fit(training_df)  ```  ```commandline              event_timestamp  driver_id  conv_rate  acc_rate  avg_daily_trips  0 2021-04-12 08:12:10+00:00       1002   0.713465  0.597095              531  1 2021-04-12 10:59:42+00:00       1001   0.072752  0.044344               11  2 2021-04-12 15:01:12+00:00       1004   0.658182  0.079150              220  3 2021-04-12 16:40:26+00:00       1003   0.162092  0.309035              959    ```    ### 6. Load feature values into your online store  ```commandline  CURRENT_TIME=$(date -u +""%Y-%m-%dT%H:%M:%S"")  feast materialize-incremental $CURRENT_TIME  ```    ```commandline  Materializing feature view driver_hourly_stats from 2021-04-14 to 2021-04-15 done!  ```    ### 7. Read online features at low latency  ```python  from pprint import pprint  from feast import FeatureStore    store = FeatureStore(repo_path=""."")    feature_vector = store.get_online_features(      features=[          'driver_hourly_stats:conv_rate',          'driver_hourly_stats:acc_rate',          'driver_hourly_stats:avg_daily_trips'      ],      entity_rows=[{""driver_id"": 1001}]  ).to_dict()    pprint(feature_vector)    # Make prediction  # model.predict(feature_vector)  ```  ```json  {      ""driver_id"": [1001],      ""driver_hourly_stats__conv_rate"": [0.49274],      ""driver_hourly_stats__acc_rate"": [0.92743],      ""driver_hourly_stats__avg_daily_trips"": [72]  }  ```    ## ðŸ“¦ Functionality and Roadmap    The list below contains the functionality that contributors are planning to develop for Feast    * Items below that are in development (or planned for development) will be indicated in parentheses.  * We welcome contribution to all items in the roadmap!  * Want to influence our roadmap and prioritization? Submit your feedback to [this form](https://docs.google.com/forms/d/e/1FAIpQLSfa1nRQ0sKz-JEFnMMCi4Jseag\_yDssO\_3nV9qMfxfrkil-wA/viewform).  * Want to speak to a Feast contributor? We are more than happy to jump on a call. Please schedule a time using [Calendly](https://calendly.com/d/x2ry-g5bb/meet-with-feast-team).    * **Data Sources**    * [x] [Snowflake source](https://docs.feast.dev/reference/data-sources/snowflake)    * [x] [Redshift source](https://docs.feast.dev/reference/data-sources/redshift)    * [x] [BigQuery source](https://docs.feast.dev/reference/data-sources/bigquery)    * [x] [Parquet file source](https://docs.feast.dev/reference/data-sources/file)    * [x] [Synapse source (community plugin)](https://github.com/Azure/feast-azure)    * [x] [Hive (community plugin)](https://github.com/baineng/feast-hive)    * [x] [Postgres (community plugin)](https://github.com/nossrannug/feast-postgres)    * [x] [Spark (community plugin)](https://docs.feast.dev/reference/data-sources/spark)    * [x] Kafka / Kinesis sources (via [push support into the online store](https://docs.feast.dev/reference/data-sources/push))    * [ ] HTTP source  * **Offline Stores**    * [x] [Snowflake](https://docs.feast.dev/reference/offline-stores/snowflake)    * [x] [Redshift](https://docs.feast.dev/reference/offline-stores/redshift)    * [x] [BigQuery](https://docs.feast.dev/reference/offline-stores/bigquery)    * [x] [Synapse (community plugin)](https://github.com/Azure/feast-azure)    * [x] [Hive (community plugin)](https://github.com/baineng/feast-hive)    * [x] [Postgres (community plugin)](https://github.com/nossrannug/feast-postgres)    * [x] [Trino (community plugin)](https://github.com/Shopify/feast-trino)    * [x] [Spark (community plugin)](https://docs.feast.dev/reference/offline-stores/spark)    * [x] [In-memory / Pandas](https://docs.feast.dev/reference/offline-stores/file)    * [x] [Custom offline store support](https://docs.feast.dev/how-to-guides/adding-a-new-offline-store)  * **Online Stores**    * [x] [DynamoDB](https://docs.feast.dev/reference/online-stores/dynamodb)    * [x] [Redis](https://docs.feast.dev/reference/online-stores/redis)    * [x] [Datastore](https://docs.feast.dev/reference/online-stores/datastore)    * [x] [SQLite](https://docs.feast.dev/reference/online-stores/sqlite)    * [x] [Azure Cache for Redis (community plugin)](https://github.com/Azure/feast-azure)    * [x] [Postgres (community plugin)](https://github.com/nossrannug/feast-postgres)    * [x] [Custom online store support](https://docs.feast.dev/how-to-guides/adding-support-for-a-new-online-store)    * [ ] Bigtable (in progress)    * [ ] Cassandra  * **Streaming**    * [x] [Custom streaming ingestion job support](https://docs.feast.dev/how-to-guides/creating-a-custom-provider)    * [x] [Push based streaming data ingestion](https://docs.feast.dev/reference/data-sources/push.md)    * [ ] Streaming ingestion on AWS    * [ ] Streaming ingestion on GCP  * **Feature Engineering**    * [x] On-demand Transformations (Alpha release. See [RFC](https://docs.google.com/document/d/1lgfIw0Drc65LpaxbUu49RCeJgMew547meSJttnUqz7c/edit#))    * [ ] Batch transformation (In progress. See [RFC](https://docs.google.com/document/d/1964OkzuBljifDvkV-0fakp2uaijnVzdwWNGdz7Vz50A/edit))    * [ ] Streaming transformation  * **Deployments**    * [x] AWS Lambda (Alpha release. See [RFC](https://docs.google.com/document/d/1eZWKWzfBif66LDN32IajpaG-j82LSHCCOzY6R7Ax7MI/edit))    * [x] Kubernetes (See [guide](https://docs.feast.dev/how-to-guides/running-feast-in-production#4.3.-java-based-feature-server-deployed-on-kubernetes))    * [ ] Cloud Run    * [ ] KNative  * **Feature Serving**    * [x] Python Client    * [x] REST Feature Server (Python) (Alpha release. See [RFC](https://docs.google.com/document/d/1iXvFhAsJ5jgAhPOpTdB3j-Wj1S9x3Ev\_Wr6ZpnLzER4/edit))    * [x] gRPC Feature Server (Java) (See [#1497](https://github.com/feast-dev/feast/issues/1497))    * [x] Push API    * [ ] Java Client    * [ ] Go Client    * [ ] Delete API    * [ ] Feature Logging (for training)  * **Data Quality Management (See [RFC](https://docs.google.com/document/d/110F72d4NTv80p35wDSONxhhPBqWRwbZXG4f9mNEMd98/edit))**    * [x] Data profiling and validation (Great Expectations)    * [ ] Training-serving skew detection (in progress)    * [ ] Metric production    * [ ] Drift detection  * **Feature Discovery and Governance**    * [x] Python SDK for browsing feature registry    * [x] CLI for browsing feature registry    * [x] Model-centric feature tracking (feature services)    * [x] Amundsen integration (see [Feast extractor](https://github.com/amundsen-io/amundsen/blob/main/databuilder/databuilder/extractor/feast_extractor.py))    * [x] Feast Web UI (alpha)    * [ ] REST API for browsing feature registry    * [ ] Feature versioning      ## ðŸŽ“ Important Resources    Please refer to the official documentation at [Documentation](https://docs.feast.dev/)   * [Quickstart](https://docs.feast.dev/getting-started/quickstart)   * [Tutorials](https://docs.feast.dev/tutorials/tutorials-overview)   * [Running Feast with Snowflake/GCP/AWS](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws)   * [Change Log](https://github.com/feast-dev/feast/blob/master/CHANGELOG.md)   * [Slack (#Feast)](https://slack.feast.dev/)    ## ðŸ‘‹ Contributing  Feast is a community project and is still under active development. Please have a look at our contributing and development guides if you want to contribute to the project:  - [Contribution Process for Feast](https://docs.feast.dev/project/contributing)  - [Development Guide for Feast](https://docs.feast.dev/project/development-guide)  - [Development Guide for the Main Feast Repository](./CONTRIBUTING.md)    ## âœ¨ Contributors    Thanks goes to these incredible people:    <a href=""https://github.com/feast-dev/feast/graphs/contributors"">    <img src=""https://contrib.rocks/image?repo=feast-dev/feast"" />  </a>"""
Big data;https://github.com/gchq/Gaffer;"""Copyright 2016-2020 Crown Copyright    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License.    <img src=""logos/logoWithText.png"" width=""300"">    Gaffer  ======    Gaffer is a graph database framework. It allows the storage of very large graphs containing rich properties on the nodes and edges. Several storage options are available, including Accumulo, Hbase and Parquet.    It is designed to be as flexible, scalable and extensible as possible, allowing for rapid prototyping and transition to production systems.    Gaffer offers:     - Rapid query across very large numbers of nodes and edges;   - Continual ingest of data at very high data rates, and batch bulk ingest of data via MapReduce or Spark;   - Storage of arbitrary Java objects on the nodes and edges;   - Automatic, user-configurable in-database aggregation of rich statistical properties (e.g. counts, histograms, sketches) on the nodes and edges;   - Versatile query-time summarisation, filtering and transformation of data;   - Fine grained data access controls;   - Hooks to apply policy and compliance rules to queries;   - Automated, rule-based removal of data (typically used to age-off old data);   - Retrieval of graph data into Apache Spark for fast and flexible analysis;   - A fully-featured REST API.    To get going with Gaffer, visit our [getting started pages](https://gchq.github.io/gaffer-doc/v1docs/summaries/getting-started.html).    Gaffer is under active development. Version 1.0 of Gaffer was released in October 2017.    License  -------    Gaffer is licensed under the Apache 2 license and is covered by [Crown Copyright](https://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/copyright-and-re-use/crown-copyright/).    Getting Started  ---------------    ### Try it out    We have a demo available to try that is based around a small uk road use dataset. See the example/road-traffic [README](https://github.com/gchq/Gaffer/blob/master/example/road-traffic/README.md) to try it out.    ### Building and Deploying    To build Gaffer run `mvn clean install -Pquick` in the top-level directory. This will build all of Gaffer's core libraries and some examples of how to load and query data.    See our [Store](https://gchq.github.io/gaffer-doc/v1docs/summaries/stores.html) documentation page for a list of available Gaffer Stores to chose from and the relevant documentation for each.    ### Inclusion in other projects    Gaffer is hosted on [Maven Central](https://mvnrepository.com/search?q=uk.gov.gchq.gaffer) and can easily be incorporated into your own maven projects.    To use Gaffer from the Java API the only required dependencies are the Gaffer graph module and a store module for the specific database technology used to store the data, e.g. for the Accumulo store:    ```  <dependency>      <groupId>uk.gov.gchq.gaffer</groupId>      <artifactId>graph</artifactId>      <version>${gaffer.version}</version>  </dependency>  <dependency>      <groupId>uk.gov.gchq.gaffer</groupId>      <artifactId>accumulo-store</artifactId>      <version>${gaffer.version}</version>  </dependency>  ```    This will include all other mandatory dependencies. Other (optional) components can be added to your project as required.    ### Documentation    Our Javadoc can be found [here](http://gchq.github.io/Gaffer/).    We have some user guides in our [docs](https://gchq.github.io/gaffer-doc/v1docs/getting-started/user-guide/contents.html).    Related repositories  --------------------    The [gaffer-tools](https://github.com/gchq/gaffer-tools) repository contains useful tools to help work with Gaffer. These include:    - `jar-shader` - Used to shade the version of Jackson to avoid incompatibility problems on CDH clusters;  - `mini-accumulo-cluster` - Allows a mini Accumulo cluster to be spun up for testing purposes;  - `performance-testing` - Methods of testing the performance of ingest and query operations against a graph;  - `python-shell` - Allows operations against a graph to be executed from a Python shell;  - `random-element-generation` - Code to generate large volumes of random graph data;  - `schema-builder` - A (beta) visual tool for writing schemas for a graph;  - `slider` - Code to deploy a Gaffer cluster to a YARN cluster using [Apache Slider](https://slider.incubator.apache.org/), including the ability to easily run Slider on an [AWS EMR cluster](https://aws.amazon.com/emr/);  - `ui` - A basic graph visualisation tool.    Contributing  ------------    We welcome contributions to the project. Detailed information on our ways of working can be found [here](https://gchq.github.io/gaffer-doc/v1docs/other/ways-of-working.html). In brief:    - Sign the [GCHQ Contributor Licence Agreement](https://cla-assistant.io/gchq/Gaffer);  - Push your changes to a fork;  - Submit a pull request. """
Big data;https://github.com/polyaxon/polyaxon;"""[![License: Apache 2](https://img.shields.io/badge/License-apache2-blue.svg?style=flat&longCache=true)](LICENSE)  [![Polyaxon API](https://img.shields.io/docker/pulls/polyaxon/polyaxon-api)](https://hub.docker.com/r/polyaxon/polyaxon-api)  [![Slack](https://img.shields.io/badge/Slack-1.4k%20members-blue.svg?style=flat&logo=slack&longCache=true)](https://polyaxon.com/slack/)    [![Docs](https://img.shields.io/badge/docs-stable-brightgreen.svg?style=flat&longCache=true)](https://polyaxon.com/docs/)  [![Release](https://img.shields.io/badge/release-v1.17.2-brightgreen.svg?longCache=true)](https://polyaxon.com/docs/releases/1-17/)  [![GitHub](https://img.shields.io/badge/issue_tracker-github-blue?style=flat&logo=github&longCache=true)](https://github.com/polyaxon/polyaxon/issues)  [![GitHub](https://img.shields.io/badge/roadmap-github-blue?style=flat&logo=github&longCache=true)](https://github.com/orgs/polyaxon/projects/5)    [![CLI](https://github.com/polyaxon/polyaxon/actions/workflows/core.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/core.yml)  [![Traceml](https://github.com/polyaxon/polyaxon/actions/workflows/traceml.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/traceml.yml)  [![Datatile](https://github.com/polyaxon/polyaxon/actions/workflows/datatile.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/datatile.yml)  [![Platform](https://github.com/polyaxon/polyaxon/actions/workflows/platform.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/platform.yml)  [![Codacy Badge](https://api.codacy.com/project/badge/Grade/90c05b6b112548c1a88b950beceacb69)](https://www.codacy.com/app/polyaxon/polyaxon?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=polyaxon/polyaxon&amp;utm_campaign=Badge_Grade)    <br>  <p align=""center"">    <p align=""center"">      <a href=""https://polyaxon.com/?utm_source=github&utm_medium=logo"" target=""_blank"">        <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/logo/vector/primary-white-default-monochrome.svg"" alt=""polyaxon"" height=""100"">      </a>    </p>    <p align=""center"">      Reproduce, Automate, Scale your data science.    </p>  </p>  <br>      Welcome to Polyaxon, a platform for building, training, and monitoring large scale deep learning applications.  We are making a system to solve reproducibility, automation, and scalability for machine learning applications.    Polyaxon deploys into any data center, cloud provider, or can be hosted and managed by Polyaxon, and it supports all the major deep learning frameworks such as Tensorflow, MXNet, Caffe, Torch, etc.    Polyaxon makes it faster, easier, and more efficient to develop deep learning applications by managing workloads with smart container and node management. And it turns GPU servers into shared, self-service resources for your team or organization.    <br>  <p align=""center"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/demo.gif"" alt=""demo"" width=""80%"">  </p>  <br>    # Install    #### TL;DR;    * Install CLI        ```bash      # Install Polyaxon CLI      $ pip install -U polyaxon      ```     * Create a deployment        ```bash      # Create a namespace      $ kubectl create namespace polyaxon        # Add Polyaxon charts repo      $ helm repo add polyaxon https://charts.polyaxon.com        # Deploy Polyaxon      $ polyaxon admin deploy -f config.yaml        # Access API      $ polyaxon port-forward      ```    Please check [polyaxon installation guide](https://polyaxon.com/docs/setup/)    # Quick start    #### TL;DR;     * Start a project        ```bash      # Create a project      $ polyaxon project create --name=quick-start --description='Polyaxon quick start.'      ```     * Train and track logs & resources        ```bash      # Upload code and start experiments      $ polyaxon run -f experiment.yaml -u -l      ```     * Dashboard        ```bash      # Start Polyaxon dashboard      $ polyaxon dashboard        Dashboard page will now open in your browser. Continue? [Y/n]: y      ```    <br>  <p align=""center"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/compare.png"" alt=""compare"" width=""400"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/dashboards.png"" alt=""dashboards"" width=""400"">  </p>  <br>     * Notebook      ```bash      # Start Jupyter notebook for your project      $ polyaxon run --hub notebook      ```    <br>  <p align=""center"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/notebook.png"" alt=""compare"" width=""400"">  </p>  <br>     * Tensorboard      ```bash      # Start TensorBoard for a run's output      $ polyaxon run --hub tensorboard -P uuid=UUID      ```    <br>  <p align=""center"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/tensorboard.png"" alt=""tensorboard"" width=""400"">  </p>  <br>    Please check our [quick start guide](https://polyaxon.com/docs/intro/quick-start/) to start training your first experiment.    # Distributed job    Polyaxon supports and simplifies distributed jobs.  Depending on the framework you are using, you need to deploy the corresponding operator, adapt your code to enable the distributed training,  and update your polyaxonfile.    Here are some examples of using distributed training:      * [Distributed Tensorflow](https://polyaxon.com/docs/experimentation/distributed/tf-jobs/)   * [Distributed Pytorch](https://polyaxon.com/docs/experimentation/distributed/pytorch-jobs/)   * [Distributed MPI](https://polyaxon.com/docs/experimentation/distributed/mpi-jobs/)   * [Horovod](https://polyaxon.com/integrations/horovod/)   * [Spark](https://polyaxon.com/docs/experimentation/distributed/spark-jobs/)   * [Dask](https://polyaxon.com/docs/experimentation/distributed/dask-jobs/)    # Hyperparameters tuning    Polyaxon has a concept for suggesting hyperparameters and managing their results very similar to Google Vizier called experiment groups.  An experiment group in Polyaxon defines a search algorithm, a search space, and a model to train.     * [Grid search](https://polyaxon.com/docs/automation/optimization-engine/grid-search/)   * [Random search](https://polyaxon.com/docs/automation/optimization-engine/random-search/)   * [Hyperband](https://polyaxon.com/docs/automation/optimization-engine/hyperband/)   * [Bayesian Optimization](https://polyaxon.com/docs/automation/optimization-engine/bayesian-optimization/)   * [Hyperopt](https://polyaxon.com/docs/automation/optimization-engine/hyperopt/)   * [Custom Iterative Optimization](https://polyaxon.com/docs/automation/optimization-engine/iterative/)    # Parallel executions    You can run your processing or model training jobs in parallel, Polyaxon provides a [mapping](https://polyaxon.com/docs/automation/mapping/) abstraction to manage concurrent jobs.    # DAGs and workflows    [Polyaxon DAGs](https://polyaxon.com/docs/automation/flow-engine/) is a tool that provides container-native engine for running machine learning pipelines.   A DAG manages multiple operations with dependencies. Each operation is defined by a component runtime.   This means that operations in a DAG can be jobs, services, distributed jobs, parallel executions, or nested DAGs.       # Architecture    ![Polyaxon architecture](artifacts/polyaxon_architecture.png)    # Documentation    Check out our [documentation](https://polyaxon.com/docs/) to learn more about Polyaxon.    # Dashboard    Polyaxon comes with a dashboard that shows the projects and experiments created by you and your team members.    To start the dashboard, just run the following command in your terminal    ```bash  $ polyaxon dashboard -y  ```    # Project status    Polyaxon is stable and it's running in production mode at many startups and Fortune 500 companies.     # Contributions    Please follow the contribution guide line: *[Contribute to Polyaxon](CONTRIBUTING.md)*.      # Research    If you use Polyaxon in your academic research, we would be grateful if you could cite it.    Feel free to [contact us](mailto:contact@polyaxon.com), we would love to learn about your project and see how we can support your custom need. """
Big data;https://github.com/onurakpolat/awesome-analytics;"""# Awesome Analytics [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)    A curated list of awesome analytics platforms, resources and other awesomeness. Inspired by [awesome-bigdata](https://github.com/onurakpolat/awesome-bigdata).     Your feedback and contributions are always welcome! Maintained by [@onurakpolat](https://github.com/onurakpolat) & [@koconder](https://github.com/koconder)    - [Awesome Analytics](#awesome-analytics)      - [General analytics](#general-analytics)      - [Real-time](#real-time)      - [Website analytics](#website-analytics)      - [Endpoints](#endpoints)      - [SEO](#seo)      - [Privacy focused analytics](#privacy-focused-analytics)      - [Heatmap analytics](#heatmap-analytics)      - [Analytics layers](#analytics-layers)      - [Mobile analytics](#mobile-analytics)      - [App store analytics](#app-store-analytics)      - [Attribution tracking](#attribution-tracking)      - [Social media analytics](#social-media-analytics)      - [Analytics dashboards](#analytics-dashboards)      - [Developer analytics](#developer-analytics)        - [Other Awesome Lists](#other-awesome-lists)    ## General analytics  * [userTrack](https://www.usertrack.net/) - Self-hosted web analytics with heatmaps, session-recordings, A/B tests and more. `Â©` `Self-Hosted` `PHP`  * [Panelbear](https://panelbear.com/) - free real-time website analytics. Supports custom event tracking, email digests, and site speed metrics. `Â©` `SaaS`  * [PostHog](https://posthog.com) - Open-source product analytics to track users, events, funnels and trends. Alternative to Mixpanel/Amplitude/Heap. ([Source Code](https://github.com/posthog/posthog)) `MIT` `Python`  * [Hotjar](https://www.hotjar.com/) - new and easy way to truly understand your web and mobile site visitors. `Â©` `SaaS`  * [Matomo](https://matomo.org/) - Leading open-source analytics platform that gives you more than just powerful analytics, formerly known as Piwik. ([Source Code](https://github.com/matomo-org/)) `GPL-3.0` `PHP`  * [Heap](https://heap.io) - tracks your app users, clicks, form submissions, and anything else. `Â©` `SaaS`  * [Opentracker](http://www.opentracker.net/) - real time reporting, geo-location user tracking. `Â©` `SaaS`  * [FoxMetrics](http://foxmetrics.com/) - analytics to track your userâ€™s actions and activities. `Â©` `SaaS`  * [Adobe Analytics](https://www.adobe.com/analytics/web-analytics.html) - web data into insights that everyone can act on. `Â©` `SaaS`  * [Google Analytics](https://www.google.com/analytics/) - de facto standard for analytics in the web analytics space. `Â©` `SaaS`  * [Screpy](https://screpy.com) - Screpy is a web analyzer and monitoring tool. Its powered by Google Lighthouse. `Â©` `SaaS`  * [Clicktale](https://www.clicktale.com) - record and watch exactly how a visitor used your website. `Â©` `SaaS`  * [GoSquared](https://www.gosquared.com/) - analytics with visitor tagging to help you dig deeper into one userâ€™s visit. `Â©` `SaaS`  * [Clicky](http://clicky.com/) - track visits and conversions, you can also track your video and audio analytics. `Â©` `SaaS`  * [Woopra](https://www.woopra.com/) - track where your users are coming from. `Â©` `SaaS`  * [Mint](https://haveamint.com/) - self-hosted analytics solution (no longer on sale).  `Â©` `SaaS`  * [Going Up](https://www.goingup.com/) - manage SEO analytics and web app analytics with one tool. `Â©` `SaaS`  * [Chartbeat](https://chartbeat.com/) - beautiful, real-time app analytics tool for web apps. `Â©` `SaaS`  * [Gauges](http://get.gaug.es/) - real-time web analytics tool. `Â©` `SaaS`  * [Indicative](https://www.indicative.com/) - Web & mobile analytics tool, with heavy emphasis on segmentation and funnel visualization. `Â©` `SaaS`  * [Open Web Analytics](http://www.openwebanalytics.com/) - Google Analytics and Matomo alternative. ([Source Code](https://github.com/padams/Open-Web-Analytics)) `GPL-2.0` `PHP`  * [Statcounter](https://statcounter.com/) - one of the ORIGINAL web analytics tools available. `Â©` `SaaS`  * [Adobe Digital Analytics](http://www.adobe.com/data-analytics-cloud/analytics/capabilities.html) - standard analytics tools plus some that large organizations can use. `Â©` `SaaS`  * [Hitslink.com](https://www.hitslink.com/) - real-time analytics, social media traffic reporting, and real-time dynamic segmentation. `Â©` `SaaS`  * [parse.ly](https://www.parse.ly) - real-time web analytics tool with a focus on tracking content. `Â©` `SaaS`  * [Loggr](http://loggr.net/) -  track your user events and monitor your web app. `Â©` `SaaS`  * [Kissmetrics](https://www.kissmetrics.com/) - real-time standard cohort analysis tool. `Â©` `SaaS`  * [Sitemeter](http://sitemeter.com/) - old analytics tool. `Â©` `SaaS`  * [Crawl Track](http://www.crawltrack.net/) - another old analytics tool. `Â©` `SaaS`  * [Sitespect](https://www.sitespect.com/) - full-suite web app analytics tool including A/B testing. `Â©` `SaaS`  * [Rakam](https://rakam.io/) - Custom analytics platform that lets you to create your own analytics service. Integrate with any data source (web, mobile, IoT etc.), analyze data with SQL and create dashboards. ([Source Code](https://github.com/rakam-io/rakam)) `Apache-2.0` `Java`  * [Metabase](https://www.metabase.com) - opensource analytics/BI tool  `Â©` `SaaS`  * [LiveSession](https://livesession.io) - session replay user analytics. `Â©` `SaaS`  * [Glassbox](https://glassboxdigital.com/) - customer experince and session recording analytics. `Â©` `SaaS`  * [Redash](https://redash.io/) - open source analytics/BI tool `Â©` `SaaS`  * [AWStats](http://www.awstats.org/) - Generates web, streaming, ftp or mail server statistics graphically. ([Source Code](https://github.com/eldy/awstats)) `GPL-3.0` `Perl`  * [Countly](https://count.ly) - Real time mobile and web analytics, crash reporting and push notifications platform. ([Source Code](https://github.com/countly)) `AGPL-3.0` `Javascript`  * [Druid](http://druid.io/) - Distributed, column-oriented, real-time analytics data store. ([Source Code](https://github.com/druid-io/druid)) `Apache-2.0` `Java`  * [Hastic](https://hastic.io) - Hackable time series pattern recognition tool with UI for Grafana. ([Source Code](https://github.com/hastic)) `Apache-2.0` `Python/Nodejs`  * [EDA](https://eda.jortilles.com/en/jortilles-english/) - Open source analytics/BI tool.  ([Source Code](https://github.com/jortilles/EDA)) `Apache-2.0` `Angular/Nodejs`  * [Count](https://count.co/) - notebook-based analytics platform, use SQL or drag-and-drop to build queries. `Â©` `SaaS`    ## Real-time    * [GoAccess](http://goaccess.io/) - Real-time web log analyzer and interactive viewer that runs in a terminal. ([Source Code](https://github.com/allinurl/goaccess)) `GPL-2.0` `C`    ## Website analytics    * [KISSS](https://kis3.dev) - Very minimalistic (KISS) website statistics tool. ([Source Code](https://github.com/kis3/kis3)) `MIT` `Go`    ## Endpoints  * [Census](https://getcensus.com/) - The easiest way to sync your customer data from your cloud data warehouse to SaaS applications like Salesforce, Marketo, HubSpot, Zendesk, etc. Census is the operational analytics platform that syncs your data warehouse with all your favorite apps. Get your customer success, sales & marketing teams on the same page by keeping customer data in sync. No engineering favors requiredâ€”just SQL. `SaaS`  * [RudderStack](https://rudderstack.com/) - The warehouse-first customer data platform (CDP) that builds your CDP on your data warehouse for you. RudderStack makes it easy to collect, unify, transform, and store your customer data as well as route it securely to a wide range of common, popular marketing, sales, and product tools (open-source alternative to Segment et al.). ([Source Code](https://github.com/rudderlabs/rudder-server/)) `AGPL-3.0` `Go`  * [Snowplow](http://snowplowanalytics.com/) - Analytics tool for web apps with a lot of data. Have every single event, from your websites, mobile apps, desktop applications and server-side systems, stored in your own data warehouse and available to action in real-time. ([Source Code](https://github.com/snowplow/)) `Apache-2.0` `Scala` `real-time`    ## SEO  * [Serposcope](https://serposcope.serphacker.com/) - Serposcope is a free and open-source rank tracker to monitor websites ranking in Google and improve your SEO performances. ([Source Code](https://github.com/serphacker/serposcope)) `MIT` `Java`    ## Privacy focused analytics    * [Fathom](https://usefathom.com/) - Fathom Analytics provides simple, useful websites stats without tracking or storing personal data of your users `Â©` `SaaS`  * [Plausible Analytics](https://plausible.io/) - Lightweight and [open source](https://github.com/plausible-insights/plausible) web analytics. Doesnâ€™t use cookies and doesn't track personal data. A privacy-friendly alternative to Google Analytics. ([Source Code](https://github.com/plausible/analytics/)) `MIT` `Elixir`  * [GoatCounter](https://www.goatcounter.com) - Easy web statistics without tracking of personal data; `SaaS` `Self-Hosted` ([Source Code](https://github.com/zgoat/goatcounter)) `EUPL-1.2` `Go`  * [Simple Analytics](https://simpleanalytics.io/) - Simple, clean, and friendly analytics for developers `Â©` `SaaS`  * [Nibspace](https://nibspace.com/) - Affordable, lightweight, privacy-friendly website analytics `Â©` `SaaS`  * [Metrical](https://metrical.xyz/) - A privacy-first web analytics tool for everyone. `Â©` `SaaS`  * [Shynet](https://github.com/milesmcc/shynet) - Modern, privacy-friendly, and detailed web analytics that works without cookies or JS. Designed for self-hosting. `Apache-2.0` `Python`  * [Umami](https://umami.is/) - Umami is a simple, easy to use, self-hosted web analytics solution. The goal is to provide you with a friendlier, privacy-focused alternative to Google Analytics and a free, open-sourced alternative to paid solutions. ([Demo](https://app.umami.is/share/ISgW2qz8/flightphp.com), [Source Code](https://github.com/mikecao/umami)) `MIT` `Nodejs`  * [Koko Analytics](https://www.kokoanalytics.com/) - Privacy-friendly and open source analytics plugin for WordPress. ([Source Code](https://github.com/ibericode/koko-analytics/)) `GPL-3.0` `PHP`  * [Offen](https://www.offen.dev/) - Offen is a fair and open web analytics tool. Gain insights while your users have full access to their data. Lightweight, self hosted and free. ([Demo](https://www.offen.dev/try-demo/), [Source Code](https://github.com/offen/offen)) `Apache-2.0` `Go/Docker`  * [Freshlytics](https://github.com/sheshbabu/freshlytics) - Privacy respecting, cookie free and low resource usage analytics platform. `MIT` `Docker/Nodejs`  * [Kindmetrics](https://kindmetrics.io/) - Clean privacy-focused website analytics. ([Source Code](https://github.com/kindmetrics/kindmetrics)) `MIT` `Crystal`  * [Ackee](https://ackee.electerious.com) - Self-hosted analytics tool for those who care about privacy. ([Demo](http://demo.ackee.electerious.com), [Source Code](https://github.com/electerious/Ackee)) `MIT` `Nodejs`  * [piratepx](https://www.piratepx.com/) - Just a little analytics insight for your personal or indie project. 100% free and open source. ([Demo](https://app.piratepx.com/shared/bGQbUJ-YADC_xIGZaYmyqp-J_PD6O1pkCdHmYdIjUvs53ExsImlzFeou4MCuZRbH), [Source](https://github.com/piratepx/app)) `MIT` `Nodejs`  * [Piwik PRO](https://piwik.pro/) - A privacy-friendly alternative to Google Analytics with built-in consent management. Hosted in EU, in your private cloud or on-premises. `Â©` `SaaS` `self-hosted`    ## Heatmap analytics    * [Crazyegg](http://www.crazyegg.com/) - a heatmaps only analytics tool. `Â©` `SaaS`  * [Inspeclet](https://www.inspectlet.com/) - another web app heatmaps tool. `Â©` `SaaS`  * [Mouseflow](http://mouseflow.com/) - live analytics and heatmaps. `Â©` `SaaS`  * [Session Cam](http://www.sessioncam.com/) - heatmaps analytics tool. `Â©` `SaaS`    ## Analytics layers    * [Keen.io](http://keen.io/) - custom-analytics API. `Â©` `SaaS`  * [Popcorn Metrics](http://www.popcornmetrics.com/) - visual editor to capture events and send to other platforms. `Â©` `SaaS`  * [Segment](https://segment.com/) - helps you integrate multiple app analytics tool with one piece of code. `Â©` `SaaS`  * [Iteratively](https://iterative.ly/) - capture clean product analytics consistently across teams & platforms. `Â©` `SaaS`  * [Analytics npm package](https://getanalytics.io/) - A lightweight, extendable analytics library designed to work with any third-party analytics provider to track page views, custom events, & identify users. Works in browsers & node.js. `Â©` `SaaS`    ## Mobile analytics    The tools listed here are not necessarily mobile analytics tools only. However they show a strong mobile focus.    * [Upsight](http://www.upsight.com/) - mobile app analytics tool for developers. `Â©` `SaaS`  * [Appsflyer](http://www.appsflyer.com/) - all-in-one marketing tool with analytics. `Â©` `SaaS`  * [Amazon Pinpoint](https://aws.amazon.com/pinpoint/) - Amazons multi-platform, basic mobile analytics tool. `Â©` `SaaS`  * [Tapstream](https://tapstream.com/) - user lifecycle analytics. `Â©` `SaaS`  * [Honeytracks](https://honeytracks.com/) - mobile app analytics for games. `Â©` `SaaS`  * [Apsalar](https://apsalar.com/) - analytics tool for larger app shops. `Â©` `SaaS`  * [Roambi](http://www.roambi.com/) - 3-in-1 analytics tool that helps you track analytics, handle mobile app business intelligence, and app reporting. `Â©` `SaaS`  * [Appcelerator](http://www.appcelerator.com/platform/appcelerator-analytics/) - entire mobile app marketing suite. `Â©` `SaaS`  * [Flurry](http://www.flurry.com/) - pretty much the â€œindustry standardâ€ for mobile app analytics. `Â©` `SaaS`  * [Countly](http://count.ly/) - open source mobile & web application analytics tool. `Â©` `SaaS`  * [Playtomatic](http://playtomic.org/) - mobile app open source analytics tool for games. `Â©` `SaaS`  * [Capptain](http://www.capptain.com/) - real-time analytics tool with segmentation and push. `Â©` `SaaS`  * [Amplitude](https://amplitude.com/) - real-time mobile analytics with all data provided in redshift. `Â©` `SaaS`  * [Appsee](http://www.appsee.com/) - mobile app analytics platform automatically tracks all users' interactions in your app `Â©` `SaaS`  * [Mixpanel](https://mixpanel.com/) - fully featured mobile analytics platform with segmentation and push. `Â©` `SaaS`  * [Localytics](http://www.localytics.com/) - fast and beautiful real-time mobile analytics platform with in-app and push. `Â©` `SaaS`  * [GameAnalytics](http://www.gameanalytics.com/) - leading game analytics platform. `Â©` `SaaS`  * [Swrve](https://swrve.com) - mobile analytics with segmentation, push, A/B testing and rich messaging `Â©` `SaaS`  * [Firebase](https://firebase.google.com/features/) - a free and unlimited analytics solution for android and iOS `Â©` `SaaS`  * [Liquid](https://onliquid.com/) - real-time mobile analytics, personalization, multivariate testing, audience segmentation and push. `Â©` `SaaS`    ## App store analytics    * [Appfigures](http://appfigures.com/) - app store analytics to track sales, reviews and rankings with an API. `Â©` `SaaS`  * [Appannie](http://www.appannie.com/) - track your app data from iTunes, Google Play & Amazon. `Â©` `SaaS`  * [Distimo](http://www.distimo.com/) - free app store analytics (acquired by [Appannie](http://www.appannie.com/)). `Â©` `SaaS`  * [Priori Data](https://prioridata.com/) - track and benchmark the performance of apps on Apple- and Play store. `Â©` `SaaS`  * [Asking Point](http://www.askingpoint.com/mobile-app-rating-widget) - track your mobile app user ratings. `Â©` `SaaS`  * [Apptrace](http://www.apptrace.com/) - fast and free app store analytics platform. `Â©` `SaaS`    ## Attribution tracking    * [Adjust](http://adjust.com/) - open-source SDK with sophisticated analysis and campaign tracking. `Â©` `SaaS`  * [Clickmeter](https://clickmeter.com) - analytics tool that helps you track marketing campaigns. `Â©` `SaaS`  * [HasOffers Mobile app tracking](http://www.mobileapptracking.com/) - attribution analytics platform. `Â©` `SaaS`    ## Social media analytics    Often there is no clear differentiation between social media management and analytics as most the tools provide analytics.    * [Brandwatch](http://www.brandwatch.com/) - Social media monitoring and analytics. `Â©` `SaaS`  * [Falconsocial](http://www.falconsocial.com/) - communications platform built on social media with analytics. `Â©` `SaaS`  * [Quintly](https://www.quintly.com/) - web-based tool to help you track, benchmark and optimize your social media performance. `Â©` `SaaS`  * [Kred](http://kred.com/) - Klout-like social media analytics platform. `Â©` `SaaS`  * [Buffer](https://bufferapp.com/) - Social media publishing and analytics platform. `Â©` `SaaS`  * [Topsy](http://topsy.com/) - Social analytics tool with search. `Â©` `SaaS`  * [SocialBlade](http://socialblade.com/) - premiere YouTube statistics tracking. `Â©` `SaaS`  * [Hootsuite](https://hootsuite.com/) - Social media management dashbaord. `Â©` `SaaS`  * [Sproutsocial](http://sproutsocial.com/) - Social media management and analytics platform. `Â©` `SaaS`    ## Developer analytics    * [GitSpo](https://gitspo.com/) - Analytics for Open-Source. `Â©` `SaaS`  * [Pull Panda](https://pullpanda.com/analytics) - Metrics and insights for engineering teams `Â©` `SaaS`  * [Screenful](https://screenful.com/) - Visualise and share your project progress `Â©` `SaaS`  * [Haystack](https://usehaystack.io) - Metrics and insights for engineering teams `Â©` `SaaS`  * [Pull Panda](https://pullpanda.com/analytics) - Metrics and insights for engineering teams `Â©` `SaaS`  * [Plandek](https://plandek.com) - Metrics and insights for software delivery `Â©` `SaaS`  * [Screenful](https://screenful.com/) - Visualise and share your project progress `Â©` `SaaS`  * [Moiva.io](https://moiva.io/) - A dashboard with charts and graphs to evaluate and compare any npm package. `Â©` `SaaS`    ## Analytics dashboards    * [Freeboard](https://github.com/Freeboard/freeboard) - open source real-time dashboard builder for IOT and other web mashups. `Â©` `SaaS`  * [Geckboard](https://www.geckoboard.com/) - dashboard for key metrics in one place. `Â©` `SaaS`  * [Klipfolio](https://www.klipfolio.com/) - Klipfolio is an online dashboard platform for building powerful real-time business dashboards for your team or your clients. `Â©` `SaaS`  * [Vizia](https://www.brandwatch.com/products/vizia/) - Visual command center dashboarding solution `Â©` `SaaS`  * [Metabase](https://metabase.com/) - Metabase is the easy, open source way for everyone in your company to ask questions and learn from data. Simple Dashboarding and GUI Query tool, Nightly Emails and Slack Integration w/ PostgreSQL, MySQL, Redshift and other DBs. ([Source Code](https://github.com/metabase/metabase)) `AGPL-3.0` `Java`  * [Chartbrew](https://chartbrew.com) - Chartbrew allows you to query your databases and APIs to create live charts and visualize your data. You can share your charts with anyone or embed them on your own sites, blogs, Notion, etc. ([Demo](https://app.chartbrew.com/live-demo), [Source Code](https://github.com/chartbrew/chartbrew)) `MIT` `NodeJS`  * [Redash](http://redash.io) - connect to over 18 types of databases (SQL and ""NoSQL""), query your data, visualize it and create dashboards. Everything has a URL that can be shared. Slack and HipChat integration. ([Demo](https://demo.redash.io), [Source Code](https://github.com/getredash/redash)) `BSD-2-Clause` `Python`  * [Superset](http://superset.apache.org/) - Modern, enterprise-ready business intelligence web application. ([Source Code](https://github.com/apache/incubator-superset)) `Apache-2.0` `Python`  * [Socioboard](https://socioboard.org/) - `âš ` Social media management, analytics, and reporting platform supporting nine social media networks out-of-the-box. ([Source Code](https://github.com/socioboard/Socioboard-4.0)) `GPL-3.0` `C#/JavaScript`  * [EDA](https://eda.jortilles.com/en/jortilles-english/) - EDA is an user friendly Analtical Tool specially designed for busines users.  ([Source Code](https://github.com/jortilles/EDA)) `Apache-2.0` `Angular/Nodejs`    # Other Awesome Lists  - Other awesome lists [awesome-awesomeness](https://github.com/bayandin/awesome-awesomeness).  - Even more lists [awesome](https://github.com/sindresorhus/awesome).  - Another list? [list](https://github.com/jnv/lists).  - WTF! [awesome-awesome-awesome](https://github.com/t3chnoboy/awesome-awesome-awesome).  - Analytics [awesome-bigdata](https://github.com/onurakpolat/awesome-bigdata). """
Big data;https://github.com/bayandin/awesome-awesomeness;"""# Awesome Awesomeness    A curated list of amazingly awesome awesomeness.  - Programming Languages Package Manager      - [Package-Manager](https://github.com/damon-kwok/awesome-package-manager)    - Programming Languages  	- [Ada(Spark)](https://github.com/ohenley/awesome-ada)	  	- [Ansible](https://github.com/jdauphant/awesome-ansible)  	- [AutoHotkey](https://github.com/ahkscript/awesome-AutoHotkey)  	- [AutoIt](https://github.com/J2TeaM/awesome-AutoIt)  	- [C](https://notabug.org/koz.ross/awesome-c)  	- [C/C++](https://github.com/fffaraz/awesome-cpp)  	- [CMake](https://github.com/onqtam/awesome-cmake)  	- Clojure  		- [by @mbuczko](https://github.com/mbuczko/awesome-clojure)  		- [by @razum2um](https://github.com/razum2um/awesome-clojure)  	- [ColdFusion](https://github.com/seancoyne/awesome-coldfusion)  	- Common Lisp  		- [Common Lisp Libraries](https://github.com/CodyReichert/awesome-cl)  		- [Learning Common Lisp](https://github.com/GustavBertram/awesome-common-lisp-learning-list)  	- [Coronavirus](https://github.com/soroushchehresa/awesome-coronavirus)  	- [Crystal](https://github.com/veelenga/awesome-crystal)  	- [D](https://github.com/zhaopuming/awesome-d)  	- [Delphi](https://github.com/Fr0sT-Brutal/awesome-delphi)  	- [Elixir](https://github.com/h4cc/awesome-elixir)  	- [Elm](https://github.com/isRuslan/awesome-elm)  	- Erlang  		- [by @0xAX](https://github.com/0xAX/erlang-bookmarks)  		- [by @drobakowski](https://github.com/drobakowski/awesome-erlang)  		- [by @unbalancedparentheses](https://github.com/unbalancedparentheses/spawnedshelter)  	- [F#](https://github.com/fsprojects/awesome-fsharp)  	- [Fortran](https://github.com/rabbiabram/awesome-fortran)  	- [Go](https://github.com/avelino/awesome-go)  	- [Go Patterns](https://github.com/tmrts/go-patterns)  	- [Groovy](https://github.com/kdabir/awesome-groovy)  	- [Haskell](https://github.com/krispo/awesome-haskell)  	- [Idris](https://github.com/joaomilho/awesome-idris)  	- [Java](https://github.com/akullpp/awesome-java)  	- [JavaScript](https://github.com/sorrycc/awesome-javascript)  		- [Angular 2](https://github.com/AngularClass/awesome-angular)  		- [Ember.js](https://github.com/nmec/awesome-ember)  		- [JavaScript Learning Resources](https://github.com/micromata/awesome-javascript-learning)  		- [Koa](https://github.com/ellerbrock/awesome-koa)  		- [Node.js](https://github.com/sindresorhus/awesome-nodejs)  			- [Cross-platform Node.js](https://github.com/bcoe/awesome-cross-platform-nodejs)  			- [Node ESM](https://github.com/talentlessguy/awesome-node-esm)  		- [React](https://github.com/enaqx/awesome-react)  		- [Svelte](https://github.com/flagello/awesome-sveltejs)  		- [VueJS](https://github.com/vuejs/awesome-vue)  	- [Julia](https://github.com/svaksha/Julia.jl)  	- [Kotlin](https://github.com/KotlinBy/awesome-kotlin)  	- [Kotlin/Native](https://github.com/bipinvaylu/awesome-kotlin-native)  	- Lua  		- [by @forhappy](https://github.com/forhappy/awesome-lua)  		- [by @lewisjellis](https://github.com/LewisJEllis/awesome-lua)  	- [MongoDB](https://github.com/ramnes/awesome-mongodb)  	- [MySQL](https://github.com/shlomi-noach/awesome-mysql)  	- .NET  	        - [by @mehdihadeli](https://github.com/mehdihadeli/awesome-dotnet-core-education)  		- [by @quozd](https://github.com/quozd/awesome-dotnet)  		- [by @tallesl](https://github.com/tallesl/net-libraries-that-make-your-life-easier)  		- [by @thangchung](https://github.com/thangchung/awesome-dotnet-core)  	- [Nim](https://github.com/VPashkov/awesome-nim)  	- [OCaml](https://github.com/ocaml-community/awesome-ocaml)  	- [Perl](https://github.com/hachiojipm/awesome-perl)  	- [PHP](https://github.com/ziadoz/awesome-php)  		- [CakePHP](https://github.com/FriendsOfCake/awesome-cakephp)  	- [Postgres](https://github.com/dhamaniasad/awesome-postgres)  	- Python  		- [by @kirang89](https://github.com/kirang89/pycrumbs)  		- [by @svaksha](https://github.com/svaksha/pythonidae)  		- [by @trekhleb](https://github.com/trekhleb/learn-python)  		- [by @vinta](https://github.com/vinta/awesome-python)  		- [awesome-python-in-education](https://github.com/quobit/awesome-python-in-education)  	- [R](https://github.com/qinwf/awesome-R)  	- Ruby  		- [by @dreikanter](https://github.com/dreikanter/ruby-bookmarks)  		- [by @markets](https://github.com/markets/awesome-ruby)  		- [by @Sdogruyol](https://github.com/Sdogruyol/awesome-ruby)  		- [by @asyraffff](https://github.com/asyraffff/Open-Source-Ruby-and-Rails-Apps)  	- [Rust](https://github.com/rust-unofficial/awesome-rust)  	- [SAS](https://github.com/huyingjie/awesome-SAS)  	- [Scala](https://github.com/lauris/awesome-scala)  	- [Shell](https://github.com/alebcay/awesome-shell)  	- Swift  		- [by @matteocrippa](https://github.com/matteocrippa/awesome-swift)  		- [by @MaxChen](https://github.com/MaxChen/awesome-swift-and-tutorial-resources)  		- [by @Wolg](https://github.com/Wolg/awesome-swift)  		- [from ZEEF by @Edubits](https://swift.zeef.com/robin.eggenkamp)  	- TypeScript  		- [by @brookshi](https://github.com/brookshi/awesome-typescript-projects)  		- [by @dzharii](https://github.com/dzharii/awesome-typescript)  		- [by @ellerbrock](https://github.com/ellerbrock/awesome-typescript)  	- [V](https://github.com/vlang/awesome-v)    - General  	- [.htaccess](https://github.com/phanan/htaccess)  	- Accessibility  		- [by @a11yproject](https://github.com/a11yproject/a11yproject.com)  		- [by @brunopulis](https://github.com/brunopulis/awesome-a11y)  	- [Agile](https://github.com/lorabv/awesome-agile)  	- [Algolia](https://github.com/algolia/awesome-algolia)  	- [Algorithms](https://github.com/tayllan/awesome-algorithms)  		- [Algorithms Visualisation](https://github.com/enjalot/algovis)  		- [Big O Notation](https://github.com/okulbilisim/awesome-big-o)  	- [Amazon Web Services](https://github.com/donnemartin/awesome-aws)  	- [Analytics](https://github.com/onurakpolat/awesome-analytics)  	- [Android](https://github.com/JStumpp/awesome-android)  		- [Android Apps](https://github.com/LinuxCafeFederation/awesome-android)  		- [Android Release Notes](https://github.com/pedronveloso/awesome-android-release-notes)  		- [Android Security](https://github.com/ashishb/android-security-awesome)  		- [Android UI](https://github.com/wasabeef/awesome-android-ui)  	- [ARM Exploitation](https://github.com/HenryHoggard/awesome-arm-exploitation)  	- [Software Architecture](https://github.com/simskij/awesome-software-architecture)  	- [Arduino](https://github.com/Lembed/Awesome-arduino)  	- [Artificial intelligence](https://github.com/owainlewis/awesome-artificial-intelligence)  	- API  		- [by @Kikobeats](https://github.com/Kikobeats/awesome-api)  		- [by @toddmotto](https://github.com/toddmotto/public-apis)  	- [Apple](https://github.com/joeljfischer/awesome-apple)  		- [OS X](https://github.com/iCHAIT/awesome-macOS)  		- [OS X and iOS Security](https://github.com/ashishb/osx-and-ios-security-awesome)  	- [Beacons](https://github.com/beaconinside/awesome-beacon)  	- Big data  		- [by @onurakpolat](https://github.com/onurakpolat/awesome-bigdata)  		- [by @zenkay](https://github.com/zenkay/bigdata-ecosystem)  		- [Hadoop](https://github.com/youngwookim/awesome-hadoop)  	- [Blazor](https://github.com/AdrienTorris/awesome-blazor)  	- Blockchain  		- [by @0xtokens](https://github.com/0xtokens/awesome-blockchain)  		- [by @imbaniac](https://github.com/imbaniac/awesome-blockchain)  		- [by @coderplex](https://github.com/coderplex/awesome-blockchain)  		- [by @hitripod](https://github.com/hitripod/awesome-blockchain)  		- [by @iNiKe](https://github.com/iNiKe/awesome-blockchain)  		- [by @igorbarinov](https://github.com/igorbarinov/awesome-blockchain)  		- [by @istinspring](https://github.com/istinspring/awesome-blockchain)  		- [by @openblockchains](https://github.com/openblockchains/awesome-blockchains)  		- [by @kennethreitz](https://github.com/kennethreitz/awesome-coins)  		- [awesome-token-sale](https://github.com/holographicio/awesome-token-sale)  		- Bitcoin  			- [by @btcbrdev](https://github.com/btcbrdev/awesome-btcdev)  			- [by @igorbarinov](https://github.com/igorbarinov/awesome-bitcoin)  			- [Bitcoin Payment Processors](https://github.com/alexk111/awesome-bitcoin-payment-processors)  		- Ethereum  			- [by @vinsgo](https://github.com/vinsgo/awesome-ethereum)  			- [awesome-ethereum-virtual-machine](https://github.com/pirapira/awesome-ethereum-virtual-machine)  			- [by @Tom2718](https://github.com/Tom2718/Awesome-Ethereum)  		- [Ripple](https://github.com/vhpoet/awesome-ripple)  	- [Boilerplates](https://github.com/melvin0008/awesome-projects-boilerplates)  	- Books  		- [Free Programming Books](https://github.com/EbookFoundation/free-programming-books)  		- [Free Software Testing Books](https://github.com/ligurio/free-software-testing-books)  		- [Mind Expanding Books](https://github.com/hackerkid/Mind-Expanding-Books)  	- [Bootstrap](https://github.com/therebelrobot/awesome-bootstrap)  	- [BSD Software](https://github.com/SaintFenix/Awesome-BSD-Ports-Programs-And-Projects)  	- [Building Blocks for Web Apps](https://github.com/componently-com/awesome-building-blocks-for-web-apps)  	- [Web Effect](https://github.com/lindelof/awesome-web-effect)  	- [Landing Page](https://github.com/nordicgiant2/awesome-landing-page)  	- [Capacitor](https://github.com/riderx/awesome-capacitor)  	- [Captcha](https://github.com/ZYSzys/awesome-captcha)  	- [Challenges](https://github.com/mauriciovieira/awesome-challenges)  	- [Code Formatters](https://github.com/rishirdua/awesome-code-formatters)  	- [Community Detection](https://github.com/benedekrozemberczki/awesome-community-detection)  	- [Competitive Programming](https://github.com/lnishan/awesome-competitive-programming)  	- [Computer Vision](https://github.com/jbhuang0604/awesome-computer-vision)  	- [Conferences](https://github.com/RichardLitt/awesome-conferences)  	- [Continuous Delivery](https://github.com/ciandcd/awesome-ciandcd)  	- [Conversational UI](https://github.com/mortenjust/awesome-conversational/)  	- [Cordova](https://github.com/busterc/awesome-cordova)  	- [Courses](https://github.com/prakhar1989/awesome-courses)  	- [Creative Commons Media](https://github.com/shime/creative-commons-media)  	- Cryptography  		- [by @MaciejCzyzewski](https://github.com/MaciejCzyzewski/retter)  		- [by @sobolevn](https://github.com/sobolevn/awesome-cryptography)  		- [by @coinpride](https://github.com/coinpride/CryptoList)  	- [Crypto Papers](https://github.com/pFarb/awesome-crypto-papers)  	- [CSS](https://github.com/sotayamashita/awesome-css)  		- [CSS Frameworks](https://github.com/troxler/awesome-css-frameworks)  	- [Data Science](https://github.com/bulutyazilim/awesome-datascience)  		- [Notebooks](https://github.com/jupyter-naas/awesome-notebooks)   		- Data Science with Python  		  	- [by @r0f1](https://github.com/r0f1/datascience)  			- [by @krzjoa](https://github.com/krzjoa/awesome-python-data-science)  	- [Data Visualization](https://github.com/fasouto/awesome-dataviz)  	- [Database](https://github.com/numetriclabz/awesome-db)  		- [SQLAlchemy](https://github.com/dahlia/awesome-sqlalchemy)  	- Datasets  		- [by @caesar0301](https://github.com/caesar0301/awesome-public-datasets)  		- [by @leomaurodesenv](https://github.com/leomaurodesenv/game-datasets)  	- Deep Learning  		- [by @ChristosChristofidis](https://github.com/ChristosChristofidis/awesome-deep-learning)  		- [by @guillaume-chevalier](https://github.com/guillaume-chevalier/awesome-deep-learning-resources)  		- [by @tigerneil](https://github.com/tigerneil/awesome-deep-rl)  		- [by @nerox8664](https://github.com/nerox8664/awesome-computer-vision-models)  	- [Decision Tree Papers](https://github.com/benedekrozemberczki/awesome-decision-tree-papers)  	- [Design Patterns](https://github.com/DovAmir/awesome-design-patterns)  	- [Design Tools](https://github.com/LisaDziuba/Awesome-Design-Tools)  	- [Design](https://github.com/gztchan/awesome-design)  	- [Dev Env](https://github.com/jondot/awesome-devenv)  	- [DevOps](https://github.com/joubertredrat/awesome-devops)  	- [DevSecOps](https://github.com/TaptuIT/awesome-devsecops)  	- [Django](https://github.com/wsvincent/awesome-django)  	- [Docker](https://github.com/veggiemonk/awesome-docker)  	- [Documentation](https://github.com/PharkMillups/beautiful-docs)  	- [Dotfiles](https://github.com/webpro/awesome-dotfiles)  	- [Electron](https://github.com/sindresorhus/awesome-electron)  	- [Emacs](https://github.com/emacs-tw/awesome-emacs)  	- [Embedded](https://github.com/nhivp/Awesome-Embedded)  	- [Ethics](https://github.com/HussainAther/awesome-ethics)  	- [Falsehood](https://github.com/kdeldycke/awesome-falsehood)  	- [FastAPI](https://github.com/mjhea0/awesome-fastapi)  	- [FIRST Robotics Competition](https://github.com/andrewda/awesome-frc)  	- [Flask](https://github.com/mjhea0/awesome-flask)  	- [FluidApp Resources](https://github.com/lborgav/awesome-fluidapp)  	- [Flutter](https://github.com/Solido/awesome-flutter)  	- [Fonts](https://github.com/brabadu/awesome-fonts)  	- [Free Open Source Software (FOSS)](https://github.com/ishanvyas22/awesome-open-source-systems)  	- [Fraud Detection Papers](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers)  	- [Free Services](https://github.com/ripienaar/free-for-dev)  	- Frontend  		- [by @dypsilon](https://github.com/dypsilon/frontend-dev-bookmarks)  		- [by @moklick](https://github.com/moklick/frontend-stuff)  	- [Game Development](https://github.com/ellisonleao/magictools)  	- [Games](https://github.com/leereilly/games)  	- GIF  		- [by @Kikobeats](https://github.com/Kikobeats/awesome-gif)  	- [Gists](https://github.com/vsouza/awesome-gists)  	- [Git](https://github.com/dictcp/awesome-git)  	- [GitHub](https://github.com/Kikobeats/awesome-github)  		- [Browser extensions for GitHub](https://github.com/stefanbuck/awesome-browser-extensions-for-github)  		- [GitHub - Chinese](https://github.com/AntBranch/awesome-github)  	- [Gradient Boosting Papers](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers)  	- [Graph Classification](https://github.com/benedekrozemberczki/awesome-graph-classification)  	- [GraphQL](https://github.com/chentsulin/awesome-graphql)  	- [Growth Hacking](https://github.com/btomashvili/awesome-growth-hacking)  	- Guides  		- [by @narkoz](https://github.com/narkoz/guides)  		- [by @RichardLitt](https://github.com/RichardLitt/awesome-styleguides)  	- Hacking  		- [by @carpedm20](https://github.com/carpedm20/awesome-hacking)  		- [by @Hack-with-Github](https://github.com/Hack-with-Github/Awesome-Hacking)  	- [HTML5](https://github.com/diegocard/awesome-html5)  	- [Honeypots](https://github.com/paralax/awesome-honeypots)  	- [Hyper](https://github.com/bnb/awesome-hyper)  	- [Incident Response](https://github.com/meirwah/awesome-incident-response)  	- [Images](https://github.com/heyalexej/awesome-images)  	- [Image coloring](https://github.com/oskar-j/awesome-image-coloring)  	- [Internationalization](https://github.com/jpomykala/awesome-i18n)  	- [Internet of Things (IOT)](https://github.com/HQarroum/awesome-iot)  	- [iOS](https://github.com/vsouza/awesome-ios)  		- [Cocoa Controls](https://github.com/v-braun/awesome-cocoa)  		- [Open Source Apps](https://github.com/dkhamsing/open-source-ios-apps)  		- [UI](https://github.com/cjwirth/awesome-ios-ui)  	- [JSON](https://github.com/burningtree/awesome-json)  	- [Jupyter](https://github.com/markusschanta/awesome-jupyter)  	- [JVM](https://github.com/deephacks/awesome-jvm)  	- [Kafka](https://github.com/monksy/awesome-kafka)  	- [Koans](https://github.com/ahmdrefat/awesome-koans)  	- [Laravel](https://github.com/chiraggude/awesome-laravel)  	- [Leadership and Management](https://github.com/LappleApple/awesome-leading-and-managing)  	- [Lego](https://github.com/adius/awesome-lego)  	- [Linux Containers](https://github.com/Friz-zy/awesome-linux-containers)  	- [Linux resources](https://github.com/itech001/awesome-linux-resources)  	- Lists  		- [by @bayandin](https://github.com/bayandin/awesome-awesomeness)  		- [by @jnv](https://github.com/jnv/lists)  		- [by @sindresorhus](https://github.com/sindresorhus/awesome)  	- [Mac]  		- [by @xyNNN](https://github.com/xyNNN/awesome-mac)  		- [by @justin-j](https://github.com/justin-j/awesome-mac-apps)  	- [Machine Learning](https://github.com/josephmisiti/awesome-machine-learning)  	- [Malware Analysis](https://github.com/rshipp/awesome-malware-analysis)  	- [Material Design](https://github.com/sachin1092/awesome-material)  	- [Math](https://github.com/rossant/awesome-math)  	- [Matlab](https://github.com/mikecroucher/awesome-MATLAB)  	- [Mental Health](https://github.com/dreamingechoes/awesome-mental-health)  	- [micro:bit](https://github.com/carlosperate/awesome-microbit)  	- [MLOps](https://github.com/kelvins/awesome-mlops)  	- [Mobile marketing and development](https://github.com/alec-c4/awesome-mobile)  	- [Mobile Web Development](https://github.com/myshov/awesome-mobile-web-development)  	- [Monitoring](https://github.com/crazy-canux/awesome-monitoring)  		- [Prometheus](https://github.com/roaldnefs/awesome-prometheus)  		- [Prometheus alerting rules](https://github.com/samber/awesome-prometheus-alerts)          - [Monte Carlo Tree Search Papers](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers)  	- [Motion Design for Web](https://github.com/lucasmaiaesilva/awesome-motion-design-web)  	- [Nginx](https://github.com/fcambus/nginx-resources)  	- Newsletters  		- [by @vredniy](https://github.com/vredniy/awesome-newsletters)  		- [by @webpro](https://github.com/webpro/awesome-newsletters)  		- [by @mpron](https://github.com/mpron/awesome-newsletters)  	- [No Login Web Apps](https://github.com/aviaryan/awesome-no-login-web-apps)  	- [Open Science](https://github.com/silky/awesome-open-science)  	- [Open Source Photography](https://github.com/ibaaj/awesome-OpenSourcePhotography)  	- [Papers](https://github.com/papers-we-love/papers-we-love)  	- [Podcasts](https://github.com/Ghosh/awesome-podcasts)  	- [Philosophy](https://github.com/HussainAther/awesome-philosophy)  	- [Pipelines](https://github.com/pditommaso/awesome-pipeline)  	- [Product Manager](https://github.com/hugo53/awesome-ProductManager)  	- Protocols  		- [OSC](https://github.com/amir-arad/awesome-osc) (open sound control)  	- [Pentest Cheat Sheets](https://github.com/coreb1t/awesome-pentest-cheat-sheets)  	- [Quick Look Plugins](https://github.com/sindresorhus/quick-look-plugins)  	- [Random-Forest](https://github.com/kjw0612/awesome-random-forest)  	- Raspberry Pi  		- [by @blackout314](https://github.com/blackout314/awesome-raspberry-pi)  		- [by @thibmaek](https://github.com/thibmaek/awesome-raspberry-pi)  	- [React Native](https://github.com/jondot/awesome-react-native)  	- [README](https://github.com/matiassingers/awesome-readme)  	- [Regex](https://github.com/aloisdg/awesome-regex)  	- [Remote Job](https://github.com/lukasz-madon/awesome-remote-job)  	- [Remote Work](https://github.com/hugo53/awesome-RemoteWork)  	- [REST](https://github.com/marmelab/awesome-rest)  	- [Robotics](https://github.com/Kiloreux/awesome-robotics)  	- [Robotic Tooling](https://github.com/protontypes/awesome-robotic-tooling)  	- [RNN](https://github.com/kjw0612/awesome-rnn)  	- [Scalability](https://github.com/binhnguyennus/awesome-scalability)  	- [Science Fiction](https://github.com/sindresorhus/awesome-scifi)  	- Search Engine Optimization (SEO)  		- [by @marcobiedermann](https://github.com/marcobiedermann/search-engine-optimization)  		- [by @sneg55](https://github.com/sneg55/curatedseotools)  		- [by @teles](https://github.com/teles/awesome-seo)  	- [Security](https://github.com/sbilly/awesome-security)  	- [Selfhosted](https://github.com/Kickball/awesome-selfhosted)  	- [Serverless](https://github.com/anaibol/awesome-serverless)  	- [Serverless Security](https://github.com/puresec/awesome-serverless-security/)  	- [Service Fabric](https://github.com/lawrencegripper/awesome-servicefabric)  	- [Services Engineering](https://github.com/mmcgrana/services-engineering)  	- [Sheet Music](https://github.com/adius/awesome-sheet-music)  	- [Slack](https://github.com/matiassingers/awesome-slack)  	- [Sound](https://github.com/hwclass/awesome-sound)  	- [Space](https://github.com/elburz/awesome-space)  		- [Books and manuals](https://github.com/Hunter-Github/awesome-space-books)  	- [Speech and Natural Language Processing](https://github.com/edobashira/speech-language-processing)  		- [NLP with Ruby](https://github.com/arbox/nlp-with-ruby)  	- [Sphinx Documentation](https://github.com/yoloseem/awesome-sphinxdoc)  	- [Startup](https://github.com/KrishMunot/awesome-startup)  	- [Static Analysis](https://github.com/mre/awesome-static-analysis/)  	- [Styleguides](https://github.com/RichardLitt/awesome-styleguides)  	- [Sublime Text](https://github.com/dreikanter/sublime-bookmarks)  	- [Sustainable Technology](https://github.com/protontypes/awesome-sustainable-technology)  	- [SVG](https://github.com/willianjusten/awesome-svg)  	- [Swedish](https://github.com/gurre/awesome-swedish-opensource)  	- [Sysadmin](https://github.com/kahun/awesome-sysadmin)  	- [Taglines](https://github.com/miketheman/awesome-taglines)  	- [Tailwind CSS](https://github.com/aniftyco/awesome-tailwindcss)  	- [Talks](https://github.com/JanVanRyswyck/awesome-talks)  		- [Gaming](https://github.com/hzoo/awesome-gametalks)  	- [Telegram](https://github.com/ebertti/awesome-telegram)  	- [Terminals Are Sexy](https://github.com/k4m4/terminals-are-sexy)  	- [Test Automation](https://github.com/atinfo/awesome-test-automation)  	- [Testing](https://github.com/TheJambo/awesome-testing)  		- [JMeter](https://github.com/aliesbelik/awesome-jmeter)  	- [Threat Intelligence](https://github.com/hslatman/awesome-threat-intelligence)  	- [Tools](https://github.com/cjbarber/ToolsOfTheTrade)  	- [Twilio](https://github.com/Twilio-org/awesome-twilio)  	- [Unity](https://github.com/RyanNielson/awesome-unity)  	- [UI Styleguide](https://github.com/kevinwuhoo/ui-styleguides)  		- [UI Components for Styleguide](https://github.com/anubhavsrivastava/awesome-ui-component-library)  	- [UNIX](https://github.com/sirredbeard/Awesome-UNIX)  	- [Vagrant](https://github.com/iJackUA/awesome-vagrant)  	- [Vehicle Security](https://github.com/jaredthecoder/awesome-vehicle-security)  	- Vim  		- [by @akrawchyk](https://github.com/akrawchyk/awesome-vim)  		- [by @matteocrippa](https://github.com/matteocrippa/awesome-vim)  	- [Vulkan](https://github.com/vinjn/awesome-vulkan)  	- [Web Performance Optimization](https://github.com/davidsonfellipe/awesome-wpo)  	- [WebComponents](https://github.com/mateusortiz/webcomponents-the-right-way)  	- [Wordpress](https://github.com/miziomon/awesome-wordpress)  	- [Workshops](https://github.com/therebelrobot/awesome-workshopper)  	- [Xamarin](https://github.com/benoitjadinon/awesome-xamarin)  	- XMPP  		- [Ejabberd](https://github.com/shantanu-deshmukh/awesome-ejabberd)  	- [Typography](https://github.com/Jolg42/awesome-typography)    ## License    [![Creative Commons License](http://i.creativecommons.org/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0/)    This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/). """
Big data;https://github.com/WeBankFinTech/DataSphereStudio;"""![DSS](images/en_US/readme/DSS_logo.png)  ====    [![License](https://img.shields.io/badge/license-Apache%202-4EB1BA.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)    English | [ä¸­æ–‡](README-ZH.md)    ## Introduction     &nbsp; &nbsp; &nbsp; &nbsp;DataSphere Studio (DSS for short) is WeDataSphere, a one-stop data application development management portal developed by WeBank.     &nbsp; &nbsp; &nbsp; &nbsp;With the pluggable integrated framework design and the Linkis, a computing middleware, DSS can easily integrate  various upper-layer data application systems, making data development simple and easy to use.     &nbsp; &nbsp; &nbsp; &nbsp;DataSphere Studio is positioned as a data application development portal, and the closed loop covers the entire process of data application development. With a unified UI, the workflow-like graphical drag-and-drop development experience meets the entire lifecycle of data application development from data import, desensitization cleaning, data analysis, data mining, quality inspection, visualization, scheduling to data output applications, etc.     &nbsp; &nbsp; &nbsp; &nbsp;With the connection, reusability, and simplification capabilities of Linkis, DSS is born with financial-grade capabilities of high concurrency, high availability, multi-tenant isolation, and resource management.    ## UI preview     &nbsp; &nbsp; &nbsp; &nbsp;Please be patient, it will take some time to load gif.    ![DSS-V1.0 GIF](images/en_US/readme/DSS_gif.gif)    ## Core features    ### 1. One-stop, full-process application development management UI     &nbsp; &nbsp; &nbsp; &nbsp;DSS is highly integrated. Currently integrated components include(**DSS version compatibility for the above components, please visit: [Compatibility list of integrated components](README.md#4-integrated-data-application-components)**):     &nbsp; &nbsp; &nbsp; &nbsp;1. Data Development IDE Tool - [Scriptis](https://github.com/WeBankFinTech/Scriptis)     &nbsp; &nbsp; &nbsp; &nbsp;2. Data Visualization Tool - [Visualis](https://github.com/WeBankFinTech/Visualis) (Based on the open source project [Davinci](https://github.com/edp963/davinci ) contributed by CreditEase)     &nbsp; &nbsp; &nbsp; &nbsp;3. Data Quality Management Tool - [Qualitis](https://github.com/WeBankFinTech/Qualitis)     &nbsp; &nbsp; &nbsp; &nbsp;4. Workflow scheduling tool - [Schedulis](https://github.com/WeBankFinTech/Schedulis)     &nbsp; &nbsp; &nbsp; &nbsp;5. Data Exchange Tool - [Exchangis](https://github.com/WeBankFinTech/Exchangis) (**The upcoming Exchangis1.0 will be integrated with the DSS workflow**)     &nbsp; &nbsp; &nbsp; &nbsp;6. Data Api Service - [DataApiService](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Using_Document/DataApiService_Usage_Documentation.md)     &nbsp; &nbsp; &nbsp; &nbsp;7. Streaming Application Development Management Tool - [Streamis](https://github.com/WeBankFinTech/Streamis)     &nbsp; &nbsp; &nbsp; &nbsp;8. One-stop machine Learning Platform - [Prophecis](https://github.com/WeBankFinTech/Prophecis) (**Integrated version will be released soon**)     &nbsp; &nbsp; &nbsp; &nbsp;9. Workflow Task Scheduling Tool - DolphinScheduler (**In Code Merging**)      &nbsp; &nbsp; &nbsp; &nbsp;10. Help documentation and beginner's guide - UserGuide (**In Code Merging**)     &nbsp; &nbsp; &nbsp; &nbsp;11. Data Model Center - DataModelCenter (**In development**)      &nbsp; &nbsp; &nbsp; &nbsp;**DSS version compatibility for the above components, please visit: [Compatibility list of integrated components](README.md#4-integrated-data-application-components)**.     &nbsp; &nbsp; &nbsp; &nbsp;With a pluggable framework architecture, DSS is designed to allow users to quickly integrate new data application tools, or replace various tools that DSS has integrated. For example, replace Scriptis with Zeppelin, and replace Schedulis with DolphinScheduler...    ![DSS one-stop video](images/en_US/readme/onestop.gif)     ### 2. AppConn, based on Linkisï¼Œdefines a unique design concept     &nbsp; &nbsp; &nbsp; &nbsp;AppConn is the core concept that enables DSS to easily and quickly integrate various upper-layer web systems.     &nbsp; &nbsp; &nbsp; &nbsp;AppConn, an application connector, defines a set of unified front-end and back-end three-level integration protocols, allowing external data application systems to easily and quickly becoming a part of DSS data application development.      &nbsp; &nbsp; &nbsp; &nbsp;The three-level specifications of AppConn are: the first-level SSO specification, the second-level organizational structure specification, and the third-level development process specification.     &nbsp; &nbsp; &nbsp; &nbsp;DSS arranges multiple AppConns in series to form a workflow that supports real-time execution and scheduled execution. Users can complete the entire process development of data applications with simple drag and drop operations.     &nbsp; &nbsp; &nbsp; &nbsp;Since AppConn is integrated with Linkis, the external data application system shares the capabilities of resource management, concurrent limiting, and high performance. AppConn also allows sharable context across system level and thus makes external data application completely gets away from application silos.    ### 3. Workspace, as the management unit     &nbsp; &nbsp; &nbsp; &nbsp;With Workspace as the management unit, it organizes and manages business applications of various data application systems, defines a set of common standards for collaborative development of workspaces across data application systems, and provides user role management capabilities.    ### 4. Integrated data application components     &nbsp; &nbsp; &nbsp; &nbsp;DSS has integrated a variety of upper-layer data application systems by implementing multiple AppConns, which can basically meet the data development needs of users.     &nbsp; &nbsp; &nbsp; &nbsp;**If desired, new data application systems can also be easily integrated to replace or enrich DSS's data application development process.** [Click me to learn how to quickly integrate new application systems](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Development_Documentation/Third-party_System_Access_Development_Guide.md)    | Component | Description | DSS0.X Compatibility(recommend DSS0.9.1) | DSS1.0 Compatibility(recommend DSS1.0.1) |  | --------------- | -------------------------------------------------------------------- | --------- | ---------- |  | [**Linkis**](https://github.com/apache/incubator-linkis) | Apache Linkis, builds a layer of computation middleware, by using standard interfaces such as REST/WS/JDBC provided by Linkis, the upper applications can easily access the underlying engines such as MySQL/Spark/Hive/Presto/Flink, etc.  | recommend Linkis0.11.0 (**Released**) | recommend Linkis1.0.3 (**Released**)|  | [**DataApiService**](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Using_Document/DataApiService_Usage_Documentation.md) | (Third-party applications built into DSS) Data API service. The SQL script can be quickly published as a Restful interface, providing Rest access capability to the outside world. | Not supported | recommend DSS1.0.1 (**Released**) |  | [**Scriptis**](https://github.com/WeBankFinTech/DataSphereStudio) | (Third-party applications built into DSS) Support online script writing such as SQL, Pyspark, HiveQL, etc., submit to [Linkis](https://github.com/WeBankFinTech/Linkis ) to perform data analysis web tools. | recommend DSS0.9.1 (**Released**) | recommend DSS1.0.1 (**Released**) |  | [**Schedulis**](https://github.com/WeBankFinTech/Schedulis) | Workflow task scheduling system based on Azkaban secondary development, with financial-grade features such as high performance, high availability and multi-tenant resource isolation. | recommend Schedulis0.6.1 (**Released**) | >= Schedulis0.6.2 (**Released**) |  | **EventCheck** | (Third-party applications built into DSS) Provides cross-business, cross-engineering, and cross-workflow signaling capabilities. | recommend DSS0.9.1 (**Released**) | recommend DSS1.0.1 (**Released**) |  | **SendEmail** | (Third-party applications built into DSS) Provides the ability to send data, all the result sets of other workflow nodes can be sent by email | recommend 0.9.1 (**Released**) | recommend 1.0.1 (**Released**) |  | [**Qualitis**](https://github.com/WeBankFinTech/Qualitis) | Data quality verification tool, providing data verification capabilities such as data integrity and correctness | recommend Qualitis0.8.0 (**Released**) | >= Qualitis0.9.1 (**Released**) |  | [**Streamis**](https://github.com/WeBankFinTech/Streamis) | Streaming application development management tool. It supports the release of Flink Jar and Flink SQL, and provides the development, debugging and production management capabilities of streaming applications, such as: start-stop, status monitoring, checkpoint, etc. | Not supported | >= Streamis0.1.0 (**Released**) |  | [**Prophecis**](https://github.com/WeBankFinTech/Prophecis) | A one-stop machine learning platform that integrates multiple open source machine learning frameworks. Prophecis' MLFlow can be connected to DSS workflow through AppConn. | Not supported | >= Prophecis 0.3.0 (**Released**) |  | [**Exchangis**](https://github.com/WeBankFinTech/Exchangis) | A data exchange platform that supports data transmission between structured and unstructured heterogeneous data sources, the upcoming Exchangis1. 0, will be connected with DSS workflow | not supported | >= Exchangis1.0.0 (**Developing**) |  | [**Visualis**](https://github.com/WeBankFinTech/Visualis) | A data visualization BI tool based on the second development of Davinci, an open source project of CreditEase, provides users with financial-level data visualization capabilities in terms of data security. | recommend Visualis0.5.0 (**Released**) | >= Visualis1.0.0 (**Developing**) |  | **UserManager** | (Third-party applications built into DSS) Automatically initialize all user environments necessary for a new DSS user, including: creating Linux users, various user paths, directory authorization, etc. | recommend DSS0.9.1 (**Released**) | Planned in DSS1.0.2 (**Developing**) |  | [**DolphinScheduler**](https://github.com/apache/dolphinscheduler) | Apache DolphinScheduler, a distributed and scalable visual workflow task scheduling platform, supports one-click publishing of DSS workflows to DolphinScheduler. | Not supported | >= DolphinScheduler1.3.6, planned in DSS1.1.0 (**Developing**) |  | **UserGuide**     | (Third-party applications to be built into DSS) It mainly provides help documentation, beginner's guide, Dark mode skinning, etc.      | Not supported | Planning in DSS1.1.0 (**Developing**) |  | **DataModelCenter** | (Third-party applications to be built into DSS) It mainly provides the capabilities of data warehouse planning, data model development and data asset management. Data warehouse planning includes subject domains, data warehouse layers, modifiers, etc.; data model development includes indicators, dimensions, metrics, wizard-based table building, etc.; data assets are connected to Apache Atlas to provide data lineage capabilities. | Not supported | Planning in DSS1.2.0 (**Developing**) |  | [**Airflow**](https://github.com/apache/airflow) | Supports publishing DSS workflows to Apache Airflow for scheduling. | recommend DSS0.9.1, not yet merged | Not supported |      ## Demo Trial environment     &nbsp; &nbsp; &nbsp; &nbsp;The function of DataSphere Studio supporting script execution has high security risks, and the isolation of the WeDataSphere Demo environment has not been completed. Considering that many users are inquiring about the Demo environment, we decided to first issue invitation codes to the community and accept trial applications from enterprises and organizations.     &nbsp; &nbsp; &nbsp; &nbsp;If you want to try out the Demo environment, please join the DataSphere Studio community user group (**Please refer to the end of the document**), and contact **WeDataSphere Group Robot** to get an invitation code.     &nbsp; &nbsp; &nbsp; &nbsp;DataSphereStudio Demo environment user registration page: [click me to enter](https://dss-open.wedatasphere.com/#/register)     &nbsp; &nbsp; &nbsp; &nbsp;DataSphereStudio Demo environment login page: [click me to enter](https://dss-open.wedatasphere.com/#/login)    ##  Download     &nbsp; &nbsp; &nbsp; &nbsp;Please go to the [DSS Releases Page](https://github.com/WeBankFinTech/DataSphereStudio/releases) to download a compiled version or a source code package of DSS.    ## Compile and deploy     &nbsp; &nbsp; &nbsp; &nbsp;Please follow [Compile Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Development_Documentation/Compilation_Documentation.md) to compile DSS from source code.     &nbsp; &nbsp; &nbsp; &nbsp;Please refer to [Deployment Documents](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/DSS_Single-Server_Deployment_Documentation.md) to do the deployment.    ## Examples and Guidance     &nbsp; &nbsp; &nbsp; &nbsp;You can find examples and guidance for how to use DSS in [User Manual](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Using_Document/DSS_User_Manual.md).      ## Documents     &nbsp; &nbsp; &nbsp; &nbsp;For a complete list of documents for DSS1.0, see [DSS-Doc](https://github.com/WeBankFinTech/DataSphereStudio-Doc)     &nbsp; &nbsp; &nbsp; &nbsp;The following is the installation guide for DSS-related AppConn plugins:    - [Visualis AppConn Plugin Installation Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/VisualisAppConn_Plugin_Installation_Documentation.md)    - [Schedulis AppConn Plugin Installation Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/SchedulisAppConn_Plugin_Installation_Documentation.md)    - [Qualitis AppConn Plugin Installation Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/QualitisAppConn_Plugin_Installation_Documentation.md)    - [Exchangis AppConn Plugin Installation Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/ExchangisAppConn_Plugin_Installation_Documentation.md)    ## Architecture    ![DSS Architecture](images/en_US/readme/architecture.png)    ## Usage Scenarios     &nbsp; &nbsp;&nbsp; &nbsp;DataSphere Studio is suitable for the following scenarios:     &nbsp; &nbsp;&nbsp; &nbsp;1. Scenarios in which big data platform capability is being prepared or initialized but no data application tools are available.     &nbsp; &nbsp;&nbsp; &nbsp;2. Scenarios in which users already have big data foundation platform capabilities but with only a few data application tools.     &nbsp; &nbsp;&nbsp; &nbsp;3. Scenarios in which users have the ability of big data foundation platform and comprehensive data application tools, but suffers strong isolation and and high learning costs because those tools have not been integrated together.     &nbsp; &nbsp;&nbsp; &nbsp;4. Scenarios in which users have the capabilities of big data foundation platform and comprehensive data application tools. but lacks unified and standardized specifications, while a part of these tools have been integrated.    ## Contributing     &nbsp; &nbsp; &nbsp; &nbsp;Contributions are always welcomed, we need more contributors to build DSS together. either code, or doc, or other supports that could help the community.     &nbsp; &nbsp; &nbsp; &nbsp;For code and documentation contributions, please follow the contribution guide.    ## Communication     &nbsp; &nbsp; &nbsp; &nbsp;For any questions or suggestions, please kindly submit an issue.     &nbsp; &nbsp; &nbsp; &nbsp;You can scan the QR code below to join our WeChat and QQ group to get more immediate response.    ![communication](images/en_US/readme/communication.png)    ## Who is using DSS     &nbsp; &nbsp; &nbsp; &nbsp;We opened an issue for users to feedback and record who is using DSS.     &nbsp; &nbsp; &nbsp; &nbsp;Since the first release of DSS in 2019, it has accumulated more than 700 trial companies and 1000+ sandbox trial users, which involving diverse industries, from finance, banking, tele-communication, to manufactory, internet companies and so on.    ## License     &nbsp; &nbsp; &nbsp; &nbsp;DSS is under the Apache 2.0 license. See the [License](LICENSE) file for details. """
Big data;https://github.com/pingcap/tidb;"""![](docs/logo_with_text.png)    [![LICENSE](https://img.shields.io/github/license/pingcap/tidb.svg)](https://github.com/pingcap/tidb/blob/master/LICENSE)  [![Language](https://img.shields.io/badge/Language-Go-blue.svg)](https://golang.org/)  [![Build Status](https://travis-ci.org/pingcap/tidb.svg?branch=master)](https://travis-ci.org/pingcap/tidb)  [![Go Report Card](https://goreportcard.com/badge/github.com/pingcap/tidb)](https://goreportcard.com/report/github.com/pingcap/tidb)  [![GitHub release](https://img.shields.io/github/tag/pingcap/tidb.svg?label=release)](https://github.com/pingcap/tidb/releases)  [![GitHub release date](https://img.shields.io/github/release-date/pingcap/tidb.svg)](https://github.com/pingcap/tidb/releases)  [![CircleCI Status](https://circleci.com/gh/pingcap/tidb.svg?style=shield)](https://circleci.com/gh/pingcap/tidb)  [![Coverage Status](https://codecov.io/gh/pingcap/tidb/branch/master/graph/badge.svg)](https://codecov.io/gh/pingcap/tidb)  [![GoDoc](https://img.shields.io/badge/Godoc-reference-blue.svg)](https://godoc.org/github.com/pingcap/tidb)    ## What is TiDB?    TiDB (""Ti"" stands for Titanium) is an open-source NewSQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads. It is MySQL compatible and features horizontal scalability, strong consistency, and high availability.    - __Horizontal Scalability__        TiDB expands both SQL processing and storage by simply adding new nodes. This makes infrastructure capacity planning both easier and more cost-effective than traditional relational databases which only scale vertically.    - __MySQL Compatible Syntax__        TiDB acts like it is a MySQL 5.7 server to your applications. You can continue to use all of the existing MySQL client libraries, and in many cases, you will not need to change a single line of code in your application. Because TiDB is built from scratch, not a MySQL fork, please check out the list of [known compatibility differences](https://docs.pingcap.com/tidb/stable/mysql-compatibility).    - __Distributed Transactions__        TiDB internally shards table into small range-based chunks that we refer to as ""Regions"". Each Region defaults to approximately 100 MiB in size, and TiDB uses an [optimized](https://pingcap.com/blog/async-commit-the-accelerator-for-transaction-commit-in-tidb-5.0) Two-phase commit to ensure that Regions are maintained in a transactionally consistent way.    - __Cloud Native__        TiDB is designed to work in the cloud -- public, private, or hybrid -- making deployment, provisioning, operations, and maintenance simple.        The storage layer of TiDB, called TiKV, is a [Cloud Native Computing Foundation (CNCF) Graduated](https://www.cncf.io/announcements/2020/09/02/cloud-native-computing-foundation-announces-tikv-graduation/) project. The architecture of the TiDB platform also allows SQL processing and storage to be scaled independently of each other in a very cloud-friendly manner.    - __Minimize ETL__        TiDB is designed to support both transaction processing (OLTP) and analytical processing (OLAP) workloads. This means that while you may have traditionally transacted on MySQL and then Extracted, Transformed and Loaded (ETL) data into a column store for analytical processing, this step is no longer required.    - __High Availability__        TiDB uses the Raft consensus algorithm to ensure that data is highly available and safely replicated throughout storage in Raft groups. In the event of failure, a Raft group will automatically elect a new leader for the failed member, and self-heal the TiDB cluster without any required manual intervention. Failure and self-healing operations are also transparent to applications.    For more details and latest updates, see [TiDB docs](https://docs.pingcap.com/tidb/stable) and [release notes](https://docs.pingcap.com/tidb/dev/release-notes).    ## Community    You can join these groups and chats to discuss and ask TiDB related questions:    - [TiDB Internals Forum](https://internals.tidb.io/)  - [Slack Channel](https://slack.tidb.io/invite?team=tidb-community&channel=everyone&ref=pingcap-tidb)  - [TiDB User Group Forum (Chinese)](https://asktug.com)    In addition, you may enjoy following:    - [@PingCAP](https://twitter.com/PingCAP) on Twitter  - Question tagged [#tidb on StackOverflow](https://stackoverflow.com/questions/tagged/tidb)  - The PingCAP Team [English Blog](https://en.pingcap.com/blog) and [Chinese Blog](https://pingcap.com/blog-cn/)    For support, please contact [PingCAP](http://bit.ly/contact_us_via_github).    ## Quick start    ### To start using TiDB Cloud    We provide TiDB Cloud - a fully-managed Database as a Service for you. You can [sign up](https://tidbcloud.com/signup) and get started with TiDB Cloud Developer Tier for free.    See [TiDB Cloud Quick Start](https://docs.pingcap.com/tidbcloud/tidb-cloud-quickstart).    ### To start using TiDB    See [Quick Start Guide](https://docs.pingcap.com/tidb/stable/quick-start-with-tidb).    ### To start developing TiDB    See [Get Started](https://pingcap.github.io/tidb-dev-guide/get-started/introduction.html) chapter of [TiDB Dev Guide](https://pingcap.github.io/tidb-dev-guide/index.html).    ## Contributing    The [community repository](https://github.com/pingcap/community) hosts all information about the TiDB community, including how to contribute to TiDB, how TiDB community is governed, how special interest groups are organized, etc.    [<img src=""docs/contribution-map.png"" alt=""contribution-map"" width=""180"">](https://github.com/pingcap/tidb-map/blob/master/maps/contribution-map.md#tidb-is-an-open-source-distributed-htap-database-compatible-with-the-mysql-protocol)    Contributions are welcomed and greatly appreciated. See [Contribution to TiDB](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/introduction.html) for details on typical contribution workflows. For more contributing information, click on the contributor icon above.    ## Adopters    View the current list of in-production TiDB adopters [here](https://docs.pingcap.com/tidb/stable/adopters).    ## Case studies    - [English](https://pingcap.com/case-studies)  - [ç®€ä½“ä¸­æ–‡](https://pingcap.com/cases-cn/)    ## Architecture    ![architecture](./docs/architecture.png)    ## License    TiDB is under the Apache 2.0 license. See the [LICENSE](./LICENSE) file for details.    ## Acknowledgments    - Thanks [cznic](https://github.com/cznic) for providing some great open source tools.  - Thanks [GolevelDB](https://github.com/syndtr/goleveldb), [BoltDB](https://github.com/boltdb/bolt), and [RocksDB](https://github.com/facebook/rocksdb) for their powerful storage engines. """
Big data;https://github.com/chrislusf/seaweedfs;"""# SeaweedFS      [![Slack](https://img.shields.io/badge/slack-purple)](https://join.slack.com/t/seaweedfs/shared_invite/enQtMzI4MTMwMjU2MzA3LTEyYzZmZWYzOGQ3MDJlZWMzYmI0OTE4OTJiZjJjODBmMzUxNmYwODg0YjY3MTNlMjBmZDQ1NzQ5NDJhZWI2ZmY)  [![Twitter](https://img.shields.io/twitter/follow/seaweedfs.svg?style=social&label=Follow)](https://twitter.com/intent/follow?screen_name=seaweedfs)  [![Build Status](https://img.shields.io/github/workflow/status/chrislusf/seaweedfs/Go)](https://github.com/chrislusf/seaweedfs/actions/workflows/go.yml)  [![GoDoc](https://godoc.org/github.com/chrislusf/seaweedfs/weed?status.svg)](https://godoc.org/github.com/chrislusf/seaweedfs/weed)  [![Wiki](https://img.shields.io/badge/docs-wiki-blue.svg)](https://github.com/chrislusf/seaweedfs/wiki)  [![Docker Pulls](https://img.shields.io/docker/pulls/chrislusf/seaweedfs?maxAge=4800)](https://hub.docker.com/r/chrislusf/seaweedfs/)  [![SeaweedFS on Maven Central](https://img.shields.io/maven-central/v/com.github.chrislusf/seaweedfs-client)](https://search.maven.org/search?q=g:com.github.chrislusf)      ![SeaweedFS Logo](https://raw.githubusercontent.com/chrislusf/seaweedfs/master/note/seaweedfs.png)    <h2 align=""center""><a href=""https://www.patreon.com/seaweedfs"">Sponsor SeaweedFS via Patreon</a></h2>    SeaweedFS is an independent Apache-licensed open source project with its ongoing development made  possible entirely thanks to the support of these awesome [backers](https://github.com/chrislusf/seaweedfs/blob/master/backers.md).  If you'd like to grow SeaweedFS even stronger, please consider joining our  <a href=""https://www.patreon.com/seaweedfs"">sponsors on Patreon</a>.    Your support will be really appreciated by me and other supporters!    <!--  <h4 align=""center"">Platinum</h4>    <p align=""center"">    <a href="""" target=""_blank"">      Add your name or icon here    </a>  </p>  -->      ### Gold Sponsors  - [![nodion](https://www.nodion.com/img/logo.svg)](https://www.nodion.com)    ---      - [Download Binaries for different platforms](https://github.com/chrislusf/seaweedfs/releases/latest)  - [SeaweedFS on Slack](https://join.slack.com/t/seaweedfs/shared_invite/enQtMzI4MTMwMjU2MzA3LTEyYzZmZWYzOGQ3MDJlZWMzYmI0OTE4OTJiZjJjODBmMzUxNmYwODg0YjY3MTNlMjBmZDQ1NzQ5NDJhZWI2ZmY)  - [SeaweedFS on Twitter](https://twitter.com/SeaweedFS)  - [SeaweedFS on Telegram](https://t.me/Seaweedfs)   - [SeaweedFS Mailing List](https://groups.google.com/d/forum/seaweedfs)  - [Wiki Documentation](https://github.com/chrislusf/seaweedfs/wiki)  - [SeaweedFS White Paper](https://github.com/chrislusf/seaweedfs/wiki/SeaweedFS_Architecture.pdf)  - [SeaweedFS Introduction Slides 2021.5](https://docs.google.com/presentation/d/1DcxKWlINc-HNCjhYeERkpGXXm6nTCES8mi2W5G0Z4Ts/edit?usp=sharing)  - [SeaweedFS Introduction Slides 2019.3](https://www.slideshare.net/chrislusf/seaweedfs-introduction)    Table of Contents  =================    * [Quick Start](#quick-start)      * [Quick Start for S3 API on Docker](#quick-start-for-s3-api-on-docker)      * [Quick Start with Single Binary](#quick-start-with-single-binary)      * [Quick Start SeaweedFS S3 on AWS](#quick-start-seaweedfs-s3-on-aws)  * [Introduction](#introduction)  * [Features](#features)      * [Additional Features](#additional-features)      * [Filer Features](#filer-features)  * [Example: Using Seaweed Object Store](#example-Using-Seaweed-Object-Store)  * [Architecture](#architecture)  * [Compared to Other File Systems](#compared-to-other-file-systems)      * [Compared to HDFS](#compared-to-hdfs)      * [Compared to GlusterFS, Ceph](#compared-to-glusterfs-ceph)      * [Compared to GlusterFS](#compared-to-glusterfs)      * [Compared to Ceph](#compared-to-ceph)  * [Dev Plan](#dev-plan)  * [Installation Guide](#installation-guide)  * [Disk Related Topics](#disk-related-topics)  * [Benchmark](#Benchmark)  * [License](#license)      ## Quick Start for S3 API on Docker ##    `docker run -p 8333:8333 chrislusf/seaweedfs server -s3`    ## Quick Start with Single Binary ##  * Download the latest binary from https://github.com/chrislusf/seaweedfs/releases and unzip a single binary file `weed` or `weed.exe`  * Run `weed server -dir=/some/data/dir -s3` to start one master, one volume server, one filer, and one S3 gateway.    Also, to increase capacity, just add more volume servers by running `weed volume -dir=""/some/data/dir2"" -mserver=""<master_host>:9333"" -port=8081` locally, or on a different machine, or on thousands of machines. That is it!    ## Quick Start SeaweedFS S3 on AWS ##  * Setup fast production-ready [SeaweedFS S3 on AWS with cloudformation](https://aws.amazon.com/marketplace/pp/prodview-nzelz5gprlrjc)    ## Introduction ##    SeaweedFS is a simple and highly scalable distributed file system. There are two objectives:    1. to store billions of files!  2. to serve the files fast!    SeaweedFS started as an Object Store to handle small files efficiently.   Instead of managing all file metadata in a central master,   the central master only manages volumes on volume servers,   and these volume servers manage files and their metadata.   This relieves concurrency pressure from the central master and spreads file metadata into volume servers,   allowing faster file access (O(1), usually just one disk read operation).    There is only 40 bytes of disk storage overhead for each file's metadata.   It is so simple with O(1) disk reads that you are welcome to challenge the performance with your actual use cases.    SeaweedFS started by implementing [Facebook's Haystack design paper](http://www.usenix.org/event/osdi10/tech/full_papers/Beaver.pdf).   Also, SeaweedFS implements erasure coding with ideas from   [f4: Facebookâ€™s Warm BLOB Storage System](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-muralidhar.pdf), and has a lot of similarities with [Facebookâ€™s Tectonic Filesystem](https://www.usenix.org/system/files/fast21-pan.pdf)    On top of the object store, optional [Filer] can support directories and POSIX attributes.   Filer is a separate linearly-scalable stateless server with customizable metadata stores,   e.g., MySql, Postgres, Redis, Cassandra, HBase, Mongodb, Elastic Search, LevelDB, RocksDB, Sqlite, MemSql, TiDB, Etcd, CockroachDB, etc.    For any distributed key value stores, the large values can be offloaded to SeaweedFS.   With the fast access speed and linearly scalable capacity,   SeaweedFS can work as a distributed [Key-Large-Value store][KeyLargeValueStore].    SeaweedFS can transparently integrate with the cloud.   With hot data on local cluster, and warm data on the cloud with O(1) access time,   SeaweedFS can achieve both fast local access time and elastic cloud storage capacity.  What's more, the cloud storage access API cost is minimized.   Faster and Cheaper than direct cloud storage!    [Back to TOC](#table-of-contents)    ## Additional Features ##  * Can choose no replication or different replication levels, rack and data center aware.  * Automatic master servers failover - no single point of failure (SPOF).  * Automatic Gzip compression depending on file mime type.  * Automatic compaction to reclaim disk space after deletion or update.  * [Automatic entry TTL expiration][VolumeServerTTL].  * Any server with some disk spaces can add to the total storage space.  * Adding/Removing servers does **not** cause any data re-balancing unless triggered by admin commands.  * Optional picture resizing.  * Support ETag, Accept-Range, Last-Modified, etc.  * Support in-memory/leveldb/readonly mode tuning for memory/performance balance.  * Support rebalancing the writable and readonly volumes.  * [Customizable Multiple Storage Tiers][TieredStorage]: Customizable storage disk types to balance performance and cost.  * [Transparent cloud integration][CloudTier]: unlimited capacity via tiered cloud storage for warm data.  * [Erasure Coding for warm storage][ErasureCoding]  Rack-Aware 10.4 erasure coding reduces storage cost and increases availability.    [Back to TOC](#table-of-contents)    ## Filer Features ##  * [Filer server][Filer] provides ""normal"" directories and files via http.  * [File TTL][FilerTTL] automatically expires file metadata and actual file data.  * [Mount filer][Mount] reads and writes files directly as a local directory via FUSE.  * [Filer Store Replication][FilerStoreReplication] enables HA for filer meta data stores.  * [Active-Active Replication][ActiveActiveAsyncReplication] enables asynchronous one-way or two-way cross cluster continuous replication.  * [Amazon S3 compatible API][AmazonS3API] accesses files with S3 tooling.  * [Hadoop Compatible File System][Hadoop] accesses files from Hadoop/Spark/Flink/etc or even runs HBase.  * [Async Replication To Cloud][BackupToCloud] has extremely fast local access and backups to Amazon S3, Google Cloud Storage, Azure, BackBlaze.  * [WebDAV] accesses as a mapped drive on Mac and Windows, or from mobile devices.  * [AES256-GCM Encrypted Storage][FilerDataEncryption] safely stores the encrypted data.  * [Super Large Files][SuperLargeFiles] stores large or super large files in tens of TB.  * [Cloud Drive][CloudDrive] mounts cloud storage to local cluster, cached for fast read and write with asynchronous write back.  * [Gateway to Remote Object Store][GatewayToRemoteObjectStore] mirrors bucket operations to remote object storage, in addition to [Cloud Drive][CloudDrive]    ## Kubernetes ##  * [Kubernetes CSI Driver][SeaweedFsCsiDriver] A Container Storage Interface (CSI) Driver. [![Docker Pulls](https://img.shields.io/docker/pulls/chrislusf/seaweedfs-csi-driver.svg?maxAge=4800)](https://hub.docker.com/r/chrislusf/seaweedfs-csi-driver/)  * [SeaweedFS Operator](https://github.com/seaweedfs/seaweedfs-operator)    [Filer]: https://github.com/chrislusf/seaweedfs/wiki/Directories-and-Files  [SuperLargeFiles]: https://github.com/chrislusf/seaweedfs/wiki/Data-Structure-for-Large-Files  [Mount]: https://github.com/chrislusf/seaweedfs/wiki/FUSE-Mount  [AmazonS3API]: https://github.com/chrislusf/seaweedfs/wiki/Amazon-S3-API  [BackupToCloud]: https://github.com/chrislusf/seaweedfs/wiki/Async-Replication-to-Cloud  [Hadoop]: https://github.com/chrislusf/seaweedfs/wiki/Hadoop-Compatible-File-System  [WebDAV]: https://github.com/chrislusf/seaweedfs/wiki/WebDAV  [ErasureCoding]: https://github.com/chrislusf/seaweedfs/wiki/Erasure-coding-for-warm-storage  [TieredStorage]: https://github.com/chrislusf/seaweedfs/wiki/Tiered-Storage  [CloudTier]: https://github.com/chrislusf/seaweedfs/wiki/Cloud-Tier  [FilerDataEncryption]: https://github.com/chrislusf/seaweedfs/wiki/Filer-Data-Encryption  [FilerTTL]: https://github.com/chrislusf/seaweedfs/wiki/Filer-Stores  [VolumeServerTTL]: https://github.com/chrislusf/seaweedfs/wiki/Store-file-with-a-Time-To-Live  [SeaweedFsCsiDriver]: https://github.com/seaweedfs/seaweedfs-csi-driver  [ActiveActiveAsyncReplication]: https://github.com/chrislusf/seaweedfs/wiki/Filer-Active-Active-cross-cluster-continuous-synchronization  [FilerStoreReplication]: https://github.com/chrislusf/seaweedfs/wiki/Filer-Store-Replication  [KeyLargeValueStore]: https://github.com/chrislusf/seaweedfs/wiki/Filer-as-a-Key-Large-Value-Store  [CloudDrive]: https://github.com/chrislusf/seaweedfs/wiki/Cloud-Drive-Architecture  [GatewayToRemoteObjectStore]: https://github.com/chrislusf/seaweedfs/wiki/Gateway-to-Remote-Object-Storage      [Back to TOC](#table-of-contents)    ## Example: Using Seaweed Object Store ##    By default, the master node runs on port 9333, and the volume nodes run on port 8080.  Let's start one master node, and two volume nodes on port 8080 and 8081. Ideally, they should be started from different machines. We'll use localhost as an example.    SeaweedFS uses HTTP REST operations to read, write, and delete. The responses are in JSON or JSONP format.    ### Start Master Server ###    ```  > ./weed master  ```    ### Start Volume Servers ###    ```  > weed volume -dir=""/tmp/data1"" -max=5  -mserver=""localhost:9333"" -port=8080 &  > weed volume -dir=""/tmp/data2"" -max=10 -mserver=""localhost:9333"" -port=8081 &  ```    ### Write File ###    To upload a file: first, send a HTTP POST, PUT, or GET request to `/dir/assign` to get an `fid` and a volume server url:    ```  > curl http://localhost:9333/dir/assign  {""count"":1,""fid"":""3,01637037d6"",""url"":""127.0.0.1:8080"",""publicUrl"":""localhost:8080""}  ```    Second, to store the file content, send a HTTP multi-part POST request to `url + '/' + fid` from the response:    ```  > curl -F file=@/home/chris/myphoto.jpg http://127.0.0.1:8080/3,01637037d6  {""name"":""myphoto.jpg"",""size"":43234,""eTag"":""1cc0118e""}  ```    To update, send another POST request with updated file content.    For deletion, send an HTTP DELETE request to the same `url + '/' + fid` URL:    ```  > curl -X DELETE http://127.0.0.1:8080/3,01637037d6  ```    ### Save File Id ###    Now, you can save the `fid`, 3,01637037d6 in this case, to a database field.    The number 3 at the start represents a volume id. After the comma, it's one file key, 01, and a file cookie, 637037d6.    The volume id is an unsigned 32-bit integer. The file key is an unsigned 64-bit integer. The file cookie is an unsigned 32-bit integer, used to prevent URL guessing.    The file key and file cookie are both coded in hex. You can store the <volume id, file key, file cookie> tuple in your own format, or simply store the `fid` as a string.    If stored as a string, in theory, you would need 8+1+16+8=33 bytes. A char(33) would be enough, if not more than enough, since most uses will not need 2^32 volumes.    If space is really a concern, you can store the file id in your own format. You would need one 4-byte integer for volume id, 8-byte long number for file key, and a 4-byte integer for the file cookie. So 16 bytes are more than enough.    ### Read File ###    Here is an example of how to render the URL.    First look up the volume server's URLs by the file's volumeId:    ```  > curl http://localhost:9333/dir/lookup?volumeId=3  {""volumeId"":""3"",""locations"":[{""publicUrl"":""localhost:8080"",""url"":""localhost:8080""}]}  ```    Since (usually) there are not too many volume servers, and volumes don't move often, you can cache the results most of the time. Depending on the replication type, one volume can have multiple replica locations. Just randomly pick one location to read.    Now you can take the public url, render the url or directly read from the volume server via url:    ```   http://localhost:8080/3,01637037d6.jpg  ```    Notice we add a file extension "".jpg"" here. It's optional and just one way for the client to specify the file content type.    If you want a nicer URL, you can use one of these alternative URL formats:    ```   http://localhost:8080/3/01637037d6/my_preferred_name.jpg   http://localhost:8080/3/01637037d6.jpg   http://localhost:8080/3,01637037d6.jpg   http://localhost:8080/3/01637037d6   http://localhost:8080/3,01637037d6  ```    If you want to get a scaled version of an image, you can add some params:    ```  http://localhost:8080/3/01637037d6.jpg?height=200&width=200  http://localhost:8080/3/01637037d6.jpg?height=200&width=200&mode=fit  http://localhost:8080/3/01637037d6.jpg?height=200&width=200&mode=fill  ```    ### Rack-Aware and Data Center-Aware Replication ###    SeaweedFS applies the replication strategy at a volume level. So, when you are getting a file id, you can specify the replication strategy. For example:    ```  curl http://localhost:9333/dir/assign?replication=001  ```    The replication parameter options are:    ```  000: no replication  001: replicate once on the same rack  010: replicate once on a different rack, but same data center  100: replicate once on a different data center  200: replicate twice on two different data center  110: replicate once on a different rack, and once on a different data center  ```    More details about replication can be found [on the wiki][Replication].    [Replication]: https://github.com/chrislusf/seaweedfs/wiki/Replication    You can also set the default replication strategy when starting the master server.    ### Allocate File Key on Specific Data Center ###    Volume servers can be started with a specific data center name:    ```   weed volume -dir=/tmp/1 -port=8080 -dataCenter=dc1   weed volume -dir=/tmp/2 -port=8081 -dataCenter=dc2  ```    When requesting a file key, an optional ""dataCenter"" parameter can limit the assigned volume to the specific data center. For example, this specifies that the assigned volume should be limited to 'dc1':    ```   http://localhost:9333/dir/assign?dataCenter=dc1  ```    ### Other Features ###    * [No Single Point of Failure][feat-1]    * [Insert with your own keys][feat-2]    * [Chunking large files][feat-3]    * [Collection as a Simple Name Space][feat-4]    [feat-1]: https://github.com/chrislusf/seaweedfs/wiki/Failover-Master-Server  [feat-2]: https://github.com/chrislusf/seaweedfs/wiki/Optimization#insert-with-your-own-keys  [feat-3]: https://github.com/chrislusf/seaweedfs/wiki/Optimization#upload-large-files  [feat-4]: https://github.com/chrislusf/seaweedfs/wiki/Optimization#collection-as-a-simple-name-space    [Back to TOC](#table-of-contents)    ## Object Store Architecture ##    Usually distributed file systems split each file into chunks, a central master keeps a mapping of filenames, chunk indices to chunk handles, and also which chunks each chunk server has.    The main drawback is that the central master can't handle many small files efficiently, and since all read requests need to go through the chunk master, so it might not scale well for many concurrent users.    Instead of managing chunks, SeaweedFS manages data volumes in the master server. Each data volume is 32GB in size, and can hold a lot of files. And each storage node can have many data volumes. So the master node only needs to store the metadata about the volumes, which is a fairly small amount of data and is generally stable.    The actual file metadata is stored in each volume on volume servers. Since each volume server only manages metadata of files on its own disk, with only 16 bytes for each file, all file access can read file metadata just from memory and only needs one disk operation to actually read file data.    For comparison, consider that an xfs inode structure in Linux is 536 bytes.    ### Master Server and Volume Server ###    The architecture is fairly simple. The actual data is stored in volumes on storage nodes. One volume server can have multiple volumes, and can both support read and write access with basic authentication.    All volumes are managed by a master server. The master server contains the volume id to volume server mapping. This is fairly static information, and can be easily cached.    On each write request, the master server also generates a file key, which is a growing 64-bit unsigned integer. Since write requests are not generally as frequent as read requests, one master server should be able to handle the concurrency well.    ### Write and Read files ###    When a client sends a write request, the master server returns (volume id, file key, file cookie, volume node url) for the file. The client then contacts the volume node and POSTs the file content.    When a client needs to read a file based on (volume id, file key, file cookie), it asks the master server by the volume id for the (volume node url, volume node public url), or retrieves this from a cache. Then the client can GET the content, or just render the URL on web pages and let browsers fetch the content.    Please see the example for details on the write-read process.    ### Storage Size ###    In the current implementation, each volume can hold 32 gibibytes (32GiB or 8x2^32 bytes). This is because we align content to 8 bytes. We can easily increase this to 64GiB, or 128GiB, or more, by changing 2 lines of code, at the cost of some wasted padding space due to alignment.    There can be 4 gibibytes (4GiB or 2^32 bytes) of volumes. So the total system size is 8 x 4GiB x 4GiB which is 128 exbibytes (128EiB or 2^67 bytes).    Each individual file size is limited to the volume size.    ### Saving memory ###    All file meta information stored on an volume server is readable from memory without disk access. Each file takes just a 16-byte map entry of <64bit key, 32bit offset, 32bit size>. Of course, each map entry has its own space cost for the map. But usually the disk space runs out before the memory does.    ### Tiered Storage to the cloud ###    The local volume servers are much faster, while cloud storages have elastic capacity and are actually more cost-efficient if not accessed often (usually free to upload, but relatively costly to access). With the append-only structure and O(1) access time, SeaweedFS can take advantage of both local and cloud storage by offloading the warm data to the cloud.    Usually hot data are fresh and warm data are old. SeaweedFS puts the newly created volumes on local servers, and optionally upload the older volumes on the cloud. If the older data are accessed less often, this literally gives you unlimited capacity with limited local servers, and still fast for new data.     With the O(1) access time, the network latency cost is kept at minimum.     If the hot/warm data is split as 20/80, with 20 servers, you can achieve storage capacity of 100 servers. That's a cost saving of 80%! Or you can repurpose the 80 servers to store new data also, and get 5X storage throughput.    [Back to TOC](#table-of-contents)    ## Compared to Other File Systems ##    Most other distributed file systems seem more complicated than necessary.    SeaweedFS is meant to be fast and simple, in both setup and operation. If you do not understand how it works when you reach here, we've failed! Please raise an issue with any questions or update this file with clarifications.    SeaweedFS is constantly moving forward. Same with other systems. These comparisons can be outdated quickly. Please help to keep them updated.    [Back to TOC](#table-of-contents)    ### Compared to HDFS ###    HDFS uses the chunk approach for each file, and is ideal for storing large files.    SeaweedFS is ideal for serving relatively smaller files quickly and concurrently.    SeaweedFS can also store extra large files by splitting them into manageable data chunks, and store the file ids of the data chunks into a meta chunk. This is managed by ""weed upload/download"" tool, and the weed master or volume servers are agnostic about it.    [Back to TOC](#table-of-contents)    ### Compared to GlusterFS, Ceph ###    The architectures are mostly the same. SeaweedFS aims to store and read files fast, with a simple and flat architecture. The main differences are    * SeaweedFS optimizes for small files, ensuring O(1) disk seek operation, and can also handle large files.  * SeaweedFS statically assigns a volume id for a file. Locating file content becomes just a lookup of the volume id, which can be easily cached.  * SeaweedFS Filer metadata store can be any well-known and proven data stores, e.g., Redis, Cassandra, HBase, Mongodb, Elastic Search, MySql, Postgres, Sqlite, MemSql, TiDB, CockroachDB, Etcd etc, and is easy to customized.  * SeaweedFS Volume server also communicates directly with clients via HTTP, supporting range queries, direct uploads, etc.    | System         | File Metadata                   | File Content Read| POSIX  | REST API | Optimized for large number of small files |  | -------------  | ------------------------------- | ---------------- | ------ | -------- | ------------------------- |  | SeaweedFS      | lookup volume id, cacheable     | O(1) disk seek   |        | Yes      | Yes                       |  | SeaweedFS Filer| Linearly Scalable, Customizable | O(1) disk seek   | FUSE   | Yes      | Yes                       |  | GlusterFS      | hashing          |                  | FUSE, NFS          |          |                           |  | Ceph           | hashing + rules  |                  | FUSE               | Yes      |                           |  | MooseFS        | in memory        |                  | FUSE               |       | No                          |  | MinIO          | separate meta file for each file  |                  |         | Yes   | No                          |    [Back to TOC](#table-of-contents)    ### Compared to GlusterFS ###    GlusterFS stores files, both directories and content, in configurable volumes called ""bricks"".    GlusterFS hashes the path and filename into ids, and assigned to virtual volumes, and then mapped to ""bricks"".    [Back to TOC](#table-of-contents)    ### Compared to MooseFS ###    MooseFS chooses to neglect small file issue. From moosefs 3.0 manual, ""even a small file will occupy 64KiB plus additionally 4KiB of checksums and 1KiB for the header"", because it ""was initially designed for keeping large amounts (like several thousands) of very big files""    MooseFS Master Server keeps all meta data in memory. Same issue as HDFS namenode.     [Back to TOC](#table-of-contents)    ### Compared to Ceph ###    Ceph can be setup similar to SeaweedFS as a key->blob store. It is much more complicated, with the need to support layers on top of it. [Here is a more detailed comparison](https://github.com/chrislusf/seaweedfs/issues/120)    SeaweedFS has a centralized master group to look up free volumes, while Ceph uses hashing and metadata servers to locate its objects. Having a centralized master makes it easy to code and manage.    Same as SeaweedFS, Ceph is also based on the object store RADOS. Ceph is rather complicated with mixed reviews.    Ceph uses CRUSH hashing to automatically manage the data placement, which is efficient to locate the data. But the data has to be placed according to the CRUSH algorithm. Any wrong configuration would cause data loss. Topology changes, such as adding new servers to increase capacity, will cause data migration with high IO cost to fit the CRUSH algorithm. SeaweedFS places data by assigning them to any writable volumes. If writes to one volume failed, just pick another volume to write. Adding more volumes are also as simple as it can be.    SeaweedFS is optimized for small files. Small files are stored as one continuous block of content, with at most 8 unused bytes between files. Small file access is O(1) disk read.    SeaweedFS Filer uses off-the-shelf stores, such as MySql, Postgres, Sqlite, Mongodb, Redis, Elastic Search, Cassandra, HBase, MemSql, TiDB, CockroachCB, Etcd, to manage file directories. These stores are proven, scalable, and easier to manage.    | SeaweedFS         | comparable to Ceph | advantage |  | -------------  | ------------- | ---------------- |  | Master  | MDS | simpler |  | Volume  | OSD | optimized for small files |  | Filer  | Ceph FS | linearly scalable, Customizable, O(1) or O(logN) |    [Back to TOC](#table-of-contents)    ### Compared to MinIO ###    MinIO follows AWS S3 closely and is ideal for testing for S3 API. It has good UI, policies, versionings, etc. SeaweedFS is trying to catch up here. It is also possible to put MinIO as a gateway in front of SeaweedFS later.    MinIO metadata are in simple files. Each file write will incur extra writes to corresponding meta file.    MinIO does not have optimization for lots of small files. The files are simply stored as is to local disks.  Plus the extra meta file and shards for erasure coding, it only amplifies the LOSF problem.    MinIO has multiple disk IO to read one file. SeaweedFS has O(1) disk reads, even for erasure coded files.    MinIO has full-time erasure coding. SeaweedFS uses replication on hot data for faster speed and optionally applies erasure coding on warm data.    MinIO does not have POSIX-like API support.    MinIO has specific requirements on storage layout. It is not flexible to adjust capacity. In SeaweedFS, just start one volume server pointing to the master. That's all.    ## Dev Plan ##    * More tools and documentation, on how to manage and scale the system.  * Read and write stream data.  * Support structured data.    This is a super exciting project! And we need helpers and [support](https://www.patreon.com/seaweedfs)!    [Back to TOC](#table-of-contents)    ## Installation Guide ##    > Installation guide for users who are not familiar with golang    Step 1: install go on your machine and setup the environment by following the instructions at:    https://golang.org/doc/install    make sure you set up your $GOPATH      Step 2: checkout this repo:  ```bash  git clone https://github.com/chrislusf/seaweedfs.git  ```  Step 3: download, compile, and install the project by executing the following command    ```bash  cd seaweedfs/weed && make install  ```    Once this is done, you will find the executable ""weed"" in your `$GOPATH/bin` directory    [Back to TOC](#table-of-contents)    ## Disk Related Topics ##    ### Hard Drive Performance ###    When testing read performance on SeaweedFS, it basically becomes a performance test of your hard drive's random read speed. Hard drives usually get 100MB/s~200MB/s.    ### Solid State Disk ###    To modify or delete small files, SSD must delete a whole block at a time, and move content in existing blocks to a new block. SSD is fast when brand new, but will get fragmented over time and you have to garbage collect, compacting blocks. SeaweedFS is friendly to SSD since it is append-only. Deletion and compaction are done on volume level in the background, not slowing reading and not causing fragmentation.    [Back to TOC](#table-of-contents)    ## Benchmark ##    My Own Unscientific Single Machine Results on Mac Book with Solid State Disk, CPU: 1 Intel Core i7 2.6GHz.    Write 1 million 1KB file:  ```  Concurrency Level:      16  Time taken for tests:   66.753 seconds  Complete requests:      1048576  Failed requests:        0  Total transferred:      1106789009 bytes  Requests per second:    15708.23 [#/sec]  Transfer rate:          16191.69 [Kbytes/sec]    Connection Times (ms)                min      avg        max      std  Total:        0.3      1.0       84.3      0.9    Percentage of the requests served within a certain time (ms)     50%      0.8 ms     66%      1.0 ms     75%      1.1 ms     80%      1.2 ms     90%      1.4 ms     95%      1.7 ms     98%      2.1 ms     99%      2.6 ms    100%     84.3 ms  ```    Randomly read 1 million files:  ```  Concurrency Level:      16  Time taken for tests:   22.301 seconds  Complete requests:      1048576  Failed requests:        0  Total transferred:      1106812873 bytes  Requests per second:    47019.38 [#/sec]  Transfer rate:          48467.57 [Kbytes/sec]    Connection Times (ms)                min      avg        max      std  Total:        0.0      0.3       54.1      0.2    Percentage of the requests served within a certain time (ms)     50%      0.3 ms     90%      0.4 ms     98%      0.6 ms     99%      0.7 ms    100%     54.1 ms  ```    [Back to TOC](#table-of-contents)    ## License ##    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at        http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License.    The text of this page is available for modification and reuse under the terms of the Creative Commons Attribution-Sharealike 3.0 Unported License and the GNU Free Documentation License (unversioned, with no invariant sections, front-cover texts, or back-cover texts).    [Back to TOC](#table-of-contents)    ## Stargazers over time    [![Stargazers over time](https://starchart.cc/chrislusf/seaweedfs.svg)](https://starchart.cc/chrislusf/seaweedfs)   """
Big data;https://github.com/gephi/gephi;"""# Gephi - The Open Graph Viz Platform    [![build](https://github.com/gephi/gephi/actions/workflows/build.yml/badge.svg)](https://github.com/gephi/gephi/actions/workflows/build.yml)  [![Downloads](https://img.shields.io/github/downloads/gephi/gephi/v0.9.3/total.svg)](https://github.com/gephi/gephi/releases/tag/v0.9.3)  [![Downloads](https://img.shields.io/github/downloads/gephi/gephi/v0.9.2/total.svg)](https://github.com/gephi/gephi/releases/tag/v0.9.2)  [![Downloads](https://img.shields.io/github/downloads/gephi/gephi/v0.9.1/total.svg)](https://github.com/gephi/gephi/releases/tag/v0.9.1)  [![Translation progress](https://hosted.weblate.org/widgets/gephi/-/svg-badge.svg)](https://hosted.weblate.org/engage/gephi/?utm_source=widget)    [Gephi](http://gephi.org) is an award-winning open-source platform for visualizing and manipulating large graphs. It runs on Windows, Mac OS X and Linux. Localization is available in English, French, Spanish, Japanese, Russian, Brazilian Portuguese, Chinese, Czech and German.    - **Fast** Powered by a built-in OpenGL engine, Gephi is able to push the envelope with very large networks. Visualize networks up to a million elements. All actions (e.g. layout, filter, drag) run in real-time.    - **Simple** Easy to install and [get started](https://gephi.github.io/users/quick-start). An UI that is centered around the visualization. Like Photoshopâ„¢ for graphs.    - **Modular** Extend Gephi with [plug-ins](https://gephi.org/plugins). The architecture is built on top of [Apache Netbeans Platform](https://netbeans.apache.org/tutorials/nbm-quick-start.html) and can be extended or reused easily through well-written APIs.    [Download Gephi](https://gephi.github.io/users/download) for Windows, Mac OS X and Linux and consult the [release notes](https://github.com/gephi/gephi/wiki/Releases). Example datasets can be found on our [wiki](https://github.com/gephi/gephi/wiki/Datasets).    ![Gephi](https://gephi.github.io/images/screenshots/select-tool-mini.png)    ## Install and use Gephi    Download and [Install](https://gephi.github.io/users/install/) Gephi on your computer.     Get started with the [Quick Start](https://gephi.github.io/users/quick-start/) and follow the [Tutorials](https://gephi.github.io/users/). Load a sample [dataset](https://github.com/gephi/gephi/wiki/Datasets) and start to play with the data.    If you run into any trouble or have questions consult our [forum](http://forum-gephi.org/).    ## Latest releases    ### Stable    - Latest stable release on [gephi.org](https://gephi.org/users/download/).    ### Nightly builds    Current version is 0.9.5-SNAPSHOT    - [gephi-0.9.5-SNAPSHOT-windows-x64.exe](https://oss.sonatype.org/service/local/artifact/maven/content?r=snapshots&g=org.gephi&a=gephi&v=0.9.5-SNAPSHOT&c=windows-x64&p=exe) (Windows)    - [gephi-0.9.5-SNAPSHOT-macos-x64.dmg](https://oss.sonatype.org/service/local/artifact/maven/content?r=snapshots&g=org.gephi&a=gephi&v=0.9.5-SNAPSHOT&c=macos-x64&p=dmg) (Mac OS X)    - [gephi-0.9.5-SNAPSHOT-linux-x64.tar.gz](https://oss.sonatype.org/service/local/artifact/maven/content?r=snapshots&g=org.gephi&a=gephi&v=0.9.5-SNAPSHOT&c=linux-x64&p=tar.gz) (Linux)    - [gephi-0.9.5-SNAPSHOT-sources.tar.gz](https://oss.sonatype.org/service/local/artifact/maven/content?r=snapshots&g=org.gephi&a=gephi&v=0.9.5-SNAPSHOT&c=sources&p=tar.gz) (Sources)    ## Developer Introduction    Gephi is developed in Java and uses OpenGL for its visualization engine. Built on the top of Netbeans Platform, it follows a loosely-coupled, modular architecture philosophy. Gephi is split into modules, which depend on other modules through well-written APIs. Plugins can reuse existing APIs, create new services and even replace a default implementation with a new one.    Consult the [**Javadoc**](http://gephi.github.io/gephi/0.9.2/apidocs/index.html) for an overview of the APIs.    ### Requirements    - Java JDK 11 (or later)    - [Apache Maven](http://maven.apache.org/) version 3.6.3 or later    ### Checkout and Build the sources    - Fork the repository and clone            git clone git@github.com:username/gephi.git    - Run the following command or [open the project in an IDE](https://github.com/gephi/gephi/wiki/How-to-build-Gephi)            mvn -T 4 clean install    - Once built, one can test running Gephi    		cd modules/application  		mvn nbm:cluster-app nbm:run-platform    ### Create Plug-ins    Gephi is extensible and lets developers create plug-ins to add new features, or to modify existing features. For example, you can create a new layout algorithm, add a metric, create a filter or a tool, support a new file format or database, or modify the visualization.    - [**Plugins Portal**](https://github.com/gephi/gephi/wiki/Plugins)    - [Plugins Quick Start (5 minutes)](https://github.com/gephi/gephi/wiki/Plugin-Quick-Start)    - Browse the [plugins](https://gephi.org/plugins) created by the community    - We've created a [**Plugins Bootcamp**](https://github.com/gephi/gephi-plugins-bootcamp) to learn by examples.    ## Gephi Toolkit    The Gephi Toolkit project packages essential Gephi modules (Graph, Layout, Filters, IOâ€¦) in a standard Java library which any Java project can use for getting things done. It can be used on a server or command-line tool to do the same things Gephi does but automatically.    - [Download](https://gephi.org/toolkit/)    - [GitHub Project](https://github.com/gephi/gephi-toolkit)    - [Toolkit Portal](https://github.com/gephi/gephi/wiki/Toolkit)    ## Localization    We use [Weblate](https://hosted.weblate.org/projects/gephi/) for localization. Follow the guidelines on the [wiki](https://github.com/gephi/gephi/wiki/Localization) for more details how to contribute.    ## License    Gephi main source code is distributed under the dual license [CDDL 1.0](http://www.opensource.org/licenses/CDDL-1.0) and [GNU General Public License v3](http://www.gnu.org/licenses/gpl.html). Read the [Legal FAQs](http://gephi.github.io/legal/faq/)  to learn more.  	  Copyright 2011 Gephi Consortium. All rights reserved.    The contents of this file are subject to the terms of either the GNU  General Public License Version 3 only (""GPL"") or the Common  Development and Distribution License (""CDDL"") (collectively, the  ""License""). You may not use this file except in compliance with the  License. You can obtain a copy of the License at  http://gephi.github.io/developers/license/  or /cddl-1.0.txt and /gpl-3.0.txt. See the License for the  specific language governing permissions and limitations under the  License.  When distributing the software, include this License Header  Notice in each file and include the License files at  /cddl-1.0.txt and /gpl-3.0.txt. If applicable, add the following below the  License Header, with the fields enclosed by brackets [] replaced by  your own identifying information:  ""Portions Copyrighted [year] [name of copyright owner]""    If you wish your version of this file to be governed by only the CDDL  or only the GPL Version 3, indicate your decision by adding  ""[Contributor] elects to include this software in this distribution  under the [CDDL or GPL Version 3] license."" If you do not indicate a  single choice of license, a recipient has the option to distribute  your version of this file under either the CDDL, the GPL Version 3 or  to extend the choice of license to its licensees as provided above.  However, if you add GPL Version 3 code and therefore, elected the GPL  Version 3 license, then the option applies only if the new code is  made subject to such option by the copyright holder. """
Big data;https://github.com/fchollet/keras;"""# Keras: Deep Learning for humans    ![Keras logo](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)    This repository hosts the development of the Keras library.  Read the documentation at [keras.io](https://keras.io/).    ## About Keras    Keras is a deep learning API written in Python,  running on top of the machine learning platform [TensorFlow](https://github.com/tensorflow/tensorflow).  It was developed with a focus on enabling fast experimentation.  *Being able to go from idea to result as fast as possible is key to doing good research.*    Keras is:    -   **Simple** -- but not simplistic. Keras reduces developer *cognitive load*      to free you to focus on the parts of the problem that really matter.  -   **Flexible** -- Keras adopts the principle of *progressive disclosure of      complexity*: simple workflows should be quick and easy, while arbitrarily      advanced workflows should be *possible* via a clear path that builds upon      what you've already learned.  -   **Powerful** -- Keras provides industry-strength performance and      scalability: it is used by organizations and companies including NASA,      YouTube, and Waymo.    ---    ## Keras & TensorFlow 2    [TensorFlow 2](https://www.tensorflow.org/) is an end-to-end, open-source machine learning platform.  You can think of it as an infrastructure layer for  [differentiable programming](https://en.wikipedia.org/wiki/Differentiable_programming).  It combines four key abilities:    - Efficiently executing low-level tensor operations on CPU, GPU, or TPU.  - Computing the gradient of arbitrary differentiable expressions.  - Scaling computation to many devices, such as clusters of hundreds of GPUs.  - Exporting programs (""graphs"") to external runtimes such as servers, browsers, mobile and embedded devices.    Keras is the high-level API of TensorFlow 2: an approachable, highly-productive interface  for solving machine learning problems,  with a focus on modern deep learning. It provides essential abstractions and building blocks for developing  and shipping machine learning solutions with high iteration velocity.    Keras empowers engineers and researchers to take full advantage of the scalability  and cross-platform capabilities of TensorFlow 2: you can run Keras on TPU or on large clusters of GPUs,  and you can export your Keras models to run in the browser or on a mobile device.    ---    ## First contact with Keras    The core data structures of Keras are __layers__ and __models__.  The simplest type of model is the [`Sequential` model](/guides/sequential_model/), a linear stack of layers.  For more complex architectures, you should use the [Keras functional API](/guides/functional_api/),  which allows to build arbitrary graphs of layers, or [write models entirely from scratch via subclasssing](/guides/making_new_layers_and_models_via_subclassing/).    Here is the `Sequential` model:    ```python  from tensorflow.keras.models import Sequential    model = Sequential()  ```    Stacking layers is as easy as `.add()`:    ```python  from tensorflow.keras.layers import Dense    model.add(Dense(units=64, activation='relu'))  model.add(Dense(units=10, activation='softmax'))  ```    Once your model looks good, configure its learning process with `.compile()`:    ```python  model.compile(loss='categorical_crossentropy',                optimizer='sgd',                metrics=['accuracy'])  ```    If you need to, you can further configure your optimizer. The Keras philosophy is to keep simple things simple,  while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code via subclassing).    ```python  model.compile(loss=tf.keras.losses.categorical_crossentropy,                optimizer=tf.keras.optimizers.SGD(                    learning_rate=0.01, momentum=0.9, nesterov=True))  ```    You can now iterate on your training data in batches:    ```python  # x_train and y_train are Numpy arrays.  model.fit(x_train, y_train, epochs=5, batch_size=32)  ```    Evaluate your test loss and metrics in one line:    ```python  loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)  ```    Or generate predictions on new data:    ```python  classes = model.predict(x_test, batch_size=128)  ```    What you just saw is the most elementary way to use Keras.    However, Keras is also a highly-flexible framework suitable to iterate on state-of-the-art research ideas.  Keras follows the principle of **progressive disclosure of complexity**: it makes it easy to get started,  yet it makes it possible to handle arbitrarily advanced use cases,  only requiring incremental learning at each step.    In much the same way that you were able to train & evaluate a simple neural network above in a few lines,  you can use Keras to quickly develop new training procedures or exotic model architectures.  Here's a low-level training loop example, combining Keras functionality with the TensorFlow `GradientTape`:    ```python  import tensorflow as tf    # Prepare an optimizer.  optimizer = tf.keras.optimizers.Adam()  # Prepare a loss function.  loss_fn = tf.keras.losses.kl_divergence    # Iterate over the batches of a dataset.  for inputs, targets in dataset:      # Open a GradientTape.      with tf.GradientTape() as tape:          # Forward pass.          predictions = model(inputs)          # Compute the loss value for this batch.          loss_value = loss_fn(targets, predictions)        # Get gradients of loss wrt the weights.      gradients = tape.gradient(loss_value, model.trainable_weights)      # Update the weights of the model.      optimizer.apply_gradients(zip(gradients, model.trainable_weights))  ```    For more in-depth tutorials about Keras, you can check out:    -   [Introduction to Keras for engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/)  -   [Introduction to Keras for researchers](https://keras.io/getting_started/intro_to_keras_for_researchers/)  -   [Developer guides](https://keras.io/guides/)  -   [Other learning resources](https://keras.io/getting_started/learning_resources/)    ---    ## Installation    Keras comes packaged with TensorFlow 2 as `tensorflow.keras`.  To start using Keras, simply [install TensorFlow 2](https://www.tensorflow.org/install).    ---    ## Release and compatibility    Keras has **nightly releases** (`keras-nightly` on PyPI)  and **stable releases** (`keras` on PyPI).  The nightly Keras releases are usually compatible with the corresponding version  of the `tf-nightly` releases  (e.g. `keras-nightly==2.7.0.dev2021100607` should be  used with `tf-nightly==2.7.0.dev2021100607`).  We don't maintain backward compatibility for nightly releases.  For stable releases, each Keras  version maps to a specific stable version of TensorFlow.    The table below shows the compatibility version mapping  between TensorFlow versions and Keras versions.    All the release branches can be found on [Github](https://github.com/keras-team/keras/releases).    All the release binaries can be found on [Pypi](https://pypi.org/project/keras/#history).    | Keras release | Note      | Compatible Tensorflow version |  | -----------   | ----------- | -----------        |  | [2.4](https://github.com/keras-team/keras/releases/tag/2.4.0)  | Last stable release of multi-backend Keras | < 2.5  | 2.5-pre| Pre-release (not formal) for standalone Keras repo | >= 2.5 < 2.6  | [2.6](https://github.com/keras-team/keras/releases/tag/v2.6.0)    | First formal release of standalone Keras.  | >= 2.6 < 2.7  | [2.7](https://github.com/keras-team/keras/releases/tag/v2.7.0-rc0)    | (Upcoming release) | >= 2.7 < 2.8  | nightly|                                            | tf-nightly    ---  ## Support    You can ask questions and join the development discussion:    - In the [TensorFlow forum](https://discuss.tensorflow.org/).  - On the [Keras Google group](https://groups.google.com/forum/#!forum/keras-users).  - On the [Keras Slack channel](https://kerasteam.slack.com). Use [this link](https://keras-slack-autojoin.herokuapp.com/) to request an invitation to the channel.    ---    ## Opening an issue    You can also post **bug reports and feature requests** (only)  in [GitHub issues](https://github.com/keras-team/keras/issues).      ---    ## Opening a PR    We welcome contributions! Before opening a PR, please read  [our contributor guide](https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md),  and the [API design guideline](https://github.com/keras-team/governance/blob/master/keras_api_design_guidelines.md). """
Big data;https://github.com/t3chnoboy/awesome-awesome-awesome;"""# Awesome-awesome-awesome    > A curated list of curated lists of awesome lists.    ## awesome-awesomes    - [@sindresorhus/awesome](https://github.com/sindresorhus/awesome)  - [@bradoyler/awesome-all](https://github.com/bradoyler/awesome-all)  - [@emijrp/awesome-awesome](https://github.com/emijrp/awesome-awesome)  - [@erichs/awesome-awesome](https://github.com/erichs/awesome-awesome)  - [@oyvindrobertsen/awesome-awesome](https://github.com/oyvindrobertsen/awesome-awesome)  - [@fleveque/awesome-awesomes](https://github.com/fleveque/awesome-awesomes)  - [@bayandin/awesome-awesomness](https://github.com/bayandin/awesome-awesomeness)  - [@jnv/lists](https://github.com/jnv/lists)  - [@t3chnoboy/awesome-awesome-awesome](https://github.com/t3chnoboy/awesome-awesome-awesome)  - [@sindresorhus/awesome-awesome-awesome-awesome](https://github.com/sindresorhus/awesome-awesome-awesome-awesome)  - [@scoopermaa/awesome-awesome](https://github.com/coopermaa/awesome-awesome)    ## License    [![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/) """
Big data;https://github.com/sindresorhus/awesome;"""<div align=""center"">  	<a href=""https://vshymanskyy.github.io/StandWithUkraine"">  		<img width=""500"" height=""350"" src=""media/logo-ua.svg"" alt=""Awesome"">  		<img src=""https://raw.githubusercontent.com/vshymanskyy/StandWithUkraine/main/banner2-direct.svg"">  	</a>  	<br>  	<br>  	<br>  	<br>  	<br>  	<br>  	<br>  	<hr>  	<p>  		<p>  			<sup>  				<a href=""https://github.com/sponsors/sindresorhus"">My open source work is supported by the community</a>  			</sup>  		</p>  		<sup>Special thanks to:</sup>  		<br>  		<br>  		<a href=""https://standardresume.co/tech"">  			<img src=""https://sindresorhus.com/assets/thanks/standard-resume-logo.svg"" width=""160""/>  		</a>  		<br>  		<br>  		<a href=""https://retool.com/?utm_campaign=sindresorhus"">  			<img src=""https://sindresorhus.com/assets/thanks/retool-logo.svg"" width=""210""/>  		</a>  		<br>  		<br>  		<a href=""https://doppler.com/?utm_campaign=github_repo&utm_medium=referral&utm_content=awesome&utm_source=github"">  			<div>  				<img src=""https://dashboard.doppler.com/imgs/logo-long.svg"" width=""230"" alt=""Doppler"">  			</div>  			<b>All your environment variables, in one place</b>  			<div>  				<sub>Stop struggling with scattered API keys, hacking together home-brewed tools,</sub>  				<br>  				<sup>and avoiding access controls. Keep your team and servers in sync with Doppler.</sup>  			</div>  		</a>  		<br>  		<a href=""https://workos.com/?utm_campaign=github_repo&utm_medium=referral&utm_content=awesome&utm_source=github"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/workos-logo-white-bg.svg"" width=""200"" alt=""WorkOS"">  			</div>  			<b>Your app, enterprise-ready.</b>  			<div>  				<sub>Start selling to enterprise customers with just a few lines of code.</sub>  				<br>  				<sup>Add Single Sign-On (and more) in minutes instead of months.</sup>  			</div>  		</a>  		<br>  		<a href=""https://strapi.io/?ref=sindresorhus"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/strapi-logo-white-bg.png"" width=""200"" alt=""Strapi"">  			</div>  			<b>Strapi is the leading open-source headless CMS.</b>  			<div>  				<sup>Itâ€™s 100% JavaScript, fully customizable, and developer-first.</sup>  			</div>  		</a>  		<br>  		<a href=""https://oss.capital"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/oss-capital-logo-white-bg.svg"" width=""300"" alt=""OSS Capital"">  			</div>  			<div>  				<sup><b>Founded in 2018, OSS Capital is the first and only venture capital platform focused<br>exclusively on supporting early-stage COSS (commercial open source) startup founders.</b></sup>  			</div>  		</a>  		<br>  		<br>  		<a href=""https://bit.io/?utm_campaign=github_repo&utm_medium=referral&utm_content=awesome&utm_source=github"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/bitio-logo.svg"" width=""190"" alt=""bit.io"">  			</div>  			<b>Instant, shareable cloud PostgreSQL database</b>  			<div>  				<sup>Import any dataset in seconds, share with anyone with a click, try without signing up</sup>  			</div>  		</a>  		<br>  		<br>  		<a href=""https://www.gitpod.io/?utm_campaign=sindresorhus&utm_medium=referral&utm_content=awesome&utm_source=github"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/gitpod-logo-white-bg.svg"" width=""220"" alt=""Gitpod"">  			</div>  			<b>Dev environments built for the cloud</b>  			<div>  				<sub>  				Natively integrated with GitLab, GitHub, and Bitbucket, Gitpod automatically and continuously prebuilds dev  				<br>  				environments for all your branches. As a result team members can instantly start coding with fresh dev environments  				<br>  				for each new task - no matter if you are building a new feature, want to fix a bug, or work on a code review.  				</sub>  			</div>  		</a>  		<br>  		<br>  		<a href=""https://keygen.sh"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/keygen-logo.svg"" width=""210"" alt=""Keygen"">  			</div>  			<b>A dead-simple software licensing and distribution API built for developers</b>  		</a>  		<br>  		<br>  		<br>  		<a href=""https://getstream.io/chat/sdk/ios/?utm_source=Github&utm_medium=Github_Repo_Content_Ad&utm_content=Developer&utm_campaign=Github_Jan2022_iOSChatSDK&utm_term=Sindresorhus#gh-light-mode-only"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/stream-logo.svg"" width=""220"" alt=""Stream"">  			</div>  			<br>  			<div>  				<b>Build Scalable Feeds & Chat Applications with Powerful APIs and Front End Components</b>  			</div>  		</a>  		<a href=""https://getstream.io/chat/sdk/ios/?utm_source=Github&utm_medium=Github_Repo_Content_Ad&utm_content=Developer&utm_campaign=Github_Jan2022_iOSChatSDK&utm_term=Sindresorhus#gh-dark-mode-only"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/stream-logo-dark.svg"" width=""220"" alt=""Stream"">  			</div>  			<br>  			<div>  				<b>Build Scalable Feeds & Chat Applications with Powerful APIs and Front End Components</b>  			</div>  		</a>  		<br>  		<br>  	</p>  	<hr>  	<br>  	<br>  	<br>  	<br>  </div>    <p align=""center"">  	<a href=""awesome.md"">What is an awesome list?</a>&nbsp;&nbsp;&nbsp;  	<a href=""contributing.md"">Contribution guide</a>&nbsp;&nbsp;&nbsp;  	<a href=""create-list.md"">Creating a list</a>&nbsp;&nbsp;&nbsp;  	<a href=""https://twitter.com/awesome__re"">Twitter</a>&nbsp;&nbsp;&nbsp;  	<a href=""https://www.redbubble.com/people/sindresorhus/works/30364188-awesome-logo"">Stickers & t-shirts</a>  </p>    <br>    <div align=""center"">  	<b>Follow the <a href=""https://twitter.com/awesome__re"">Awesome Twitter account</a> for updates on new list additions.</b>  </div>    <br>    <p align=""center"">  	<sub>Just type <a href=""https://awesome.re""><code>awesome.re</code></a> to go here. Check out my <a href=""https://blog.sindresorhus.com"">blog</a> and follow me on <a href=""https://twitter.com/sindresorhus"">Twitter</a>.</sub>  </p>  <br>    ## Contents    - [Platforms](#platforms)  - [Programming Languages](#programming-languages)  - [Front-End Development](#front-end-development)  - [Back-End Development](#back-end-development)  - [Computer Science](#computer-science)  - [Big Data](#big-data)  - [Theory](#theory)  - [Books](#books)  - [Editors](#editors)  - [Gaming](#gaming)  - [Development Environment](#development-environment)  - [Entertainment](#entertainment)  - [Databases](#databases)  - [Media](#media)  - [Learn](#learn)  - [Security](#security)  - [Content Management Systems](#content-management-systems)  - [Hardware](#hardware)  - [Business](#business)  - [Work](#work)  - [Networking](#networking)  - [Decentralized Systems](#decentralized-systems)  - [Higher Education](#higher-education)  - [Events](#events)  - [Testing](#testing)  - [Miscellaneous](#miscellaneous)  - [Related](#related)    ## Platforms    - [Node.js](https://github.com/sindresorhus/awesome-nodejs#readme) - Async non-blocking event-driven JavaScript runtime built on Chrome's V8 JavaScript engine.  	- [Cross-Platform](https://github.com/bcoe/awesome-cross-platform-nodejs#readme) - Writing cross-platform code on Node.js.  - [Frontend Development](https://github.com/dypsilon/frontend-dev-bookmarks#readme)  - [iOS](https://github.com/vsouza/awesome-ios#readme) - Mobile operating system for Apple phones and tablets.  - [Android](https://github.com/JStumpp/awesome-android#readme) - Mobile operating system developed by Google.  - [IoT & Hybrid Apps](https://github.com/weblancaster/awesome-IoT-hybrid#readme)  - [Electron](https://github.com/sindresorhus/awesome-electron#readme) - Cross-platform native desktop apps using JavaScript/HTML/CSS.  - [Cordova](https://github.com/busterc/awesome-cordova#readme) - JavaScript API for hybrid apps.  - [React Native](https://github.com/jondot/awesome-react-native#readme) - JavaScript framework for writing natively rendering mobile apps for iOS and Android.  - [Xamarin](https://github.com/XamSome/awesome-xamarin#readme) - Mobile app development IDE, testing, and distribution.  - [Linux](https://github.com/inputsh/awesome-linux#readme)  	- [Containers](https://github.com/Friz-zy/awesome-linux-containers#readme)  	- [eBPF](https://github.com/zoidbergwill/awesome-ebpf#readme) - Virtual machine that allows you to write more efficient and powerful tracing and monitoring for Linux systems.  	- [Arch-based Projects](https://github.com/PandaFoss/Awesome-Arch#readme) - Linux distributions and projects based on Arch Linux.  	- [AppImage](https://github.com/AppImage/awesome-appimage#readme) - Package apps in a single file that works on various mainstream Linux distributions.  - macOS - Operating system for Apple's Mac computers.  	- [Screensavers](https://github.com/agarrharr/awesome-macos-screensavers#readme)  	- [Apps](https://github.com/jaywcjlove/awesome-mac#readme)  	- [Open Source Apps](https://github.com/serhii-londar/open-source-mac-os-apps#readme)  - [watchOS](https://github.com/yenchenlin/awesome-watchos#readme) - Operating system for the Apple Watch.  - [JVM](https://github.com/deephacks/awesome-jvm#readme)  - [Salesforce](https://github.com/mailtoharshit/awesome-salesforce#readme)  - [Amazon Web Services](https://github.com/donnemartin/awesome-aws#readme)  - [Windows](https://github.com/Awesome-Windows/Awesome#readme)  - [IPFS](https://github.com/ipfs/awesome-ipfs#readme) - P2P hypermedia protocol.  - [Fuse](https://github.com/fuse-compound/awesome-fuse#readme) - Mobile development tools.  - [Heroku](https://github.com/ianstormtaylor/awesome-heroku#readme) - Cloud platform as a service.  - [Raspberry Pi](https://github.com/thibmaek/awesome-raspberry-pi#readme) - Credit card-sized computer aimed at teaching kids programming, but capable of a lot more.  - [Qt](https://github.com/JesseTG/awesome-qt#readme) - Cross-platform GUI app framework.  - [WebExtensions](https://github.com/fregante/Awesome-WebExtensions#readme) - Cross-browser extension system.  - [Smart TV](https://github.com/vitalets/awesome-smart-tv#readme) - Create apps for different TV platforms.  - [GNOME](https://github.com/Kazhnuz/awesome-gnome#readme) - Simple and distraction-free desktop environment for Linux.  - [KDE](https://github.com/francoism90/awesome-kde#readme) - A free software community dedicated to creating an open and user-friendly computing experience.  - [.NET](https://github.com/quozd/awesome-dotnet#readme)  	- [Core](https://github.com/thangchung/awesome-dotnet-core#readme)  	- [Roslyn](https://github.com/ironcev/awesome-roslyn#readme) - Open-source compilers and code analysis APIs for C# and VB.NET languages.  - [Amazon Alexa](https://github.com/miguelmota/awesome-amazon-alexa#readme) - Virtual home assistant.  - [DigitalOcean](https://github.com/jonleibowitz/awesome-digitalocean#readme) - Cloud computing platform designed for developers.  - [Flutter](https://github.com/Solido/awesome-flutter#readme) - Google's mobile SDK for building native iOS and Android apps from a single codebase written in Dart.  - [Home Assistant](https://github.com/frenck/awesome-home-assistant#readme) - Open source home automation that puts local control and privacy first.  - [IBM Cloud](https://github.com/victorshinya/awesome-ibmcloud#readme) - Cloud platform for developers and companies.  - [Firebase](https://github.com/jthegedus/awesome-firebase#readme) - App development platform built on Google Cloud Platform.  - [Robot Operating System 2.0](https://github.com/fkromer/awesome-ros2#readme) - Set of software libraries and tools that help you build robot apps.  - [Adafruit IO](https://github.com/adafruit/awesome-adafruitio#readme) - Visualize and store data from any device.  - [Cloudflare](https://github.com/irazasyed/awesome-cloudflare#readme) - CDN, DNS, DDoS protection, and security for your site.  - [Actions on Google](https://github.com/ravirupareliya/awesome-actions-on-google#readme) - Developer platform for Google Assistant.  - [ESP](https://github.com/agucova/awesome-esp#readme) - Low-cost microcontrollers with WiFi and broad IoT applications.  - [Deno](https://github.com/denolib/awesome-deno#readme) - A secure runtime for JavaScript and TypeScript that uses V8 and is built in Rust.  - [DOS](https://github.com/balintkissdev/awesome-dos#readme) - Operating system for x86-based personal computers that was popular during the 1980s and early 1990s.  - [Nix](https://github.com/nix-community/awesome-nix#readme) - Package manager for Linux and other Unix systems that makes package management reliable and reproducible.  - [Integration](https://github.com/stn1slv/awesome-integration#readme) - Linking together different IT systems (components) to functionally cooperate as a whole.  - [Node-RED](https://github.com/naimo84/awesome-nodered#readme) - A programming tool for wiring together hardware devices, APIs, and online services.  - [Low Code](https://github.com/zenitysec/awesome-low-code#readme) - Allowing business professionals to address their needs on their own with little to no coding skills.    ## Programming Languages    - [JavaScript](https://github.com/sorrycc/awesome-javascript#readme)  	- [Promises](https://github.com/wbinnssmith/awesome-promises#readme)  	- [Standard Style](https://github.com/standard/awesome-standard#readme) - Style guide and linter.  	- [Must Watch Talks](https://github.com/bolshchikov/js-must-watch#readme)  	- [Tips](https://github.com/loverajoel/jstips#readme)  	- [Network Layer](https://github.com/Kikobeats/awesome-network-js#readme)  	- [Micro npm Packages](https://github.com/parro-it/awesome-micro-npm-packages#readme)  	- [Mad Science npm Packages](https://github.com/feross/awesome-mad-science#readme) - Impossible sounding projects that exist.  	- [Maintenance Modules](https://github.com/maxogden/maintenance-modules#readme) - For npm packages.  	- [npm](https://github.com/sindresorhus/awesome-npm#readme) - Package manager.  	- [AVA](https://github.com/avajs/awesome-ava#readme) - Test runner.  	- [ESLint](https://github.com/dustinspecker/awesome-eslint#readme) - Linter.  	- [Functional Programming](https://github.com/stoeffel/awesome-fp-js#readme)  	- [Observables](https://github.com/sindresorhus/awesome-observables#readme)  	- [npm scripts](https://github.com/RyanZim/awesome-npm-scripts#readme) - Task runner.  	- [30 Seconds of Code](https://github.com/30-seconds/30-seconds-of-code#readme) - Code snippets you can understand in 30 seconds.  	- [Ponyfills](https://github.com/Richienb/awesome-ponyfills#readme) - Like polyfills but without overriding native APIs.  - [Swift](https://github.com/matteocrippa/awesome-swift#readme) - Apple's compiled programming language that is secure, modern, programmer-friendly, and fast.  	- [Education](https://github.com/hsavit1/Awesome-Swift-Education#readme)  	- [Playgrounds](https://github.com/uraimo/Awesome-Swift-Playgrounds#readme)  - [Python](https://github.com/vinta/awesome-python#readme) - General-purpose programming language designed for readability.  	- [Asyncio](https://github.com/timofurrer/awesome-asyncio#readme) - Asynchronous I/O in Python 3.  	- [Scientific Audio](https://github.com/faroit/awesome-python-scientific-audio#readme) - Scientific research in audio/music.  	- [CircuitPython](https://github.com/adafruit/awesome-circuitpython#readme) - A version of Python for microcontrollers.  	- [Data Science](https://github.com/krzjoa/awesome-python-data-science#readme) - Data analysis and machine learning.  	- [Typing](https://github.com/typeddjango/awesome-python-typing#readme) - Optional static typing for Python.  	- [MicroPython](https://github.com/mcauser/awesome-micropython#readme) - A lean and efficient implementation of Python 3 for microcontrollers.  - [Rust](https://github.com/rust-unofficial/awesome-rust#readme)  - [Haskell](https://github.com/krispo/awesome-haskell#readme)  - [PureScript](https://github.com/passy/awesome-purescript#readme)  - [Go](https://github.com/avelino/awesome-go#readme)  - [Scala](https://github.com/lauris/awesome-scala#readme)  	- [Scala Native](https://github.com/tindzk/awesome-scala-native#readme) - Optimizing ahead-of-time compiler for Scala based on LLVM.  - [Ruby](https://github.com/markets/awesome-ruby#readme)  - [Clojure](https://github.com/razum2um/awesome-clojure#readme)  - [ClojureScript](https://github.com/hantuzun/awesome-clojurescript#readme)  - [Elixir](https://github.com/h4cc/awesome-elixir#readme)  - [Elm](https://github.com/sporto/awesome-elm#readme)  - [Erlang](https://github.com/drobakowski/awesome-erlang#readme)  - [Julia](https://github.com/svaksha/Julia.jl#readme) - High-level dynamic programming language designed to address the needs of high-performance numerical analysis and computational science.  - [Lua](https://github.com/LewisJEllis/awesome-lua#readme)  - [C](https://github.com/inputsh/awesome-c#readme)  - [C/C++](https://github.com/fffaraz/awesome-cpp#readme) - General-purpose language with a bias toward system programming and embedded, resource-constrained software.  - [R](https://github.com/qinwf/awesome-R#readme) - Functional programming language and environment for statistical computing and graphics.  	- [Learning](https://github.com/iamericfletcher/awesome-r-learning-resources#readme)  - [D](https://github.com/dlang-community/awesome-d#readme)  - [Common Lisp](https://github.com/CodyReichert/awesome-cl#readme) - Powerful dynamic multiparadigm language that facilitates iterative and interactive development.  	- [Learning](https://github.com/GustavBertram/awesome-common-lisp-learning#readme)  - [Perl](https://github.com/hachiojipm/awesome-perl#readme)  - [Groovy](https://github.com/kdabir/awesome-groovy#readme)  - [Dart](https://github.com/yissachar/awesome-dart#readme)  - [Java](https://github.com/akullpp/awesome-java#readme) - Popular secure object-oriented language designed for flexibility to ""write once, run anywhere"".  	- [RxJava](https://github.com/eleventigers/awesome-rxjava#readme)  - [Kotlin](https://github.com/KotlinBy/awesome-kotlin#readme)  - [OCaml](https://github.com/ocaml-community/awesome-ocaml#readme)  - [ColdFusion](https://github.com/seancoyne/awesome-coldfusion#readme)  - [Fortran](https://github.com/rabbiabram/awesome-fortran#readme)  - [PHP](https://github.com/ziadoz/awesome-php#readme) - Server-side scripting language.  	- [Composer](https://github.com/jakoch/awesome-composer#readme) - Package manager.  - [Pascal](https://github.com/Fr0sT-Brutal/awesome-pascal#readme)  - [AutoHotkey](https://github.com/ahkscript/awesome-AutoHotkey#readme)  - [AutoIt](https://github.com/J2TeaM/awesome-AutoIt#readme)  - [Crystal](https://github.com/veelenga/awesome-crystal#readme)  - [Frege](https://github.com/sfischer13/awesome-frege#readme) - Haskell for the JVM.  - [CMake](https://github.com/onqtam/awesome-cmake#readme) - Build, test, and package software.  - [ActionScript 3](https://github.com/robinrodricks/awesome-actionscript3#readme) - Object-oriented language targeting Adobe AIR.  - [Eta](https://github.com/sfischer13/awesome-eta#readme) - Functional programming language for the JVM.  - [Idris](https://github.com/joaomilho/awesome-idris#readme) - General purpose pure functional programming language with dependent types influenced by Haskell and ML.  - [Ada/SPARK](https://github.com/ohenley/awesome-ada#readme) - Modern programming language designed for large, long-lived apps where reliability and efficiency are essential.  - [Q#](https://github.com/ebraminio/awesome-qsharp#readme) - Domain-specific programming language used for expressing quantum algorithms.  - [Imba](https://github.com/koolamusic/awesome-imba#readme) - Programming language inspired by Ruby and Python and compiles to performant JavaScript.  - [Vala](https://github.com/desiderantes/awesome-vala#readme) - Programming language designed to take full advantage of the GLib and GNOME ecosystems, while preserving the speed of C code.  - [Coq](https://github.com/coq-community/awesome-coq#readme) - Formal language and environment for programming and specification which facilitates interactive development of machine-checked proofs.  - [V](https://github.com/vlang/awesome-v#readme) - Simple, fast, safe, compiled language for developing maintainable software.    ## Front-End Development    - [ES6 Tools](https://github.com/addyosmani/es6-tools#readme)  - [Web Performance Optimization](https://github.com/davidsonfellipe/awesome-wpo#readme)  - [Web Tools](https://github.com/lvwzhen/tools#readme)  - [CSS](https://github.com/awesome-css-group/awesome-css#readme) - Style sheet language that specifies how HTML elements are displayed on screen.  	- [Critical-Path Tools](https://github.com/addyosmani/critical-path-css-tools#readme)  	- [Scalability](https://github.com/davidtheclark/scalable-css-reading-list#readme)  	- [Must-Watch Talks](https://github.com/AllThingsSmitty/must-watch-css#readme)  	- [Protips](https://github.com/AllThingsSmitty/css-protips#readme)  	- [Frameworks](https://github.com/troxler/awesome-css-frameworks#readme)  - [React](https://github.com/enaqx/awesome-react#readme) - App framework.  	- [Relay](https://github.com/expede/awesome-relay#readme) - Framework for building data-driven React apps.  	- [React Hooks](https://github.com/glauberfc/awesome-react-hooks#readme) - A new feature that lets you use state and other React features without writing a class.  - [Web Components](https://github.com/mateusortiz/webcomponents-the-right-way#readme)  - [Polymer](https://github.com/Granze/awesome-polymer#readme) - JavaScript library to develop Web Components.  - [Angular](https://github.com/PatrickJS/awesome-angular#readme) - App framework.  - [Backbone](https://github.com/sadcitizen/awesome-backbone#readme) - App framework.  - [HTML5](https://github.com/diegocard/awesome-html5#readme) - Markup language used for websites & web apps.  - [SVG](https://github.com/willianjusten/awesome-svg#readme) - XML-based vector image format.  - [Canvas](https://github.com/raphamorim/awesome-canvas#readme)  - [KnockoutJS](https://github.com/dnbard/awesome-knockout#readme) - JavaScript library.  - [Dojo Toolkit](https://github.com/petk/awesome-dojo#readme) - JavaScript toolkit.  - [Inspiration](https://github.com/NoahBuscher/Inspire#readme)  - [Ember](https://github.com/ember-community-russia/awesome-ember#readme) - App framework.  - [Android UI](https://github.com/wasabeef/awesome-android-ui#readme)  - [iOS UI](https://github.com/cjwirth/awesome-ios-ui#readme)  - [Meteor](https://github.com/Urigo/awesome-meteor#readme)  - [BEM](https://github.com/sturobson/BEM-resources#readme)  - [Flexbox](https://github.com/afonsopacifer/awesome-flexbox#readme)  - [Web Typography](https://github.com/deanhume/typography#readme)  - [Web Accessibility](https://github.com/brunopulis/awesome-a11y#readme)  - [Material Design](https://github.com/sachin1092/awesome-material#readme)  - [D3](https://github.com/wbkd/awesome-d3#readme) - Library for producing dynamic, interactive data visualizations.  - [Emails](https://github.com/jonathandion/awesome-emails#readme)  - [jQuery](https://github.com/petk/awesome-jquery#readme) - Easy to use JavaScript library for DOM manipulation.  	- [Tips](https://github.com/AllThingsSmitty/jquery-tips-everyone-should-know#readme)  - [Web Audio](https://github.com/notthetup/awesome-webaudio#readme)  - [Offline-First](https://github.com/pazguille/offline-first#readme)  - [Static Website Services](https://github.com/agarrharr/awesome-static-website-services#readme)  - [Cycle.js](https://github.com/cyclejs-community/awesome-cyclejs#readme) - Functional and reactive JavaScript framework.  - [Text Editing](https://github.com/dok/awesome-text-editing#readme)  - [Motion UI Design](https://github.com/fliptheweb/motion-ui-design#readme)  - [Vue.js](https://github.com/vuejs/awesome-vue#readme) - App framework.  - [Marionette.js](https://github.com/sadcitizen/awesome-marionette#readme) - App framework.  - [Aurelia](https://github.com/aurelia-contrib/awesome-aurelia#readme) - App framework.  - [Charting](https://github.com/zingchart/awesome-charting#readme)  - [Ionic Framework 2](https://github.com/candelibas/awesome-ionic#readme)  - [Chrome DevTools](https://github.com/ChromeDevTools/awesome-chrome-devtools#readme)  - [PostCSS](https://github.com/jdrgomes/awesome-postcss#readme) - CSS tool.  - [Draft.js](https://github.com/nikgraf/awesome-draft-js#readme) - Rich text editor framework for React.  - [Service Workers](https://github.com/TalAter/awesome-service-workers#readme)  - [Progressive Web Apps](https://github.com/TalAter/awesome-progressive-web-apps#readme)  - [choo](https://github.com/choojs/awesome-choo#readme) - App framework.  - [Redux](https://github.com/brillout/awesome-redux#readme) - State container for JavaScript apps.  - [Browserify](https://github.com/browserify/awesome-browserify#readme) - Module bundler.  - [Sass](https://github.com/Famolus/awesome-sass#readme) - CSS preprocessor.  - [Ant Design](https://github.com/websemantics/awesome-ant-design#readme) - Enterprise-class UI design language.  - [Less](https://github.com/LucasBassetti/awesome-less#readme) - CSS preprocessor.  - [WebGL](https://github.com/sjfricke/awesome-webgl#readme) - JavaScript API for rendering 3D graphics.  - [Preact](https://github.com/preactjs/awesome-preact#readme) - App framework.  - [Progressive Enhancement](https://github.com/jbmoelker/progressive-enhancement-resources#readme)  - [Next.js](https://github.com/unicodeveloper/awesome-nextjs#readme) - Framework for server-rendered React apps.  - [lit](https://github.com/web-padawan/awesome-lit#readme) - Library for building web components with a declarative template system.  - [JAMstack](https://github.com/automata/awesome-jamstack#readme) - Modern web development architecture based on client-side JavaScript, reusable APIs, and prebuilt markup.  - [WordPress-Gatsby](https://github.com/henrikwirth/awesome-wordpress-gatsby#readme) - Web development technology stack with WordPress as a back end and Gatsby as a front end.  - [Mobile Web Development](https://github.com/myshov/awesome-mobile-web-development#readme) - Creating a great mobile web experience.  - [Storybook](https://github.com/lauthieb/awesome-storybook#readme) - Development environment for UI components.  - [Blazor](https://github.com/AdrienTorris/awesome-blazor#readme) - .NET web framework using C#/Razor and HTML that runs in the browser with WebAssembly.  - [PageSpeed Metrics](https://github.com/csabapalfi/awesome-pagespeed-metrics#readme) - Metrics to help understand page speed and user experience.  - [Tailwind CSS](https://github.com/aniftyco/awesome-tailwindcss#readme) - Utility-first CSS framework for rapid UI development.  - [Seed](https://github.com/seed-rs/awesome-seed-rs#readme) - Rust framework for creating web apps running in WebAssembly.  - [Web Performance Budget](https://github.com/pajaydev/awesome-web-performance-budget#readme) - Techniques to ensure certain performance metrics for a website.  - [Web Animation](https://github.com/sergey-pimenov/awesome-web-animation#readme) - Animations in the browser with JavaScript, CSS, SVG, etc.  - [Yew](https://github.com/jetli/awesome-yew#readme) - Rust framework inspired by Elm and React for creating multi-threaded frontend web apps with WebAssembly.  - [Material-UI](https://github.com/nadunindunil/awesome-material-ui#readme) - Material Design React components for faster and easier web development.  - [Building Blocks for Web Apps](https://github.com/componently-com/awesome-building-blocks-for-web-apps#readme) - Standalone features to be integrated into web apps.  - [Svelte](https://github.com/TheComputerM/awesome-svelte#readme) - App framework.  - [Design systems](https://github.com/klaufel/awesome-design-systems#readme) - Collection of reusable components, guided by rules that ensure consistency and speed.  - [Inertia.js](https://github.com/innocenzi/awesome-inertiajs#readme) - Make single-page apps without building an API.  - [MDBootstrap](https://github.com/mdbootstrap/awesome-mdbootstrap#readme) - Templates, layouts, components, and widgets to rapidly build websites.    ## Back-End Development    - [Flask](https://github.com/mjhea0/awesome-flask#readme) - Python framework.  - [Docker](https://github.com/veggiemonk/awesome-docker#readme)  - [Vagrant](https://github.com/iJackUA/awesome-vagrant#readme) - Automation virtual machine environment.  - [Pyramid](https://github.com/uralbash/awesome-pyramid#readme) - Python framework.  - [Play1 Framework](https://github.com/PerfectCarl/awesome-play1#readme)  - [CakePHP](https://github.com/friendsofcake/awesome-cakephp#readme) - PHP framework.  - [Symfony](https://github.com/sitepoint-editors/awesome-symfony#readme) - PHP framework.  	- [Education](https://github.com/pehapkari/awesome-symfony-education#readme)  - [Laravel](https://github.com/chiraggude/awesome-laravel#readme) - PHP framework.  	- [Education](https://github.com/fukuball/Awesome-Laravel-Education#readme)  	- [TALL Stack](https://github.com/livewire/awesome-tall-stack#readme) - Full-stack development solution featuring libraries built by the Laravel community.  - [Rails](https://github.com/gramantin/awesome-rails#readme) - Web app framework for Ruby.  	- [Gems](https://github.com/hothero/awesome-rails-gem#readme) - Packages.  - [Phalcon](https://github.com/phalcon/awesome-phalcon#readme) - PHP framework.  - [Useful `.htaccess` Snippets](https://github.com/phanan/htaccess#readme)  - [nginx](https://github.com/fcambus/nginx-resources#readme) - Web server.  - [Dropwizard](https://github.com/stve/awesome-dropwizard#readme) - Java framework.  - [Kubernetes](https://github.com/ramitsurana/awesome-kubernetes#readme) - Open-source platform that automates Linux container operations.  - [Lumen](https://github.com/unicodeveloper/awesome-lumen#readme) - PHP micro-framework.  - [Serverless Framework](https://github.com/pmuens/awesome-serverless#readme) - Serverless computing and serverless architectures.  - [Apache Wicket](https://github.com/PhantomYdn/awesome-wicket#readme) - Java web app framework.  - [Vert.x](https://github.com/vert-x3/vertx-awesome#readme) - Toolkit for building reactive apps on the JVM.  - [Terraform](https://github.com/shuaibiyy/awesome-terraform#readme) - Tool for building, changing, and versioning infrastructure.  - [Vapor](https://github.com/vapor-community/awesome-vapor#readme) - Server-side development in Swift.  - [Dash](https://github.com/ucg8j/awesome-dash#readme) - Python web app framework.  - [FastAPI](https://github.com/mjhea0/awesome-fastapi#readme) - Python web app framework.  - [CDK](https://github.com/kolomied/awesome-cdk#readme) - Open-source software development framework for defining cloud infrastructure in code.  - [IAM](https://github.com/kdeldycke/awesome-iam#readme) - User accounts, authentication and authorization.  - [Slim](https://github.com/nekofar/awesome-slim#readme) - PHP framework.    ## Computer Science    - [University Courses](https://github.com/prakhar1989/awesome-courses#readme)  - [Data Science](https://github.com/academic/awesome-datascience#readme)  	- [Tutorials](https://github.com/siboehm/awesome-learn-datascience#readme)  - [Machine Learning](https://github.com/josephmisiti/awesome-machine-learning#readme)  	- [Tutorials](https://github.com/ujjwalkarn/Machine-Learning-Tutorials#readme)  	- [ML with Ruby](https://github.com/arbox/machine-learning-with-ruby#readme) - Learning, implementing, and applying Machine Learning using Ruby.  	- [Core ML Models](https://github.com/likedan/Awesome-CoreML-Models#readme) - Models for Apple's machine learning framework.  	- [H2O](https://github.com/h2oai/awesome-h2o#readme) - Open source distributed machine learning platform written in Java with APIs in R, Python, and Scala.  	- [Software Engineering for Machine Learning](https://github.com/SE-ML/awesome-seml#readme) - From experiment to production-level machine learning.  	- [AI in Finance](https://github.com/georgezouq/awesome-ai-in-finance#readme) - Solving problems in finance with machine learning.  	- [JAX](https://github.com/n2cholas/awesome-jax#readme) - Automatic differentiation and XLA compilation brought together for high-performance machine learning research.  	- [XAI](https://github.com/altamiracorp/awesome-xai#readme) - Providing insight, explanations, and interpretability to machine learning methods.  - [Speech and Natural Language Processing](https://github.com/edobashira/speech-language-processing#readme)  	- [Spanish](https://github.com/dav009/awesome-spanish-nlp#readme)  	- [NLP with Ruby](https://github.com/arbox/nlp-with-ruby#readme)  	- [Question Answering](https://github.com/seriousran/awesome-qa#readme) - The science of asking and answering in natural language with a machine.  	- [Natural Language Generation](https://github.com/accelerated-text/awesome-nlg#readme) - Generation of text used in data to text, conversational agents, and narrative generation applications.  - [Linguistics](https://github.com/theimpossibleastronaut/awesome-linguistics#readme)  - [Cryptography](https://github.com/sobolevn/awesome-cryptography#readme)  	- [Papers](https://github.com/pFarb/awesome-crypto-papers#readme) - Theory basics for using cryptography by non-cryptographers.  - [Computer Vision](https://github.com/jbhuang0604/awesome-computer-vision#readme)  - [Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning#readme) - Neural networks.  	- [TensorFlow](https://github.com/jtoy/awesome-tensorflow#readme) - Library for machine intelligence.  	- [TensorFlow.js](https://github.com/aaronhma/awesome-tensorflow-js#readme) - WebGL-accelerated machine learning JavaScript library for training and deploying models.  	- [TensorFlow Lite](https://github.com/margaretmz/awesome-tensorflow-lite#readme) - Framework that optimizes TensorFlow models for on-device machine learning.  	- [Papers](https://github.com/terryum/awesome-deep-learning-papers#readme) - The most cited deep learning papers.  	- [Education](https://github.com/guillaume-chevalier/awesome-deep-learning-resources#readme)  - [Deep Vision](https://github.com/kjw0612/awesome-deep-vision#readme)  - [Open Source Society University](https://github.com/ossu/computer-science#readme)  - [Functional Programming](https://github.com/lucasviola/awesome-functional-programming#readme)  - [Empirical Software Engineering](https://github.com/dspinellis/awesome-msr#readme) - Evidence-based research on software systems.  - [Static Analysis & Code Quality](https://github.com/analysis-tools-dev/static-analysis#readme)  - [Information Retrieval](https://github.com/harpribot/awesome-information-retrieval#readme) - Learn to develop your own search engine.  - [Quantum Computing](https://github.com/desireevl/awesome-quantum-computing#readme) - Computing which utilizes quantum mechanics and qubits on quantum computers.  - [Theoretical Computer Science](https://github.com/mostafatouny/awesome-theoretical-computer-science#readme) - The interplay of computer science and pure mathematics, distinguished by its emphasis on mathematical rigour and technique.    ## Big Data    - [Big Data](https://github.com/0xnr/awesome-bigdata#readme)  - [Public Datasets](https://github.com/awesomedata/awesome-public-datasets#readme)  - [Hadoop](https://github.com/youngwookim/awesome-hadoop#readme) - Framework for distributed storage and processing of very large data sets.  - [Data Engineering](https://github.com/igorbarinov/awesome-data-engineering#readme)  - [Streaming](https://github.com/manuzhang/awesome-streaming#readme)  - [Apache Spark](https://github.com/awesome-spark/awesome-spark#readme) - Unified engine for large-scale data processing.  - [Qlik](https://github.com/ambster-public/awesome-qlik#readme) - Business intelligence platform for data visualization, analytics, and reporting apps.  - [Splunk](https://github.com/sduff/awesome-splunk#readme) - Platform for searching, monitoring, and analyzing structured and unstructured machine-generated big data in real-time.    ## Theory    - [Papers We Love](https://github.com/papers-we-love/papers-we-love#readme)  - [Talks](https://github.com/JanVanRyswyck/awesome-talks#readme)  - [Algorithms](https://github.com/tayllan/awesome-algorithms#readme)  	- [Education](https://github.com/gaerae/awesome-algorithms-education#readme) - Learning and practicing.  - [Algorithm Visualizations](https://github.com/enjalot/algovis#readme)  - [Artificial Intelligence](https://github.com/owainlewis/awesome-artificial-intelligence#readme)  - [Search Engine Optimization](https://github.com/marcobiedermann/search-engine-optimization#readme)  - [Competitive Programming](https://github.com/lnishan/awesome-competitive-programming#readme)  - [Math](https://github.com/rossant/awesome-math#readme)  - [Recursion Schemes](https://github.com/passy/awesome-recursion-schemes#readme) - Traversing nested data structures.    ## Books    - [Free Programming Books](https://github.com/EbookFoundation/free-programming-books#readme)  - [Go Books](https://github.com/dariubs/GoBooks#readme)  - [R Books](https://github.com/RomanTsegelskyi/rbooks#readme)  - [Mind Expanding Books](https://github.com/hackerkid/Mind-Expanding-Books#readme)  - [Book Authoring](https://github.com/TalAter/awesome-book-authoring#readme)  - [Elixir Books](https://github.com/sger/ElixirBooks#readme)    ## Editors    - [Sublime Text](https://github.com/dreikanter/sublime-bookmarks#readme)  - [Vim](https://github.com/mhinz/vim-galore#readme)  - [Neovim](https://github.com/rockerBOO/awesome-neovim#readme) - Vim-fork focused on extensibility and usability.  - [Emacs](https://github.com/emacs-tw/awesome-emacs#readme)  - [Atom](https://github.com/mehcode/awesome-atom#readme) - Open-source and hackable text editor.  - [Visual Studio Code](https://github.com/viatsko/awesome-vscode#readme) - Cross-platform open-source text editor.    ## Gaming    - [Game Development](https://github.com/ellisonleao/magictools#readme)  - [Game Talks](https://github.com/hzoo/awesome-gametalks#readme)  - [Godot](https://github.com/godotengine/awesome-godot#readme) - Game engine.  - [Open Source Games](https://github.com/leereilly/games#readme)  - [Unity](https://github.com/RyanNielson/awesome-unity#readme) - Game engine.  - [Chess](https://github.com/hkirat/awesome-chess#readme)  - [LÃ–VE](https://github.com/love2d-community/awesome-love2d#readme) - Game engine.  - [PICO-8](https://github.com/pico-8/awesome-PICO-8#readme) - Fantasy console.  - [Game Boy Development](https://github.com/gbdev/awesome-gbdev#readme)  - [Construct 2](https://github.com/ConstructCommunity/awesome-construct#readme) - Game engine.  - [Gideros](https://github.com/stetso/awesome-gideros#readme) - Game engine.  - [Minecraft](https://github.com/bs-community/awesome-minecraft#readme) - Sandbox video game.  - [Game Datasets](https://github.com/leomaurodesenv/game-datasets#readme) - Materials and datasets for Artificial Intelligence in games.  - [Haxe Game Development](https://github.com/Dvergar/awesome-haxe-gamedev#readme) - A high-level strongly typed programming language used to produce cross-platform native code.  - [libGDX](https://github.com/rafaskb/awesome-libgdx#readme) - Java game framework.  - [PlayCanvas](https://github.com/playcanvas/awesome-playcanvas#readme) - Game engine.  - [Game Remakes](https://github.com/radek-sprta/awesome-game-remakes#readme) - Actively maintained open-source game remakes.  - [Flame](https://github.com/flame-engine/awesome-flame#readme) - Game engine for Flutter.  - [Discord Communities](https://github.com/mhxion/awesome-discord-communities#readme) - Chat with friends and communities.  - [CHIP-8](https://github.com/tobiasvl/awesome-chip-8#readme) - Virtual computer game machine from the 70s.  - [Games of Coding](https://github.com/michelpereira/awesome-games-of-coding#readme) - Learn a programming language by making games.  - [Esports](https://github.com/strift/awesome-esports#readme) - Video games played as a sport.    ## Development Environment    - [Quick Look Plugins](https://github.com/sindresorhus/quick-look-plugins#readme) - For macOS.  - [Dev Env](https://github.com/jondot/awesome-devenv#readme)  - [Dotfiles](https://github.com/webpro/awesome-dotfiles#readme)  - [Shell](https://github.com/alebcay/awesome-shell#readme)  - [Fish](https://github.com/jorgebucaran/awsm.fish#readme) - User-friendly shell.  - [Command-Line Apps](https://github.com/agarrharr/awesome-cli-apps#readme)  - [ZSH Plugins](https://github.com/unixorn/awesome-zsh-plugins#readme)  - [GitHub](https://github.com/phillipadsmith/awesome-github#readme) - Hosting service for Git repositories.  	- [Browser Extensions](https://github.com/stefanbuck/awesome-browser-extensions-for-github#readme)  	- [Cheat Sheet](https://github.com/tiimgreen/github-cheat-sheet#readme)  	- [Pinned Gists](https://github.com/matchai/awesome-pinned-gists#readme) - Dynamic pinned gists for your GitHub profile.  - [Git Cheat Sheet & Git Flow](https://github.com/arslanbilal/git-cheat-sheet#readme)  - [Git Tips](https://github.com/git-tips/tips#readme)  - [Git Add-ons](https://github.com/stevemao/awesome-git-addons#readme) - Enhance the `git` CLI.  - [Git Hooks](https://github.com/compscilauren/awesome-git-hooks#readme) - Scripts for automating tasks during `git` workflows.  - [SSH](https://github.com/moul/awesome-ssh#readme)  - [FOSS for Developers](https://github.com/tvvocold/FOSS-for-Dev#readme)  - [Hyper](https://github.com/bnb/awesome-hyper#readme) - Cross-platform terminal app built on web technologies.  - [PowerShell](https://github.com/janikvonrotz/awesome-powershell#readme) - Cross-platform object-oriented shell.  - [Alfred Workflows](https://github.com/alfred-workflows/awesome-alfred-workflows#readme) - Productivity app for macOS.  - [Terminals Are Sexy](https://github.com/k4m4/terminals-are-sexy#readme)  - [GitHub Actions](https://github.com/sdras/awesome-actions#readme) - Create tasks to automate your workflow and share them with others on GitHub.    ## Entertainment    - [Science Fiction](https://github.com/sindresorhus/awesome-scifi#readme) - Scifi.  - [Fantasy](https://github.com/RichardLitt/awesome-fantasy#readme)  - [Podcasts](https://github.com/ayr-ton/awesome-geek-podcasts#readme)  - [Email Newsletters](https://github.com/zudochkin/awesome-newsletters#readme)  - [IT Quotes](https://github.com/victorlaerte/awesome-it-quotes#readme)    ## Databases    - [Database](https://github.com/numetriclabz/awesome-db#readme)  - [MySQL](https://github.com/shlomi-noach/awesome-mysql#readme)  - [SQLAlchemy](https://github.com/dahlia/awesome-sqlalchemy#readme)  - [InfluxDB](https://github.com/mark-rushakoff/awesome-influxdb#readme)  - [Neo4j](https://github.com/neueda/awesome-neo4j#readme)  - [MongoDB](https://github.com/ramnes/awesome-mongodb#readme) - NoSQL database.  - [RethinkDB](https://github.com/d3viant0ne/awesome-rethinkdb#readme)  - [TinkerPop](https://github.com/mohataher/awesome-tinkerpop#readme) - Graph computing framework.  - [PostgreSQL](https://github.com/dhamaniasad/awesome-postgres#readme) - Object-relational database.  - [CouchDB](https://github.com/quangv/awesome-couchdb#readme) - Document-oriented NoSQL database.  - [HBase](https://github.com/rayokota/awesome-hbase#readme) - Distributed, scalable, big data store.  - [NoSQL Guides](https://github.com/erictleung/awesome-nosql-guides#readme) - Help on using non-relational, distributed, open-source, and horizontally scalable databases.  - [Database Tools](https://github.com/mgramin/awesome-db-tools#readme) - Everything that makes working with databases easier.  - [TypeDB](https://github.com/vaticle/typedb-awesome#readme) - Logical database to organize large and complex networks of data as one body of knowledge.  - [Cassandra](https://github.com/Anant/awesome-cassandra#readme) - Open-source, distributed, wide column store, NoSQL database management system.    ## Media    - [Creative Commons Media](https://github.com/shime/creative-commons-media#readme)  - [Fonts](https://github.com/brabadu/awesome-fonts#readme)  - [Codeface](https://github.com/chrissimpkins/codeface#readme) - Text editor fonts.  - [Stock Resources](https://github.com/neutraltone/awesome-stock-resources#readme)  - [GIF](https://github.com/davisonio/awesome-gif#readme) - Image format known for animated images.  - [Music](https://github.com/ciconia/awesome-music#readme)  - [Open Source Documents](https://github.com/44bits/awesome-opensource-documents#readme)  - [Audio Visualization](https://github.com/willianjusten/awesome-audio-visualization#readme)  - [Broadcasting](https://github.com/ebu/awesome-broadcasting#readme)  - [Pixel Art](https://github.com/Siilwyn/awesome-pixel-art#readme) - Pixel-level digital art.  - [FFmpeg](https://github.com/transitive-bullshit/awesome-ffmpeg#readme) - Cross-platform solution to record, convert and stream audio and video.  - [Icons](https://github.com/notlmn/awesome-icons#readme) - Downloadable SVG/PNG/font icon projects.  - [Audiovisual](https://github.com/stingalleman/awesome-audiovisual#readme) - Lighting, audio and video in professional environments.  - [VLC](https://github.com/mfkl/awesome-vlc#readme) - Cross-platform media player software and streaming server.    ## Learn    - [CLI Workshoppers](https://github.com/therebelrobot/awesome-workshopper#readme) - Interactive tutorials.  - [Learn to Program](https://github.com/karlhorky/learn-to-program#readme)  - [Speaking](https://github.com/matteofigus/awesome-speaking#readme)  - [Tech Videos](https://github.com/lucasviola/awesome-tech-videos#readme)  - [Dive into Machine Learning](https://github.com/hangtwenty/dive-into-machine-learning#readme)  - [Computer History](https://github.com/watson/awesome-computer-history#readme)  - [Programming for Kids](https://github.com/HollyAdele/awesome-programming-for-kids#readme)  - [Educational Games](https://github.com/yrgo/awesome-educational-games#readme) - Learn while playing.  - [JavaScript Learning](https://github.com/micromata/awesome-javascript-learning#readme)  - [CSS Learning](https://github.com/micromata/awesome-css-learning#readme) - Mainly about CSS â€“ the language and the modules.  - [Product Management](https://github.com/dend/awesome-product-management#readme) - Learn how to be a better product manager.  - [Roadmaps](https://github.com/liuchong/awesome-roadmaps#readme) - Gives you a clear route to improve your knowledge and skills.  - [YouTubers](https://github.com/JoseDeFreitas/awesome-youtubers#readme) - Watch video tutorials from YouTubers that teach you about technology.    ## Security    - [Application Security](https://github.com/paragonie/awesome-appsec#readme)  - [Security](https://github.com/sbilly/awesome-security#readme)  - [CTF](https://github.com/apsdehal/awesome-ctf#readme) - Capture The Flag.  - [Malware Analysis](https://github.com/rshipp/awesome-malware-analysis#readme)  - [Android Security](https://github.com/ashishb/android-security-awesome#readme)  - [Hacking](https://github.com/carpedm20/awesome-hacking#readme)  - [Honeypots](https://github.com/paralax/awesome-honeypots#readme) - Deception trap, designed to entice an attacker into attempting to compromise the information systems in an organization.  - [Incident Response](https://github.com/meirwah/awesome-incident-response#readme)  - [Vehicle Security and Car Hacking](https://github.com/jaredthecoder/awesome-vehicle-security#readme)  - [Web Security](https://github.com/qazbnm456/awesome-web-security#readme) - Security of web apps & services.  - [Lockpicking](https://github.com/fabacab/awesome-lockpicking#readme) - The art of unlocking a lock by manipulating its components without the key.  - [Cybersecurity Blue Team](https://github.com/fabacab/awesome-cybersecurity-blueteam#readme) - Groups of individuals who identify security flaws in information technology systems.  - [Fuzzing](https://github.com/cpuu/awesome-fuzzing#readme) - Automated software testing technique that involves feeding pseudo-randomly generated input data.  - [Embedded and IoT Security](https://github.com/fkie-cad/awesome-embedded-and-iot-security#readme)  - [GDPR](https://github.com/bakke92/awesome-gdpr#readme) - Regulation on data protection and privacy for all individuals within EU.  - [DevSecOps](https://github.com/TaptuIT/awesome-devsecops#readme) - Integration of security practices into [DevOps](https://en.wikipedia.org/wiki/DevOps).  - [Executable Packing](https://github.com/dhondta/awesome-executable-packing#readme) - Packing and unpacking executable formats.    ## Content Management Systems    - [Umbraco](https://github.com/umbraco-community/awesome-umbraco#readme)  - [Refinery CMS](https://github.com/refinerycms-contrib/awesome-refinerycms#readme) - Ruby on Rails CMS.  - [Wagtail](https://github.com/springload/awesome-wagtail#readme) - Django CMS focused on flexibility and user experience.  - [Textpattern](https://github.com/drmonkeyninja/awesome-textpattern#readme) - Lightweight PHP-based CMS.  - [Drupal](https://github.com/nirgn975/awesome-drupal#readme) - Extensible PHP-based CMS.  - [Craft CMS](https://github.com/craftcms/awesome#readme) - Content-first CMS.  - [Sitecore](https://github.com/MartinMiles/Awesome-Sitecore#readme) - .NET digital marketing platform that combines CMS with tools for managing multiple websites.  - [Silverstripe CMS](https://github.com/wernerkrauss/awesome-silverstripe-cms#readme) - PHP MVC framework that serves as a classic or headless CMS.    ## Hardware    - [Robotics](https://github.com/Kiloreux/awesome-robotics#readme)  - [Internet of Things](https://github.com/HQarroum/awesome-iot#readme)  - [Electronics](https://github.com/kitspace/awesome-electronics#readme) - For electronic engineers and hobbyists.  - [Bluetooth Beacons](https://github.com/rabschi/awesome-beacon#readme)  - [Electric Guitar Specifications](https://github.com/gitfrage/guitarspecs#readme) - Checklist for building your own electric guitar.  - [Plotters](https://github.com/beardicus/awesome-plotters#readme) - Computer-controlled drawing machines and other visual art robots.  - [Robotic Tooling](https://github.com/protontypes/awesome-robotic-tooling#readme) - Free and open tools for professional robotic development.  - [LIDAR](https://github.com/szenergy/awesome-lidar#readme) - Sensor for measuring distances by illuminating the target with laser light.    ## Business    - [Open Companies](https://github.com/opencompany/awesome-open-company#readme)  - [Places to Post Your Startup](https://github.com/mmccaff/PlacesToPostYourStartup#readme)  - [OKR Methodology](https://github.com/domenicosolazzo/awesome-okr#readme) - Goal setting & communication best practices.  - [Leading and Managing](https://github.com/LappleApple/awesome-leading-and-managing#readme) - Leading people and being a manager in a technology company/environment.  - [Indie](https://github.com/mezod/awesome-indie#readme) - Independent developer businesses.  - [Tools of the Trade](https://github.com/cjbarber/ToolsOfTheTrade#readme) - Tools used by companies on Hacker News.  - [Clean Tech](https://github.com/nglgzz/awesome-clean-tech#readme) - Fighting climate change with technology.  - [Wardley Maps](https://github.com/wardley-maps-community/awesome-wardley-maps#readme) - Provides high situational awareness to help improve strategic planning and decision making.  - [Social Enterprise](https://github.com/RayBB/awesome-social-enterprise#readme) - Building an organization primarily focused on social impact that is at least partially self-funded.  - [Engineering Team Management](https://github.com/kdeldycke/awesome-engineering-team-management#readme) - How to transition from software development to engineering management.  - [Developer-First Products](https://github.com/agamm/awesome-developer-first#readme) - Products that target developers as the user.  - [Billing](https://github.com/kdeldycke/awesome-billing#readme) - Payments, invoicing, pricing, accounting, marketplace, fraud, and business intelligence.    ## Work    - [Slack](https://github.com/matiassingers/awesome-slack#readme) - Team collaboration.  	- [Communities](https://github.com/filipelinhares/awesome-slack#readme)  - [Remote Jobs](https://github.com/lukasz-madon/awesome-remote-job#readme)  - [Productivity](https://github.com/jyguyomarch/awesome-productivity#readme)  - [Niche Job Boards](https://github.com/tramcar/awesome-job-boards#readme)  - [Programming Interviews](https://github.com/DopplerHQ/awesome-interview-questions#readme)  - [Code Review](https://github.com/joho/awesome-code-review#readme) - Reviewing code.  - [Creative Technology](https://github.com/j0hnm4r5/awesome-creative-technology#readme) - Businesses & groups that specialize in combining computing, design, art, and user experience.  - [Internships](https://github.com/lodthe/awesome-internships#readme) - CV writing guides and companies that hire interns.    ## Networking    - [Software-Defined Networking](https://github.com/sdnds-tw/awesome-sdn#readme)  - [Network Analysis](https://github.com/briatte/awesome-network-analysis#readme)  - [PCAPTools](https://github.com/caesar0301/awesome-pcaptools#readme)  - [Real-Time Communications](https://github.com/rtckit/awesome-rtc#readme) - Network protocols for near simultaneous exchange of media and data.    ## Decentralized Systems    - [Bitcoin](https://github.com/igorbarinov/awesome-bitcoin#readme) - Bitcoin services and tools for software developers.  - [Ripple](https://github.com/vhpoet/awesome-ripple#readme) - Open source distributed settlement network.  - [Non-Financial Blockchain](https://github.com/machinomy/awesome-non-financial-blockchain#readme) - Non-financial blockchain applications.  - [Mastodon](https://github.com/tleb/awesome-mastodon#readme) - Open source decentralized microblogging network.  - [Ethereum](https://github.com/ttumiel/Awesome-Ethereum#readme) - Distributed computing platform for smart contract development.  - [Blockchain AI](https://github.com/steven2358/awesome-blockchain-ai#readme) - Blockchain projects for artificial intelligence and machine learning.  - [EOSIO](https://github.com/DanailMinchev/awesome-eosio#readme) - A decentralized operating system supporting industrial-scale apps.  - [Corda](https://github.com/chainstack/awesome-corda#readme) - Open source blockchain platform designed for business.  - [Waves](https://github.com/msmolyakov/awesome-waves#readme) - Open source blockchain platform and development toolset for Web 3.0 apps and decentralized solutions.  - [Substrate](https://github.com/substrate-developer-hub/awesome-substrate#readme) - Framework for writing scalable, upgradeable blockchains in Rust.  - [Golem](https://github.com/golemfactory/awesome-golem#readme) - Open source peer-to-peer marketplace for computing resources.  - [Stacks](https://github.com/friedger/awesome-stacks-chain#readme) - A smart contract platform secured by Bitcoin.  - [Algorand](https://github.com/aorumbayev/awesome-algorand#readme) - An open-source, proof of stake blockchain and smart contract computing platform.    ## Higher Education    - [Computational Neuroscience](https://github.com/eselkin/awesome-computational-neuroscience#readme) - A multidisciplinary science which uses computational approaches to study the nervous system.  - [Digital History](https://github.com/maehr/awesome-digital-history#readme) - Computer-aided scientific investigation of history.  - [Scientific Writing](https://github.com/writing-resources/awesome-scientific-writing#readme) - Distraction-free scientific writing with Markdown, reStructuredText and Jupyter notebooks.    ## Events    - [Creative Tech Events](https://github.com/danvoyce/awesome-creative-tech-events#readme) - Events around the globe for creative coding, tech, design, music, arts and cool stuff.  - [Events in Italy](https://github.com/ildoc/awesome-italy-events#readme) - Tech-related events in Italy.  - [Events in the Netherlands](https://github.com/awkward/awesome-netherlands-events#readme) - Tech-related events in the Netherlands.    ## Testing    - [Testing](https://github.com/TheJambo/awesome-testing#readme) - Software testing.  - [Visual Regression Testing](https://github.com/mojoaxel/awesome-regression-testing#readme) - Ensures changes did not break the functionality or style.  - [Selenium](https://github.com/christian-bromann/awesome-selenium#readme) - Open-source browser automation framework and ecosystem.  - [Appium](https://github.com/SrinivasanTarget/awesome-appium#readme) - Test automation tool for apps.  - [TAP](https://github.com/sindresorhus/awesome-tap#readme) - Test Anything Protocol.  - [JMeter](https://github.com/aliesbelik/awesome-jmeter#readme) - Load testing and performance measurement tool.  - [k6](https://github.com/grafana/awesome-k6#readme) - Open-source, developer-centric performance monitoring and load testing solution.  - [Playwright](https://github.com/mxschmitt/awesome-playwright#readme) - Node.js library to automate Chromium, Firefox and WebKit with a single API.  - [Quality Assurance Roadmap](https://github.com/fityanos/awesome-quality-assurance-roadmap#readme) - How to start & build a career in software testing.  - [Gatling](https://github.com/aliesbelik/awesome-gatling#readme) - Open-source load and performance testing framework based on Scala, Akka, and Netty.    ## Miscellaneous    - [JSON](https://github.com/burningtree/awesome-json#readme) - Text based data interchange format.  	- [GeoJSON](https://github.com/tmcw/awesome-geojson#readme)  	- [Datasets](https://github.com/jdorfman/awesome-json-datasets#readme)  - [CSV](https://github.com/secretGeek/awesomeCSV#readme) - A text file format that stores tabular data and uses a comma to separate values.  - [Discounts for Student Developers](https://github.com/AchoArnold/discount-for-student-dev#readme)  - [Radio](https://github.com/kyleterry/awesome-radio#readme)  - [Awesome](https://github.com/sindresorhus/awesome#readme) - Recursion illustrated.  - [Analytics](https://github.com/0xnr/awesome-analytics#readme)  - [REST](https://github.com/marmelab/awesome-rest#readme)  - [Continuous Integration and Continuous Delivery](https://github.com/cicdops/awesome-ciandcd#readme)  - [Services Engineering](https://github.com/mmcgrana/services-engineering#readme)  - [Free for Developers](https://github.com/ripienaar/free-for-dev#readme)  - [Answers](https://github.com/cyberglot/awesome-answers#readme) - Stack Overflow, Quora, etc.  - [Sketch](https://github.com/diessica/awesome-sketch#readme) - Design app for macOS.  - [Boilerplate Projects](https://github.com/melvin0008/awesome-projects-boilerplates#readme)  - [Readme](https://github.com/matiassingers/awesome-readme#readme)  - [Design and Development Guides](https://github.com/NARKOZ/guides#readme)  - [Software Engineering Blogs](https://github.com/kilimchoi/engineering-blogs#readme)  - [Self Hosted](https://github.com/awesome-selfhosted/awesome-selfhosted#readme)  - [FOSS Production Apps](https://github.com/DataDaoDe/awesome-foss-apps#readme)  - [Gulp](https://github.com/alferov/awesome-gulp#readme) - Task runner.  - [AMA](https://github.com/sindresorhus/amas#readme) - Ask Me Anything.  	- [Answers](https://github.com/stoeffel/awesome-ama-answers#readme)  - [Open Source Photography](https://github.com/ibaaj/awesome-OpenSourcePhotography#readme)  - [OpenGL](https://github.com/eug/awesome-opengl#readme) - Cross-platform API for rendering 2D and 3D graphics.  - [GraphQL](https://github.com/chentsulin/awesome-graphql#readme)  - [Urban & Regional Planning](https://github.com/APA-Technology-Division/urban-and-regional-planning-resources#readme) - Concerning the built environment and communities.  - [Transit](https://github.com/CUTR-at-USF/awesome-transit#readme)  - [Research Tools](https://github.com/emptymalei/awesome-research#readme)  - [Data Visualization](https://github.com/javierluraschi/awesome-dataviz#readme)  - [Social Media Share Links](https://github.com/vinkla/shareable-links#readme)  - [Microservices](https://github.com/mfornos/awesome-microservices#readme)  - [Unicode](https://github.com/jagracey/Awesome-Unicode#readme) - Unicode standards, quirks, packages and resources.  	- [Code Points](https://github.com/Codepoints/awesome-codepoints#readme)  - [Beginner-Friendly Projects](https://github.com/MunGell/awesome-for-beginners#readme)  - [Katas](https://github.com/gamontal/awesome-katas#readme)  - [Tools for Activism](https://github.com/drewrwilson/toolsforactivism#readme)  - [Citizen Science](https://github.com/dylanrees/citizen-science#readme) - For community-based and non-institutional scientists.  - [MQTT](https://github.com/hobbyquaker/awesome-mqtt#readme) - ""Internet of Things"" connectivity protocol.  - [Hacking Spots](https://github.com/daviddias/awesome-hacking-locations#readme)  - [For Girls](https://github.com/cristianoliveira/awesome4girls#readme)  - [Vorpal](https://github.com/vorpaljs/awesome-vorpal#readme) - Node.js CLI framework.  - [Vulkan](https://github.com/vinjn/awesome-vulkan#readme) - Low-overhead, cross-platform 3D graphics and compute API.  - [LaTeX](https://github.com/egeerardyn/awesome-LaTeX#readme) - Typesetting language.  - [Economics](https://github.com/antontarasenko/awesome-economics#readme) - An economist's starter kit.  - [Funny Markov Chains](https://github.com/sublimino/awesome-funny-markov#readme)  - [Bioinformatics](https://github.com/danielecook/Awesome-Bioinformatics#readme)  - [Cheminformatics](https://github.com/hsiaoyi0504/awesome-cheminformatics#readme) - Informatics techniques applied to problems in chemistry.  - [Colorful](https://github.com/Siddharth11/Colorful#readme) - Choose your next color scheme.  - [Steam](https://github.com/scholtzm/awesome-steam#readme) - Digital distribution platform.  - [Bots](https://github.com/hackerkid/bots#readme) - Building bots.  - [Site Reliability Engineering](https://github.com/dastergon/awesome-sre#readme)  - [Empathy in Engineering](https://github.com/KimberlyMunoz/empathy-in-engineering#readme) - Building and promoting more compassionate engineering cultures.  - [DTrace](https://github.com/xen0l/awesome-dtrace#readme) - Dynamic tracing framework.  - [Userscripts](https://github.com/bvolpato/awesome-userscripts#readme) - Enhance your browsing experience.  - [PokÃ©mon](https://github.com/tobiasbueschel/awesome-pokemon#readme) - PokÃ©mon and PokÃ©mon GO.  - [ChatOps](https://github.com/exAspArk/awesome-chatops#readme) - Managing technical and business operations through a chat.  - [Falsehood](https://github.com/kdeldycke/awesome-falsehood#readme) - Falsehoods programmers believe in.  - [Domain-Driven Design](https://github.com/heynickc/awesome-ddd#readme) - Software development approach for complex needs by connecting the implementation to an evolving model.  - [Quantified Self](https://github.com/woop/awesome-quantified-self#readme) - Self-tracking through technology.  - [SaltStack](https://github.com/hbokh/awesome-saltstack#readme) - Python-based config management system.  - [Web Design](https://github.com/nicolesaidy/awesome-web-design#readme) - For digital designers.  - [Creative Coding](https://github.com/terkelg/awesome-creative-coding#readme) - Programming something expressive instead of something functional.  - [No-Login Web Apps](https://github.com/aviaryan/awesome-no-login-web-apps#readme) - Web apps that work without login.  - [Free Software](https://github.com/johnjago/awesome-free-software#readme) - Free as in freedom.  - [Framer](https://github.com/podo/awesome-framer#readme) - Prototyping interactive UI designs.  - [Markdown](https://github.com/BubuAnabelas/awesome-markdown#readme) - Markup language.  - [Dev Fun](https://github.com/mislavcimpersak/awesome-dev-fun#readme) - Funny developer projects.  - [Healthcare](https://github.com/kakoni/awesome-healthcare#readme) - Open source healthcare software for facilities, providers, developers, policy experts, and researchers.  - [Magento 2](https://github.com/DavidLambauer/awesome-magento2#readme) - Open Source eCommerce built with PHP.  - [TikZ](https://github.com/xiaohanyu/awesome-tikz#readme) - Graph drawing packages for TeX/LaTeX/ConTeXt.  - [Neuroscience](https://github.com/analyticalmonk/awesome-neuroscience#readme) - Study of the nervous system and brain.  - [Ad-Free](https://github.com/johnjago/awesome-ad-free#readme) - Ad-free alternatives.  - [Esolangs](https://github.com/angrykoala/awesome-esolangs#readme) - Programming languages designed for experimentation or as jokes rather than actual use.  - [Prometheus](https://github.com/roaldnefs/awesome-prometheus#readme) - Open-source monitoring system.  - [Homematic](https://github.com/homematic-community/awesome-homematic#readme) - Smart home devices.  - [Ledger](https://github.com/sfischer13/awesome-ledger#readme) - Double-entry accounting on the command-line.  - [Web Monetization](https://github.com/thomasbnt/awesome-web-monetization#readme) - A free open web standard service that allows you to send money directly in your browser.  - [Uncopyright](https://github.com/johnjago/awesome-uncopyright#readme) - Public domain works.  - [Crypto Currency Tools & Algorithms](https://github.com/Zheaoli/awesome-coins#readme) - Digital currency where encryption is used to regulate the generation of units and verify transfers.  - [Diversity](https://github.com/folkswhocode/awesome-diversity#readme) - Creating a more inclusive and diverse tech community.  - [Open Source Supporters](https://github.com/zachflower/awesome-open-source-supporters#readme) - Companies that offer their tools and services for free to open source projects.  - [Design Principles](https://github.com/robinstickel/awesome-design-principles#readme) - Create better and more consistent designs and experiences.  - [Theravada](https://github.com/johnjago/awesome-theravada#readme) - Teachings from the Theravada Buddhist tradition.  - [inspectIT](https://github.com/inspectit-labs/awesome-inspectit#readme) - Open source Java app performance management tool.  - [Open Source Maintainers](https://github.com/nayafia/awesome-maintainers#readme) - The experience of being an open source maintainer.  - [Calculators](https://github.com/xxczaki/awesome-calculators#readme) - Calculators for every platform.  - [Captcha](https://github.com/ZYSzys/awesome-captcha#readme) - A type of challengeâ€“response test used in computing to determine whether or not the user is human.  - [Jupyter](https://github.com/markusschanta/awesome-jupyter#readme) - Create and share documents that contain code, equations, visualizations and narrative text.  - [FIRST Robotics Competition](https://github.com/andrewda/awesome-frc#readme) - International high school robotics championship.  - [Humane Technology](https://github.com/humanetech-community/awesome-humane-tech#readme) - Open source projects that help improve society.  - [Speakers](https://github.com/karlhorky/awesome-speakers#readme) - Conference and meetup speakers in the programming and design community.  - [Board Games](https://github.com/edm00se/awesome-board-games#readme) - Table-top gaming fun for all.  - [Software Patreons](https://github.com/uraimo/awesome-software-patreons#readme) - Fund individual programmers or the development of open source projects.  - [Parasite](https://github.com/ecohealthalliance/awesome-parasite#readme) - Parasites and host-pathogen interactions.  - [Food](https://github.com/jzarca01/awesome-food#readme) - Food-related projects on GitHub.  - [Mental Health](https://github.com/dreamingechoes/awesome-mental-health#readme) - Mental health awareness and self-care in the software industry.  - [Bitcoin Payment Processors](https://github.com/alexk111/awesome-bitcoin-payment-processors#readme) - Start accepting Bitcoin.  - [Scientific Computing](https://github.com/nschloe/awesome-scientific-computing#readme) - Solving complex scientific problems using computers.  - [Amazon Sellers](https://github.com/ScaleLeap/awesome-amazon-seller#readme)  - [Agriculture](https://github.com/brycejohnston/awesome-agriculture#readme) - Open source technology for farming and gardening.  - [Product Design](https://github.com/ttt30ga/awesome-product-design#readme) - Design a product from the initial concept to production.  - [Prisma](https://github.com/catalinmiron/awesome-prisma#readme) - Turn your database into a GraphQL API.  - [Software Architecture](https://github.com/simskij/awesome-software-architecture#readme) - The discipline of designing and building software.  - [Connectivity Data and Reports](https://github.com/stevesong/awesome-connectivity-info#readme) - Better understand who has access to telecommunication and internet infrastructure and on what terms.  - [Stacks](https://github.com/stackshareio/awesome-stacks#readme) - Tech stacks for building different apps and features.  - [Cytodata](https://github.com/cytodata/awesome-cytodata#readme) - Image-based profiling of biological phenotypes for computational biologists.  - [IRC](https://github.com/davisonio/awesome-irc#readme) - Open source messaging protocol.  - [Advertising](https://github.com/cenoura/awesome-ads#readme) - Advertising and programmatic media for websites.  - [Earth](https://github.com/philsturgeon/awesome-earth#readme) - Find ways to resolve the climate crisis.  - [Naming](https://github.com/gruhn/awesome-naming#readme) - Naming things in computer science done right.  - [Biomedical Information Extraction](https://github.com/caufieldjh/awesome-bioie#readme) - How to extract information from unstructured biomedical data and text.  - [Web Archiving](https://github.com/iipc/awesome-web-archiving#readme) - An effort to preserve the Web for future generations.  - [WP-CLI](https://github.com/schlessera/awesome-wp-cli#readme) - Command-line interface for WordPress.  - [Credit Modeling](https://github.com/mourarthur/awesome-credit-modeling#readme) - Methods for classifying credit applicants into risk classes.  - [Ansible](https://github.com/ansible-community/awesome-ansible#readme) - A Python-based, open source IT configuration management and automation platform.  - [Biological Visualizations](https://github.com/keller-mark/awesome-biological-visualizations#readme) - Interactive visualization of biological data on the web.  - [QR Code](https://github.com/make-github-pseudonymous-again/awesome-qr-code#readme) - A type of matrix barcode that can be used to store and share a small amount of information.  - [Veganism](https://github.com/sdassow/awesome-veganism#readme) - Making the plant-based lifestyle easy and accessible.  - [Translations](https://github.com/mbiesiad/awesome-translations#readme) - The transfer of the meaning of a text from one language to another.  - [Scriptable](https://github.com/dersvenhesse/awesome-scriptable#readme) - An iOS app for automations in JavaScript.  - [WebXR](https://github.com/msub2/awesome-webxr#readme) - Enables immersive virtual reality and augmented reality content on the web.    ## Related    - [All Awesome Lists](https://github.com/topics/awesome) - All the Awesome lists on GitHub.  - [Awesome Indexed](https://awesome-indexed.mathew-davies.co.uk) - Search the Awesome dataset.  - [Awesome Search](https://awesomelists.top) - Quick search for Awesome lists.  - [StumbleUponAwesome](https://github.com/basharovV/StumbleUponAwesome) - Discover random pages from the Awesome dataset using a browser extension.  - [Awesome CLI](https://github.com/umutphp/awesome-cli) - A simple command-line tool to dive into Awesome lists.  - [Awesome Viewer](https://awesome.digitalbunker.dev) - A visualizer for all of the above Awesome lists.  - [Track Awesome List](https://www.trackawesomelist.com) - View the latest updates of Awesome lists. """
Big data;https://github.com/ufukomer/bloomery;"""# Bloomery    Bloomery is an open source query execution tool for [Impala](http://impala.io/). It uses [node-impala](https://github.com/ufukomer/node-impala) which provides communication between Impala and Node client using the Beeswax Service. Bloomery has ability to show tables of database, columns of tables, saved queries, recent queries and so on.    * [Features](#features)  * [Architecture](#architecture)  * [Requirements](#requirements)  * [Installation](#installation)    ![Bloomery UI](screenshots/bloomery.png)    ## Features    Bloomery has the following features:    * _Syntax highlighting:_ On the web ui, the editor immediately highlights the sql keywords (e.g select, from, as, where, group, etc.) after the user types it.  * _Autocomplete:_ While user typing any of the sql keywords, the editorâ€™s suggestion dropdown pops up.  * _Saved queries:_ It is possible to save any query using save query section.  * _Query history:_ Application saves the all recent queries.  * _Show query results:_ Shows all results under the results tab.  * _Remote connection:_ User can change connection settings for the purpose of connecting server from different machine either with different Host IP or Port Number.  * _Show tables:_ On top left side of the application it shows all tables of database.  * _Show columns:_ On the bottom left side of the application it shows table columns with their type property.    ## Architecture    Bloomery communicates with node-impala using Express Rest API. Express Rest API maps URLs with node-impalaâ€™s connect and query methods. Then the actions like _executeQuery_, _showTables_, _showColumns_ sends query with request parameters to Rest API then Express forwards that query to node-impala which handles and returns results using Thrift. Eventually, Express puts this results to response body which we present to users inside the table under results tab of ui menu.    ![Bloomery Architecture](screenshots/architecture.jpg)    ## Requirements    * Node.js  * Impala    ## Installation    1. Install dependencies       ```    $ npm install    ```    2. Build Bloomery      ```    $ npm run build    ```    3. Start server      ```    $ npm run serve    ```    ## License    Apache License, Version 2.0 """
