{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- We express gratitudes to the great work [AdaIN](https://github.com/xunhuang1995/AdaIN-style) and [Style-swap](https://github.com/rtqichen/style-swap) as we benefit a lot from both their papers and codes.\n\n- We thank [Wei-Sheng Lai](http://graduatestudents.ucmerced.edu/wlai24/) for his huge help on the user study design and all subjects that provide helpful feedbacks and suggestions.\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{WCT-NIPS-2017,\n    author = {Li, Yijun and Fang, Chen and Yang, Jimei and Wang, Zhaowen and Lu, Xin and Yang, Ming-Hsuan},\n    title = {Universal Style Transfer via Feature Transforms},\n    booktitle = {Advances in Neural Information Processing Systems},\n    year = {2017}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{WCT-NIPS-2017,\n    author = {Li, Yijun and Fang, Chen and Yang, Jimei and Wang, Zhaowen and Lu, Xin and Yang, Ming-Hsuan},\n    title = {Universal Style Transfer via Feature Transforms},\n    booktitle = {Advances in Neural Information Processing Systems},\n    year = {2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "th test_wct.lua -style YourTexturePath -synthesis 1 -styleSize 512 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "th test_wct_interpolation2.lua -style YourTexturePath1,YourTexturePath2 -beta 0.5 -synthesis 1 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Yijunmaverick/UniversalStyleTransfer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-08-20T19:23:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-13T06:29:54Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9774171052371489
      ],
      "excerpt": "Torch implementation of our NIPS17 paper on universal style transfer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9808961640142531,
        0.8705139597093114
      ],
      "excerpt": "By default, we perform WCT (whitening and coloring transform) on conv1-5 features. The \"-alpha\" serves as the style weight to control the transfer effect. Some transfer results and comparisons with existing methods are shown here. \nFor comparison, we replace WCT with AdaIN (adaptive instance normalization), another way of transform proposed in [Huang et al., ICCV17]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9351320254255244
      ],
      "excerpt": "By setting the content image as a random noise image, our stylization framework can be easily applied to texture synthesis. Different input noise leads to diverse synthesis results. Moreover, we can adjust the parameter \"-styleSize\" as a kind of scale control to obtain different effects. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9921438026984696
      ],
      "excerpt": "Our method also supports the transferring or synthesis of multiple styles thourgh interpolation. Below is an example of handling two styles. The \"-beta\" serves as the intepolation weight. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936054558007817
      ],
      "excerpt": "Below we show an example of texture synthesis path from t1 (left) to t2, then t3, ..., to t8 and back to t1. We also try the Neural 3D Mesh Renderer to apply the synthesized texture on a 3D Mesh. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.959816340972305
      ],
      "excerpt": "Often times, the one-click global transfer still does not meet requirements from professinal users (e.g., artists). Users prefer to transfer different styles to different regions in the content image, i.e., spatial control. We provide an example of transferring two styles to the foreground and background respectively, i.e., Style I for foreground (mask=1), Style II for background (mask=0), provided a binary mask. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8958688252129181,
        0.9867877083733534
      ],
      "excerpt": "We also include the Style-swap function in our algorithm. For each whitened content feature patch, we swap it with nearest whitened style feature patch. Please refer to the Style-swap paper for more details. \nWe provide a parameter \"-swap5\" to perform swap operation on conv5 features. As the swap operation is computationally expensive (searching nearest patches), we do not carry out the swapping on early layers with large feature maps (e.g., conv1-4). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9958001388125635
      ],
      "excerpt": "Below is an exemplary comparison between w/o and w/ swap operation on conv5. With the swapping, the eyeball in the content is replaced with the ball in the style (bottom) as they are cloeset neighbours in whitened feature space. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "The source code of NIPS17 'Universal Style Transfer via Feature Transforms'.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Yijunmaverick/UniversalStyleTransfer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 91,
      "date": "Thu, 30 Dec 2021 10:55:12 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Yijunmaverick/UniversalStyleTransfer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Yijunmaverick/UniversalStyleTransfer",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.9267662332050112
      ],
      "excerpt": "<img src='figs/p3.jpg' width=800> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9197735714648932
      ],
      "excerpt": "<img src='figs/p4.jpg' width=800> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9197735714648932
      ],
      "excerpt": "<img src='figs/p6.jpg' width=800> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9197735714648932
      ],
      "excerpt": "<img src='figs/p5.jpg' width=800> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9284696707931025,
        0.9284696707931025,
        0.9284696707931025,
        0.9284696707931025,
        0.9284696707931025,
        0.9284696707931025,
        0.9284696707931025,
        0.9284696707931025
      ],
      "excerpt": "    <img src='figs/t.jpg' height=100 width=100 /> \n    <img src='figs/t0.jpg' height=100 width=100 /> \n    <img src='figs/t3.jpg' height=100 width=100 />  \n    <img src='figs/t4.jpg' height=100 width=100 /> \n    <img src='figs/t5.jpg' height=100 width=100 /> \n    <img src='figs/t6.jpg' height=100 width=100 /> \n    <img src='figs/t7.jpg' height=100 width=100 /> \n    <img src='figs/t8.jpg' height=100 width=100 /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9284696707931025,
        0.9284696707931025,
        0.8771009405499612,
        0.8771009405499612
      ],
      "excerpt": "    <img src='figs/white.jpg' height=100 width=100 /> \n    <img src='figs/white.jpg' height=100 width=100 />  \n    <img src='figs/texture_interpolation.gif' width=200 /> \n    <img src='figs/3D_render.gif' width=200 /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9144138700892136
      ],
      "excerpt": "<img src='figs/p2.jpg' width=800> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9267662332050112
      ],
      "excerpt": "<img src='figs/p1.jpg' width=840> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Yijunmaverick/UniversalStyleTransfer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Lua"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "[UniversalStyleTransfer](https://arxiv.org/pdf/1705.08086.pdf)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "UniversalStyleTransfer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Yijunmaverick",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Yijunmaverick/UniversalStyleTransfer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Linux\n- NVIDIA GPU + CUDA CuDNN\n- Torch \n- Pretrained [encoders & decoders](https://drive.google.com/open?id=0B8_MZ8a8aoSeWm9HSTdXNE9Eejg) for image reconstruction only (put them under models/). If no CuDNN installed, please download the decoders [here](https://drive.google.com/open?id=1wVPtu4gGkUWpb9YtRecveFvdo4ZN5kMD).\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 532,
      "date": "Thu, 30 Dec 2021 10:55:12 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "A common issue in style transfer is the evaluation. To understand the user preference, we conduct a user study to evaluate 5 methods shown in Figure 6 of our paper. We appreciate your help if you can spend 3~4 minutes on this [study](http://169.236.182.49:8008/). Check the updated feedback [here](http://169.236.182.49:8008/dump) from extensive users. Please be polite and serious on results and comments.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}