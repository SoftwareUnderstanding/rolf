{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This code is modelled based on another [implementation](https://raw.githubusercontent.com/blackbbc/NER) of the research paper.\n\nThis is repository is part of the [DITK](https://github.com/data-integration-toolkit/ditk) project as per the [USC CSCI 548, Spring '19](https://classes.usc.edu/term-20191/course/csci-548/) graduate course.\n\nPlease email your questions or comments to [Sarvesh Parab](http://www.sarveshparab.com/).\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        0.9217379280481747
      ],
      "excerpt": "Matthew E. Peters \u2022 Mark Neumann \u2022 Mohit Iyyer \u2022 Matt Gardner \u2022 Christopher Clark \u2022 Kenton Lee \u2022 Luke Zettlemoyer \u2022 Deep contextualized word representations ACL 2015.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9893482948343519
      ],
      "excerpt": "| Peters et. al | CoNLL 2003 | 92.22(+/-0.10)   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554441738822752
      ],
      "excerpt": "Video Link \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sarveshsparab/DeepElmoEmbedNer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-28T04:41:36Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-07T17:29:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9788693109194366
      ],
      "excerpt": "This repo is based on the following paper and Github implementation: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8465201378881878,
        0.8725337925037437,
        0.8779048511966381,
        0.8197401021417796
      ],
      "excerpt": "Tokenize the input and fetch the glove representations for each token \nFeed these tokenized sentences in the reverse order into model. This paper uses a stacked, multi-layer(L) LSTM. Whereas a single-layer LSTM would take the sequence of words as input, a multi-layer LSTM trains multiple LSTMs to take the output sequence of the LSTM in the previous layer as input \nAdd the original word vectors, we have 2L + 1 vectors that can be used to compute the context representation of every word \nUsing these word representations the CRF layer estimates the best tag candidate with highest probability. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sarveshsparab/DeepElmoEmbedNer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 08:39:12 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sarveshsparab/DeepElmoEmbedNer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "sarveshsparab/DeepElmoEmbedNer",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/sarveshsparab/DeepElmoEmbedNer/master/notebook/DeepElmoEmbedNer.ipynb",
      "https://raw.githubusercontent.com/sarveshsparab/DeepElmoEmbedNer/master/notebook/.ipynb_checkpoints/DeepElmoEmbedNer-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8891401888405708
      ],
      "excerpt": "GET     VB      B-VP    O \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8891401888405708
      ],
      "excerpt": "    GET     O       O \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8693252575240425
      ],
      "excerpt": "text file containing sentence in following format: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8306403994737503,
        0.858574204543802
      ],
      "excerpt": "Sample Input \nJAPAN   NNP     B-NP    B-LOC \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468,
        0.8997243352845468
      ],
      "excerpt": "LUCKY   NNP     B-NP    O \nWIN     NNP     I-NP    O \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8775139593253132
      ],
      "excerpt": "Sample Output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8825877338537679
      ],
      "excerpt": "train/test/dev dataset in input files \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8710041956474793,
        0.8532453456487425
      ],
      "excerpt": "Download pre-trained word vector from http://nlp.stanford.edu/data/glove.6B.zip, unzip glove.6B.50d.txt to resources/pretrained/glove. \nDownload pre-trained elmo models elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5 and elmo_2x1024_128_2048cnn_1xhighway_options.json, put them in resources/elmo. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924051348565278
      ],
      "excerpt": "| Model  | Dataset    | Test F1 | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sarveshsparab/DeepElmoEmbedNer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep contextualized word representations",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepElmoEmbedNer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "sarveshsparab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sarveshsparab/DeepElmoEmbedNer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- python 3.6\n- tensorflow 1.10.0\n- numpy 1.14.3\n- gensim 3.6.0\n- tqdm 4.26.0\n  - Find specific machine TensorFlow version from the follwoing link\n    - https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.12/tensorflow/g3doc/get_started/os_setup.md\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": " 1. Import the Module from main.py\n\n    ```python\n    from model.main import DeepElmoEmbedNer\n    ```\n    \n 2. Create Instance of the module\n\n    ```python\n    deen = DeepElmoEmbedNer()\n    ```\n\n 3. First call read_dataset() and give all the files required to train in the given order\n   - file_dict\n   - dataset_name\n   *Refer [notebook](./notebook/DeepElmoEmbedNer.ipynb) for details on additional parameters*\n\n        ```python\n        file_dict = dict()\n        file_dict['train'] = './data/sample/ner_test_input.txt'\n        file_dict['test'] = './data/sample/ner_test_input.txt'\n        file_dict['dev'] = './data/sample/ner_test_input.txt'\n        \n        data = deen.read_dataset( file_dict, \"CoNLL03\" )\n        ```\n\n 4. To train run the following commands. If already trained you can skip this.\n    Parameters needed in order\n    - data\n    *Refer [notebook](./notebook/DeepElmoEmbedNer.ipynb) for details on additional parameters*\n    \n        ```python\n        model, sess, saver = deen.train(data)\n        ```\n\n 5. To predict on any sentence give path to file.\n    Parameters needed in order\n    - Test file path\n    *Refer [notebook](./notebook/DeepElmoEmbedNer.ipynb) for details on additional parameters*\n\n    And each line should contain text in following format:\n    - **\\<word\\>  \\<POS\\>   \\<Chung tag\\>   \\<NER tag\\>**\n    - Sample file in the correct format can be found [here](data/sample/ner_test_input.txt)\n\n        ```python\n        predictions = deen.predict(test_file_path, writeInputToFile=False, model=model, sess=sess, saver=saver, trainedData=data['train'])\n        print(predictions)\n        ```\n\n 6. Evaluate on provided dataset in the predict\n    Parameters needed in order\n    - predictions\n    - groundTruths\n    *Refer [notebook](./notebook/DeepElmoEmbedNer.ipynb) for details on additional parameters*\n    - **Note**: In this case ground_truth must be present, or can be generated with the *convert_ground_truth* method \n\n      ```python\n       precision, recall, f1_score = deen.evaluate(predictions, groundTruths)\n       print(\"precision: {}\\trecall: {}\\tf1: {}\".format(precision, recall, f1_score))\n      ```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sun, 26 Dec 2021 08:39:12 GMT"
    },
    "technique": "GitHub API"
  }
}