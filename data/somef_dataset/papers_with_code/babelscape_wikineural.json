{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We gratefully acknowledge the support of the **ERC Consolidator Grant MOUSSE No. 726487** under the European Union\u2019s Horizon2020 research and innovation programme ([http://mousse-project.org/](http://mousse-project.org/)).\n\nThis work was also supported by the **PerLIR project** (Personal Linguistic resources in Information Retrieval) funded by the MIUR Progetti di ricerca di Rilevante Interesse Nazionale programme (PRIN2017).\n\nThe code in this repository is built on top of [![](https://shields.io/badge/-nn--template-emerald?style=flat&logo=github&labelColor=gray)](https://github.com/lucmos/nn-template).\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{tedeschi-etal-2021-wikineural-combined,\n    title = \"{W}iki{NE}u{R}al: {C}ombined Neural and Knowledge-based Silver Data Creation for Multilingual {NER}\",\n    author = \"Tedeschi, Simone  and\n      Maiorca, Valentino  and\n      Campolungo, Niccol{\\`o}  and\n      Cecconi, Francesco  and\n      Navigli, Roberto\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2021\",\n    month = nov,\n    year = \"2021\",\n    address = \"Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.findings-emnlp.215\",\n    pages = \"2521--2533\",\n    abstract = \"Multilingual Named Entity Recognition (NER) is a key intermediate task which is needed in many areas of NLP. In this paper, we address the well-known issue of data scarcity in NER, especially relevant when moving to a multilingual scenario, and go beyond current approaches to the creation of multilingual silver data for the task. We exploit the texts of Wikipedia and introduce a new methodology based on the effective combination of knowledge-based approaches and neural models, together with a novel domain adaptation technique, to produce high-quality training corpora for NER. We evaluate our datasets extensively on standard benchmarks for NER, yielding substantial improvements up to 6 span-based F1-score points over previous state-of-the-art systems for data creation.\",\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| WikiNEuRal DE | 124k | 2.19M | 60k | 32k | 59k | 25k | 1.87M | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "| WikiNEuRal DE DA (CoNLL) | 41k | 706k | 17k | 12k | 23k | 3k | 0.61M | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Babelscape/wikineural",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-10T19:23:17Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-26T23:11:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Data and evaluation code for the paper WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER (EMNLP 2021).",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/babelscape/wikineural/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Mon, 27 Dec 2021 07:42:41 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Babelscape/wikineural/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Babelscape/wikineural",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/babelscape/wikineural/master/test.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8628315172195287
      ],
      "excerpt": "If you evaluate your NER system on one or more WikiNEuRal test sets, you can submit your results on the corresponding Papers With Code WikiNEuRal Leaderboards, and obtain a badge if your system performs well!! \ud83d\ude80  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Babelscape/wikineural/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/babelscape/wikineural/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Valentino Maiorca, Luca Moschella\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Data",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "wikineural",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Babelscape",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Babelscape/wikineural/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Mon, 27 Dec 2021 07:42:41 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "natural-language-processing",
      "named-entity-recognition",
      "multilinguality",
      "dataset"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. To train 10 models on WikiNEuRal English, run:\n    ```\n    python run.py -m +train.seed_idx=0,1,2,3,4,5,6,7,8,9 data.datamodule.source=wikineural data.datamodule.language=en\n    ```\n    **note 1**: we show how to train 10 models on the same training set because in our paper we used exactly 10 different seeds to compute the mean and standard deviation, which were then used to measure the [statistical significance](scripts/significance.py) w.r.t. other training sets.\n\n    **note 2**: for the EN, ES, NL and DE versions of WikiNEuRal, you can combine the train, val and test sets of WikiNEuRal to obtain a larger training set (as we did in the paper), and use the CoNLL splits as validation and testing material (e.g., copy `data/conll/en/val.conllu` into `data/wikineural/en/`). Similarly, for RU and PL you can use the BSNLP splits for validation and testing. For the other languages instead, since no gold-standard data are available, you can use the standard WikiNEuRal splits to validate your model. Finally, for the domain-adapted datasets you can use both the val and test sets of the datasets on which they have been adapted (if available) and the val and test sets of WikiNEuRal for the same language.\n\n2. To produce results for the 10 trained models, run:\n    ```\n    bash test.sh\n    ```\n    \n    `test.sh` also contains more complex bash for loops that can produce results on multiple datasets / models at once.\n\n<br>\n\n",
      "technique": "Header extraction"
    }
  ]
}