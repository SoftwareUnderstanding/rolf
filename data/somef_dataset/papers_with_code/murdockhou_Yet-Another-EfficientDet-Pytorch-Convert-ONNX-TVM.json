{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1911.09070\n\n\n# Performance\n\n## Pretrained weights and benchmark\n\nThe performance is very close to the paper's, it is still SOTA. \n\nThe speed/FPS test includes the time of post-processing with no jit/data precision trick.\n\n| coefficient | pth_download | GPU Mem(MB"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Appreciate the great work from the following repositories:\n- [google/automl](https://github.com/google/automl)\n- [lukemelas/EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch)\n- [signatrix/efficientdet](https://github.com/signatrix/efficientdet)\n- [vacancy/Synchronized-BatchNorm-PyTorch](https://github.com/vacancy/Synchronized-BatchNorm-PyTorch)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8594278631206737
      ],
      "excerpt": "Sincerely thank you for your generosity. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/murdockhou/Yet-Another-EfficientDet-Pytorch-Convert-ONNX-TVM",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-29T09:31:50Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-15T03:51:16Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9830507302806305,
        0.9468788183467789
      ],
      "excerpt": "This repo is based on the Yet-Another-EfficientDet-Pytorch repo. There are many needs to convert this efficientdet network into ONNX, so we make this repo to help poeple to convert model into ONNX or TVM. Note that this repo only provide function how to convert model to ONNX or TVM, not focusing on model training or other things. If you want to train or test this efficientdet model, the best way is refer to original Yet-Another-EfficientDet-Pytorch repo. \nWe have change some code based on this original repo to help convert successfully. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "        return _op.nn.pad(data, pad_width, pad_value) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.8144364213466861
      ],
      "excerpt": "def transpose(data, axes=None): \n    \"\"\"Permutes the dimensions of an array. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.9578054974183009
      ],
      "excerpt": "data : relay.Expr \n    The input data to the operator. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "return _make.transpose(data, axes) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9388617563709796,
        0.9166515160490516,
        0.9828986790271466,
        0.8840432864294594
      ],
      "excerpt": "The work was a collaborative effort between lampson and I, mostly is lampson contribution. Also thanks for the original Yet-Another-EfficientDet-Pytorch repo provided correct efficientdet results. \nThe pytorch re-implement of the official EfficientDet with SOTA performance in real time, original paper link: https://arxiv.org/abs/1911.09070 \nThe performance is very close to the paper's, it is still SOTA.  \nThe speed/FPS test includes the time of post-processing with no jit/data precision trick. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9248581730592745
      ],
      "excerpt": "This pure-pytorch implement is 26 times faster than the official Tensorflow version without any trick. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8337753695480845
      ],
      "excerpt": "1. Prepare two image tensor with the same content, size (1,3,512,512)-pytorch, (1,512,512,3)-tensorflow. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8573675457944262
      ],
      "excerpt": "3. Run 10 times with batchsize 1 and calculate the average time, including post-processing and visualization, to make the test more practical. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391544905262821
      ],
      "excerpt": "[2020-04-05] create this repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8675681980947753,
        0.8675681980947753,
        0.9111685464440133
      ],
      "excerpt": "[X] efficientdet D6 supports \n[X] efficientdet D7 supports \nOfficial EfficientDet use TensorFlow bilinear interpolation to resize image inputs, while it is different from many other methods (opencv/pytorch), so the output is definitely slightly different from the official one. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/murdockhou/Yet-Another-EfficientDet-Pytorch-Convert-ONNX-TVM/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "**Q1. Why implement this while there are several efficientdet pytorch projects already.**\n\nA1: Because AFAIK none of them fully recovers the true algorithm of the official efficientdet, that's why their communities could not achieve or having a hard time to achieve the same score as the official efficientdet by training from scratch.\n\n**Q2: What exactly is the difference among this repository and the others?**\n\nA2: For example, these two are the most popular efficientdet-pytorch,\n\nhttps://github.com/toandaominh1997/EfficientDet.Pytorch\n\nhttps://github.com/signatrix/efficientdet\n\nHere is the issues and why these are difficult to achieve the same score as the official one:\n\nThe first one:\n\n1. Altered EfficientNet the wrong way, strides have been changed to adapt the BiFPN, but we should be aware that efficientnet's great performance comes from it's specific parameters combinations. Any slight alteration could lead to worse performance.\n\nThe second one:\n\n1. Pytorch's BatchNormalization is slightly different from TensorFlow, momentum_pytorch = 1 - momentum_tensorflow. Well I didn't realize this trap if I paid less attentions. signatrix/efficientdet succeeded the parameter from TensorFlow, so the BN will perform badly because running mean and the running variance is being dominated by the new input.\n\n2. Mis-implement of Depthwise-Separable Conv2D. Depthwise-Separable Conv2D is Depthwise-Conv2D and Pointwise-Conv2D and BiasAdd ,there is only a BiasAdd after two Conv2D, while signatrix/efficientdet has a extra BiasAdd on Depthwise-Conv2D.\n\n3. Misunderstand the first parameter of MaxPooling2D, the first parameter is kernel_size, instead of stride.\n\n4. Missing BN after downchannel of the feature of the efficientnet output.\n\n5. Using the wrong output feature of the efficientnet. This is big one. It takes whatever output that has the conv.stride of 2, but it's wrong. It should be the one whose next conv.stride is 2 or the final output of efficientnet.\n\n6. Does not apply same padding on Conv2D and Pooling.\n\n7. Missing swish activation after several operations.\n\n8. Missing Conv/BN operations in BiFPN, Regressor and Classifier. This one is very tricky, if you don't dig deeper into the official implement, there are some same operations with different weights.\n\n        \n        illustration of a minimal bifpn unit\n            P7_0 -------------------------> P7_2 -------->\n               |-------------|                \u2191\n                             \u2193                |\n            P6_0 ---------> P6_1 ---------> P6_2 -------->\n               |-------------|--------------\u2191 \u2191\n                             \u2193                |\n            P5_0 ---------> P5_1 ---------> P5_2 -------->\n               |-------------|--------------\u2191 \u2191\n                             \u2193                |\n            P4_0 ---------> P4_1 ---------> P4_2 -------->\n               |-------------|--------------\u2191 \u2191\n                             |--------------\u2193 |\n            P3_0 -------------------------> P3_2 -------->\n        \n    For example, P4 will downchannel to P4_0, then it goes P4_1, \n    anyone may takes it for granted that P4_0 goes to P4_2 directly, right?\n    \n    That's why they are wrong, \n    P4 should downchannel again with a different weights to P4_0_another, \n    then it goes to P4_2.\n    \nAnd finally some common issues, their anchor decoder and encoder are different from the original one, but it's not the main reason that it performs badly.\n\nAlso, Conv2dStaticSamePadding from [EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch) does not perform like TensorFlow, the padding strategy is different. So I implement a real tensorflow-style [Conv2dStaticSamePadding](efficientnet/utils_extra.py#L9) and [MaxPool2dStaticSamePadding](efficientnet/utils_extra.py#L55) myself.\n\nDespite of the above issues, they are great repositories that enlighten me, hence there is this repository.\n\nThis repository is mainly based on [efficientdet](https://github.com/signatrix/efficientdet), with the changing that makes sure that it performs as closer as possible as the paper.\n\nBtw, debugging static-graph TensorFlow v1 is really painful. Don't try to export it with automation tools like tf-onnx or mmdnn, they will only cause more problems because of its custom/complex operations. \n\nAnd even if you succeeded, like I did, you will have to deal with the crazy messed up machine-generated code under the same class that takes more time to refactor than translating it from scratch.\n\n**Q3: What should I do when I find a bug?** \n\nA3: Check out the update log if it's been fixed, then pull the latest code to try again. If it doesn't help, create a new issue and describe it in detail.\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 13,
      "date": "Sun, 26 Dec 2021 11:15:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/murdockhou/Yet-Another-EfficientDet-Pytorch-Convert-ONNX-TVM/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "murdockhou/Yet-Another-EfficientDet-Pytorch-Convert-ONNX-TVM",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/murdockhou/Yet-Another-EfficientDet-Pytorch-Convert-ONNX-TVM/master/tutorial/train_shape.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    \n    python train.py -c 2 --batch_size 8 --lr 1e-5 \\\n     --load_weights last \\\n     --head_only True\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    pip install pycocotools numpy opencv-python tqdm tensorboard tensorboardX pyyaml\n    pip install torch==1.4.0\n    pip install torchvision==0.5.0\n     \n    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8994820745206502,
        0.9417342080153969
      ],
      "excerpt": "We have tested on tvm version on commit f08d5d78ee000b2c113ac451f8d73817960eafd5 , other version not tested so can not make sure work well too. \nFirst, you need to install tvm, refer to its documentation. We call your tvm install source dir is tvm_home. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8528156738301135
      ],
      "excerpt": "Finally, you can run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.871073466070927
      ],
      "excerpt": "[2020-04-14] for those who needs help or can't get a good result after several epochs, check out this tutorial. You can run it on colab with GPU support. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.895222649498447
      ],
      "excerpt": " -w /path/to/your/weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9220841133565062
      ],
      "excerpt": "If you like this repository, or if you'd like to support the author for any reason, you can donate to the author. Feel free to send me your name or introducing pages, I will make sure your name(s) on the sponsors list.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9269381732104042
      ],
      "excerpt": "Sincerely thank you for your generosity. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8931879487461446
      ],
      "excerpt": "python3 convert/convert_onnx.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.853903713292443
      ],
      "excerpt": "        pad_v = padding.type_annotation.shape  #: change here and next line \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138276678013063
      ],
      "excerpt": "Next, change code on tvm_home/python/tvm/relay/op/transform.py line 107, change transpose(data, axes=None) function to: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8102780086823556
      ],
      "excerpt": "result : relay.Expr \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8931879487461446
      ],
      "excerpt": "python3 convert/from_pytorch.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.910482952524268
      ],
      "excerpt": "| D3 | efficientdet-d3.pth | 1647 | 22.73 | - | 44.9 | 45.8 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8169444991061906
      ],
      "excerpt": "| D6 | efficientdet-d6.pth | 2985 | 5.30 | - | 50.1 | 51.7 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356187631491789
      ],
      "excerpt": "[2020-04-10] warp the loss function within the training model, so that the memory usage will be balanced when training with multiple gpus, enabling training with bigger batchsize.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579
      ],
      "excerpt": "            -*.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579
      ],
      "excerpt": "            -*.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8966741630551273,
        0.9127575607363352,
        0.9258996476701051
      ],
      "excerpt": "python train.py -c 0 --batch_size 12 \npython train.py -c 1 --batch_size 8 --lr 1e-5 \npython train.py -c 2 --batch_size 8 --lr 1e-5 --num_epochs 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python coco_eval.py -p your_project_name -c 5 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/murdockhou/Yet-Another-EfficientDet-Pytorch-Convert-ONNX-TVM/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "GNU Lesser General Public License v3.0",
      "url": "https://api.github.com/licenses/lgpl-3.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                   GNU LESSER GENERAL PUBLIC LICENSE\\n                       Version 3, 29 June 2007\\n\\n Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/\\n Everyone is permitted to copy and distribute verbatim copies\\n of this license document, but changing it is not allowed.\\n\\n\\n  This version of the GNU Lesser General Public License incorporates\\nthe terms and conditions of version 3 of the GNU General Public\\nLicense, supplemented by the additional permissions listed below.\\n\\n  0. Additional Definitions.\\n\\n  As used herein, \"this License\" refers to version 3 of the GNU Lesser\\nGeneral Public License, and the \"GNU GPL\" refers to version 3 of the GNU\\nGeneral Public License.\\n\\n  \"The Library\" refers to a covered work governed by this License,\\nother than an Application or a Combined Work as defined below.\\n\\n  An \"Application\" is any work that makes use of an interface provided\\nby the Library, but which is not otherwise based on the Library.\\nDefining a subclass of a class defined by the Library is deemed a mode\\nof using an interface provided by the Library.\\n\\n  A \"Combined Work\" is a work produced by combining or linking an\\nApplication with the Library.  The particular version of the Library\\nwith which the Combined Work was made is also called the \"Linked\\nVersion\".\\n\\n  The \"Minimal Corresponding Source\" for a Combined Work means the\\nCorresponding Source for the Combined Work, excluding any source code\\nfor portions of the Combined Work that, considered in isolation, are\\nbased on the Application, and not on the Linked Version.\\n\\n  The \"Corresponding Application Code\" for a Combined Work means the\\nobject code and/or source code for the Application, including any data\\nand utility programs needed for reproducing the Combined Work from the\\nApplication, but excluding the System Libraries of the Combined Work.\\n\\n  1. Exception to Section 3 of the GNU GPL.\\n\\n  You may convey a covered work under sections 3 and 4 of this License\\nwithout being bound by section 3 of the GNU GPL.\\n\\n  2. Conveying Modified Versions.\\n\\n  If you modify a copy of the Library, and, in your modifications, a\\nfacility refers to a function or data to be supplied by an Application\\nthat uses the facility (other than as an argument passed when the\\nfacility is invoked), then you may convey a copy of the modified\\nversion:\\n\\n   a) under this License, provided that you make a good faith effort to\\n   ensure that, in the event an Application does not supply the\\n   function or data, the facility still operates, and performs\\n   whatever part of its purpose remains meaningful, or\\n\\n   b) under the GNU GPL, with none of the additional permissions of\\n   this License applicable to that copy.\\n\\n  3. Object Code Incorporating Material from Library Header Files.\\n\\n  The object code form of an Application may incorporate material from\\na header file that is part of the Library.  You may convey such object\\ncode under terms of your choice, provided that, if the incorporated\\nmaterial is not limited to numerical parameters, data structure\\nlayouts and accessors, or small macros, inline functions and templates\\n(ten or fewer lines in length), you do both of the following:\\n\\n   a) Give prominent notice with each copy of the object code that the\\n   Library is used in it and that the Library and its use are\\n   covered by this License.\\n\\n   b) Accompany the object code with a copy of the GNU GPL and this license\\n   document.\\n\\n  4. Combined Works.\\n\\n  You may convey a Combined Work under terms of your choice that,\\ntaken together, effectively do not restrict modification of the\\nportions of the Library contained in the Combined Work and reverse\\nengineering for debugging such modifications, if you also do each of\\nthe following:\\n\\n   a) Give prominent notice with each copy of the Combined Work that\\n   the Library is used in it and that the Library and its use are\\n   covered by this License.\\n\\n   b) Accompany the Combined Work with a copy of the GNU GPL and this license\\n   document.\\n\\n   c) For a Combined Work that displays copyright notices during\\n   execution, include the copyright notice for the Library among\\n   these notices, as well as a reference directing the user to the\\n   copies of the GNU GPL and this license document.\\n\\n   d) Do one of the following:\\n\\n       0) Convey the Minimal Corresponding Source under the terms of this\\n       License, and the Corresponding Application Code in a form\\n       suitable for, and under terms that permit, the user to\\n       recombine or relink the Application with a modified version of\\n       the Linked Version to produce a modified Combined Work, in the\\n       manner specified by section 6 of the GNU GPL for conveying\\n       Corresponding Source.\\n\\n       1) Use a suitable shared library mechanism for linking with the\\n       Library.  A suitable mechanism is one that (a) uses at run time\\n       a copy of the Library already present on the user\\'s computer\\n       system, and (b) will operate properly with a modified version\\n       of the Library that is interface-compatible with the Linked\\n       Version.\\n\\n   e) Provide Installation Information, but only if you would otherwise\\n   be required to provide such information under section 6 of the\\n   GNU GPL, and only to the extent that such information is\\n   necessary to install and execute a modified version of the\\n   Combined Work produced by recombining or relinking the\\n   Application with a modified version of the Linked Version. (If\\n   you use option 4d0, the Installation Information must accompany\\n   the Minimal Corresponding Source and Corresponding Application\\n   Code. If you use option 4d1, you must provide the Installation\\n   Information in the manner specified by section 6 of the GNU GPL\\n   for conveying Corresponding Source.)\\n\\n  5. Combined Libraries.\\n\\n  You may place library facilities that are a work based on the\\nLibrary side by side in a single library together with other library\\nfacilities that are not Applications and are not covered by this\\nLicense, and convey such a combined library under terms of your\\nchoice, if you do both of the following:\\n\\n   a) Accompany the combined library with a copy of the same work based\\n   on the Library, uncombined with any other library facilities,\\n   conveyed under the terms of this License.\\n\\n   b) Give prominent notice with the combined library that part of it\\n   is a work based on the Library, and explaining where to find the\\n   accompanying uncombined form of the same work.\\n\\n  6. Revised Versions of the GNU Lesser General Public License.\\n\\n  The Free Software Foundation may publish revised and/or new versions\\nof the GNU Lesser General Public License from time to time. Such new\\nversions will be similar in spirit to the present version, but may\\ndiffer in detail to address new problems or concerns.\\n\\n  Each version is given a distinguishing version number. If the\\nLibrary as you received it specifies that a certain numbered version\\nof the GNU Lesser General Public License \"or any later version\"\\napplies to it, you have the option of following the terms and\\nconditions either of that published version or of any later version\\npublished by the Free Software Foundation. If the Library as you\\nreceived it does not specify a version number of the GNU Lesser\\nGeneral Public License, you may choose any version of the GNU Lesser\\nGeneral Public License ever published by the Free Software Foundation.\\n\\n  If the Library as you received it specifies that a proxy can decide\\nwhether future versions of the GNU Lesser General Public License shall\\napply, that proxy\\'s public statement of acceptance of any version is\\npermanent authorization for you to choose that version for the\\nLibrary.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Update (20200429)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Yet-Another-EfficientDet-Pytorch-Convert-ONNX-TVM",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "murdockhou",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/murdockhou/Yet-Another-EfficientDet-Pytorch-Convert-ONNX-TVM/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    pip install pycocotools numpy opencv-python tqdm tensorboard tensorboardX pyyaml\n    pip install torch==1.4.0\n    pip install torchvision==0.5.0\n     \n    ",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    python efficientdet_test.py\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 50,
      "date": "Sun, 26 Dec 2021 11:15:58 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    \n    python train.py -c 2 --batch_size 8 --lr 1e-5 --num_epochs 10 \\\n     --load_weights /path/to/your/weights/efficientdet-d2.pth \\\n     --head_only True\n     \n",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    datasets/\n        -coco2017/\n            -train2017/\n                -000000000001.jpg\n                -000000000002.jpg\n                -000000000003.jpg\n            -val2017/\n                -000000000004.jpg\n                -000000000005.jpg\n                -000000000006.jpg\n            -annotations\n                -instances_train2017.json\n                -instances_val2017.json\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    project_name: coco\n    train_set: train2017\n    val_set: val2017\n    num_gpus: 4  ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    obj_list: ['person', 'bicycle', 'car', ...]\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    \n    python train.py -c 2 --batch_size 8 --lr 1e-5 \\\n     --load_weights /path/to/your/weights/efficientdet-d2.pth \\\n     --head_only True\n     \n    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    python train.py -c 2 --batch_size 8 --lr 1e-5 --debug True\n    \n    ",
      "technique": "Header extraction"
    }
  ]
}