{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/khalidsaifullaah/BERTify",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-17T21:38:56Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-16T09:36:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.958171753298394,
        0.9489223141802468,
        0.9270801756333322
      ],
      "excerpt": "This is an easy-to-use python module that helps you to extract the BERT embeddings for a large text dataset efficiently. It is intended to be used for Bengali and English texts. \nSpecially, optimized for usability in limited computational setups (i.e. free colab/kaggle GPUs). Extracting embeddings for IMDB dataset (a list of 25000 texts) took less than ~28 mins. on Colab's GPU. (Haven't perform any hardcore benchmark, so take these numbers with a grain of salt). \nTry passing all your text data through the .embedding() function at once by turning it into a list of texts. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241816043596851,
        0.8810124988092809,
        0.9631809880652219,
        0.9631809880652219
      ],
      "excerpt": "Try increasing the batch_size to make it even faster, by default we're using 64 (to be on the safe side) which doesn't throw any CUDA out of memory but I believe we can go even further. Thanks to Alex, from his empirical findings, it seems like it can be pushed until 96. So, before making the .embedding() call, you can do bertify.batch_zie=96 to set a larger batch_zie \nA module for extracting embedding from BERT model for Bengali or English text datasets. \n    For 'en' -> English data, it uses bert-base-uncased model embeddings,  \n    for 'bn' -> Bengali data, it uses sahajBERT model embeddings. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.989403511710115
      ],
      "excerpt": "lang (str, optional): language of your data. Currently supports only 'en' and 'bn'. Defaults to 'en'. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9137787877879694
      ],
      "excerpt": "The embedding function, that takes a list of texts, feed them through the model and returns a list of embeddings. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.873064955268533
      ],
      "excerpt": "texts (List[str]): A list of texts, that you want to extract embedding for (e.g. [\"This movie was a total waste of time.\", \"Whoa! Loved this movie, totally loved all the characters\"]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "An easy-to-use Python module that helps you to extract the BERT embeddings for a large text dataset (Bengali/English) efficiently.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/khalidsaifullaah/bertify/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 10:36:12 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/khalidsaifullaah/BERTify/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "khalidsaifullaah/BERTify",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ pip install git+https://github.com/khalidsaifullaah/BERTify\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8388744049658228,
        0.8590024200943598
      ],
      "excerpt": "For faster inference, make sure you're using your colab/kaggle GPU while making the .embedding() call \nTry increasing the batch_size to make it even faster, by default we're using 64 (to be on the safe side) which doesn't throw any CUDA out of memory but I believe we can go even further. Thanks to Alex, from his empirical findings, it seems like it can be pushed until 96. So, before making the .embedding() call, you can do bertify.batch_zie=96 to set a larger batch_zie \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/khalidsaifullaah/BERTify/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Khalid Saifullah\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERTify",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERTify",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "khalidsaifullaah",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/khalidsaifullaah/BERTify/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- numpy\n- torch\n- tqdm\n- transformers\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 36,
      "date": "Fri, 24 Dec 2021 10:36:12 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "bert-embeddings",
      "text-processing",
      "bert",
      "feature-extraction"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom bertify import BERTify\n\n#: Example 1: Bengali Embedding Extraction\nbn_bertify = BERTify(\n    lang=\"bn\",  #: language of your text.\n    last_four_layers_embedding=True  #: to get richer embeddings.\n)\n\n#: By default, `batch_size` is set to 64. Set `batch_size` higher for making things even faster but higher value than 96 may throw `CUDA out of memory` on Colab's GPU, so try at your own risk.\n\n#: bn_bertify.batch_size = 96\n\n#: A list of texts that we want the embedding for, can be one or many. (You can turn your whole dataset into a list of texts and pass it into the method for faster embedding extraction)\ntexts = [\"\u09ac\u09bf\u0996\u09cd\u09af\u09be\u09a4 \u09b9\u0993\u09af\u09bc\u09be\u09b0 \u09aa\u09cd\u09b0\u09a5\u09ae \u09aa\u09a6\u0995\u09cd\u09b7\u09c7\u09aa\", \"\u099c\u09c0\u09ac\u09a8\u09c7 \u09b8\u09ac\u099a\u09c7\u09df\u09c7 \u09ae\u09c2\u09b2\u09cd\u09af\u09ac\u09be\u09a8 \u099c\u09bf\u09a8\u09bf\u09b8 \u09b9\u099a\u09cd\u099b\u09c7\", \"\u09ac\u09c7\u09b6\u09bf\u09b0\u09ad\u09be\u0997 \u09ae\u09be\u09a8\u09c1\u09b7\u09c7\u09b0 \u09aa\u099b\u09a8\u09cd\u09a6\u09c7\u09b0 \u099c\u09bf\u09a8\u09bf\u09b8 \u09b9\u099a\u09cd\u099b\u09c7\"]\n\nbn_embeddings = bn_bertify.embedding(texts)   #: returns numpy matrix \n#: shape of the returned matrix in this example 3x4096 (3 -> num. of texts, 4096 -> embedding dim.)\n\n\n\n\n#: Example 2: English Embedding Extraction\nen_bertify = BERTify(\n    lang=\"en\",\n    last_four_layers_embedding=True\n)\n\n#: bn_bertify.batch_size = 96\n\ntexts = [\"how are you doing?\", \"I don't know about this.\", \"This is the most important thing.\"]\nen_embeddings = en_bertify.embedding(texts) \n#: shape of the returned matrix in this example 3x3072 (3 -> num. of texts, 3072 -> embedding dim.)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}