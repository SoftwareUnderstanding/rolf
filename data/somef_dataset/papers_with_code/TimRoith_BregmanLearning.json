{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2105.04319\n\n<a id=\"2\">[2]</a> Woatao Yin, Stanley Osher, Donald Goldfarb, Jerome Darbon. \"Bregman iterative algorithms for \\ell_1-minimization with applications to compressed sensing.\" SIAM Journal on Imaging sciences 1.1 (2008",
      "https://arxiv.org/abs/1412.6980\n\n<a id=\"4\">[4]</a> Leon Bungert, Tim Roith, Daniel Tenbrinck, Martin Burger. \"Neural Architecture Search via Bregman Iterations.\" arXiv preprint arXiv:2106.02479 (2021",
      "https://arxiv.org/abs/2106.02479",
      "https://arxiv.org/abs/2105.04319 (2021). https://arxiv.org/abs/2105.04319\n\n<a id=\"2\">[2]</a> Woatao Yin, Stanley Osher, Donald Goldfarb, Jerome Darbon. \"Bregman iterative algorithms for \\ell_1-minimization with applications to compressed sensing.\" SIAM Journal on Imaging sciences 1.1 (2008): 143-168.\n\n<a id=\"3\">[3]</a> Diederik Kingma, Jimmy Lei Ba. \"Adam: A Method for Stochastic Optimization.\" arXiv preprint https://arxiv.org/abs/1412.6980 (2014). https://arxiv.org/abs/1412.6980\n\n<a id=\"4\">[4]</a> Leon Bungert, Tim Roith, Daniel Tenbrinck, Martin Burger. \"Neural Architecture Search via Bregman Iterations.\" arXiv preprint https://arxiv.org/abs/2106.02479 (2021). https://arxiv.org/abs/2106.02479",
      "https://arxiv.org/abs/1412.6980 (2014). https://arxiv.org/abs/1412.6980\n\n<a id=\"4\">[4]</a> Leon Bungert, Tim Roith, Daniel Tenbrinck, Martin Burger. \"Neural Architecture Search via Bregman Iterations.\" arXiv preprint https://arxiv.org/abs/2106.02479 (2021). https://arxiv.org/abs/2106.02479",
      "https://arxiv.org/abs/2106.02479 (2021). https://arxiv.org/abs/2106.02479"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<a id=\"1\">[1]</a> Leon Bungert, Tim Roith, Daniel Tenbrinck, Martin Burger. \"A Bregman Learning Framework for Sparse Neural Networks.\" arXiv preprint arXiv:2105.04319 (2021). https://arxiv.org/abs/2105.04319\n\n<a id=\"2\">[2]</a> Woatao Yin, Stanley Osher, Donald Goldfarb, Jerome Darbon. \"Bregman iterative algorithms for \\ell_1-minimization with applications to compressed sensing.\" SIAM Journal on Imaging sciences 1.1 (2008): 143-168.\n\n<a id=\"3\">[3]</a> Diederik Kingma, Jimmy Lei Ba. \"Adam: A Method for Stochastic Optimization.\" arXiv preprint arXiv:1412.6980 (2014). https://arxiv.org/abs/1412.6980\n\n<a id=\"4\">[4]</a> Leon Bungert, Tim Roith, Daniel Tenbrinck, Martin Burger. \"Neural Architecture Search via Bregman Iterations.\" arXiv preprint arXiv:2106.02479 (2021). https://arxiv.org/abs/2106.02479\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{bungert2021bregman,\n      title={A Bregman Learning Framework for Sparse Neural Networks}, \n      author={Leon Bungert and Tim Roith and Daniel Tenbrinck and Martin Burger},\n      year={2021},\n      eprint={2105.04319},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TimRoith/BregmanLearning",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-07T22:02:24Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-28T01:53:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Our Bregman learning framework aims at training sparse neural networks in an inverse scale space manner, starting with very few parameters and gradually adding only relevant parameters during training. We train a neural network <img src=\"https://latex.codecogs.com/svg.latex?f_\\theta:\\mathcal{X}\\rightarrow\\mathcal{Y}\" title=\"net\"/> parametrized by weights <img src=\"https://latex.codecogs.com/svg.latex?\\theta\" title=\"weights\"/> using the simple baseline algorithm\n<p align=\"center\">\n      <img src=\"https://latex.codecogs.com/svg.latex?\\begin{cases}v\\gets\\,v-\\tau\\hat{\\nabla}\\mathcal{L}(\\theta),\\\\\\theta\\gets\\mathrm{prox}_{\\delta\\,J}(\\delta\\,v),\\end{cases}\" title=\"Update\" />\n</p>\n\nwhere \n* <img src=\"https://latex.codecogs.com/svg.latex?\\mathcal{L}\" title=\"loss\"/> denotes a loss function with stochastic gradient <img src=\"https://latex.codecogs.com/svg.latex?\\hat{\\nabla}\\mathcal{L}\" title=\"stochgrad\"/>,\n* <img src=\"https://latex.codecogs.com/svg.latex?J\" title=\"J\"/> is a sparsity-enforcing functional, e.g., the <img src=\"https://latex.codecogs.com/svg.latex?\\ell_1\" title=\"ell1\"/>-norm,\n* <img src=\"https://latex.codecogs.com/svg.latex?\\mathrm{prox}_{\\delta\\,J}\" title=\"prox\"/> is the proximal operator of <img src=\"https://latex.codecogs.com/svg.latex?J\" title=\"J\"/>.\n\nOur algorithm is based on linearized Bregman iterations [[2]](#2) and is a simple extension of stochastic gradient descent which is recovered choosing <img src=\"https://latex.codecogs.com/svg.latex?J=0\" title=\"Jzero\"/>. We also provide accelerations of our baseline algorithm using momentum and Adam [[3]](#3). \n\nThe variable <img src=\"https://latex.codecogs.com/svg.latex?v\" title=\"v\"/> is a subgradient of <img src=\"https://latex.codecogs.com/svg.latex?\\theta\" title=\"weights\"/> with respect to the *elastic net* functional \n\n<p align=\"center\">\n      <img src=\"https://latex.codecogs.com/svg.latex?J_\\delta(\\theta)=J(\\theta)+\\frac1\\delta\\|\\theta\\|^2\" title=\"el-net\"/>\n</p>\n\nand stores the information which parameters are non-zero.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9479654087691911,
        0.943835178372322
      ],
      "excerpt": "Implementation of the inverse scale space training algorithms for sparse neural networks, proposed in A Bregman Learning Framework for Sparse Neural Networks [1]. \nFeel free to use it and please refer to our paper when doing so. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8971557476068865
      ],
      "excerpt": "We compare the LinBreg optimizer to standard SGD and proximal descent. The respective notebook can be found at  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "DenseNet and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Optimizing neural networks via an inverse scale space flow.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TimRoith/BregmanLearning/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 21:39:22 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/TimRoith/BregmanLearning/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "TimRoith/BregmanLearning",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/TimRoith/BregmanLearning/main/notebooks/MLP-Classification.ipynb",
      "https://raw.githubusercontent.com/TimRoith/BregmanLearning/main/notebooks/ConvNet-Classification.ipynb",
      "https://raw.githubusercontent.com/TimRoith/BregmanLearning/main/notebooks/Skip-Encoder.ipynb",
      "https://raw.githubusercontent.com/TimRoith/BregmanLearning/main/notebooks/ResNet-Classification.ipynb",
      "https://raw.githubusercontent.com/TimRoith/BregmanLearning/main/notebooks/DenseNet.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8061295512858125
      ],
      "excerpt": "The notebooks will throw errors if the datasets cannot be found. You can change the default configuration 'download':False to 'download':True in order to automatically download the necessary dataset and store it in the appropriate folder. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/TimRoith/BregmanLearning/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Tim Roith\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "\ud83d\udcc8 BregmanLearning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BregmanLearning",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "TimRoith",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TimRoith/BregmanLearning/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 29 Dec 2021 21:39:22 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "machine-learning"
    ],
    "technique": "GitHub API"
  }
}