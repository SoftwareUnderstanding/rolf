{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1502.03167. The idea is to perform normalization inside the architecture, after the layers for example. This is a strong help against overfitting and allowed to push further the state of the art accuracy.\n\nTo increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. There are a lot of papers and article dealing with how batch normalization improves the stability of the neural network, but in a nutshell :\n- It makes each layer a bit more independant from one another, and thus the model is less sensitive to noisy images, and variations in the distribution of the inputs.\n- After the normalization, there are no output of the layer that has gone really high or really low, and therefore the learning rate can be increased without fearing a gradient divergence.\n\n#### Dropout\n\nDropout was proposed by Hinton in *Dropout:  A Simple Way to Prevent Neural Networks fromOverfitting* : http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer. The idea is to randomly set the activation function of a certain percentage of a layer to the null function during the training phase. By doing this, training is more difficult because the layer can't \"collaborate\" as well with the next one. It learns then to generalize more.\n\nThe second method that we used is the Dropout. It consists in not updating each parameters of a layer when the optimizer moves one step forward. With a probability p, a neuron will remain inactive for the training minibatch. We offer more details in the section regarding state-of-the-art techniques.\n\n#### Optimization strategy\n\nAt each epoch, it will get the validation loss on the validation set. It will save only best model weight which was obtained, so prediction is not sensible to the possible overfitting. \nTo speed the training, if the val loss has not improved during n epochs, we stop early the training, we consider that overfitting point have been reached. \n\n#### New architecture\n\n![](https://markdown.data-ensta.fr/uploads/upload_2729b88fcb063f847e1b5214dc62048b.png",
      "https://arxiv.org/abs/1708.04552.\n\nThe idea is to randomly masking out square regions of input during training. It is a crucial point for convolutional nets because they spot patterns, and ocluding spatial patterns force neural networks to spot multiple patterns who could have first not be used. This can be used to improve the robustness and overall performance of convolutional neural networks. We stacked this method in our data augmentation methods to be improve the robustess of our model. An implementation can be found here:\nhttps://github.com/uoguelph-mlrg/Cutout\n\n\n\n#### Results after data augmentation\n\nWe decided to firstly launch a new training phase on the network trained after the introduction of regularization methods, but using our augmented dataset this time.\n\nThere are still 20 epochs and a learning rate of 0.001, these parameters haven't changed yet.\n\n![](https://markdown.data-ensta.fr/uploads/upload_208650a43164aef58e8f4d141a45cbe0.png",
      "https://arxiv.org/abs/1705.07485\n\nShake Shake is one of the best architectures presented on CIFAR10. \nShake Shake is based on Resnet blocks, which, stacked, make Resnet branches. \nThe idea is to replace, in the multi-branch network, the standard summation of parallel branches with a stochastic affine combination.\nIt is a good technique to work against overfitting and was first implemented in \"Shake-Shake regularization\". \n\n\nIn parallel we implemented a shake shake model with all state of the art methods presented before. Previous implementation was too big for the capacities of google collab so we had to tune it so that it wasn't too long to train. \n\nTraining parameters are the following\n- **Minibatch** : 32 images\n- **Epochs** : 125\n- **Learning rate** : 0.01 initially, 0.005 after 10 epochs, 0.025 after 20 epochs 0.001 after 40 epochs. \n\nWe stopped too early because of google collabs limitations. With our pre stop, we obtained \nAccuracy of the network on the 45000 train images: 79.52 %\nAccuracy of the network on the 5000 validation images: 89.42 %\nAccuracy of the network on the 5000 test images: 77.32 %\n\nWe recoded the model on our personnal machine, based of the following implementation: https://github.com/owruby/shake-shake_pytorch. We have a 1070 Nvidia and the code is parallelized on it so the computation is much faster.\n\nWe had the same structure of shake shake, with a netowrk depth of 26 and a network width of 64.\n\n\nWith the following hyperparameters\n- **Minibatch** : 64 images\n- **Epochs** : 1800\n- **Learning rate** : 0.001 \n\nWe obtained 96.03 accuracy on test. The code and the detailed results of the experiments are on the github link dedicated to the shake shake implementation.\n\n"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/LMaxence/cifar10-classification",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-22T03:04:42Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-20T14:46:15Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9852233752283313
      ],
      "excerpt": "As part of the class ROB313 at ENSTA, about Computer Vision, we followed an introduction course to Pytorch and deep convolutional neural networks. The project aims at implementing a deep neural network model for predicting the class of an object in a 32x32 pixels image, among 10 different classes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9772080158295775,
        0.9877258634406992,
        0.9254801026117685,
        0.871489754315226,
        0.8597552100624364
      ],
      "excerpt": "We worked on the two following aspect of the project :  \n- Reaching a good accuracy by using a personal, simple and quite na\u00efve model. This has brought us into considering different aspects of a CNN architecture, such as regularization and data augmentation. \n- We also aimed at implementing and explaining some recent architectures which achieved top score in benchmarks. \nWe will explain the different concepts in deep learning that were used, how they were implemented and optimized, then we will describe the results that we obtained. \nWe will focus on two architectures: one we built by ourselves, and one state of the art architecture: Shake Shake.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9651462232691672,
        0.9662892767313866,
        0.9873400740360119,
        0.9973514198620285,
        0.8110736881302818,
        0.9602414615261243
      ],
      "excerpt": "Our architecture is based on the stack of 3 convolution layers, followed by a dense classification layer. \nThe initial structure of our custom network is as follows: \nThe goal of the multiple convolution layers is to create features and reduce the dimensionality of the inputs (which is $batch_size\\times32\\times32\\times3$ initially). The process can be presented as follows : \nThe idea behind convolution layer in neural networks is to apply kernels to the inputs (which is a $32\\times 32$ pixels image in this case), rather than applying a linear function as it is in basic neural network layers. The main advantage of CNN compared to its predecessors is that it automatically detects the important features without any human supervision. For example, given many pictures of cats and dogs it learns distinctive features for each class by itself. \nHow a convolution layer is applied to the $\\it{32\\times 32 \\times 3}$ image ? \nThe input image is a actually a volume, meaning here that it has 3 features for each pixels, for the red, green and blue channels. Let's say that we use a convolution layer bringing the number of channels to 32, for instance one that is defined as follows in Pytorch :  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9582223170147467,
        0.9329288280920077
      ],
      "excerpt": "We apply 32 convolutions of size 3 to the image. Each convolution is applied on every channels of the input (which form $32\\times 32 \\times 1$ inputs), resulting in 3 $32\\times 32 \\times 1$ outputs for each convolution. For each pixels, a linear combination of the 3 outputs of that pixel form the final output of the kernel for that layer, this operation applied to every pixel resulting in the $32\\times 32 \\times 1$ feature map of this specific kernel. This operation applied to every kernel builds the final $32\\times 32 \\times 32$ volume output of this layer. \nA Max-Pooling layer defines a window (which size is an argument) that goes through the feature maps and selects the highest feature on each window. The result of this operation is a reduction of the dimensionality of the feature map. For instance the following layer will bring the volume from our first convolution layer to a feature map of size $16\\times 16 \\times 32$. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8615815596882076,
        0.9521179060907078,
        0.987644853229894,
        0.9888856531317121,
        0.8884631600562268
      ],
      "excerpt": "Other pooling layers can be experimented, such as Average-Pooling but we only made usage of Max-Pooling in this work. \nThe succession of convolution layers and max pooling layers leads to a final $2048 \\times 1$ tensor which is the final features that will be interpreted by the classification layers. Each one of these outputs may describe an specific aspect of the image, like a corner or a circular-shaped item, as it can be seen in the next picture : \nFirstly, we can observe that we duplicated the convolution layers. This is meant to increase the level of abstraction of the network. Finally, there are 300k trainable parameters in this model. \nOne of the key features of our algorithm, which is supposed to train with the experiment, is it's evaluation function and it's optimizer. \nHere we use a softmax cross-entropy loss function because it has proven to have good results on multiclass classification problems. As optimizer, we use Adam, which the state of the art for convolutional neural networks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9392037038668861
      ],
      "excerpt": "The classification layer used is nothing more than a Dense layer, we crush the final output from the conv layers (of size $4 \\times 4 \\times 128$) into one big vector of length 2048. The dense layer, combined with the softmax non-linearity produces a final output vector of size 10. This vector represents the probability that an image belongs to each one of the 10 classes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8848174511543827
      ],
      "excerpt": "Standard activation function is relu for middle neurons and softmax for the last one, because we are in a multiclass classification problem. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9804328405553578
      ],
      "excerpt": "We can observe here that the training loss is decreasing until the end of the training, but the validation loss start increasing again very quickly. The phenomenon obesrved here is the overfitting. Our algorithm performs on it's training data but is really bad at inferring the class of new entries. The algorithm finally inferred rules that might be significantly observed on the training set, but that are not verified in reality. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8294644496197585,
        0.8635626063467572,
        0.9597507576231825,
        0.9451178626014598
      ],
      "excerpt": "There are multiple regularization methods. We firstly added new regularization layers, then we used data augmentation in order to use diversified training data. \nBatch normalization was proposed by Sergey Ioffe and Christian Szegedy in the article Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift : https://arxiv.org/abs/1502.03167. The idea is to perform normalization inside the architecture, after the layers for example. This is a strong help against overfitting and allowed to push further the state of the art accuracy. \nTo increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. There are a lot of papers and article dealing with how batch normalization improves the stability of the neural network, but in a nutshell : \n- It makes each layer a bit more independant from one another, and thus the model is less sensitive to noisy images, and variations in the distribution of the inputs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9713944613912026,
        0.9904570712330245,
        0.9053895591015007
      ],
      "excerpt": "Dropout was proposed by Hinton in Dropout:  A Simple Way to Prevent Neural Networks fromOverfitting : http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer. The idea is to randomly set the activation function of a certain percentage of a layer to the null function during the training phase. By doing this, training is more difficult because the layer can't \"collaborate\" as well with the next one. It learns then to generalize more. \nThe second method that we used is the Dropout. It consists in not updating each parameters of a layer when the optimizer moves one step forward. With a probability p, a neuron will remain inactive for the training minibatch. We offer more details in the section regarding state-of-the-art techniques. \nAt each epoch, it will get the validation loss on the validation set. It will save only best model weight which was obtained, so prediction is not sensible to the possible overfitting.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9529351074119129,
        0.9585932811607026
      ],
      "excerpt": "This model shows slightly better results, however we would like to have the validation loss to decrease during more than 3 or 5 epochs, in order to reach a better final validation loss. \nData augmentation is the idea that some classes are invariant by some transformations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.947960059295756
      ],
      "excerpt": "With this, we can increase the size of our data set, and train the future model with much more images. It will then be more robust to the potential noise it will encounter on the test set.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8519484882531172
      ],
      "excerpt": "* random cropping of the image : torchvision.transforms.RandomCrop(32, padding=4), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.952086338462156,
        0.9300613175135171
      ],
      "excerpt": "* rotation of the image: transforms.RandomRotation(5) \nState of the art data-augmentation methods will be described in the following section. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9777998408418334
      ],
      "excerpt": "Data augmentation is a very powerfull tool widely used in deep learning on vision problem. Two state of the art techniques in data augmentation were proposed, and deeply improved state of the art accuracy in deep learning vision : Autoaugment and Cutout. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9647350961963951,
        0.9789248884498025
      ],
      "excerpt": "Here they considered an ensemble of data augmentation techniques policies. On it, for a given dataset, they consider each policy as composed by many subpolicies, where each subpolicy consists in two operations( translation, rotation, or shearing, and their associated probabilities of being applied). A search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset is proposed.  \nComputing this data augmentation technique would cost a very long time, but fortunately the following technique proposed was open-sourced, and is named CIFAR10Policy. We have taken an implementation to boost our data augmentation techniques on the following github:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8463743040776249,
        0.9937912055251672
      ],
      "excerpt": "Cutout is a simple yet very powerfull data augmentation technique, and was proposed in the article Improved Regularization of Convolutional Neural Networks with Cutout : https://arxiv.org/abs/1708.04552. \nThe idea is to randomly masking out square regions of input during training. It is a crucial point for convolutional nets because they spot patterns, and ocluding spatial patterns force neural networks to spot multiple patterns who could have first not be used. This can be used to improve the robustness and overall performance of convolutional neural networks. We stacked this method in our data augmentation methods to be improve the robustess of our model. An implementation can be found here: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9775826612894728,
        0.9093328621547723,
        0.9845019584212653,
        0.8312245341137906,
        0.9123151823706895
      ],
      "excerpt": "We decided to firstly launch a new training phase on the network trained after the introduction of regularization methods, but using our augmented dataset this time. \nThere are still 20 epochs and a learning rate of 0.001, these parameters haven't changed yet. \nThe validation error is higher than before the data augmentation, but this is because the augmented dataset is much more harder to train on than the original one. However we can see that the plateau from before is not anymore, and that the validation loss is still decreasing after 20 epochs. \nWe improved the accuracy. \nWe decide then to use more epochs for the training phase, and to do so, we have to configure a learning rate scheduler. This will aim at preventing overfitting as the number of trained epochs increases. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8561516910012531
      ],
      "excerpt": "The final training phase has been realized with all the former improvements (batch normalization, dropout, data augmentation, full dataset, use of a learning scheduler). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9297292163876272
      ],
      "excerpt": "We managed to continuously decrease the validation loss over the 125 epochs. The minimal validation loss is 0.45. A very interesting observation is the drop in the loss after iteration 75 and 100, which are the epochs where we manually fixed a smaller learning rate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9564852277141082,
        0.8785076056695909,
        0.9634742699370824,
        0.9164420520570187,
        0.9756825824159252
      ],
      "excerpt": "Shake Shake is one of the best architectures presented on CIFAR10.  \nShake Shake is based on Resnet blocks, which, stacked, make Resnet branches.  \nThe idea is to replace, in the multi-branch network, the standard summation of parallel branches with a stochastic affine combination. \nIt is a good technique to work against overfitting and was first implemented in \"Shake-Shake regularization\".  \nIn parallel we implemented a shake shake model with all state of the art methods presented before. Previous implementation was too big for the capacities of google collab so we had to tune it so that it wasn't too long to train.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9660481717825252
      ],
      "excerpt": "We stopped too early because of google collabs limitations. With our pre stop, we obtained  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8908226574510666
      ],
      "excerpt": "Accuracy of the network on the 5000 validation images: 89.42 % \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9914821644780675,
        0.9618020938211527,
        0.8599753703401478
      ],
      "excerpt": "We recoded the model on our personnal machine, based of the following implementation: https://github.com/owruby/shake-shake_pytorch. We have a 1070 Nvidia and the code is parallelized on it so the computation is much faster. \nWe had the same structure of shake shake, with a netowrk depth of 26 and a network width of 64. \nWith the following hyperparameters \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Creation of a simple 8-layered CNN with 78% accuracy on Cifar10",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/LMaxence/Cifar10_Classification/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 22:01:09 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/LMaxence/cifar10-classification/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "LMaxence/cifar10-classification",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/LMaxence/Cifar10_Classification/master/TP_pytorch.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8000246133162248
      ],
      "excerpt": "github to the shake shake architecture: https://github.com/FlavienGelineau/shake-shake_pytorch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9257175720597752
      ],
      "excerpt": "With the following hyperparameters \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8528155479098645
      ],
      "excerpt": "The set of images, on which we will train the models and infer classes is the dataset called CIFAR-10. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 5000 test images.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444374084500096
      ],
      "excerpt": "The training phase is realized with a training set of size 40000 images and 10000 test images on each epoch. There are 20 epochs. We also use 32 images per mini-batch. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/LMaxence/cifar10-classification/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "TP3 Deep Learning with PyTorch: CIFAR10 object classification",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "cifar10-classification",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "LMaxence",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/LMaxence/cifar10-classification/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Thu, 23 Dec 2021 22:01:09 GMT"
    },
    "technique": "GitHub API"
  }
}