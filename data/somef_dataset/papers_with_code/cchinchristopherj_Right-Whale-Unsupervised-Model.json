{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1702.06286",
      "https://arxiv.org/abs/1306.0152*, 2013\n\nLin M., Chen Q., Yan S. [Network In Network.](https://arxiv.org/pdf/1312.4400.pdf) *arXiv preprint https://arxiv.org/abs/1312.4400*, 2014\n\nKnyazev B., Barth E., Martinetz T. [Recursive Autoconvolution for Unsupervised Learning of Convolutional Neural Networks.](https://arxiv.org/pdf/1606.00611.pdf) *arXiv preprint https://arxiv.org/abs/1606.00611*, 2017\n\nCakir E., Parascandalo G., Heittola T., Huttunen H.,Virtanen T. [Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection.](https://arxiv.org/abs/1702.06286) *arXiv preprint https://arxiv.org/abs/1702.06286*, 2017",
      "https://arxiv.org/abs/1312.4400*, 2014\n\nKnyazev B., Barth E., Martinetz T. [Recursive Autoconvolution for Unsupervised Learning of Convolutional Neural Networks.](https://arxiv.org/pdf/1606.00611.pdf) *arXiv preprint https://arxiv.org/abs/1606.00611*, 2017\n\nCakir E., Parascandalo G., Heittola T., Huttunen H.,Virtanen T. [Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection.](https://arxiv.org/abs/1702.06286) *arXiv preprint https://arxiv.org/abs/1702.06286*, 2017",
      "https://arxiv.org/abs/1606.00611*, 2017\n\nCakir E., Parascandalo G., Heittola T., Huttunen H.,Virtanen T. [Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection.](https://arxiv.org/abs/1702.06286) *arXiv preprint https://arxiv.org/abs/1702.06286*, 2017",
      "https://arxiv.org/abs/1702.06286*, 2017"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Coates A., Ng A.Y. (2012) [Learning Feature Representations with K-Means.](https://www-cs.stanford.edu/~acoates/papers/coatesng_nntot2012.pdf) In: Montavon G., Orr G.B., M\u00fcller KR. (eds) Neural Networks: Tricks of the Trade. Lecture Notes in Computer Science, vol 7700. Springer, Berlin, Heidelberg\n\nCoates A., Ng A.Y. [Selecting receptive fields in deep networks.](http://robotics.stanford.edu/~ang/papers/nips11-SelectingReceptiveFields.pdf) In: Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., Weinberger, K. (eds.) Advances in Neural Information Processing Systems 24, pp. 2528\u20132536. Curran Associates, Inc. (2011)\n\nSalamon J., Bello J.P., Farnsworth A., Robbins M., Keen S., Klinck H., et al. (2016) [Towards the Automatic Classification of Avian Flight Calls for Bioacoustic Monitoring.](http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0166866&type=printable) PLoS ONE 11(11): e0166866. doi:10.1371/journal.pone.0166866\n\nCulurciello E., Jin J., Dundar, A., Bates J. [An Analysis of the Connections Between Layers of Deep Neural Networks.](https://arxiv.org/pdf/1306.0152.pdf) *arXiv preprint arXiv:1306.0152*, 2013\n\nLin M., Chen Q., Yan S. [Network In Network.](https://arxiv.org/pdf/1312.4400.pdf) *arXiv preprint arXiv:1312.4400*, 2014\n\nKnyazev B., Barth E., Martinetz T. [Recursive Autoconvolution for Unsupervised Learning of Convolutional Neural Networks.](https://arxiv.org/pdf/1606.00611.pdf) *arXiv preprint arXiv:1606.00611*, 2017\n\nCakir E., Parascandalo G., Heittola T., Huttunen H.,Virtanen T. [Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection.](https://arxiv.org/abs/1702.06286) *arXiv preprint arXiv:1702.06286*, 2017\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cchinchristopherj/Right-Whale-Unsupervised-Model",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-16T01:56:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-16T11:28:13Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9599368523371338
      ],
      "excerpt": "Convolutional Neural Network to Recognize Right Whale Upcalls (Unsupervised Learning of Filters) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9911593618054092,
        0.9923905255914961,
        0.9172956526567804
      ],
      "excerpt": "Due to expert annotation being an expensive process, unsupervised learning holds great potential for the development of more robust classification models - its speed (in the case of K-Means-like algorithms) and ability to learn abstract, generalized features from readily available unlabeled data make it a powerful tool for future monitoring systems.  \nIn order to potentially improve the performance of the baseline (supervised) CNN model, the weights of the convolutional filters will be learned unsupervised instead of through standard backpropagation and gradient descent. Concretely, K-Means Clustering, an approach that clusters data into groups, the members of which bear a strong similarity with one another based on a specified distance metric, will be used to identify optimum filters.  \nThe rationale for K-Means Clustering being a viable alternative for feature learning can be traced back to the basic operation of a convolutional filter. The filters below, for example, are useful for edge detection: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9791145077358477,
        0.9462241078458061,
        0.855867860079023,
        0.9458305638314402,
        0.9962320203984312,
        0.986615649750657,
        0.842387462553411
      ],
      "excerpt": "Recall that a convolution operation involves computing \"matrix dot products\" (elementwise multiplications and summations) between patches of an input feature map (or feature map volume) and a learned convolutional filter. If a 3x3 learned convolutional filter contains large magnitude values in the top and bottom row and 0s in the middle row, it will be able to \"detect\" horizontal edges (since large magnitude values would result from the elementwise multiplication between the top and/or bottom row of the filter and horizontal edges in the input image patches). Essentially, therefore, convolutional filters in fact perform cross-correlation, measuring the similarity between two matrices A and B as a function of the displacement of A relative to B. (It can be mathematically proven that the only difference between convolution and cross-correlation is the \u201cflipping\u201d of B relative to A in the relevant equations).  \nCentroids learned by K-Means Clustering can therefore be interpreted as characteristic sets of features of samples in the dataset.  \nK-Means Clustering Algorithm Identifies K Centroids Corresponding to Clusters in the Dataset. Image Source: K Means Clustering: Identifying F.R.I.E.N.D.S in the World of Strangers \nReinterpreted as convolutional filters, these centroids can be cross-correlated (or convolved) with input images to determine whether or not they contain these characteristic features, with the output of the convolution operation being \"match scores\" indicating the degree of similarity between the patches in the input images and the representative centroids. A neural network can use these match scores to discriminate between the classes of interest. \nThere is, however, one large shortcoming to K-Means Clustering: as the dimensionality of the inputs increases, the algorithm requires larger and larger amounts of data to identify representative centroids. One author for example found that using K-Means on 5,000,000 patches of size (64,64), i.e. an input dimensionality of 64x64=4096, yielded sub-optimal results: many clusters were empty or comprised of only a few samples, meaning that algorithm failed to capture centroids representative of features in the dataset. The author found empirically that, given the same number of patches, performance of the algorithm improved when the patches\u2019 input dimensionalities were reduced by an order of magnitude to the hundreds (such as sizes of (6x6) or (8x8)) instead of the thousands.  \nLearning filters for the first convolutional layer of a CNN via K-Means should not be problematic since patch sizes are conventionally chosen to be relatively small (in the range of (3,3) to (11,11)) and the number of channels conventionally ranges from 1 (greyscale) to 3 (RGB color). However, due to the conventional increase in number of channels with depth in a CNN, learning filters for deeper convolutional layers becomes problematic due to the increasing dimensionality of inputs given to the K-Means algorithm.  \nIn this application, two different approaches for dimensionality reduction will be taken to ensure effective filter learning for the second convolutional layer:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9739548220762262,
        0.9391695727079419,
        0.8565203112002534,
        0.9868401695123925,
        0.9257650850462372,
        0.9775568256647048,
        0.9918561127630996,
        0.968711578851612,
        0.9377089381448586
      ],
      "excerpt": "Several authors have handled this shortcoming of K-Means by randomly creating smaller groups of feature maps from the entire feature map volume and passing these groups of reduced dimensionality to the K-Means algorithm to learn representative centroids.  \nFeature Map Grouping for Filter Learning via K-Means. Image Source: An Analysis of the Connections Between Layers in Deep Neural Networks \nThese groups can be created randomly or through a pairwise similarity metric such as squared-energy correlation. (Squared-energy correlation is used rather than correlation because the inputs are, as per convention, already linearly-uncorrelated via PCA). In this approach, a similarity matrix is created where each (j,k) element represents the squared energy correlation between the j-th and k-th individual feature map in the entire feature map volume.  \nGroups of group size G can be created by selecting N random rows (N random feature maps) in the similarity matrix. For each of these rows, one can select the G elements with the highest values (corresponding to the feature maps with the strongest correlation) and thereby form N groups of size G.  \nThese groups of reduced dimensionality can then be given to the K-Means algorithm to learn representative centroids.  \nExample of a Group Chosen using the Squared-Energy Correlation Metric, demonstrating the elements' similarity in orientation but difference in phase and frequency. Image Source: Selecting Receptive Fields in Deep Networks \nThe authors of the approach noted that applying K-Means to these correlated groups, rather than random groups, resulted in better classifier performance: since feature maps that respond similarly are grouped together, the algorithm is now able to even more finely model the patterns and structures they are responding together to in the images. \nUnlike conventional convolutional filters that have receptive fields covering entire patches in each channel of an input, (1x1) convolutional filters have a receptive field of one pixel in each channel, thereby precluding their ability to learn features in local areas. These filters therefore operate in a fundamentally different way, a way that in fact mirrors the operation of fully-connected layers. \nUsing 1x1 filters can be thought of as sliding a fully-connected layer with n nodes from left to right and top to bottom across the image, as each operation in each position yields n values (just as to be expected for the output of a fully-connected layer with n nodes).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.980122573734293
      ],
      "excerpt": "Furthermore, 1x1 convolutions are able to reduce the number of channels in an input, in essence summarizing or pooling information across the channels into one feature map. (1x1) convolutional filters are therefore an extremely effective and parameter-efficient means of reducing input dimensionality, and are used in this model to help K-Means learn effective filters for the second convolutional layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9341397715903069,
        0.9087179732742694
      ],
      "excerpt": "The final tuned model architecture for Model 2 is as depicted below: \nModel 1 was trained for 17 epochs and a batch size of 100 on a training set of 84000 audio files (42000 vertically-enhanced spectrograms and 42000 horizontally-enhanced spectrograms). Training took approximately 4 hours on a Tesla K80 GPU (via FloydHub Cloud Service). The test set consisted of 10000 audio files (5000 vertically-enhanced spectrograms and 5000 horizontally-enhanced spectrograms). The loss and accuracy of the training set, and ROC-AUC score of the test set, are evaluated by Keras for every epoch during training and depicted below. The final ROC-AUC score for the training set after 17 epochs was found to be 94.14%, while the ROC-AUC score for the test set was found to be 93.13%. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8929721238702126
      ],
      "excerpt": "Note: Predictions on the test set are made using the union of the predictions on the vertically-enhanced spectrograms and horizontally-enhanced spectrograms (BP=Both Predictions). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979415705999271
      ],
      "excerpt": "Note: The three curves represent predictions only on the vertically-enhanced spectrograms in the test set (VP=Vertically-Enhanced Predictions, predictions only on the horizontally-enhanced spectrograms in the test set (HP=Horizontally-Enhanced Predictions), and the union of the predictions on both types of images (BP=Both Predictions). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.894809163264843
      ],
      "excerpt": "Model 2 was trained for 16 epochs and a batch size of 100 on a training set of 84000 audio files (42000 vertically-enhanced spectrograms and 42000 horizontally-enhanced spectrograms). Training took approximately 50 minutes on a Tesla K80 GPU (via FloydHub Cloud Service). The test set consisted of 10000 audio files (5000 vertically-enhanced spectrograms and 5000 horizontally-enhanced spectrograms). The loss and accuracy of the training set, and ROC-AUC score of the test set, are evaluated by Keras for every epoch during training and depicted below. The final ROC-AUC score for the training set after 16 epochs was found to be 96.07%, while the ROC-AUC score for the test set was found to be 95.97%. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8929721238702126
      ],
      "excerpt": "Note: Predictions on the test set are made using the union of the predictions on the vertically-enhanced spectrograms and horizontally-enhanced spectrograms (BP=Both Predictions). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979415705999271
      ],
      "excerpt": "Note: The three curves represent predictions only on the vertically-enhanced spectrograms in the test set (VP=Vertically-Enhanced Predictions, predictions only on the horizontally-enhanced spectrograms in the test set (HP=Horizontally-Enhanced Predictions), and the union of the predictions on both types of images (BP=Both Predictions). \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cchinchristopherj/Right-Whale-Unsupervised-Model/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 10:05:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cchinchristopherj/Right-Whale-Unsupervised-Model/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "cchinchristopherj/Right-Whale-Unsupervised-Model",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**\"Standard\" Modules Used (Included in Anaconda Distribution):** numpy, matplotlib, pylab, glob, aifc, os\n\nIf necessary, these modules can also be installed via PyPI. For example, for the \"numpy\" module: \n\n        pip install numpy\n\n**Additional Modules used:** skimage, sklearn, keras\n\nskimage and sklearn can be installed via PyPI. For example, for the \"scikit-image\" module:\n\n        pip install scikit-image\n\nFor Keras, follow the instructions given in the [documentation](https://keras.io/#installation). Specifically, install the TensorFlow backend and, of the optional dependencies, install HDF5 and h5py if you would like to load and save your Keras models. \n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8003261166765615,
        0.8274035157233471
      ],
      "excerpt": "Test ROC_AUC Score = 0.9313 \nTraining Set ROC Curve vs Test Set ROC Curve \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8219442269200298
      ],
      "excerpt": "Test Set ROC Curves \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8432367162911694
      ],
      "excerpt": "Training Set ROC Curve vs Test Set ROC Curves \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003261166765615,
        0.8274035157233471
      ],
      "excerpt": "Test ROC_AUC Score = 0.9507 \nTraining Set ROC Curve vs Test Set ROC Curve \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8219442269200298
      ],
      "excerpt": "Test Set ROC Curves \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8432367162911694
      ],
      "excerpt": "Training Set ROC Curve vs Test Set ROC Curves \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cchinchristopherj/Right-Whale-Unsupervised-Model/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Right Whale Detection Challenge Part II",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Right-Whale-Unsupervised-Model",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "cchinchristopherj",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cchinchristopherj/Right-Whale-Unsupervised-Model/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Mon, 27 Dec 2021 10:05:23 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "My trained CNN model architecture and weights for Model 2 are saved in the \"model_v2.h5\" file. This file can be loaded using the command:\n\n    loaded_model = load_model('model_v2.h5')  \n    \nNote: \"load_model\" is a function from keras.models. \nWith this model loaded, you can follow the procedure as described in training_v2.py to predict the label of a new audio file that may or may not contain a right whale upcall. \n\nDue to the complicated architecture of Model 1, it is not possible to directly load the model using Keras' \"load_model\" command. Instead, use the \"model_v1_load.py\" function to first re-create the model architecture, then load in the weights to the appropriate layers: \n\n    python model_v1_load.py \n    \nWith this model loaded, you can once again follow the procedure as described in training_v1.py to predict the label of a new audio file.\n\nIf you would like to replicate the process I used to train the CNN models, perform the following:\nFirst, download the training set \"train_2.zip\" from [here](https://www.kaggle.com/c/the-icml-2013-whale-challenge-right-whale-redux/data) to the desired directory on your computer.\nThen run either:\n\n    python training_v1.py \n    \nor:\n\n    python training_v2.py \n    \nThis constructs the CNN model architectures of Model 1 or Model 2, respectively, trains the filters unsupervised via K-Means, and trains the weights on the dataset. This trained model can be saved to your computer using the command:\n\n    model.save('model.h5')  \n\n",
      "technique": "Header extraction"
    }
  ]
}