{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1607.04606",
      "https://arxiv.org/abs/1607.01759",
      "https://arxiv.org/abs/1612.03651",
      "https://arxiv.org/abs/1607.04606",
      "https://arxiv.org/abs/1607.01759",
      "https://arxiv.org/abs/1612.03651"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{joulin2016fasttext,\n  title={FastText.zip: Compressing text classification models},\n  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\\'e}gou, H{\\'e}rve and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1612.03651},\n  year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{joulin2016bag,\n  title={Bag of Tricks for Efficient Text Classification},\n  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1607.01759},\n  year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{bojanowski2016enriching,\n  title={Enriching Word Vectors with Subword Information},\n  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1607.04606},\n  year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{menegaux2018continuous,\n  title={Continuous Embedding of DNA reads and application to metagenomics},\n  author={Menegaux, Romain and Vert, Jean-Philippe},\n  journal={bioRxiv preprint 335943},\n  year={2018}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rmenegaux/fastDNA",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-14T16:06:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-09T17:22:56Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[fastDNA](#continuous-embedding-of-dna-reads-and-application-to-metagenomics) is a library for classification of short DNA sequences.\nIt is adapted from the [fastText](https://fasttext.cc/) library.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9552464039522296
      ],
      "excerpt": "Continuous Embedding of DNA reads and application to metagenomics \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8965799723048201
      ],
      "excerpt": "Bag of Tricks for Efficient Text Classification \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8273099514127742
      ],
      "excerpt": "This will produce object files for all the classes as well as the main binary fastdna. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9316704882694586,
        0.9614742823684236,
        0.8207525161695861
      ],
      "excerpt": "where test.fasta is a FASTA file containing the DNA fragments to be classified, and test_labels.txt contains the labels for each of the fragments. \nThe argument n is optional, and is equal to 1 by default. \nIn order to obtain the n most likely labels for a set of reads, use: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8317909397572478
      ],
      "excerpt": "or use predict-prob to also get the probability for each label \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186981690125441
      ],
      "excerpt": "The argument n is optional, and equal to 1 by default. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8475365092121306
      ],
      "excerpt": "This assumes that the text.fasta file contains the DNA sequences that you want to get vectors for. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.838895537599196
      ],
      "excerpt": "This will create a .ftz file with a smaller memory footprint. All the standard functionality, like test or predict work the same way on the quantized models: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "  -quantize             quantize model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9065840522335811
      ],
      "excerpt": "  -predict_quant        make and evaluate predictions with quantized model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "  -freeze               freeze the embeddings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9451364873114011,
        0.8851106435326525,
        0.9552464039522296
      ],
      "excerpt": "The data used in the paper is available here: http://projects.cbio.mines-paristech.fr/largescalemetagenomics/. \nThe small and large datasets used in the paper can be found here \n[1] R. Menegaux, J. Vert, Continuous Embedding of DNA reads, and application to metagenomics \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8965799723048201
      ],
      "excerpt": "[3] A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, Bag of Tricks for Efficient Text Classification \n",
      "technique": "Supervised classification"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Invoke a command without arguments to list available arguments and their default values:\n```\n$ ./fastdna supervised\nEmpty input or output path.\n\nThe following arguments are mandatory:\n  -input              training file path\n  -output             output file path\n\nThe following arguments are optional:\n  -verbose            verbosity level [2]\n\nThe following arguments for the dictionary are optional:\n  -minn               min length of char ngram [0]\n  -maxn               max length of char ngram [0]\n  -label              labels prefix [__label__]\n\nThe following arguments for training are optional:\n  -lr                 learning rate [0.1]\n  -lrUpdateRate       change the rate of updates for the learning rate [100]\n  -dim                size of word vectors [100]\n  -noise              mutation rate (/100,000)[0]\n  -length             length of fragments for training [200]\n  -epoch              number of epochs [5]\n  -loss               loss function {ns, hs, softmax} [softmax]\n  -thread             number of threads [12]\n  -pretrainedVectors  pretrained word vectors for supervised learning []\n  -loadModel          pretrained model for supervised learning []\n  -saveOutput         whether output params should be saved [false]\n  -freezeEmbeddings   model does not update the embedding vectors [false]\n\nThe following arguments for quantization are optional:\n  -cutoff             number of words and ngrams to retain [0]\n  -retrain            whether embeddings are finetuned if a cutoff is applied [false]\n  -qnorm              whether the norm is quantized separately [false]\n  -qout               whether the classifier is quantized [false]\n  -dsub               size of each sub-vector [2]\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rmenegaux/fastDNA/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Tue, 28 Dec 2021 06:38:48 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rmenegaux/fastDNA/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rmenegaux/fastDNA",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rmenegaux/fastDNA/master/test/test.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9893272198983933,
        0.9906248903846466,
        0.8474895321345809
      ],
      "excerpt": "$ git clone https://github.com/rmenegaux/fastDNA.git \n$ cd fastDNA \n$ make \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9682048251497578,
        0.8990812940131584
      ],
      "excerpt": "$ cd test \n$ sh test.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9211623389154807,
        0.8147438040199454
      ],
      "excerpt": "Press Enter, then type the sequence and finish with Ctrl+D (Linux, Mac) or Ctrl+Z (Windows) \nYou can also quantize a supervised model to reduce its memory usage with the following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.864003161007744
      ],
      "excerpt": "                        output directory \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8514016617147449
      ],
      "excerpt": "FastText.zip: Compressing text classification models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8036072029549625,
        0.8709872213433483,
        0.8271340404619316
      ],
      "excerpt": "$ ./fastdna supervised -input train.fasta -labels labels.txt -output model \nwhere train.fasta is a FASTA file containing the full reference genomes and labels.txt is a text file containing the genome labels (one label per line). \nThis will output two files: model.bin and model.vec. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8951381859294457
      ],
      "excerpt": "$ ./fastdna test model.bin test.fasta test_labels.txt n \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9089597657153919
      ],
      "excerpt": "$ ./fastdna predict model.bin test.fasta n \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9089597657153919,
        0.855185380383668
      ],
      "excerpt": "$ ./fastdna predict-prob model.bin test.fasta n \nDoing so will print to the standard output the n most likely labels for each line. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8407764851370688
      ],
      "excerpt": "$ ./fastdna print-word-vectors model.bin &lt; text.fasta \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8504965059621983,
        0.8261516191526119
      ],
      "excerpt": "The program will output one vector representation per sequence in the file. \nTo write the vectors to a file, redirect the output as so: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490887538711107
      ],
      "excerpt": "$ ./fastdna print-word-vectors model.bin \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.857129346874625
      ],
      "excerpt": "$ ./fastdna quantize -output model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8910782061406535
      ],
      "excerpt": "$ ./fastdna test model.ftz test.fasta test_labels.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8664713104808358
      ],
      "excerpt": "python fdna.py -train -train_fasta /path/to/train_large_fasta -train_labels /path/to/train_large_labels \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8121176106242236
      ],
      "excerpt": "Full usage: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8971823465573899,
        0.9628056311279102
      ],
      "excerpt": "python fdna.py --help \nusage: fdna.py [-h] [-train] [-quantize] [-predict] [-eval] [-predict_quant] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8676060246290176
      ],
      "excerpt": "train, predict and/or quantize fdna model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807914812274519
      ],
      "excerpt": "  -train                train model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8864373852101123
      ],
      "excerpt": "                        training dataset, fasta file containing full genomes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8026141920424932
      ],
      "excerpt": "                        training labels, text file containing as many labels \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8655242169143564
      ],
      "excerpt": "                        testing dataset, fasta file containing reads \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8735399179637832
      ],
      "excerpt": "                        testing dataset, text file containing as many labels \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8585707754631647
      ],
      "excerpt": "                        output directory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8881537835346903
      ],
      "excerpt": "  -L L                  training read length \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8514016617147449
      ],
      "excerpt": "[4] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J\u00e9gou, T. Mikolov, FastText.zip: Compressing text classification models \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rmenegaux/fastDNA/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Python",
      "Makefile",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/rmenegaux/fastDNA/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD License\\n\\nFor fastText software\\n\\nCopyright (c) 2016-present, Facebook, Inc. All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification,\\nare permitted provided that the following conditions are met:\\n\\n * Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n * Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\n * Neither the name Facebook nor the names of its contributors may be used to\\n   endorse or promote products derived from this software without specific\\n   prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## Table of contents\n\n* [Introduction](#introduction)\n* [Requirements](#requirements)\n* [Building fastDNA](#building-fastdna)\n* [Command line interface](#command-line-interface)\n   * [Full documentation](#full-documentation)\n* [Python](#python)\n* [Data](#data)\n* [References](#references)\n   * [Continuous Embedding of DNA reads and application to metagenomics](#continuous-embedding-of-dna-reads-and-application-to-metagenomics)\n   * [Enriching Word Vectors with Subword Information](#enriching-word-vectors-with-subword-information)\n   * [Bag of Tricks for Efficient Text Classification](#bag-of-tricks-for-efficient-text-classification)\n   * [FastText.zip: Compressing text classification models](#fasttextzip-compressing-text-classification-models)\n* [License](#license)\n\n## Introduction\n\n[fastDNA](#continuous-embedding-of-dna-reads-and-application-to-metagenomics) is a library for classification of short DNA sequences.\nIt is adapted from the [fastText](https://fasttext.cc/) library.\n\n\n## Requirements\n\nGenerally, **fastText** builds on modern Mac OS and Linux distributions.\nSince it uses some C++11 features, it requires a compiler with good C++11 support.\nThese include :\n\n* (g++-4.7.2 or newer) or (clang-3.3 or newer)\n\nCompilation is carried out using a Makefile, so you will need to have a working **make**.\n\n\n## Building fastDNA\n\n\n```\n$ git clone https://github.com/rmenegaux/fastDNA.git\n$ cd fastDNA\n$ make\n```\n\nThis will produce object files for all the classes as well as the main binary `fastdna`.\n\nFor a trial run:\n```\n$ cd test\n$ sh test.sh\n```\nThis should train and evaluate a small model on the toy dataset provided. \n\n### DNA short read classification\n\nIn order to train a dna classifier using the method described in [1](#continuous-embedding-of-dna-reads-and-application-to-metagenomics), use:\n\n```\n$ ./fastdna supervised -input train.fasta -labels labels.txt -output model\n```\n\nwhere `train.fasta` is a FASTA file containing the full reference genomes and `labels.txt` is a text file containing the genome labels (one label per line).\nThis will output two files: `model.bin` and `model.vec`.\n\nOnce the model was trained, you can evaluate it by computing the precision and recall at k (P@k and R@k) on a test set using:\n\n```\n$ ./fastdna test model.bin test.fasta test_labels.txt n\n```\n\nwhere `test.fasta` is a FASTA file containing the DNA fragments to be classified, and `test_labels.txt` contains the labels for each of the fragments.\n\nThe argument `n` is optional, and is equal to `1` by default.\n\nIn order to obtain the n most likely labels for a set of reads, use:\n\n```\n$ ./fastdna predict model.bin test.fasta n\n```\n\nor use `predict-prob` to also get the probability for each label\n\n```\n$ ./fastdna predict-prob model.bin test.fasta n\n```\n\nDoing so will print to the standard output the n most likely labels for each line.\nThe argument `n` is optional, and equal to `1` by default.\n\nIf you want to compute vector representations of DNA sequences, please use:\n\n```\n$ ./fastdna print-word-vectors model.bin < text.fasta\n```\n\nThis assumes that the `text.fasta` file contains the DNA sequences that you want to get vectors for.\nThe program will output one vector representation per sequence in the file.\n\nTo write the vectors to a file, redirect the output as so:\n```\n$ ./fastdna print-word-vectors model.bin < text.fasta > vectors.txt\n```\nTo get vectors from standard input, just type\n```\n$ ./fastdna print-word-vectors model.bin\n```\nPress Enter, then type the sequence and finish with Ctrl+D (Linux, Mac) or Ctrl+Z (Windows)\n\nYou can also quantize a supervised model to reduce its memory usage with the following command:\n\n```\n$ ./fastdna quantize -output model\n```\nThis will create a `.ftz` file with a smaller memory footprint. All the standard functionality, like `test` or `predict` work the same way on the quantized models:\n```\n$ ./fastdna test model.ftz test.fasta test_labels.txt\n```\nThe quantization procedure follows the steps described in [3](#fasttextzip-compressing-text-classification-models). \n\n\n### Full documentation\n\nInvoke a command without arguments to list available arguments and their default values:\n```\n$ ./fastdna supervised\nEmpty input or output path.\n\nThe following arguments are mandatory:\n  -input              training file path\n  -output             output file path\n\nThe following arguments are optional:\n  -verbose            verbosity level [2]\n\nThe following arguments for the dictionary are optional:\n  -minn               min length of char ngram [0]\n  -maxn               max length of char ngram [0]\n  -label              labels prefix [__label__]\n\nThe following arguments for training are optional:\n  -lr                 learning rate [0.1]\n  -lrUpdateRate       change the rate of updates for the learning rate [100]\n  -dim                size of word vectors [100]\n  -noise              mutation rate (/100,000)[0]\n  -length             length of fragments for training [200]\n  -epoch              number of epochs [5]\n  -loss               loss function {ns, hs, softmax} [softmax]\n  -thread             number of threads [12]\n  -pretrainedVectors  pretrained word vectors for supervised learning []\n  -loadModel          pretrained model for supervised learning []\n  -saveOutput         whether output params should be saved [false]\n  -freezeEmbeddings   model does not update the embedding vectors [false]\n\nThe following arguments for quantization are optional:\n  -cutoff             number of words and ngrams to retain [0]\n  -retrain            whether embeddings are finetuned if a cutoff is applied [false]\n  -qnorm              whether the norm is quantized separately [false]\n  -qout               whether the classifier is quantized [false]\n  -dsub               size of each sub-vector [2]\n```\n\n## Python\n\nMost use cases are covered in the python script `fdna.py`.\n\nTo reproduce the results from the paper, download the [data](#data) then run:\n```\npython fdna.py -train -train_fasta /path/to/train_large_fasta -train_labels /path/to/train_large_labels \\\n    -eval -test_fasta /path/to/test_large_fasta -test_labels /path/to/test_large_labels \\\n    -k 13 -d 100 -noise 4 -e 200\n```\nNB: Best parameters for classification tasks are `k=14, d=50, noise=4`\n\nFull usage:\n```\npython fdna.py --help\nusage: fdna.py [-h] [-train] [-quantize] [-predict] [-eval] [-predict_quant]\n               [-train_fasta TRAIN_FASTA] [-train_labels TRAIN_LABELS]\n               [-test_fasta TEST_FASTA] [-test_labels TEST_LABELS]\n               [-output_dir OUTPUT_DIR] [-model_name MODEL_NAME]\n               [-threads THREADS] [-d D] [-k K] [-e E] [-lr LR] [-noise NOISE]\n               [-L L] [-freeze] [-pretrained_vectors PRETRAINED_VECTORS]\n               [-verbose VERBOSE]\n\ntrain, predict and/or quantize fdna model\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -train                train model\n  -quantize             quantize model\n  -predict              make predictions\n  -eval                 make and evaluate predictions\n  -predict_quant        make and evaluate predictions with quantized model\n  -train_fasta TRAIN_FASTA\n                        training dataset, fasta file containing full genomes\n  -train_labels TRAIN_LABELS\n                        training labels, text file containing as many labels\n                        as there are training genomes\n  -test_fasta TEST_FASTA\n                        testing dataset, fasta file containing reads\n  -test_labels TEST_LABELS\n                        testing dataset, text file containing as many labels\n                        as there are reads\n  -output_dir OUTPUT_DIR\n                        output directory\n  -model_name MODEL_NAME\n                        optional user-defined model name\n  -threads THREADS      number of threads\n  -d D                  embedding dimension\n  -k K                  k-mer length\n  -e E                  number of training epochs\n  -lr LR                learning rate\n  -noise NOISE          level of training noise, percent of random mutations\n  -L L                  training read length\n  -freeze               freeze the embeddings\n  -pretrained_vectors PRETRAINED_VECTORS\n                        pretrained vectors .vec files\n  -verbose VERBOSE      output verbosity, 0 1 or 2\n```\nThe python scripts require `numpy` and `scikit-learn` for evaluating predictions.\n\n## Data\n\nThe data used in the paper is available here: [http://projects.cbio.mines-paristech.fr/largescalemetagenomics/](http://projects.cbio.mines-paristech.fr/largescalemetagenomics/).",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "fastDNA",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rmenegaux",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rmenegaux/fastDNA/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Generally, **fastText** builds on modern Mac OS and Linux distributions.\nSince it uses some C++11 features, it requires a compiler with good C++11 support.\nThese include :\n\n* (g++-4.7.2 or newer) or (clang-3.3 or newer)\n\nCompilation is carried out using a Makefile, so you will need to have a working **make**.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 18,
      "date": "Tue, 28 Dec 2021 06:38:48 GMT"
    },
    "technique": "GitHub API"
  }
}