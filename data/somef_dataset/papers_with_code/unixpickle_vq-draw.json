{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1502.04623",
      "https://arxiv.org/abs/1711.00937"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/unixpickle/vq-draw",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-12T14:21:16Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-19T05:44:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9712724936882717,
        0.9935965045361825,
        0.9453481308853335
      ],
      "excerpt": "VQ-DRAW is a discrete auto-encoder which encodes inputs in a sequential way similar to the DRAW architecture. Unlike VQ-VAE, VQ-DRAW can generate good samples without learning an autoregressive prior on top of the dicrete latents. \nThis repository is still being used for active experimentation and research. See the official-release branch for the version of the source code that came with the initial paper and blog post. \nIn addition to the code for training, I've provided notebooks to play around with some small pre-trained models. These are intended to be runnable on a desktop PC, even without any GPU. Here is a list: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8830725796190015
      ],
      "excerpt": "The VQ-DRAW model reconstructs an image in stages, adding more details at every stage. Each stage adds a few extra bits of information to the latent code, allowing VQ-DRAW to make very good use of the latent information. Here is an example of 10 different MNIST digits being decoded stage by stage. In this example, each stage adds 6 bits: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A discrete sequential VAE",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/unixpickle/vq-draw/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Fri, 24 Dec 2021 06:37:37 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/unixpickle/vq-draw/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "unixpickle/vq-draw",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/unixpickle/vq-draw/master/svhn_demo.ipynb",
      "https://raw.githubusercontent.com/unixpickle/vq-draw/master/mnist_classify.ipynb",
      "https://raw.githubusercontent.com/unixpickle/vq-draw/master/mnist_demo.ipynb",
      "https://raw.githubusercontent.com/unixpickle/vq-draw/master/training_logs/plots.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8405794486632646
      ],
      "excerpt": "Here are samples from the trained VQ-DRAW models: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9081218911792351,
        0.9081218911792351
      ],
      "excerpt": "        <td><img src=\"images/mnist_samples_60bit.png\" width=\"200\"></td> \n        <td><img src=\"images/svhn_samples.png\" width=\"200\"></td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9081218911792351,
        0.9081218911792351
      ],
      "excerpt": "        <td><img src=\"images/celeb_samples.png\" width=\"200\"></td> \n        <td><img src=\"images/cifar_samples.png\" width=\"200\"></td> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/unixpickle/vq-draw/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Go"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "VQ-DRAW",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "vq-draw",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "unixpickle",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/unixpickle/vq-draw/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "All of these experiments use PyTorch. I used Python 3.6.1, but future versions of Python should work as well. Here are package versions I used:\n\n```\ntorch==1.4.0\ntorchvision=0.5.0\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "These commands run the four experiments. Each command will periodically save the model as a `.pt` file; it will also save reconstructions and samples as images in the current directory. MNIST should run in an hour or two on a GPU. The other experiments may take up to a few days.\n\nIf you are running out of GPU memory, decrease the batch size by a factor of K and multiply the step interval and step limit by K. This will run an equivalent experiment with more gradient accumulation.\n\n```\npython -u train_mnist.py --batch 32 --step-limit 50000 --save-interval 500\npython -u train_svhn.py --batch 32 --step-interval 16 --step-limit 70000 --save-interval 500\npython -u train_cifar.py --batch 32 --step-interval 16 --step-limit 34811 --save-interval 500 --grad-checkpoint --lr-final 0.001\npython -u train_celeba.py --batch 32 --step-interval 16 --step-limit 36194 --save-interval 500 --grad-checkpoint --lr-final 0.001\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 37,
      "date": "Fri, 24 Dec 2021 06:37:37 GMT"
    },
    "technique": "GitHub API"
  }
}