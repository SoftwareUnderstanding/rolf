{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1502.04623",
      "https://arxiv.org/abs/1802.04208"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nT\u00f3th, Viktor & Laurie, Parkkonen. Autoencoding sensory substitution. (Aalto University, 2019).\n```\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/csiki/v2a",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-11T17:57:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-21T19:37:50Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9243663947460989,
        0.867288154536034,
        0.923362867731069
      ],
      "excerpt": "Visual-to-auditory (V2A) sensory substitution stands for the translation of images to sound, \nin the interest of aiding the blind. The generated soundscapes should convey visual information, \nideally representing all the details on the given image, with as short a sound sequences as possible. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8986771680102131,
        0.8648888065187689,
        0.879717763971041,
        0.9307697561693905,
        0.8989564040772897,
        0.9917768936307764
      ],
      "excerpt": "image pixel-by-pixel to soundscapes, superimposing them in the final step. Here is the implementation \nof a novel conversion approach, which posits sensory substitution as a compression problem. \nOptimal compression is learnt and computed by a recurrent variational autoencoder, called AEV2A. \nThe autoencoder takes an image as input, translates it to a sequence of soundscapes, before \nreconstructing the image in an iterative manner, drawing on a canvas. The neural network implementation \nis based on the DRAW model; the repository from which the code was \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8637012158802224
      ],
      "excerpt": "Videos on the visual-auditory correspondence of two models have been compiled and merged. Here are \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.93412952723426
      ],
      "excerpt": "For further details check this blog post or the thesis here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9299192884602515
      ],
      "excerpt": "Have fun with the tools given here! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8948370962103742
      ],
      "excerpt": "AEV2A is trained unsupervised, meaning, the image set is the only data needed to train the network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8813061359313822
      ],
      "excerpt": "from a set of images or videos. Consult the readme under data/ for further info. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8286153930924873,
        0.9098449847534166
      ],
      "excerpt": "The hand gesture dataset includes contour images of 15 different postures, in varying horizontal and vertical positions. \nThe table image set depicts contours of either a beer can or a gear on a surface, again in varying positions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8877733515941864,
        0.8705937746512699,
        0.9047191078303888
      ],
      "excerpt": "into sound. AEV2A model trained on table image samples was part of a experiment testing whether spatial \ninformation is perceptually maintained in the auditory domain. \nBefore starting the training, a configuration has to be defined containing the hyperparameters of the autoencoder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8703128591737921
      ],
      "excerpt": "The default config contains the same parameters as we used in the study to learn hand posture images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8173180513593202,
        0.9959309377582957
      ],
      "excerpt": "as (in order): number of training epochs, frequency of logs (number of trained batches between Tensorboard logs), \nfrequency of model saves and a postfix string for the model in case there are multiple models with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9485180492370665
      ],
      "excerpt": "to assess the efficacy of your model, run it like: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8551394360124108
      ],
      "excerpt": ": now open a web browser and type the url 'localhost:6006' to open the dashboard \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8663815920537197
      ],
      "excerpt": "trained models using regex for the purpose of comparing them. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9724291236849387,
        0.9127557812479704
      ],
      "excerpt": "the drawing, as a mean for the intuitive assessment of the sound-to-visual correspondence. \nVideos are stored under the vids/ folder; there should also be a longer video, which is the concatenation of the rest. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8867544768149767
      ],
      "excerpt": "This script could be used as an experimental tool, in which the presented image has to be named by the listener, and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9576929129469726,
        0.8268687488089088
      ],
      "excerpt": "In order to attain a high level, intuitive understanding of the V2A mapping, correlations between visual and \nauditory features can be computed, and such feature pairs may be plotted together to see the conversion \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.885375954190273
      ],
      "excerpt": "listen to the sound representation of instances to further build your intuition. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8102825797369858
      ],
      "excerpt": "or deactivate by editing the ANAL dictionary. The script originally was designed to examine two models, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.94253776883498,
        0.9662042462614564
      ],
      "excerpt": "It's essential to have an intuitive understanding of the conversion function, so, when used by blind people, \nit can be described in words, which is shown to lead to rapid learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9940452708021306,
        0.8693731351314149
      ],
      "excerpt": "tf_carfac is a Tensorflow implementation of the CARFAC cochlear model. \nBuilding and running the model takes way too much memory and time, even for short sound bits. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9009800820166265,
        0.9235334547773457
      ],
      "excerpt": "step of drawing. \nContribute to the project as much as you like! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Autoencoding visual-to-auditory sensory substitution",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/csiki/v2a/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Tue, 28 Dec 2021 20:25:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/csiki/v2a/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "csiki/v2a",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/csiki/v2a/master/data/gen_imgs_from_vids.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9760309766293735
      ],
      "excerpt": "If you installed the optional packages above, you can generate videos that play the soundscapes alongside \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.919747336777278
      ],
      "excerpt": "one trained on the hand, the other on the table dataset. You may provide the same config name for both \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8036700129120069,
        0.8137955362273385
      ],
      "excerpt": "All configurations should be specified in the configurations.json file. You could just use the default \nconfiguration already present in the json file, or create a new one according to the default. In config.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8580581372755205
      ],
      "excerpt": "To start the training process, just run the aev2a.py script like so: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9184875735215755
      ],
      "excerpt": "python aev2a.py config_name data/dataset.hdf5 train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152618241738402
      ],
      "excerpt": "simply run the script like: python aev2a.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.919338053424711
      ],
      "excerpt": "python aev2a.py config_name data/dataset.hdf5 test 0 0 model_name_postfix \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8291168924857603
      ],
      "excerpt": "The test_on_imgs.py script initiates a selected, already trained model, feeds images to it from the given dataset, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8367215230469378
      ],
      "excerpt": "from either the train or the test set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9515752551715031
      ],
      "excerpt": "python test_on_imgs.py cfg_name test seq model_name_postfix \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8253903337825405
      ],
      "excerpt": "Run gen_disentangle_data.py first to generate a dataset of drawings and corresponding sound features \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9515752551715031
      ],
      "excerpt": "python gen_disentangle_data.py cfg_name test seq model_name_postfix \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9515752551715031
      ],
      "excerpt": "python disentangle_anal.py cfg_name test model1_cfg_name model2_cfg_name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8022051171797697
      ],
      "excerpt": "Trained models are saved under the training folder, Tensorboard summaries under summary, \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/csiki/v2a/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "MATLAB",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Autoencoded sensory substitution",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "v2a",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "csiki",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/csiki/v2a/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Implementation has been tested on Linux (Ubuntu), but the core learning parts should run regardless of the operating system.\nThe following instructions are for Ubuntu only. Every library that's in brackets are not necessary for training purposes,\nbut needed for either dataset generation or testing/visualization.\n\nDataset generation may involve running a Matlab script, if only the contour of the images are planned to fed to the\nnetwork. The contour detection Matlab scripts are under `matlab/faster_corf/`, originally cloned from\nthe [Push-Pull CORF repository](https://www.mathworks.com/matlabcentral/fileexchange/47685-contour-detection-with-the-push-pull-corf-model).\nFor further information [consult the paper](https://link.springer.com/article/10.1007/s00422-012-0486-6).\nAlternatively, you can choose to perform Sobel edge detection or no edge detection at all, in which cases\nMatlab is not necessary to be installed.\n\n- `python >=3.6`, \\[`ffmpeg`, `opencv`\\]\n- Python packages: `numpy`, `scikit-image`, `matplotlib`, `tables`, \\[`csv`, `simpleaudio`, `scipy`, `scikit-learn`, `pymatlab`\\]\n- [`Tensorflow 1.9.x`](https://www.tensorflow.org/install): other versions may work too; GPU package recommended.\n```bash\nsudo apt-get install python3.6\nsudo python3.6 -m pip install\nsudo pip3 install numpy scikit-image tables matplotlib\nsudo pip3 install tensorflow-gpu  #: requires CUDA-enabled GPU\n```\n```bash\n#: not mandatory for training purposes:\nsudo apt-get install python3-opencv ffmpeg\nsudo pip3 install csv simpleaudio scipy scikit-learn pymatlab\n```\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Tue, 28 Dec 2021 20:25:26 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "sensory",
      "substitution",
      "autoencoder",
      "visual",
      "to",
      "auditory"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You may run your AEV2A model live by taking a video with your Android phone and listening to the corresponding\naudio representation at the same time. In our implementation, the captured video is streamed to your PC,\nwhere it gets translated into sound, so you may listen to it. Ideally, you would place your phone\ninside a VR helmet/cardbox, fastening the camera at eye level; headphones are essential.\n\n`run_proto.py` runs the live demo. Similarly to the dataset generation phase, you can set whether to\napply the more sophisticated CORF edge detection algorithm (Matlab required), just Sobel, or nothing at all.\n\nTo set up your Android phone with the trained AEV2A model, you first need to:\n\n1. Install the IP Webcam app from Google Play, launch it and set the video resolution to `320x240` under Video preferences\n2. Connect your phone via USB to the computer that runs the script\n3. Turn USB tethering on the phone, but turn off WiFi and mobile data\n4. Launch the IP Webcam app and press \"Start server\".\n5. Start the `run_proto.py` script with parameters providing whether to run in \"test\" or \"fast\" mode\n(test mode shows how the contour image and the decoder reconstructed image looks like real time),\nthe edge detection algo to apply (`corf`, `sobel` or `nothing`), the mobile ip of your phone (displayed in the IP Webcam app),\nthe name of the model configuration and postfix identifier, if used any.\n\n```bash\npython run_proto.py test corf mobile_ip config_name model_name_postfix\n```\n\nAfter the model is loaded, you should be seeing three windows of images showing the original, contour and\nreconstruction stages. The audio should be playing at the same time, too.\n\n",
      "technique": "Header extraction"
    }
  ]
}