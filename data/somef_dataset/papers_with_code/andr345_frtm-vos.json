{
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{Robinson_2020_CVPR,\n    author = {Robinson, Andreas and Lawin, Felix Jaremo and Danelljan, Martin and Khan, Fahad Shahbaz and Felsberg, Michael},\n    title = {Learning Fast and Robust Target Models for Video Object Segmentation},\n    booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month = {June},\n    year = {2020}\n\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9299784072695287
      ],
      "excerpt": "found in the CVPR 2020 paper <em>\"Learning Fast and Robust Target Models for Video Object Segmentation\"</em> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9792428879788975,
        0.9994335638103145,
        0.9996213726593993,
        0.99916104956856,
        0.9999999995130295,
        0.8955886365383559,
        0.9664456561658856
      ],
      "excerpt": "Arxiv: [paper] \nIf you find the code useful, please cite using: \n    author = {Robinson, Andreas and Lawin, Felix Jaremo and Danelljan, Martin and Khan, Fahad Shahbaz and Felsberg, Michael}, \n    title = {Learning Fast and Robust Target Models for Video Object Segmentation}, \n    booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, \n    month = {June}, \n    year = {2020} \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/andr345/frtm-vos",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Andreas Robinson\n\nemail: andreas.robinson@liu.se\n\nFelix J\u00e4remo Lawin\n\nemail: felix.jaremo-lawin@liu.se\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-27T17:32:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T10:14:52Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9169164443617861,
        0.9854570601176297
      ],
      "excerpt": "This repository contains an implementation of the video object segmentation \nmethod FRTM. A detailed description of the method is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.915667281951831
      ],
      "excerpt": "in lib/ytvos_jjvalid.txt. Thanks to Joakim Johnander for providing this split. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8054756483738261,
        0.9736378020817893,
        0.9530634189337328,
        0.9461607522812278,
        0.88137752793025
      ],
      "excerpt": "Once the cache is mostly full, the next training session should take less than 24 hours. \nThe cache requires 17 GB disk space for training with ResNet-101 features and 32 intermediate \nchannels (as in the paper) and 5 GB for ResNet-18 and the same number of channels. \nOur own cache (20 GB) is available here. \nThe link is not permanent and will change eventually, so make sure to check this readme in the GitHub \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code accompanying the paper Learning Fast and Robust Target Models for Video Object Segmentation",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/andr345/frtm-vos/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 24,
      "date": "Mon, 27 Dec 2021 18:14:22 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/andr345/frtm-vos/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "andr345/frtm-vos",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/andr345/frtm-vos/master/weights/download_weights.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Clone the repository: `git clone https://github.com/andr345/frtm-vos.git`\n\nCreate a conda environment and install the following dependencies:\n```shell script\nsudo apt install ninja-build  #: For Debian/Ubuntu\nconda install -y cython pip scipy scikit-image tqdm\nconda install -y pytorch torchvision cudatoolkit=10.1 -c pytorch\npip install opencv-python easydict\n```\n\nPyTorch 1.0.1 is slightly faster. If you wish to try to this version, replace the `conda install pytorch` above\nwith the following:\n```shell script\nconda install pytorch==1.0.1 torchvision==0.2.2 -c pytorch\npip install \"pillow<7\"\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.807962173405798
      ],
      "excerpt": "These pretrained models are available for download:  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "|-- train/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.861400960564515,
        0.861400960564515,
        0.861400960564515,
        0.861400960564515,
        0.861400960564515,
        0.8222283271652884,
        0.8097097395299968
      ],
      "excerpt": "| rn18_ytvos.pth  | ResNet18  | YouTubeVOS         | download | \n| rn18_all.pth    | ResNet18  | YouTubeVOS + DAVIS | download \n| rn101_ytvos.pth | ResNet101 | YouTubeVOS         | download | \n| rn101_all.pth   | ResNet101 | YouTubeVOS + DAVIS | download | \n| rn101_dv.pth    | ResNet101 | DAVIS              | download | \nThe script weights/download_weights.sh will download all models and put them in the folder weights/. \nTraining target models from scratch and filling the cache take approximately 5 days of training. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/andr345/frtm-vos/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "GNU General Public License v3.0",
      "url": "https://api.github.com/licenses/gpl-3.0"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "FRTM-VOS",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "frtm-vos",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "andr345",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/andr345/frtm-vos/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Open `evaluate.py` and adjust the `paths` dict to your dataset locations and where you want the output.\nThe dictionary is found near line 110, and looks approximately like this:\n\n```python\n    paths = dict(\n        models=Path(__file__).parent / \"weights\",  #: The .pth files should be here\n        davis=\"/path/to/DAVIS\",  #: DAVIS dataset root\n        yt2018=\"/path/to/ytvos2018\",  #: YouTubeVOS 2018 root\n        output=\"/path/to/results\",  #: Output path\n    )\n```\n\nThen try one of the evaluations below. The first run will pause for a few seconds while compiling a\nPyTorch C++ extension.\n\nScripts for generating the results in the paper:\n```shell script\npython evaluate.py --model rn101_ytvos.pth --dset yt2018val       #: Ours YouTubeVos 2018\npython evaluate.py --model rn101_all.pth --dset dv2016val         #: Ours DAVIS 2016\npython evaluate.py --model rn101_all.pth --dset dv2017val         #: Ours DAVIS 2017\n\npython evaluate.py --model rn18_ytvos.pth --fast --dset yt2018val #: Ours fast YouTubeVos 2018\npython evaluate.py --model rn18_all.pth --fast --dset dv2016val   #: Ours fast DAVIS 2016\npython evaluate.py --model rn18_all.pth --fast --dset dv2017val   #: Ours fast DAVIS 2017\n```\n\n`--model` is the name of the checkpoint to use in the `weights` directory.\n\n`--fast` reduces the number of optimizer iterations to match \"Ours fast\" in the paper.\n\n`--dset` is one of\n\n  | Name        | Description                                                |\n  |-------------|------------------------------------------------------------|\n  | dv2016val   | DAVIS 2016 validation set                                  |\n  | dv2017val   | DAVIS 2017 validation set                                  |\n  | yt2018jjval | Our validation split of YouTubeVOS 2018 \"train_all_frames\" |\n  | yt2018val   | YouTubeVOS 2018 official \"valid_all_frames\" set            |\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Training is set up similarly to evaluation.\n\nOpen `train.py` and adjust the `paths` dict to your dataset locations, checkpoint and tensorboard\noutput directories and the place to cache target model weights.\n\nTo train a network, run\n\n```shell script\npython train.py <session-name> --ftext resnet101 --dset all --dev cuda:0\n```\n`--ftext` is the name of the feature extractor, either resnet18 or resnet101.\n\n`--dset` is one of dv2017, ytvos2018 or all (\"all\" really means \"both\").\n\n`--dev` is the name of the device to train on.\n\nReplace \"session-name\" with whatever you like. Subdirectories with this name\nwill be created under your checkpoint and tensorboard paths.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 111,
      "date": "Mon, 27 Dec 2021 18:14:22 GMT"
    },
    "technique": "GitHub API"
  }
}