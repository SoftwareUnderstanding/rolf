{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1512.04150",
      "https://arxiv.org/abs/1610.02391",
      "https://arxiv.org/abs/1710.11063",
      "https://arxiv.org/abs/1908.01224",
      "https://arxiv.org/abs/1910.01279",
      "https://arxiv.org/abs/2006.14255",
      "https://arxiv.org/abs/2010.03023",
      "https://arxiv.org/abs/2008.02312"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you wish to cite this project, feel free to use this [BibTeX](http://www.bibtex.org/) reference:\n\n```bibtex\n@misc{torcham2020,\n    title={TorchCAM: class activation explorer},\n    author={Fran\u00e7ois-Guillaume Fernandez},\n    year={2020},\n    month={March},\n    publisher = {GitHub},\n    howpublished = {\\url{https://github.com/frgfm/torch-cam}}\n}\n```\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{torcham2020,\n    title={TorchCAM: class activation explorer},\n    author={Fran\u00e7ois-Guillaume Fernandez},\n    year={2020},\n    month={March},\n    publisher = {GitHub},\n    howpublished = {\\url{https://github.com/frgfm/torch-cam}}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/frgfm/torch-cam/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/frgfm/torch-cam",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to torchcam\nEverything you need to know to contribute efficiently to the project!\nWhatever the way you wish to contribute to the project, please respect the code of conduct.\nCodebase structure\n\ntorchcam - The actual torchcam library\ntests - Python unit tests\ndocs - Sphinx documentation building\nscripts - Example and utilities scripts\ndemo - Small demo app to showcase docTR capabilities \n\nContinuous Integration\nThis project uses the following integrations to ensure proper codebase maintenance:\n\nGithub Worklow - run jobs for package build and coverage\nCodacy - analyzes commits for code quality\nCodecov - reports back coverage results\n\nAs a contributor, you will only have to ensure coverage of your code by adding appropriate unit testing of your code.\nFeedback\nFeature requests & bug report\nWhether you encountered a problem, or you have a feature suggestion, your input has value and can be used by contributors to reference it in their developments. For this purpose, we advise you to use Github issues. \nFirst, check whether the topic wasn't already covered in an open / closed issue. If not, feel free to open a new one! When doing so, use issue templates whenever possible and provide enough information for other contributors to jump in.\nQuestions\nIf you are wondering how to do something with TorchCAM, or a more general question, you should consider checking out Github discussions. See it as a Q&A forum, or the TorchCAM-specific StackOverflow!\nSubmitting a Pull Request\nPreparing your local branch\n1 - Fork this repository by clicking on the \"Fork\" button at the top right of the page. This will create a copy of the project under your GitHub account (cf. Fork a repo).\n2 - Clone your fork to your local disk and set the upstream to this repo\nshell\ngit clone git@github.com:&lt;YOUR_GITHUB_ACCOUNT&gt;/torch-cam.git\ncd torch-cam\ngit remote add upstream https://github.com/frgfm/torch-cam.git\n3 - You should not work on the master branch, so let's create a new one\nshell\ngit checkout -b a-short-description\n4 - You only have to set your development environment now. First uninstall any existing installation of the library with pip uninstall torch-cam, then:\nshell\npip install -e \".[dev]\"\nDeveloping your feature\nCommits\n\nCode: ensure to provide docstrings to your Python code. In doing so, please follow Google-style so it can ease the process of documentation later.\nCommit message: please follow Udacity guide\n\nUnit tests\nIn order to run the same unit tests as the CI workflows, you can run unittests locally:\nshell\nmake test\nCode quality\nTo run all quality checks together\nshell\nmake quality\nLint verification\nTo ensure that your incoming PR complies with the lint settings, you need to install flake8 and run the following command from the repository's root folder:\nshell\nflake8 ./\nThis will read the .flake8 setting file and let you know whether your commits need some adjustments.\nImport order\nIn order to ensure there is a common import order convention, run isort as follows:\nshell\nisort **/*.py\nThis will reorder the imports of your local files.\nAnnotation typing\nAdditionally, to catch type-related issues and have a cleaner codebase, annotation typing are expected. After installing mypy, you can run the verifications as follows:\nshell\nmypy --config-file mypy.ini\nThe mypy.ini file will be read to check your typing.\nSubmit your modifications\nPush your last modifications to your remote branch\nshell\ngit push -u origin a-short-description\nThen open a Pull Request from your fork's branch. Follow the instructions of the Pull Request template and then click on \"Create a pull request\".",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-23T18:34:51Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-26T12:34:31Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8491125736162916
      ],
      "excerpt": "Simple way to leverage the class-specific activation of convolutional layers in PyTorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8953709355773997
      ],
      "excerpt": "Once your CAM extractor is set, you only need to use your model to infer on your data as usual. If any additional information is required, the extractor will get it for you automatically. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "cam_extractor = SmoothGradCAMpp(model) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9461178907091387
      ],
      "excerpt": ": Preprocess it for your chosen model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9921146333669704
      ],
      "excerpt": ": Preprocess your data and feed it to the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9438812358390645
      ],
      "excerpt": ": Resize the CAM and overlay it \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9825210705369076,
        0.9642174994314222
      ],
      "excerpt": "This project is developed and maintained by the repo owner, but the implementation was based on the following research papers: \nLearning Deep Features for Discriminative Localization: the original CAM paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9759913999566586
      ],
      "excerpt": "Grad-CAM++: improvement of GradCAM++ for more accurate pixel-level contribution to the activation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8887326855797043
      ],
      "excerpt": "Score-CAM: score-weighting of class activation for better interpretability. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9829888861784798,
        0.9844721912716828,
        0.8599217130561153
      ],
      "excerpt": "IS-CAM: integration-based variant of Score-CAM. \nXGrad-CAM: improved version of Grad-CAM in terms of sensitivity and conservation. \nLayer-CAM: Grad-CAM alternative leveraging pixel-wise contribution of the gradient to the activation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8472337908452944
      ],
      "excerpt": "You crave for beautiful activation maps, but you don't know whether it fits your needs in terms of latency? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.940102751210876,
        0.9475219524783951
      ],
      "excerpt": "*The base CAM method cannot work with architectures that have multiple fully-connected layers \nThis benchmark was performed over 100 iterations on (224, 224) inputs, on a laptop to better reflect performances that can be expected by common users. The hardware setup includes an Intel(R) Core(TM) i7-10750H for the CPU, and a NVIDIA GeForce RTX 2070 with Max-Q Design for the GPU. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9915200574081435
      ],
      "excerpt": "Feeling like extending the range of possibilities of CAM? Or perhaps submitting a paper implementation? Any sort of contribution is greatly appreciated! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Class activation maps for your PyTorch models (CAM, Grad-CAM, Grad-CAM++, Smooth Grad-CAM++, Score-CAM, SS-CAM, IS-CAM, XGrad-CAM, Layer-CAM)",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The full package documentation is available [here](https://frgfm.github.io/torch-cam/) for detailed specifications.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/frgfm/torch-cam/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 61,
      "date": "Mon, 27 Dec 2021 06:05:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/frgfm/torch-cam/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "frgfm/torch-cam",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/frgfm/torch-cam/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/frgfm/torch-cam/master/docs/build.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Alternatively, if you wish to use the latest features of the project that haven't made their way to a release yet, you can install the package from source:\n\n```shell\ngit clone https://github.com/frgfm/torch-cam.git\npip install -e torch-cam/.\n```\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Python 3.6 (or higher) and [pip](https://pip.pypa.io/en/stable/)/[conda](https://docs.conda.io/en/latest/miniconda.html) are required to install TorchCAM.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "TorchCAM leverages [PyTorch hooking mechanisms](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) to seamlessly retrieve all required information to produce the class activation without additional efforts from the user. Each CAM object acts as a wrapper around your model.\n\nYou can find the exhaustive list of supported CAM methods in the [documentation](https://frgfm.github.io/torch-cam/methods.html), then use it as follows:\n\n```python\n#: Define your model\nfrom torchvision.models import resnet18\nmodel = resnet18(pretrained=True).eval()\n\n#: Set your CAM extractor\nfrom torchcam.methods import SmoothGradCAMpp\ncam_extractor = SmoothGradCAMpp(model)\n```\n\n*Please note that by default, the layer at which the CAM is retrieved is set to the last non-reduced convolutional layer. If you wish to investigate a specific layer, use the `target_layer` argument in the constructor.*\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8309637544301691
      ],
      "excerpt": "        <img src=\"https://github.com/frgfm/torch-cam/releases/download/v0.2.0/cam_example_2rows.png\" /></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8792947804879951
      ],
      "excerpt": "If you want to visualize your heatmap, you only need to cast the CAM to a numpy ndarray: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9732168512936369
      ],
      "excerpt": "You can install the last stable release of the package using pypi as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999746712887969,
        0.9655328829406837
      ],
      "excerpt": "pip install torchcam \nor using conda: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9995234057802402
      ],
      "excerpt": "conda install -c frgfm torchcam \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8874176029437957
      ],
      "excerpt": "        <img src=\"https://github.com/frgfm/torch-cam/releases/download/v0.2.0/video_example_wallaby.gif\" /></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044336589134822
      ],
      "excerpt": "This benchmark was performed over 100 iterations on (224, 224) inputs, on a laptop to better reflect performances that can be expected by common users. The hardware setup includes an Intel(R) Core(TM) i7-10750H for the CPU, and a NVIDIA GeForce RTX 2070 with Max-Q Design for the GPU. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8047347356538868
      ],
      "excerpt": "from torchvision.io.image import read_image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8900486270063179
      ],
      "excerpt": "from torchvision.models import resnet18 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9036224820499943
      ],
      "excerpt": "model = resnet18(pretrained=True).eval() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9068127677393759
      ],
      "excerpt": "import matplotlib.pyplot as plt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8690049869015287
      ],
      "excerpt": "plt.imshow(activation_map[0].numpy()); plt.axis('off'); plt.tight_layout(); plt.show() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9068127677393759,
        0.9416522774131079
      ],
      "excerpt": "import matplotlib.pyplot as plt \nfrom torchcam.utils import overlay_mask \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8833342485326611
      ],
      "excerpt": "plt.imshow(result); plt.axis('off'); plt.tight_layout(); plt.show() \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/frgfm/torch-cam/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "TorchCAM: class activation explorer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "torch-cam",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "frgfm",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/frgfm/torch-cam/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "frgfm",
        "body": "This patch release adds new features to the demo and reorganizes the package for a clearer hierarchy.\r\n\r\n**Note**: TorchCAM 0.3.1 requires PyTorch 1.5.1 or higher.\r\n\r\n# Highlights\r\n## CAM fusion is coming to the demo :rocket: \r\n\r\nWith release 0.3.0, the support of multiple target layers was added as well as CAM fusion. The demo was updated to automatically fuse CAMs when you hooked multiple layers (add a \"+\" separator between each layer name):\r\n\r\n![demo](https://user-images.githubusercontent.com/26927750/139595071-190aad5b-6515-4833-9f18-1774fb1fd719.png)\r\n\r\n\r\n# Breaking changes\r\n## Submodule renaming\r\n\r\nTo anticipate further developments of the library, modules were renamed:\r\n- `torchcam.cams` was renamed into `torchcam.methods`\r\n- `torchcam.cams.utils` was renamed and made private (`torchcam.methods._utils`) since it's API may evolve quickly\r\n- activation-based CAM methods are now implemented in `torchcam.methods.activation` rather than `torchcam.cams.cam`\r\n- gradient-based CAM methods are now implemented in `torchcam.methods.gradient` rather than `torchcam.cams.gradcam`\r\n\r\n0.3.0 | 0.3.1\r\n-- | --\r\n`>>> from torchcam.cams import LayerCAM` | `>>> from torchcam.methods import LayerCAM`  |\r\n\r\n\r\n## What's Changed\r\n* chore: Made post release modifications by @frgfm in https://github.com/frgfm/torch-cam/pull/103\r\n* docs: Updated changelog by @frgfm in https://github.com/frgfm/torch-cam/pull/104\r\n* feat: Added possibility to retrieve multiple CAMs in demo by @frgfm in https://github.com/frgfm/torch-cam/pull/105\r\n* refactor: Reorganized package hierarchy by @frgfm in https://github.com/frgfm/torch-cam/pull/106\r\n* docs: Fixed LaTeX syntax in docstrings by @frgfm in https://github.com/frgfm/torch-cam/pull/107\r\n\r\n\r\n**Full Changelog**: https://github.com/frgfm/torch-cam/compare/v0.3.0...v0.3.1",
        "dateCreated": "2021-10-31T17:51:32Z",
        "datePublished": "2021-10-31T17:55:10Z",
        "html_url": "https://github.com/frgfm/torch-cam/releases/tag/v0.3.1",
        "name": "v0.3.1: Improved demo & reorganized package",
        "tag_name": "v0.3.1",
        "tarball_url": "https://api.github.com/repos/frgfm/torch-cam/tarball/v0.3.1",
        "url": "https://api.github.com/repos/frgfm/torch-cam/releases/52404987",
        "zipball_url": "https://api.github.com/repos/frgfm/torch-cam/zipball/v0.3.1"
      },
      {
        "authorType": "User",
        "author_name": "frgfm",
        "body": "This release extends CAM methods with Layer-CAM, greatly improves the core features (CAM computation for multiple layers at once, CAM fusion, support of `torch.nn.Module`), while improving accessibility for entry users.\r\n\r\n**Note**: TorchCAM 0.3.0 requires PyTorch 1.5.1 or higher.\r\n\r\n# Highlights\r\n### Enters Layer-CAM\r\n\r\nThe previous release saw the introduction of Score-CAM variants, and this one introduces you to [Layer-CAM](http://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf), which is meant to be considerably faster, while offering very competitive localization cues!\r\n\r\nJust like any other CAM methods, you can now use it as follows:\r\n\r\n```python\r\nfrom torchcam.cams import LayerCAM\r\n# model = ....\r\n# Hook the model\r\ncam_extractor = LayerCAM(model)\r\n```\r\n\r\nConsequently, the illustration of visual outputs for all CAM methods has been updated so that you can better choose the option that suits you:\r\n\r\n![cam_example](https://github.com/frgfm/torch-cam/releases/download/v0.2.0/cam_example_2rows.png)\r\n\r\n\r\n### Computing CAMs for multiple layers & CAM fusion\r\n\r\nA class activation map is specific to a given layer in a model. To fully capture the influence of visual traits on your classification output, you might want to explore the CAMs for multiple layers.\r\n\r\nFor instance, here are the CAMs on the layers \"layer2\", \"layer3\" and \"layer4\" of a `resnet18`:\r\n\r\n```python\r\nfrom torchvision.io.image import read_image\r\nfrom torchvision.models import resnet18\r\nfrom torchvision.transforms.functional import normalize, resize, to_pil_image\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom torchcam.cams import LayerCAM\r\nfrom torchcam.utils import overlay_mask\r\n\r\n# Download an image\r\n!wget https://www.woopets.fr/assets/races/000/066/big-portrait/border-collie.jpg\r\n# Set this to your image path if you wish to run it on your own data\r\nimg_path = \"border-collie.jpg\"\r\n\r\n# Get your input\r\nimg = read_image(img_path)\r\n# Preprocess it for your chosen model\r\ninput_tensor = normalize(resize(img, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\r\n# Get your model\r\nmodel = resnet18(pretrained=True).eval()\r\n# Hook the model\r\ncam_extractor = LayerCAM(model, [\"layer2\", \"layer3\", \"layer4\"])\r\n\r\nout = model(input_tensor.unsqueeze(0))\r\ncams = cam_extractor(out.squeeze(0).argmax().item(), out)\r\n# Plot the CAMs\r\n_, axes = plt.subplots(1, len(cam_extractor.target_names))\r\nfor idx, name, cam in zip(range(len(cam_extractor.target_names)), cam_extractor.target_names, cams):\r\n  axes[idx].imshow(cam.numpy()); axes[idx].axis('off'); axes[idx].set_title(name);\r\nplt.show()\r\n```\r\n\r\n![multi_cams](https://user-images.githubusercontent.com/26927750/139589179-6ad35490-d6f3-4705-900f-3a0267bf02c9.png)\r\n\r\nNow, the way you would combine those together is up to you. By default, most approaches use an element-wise maximum. But, LayerCAM has its own fusion method:\r\n\r\n```python\r\n# Let's fuse them\r\nfused_cam = cam_extractor.fuse_cams(cams)\r\n# Plot the raw version\r\nplt.imshow(fused_cam.numpy()); plt.axis('off'); plt.title(\" + \".join(cam_extractor.target_names)); plt.show()\r\n```\r\n\r\n![fused_cams](https://user-images.githubusercontent.com/26927750/139589184-325bfe66-762a-421d-9fd3-f9e62c3c4558.png)\r\n\r\n```python\r\n# Overlay it on the image\r\nresult = overlay_mask(to_pil_image(img), to_pil_image(fused_cam, mode='F'), alpha=0.5)\r\n# Plot the result\r\nplt.imshow(result); plt.axis('off'); plt.title(\" + \".join(cam_extractor.target_names)); plt.show()\r\n```\r\n\r\n![fused_overlay](https://user-images.githubusercontent.com/26927750/139590233-9217217e-2bc4-4a4e-9b41-3178db9afc8a.png)\r\n\r\n\r\n### Support of `torch.nn.Module` as `target_layer`\r\n\r\nWhile making the API more robust, CAM constructors now also accept `torch.nn.Module` as `target_layer`. Previously, you had to pass the name of the layer as string, but you can now pass the object reference directly if you prefer:\r\n```python\r\nfrom torchcam.cams import LayerCAM\r\n# model = ....\r\n# Hook the model\r\ncam_extractor = LayerCAM(model, model.layer4)\r\n```\r\n\r\n### :zap: Latency benchmark :zap: \r\n\r\nSince CAMs can be used from localization or production pipelines, it is important to consider latency along with pure visual output quality. For this reason, a latency evaluation script has been included in this release along with a full [benchmark table](https://github.com/frgfm/torch-cam#latency-benchmark).\r\n\r\nShould you wish to have latency metrics on your dedicated hardware, you can run the script on your own:\r\n```shell\r\npython scripts/eval_latency.py SmoothGradCAMpp --size 224\r\n```\r\n\r\n### Notebooks :play_or_pause_button: \r\n\r\nDo you prefer to only run code rather than write it? Perhaps you only want to tweak a few things?\r\nThen enjoy the brand new [Jupyter notebooks](https://github.com/frgfm/torch-cam/tree/master/notebooks) than you can either run locally or on [Google Colab](https://colab.research.google.com/)!\r\n\r\n### :hugs: Live demo :hugs:  \r\n\r\nThe ML community was recently blessed by HuggingFace with their beta of [Spaces](https://huggingface.co/spaces), which let you host free-of-charge your ML demos!\r\n\r\nPreviously, you were able to run the demo locally on deploy it on your own, but now, you can enjoy the [live demo of TorchCAM](https://huggingface.co/spaces/frgfm/torch-cam) :art: \r\n\r\n\r\n# Breaking changes\r\n\r\n## Multiple CAM output\r\n\r\nSince CAM extractor can now compute the resulting maps for multiple layer at a time, the return type of all CAMs has been changed from `torch.Tensor` to `List[torch.Tensor]` with N elements, where N is the number of target layers.\r\n\r\n0.2.0 | 0.3.0\r\n-- | --\r\n`>>> from torchcam.cams import SmoothGradCAMpp` <br/> `>>> extractor = SmoothGradCAMpp(model)` <br/> `>>> out = model(input_tensor.unsqueeze(0))` <br/> `>>> print(type(cam_extractor(out.squeeze(0).argmax().item(), out)))` <br/> `<class 'torch.Tensor'>` | `>>> from torchcam.cams import SmoothGradCAMpp` <br/> `>>> extractor = SmoothGradCAMpp(model)` <br/> `>>> out = model(input_tensor.unsqueeze(0))` <br/> `>>> print(type(cam_extractor(out.squeeze(0).argmax().item(), out)))` <br/> `<class 'list'>` |\r\n\r\n\r\n# New features\r\n\r\n## CAMs\r\nImplementations of CAM method\r\n- Added support of conv1x1 as FC candidate in base CAM #69 (@frgfm)\r\n- Added support of LayerCAM #77 (@frgfm)\r\n- Added support of `torch.nn.Module` as `target_layer` or `fc_layer` #83 (@frgfm)\r\n- Added support of multiple target layers for all CAM methods #89 #92 (@frgfm)\r\n- Added layer-specific CAM fusion method #93 (@frgfm)\r\n\r\n## Scripts\r\nSide scripts to make the most out of TorchCAM\r\n- Added latency evaluation script #95 (@frgfm)\r\n\r\n## Test\r\nVerifications of the package well-being before release\r\n- Added unittests to verify that conv1x1 can be used as FC in base CAM #69 (@frgfm)\r\n- Added unittest for LayerCAM #77 (@frgfm)\r\n- Added unittest for gradient-based CAM method for models with in-place ops #80 (@frgfm)\r\n- Added unittest to check support of `torch.nn.Module` as `target_layer` in CAM constructor #83 #88 (@frgfm)\r\n- Added unittest for CAM fusion #93 (@frgfm)\r\n\r\n## Documentation\r\nOnline resources for potential users\r\n- Added LayerCAM ref in the README and in the documentation #77 (@frgfm)\r\n- Added CODE_OF_CONDUCT #86 (@frgfm)\r\n- Added changelog to the documentation #91 (@frgfm)\r\n- Added latency benchmark & GIF illustration of CAM on a video in README #95 (@frgfm)\r\n- Added documentation of `.fuse_cams` method #93 (@frgfm)\r\n- Added ref to HF Space demo in README and documentation #96 (@frgfm)\r\n- Added tutorial notebooks and reference page in the documentation #99 #100 #101 #102 (@frgfm)\r\n\r\n## Others\r\nOther tools and implementations\r\n- Added `class_idx` & `target_layer` selection in the demo #67 (@frgfm)\r\n- Added CI jobs to build on different OS & Python versions, to validate the demo, and the example script #73 #74  (@frgfm)\r\n- Added LayerCAM to the demo #77 (@frgfm)\r\n- Added an environment collection script #78 (@frgfm)\r\n- Added CI check for the latency evaluation script #95 (@frgfm)\r\n\r\n# Bug fixes\r\n## CAMs\r\n- Fixes backward hook mechanism for in-place operations #80 (@frgfm)\r\n\r\n## Documentation\r\n- Fixed `docutils` version constraint for documentation building #98 (@frgfm)\r\n\r\n## Others\r\n- Fixed CI job to build documentation #64 (@frgfm)\r\n- Fixed Pillow version constraint #73 #84 (@frgfm)\r\n\r\n# Improvements\r\n\r\n## CAMs\r\n- Improved weight broadcasting for all CAMs #77 (@frgfm)\r\n- Refactored hook enabling #80 (@frgfm)\r\n- Improved the warning message for target automatic resolution #87 #92 (@frgfm)\r\n- Improved arg type checking for CAM constructor #88 (@frgfm)\r\n\r\n## Scripts\r\n- Improved the layout option of the example script #66 (@frgfm)\r\n- Refactored example script #80 #94 (@frgfm)\r\n- Updated all scripts for support of multiple target layers #89 (@frgfm)\r\n\r\n## Test\r\n- Updated unittests for multiple target layer support #89 (@frgfm)\r\n\r\n## Documentation\r\n- Added latest release doc version & updated README badge #63 (@frgfm)\r\n- Added demo screenshot in the README #67 (@frgfm)\r\n- Updated instructions in README #89 (@frgfm)\r\n- Improved documentation landing page #91 (@frgfm)\r\n- Updated contribution guidelines #94 (@frgfm)\r\n- Updated documentation requirements #99 (@frgfm)\r\n\r\n## Others\r\n- Updated package version and fixed CI jobs to validate release publish #63 (@frgfm)\r\n- Updated license from MIT to Apache 2.0 #70 (@frgfm)\r\n- Refactored CI jobs #73 (@frgfm)\r\n- Improved bug report template #78 (@frgfm)\r\n- Updated streamlit syntax in demo #94 (@frgfm)\r\n- Added isort config and CI job #97 (@frgfm)\r\n- Added CI job for sanity check of the documentation build #98 (@frgfm)",
        "dateCreated": "2021-10-31T12:34:18Z",
        "datePublished": "2021-10-31T15:24:56Z",
        "html_url": "https://github.com/frgfm/torch-cam/releases/tag/v0.3.0",
        "name": "v0.3.0: Support of Layer-CAM & multi-layer CAM computation",
        "tag_name": "v0.3.0",
        "tarball_url": "https://api.github.com/repos/frgfm/torch-cam/tarball/v0.3.0",
        "url": "https://api.github.com/repos/frgfm/torch-cam/releases/52401522",
        "zipball_url": "https://api.github.com/repos/frgfm/torch-cam/zipball/v0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "frgfm",
        "body": "This release extends TorchCAM compatibility to 3D inputs, and improves documentation.\r\n\r\n**Note**: TorchCAM 0.2.0 requires PyTorch 1.5.1 or higher.\r\n\r\n# Highlights\r\n### Compatibility for inputs with more than 2 spatial dimensions\r\nThe first papers about CAM methods were built for classification models using 2D (spatially) inputs. However, the latest methods can be extrapolated to higher dimension inputs and it's now live:\r\n```python\r\nimport torch\r\nfrom torchcam.cams import SmoothGradCAMpp\r\n# Define your model compatible with 3D inputs\r\nvideo_model = ...\r\nextractor = SmoothGradCAMpp(video_model)\r\n# Forward your input\r\nscores = model(torch.rand((1, 3, 32, 224, 224)))\r\n# Retrieve the CAM\r\ncam = extractor(scores[0].argmax().item(), scores)\r\n```\r\n\r\n### Multi-version documentation\r\nWhile documentation was up-to-date with the latest commit on the main branch, previously if you were running an older release of the library, you had no corresponding documentation.\r\n\r\nAs of now, you can select the version of the documentation you wish to access (stable releases or latest commit):\r\n![torchcam_doc](https://user-images.githubusercontent.com/26927750/114251220-8f202e00-99a0-11eb-88c4-3bc43155da3f.png)\r\n\r\n\r\n### Demo app\r\nSince spatial information is at the very core of TorchCAM, a minimal [Streamlit](https://streamlit.io/) demo app was added to explore the activation of your favorite models. You can run the demo with the following commands:\r\n```\r\nstreamlit run demo/app.py\r\n```\r\n\r\nHere is how it renders retrieving the heatmap using `SmoothGradCAMpp` on a pretrained `resnet18`:\r\n![torchcam_demo](https://github.com/frgfm/torch-cam/releases/download/v0.1.2/torchcam_demo.png)\r\n\r\n# New features\r\n\r\n## CAMs\r\nImplementations of CAM method\r\n- Enabled CAM compatibility for inputs with more than 2 spatial dimensions #45 (@frgfm)\r\n- Added support of XGradCAM #47 (@frgfm)\r\n\r\n## Test\r\nVerifications of the package well-being before release\r\n- Added unittests for XGradCAM #47 (@frgfm)\r\n\r\n## Documentation\r\nOnline resources for potential users\r\n- Added references to XGradCAM in README and documentation #47 (@frgfm)\r\n- Added multi-version documentation & added github star button #53, #54, #55, #56 (@frgfm)\r\n- Revamped README #59 (@frgfm) focusing on short easy code snippets\r\n- Improved documentation #60 (@frgfm)\r\n\r\n## Others\r\nOther tools and implementations\r\n- Added issue templates for bug report and feature request #49 (@frgfm)\r\n- Added option to specify a single CAM method in example script #52 (@frgfm)\r\n- Added minimal demo app #59 (@frgfm)\r\n\r\n# Bug fixes\r\n## CAMs\r\n- Fixed automatic layer resolution on GPU #41 (@frgfm)\r\n- Fixed backward hook warnings for Pytorch >= 1.8.0 #58 (@frgfm)\r\n\r\n## Utils\r\n- Fixed RGBA -> RGB conversion in `overlay_mask` #38 (@alexandrosstergiou)\r\n\r\n## Test\r\n- Fixed `overlay_mask` unittest #38 (@alexandrosstergiou)\r\n\r\n## Documentation\r\n- Fixed codacy badge in README #46 (@frgfm)\r\n- Fixed typo in documentation #62 (@frgfm)\r\n\r\n## Others\r\n- Fixed CI job for conda build #34 (@frgfm)\r\n- Fixed model mode in example script #37 (@frgfm)\r\n- Fixed sphinx version #40 (@frgfm)\r\n- Fixed usage instructions in README #43 (@frgfm)\r\n- Fixed example script for local image input #51 (@frgfm)\r\n\r\n# Improvements\r\n\r\n## CAMs\r\n- Added NaN check in gradcams #37 (@frgfm)\r\n\r\n## Test\r\n- Added NaN check unittest for gradcam #37 (@frgfm)\r\n- Switched from `unittest` to `pytest` #45 (@frgfm) and split test files by module\r\n\r\n## Documentation\r\n- Updated README badges #34, illustration #39 and usage instructions #41 (@frgfm)\r\n- Added instructions to run all CI checks locally in CONTRIBUTING #34, #45 (@frgfm)\r\n- Updated project hierarchy description in CONTRIBUTING #43 (@frgfm)\r\n- Added minimal code snippet in documentation #41 (@frgfm)\r\n\r\n## Others\r\n- Updated version in setup #34 and requirements #61 (@frgfm)\r\n- Leveraged automatic layer resolution in example script #41 (@frgfm)\r\n- Updated CI job to run unittests #45 (@frgfm)",
        "dateCreated": "2021-04-09T23:53:39Z",
        "datePublished": "2021-04-10T00:05:43Z",
        "html_url": "https://github.com/frgfm/torch-cam/releases/tag/v0.2.0",
        "name": "Compatibility with 3D inputs and improved documentation",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/frgfm/torch-cam/tarball/v0.2.0",
        "url": "https://api.github.com/repos/frgfm/torch-cam/releases/41212205",
        "zipball_url": "https://api.github.com/repos/frgfm/torch-cam/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "frgfm",
        "body": "This release adds an implementation of IS-CAM and greatly improves interface.\r\n\r\n**Note**: torchcam 0.1.2 requires PyTorch 1.1 or newer.\r\n\r\n# Highlights\r\n\r\n## CAMs\r\nImplementation of CAM extractor\r\n**New**\r\n- Add an IS-CAM implementation #13 (@frgfm)\r\n- Added automatic target layer resolution #32 (@frgfm)\r\n\r\n**Improvements**\r\n- Added support for submodule hooking #21 (@pkmandke)\r\n\r\n**Fixes**\r\n- Fixed hooking mechanism edge case #23 (@frgfm)\r\n\r\n## Test\r\nVerifications of the package well-being before release\r\n**New**\r\n- Updated test for `torchcam.cams` #13, #30 (@frgfm)\r\n\r\n**Improvements**\r\n- Removed pretrained model loading in unittests #25 (@frgfm)\r\n- Switched all models to eval, removed gradient when not required, and changed to simpler models #33 (@frgfm)\r\n\r\n## Documentation\r\nOnline resources for potential users\r\n**New**\r\n- Added entry for IS-CAM #13, #30 (@frgfm)\r\n\r\n**Fixes**\r\n- Fixed examples in docstrings of gradient-based CAMs #28, #33 (@frgfm)\r\n\r\n## Others\r\nOther tools and implementations\r\n**New**\r\n- Added annotation typing to the codebase & mypy verification CI job #19 (@frgfm)\r\n- Added package publishing verification jobs #12 (@frgfm)\r\n\r\n**Improvements**\r\n- Improved example script #15 (@frgfm)\r\n- Optimized CI cache #20 (@frgfm)\r\n\r\n**Fixes**\r\n- Fixed coverage upload job #16 (@frgfm)\r\n- Fixed doc deployment job #24 (@frgfm)\r\n- Fixed conda recipe #29 (@frgfm)\r\n",
        "dateCreated": "2020-12-27T01:42:03Z",
        "datePublished": "2020-12-27T01:43:31Z",
        "html_url": "https://github.com/frgfm/torch-cam/releases/tag/v0.1.2",
        "name": "Automatic target layer resolution and support of IS-CAM",
        "tag_name": "v0.1.2",
        "tarball_url": "https://api.github.com/repos/frgfm/torch-cam/tarball/v0.1.2",
        "url": "https://api.github.com/repos/frgfm/torch-cam/releases/35742403",
        "zipball_url": "https://api.github.com/repos/frgfm/torch-cam/zipball/v0.1.2"
      },
      {
        "authorType": "User",
        "author_name": "frgfm",
        "body": "This release adds implementations of SmoothGradCAM++, Score-CAM and SS-CAM.\r\n\r\n**Note**: torchcam 0.1.1 requires PyTorch 1.1 or newer.\r\n\r\n_brought to you by @frgfm_\r\n\r\n# Highlights\r\n\r\n## CAMs\r\nImplementation of CAM extractor\r\n**New**\r\n- Add a SmoothGradCAM++ implementation (#4)\r\n- Add a Score-CAM implementation (#5)\r\n- Add a SS-CAM  implementation (#11).\r\n\r\n**Improvements**\r\n- Refactor CAM extractor for better code reusability (#6)\r\n\r\n## Test\r\nVerifications of the package well-being before release\r\n**New**\r\n- Updated test for `torchcam.cams` (#4, #5, #11)\r\n\r\n## Documentation\r\nOnline resources for potential users\r\n**Improvements**\r\n- Add detailed explanation of CAM computation (#8, #11)\r\n- Add websearch referencing of documentation (#7)\r\n\r\n## Others\r\nOther tools and implementations\r\n- Fixed conda upload job (#3)\r\n",
        "dateCreated": "2020-08-03T21:31:32Z",
        "datePublished": "2020-08-03T21:47:49Z",
        "html_url": "https://github.com/frgfm/torch-cam/releases/tag/v0.1.1",
        "name": "Support of SmoothGradCAM++, Score-CAM and SS-CAM",
        "tag_name": "v0.1.1",
        "tarball_url": "https://api.github.com/repos/frgfm/torch-cam/tarball/v0.1.1",
        "url": "https://api.github.com/repos/frgfm/torch-cam/releases/29248114",
        "zipball_url": "https://api.github.com/repos/frgfm/torch-cam/zipball/v0.1.1"
      },
      {
        "authorType": "User",
        "author_name": "frgfm",
        "body": "This release adds implementations of CAM, GradCAM and GradCAM++.\r\n\r\n**Note**: torchcam 0.1.0 requires PyTorch 1.1 or newer.\r\n\r\n_brought to you by @frgfm_\r\n\r\n# Highlights\r\n\r\n## GradCAM\r\nImplementation of gradient-based CAM extractor\r\n**New**\r\n- Add a CAM implementation (#2)\r\n- Add Grad-CAM and Grad-CAM++ implementations (#1, #2).\r\n\r\n## Test\r\nVerifications of the package well-being before release\r\n**New**\r\n- Add test for `torchcam.cams` (#1, #2)\r\n- Add test for `torschscan.utils` (#1)\r\n\r\n## Documentation\r\nOnline resources for potential users\r\n**New**\r\n- Add sphinx automatic documentation build for existing features (#1, #2)\r\n- Add contribution guidelines (#1)\r\n- Add installation, usage, and benchmark in readme (#1, #2)\r\n\r\n## Others\r\nOther tools and implementations\r\n- Add  \u0300overlay_mask` to easily overlay mask on images (#1).\r\n",
        "dateCreated": "2020-03-24T01:25:15Z",
        "datePublished": "2020-03-24T01:31:36Z",
        "html_url": "https://github.com/frgfm/torch-cam/releases/tag/v0.1.0",
        "name": "Class activation maps for CNN in PyTorch",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/frgfm/torch-cam/tarball/v0.1.0",
        "url": "https://api.github.com/repos/frgfm/torch-cam/releases/24793812",
        "zipball_url": "https://api.github.com/repos/frgfm/torch-cam/zipball/v0.1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 588,
      "date": "Mon, 27 Dec 2021 06:05:38 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "python",
      "deep-learning",
      "cnn",
      "activation-maps",
      "gradcam-plus-plus",
      "gradcam",
      "saliency-map",
      "interpretability",
      "interpretable-deep-learning",
      "smoothgrad",
      "score-cam",
      "class-activation-map",
      "grad-cam"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "A minimal demo app is provided for you to play with the supported CAM methods! Feel free to check out the live demo on [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/frgfm/torch-cam)\n\nIf you prefer running the demo by yourself, you will need an extra dependency ([Streamlit](https://streamlit.io/)) for the app to run:\n\n```\npip install -r demo/requirements.txt\n```\n\nYou can then easily run your app in your default browser by running:\n\n```\nstreamlit run demo/app.py\n```\n\n![torchcam_demo](https://github.com/frgfm/torch-cam/releases/download/v0.2.0/torchcam_demo.png)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "An example script is provided for you to benchmark the heatmaps produced by multiple CAM approaches on the same image:\n\n```shell\npython scripts/cam_example.py --arch resnet18 --class-idx 232 --rows 2\n```\n\n![gradcam_sample](https://github.com/frgfm/torch-cam/releases/download/v0.2.0/cam_example_2rows.png)\n\n*All script arguments can be checked using `python scripts/cam_example.py --help`*\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Looking for more illustrations of TorchCAM features?\nYou might want to check the [Jupyter notebooks](notebooks) designed to give you a broader overview.\n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}