{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2110.00476",
      "https://arxiv.org/abs/2103.12731",
      "https://arxiv.org/abs/2101.11605",
      "https://arxiv.org/abs/2102.08602",
      "https://arxiv.org/abs/2103.06877",
      "https://arxiv.org/abs/2103.14899",
      "https://arxiv.org/abs/2106.08254",
      "https://arxiv.org/abs/2106.01548",
      "https://arxiv.org/abs/2105.08050",
      "https://arxiv.org/abs/2106.10270",
      "https://arxiv.org/abs/2106.05237",
      "https://arxiv.org/abs/2102.08602\n  * Bottleneck Transformers - https://arxiv.org/abs/2101.11605\n  * Halo Nets - https://arxiv.org/abs/2103.12731\n* Adabelief optimizer contributed by Juntang Zhuang\n\n### April 1, 2021\n* Add snazzy `benchmark.py` script for bulk `timm` model benchmarking of train and/or inference\n* Add Pooling-based Vision Transformer (PiT",
      "https://arxiv.org/abs/2101.11605\n  * Halo Nets - https://arxiv.org/abs/2103.12731\n* Adabelief optimizer contributed by Juntang Zhuang\n\n### April 1, 2021\n* Add snazzy `benchmark.py` script for bulk `timm` model benchmarking of train and/or inference\n* Add Pooling-based Vision Transformer (PiT",
      "https://arxiv.org/abs/2103.12731\n* Adabelief optimizer contributed by Juntang Zhuang\n\n### April 1, 2021\n* Add snazzy `benchmark.py` script for bulk `timm` model benchmarking of train and/or inference\n* Add Pooling-based Vision Transformer (PiT",
      "https://arxiv.org/abs/2102.06171. Integrated w/ PyTorch gradient clipping via mode arg that defaults to prev 'norm' mode. For backward arg compat, clip-grad arg must be specified to enable when using train.py.\n  * AGC w/ default clipping factor `--clip-grad .01 --clip-mode agc`\n  * PyTorch global norm of 1.0 (old behaviour, always norm",
      "https://arxiv.org/abs/2102.06171",
      "https://arxiv.org/abs/2101.08692",
      "https://arxiv.org/abs/2105.12723\n* Big Transfer ResNetV2 (BiT",
      "https://arxiv.org/abs/1912.11370\n* Bottleneck Transformers - https://arxiv.org/abs/2101.11605\n* CaiT (Class-Attention in Image Transformers",
      "https://arxiv.org/abs/2101.11605\n* CaiT (Class-Attention in Image Transformers",
      "https://arxiv.org/abs/2103.17239\n* CoaT (Co-Scale Conv-Attentional Image Transformers",
      "https://arxiv.org/abs/2104.06399\n* ConViT (Soft Convolutional Inductive Biases Vision Transformers",
      "https://arxiv.org/abs/2103.10697\n* CspNet (Cross-Stage Partial Networks",
      "https://arxiv.org/abs/1911.11929\n* DeiT (Vision Transformer",
      "https://arxiv.org/abs/2012.12877\n* DenseNet - https://arxiv.org/abs/1608.06993\n* DLA - https://arxiv.org/abs/1707.06484\n* DPN (Dual-Path Network",
      "https://arxiv.org/abs/1608.06993\n* DLA - https://arxiv.org/abs/1707.06484\n* DPN (Dual-Path Network",
      "https://arxiv.org/abs/1707.06484\n* DPN (Dual-Path Network",
      "https://arxiv.org/abs/1707.01629\n* EfficientNet (MBConvNet Family",
      "https://arxiv.org/abs/1911.04252\n    * EfficientNet AdvProp (B0-B8",
      "https://arxiv.org/abs/1911.09665\n    * EfficientNet (B0-B7",
      "https://arxiv.org/abs/1905.11946\n    * EfficientNet-EdgeTPU (S, M, L",
      "https://arxiv.org/abs/2104.00298\n    * FBNet-C - https://arxiv.org/abs/1812.03443\n    * MixNet - https://arxiv.org/abs/1907.09595\n    * MNASNet B1, A1 (Squeeze-Excite",
      "https://arxiv.org/abs/1812.03443\n    * MixNet - https://arxiv.org/abs/1907.09595\n    * MNASNet B1, A1 (Squeeze-Excite",
      "https://arxiv.org/abs/1907.09595\n    * MNASNet B1, A1 (Squeeze-Excite",
      "https://arxiv.org/abs/1807.11626\n    * MobileNet-V2 - https://arxiv.org/abs/1801.04381\n    * Single-Path NAS - https://arxiv.org/abs/1904.02877\n* GhostNet - https://arxiv.org/abs/1911.11907\n* gMLP - https://arxiv.org/abs/2105.08050\n* GPU-Efficient Networks - https://arxiv.org/abs/2006.14090\n* Halo Nets - https://arxiv.org/abs/2103.12731\n* HardCoRe-NAS - https://arxiv.org/abs/2102.11646\n* HRNet - https://arxiv.org/abs/1908.07919\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing",
      "https://arxiv.org/abs/1801.04381\n    * Single-Path NAS - https://arxiv.org/abs/1904.02877\n* GhostNet - https://arxiv.org/abs/1911.11907\n* gMLP - https://arxiv.org/abs/2105.08050\n* GPU-Efficient Networks - https://arxiv.org/abs/2006.14090\n* Halo Nets - https://arxiv.org/abs/2103.12731\n* HardCoRe-NAS - https://arxiv.org/abs/2102.11646\n* HRNet - https://arxiv.org/abs/1908.07919\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing",
      "https://arxiv.org/abs/1904.02877\n* GhostNet - https://arxiv.org/abs/1911.11907\n* gMLP - https://arxiv.org/abs/2105.08050\n* GPU-Efficient Networks - https://arxiv.org/abs/2006.14090\n* Halo Nets - https://arxiv.org/abs/2103.12731\n* HardCoRe-NAS - https://arxiv.org/abs/2102.11646\n* HRNet - https://arxiv.org/abs/1908.07919\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing",
      "https://arxiv.org/abs/1911.11907\n* gMLP - https://arxiv.org/abs/2105.08050\n* GPU-Efficient Networks - https://arxiv.org/abs/2006.14090\n* Halo Nets - https://arxiv.org/abs/2103.12731\n* HardCoRe-NAS - https://arxiv.org/abs/2102.11646\n* HRNet - https://arxiv.org/abs/1908.07919\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing",
      "https://arxiv.org/abs/2105.08050\n* GPU-Efficient Networks - https://arxiv.org/abs/2006.14090\n* Halo Nets - https://arxiv.org/abs/2103.12731\n* HardCoRe-NAS - https://arxiv.org/abs/2102.11646\n* HRNet - https://arxiv.org/abs/1908.07919\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing",
      "https://arxiv.org/abs/2006.14090\n* Halo Nets - https://arxiv.org/abs/2103.12731\n* HardCoRe-NAS - https://arxiv.org/abs/2102.11646\n* HRNet - https://arxiv.org/abs/1908.07919\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing",
      "https://arxiv.org/abs/2103.12731\n* HardCoRe-NAS - https://arxiv.org/abs/2102.11646\n* HRNet - https://arxiv.org/abs/1908.07919\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing",
      "https://arxiv.org/abs/2102.11646\n* HRNet - https://arxiv.org/abs/1908.07919\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing",
      "https://arxiv.org/abs/1908.07919\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing",
      "https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing",
      "https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing",
      "https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing",
      "https://arxiv.org/abs/2104.01136\n* MLP-Mixer - https://arxiv.org/abs/2105.01601\n* MobileNet-V3 (MBConvNet w/ Efficient Head",
      "https://arxiv.org/abs/2105.01601\n* MobileNet-V3 (MBConvNet w/ Efficient Head",
      "https://arxiv.org/abs/1905.02244\n* NASNet-A - https://arxiv.org/abs/1707.07012\n* NFNet-F - https://arxiv.org/abs/2102.06171\n* NF-RegNet / NF-ResNet - https://arxiv.org/abs/2101.08692\n* PNasNet - https://arxiv.org/abs/1712.00559\n* Pooling-based Vision Transformer (PiT",
      "https://arxiv.org/abs/1707.07012\n* NFNet-F - https://arxiv.org/abs/2102.06171\n* NF-RegNet / NF-ResNet - https://arxiv.org/abs/2101.08692\n* PNasNet - https://arxiv.org/abs/1712.00559\n* Pooling-based Vision Transformer (PiT",
      "https://arxiv.org/abs/2102.06171\n* NF-RegNet / NF-ResNet - https://arxiv.org/abs/2101.08692\n* PNasNet - https://arxiv.org/abs/1712.00559\n* Pooling-based Vision Transformer (PiT",
      "https://arxiv.org/abs/2101.08692\n* PNasNet - https://arxiv.org/abs/1712.00559\n* Pooling-based Vision Transformer (PiT",
      "https://arxiv.org/abs/1712.00559\n* Pooling-based Vision Transformer (PiT",
      "https://arxiv.org/abs/2103.16302\n* RegNet - https://arxiv.org/abs/2003.13678\n* RepVGG - https://arxiv.org/abs/2101.03697\n* ResMLP - https://arxiv.org/abs/2105.03404\n* ResNet/ResNeXt\n    * ResNet (v1b/v1.5",
      "https://arxiv.org/abs/2003.13678\n* RepVGG - https://arxiv.org/abs/2101.03697\n* ResMLP - https://arxiv.org/abs/2105.03404\n* ResNet/ResNeXt\n    * ResNet (v1b/v1.5",
      "https://arxiv.org/abs/2101.03697\n* ResMLP - https://arxiv.org/abs/2105.03404\n* ResNet/ResNeXt\n    * ResNet (v1b/v1.5",
      "https://arxiv.org/abs/2105.03404\n* ResNet/ResNeXt\n    * ResNet (v1b/v1.5",
      "https://arxiv.org/abs/1512.03385\n    * ResNeXt - https://arxiv.org/abs/1611.05431\n    * 'Bag of Tricks' / Gluon C, D, E, S variations - https://arxiv.org/abs/1812.01187\n    * Weakly-supervised (WSL",
      "https://arxiv.org/abs/1611.05431\n    * 'Bag of Tricks' / Gluon C, D, E, S variations - https://arxiv.org/abs/1812.01187\n    * Weakly-supervised (WSL",
      "https://arxiv.org/abs/1812.01187\n    * Weakly-supervised (WSL",
      "https://arxiv.org/abs/1805.00932\n    * Semi-supervised (SSL",
      "https://arxiv.org/abs/1905.00546\n    * ECA-Net (ECAResNet",
      "https://arxiv.org/abs/1910.03151v4\n    * Squeeze-and-Excitation Networks (SEResNet",
      "https://arxiv.org/abs/1709.01507\n    * ResNet-RS - https://arxiv.org/abs/2103.07579\n* Res2Net - https://arxiv.org/abs/1904.01169\n* ResNeSt - https://arxiv.org/abs/2004.08955\n* ReXNet - https://arxiv.org/abs/2007.00992\n* SelecSLS - https://arxiv.org/abs/1907.00837\n* Selective Kernel Networks - https://arxiv.org/abs/1903.06586\n* Swin Transformer - https://arxiv.org/abs/2103.14030\n* Transformer-iN-Transformer (TNT",
      "https://arxiv.org/abs/2103.07579\n* Res2Net - https://arxiv.org/abs/1904.01169\n* ResNeSt - https://arxiv.org/abs/2004.08955\n* ReXNet - https://arxiv.org/abs/2007.00992\n* SelecSLS - https://arxiv.org/abs/1907.00837\n* Selective Kernel Networks - https://arxiv.org/abs/1903.06586\n* Swin Transformer - https://arxiv.org/abs/2103.14030\n* Transformer-iN-Transformer (TNT",
      "https://arxiv.org/abs/1904.01169\n* ResNeSt - https://arxiv.org/abs/2004.08955\n* ReXNet - https://arxiv.org/abs/2007.00992\n* SelecSLS - https://arxiv.org/abs/1907.00837\n* Selective Kernel Networks - https://arxiv.org/abs/1903.06586\n* Swin Transformer - https://arxiv.org/abs/2103.14030\n* Transformer-iN-Transformer (TNT",
      "https://arxiv.org/abs/2004.08955\n* ReXNet - https://arxiv.org/abs/2007.00992\n* SelecSLS - https://arxiv.org/abs/1907.00837\n* Selective Kernel Networks - https://arxiv.org/abs/1903.06586\n* Swin Transformer - https://arxiv.org/abs/2103.14030\n* Transformer-iN-Transformer (TNT",
      "https://arxiv.org/abs/2007.00992\n* SelecSLS - https://arxiv.org/abs/1907.00837\n* Selective Kernel Networks - https://arxiv.org/abs/1903.06586\n* Swin Transformer - https://arxiv.org/abs/2103.14030\n* Transformer-iN-Transformer (TNT",
      "https://arxiv.org/abs/1907.00837\n* Selective Kernel Networks - https://arxiv.org/abs/1903.06586\n* Swin Transformer - https://arxiv.org/abs/2103.14030\n* Transformer-iN-Transformer (TNT",
      "https://arxiv.org/abs/1903.06586\n* Swin Transformer - https://arxiv.org/abs/2103.14030\n* Transformer-iN-Transformer (TNT",
      "https://arxiv.org/abs/2103.14030\n* Transformer-iN-Transformer (TNT",
      "https://arxiv.org/abs/2103.00112\n* TResNet - https://arxiv.org/abs/2003.13630\n* Twins (Spatial Attention in Vision Transformers",
      "https://arxiv.org/abs/2003.13630\n* Twins (Spatial Attention in Vision Transformers",
      "https://arxiv.org/abs/2010.11929\n* VovNet V2 and V1 - https://arxiv.org/abs/1911.06667\n* Xception - https://arxiv.org/abs/1610.02357\n* Xception (Modified Aligned, Gluon",
      "https://arxiv.org/abs/1911.06667\n* Xception - https://arxiv.org/abs/1610.02357\n* Xception (Modified Aligned, Gluon",
      "https://arxiv.org/abs/1610.02357\n* Xception (Modified Aligned, Gluon",
      "https://arxiv.org/abs/1802.02611\n* Xception (Modified Aligned, TF",
      "https://arxiv.org/abs/1802.02611\n* XCiT (Cross-Covariance Image Transformers",
      "https://arxiv.org/abs/2106.09681\n\n## Features\n\nSeveral (less common",
      "https://arxiv.org/abs/1608.03983",
      "https://arxiv.org/abs/1908.03265",
      "https://arxiv.org/abs/1905.11286",
      "https://arxiv.org/abs/1907.08610",
      "https://arxiv.org/abs/2006.08217",
      "https://arxiv.org/abs/1804.04235",
      "https://arxiv.org/abs/2006.00719",
      "https://arxiv.org/abs/1708.04896",
      "https://arxiv.org/abs/1710.09412",
      "https://arxiv.org/abs/1905.04899",
      "https://arxiv.org/abs/1805.09501",
      "https://arxiv.org/abs/1909.13719",
      "https://arxiv.org/abs/1912.02781",
      "https://arxiv.org/abs/1603.09382",
      "https://arxiv.org/abs/1810.12890",
      "https://arxiv.org/abs/1904.11486",
      "https://arxiv.org/abs/1801.04590",
      "https://arxiv.org/abs/2102.06171, https://github.com/deepmind/deepmind-research/tree/master/nfnets",
      "https://arxiv.org/abs/2101.11605\n    * CBAM - https://arxiv.org/abs/1807.06521\n    * Effective Squeeze-Excitation (ESE",
      "https://arxiv.org/abs/1807.06521\n    * Effective Squeeze-Excitation (ESE",
      "https://arxiv.org/abs/1911.06667\n    * Efficient Channel Attention (ECA",
      "https://arxiv.org/abs/1910.03151\n    * Gather-Excite (GE",
      "https://arxiv.org/abs/1810.12348\n    * Global Context (GC",
      "https://arxiv.org/abs/1904.11492\n    * Halo - https://arxiv.org/abs/2103.12731\n    * Involution - https://arxiv.org/abs/2103.06255\n    * Lambda Layer - https://arxiv.org/abs/2102.08602\n    * Non-Local (NL",
      "https://arxiv.org/abs/2103.12731\n    * Involution - https://arxiv.org/abs/2103.06255\n    * Lambda Layer - https://arxiv.org/abs/2102.08602\n    * Non-Local (NL",
      "https://arxiv.org/abs/2103.06255\n    * Lambda Layer - https://arxiv.org/abs/2102.08602\n    * Non-Local (NL",
      "https://arxiv.org/abs/2102.08602\n    * Non-Local (NL",
      "https://arxiv.org/abs/1711.07971\n    * Squeeze-and-Excitation (SE",
      "https://arxiv.org/abs/1709.01507\n    * Selective Kernel (SK",
      "https://arxiv.org/abs/1903.06586\n    * Split (SPLAT",
      "https://arxiv.org/abs/2004.08955\n    * Shifted Window (SWIN",
      "https://arxiv.org/abs/2103.14030\n\n## Results\n\nModel validation results can be found in the [documentation](https://rwightman.github.io/pytorch-image-models/results/"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/rwightman/pytorch-image-models}}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9944484218006108,
        0.9912556242864674,
        0.9944484218006108
      ],
      "excerpt": "Halo (https://arxiv.org/abs/2103.12731) \nBottleneck Transformer (https://arxiv.org/abs/2101.11605) \nLambdaNetworks (https://arxiv.org/abs/2102.08602) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999958691161325
      ],
      "excerpt": "ConvMixer (https://openreview.net/forum?id=TVHS5Y4dNvM), CrossVit (https://arxiv.org/abs/2103.14899), and BeiT (https://arxiv.org/abs/2106.08254) architectures + weights added \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8304301940462457
      ],
      "excerpt": "Add weights from official ResMLP release (https://github.com/facebookresearch/deit) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9964838015383283,
        0.9912556242864674,
        0.9985319060770389
      ],
      "excerpt": "Lambda Networks - https://arxiv.org/abs/2102.08602 \nBottleneck Transformers - https://arxiv.org/abs/2101.11605 \nHalo Nets - https://arxiv.org/abs/2103.12731 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9944484218006108,
        0.9944484218006108,
        0.9912556242864674,
        0.9892001370565694,
        0.9868686893653784,
        0.998158442971818,
        0.9944550550092732,
        0.9996926020863115,
        0.9944484218006108,
        0.9944484218006108,
        0.9738201839191447
      ],
      "excerpt": "Aggregating Nested Transformers - https://arxiv.org/abs/2105.12723 \nBig Transfer ResNetV2 (BiT) - https://arxiv.org/abs/1912.11370 \nBottleneck Transformers - https://arxiv.org/abs/2101.11605 \nCaiT (Class-Attention in Image Transformers) - https://arxiv.org/abs/2103.17239 \nCoaT (Co-Scale Conv-Attentional Image Transformers) - https://arxiv.org/abs/2104.06399 \nConViT (Soft Convolutional Inductive Biases Vision Transformers)- https://arxiv.org/abs/2103.10697 \nCspNet (Cross-Stage Partial Networks) - https://arxiv.org/abs/1911.11929 \nDeiT (Vision Transformer) - https://arxiv.org/abs/2012.12877 \nDenseNet - https://arxiv.org/abs/1608.06993 \nDLA - https://arxiv.org/abs/1707.06484 \nDPN (Dual-Path Network) - https://arxiv.org/abs/1707.01629 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9944484218006108,
        0.9944484218006108,
        0.9944484218006108,
        0.9665437545861224,
        0.9944484218006108,
        0.9944484218006108,
        0.9944484218006108,
        0.9875631528765129,
        0.9944484218006108,
        0.9845600173250374,
        0.9944484218006108,
        0.9944484218006108,
        0.9956138652598038,
        0.9985319060770389,
        0.9944484218006108,
        0.9944484218006108,
        0.9912556242864674,
        0.9741524126558423,
        0.9964838015383283,
        0.9984884792368299,
        0.9944484218006108,
        0.9977994744046882,
        0.9944484218006108,
        0.9944484218006108,
        0.9826628511165726,
        0.9944484218006108,
        0.9987715412222475,
        0.9944484218006108,
        0.9944484218006108,
        0.9944484218006108
      ],
      "excerpt": "EfficientNet NoisyStudent (B0-B7, L2) - https://arxiv.org/abs/1911.04252 \nEfficientNet AdvProp (B0-B8) - https://arxiv.org/abs/1911.09665 \nEfficientNet (B0-B7) - https://arxiv.org/abs/1905.11946 \nEfficientNet-EdgeTPU (S, M, L) - https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html \nEfficientNet V2 - https://arxiv.org/abs/2104.00298 \nFBNet-C - https://arxiv.org/abs/1812.03443 \nMixNet - https://arxiv.org/abs/1907.09595 \nMNASNet B1, A1 (Squeeze-Excite), and Small - https://arxiv.org/abs/1807.11626 \nMobileNet-V2 - https://arxiv.org/abs/1801.04381 \nSingle-Path NAS - https://arxiv.org/abs/1904.02877 \nGhostNet - https://arxiv.org/abs/1911.11907 \ngMLP - https://arxiv.org/abs/2105.08050 \nGPU-Efficient Networks - https://arxiv.org/abs/2006.14090 \nHalo Nets - https://arxiv.org/abs/2103.12731 \nHardCoRe-NAS - https://arxiv.org/abs/2102.11646 \nHRNet - https://arxiv.org/abs/1908.07919 \nInception-V3 - https://arxiv.org/abs/1512.00567 \nInception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261 \nLambda Networks - https://arxiv.org/abs/2102.08602 \nLeViT (Vision Transformer in ConvNet's Clothing) - https://arxiv.org/abs/2104.01136 \nMLP-Mixer - https://arxiv.org/abs/2105.01601 \nMobileNet-V3 (MBConvNet w/ Efficient Head) - https://arxiv.org/abs/1905.02244 \nNASNet-A - https://arxiv.org/abs/1707.07012 \nNFNet-F - https://arxiv.org/abs/2102.06171 \nNF-RegNet / NF-ResNet - https://arxiv.org/abs/2101.08692 \nPNasNet - https://arxiv.org/abs/1712.00559 \nPooling-based Vision Transformer (PiT) - https://arxiv.org/abs/2103.16302 \nRegNet - https://arxiv.org/abs/2003.13678 \nRepVGG - https://arxiv.org/abs/2101.03697 \nResMLP - https://arxiv.org/abs/2105.03404 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9826628511165726,
        0.9982820744442126,
        0.9896978521113555,
        0.9930784137153941,
        0.9826628511165726,
        0.9985319060770389,
        0.9970238749064742,
        0.9826628511165726,
        0.9944484218006108,
        0.9944484218006108,
        0.9976821089163788,
        0.9944484218006108,
        0.9964838015383283,
        0.9944484218006108,
        0.9892593653091472,
        0.9944484218006108,
        0.9982091351661889,
        0.9996926020863115,
        0.9966489686656222,
        0.9944484218006108,
        0.9944484218006108,
        0.9826628511165726,
        0.994417646656845
      ],
      "excerpt": "ResNet (v1b/v1.5) - https://arxiv.org/abs/1512.03385 \nResNeXt - https://arxiv.org/abs/1611.05431 \n'Bag of Tricks' / Gluon C, D, E, S variations - https://arxiv.org/abs/1812.01187 \nWeakly-supervised (WSL) Instagram pretrained / ImageNet tuned ResNeXt101 - https://arxiv.org/abs/1805.00932 \nSemi-supervised (SSL) / Semi-weakly Supervised (SWSL) ResNet/ResNeXts - https://arxiv.org/abs/1905.00546 \nECA-Net (ECAResNet) - https://arxiv.org/abs/1910.03151v4 \nSqueeze-and-Excitation Networks (SEResNet) - https://arxiv.org/abs/1709.01507 \nResNet-RS - https://arxiv.org/abs/2103.07579 \nRes2Net - https://arxiv.org/abs/1904.01169 \nResNeSt - https://arxiv.org/abs/2004.08955 \nReXNet - https://arxiv.org/abs/2007.00992 \nSelecSLS - https://arxiv.org/abs/1907.00837 \nSelective Kernel Networks - https://arxiv.org/abs/1903.06586 \nSwin Transformer - https://arxiv.org/abs/2103.14030 \nTransformer-iN-Transformer (TNT) - https://arxiv.org/abs/2103.00112 \nTResNet - https://arxiv.org/abs/2003.13630 \nTwins (Spatial Attention in Vision Transformers) - https://arxiv.org/pdf/2104.13840.pdf \nVision Transformer - https://arxiv.org/abs/2010.11929 \nVovNet V2 and V1 - https://arxiv.org/abs/1911.06667 \nXception - https://arxiv.org/abs/1610.02357 \nXception (Modified Aligned, Gluon) - https://arxiv.org/abs/1802.02611 \nXception (Modified Aligned, TF) - https://arxiv.org/abs/1802.02611 \nXCiT (Cross-Covariance Image Transformers) - https://arxiv.org/abs/2106.09681 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9811934373974328
      ],
      "excerpt": "SGDR: Stochastic Gradient Descent with Warm Restarts (https://arxiv.org/abs/1608.03983) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9976821089163752,
        0.9855105081843352,
        0.9663244222454193
      ],
      "excerpt": "radam by Liyuan Liu (https://arxiv.org/abs/1908.03265) \nnovograd by Masashi Kimura (https://arxiv.org/abs/1905.11286) \nlookahead adapted from impl by Liam (https://arxiv.org/abs/1907.08610) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9912226146645449,
        0.9869405348645832,
        0.9885170098636817,
        0.987115451605939,
        0.9944484218006108,
        0.9944484218006108,
        0.9993559408204563
      ],
      "excerpt": "adamp and sgdp by Naver ClovAI (https://arxiv.org/abs/2006.08217) \nadafactor adapted from FAIRSeq impl (https://arxiv.org/abs/1804.04235) \nadahessian by David Samuel (https://arxiv.org/abs/2006.00719) \nRandom Erasing from Zhun Zhong  (https://arxiv.org/abs/1708.04896) \nMixup (https://arxiv.org/abs/1710.09412) \nCutMix (https://arxiv.org/abs/1905.04899) \nAutoAugment (https://arxiv.org/abs/1805.09501) and RandAugment (https://arxiv.org/abs/1909.13719) ImageNet configurations modeled after impl for EfficientNet training (https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9970681164345253,
        0.9977994744046882,
        0.9944484218006108
      ],
      "excerpt": "DropPath aka \"Stochastic Depth\" (https://arxiv.org/abs/1603.09382)  \nDropBlock (https://arxiv.org/abs/1810.12890) \nBlur Pooling (https://arxiv.org/abs/1904.11486) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9993613373615873
      ],
      "excerpt": "Adaptive Gradient Clipping (https://arxiv.org/abs/2102.06171, https://github.com/deepmind/deepmind-research/tree/master/nfnets) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9912556242864674,
        0.9944484218006108,
        0.9877612921698276,
        0.9977994744046882,
        0.9977994744046882,
        0.990792461877055,
        0.9944484218006108,
        0.9944484218006108,
        0.9742177718389462,
        0.9598596565459202,
        0.9952996463412694,
        0.9944484218006108,
        0.9944484218006108,
        0.9944484218006108
      ],
      "excerpt": "Bottleneck Transformer - https://arxiv.org/abs/2101.11605 \nCBAM - https://arxiv.org/abs/1807.06521 \nEffective Squeeze-Excitation (ESE) - https://arxiv.org/abs/1911.06667 \nEfficient Channel Attention (ECA) - https://arxiv.org/abs/1910.03151 \nGather-Excite (GE) - https://arxiv.org/abs/1810.12348 \nGlobal Context (GC) - https://arxiv.org/abs/1904.11492 \nHalo - https://arxiv.org/abs/2103.12731 \nInvolution - https://arxiv.org/abs/2103.06255 \nLambda Layer - https://arxiv.org/abs/2102.08602 \nNon-Local (NL) -  https://arxiv.org/abs/1711.07971 \nSqueeze-and-Excitation (SE) - https://arxiv.org/abs/1709.01507 \nSelective Kernel (SK) - (https://arxiv.org/abs/1903.06586 \nSplit (SPLAT) - https://arxiv.org/abs/2004.08955 \nShifted Window (SWIN) - https://arxiv.org/abs/2103.14030 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479,
        0.9105368110547479,
        0.9105368110547479,
        0.9105368110547479
      ],
      "excerpt": "Albumentations - https://github.com/albumentations-team/albumentations \nKornia - https://github.com/kornia/kornia \nRepDistiller - https://github.com/HobbitLong/RepDistiller \ntorchdistill - https://github.com/yoshitomo-matsubara/torchdistill \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "fastai - https://github.com/fastai/fastai \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rwightman/pytorch-image-models",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-02T05:51:12Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T12:49:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Py**T**orch **Im**age **M**odels (`timm`) is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders / augmentations, and reference training / validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.\n\nThe work of many others is present here. I've tried to make sure all source material is acknowledged via links to github, arxiv papers, etc in the README, documentation, and code docstrings. Please let me know if I missed anything.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9288869272275798
      ],
      "excerpt": "In addition to the sponsors at the link above, I've received hardware and/or cloud resources from \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9883475748871402,
        0.87041319644083
      ],
      "excerpt": "I'm fortunate to be able to dedicate significant time and money of my own supporting this and other open source projects. However, as the projects increase in scope, outside support is needed to continue with the current trajectory of cloud services, hardware, and electricity costs. \nA number of updated weights anew new model defs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8781612689159285,
        0.954067852117597
      ],
      "excerpt": "Groundwork in for FX feature extraction thanks to Alexander Soare \nmodels updated for tracing compatibility (almost full support with some distlled transformer exceptions) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.829255569602086,
        0.8994597265746361,
        0.9114033462405826
      ],
      "excerpt": "BCE loss and Repeated Augmentation support for RSB paper \n4 series of ResNet based attention model experiments being added (implemented across byobnet.py/byoanet.py). These include all sorts of attention, from channel attn like SE, ECA to 2D QKV self-attention layers such as Halo, Bottlneck, Lambda. Details here (https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights) \nWorking implementations of the following 2D self-attention modules (likely to be differences from paper or eventual official impl): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9538298374403898
      ],
      "excerpt": "A RegNetZ series of models with some attention experiments (being added to). These do not follow the paper (https://arxiv.org/abs/2103.06877) in any way other than block architecture, details of official models are not available. See more here (https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9061367057541451
      ],
      "excerpt": "Add LAMB and LARS optimizers, incl trust ratio clipping options. Tweaked to work properly in PyTorch XLA (tested on TPUs w/ timm bits branch) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9403566700418734,
        0.8942296510475087,
        0.9575299932161156,
        0.9410096049027175,
        0.8797456898131721,
        0.8819160107891769
      ],
      "excerpt": "Some cleanup on all optimizers and factory. No more .data, a bit more consistency, unit tests for all! \nSGDP and AdamP still won't work with PyTorch XLA but others should (have yet to test Adabelief, Adafactor, Adahessian myself). \nEfficientNet-V2 XL TF ported weights added, but they don't validate well in PyTorch (L is better). The pre-processing for the V2 TF training is a bit diff and the fine-tuned 21k -> 1k weights are very sensitive and less robust than the 1k weights. \nAdded PyTorch trained EfficientNet-V2 'Tiny' w/ GlobalContext attn weights. Only .1-.2 top-1 better than the SE so more of a curiosity for those interested. \nAdd XCiT models from official facebook impl. Contributed by Alexander Soare \nAdd efficientnetv2_rw_t weights, a custom 'tiny' 13.6M param variant that is a bit better than (non NoisyStudent) B3 models. Both faster and better accuracy (at same or lower res) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8540523607701941
      ],
      "excerpt": "See example notebook from official impl for navigating the augreg weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8763842220108049,
        0.8104882441398186
      ],
      "excerpt": "Remove my old small model, replace with DeiT compatible small w/ AugReg weights \nAdd 1st training of my gmixer_24_224 MLP /w GLU, 78.1 top-1 w/ 25M params. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8026214216836897
      ],
      "excerpt": "Add distilled BiT 50x1 student and 152x2 Teacher weights from  Knowledge distillation: A good teacher is patient and consistent \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.866715589134445
      ],
      "excerpt": "weight standardization uses F.batch_norm instead of std_mean (std_mean wasn't lowered) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184621603738546
      ],
      "excerpt": "Same param count (35.7M) and throughput as ResNetRS-50 but +1.5 top-1 @ 224x224 and +2.5 top-1 at 288x288 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.916934702284438,
        0.8881019048603969,
        0.8184861401782167
      ],
      "excerpt": "More flexible pos embedding resize (non-square) for ViT and TnT. Thanks Alexander Soare \nAdd efficientnetv2_rw_m model and weights (started training before official code). 84.8 top-1, 53M params. \nAdd EfficientNet-V2 official model defs w/ ported weights from official Tensorflow/Keras impl. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8487648623050642
      ],
      "excerpt": "Some blank efficientnetv2_* models in-place for future native PyTorch training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8055211712686954
      ],
      "excerpt": "Consistent '26t' model defs for experiments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8076639553457231,
        0.9362047518296541
      ],
      "excerpt": "Add EfficientNet-V2S model (unverified model definition) weights. 83.3 top-1 @ 288x288. Only trained single res 224. Working on progressive training. \nAdd ByoaNet model definition (Bring-your-own-attention) w/ SelfAttention block and corresponding SA/SA-like modules and model defs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8133558597385848
      ],
      "excerpt": "Add snazzy benchmark.py script for bulk timm model benchmarking of train and/or inference \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8520556161891794
      ],
      "excerpt": "Merge distilled (DeiT) model into main so that torchscript can work \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9766391231732974,
        0.9111625987160337
      ],
      "excerpt": "Separate hybrid model defs into different file and add several new model defs to fiddle with, support patch_size != 1 for hybrids \nFix fine-tuning num_class changes (PiT and ViT) and pos_embed resizing (Vit) with distilled variants \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.963826556132956
      ],
      "excerpt": "Integrate Hugging Face model hub into timm create_model and default_cfg handling for pretrained weight and config sharing (more on this soon!) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8427869308514967
      ],
      "excerpt": "Change feature extraction for pre-activation nets (NFNets, ResNetV2) to return features before activation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8309720487890326,
        0.9006007431913993,
        0.8105867000098067,
        0.9439952452101822,
        0.9629889795756353,
        0.8004309707874495
      ],
      "excerpt": "Benchmarked several arch on RTX 3090, Titan RTX, and V100 across 1.7.1, 1.8, NGC 20.12, and 21.02. Some interesting performance variations to take note of https://gist.github.com/rwightman/bb59f9e245162cee0e38bd66bd8cd77f \nAdd pretrained weights and model variants for NFNet-F* models from DeepMind Haiku impl. \nModels are prefixed with dm_. They require SAME padding conv, skipinit enabled, and activation gains applied in act fn. \nThese models are big, expect to run out of GPU memory. With the GELU activiation + other options, they are roughly 1/2 the inference speed of my SiLU PyTorch optimized s variants. \nOriginal model results are based on pre-processing that is not the same as all other models so you'll see different results in the results csv (once updated). \nMatching the original pre-processing as closely as possible I get these results: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8204757124967236
      ],
      "excerpt": "PyTorch global norm of 1.0 (old behaviour, always norm), --clip-grad 1.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.98051508116669
      ],
      "excerpt": "AGC performance is definitely sensitive to the clipping factor. More experimentation needed to determine good values for smaller batch sizes and optimizers besides those in paper. So far I've found .001-.005 is necessary for stable RMSProp training w/ NFNet/NF-ResNet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9052934937393465
      ],
      "excerpt": "More model archs, incl a flexible ByobNet backbone ('Bring-your-own-blocks') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.804606118202606,
        0.9302196308375661
      ],
      "excerpt": "Refinements to normalizer layer arg handling and normalizer+act layer handling in some models \nDefault AMP mode changed to native PyTorch AMP instead of APEX. Issues not being fixed with APEX. Native works with --channels-last and --torchscript model training, APEX does not. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9391476614848162
      ],
      "excerpt": "Remove separate tiered (t) vs tiered_narrow (tn) ResNet model defs, all tn changed to t and t models removed (seresnext26t_32x4d only model w/ weights that was removed). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9364984386331858
      ],
      "excerpt": "Add initial \"Normalization Free\" NF-RegNet-B* and NF-ResNet model definitions based on paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9616618295995472,
        0.8825641120451675
      ],
      "excerpt": "All model architecture families include variants with pretrained weights. There are specific model variants without any weights, it is NOT a bug. Help training new or better weights is always appreciated. Here are some example training hparams to get you started. \nA full version of the list below with source links can be found in the documentation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9748042306189888,
        0.9164506828165733,
        0.9554025773396336,
        0.9009115378983789,
        0.9896666518147116
      ],
      "excerpt": "Several (less common) features that I often utilize in my projects are included. Many of their additions are the reason why I maintain my own set of models, instead of using others' via PIP: \nAll models have a common default configuration interface and API for \naccessing/changing the classifier - get_classifier and reset_classifier \ndoing a forward pass on just the features - forward_features (see documentation) \nthese makes it easy to write consistent network wrappers that work with any of the models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.958696973156532,
        0.8728686979773158
      ],
      "excerpt": "out_indices creation arg specifies which feature maps to return, these indices are 0 based and generally correspond to the C(i + 1) feature level. \noutput_stride creation arg controls output stride of the network by using dilated convolutions. Most networks are stride 32 by default. Not all networks support this. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8523827514739108
      ],
      "excerpt": "High performance reference training, validation, and inference scripts that work in several process/GPU modes: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8085504139224394,
        0.8425118943138757
      ],
      "excerpt": "A dynamic global pool implementation that allows selecting from average pooling, max pooling, average + max, or concat([average, max]) at model creation. All global pooling is adaptive average by default and compatible with pretrained weights. \nA 'Test Time Pool' wrapper that can wrap any of the included models and usually provides improved performance doing inference with input images larger than the training size. Idea adapted from original DPN implementation when I ported (https://github.com/cypw/DPNs) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9098018130619737
      ],
      "excerpt": "An extensive selection of channel and/or spatial attention modules: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8912505439722056,
        0.9942992013553477,
        0.9921201750331988
      ],
      "excerpt": "Model validation results can be found in the documentation and in the results tables \nThe root folder of the repository contains reference train, validation, and inference scripts that work with the included models and other features of this repository. They are adaptable for other datasets and use cases with a little hacking. See documentation for some basics and training hparams for some train examples that produce SOTA ImageNet results. \nOne of the greatest assets of PyTorch is the community and their contributions. A few of my favourite resources that pair well with the models and components here are listed below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch image models, scripts, pretrained weights -- ResNet, ResNeXT, EfficientNet, EfficientNetV2, NFNet, Vision Transformer, MixNet, MobileNet-V3/V2, RegNet, DPN, CSPNet, and more",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "My current [documentation](https://rwightman.github.io/pytorch-image-models/) for `timm` covers the basics.\n\n[timmdocs](https://fastai.github.io/timmdocs/) is quickly becoming a much more comprehensive set of documentation for `timm`. A big thanks to [Aman Arora](https://github.com/amaarora) for his efforts creating timmdocs.\n\n[paperswithcode](https://paperswithcode.com/lib/timm) is a good resource for browsing the models within `timm`.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rwightman/pytorch-image-models/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2468,
      "date": "Fri, 24 Dec 2021 18:01:05 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rwightman/pytorch-image-models/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rwightman/pytorch-image-models",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/rwightman/pytorch-image-models/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rwightman/pytorch-image-models/master/notebooks/GeneralizationToImageNetV2.ipynb",
      "https://raw.githubusercontent.com/rwightman/pytorch-image-models/master/notebooks/EffResNetComparison.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rwightman/pytorch-image-models/master/distributed_train.sh"
    ],
    "technique": "File Exploration"
  },
  "identifier": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://zenodo.org/badge/latestdoi/168799526",
      "technique": "Regular expression"
    }
  ],
  "installation": [
    {
      "confidence": [
        0.9322609392449874
      ],
      "excerpt": "Awesome PyTorch Resources \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9382251794165012
      ],
      "excerpt": "Update ByoaNet attention modules \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8435413265655823
      ],
      "excerpt": "Add Swin Transformer models and weights from https://github.com/microsoft/Swin-Transformer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9188169013903633
      ],
      "excerpt": "First 0.4.x PyPi release w/ NFNets (& related), ByoB (GPU-Efficient, RepVGG, etc). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9458574545693577
      ],
      "excerpt": "Tested with PyTorch 1.8 release. Updated CI to use 1.8. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8532505209007303
      ],
      "excerpt": "Fix a few bugs introduced since last pypi release \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8269480849213893
      ],
      "excerpt": "A full version of the list below with source links can be found in the documentation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8429332599408815
      ],
      "excerpt": "PyTorch w/ single GPU single process (AMP optional) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9087115256150737
      ],
      "excerpt": "fused&lt;name&gt; optimizers by name with NVIDIA Apex installed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918974083095406,
        0.8918974083095406,
        0.8918974083095406,
        0.875936585602119,
        0.8918974083095406
      ],
      "excerpt": "Kornia - https://github.com/kornia/kornia \nRepDistiller - https://github.com/HobbitLong/RepDistiller \ntorchdistill - https://github.com/yoshitomo-matsubara/torchdistill \nPyTorch Metric Learning - https://github.com/KevinMusgrave/pytorch-metric-learning \nfastai - https://github.com/fastai/fastai \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8104757491860769
      ],
      "excerpt": "Some cleanup on all optimizers and factory. No more .data, a bit more consistency, unit tests for all! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8254848879087552
      ],
      "excerpt": "Add 1st training of my gmixer_24_224 MLP /w GLU, 78.1 top-1 w/ 25M params. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8270930438285246
      ],
      "excerpt": "Add snazzy benchmark.py script for bulk timm model benchmarking of train and/or inference \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8558992355526335
      ],
      "excerpt": "AGC w/ default clipping factor --clip-grad .01 --clip-mode agc \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8127843693725421
      ],
      "excerpt": "PyTorch value clipping of 10, --clip-grad 10. --clip-mode value \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8070286542925227
      ],
      "excerpt": "First Normalization-Free model training experiments done, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567623989303819
      ],
      "excerpt": "classic VGG (from torchvision, impl in vgg.py) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8507766880832002
      ],
      "excerpt": "Add several ResNet weights with ECA attention. 26t & 50t trained @ 256, test @ 320. 269d train @ 256, fine-tune @320, test @ 352. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9412489741818076
      ],
      "excerpt": "Ex: train.py /data/tfds --dataset tfds/oxford_iiit_pet --val-split test --model resnet50 -b 256 --amp --num-classes 37 --opt adamw --lr 3e-4 --weight-decay .001 --pretrained -j 2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874432480433669
      ],
      "excerpt": "Run validation on full ImageNet-21k directly from tar w/ BiT model: validate.py /data/fall11_whole.tar --model resnetv2_50x1_bitm_in21k --amp \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.912631828322746
      ],
      "excerpt": "create_model(name, features_only=True, out_indices=..., output_stride=...) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rwightman/pytorch-image-models/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright 2019 Ross Wightman\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "PyTorch Image Models",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-image-models",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rwightman",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rwightman/pytorch-image-models/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "# Weights for ResNet Strikes Back\r\nPaper: https://arxiv.org/abs/2110.00476\r\n\r\nMore details on weights and hparams to come...\r\n\r\n",
        "dateCreated": "2021-09-30T20:32:15Z",
        "datePublished": "2021-10-04T00:02:48Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-rsb-weights",
        "name": "v0.1-rsb-weights",
        "tag_name": "v0.1-rsb-weights",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-rsb-weights",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/50708573",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-rsb-weights"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "A collection of weights I've trained comparing various types of SE-like (SE, ECA, GC, etc), self-attention (bottleneck, halo, lambda) blocks, and related non-attn baselines.\r\n\r\n# ResNet-26-T series\r\n* [2, 2, 2, 2] repeat Bottlneck block ResNet architecture\r\n* ReLU activations\r\n* 3 layer stem with 24, 32, 64 chs, max-pool\r\n* avg pool in shortcut downsample\r\n* self-attn blocks replace 3x3 in both blocks for last stage, and second block of penultimate stage\r\n\r\n|model         |top1  |top1_err|top5  |top5_err|param_count|img_size|cropt_pct|interpolation|\r\n|--------------|------|--------|------|--------|-----------|--------|---------|-------------|\r\n|botnet26t_256 |79.246|20.754  |94.53 |5.47    |12.49      |256     |0.95     |bicubic      |\r\n|halonet26t    |79.13 |20.87   |94.314|5.686   |12.48      |256     |0.95     |bicubic      |\r\n|lambda_resnet26t|79.112|20.888  |94.59 |5.41    |10.96      |256     |0.94     |bicubic      |\r\n|lambda_resnet26rpt_256|78.964|21.036  |94.428|5.572   |10.99      |256     |0.94     |bicubic      |\r\n|resnet26t     |77.872|22.128  |93.834|6.166   |16.01      |256     |0.94     |bicubic      |\r\n\r\nDetails:\r\n* HaloNet - 8 pixel block size, 2 pixel halo (overlap), relative position embedding\r\n* BotNet - relative position embedding\r\n* Lambda-ResNet-26-T - 3d lambda conv, kernel = 9\r\n* Lambda-ResNet-26-RPT - relative position embedding\r\n\r\n## Benchmark - RTX 3090  - AMP - NCHW - NGC 21.09\r\n|model         |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|--------------|---------------------|---------------|----------------|--------------|---------------------|---------------|----------------|--------------|-----------|\r\n|resnet26t     |2967.55              |86.252         |256             |256           |857.62               |297.984        |256             |256           |16.01      |\r\n|botnet26t_256 |2642.08              |96.879         |256             |256           |809.41               |315.706        |256             |256           |12.49      |\r\n|halonet26t    |2601.91              |98.375         |256             |256           |783.92               |325.976        |256             |256           |12.48      |\r\n|lambda_resnet26t|2354.1               |108.732        |256             |256           |697.28               |366.521        |256             |256           |10.96      |\r\n|lambda_resnet26rpt_256|1847.34              |138.563        |256             |256           |644.84               |197.892        |128             |256           |10.99      |\r\n\r\n## Benchmark - RTX 3090 - AMP - NHWC - NGC 21.09\r\n|model                 |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|----------------------|---------------------|---------------|----------------|--------------|---------------------|---------------|----------------|--------------|-----------|\r\n|resnet26t             |3691.94              |69.327         |256             |256           |1188.17              |214.96         |256             |256           |16.01      |\r\n|botnet26t_256         |3291.63              |77.76          |256             |256           |1126.68              |226.653        |256             |256           |12.49      |\r\n|halonet26t            |3230.5               |79.232         |256             |256           |1077.82              |236.934        |256             |256           |12.48      |\r\n|lambda_resnet26rpt_256|2324.15              |110.133        |256             |256           |864.42               |147.485        |128             |256           |10.99      |\r\n|lambda_resnet26t|Not Supported             |       |            |          |              |        | \r\n\r\n# ResNeXT-26-T series\r\n* [2, 2, 2, 2] repeat Bottlneck block ResNeXt architectures\r\n* SiLU activations\r\n* grouped 3x3 convolutions in bottleneck, 32 channels per group\r\n* 3 layer stem with 24, 32, 64 chs, max-pool\r\n* avg pool in shortcut downsample\r\n* channel attn (active in non self-attn blocks) between 3x3 and last 1x1 conv\r\n* when active, self-attn blocks replace 3x3 conv in both blocks for last stage, and second block of penultimate stage\r\n\r\n|model         |top1  |top1_err|top5  |top5_err|param_count|img_size|cropt_pct|interpolation|\r\n|--------------|------|--------|------|--------|-----------|--------|---------|-------------|\r\n|eca_halonext26ts|79.484  |20.516  |94.600  |5.400  |10.76      |256     |0.94     |bicubic      |\r\n|eca_botnext26ts_256|79.270  |20.730  |94.594  |5.406  |10.59   |256     |0.95      |bicubic      |\r\n|bat_resnext26ts|78.268|21.732  |94.1  |5.9     |10.73      |256     |0.9      |bicubic      |\r\n|seresnext26ts |77.852|22.148  |93.784|6.216   |10.39      |256     |0.9      |bicubic      |\r\n|gcresnext26ts |77.804|22.196  |93.824|6.176   |10.48      |256     |0.9      |bicubic      |\r\n|eca_resnext26ts|77.446|22.554  |93.57 |6.43    |10.3       |256     |0.9      |bicubic      |\r\n|resnext26ts   |76.764|23.236  |93.136|6.864   |10.3       |256     |0.9      |bicubic      |\r\n\r\n\r\n## Benchmark - RTX 3090 - AMP - NCHW - NGC 21.09\r\n|model                 |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|----------------------|---------------------|---------------|----------------|--------------|---------------------|---------------|----------------|--------------|-----------|\r\n|resnext26ts           |3006.57              |85.134         |256             |256           |864.4                |295.646        |256             |256           |10.3       |\r\n|seresnext26ts         |2931.27              |87.321         |256             |256           |836.92               |305.193        |256             |256           |10.39      |\r\n|eca_resnext26ts       |2925.47              |87.495         |256             |256           |837.78               |305.003        |256             |256           |10.3       |\r\n|gcresnext26ts         |2870.01              |89.186         |256             |256           |818.35               |311.97         |256             |256           |10.48      |\r\n|eca_botnext26ts_256   |2652.03              |96.513         |256             |256           |790.43               |323.257        |256             |256           |10.59      |\r\n|eca_halonext26ts      |2593.03              |98.705         |256             |256           |766.07               |333.541        |256             |256           |10.76      |\r\n|bat_resnext26ts       |2469.78              |103.64         |256             |256           |697.21               |365.964        |256             |256           |10.73      |\r\n\r\n## Benchmark - RTX 3090 - AMP - NHWC - NGC 21.09\r\nNOTE: there are performance issues with certain grouped conv configs with channels last layout, backwards pass in particular is really slow. Also causing issues for RegNet and NFNet networks.\r\n|model                 |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|----------------------|---------------------|---------------|----------------|--------------|---------------------|---------------|----------------|--------------|-----------|\r\n|resnext26ts           |3952.37              |64.755         |256             |256           |608.67               |420.049        |256             |256           |10.3       |\r\n|eca_resnext26ts       |3815.77              |67.074         |256             |256           |594.35               |430.146        |256             |256           |10.3       |\r\n|seresnext26ts         |3802.75              |67.304         |256             |256           |592.82               |431.14         |256             |256           |10.39      |\r\n|gcresnext26ts         |3626.97              |70.57          |256             |256           |581.83               |439.119        |256             |256           |10.48      |\r\n|eca_botnext26ts_256   |3515.84              |72.8           |256             |256           |611.71               |417.862        |256             |256           |10.59      |\r\n|eca_halonext26ts      |3410.12              |75.057         |256             |256           |597.52               |427.789        |256             |256           |10.76      |\r\n|bat_resnext26ts       |3053.83              |83.811         |256             |256           |533.23               |478.839        |256             |256           |10.73      |\r\n\r\n# ResNet-33-T series.\r\n* [2, 3, 3, 2] repeat Bottlneck block  ResNet architecture\r\n* SiLU activations\r\n* 3 layer stem with 24, 32, 64 chs, no max-pool, 1st and 3rd conv stride 2\r\n* avg pool in shortcut downsample\r\n* channel attn (active in non self-attn blocks) between 3x3 and last 1x1 conv\r\n* when active, self-attn blocks replace 3x3 conv last block of stage 2 and 3, and both blocks of final stage\r\n* FC 1x1 conv between last block and classifier\r\n\r\nThe 33-layer models have an extra 1x1 FC layer between last conv block and classifier. There is both a non-attenion 33 layer baseline and a 32 layer without the extra FC.\r\n\r\n|model         |top1  |top1_err|top5  |top5_err|param_count|img_size|cropt_pct|interpolation|\r\n|--------------|------|--------|------|--------|-----------|--------|---------|-------------|\r\n|sehalonet33ts  |80.986|19.014  |95.272|4.728   |13.69      |256     |0.94     |bicubic      |\r\n|seresnet33ts  |80.388|19.612  |95.108|4.892   |19.78      |256     |0.94     |bicubic      |\r\n|eca_resnet33ts|80.132|19.868  |95.054|4.946   |19.68      |256     |0.94     |bicubic      |\r\n|gcresnet33ts  |79.99 |20.01   |94.988|5.012   |19.88      |256     |0.94     |bicubic      |\r\n|resnet33ts    |79.352|20.648  |94.596|5.404   |19.68      |256     |0.94     |bicubic      |\r\n|resnet32ts    |79.028|20.972  |94.444|5.556   |17.96      |256     |0.94     |bicubic      |\r\n\r\n## Benchmark - RTX 3090 - AMP - NCHW - NGC 21.09\r\n|model                 |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|----------------------|---------------------|---------------|----------------|--------------|---------------------|---------------|----------------|--------------|-----------|\r\n|resnet32ts            |2502.96              |102.266        |256             |256           |733.27               |348.507        |256             |256           |17.96      |\r\n|resnet33ts            |2473.92              |103.466        |256             |256           |725.34               |352.309        |256             |256           |19.68      |\r\n|seresnet33ts          |2400.18              |106.646        |256             |256           |695.19               |367.413        |256             |256           |19.78      |\r\n|eca_resnet33ts        |2394.77              |106.886        |256             |256           |696.93               |366.637        |256             |256           |19.68      |\r\n|gcresnet33ts          |2342.81              |109.257        |256             |256           |678.22               |376.404        |256             |256           |19.88      |\r\n|sehalonet33ts         |1857.65              |137.794        |256             |256           |577.34               |442.545        |256             |256           |13.69      |\r\n\r\n## Benchmark - RTX 3090 - AMP - NHWC - NGC 21.09\r\n|model                 |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|----------------------|---------------------|---------------|----------------|--------------|---------------------|---------------|----------------|--------------|-----------|\r\n|resnet32ts            |3306.22              |77.416         |256             |256           |1012.82              |252.158        |256             |256           |17.96      |\r\n|resnet33ts            |3257.59              |78.573         |256             |256           |1002.38              |254.778        |256             |256           |19.68      |\r\n|seresnet33ts          |3128.08              |81.826         |256             |256           |950.27               |268.581        |256             |256           |19.78      |\r\n|eca_resnet33ts        |3127.11              |81.852         |256             |256           |948.84               |269.123        |256             |256           |19.68      |\r\n|gcresnet33ts          |2984.87              |85.753         |256             |256           |916.98               |278.169        |256             |256           |19.88      |\r\n|sehalonet33ts         |2188.23              |116.975        |256             |256           |711.63               |179.03         |128    |256   |13.69   |\r\n\r\n# ResNet-50(ish) models\r\nIn Progress\r\n\r\n# RegNet\"Z\" series\r\n* RegNetZ inspired architecture, inverted bottleneck, SE attention, pre-classifier FC, essentially an EfficientNet w/ grouped conv instead of depthwise\r\n* b, c, and d are three different sizes I put together to cover differing flop ranges, not based on the paper (https://arxiv.org/abs/2103.06877) or a search process\r\n* for comparison to RegNetY and paper RegNetZ models, at 224x224 b,c, and d models are 1.45, 1.92, and 4.58 GMACs respectively, b, and c are trained at 256 here so higher than that (see tables)\r\n* `haloregnetz_c` uses halo attention for all of last stage, and interleaved every 3 (for 4) of penultimate stage\r\n* b, c variants use a stem / 1st stage like the paper, d uses a 3-deep tiered stem with 2-1-2 striding\r\n\r\n## ImageNet-1k validation at train resolution\r\n|model        |top1  |top1_err|top5  |top5_err|param_count|img_size|cropt_pct|interpolation|\r\n|-------------|------|--------|------|--------|-----------|--------|---------|-------------|\r\n|regnetz_d    |83.422|16.578  |96.636|3.364   |27.58      |256     |0.95     |bicubic      |\r\n|regnetz_c    |82.164|17.836  |96.058|3.942   |13.46      |256     |0.94     |bicubic      |\r\n|haloregnetz_b|81.058|18.942  |95.2  |4.8     |11.68      |224     |0.94     |bicubic      |\r\n|regnetz_b    |79.868|20.132  |94.988|5.012   |9.72       |224     |0.94     |bicubic      |\r\n\r\n## ImageNet-1k validation at optimal test res\r\n|model        |top1  |top1_err|top5  |top5_err|param_count|img_size|cropt_pct|interpolation|\r\n|-------------|------|--------|------|--------|-----------|--------|---------|-------------|\r\n|regnetz_d    |84.04 |15.96   |96.87 |3.13    |27.58      |320     |0.95     |bicubic      |\r\n|regnetz_c    |82.516|17.484  |96.356|3.644   |13.46      |320     |0.94     |bicubic      |\r\n|haloregnetz_b|81.058|18.942  |95.2  |4.8     |11.68      |224     |0.94     |bicubic      |\r\n|regnetz_b    |80.728|19.272  |95.47 |4.53    |9.72       |288     |0.94     |bicubic      |\r\n\r\n## Benchmark - RTX 3090 - AMP - NCHW - NGC 21.09\r\n|model        |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|infer_GMACs|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|-------------|---------------------|---------------|----------------|--------------|-----------|---------------------|---------------|----------------|--------------|-----------|\r\n|regnetz_b    |2703.42              |94.68          |256             |224           |1.45       |764.85               |333.348        |256             |224           |9.72       |\r\n|haloregnetz_b|2086.22              |122.695        |256             |224           |1.88       |620.1                |411.415        |256             |224           |11.68      |\r\n|regnetz_c    |1653.19              |154.836        |256             |256           |2.51       |459.41               |277.268        |128             |256           |13.46      |\r\n|regnetz_d    |1060.91              |241.284        |256             |256           |5.98       |296.51               |430.143        |128             |256           |27.58      |\r\n\r\n## Benchmark - RTX 3090 - AMP - NHWC - NGC 21.09\r\nNOTE: channels last layout is painfully slow for backward pass here due to some sort of cuDNN issue\r\n|model        |infer_samples_per_sec|infer_step_time|infer_batch_size|infer_img_size|infer_GMACs|train_samples_per_sec|train_step_time|train_batch_size|train_img_size|param_count|\r\n|-------------|---------------------|---------------|----------------|--------------|-----------|---------------------|---------------|----------------|--------------|-----------|\r\n|regnetz_b    |4152.59              |61.634         |256             |224           |1.45       |399.37               |639.572        |256             |224           |9.72       |\r\n|haloregnetz_b|2770.78              |92.378         |256             |224           |1.88       |364.22               |701.386        |256             |224           |11.68      |\r\n|regnetz_c    |2512.4               |101.878        |256             |256           |2.51       |376.72               |338.372        |128             |256           |13.46      |\r\n|regnetz_d    |1456.05              |175.8          |256             |256           |5.98       |111.32               |1148.279       |128             |256           |27.58      |\r\n\r\n\r\n",
        "dateCreated": "2021-08-27T16:22:20Z",
        "datePublished": "2021-09-04T00:25:26Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights",
        "name": "v0.1-attn-weights",
        "tag_name": "v0.1-attn-weights",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-attn-weights",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/48999067",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-attn-weights"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "* Vision Transformer AugReg weights and model defs (https://arxiv.org/abs/2106.10270)\r\n* ResMLP official weights\r\n* ECA-NFNet-L2 weights\r\n* gMLP-S weights\r\n* ResNet51-Q\r\n* Visformer, LeViT, ConViT, Twins\r\n* Many fixes, improvements, better test coverage",
        "dateCreated": "2021-06-30T16:25:58Z",
        "datePublished": "2021-06-30T16:35:55Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.4.12",
        "name": "v0.4.12. Vision Transformer AugReg support and more",
        "tag_name": "v0.4.12",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.4.12",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/45505300",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.4.12"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "A catch-all (ish) release for storing vision transformer weights adapted/rehosted from 3rd parties. Too many incoming models for one release per source...\r\n\r\nContaining weights from:\r\n* Twins - https://github.com/Meituan-AutoML/Twins\r\n* Visformer - https://github.com/danczs/Visformer/issues/2\r\n* NesT (Aggregated Nested Transformer) - weights converted from https://github.com/google-research/nested-transformer by @alexander-soare ' script\r\n",
        "dateCreated": "2021-05-20T14:55:38Z",
        "datePublished": "2021-05-21T23:02:50Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-vt3p-weights",
        "name": "3rd Party Vision Transformer Weights",
        "tag_name": "v0.1-vt3p-weights",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-vt3p-weights",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/43397817",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-vt3p-weights"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "",
        "dateCreated": "2021-05-16T15:31:52Z",
        "datePublished": "2021-05-18T23:17:02Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.4.9",
        "name": "v0.4.9. EfficientNetV2. MLP-Mixer. ResNet-RS. More vision transformers.",
        "tag_name": "v0.4.9",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.4.9",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/43180400",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.4.9"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "Weights from https://github.com/google/automl/tree/master/efficientnetv2\r\n\r\nPaper: `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298",
        "dateCreated": "2021-05-11T17:50:46Z",
        "datePublished": "2021-05-14T18:10:09Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-effv2-weights",
        "name": "EfficientNet-V2 weights ported from Tensorflow impl",
        "tag_name": "v0.1-effv2-weights",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-effv2-weights",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/42973817",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-effv2-weights"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "Weights for ResNet-RS models as per #554 . Ported from Tensorflow impl (https://github.com/tensorflow/tpu/tree/master/models/official/resnet/resnet_rs) by @amaarora ",
        "dateCreated": "2021-04-30T04:08:37Z",
        "datePublished": "2021-05-04T01:18:40Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-rs-weights",
        "name": "ResNet-RS weights",
        "tag_name": "v0.1-rs-weights",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-rs-weights",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/42388568",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-rs-weights"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "Weights for CoaT: Co-Scale Conv-Attentional Image Transformers (from https://github.com/mlpc-ucsd/CoaT)",
        "dateCreated": "2021-04-28T17:33:48Z",
        "datePublished": "2021-04-28T23:08:21Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-coat-weights",
        "name": "Weights for CoaT (vision transformer) models",
        "tag_name": "v0.1-coat-weights",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-coat-weights",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/42169942",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-coat-weights"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "Weights from https://github.com/naver-ai/pit \r\n\r\nCopyright 2021-present NAVER Corp.\r\n\r\nRehosted here for easy pytorch hub downloads.\r\n",
        "dateCreated": "2021-03-29T18:33:32Z",
        "datePublished": "2021-03-31T19:11:46Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-pit-weights",
        "name": "Weights for PiT (Pooling-based Vision Transformer) models",
        "tag_name": "v0.1-pit-weights",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-pit-weights",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/40779626",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-pit-weights"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "",
        "dateCreated": "2021-03-08T00:18:31Z",
        "datePublished": "2021-03-08T00:35:03Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.4.5",
        "name": "v0.4.5. Lots of models. NFNets (& NF-ResNet, NF-RegNet), GPU-Efficient Nets, RepVGG, VGG. ",
        "tag_name": "v0.4.5",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.4.5",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/39409735",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.4.5"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "Weights converted from DeepMind Haiku impl of NFNets (https://github.com/deepmind/deepmind-research/tree/master/nfnets)",
        "dateCreated": "2021-02-16T18:27:58Z",
        "datePublished": "2021-02-18T08:02:36Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-dnf-weights",
        "name": "DeepMind NFNet-F* weights",
        "tag_name": "v0.1-dnf-weights",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-dnf-weights",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/38211668",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-dnf-weights"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "Checkpoints remapped from official repository at https://github.com/DingXiaoH/RepVGG",
        "dateCreated": "2021-02-08T20:04:48Z",
        "datePublished": "2021-02-09T07:22:20Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-repvgg-weights",
        "name": "RepVGG checkpoints remapped from official repo",
        "tag_name": "v0.1-repvgg-weights",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-repvgg-weights",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/37782708",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-repvgg-weights"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "Checkpoints remapped from official repo at https://github.com/idstcv/GPU-Efficient-Networks",
        "dateCreated": "2021-02-08T20:04:48Z",
        "datePublished": "2021-02-09T07:00:06Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-ger-weights",
        "name": "GPU-Efficient (Residual) Networks checkpoints",
        "tag_name": "v0.1-ger-weights",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-ger-weights",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/37782488",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-ger-weights"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "",
        "dateCreated": "2021-01-06T05:25:40Z",
        "datePublished": "2021-01-08T19:51:18Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.3.4",
        "name": "v0.3.4. Minor release. Conda setup.cfg added",
        "tag_name": "v0.3.4",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.3.4",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/36159807",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.3.4"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "",
        "dateCreated": "2021-01-03T20:49:31Z",
        "datePublished": "2021-01-03T23:30:24Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.3.3",
        "name": "v0.3.3. ResNet-101D/152D/200D and SE-ResNet-152D models w/ weights.",
        "tag_name": "v0.3.3",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.3.3",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/35931743",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.3.3"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "Converted to PyTorch from https://github.com/google-research/vision_transformer\r\n",
        "dateCreated": "2020-10-22T23:07:22Z",
        "datePublished": "2020-10-26T23:59:21Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-vitjx",
        "name": "Ported weights from official JAX impl of Vision Transformers and MLP-Mixer",
        "tag_name": "v0.1-vitjx",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-vitjx",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/33084132",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-vitjx"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "### Aug 12, 2020\r\n* New/updated weights from training experiments\r\n  * EfficientNet-B3 - 82.1 top-1 (vs 81.6 for official with AA and 81.9 for AdvProp)\r\n  * RegNetY-3.2GF - 82.0 top-1 (78.9 from official ver)\r\n  * CSPResNet50 - 79.6 top-1 (76.6 from official ver)\r\n* Add CutMix integrated w/ Mixup. See [pull request](https://github.com/rwightman/pytorch-image-models/pull/218) for some usage examples\r\n* Some fixes for using pretrained weights with `in_chans` != 3 on several models.\r\n\r\n### Aug 5, 2020\r\nUniversal feature extraction, new models, new weights, new test sets.\r\n* All models support the `features_only=True` argument for `create_model` call to return a network that extracts feature maps from the deepest layer at each stride.\r\n* New models\r\n  * CSPResNet, CSPResNeXt, CSPDarkNet, DarkNet\r\n  * ReXNet\r\n  * (Modified Aligned) Xception41/65/71 (a proper port of TF models)\r\n* New trained weights\r\n  * SEResNet50 - 80.3 top-1\r\n  * CSPDarkNet53 - 80.1 top-1\r\n  * CSPResNeXt50 - 80.0 top-1\r\n  * DPN68b - 79.2 top-1\r\n  * EfficientNet-Lite0 (non-TF ver) - 75.5 (submitted by [@hal-314](https://github.com/hal-314))\r\n* Add 'real' labels for ImageNet and ImageNet-Renditions test set, see [`results/README.md`](results/README.md)\r\n* Test set ranking/top-n diff script by [@KushajveerSingh](https://github.com/KushajveerSingh)\r\n* Train script and loader/transform tweaks to punch through more aug arguments\r\n* README and documentation overhaul. See initial (WIP) documentation at https://rwightman.github.io/pytorch-image-models/\r\n* adamp and sgdp optimizers added by [@hellbell](https://github.com/hellbell)\r\n",
        "dateCreated": "2020-08-13T06:26:59Z",
        "datePublished": "2020-08-13T16:52:05Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.2.1",
        "name": "Feature Maps, More Models, CutMix",
        "tag_name": "v0.2.1",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.2.1",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/29663480",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.2.1"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "ReXNet weights from https://github.com/clovaai/rexnet#pretrained remapped for `timm` model changes",
        "dateCreated": "2020-07-10T19:39:20Z",
        "datePublished": "2020-07-23T16:13:12Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-rexnet",
        "name": "RexNet remapped weights",
        "tag_name": "v0.1-rexnet",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-rexnet",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/28879661",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-rexnet"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "These are a mirror of weights from the official repository (https://github.com/zhanghang1989/ResNeSt ) to avoid issues with hosting changes/relocation",
        "dateCreated": "2020-06-17T05:57:26Z",
        "datePublished": "2020-06-30T04:23:42Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-resnest",
        "name": "Mirror of ResNeSt weights",
        "tag_name": "v0.1-resnest",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-resnest",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/28050508",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-resnest"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "RegNet weights cleaned and remapped from https://github.com/facebookresearch/pycls/blob/master/MODEL_ZOO.md\r\n\r\nChanges:\r\n  * first layer remapped from BGR to RGB\r\n  * cleaned out training details such as optimizer state, etc and leave just model state_dict (1/2 size)\r\n  * map layer names to mine",
        "dateCreated": "2020-05-13T23:30:34Z",
        "datePublished": "2020-05-18T01:48:37Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-regnet",
        "name": "RegNet official weights (remapped and cleaned)",
        "tag_name": "v0.1-regnet",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-regnet",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/26609045",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-regnet"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "Weights copied and cleaned (just state dict) from https://github.com/mrT23/TResNet/blob/master/MODEL_ZOO.md for more consistent/fast transfer speeds",
        "dateCreated": "2020-04-13T01:23:46Z",
        "datePublished": "2020-04-26T05:34:50Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-tresnet",
        "name": "TResNet weights",
        "tag_name": "v0.1-tresnet",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-tresnet",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/25888549",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-tresnet"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "These weights are re-hosted from original repository (https://github.com/mehtadushy/SelecSLS-Pytorch) with permission of the author, Dushyant Mehta (@mehtadushy), under a CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/legalcode) license. \r\n\r\nSelecSLS (core) Network Architecture as proposed in \"XNect: Real-time Multi-person 3D\r\nHuman Pose Estimation with a Single RGB Camera, Mehta et al.\"\r\nhttps://arxiv.org/abs/1907.00837",
        "dateCreated": "2019-12-30T22:23:53Z",
        "datePublished": "2019-12-30T23:54:13Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-selecsls",
        "name": "SelecSLS Weights",
        "tag_name": "v0.1-selecsls",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-selecsls",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/22524151",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-selecsls"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "HRNet weights downloaded from official impl OneDrive links at: https://github.com/HRNet/HRNet-Image-Classification. Rehosted here with SHA hash for hub/modelzoo download compatibility.",
        "dateCreated": "2019-11-29T02:05:32Z",
        "datePublished": "2019-11-30T00:35:39Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-hrnet",
        "name": "HRNet weights from official impl",
        "tag_name": "v0.1-hrnet",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-hrnet",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/21867053",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-hrnet"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "Res2Net weights from https://github.com/gasvn/Res2Net for easier/faster access from North America that's compatible with model_zoo load_url",
        "dateCreated": "2019-09-02T05:10:03Z",
        "datePublished": "2019-09-04T21:43:00Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-res2net",
        "name": "Res2Net weights",
        "tag_name": "v0.1-res2net",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-res2net",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/19759508",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-res2net"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "https://pypi.org/project/timm/",
        "dateCreated": "2019-06-21T21:08:43Z",
        "datePublished": "2019-06-21T21:11:20Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1.1",
        "name": "Released on PyPi",
        "tag_name": "v0.1.1",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1.1",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/18153851",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1.1"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "These weights have all originated from Cadene's Pretrained model repository: https://github.com/Cadene/pretrained-models.pytorch \r\n\r\nI'm re-hosting some of the weights here that I use more often to reduce download times as the US/Canada to France link can be slow.\r\n",
        "dateCreated": "2019-06-21T18:57:43Z",
        "datePublished": "2019-06-21T19:44:05Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-cadene",
        "name": "Pretrained weights (from Cadene)",
        "tag_name": "v0.1-cadene",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-cadene",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/18152115",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-cadene"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "All weights present here were either trained by me with the code in this repository or ported by me from original implementations.",
        "dateCreated": "2019-05-30T15:42:12Z",
        "datePublished": "2019-05-31T00:11:39Z",
        "html_url": "https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-weights",
        "name": "Pretrained weights",
        "tag_name": "v0.1-weights",
        "tarball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/tarball/v0.1-weights",
        "url": "https://api.github.com/repos/rwightman/pytorch-image-models/releases/17694528",
        "zipball_url": "https://api.github.com/repos/rwightman/pytorch-image-models/zipball/v0.1-weights"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 15348,
      "date": "Fri, 24 Dec 2021 18:01:05 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "imagenet-classifier",
      "resnet",
      "dual-path-networks",
      "cnn-classification",
      "pretrained-models",
      "pretrained-weights",
      "distributed-training",
      "mobile-deep-learning",
      "mobilenet-v2",
      "mnasnet",
      "mobilenetv3",
      "efficientnet",
      "augmix",
      "randaugment",
      "mixnet",
      "efficientnet-training",
      "vision-transformer-models",
      "nfnets",
      "normalization-free-training"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "My current [documentation](https://rwightman.github.io/pytorch-image-models/) for `timm` covers the basics.\n\n[timmdocs](https://fastai.github.io/timmdocs/) is quickly becoming a much more comprehensive set of documentation for `timm`. A big thanks to [Aman Arora](https://github.com/amaarora) for his efforts creating timmdocs.\n\n[paperswithcode](https://paperswithcode.com/lib/timm) is a good resource for browsing the models within `timm`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "* Detectron2 - https://github.com/facebookresearch/detectron2\n* Segmentation Models (Semantic) - https://github.com/qubvel/segmentation_models.pytorch\n* EfficientDet (Obj Det, Semantic soon) - https://github.com/rwightman/efficientdet-pytorch\n\n",
      "technique": "Header extraction"
    }
  ]
}