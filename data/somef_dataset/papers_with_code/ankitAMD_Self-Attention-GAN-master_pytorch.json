{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1805.08318",
      "https://arxiv.org/abs/1805.08318",
      "https://arxiv.org/abs/1802.05957",
      "https://arxiv.org/abs/1805.08318 (2018)](https://arxiv.org/abs/1805.08318).**\n\n## Meta overview\nThis repository provides a PyTorch implementation of [SAGAN](https://arxiv.org/abs/1805.08318). Both wgan-gp and wgan-hinge loss are ready, but note that wgan-gp is somehow not compatible with the spectral normalization. Remove all the spectral normalization at the model for the adoption of wgan-gp.\n\nSelf-attentions are applied to later two layers of both discriminator and generator.\n\n<p align=\"center\"><img width=\"100%\" src=\"image/main_model.PNG\" /></p>\n\n## Current update status\n* [ ] Supervised setting\n* [ ] Tensorboard loggings\n* [x] **[20180608] updated the self-attention module. Thanks to my colleague [Cheonbok Park](https://github.com/cheonbok94)! see 'sagan_models.py' for the update. Should be efficient, and run on large sized images**\n* [x] Attention visualization (LSUN Church-outdoor)\n* [x] Unsupervised setting (use no label yet) \n* [x] Applied: [Spectral Normalization](https://arxiv.org/abs/1802.05957), code from [here](https://github.com/christiancosgrove/pytorch-spectral-normalization-gan)\n* [x] Implemented: self-attention module, two-timescale update rule (TTUR), wgan-hinge loss, wgan-gp loss\n\n&nbsp;\n&nbsp;\n\n## Results\n\n### Attention result on LSUN (epoch #8)\n<p align=\"center\"><img width=\"100%\" src=\"image/sagan_attn.png\" /></p>\nPer-pixel attention result of SAGAN on LSUN church-outdoor dataset. It shows that unsupervised training of self-attention module still works, although it is not interpretable with the attention map itself. Better results with regard to the generated images will be added. These are the visualization of self-attention in generator layer3 and layer4, which are in the size of 16 x 16 and 32 x 32 respectively, each for 64 images. To visualize the per-pixel attentions, only a number of pixels are chosen, as shown on the leftmost and the rightmost numbers indicate. \n\n### CelebA dataset (epoch on the left, still under training)\n<p align=\"center\"><img width=\"80%\" src=\"image/sagan_celeb.png\" /></p>\n\n### LSUN church-outdoor dataset (epoch on the left, still under training)\n<p align=\"center\"><img width=\"70%\" src=\"image/sagan_lsun.png\" /></p>\n\n## Prerequisites\n* [Python 3.5+](https://www.continuum.io/downloads)\n* [PyTorch 0.3.0](http://pytorch.org/)\n\n&nbsp;\n\n## Usage\n\n#### 1. Clone the repository\n```bash\n$ git clone https://github.com/heykeetae/Self-Attention-GAN.git\n$ cd Self-Attention-GAN\n```\n\n#### 2. Install datasets (CelebA or LSUN)\n```bash\n$ bash download.sh CelebA\nor\n$ bash download.sh LSUN\n```\n\n\n#### 3. Train \n##### (i) Train\n```bash\n$ python python main.py --batch_size 64 --imsize 64 --dataset celeb --adv_loss hinge --version sagan_celeb\nor\n$ python python main.py --batch_size 64 --imsize 64 --dataset lsun --adv_loss hinge --version sagan_lsun\n```\n#### 4. Enjoy the results\n```bash\n$ cd samples/sagan_celeb\nor\n$ cd samples/sagan_lsun\n\n```\n\nSOME IMPORTANT POINTS\n\n1.  Samples generated every 10(in parent file its 100) iterations are located. The rate            of sampling could be controlled via --sample_step (ex, --sample_step 10).\n\n2.  #code->> parser.add_argument('--total_step', type=int, default=100, help='how many times to update the generator')  which are used in \n\nparameter.py file and  update the generator after 100 --totalstep but in its parent file \n\nit originally is default =1000000, so i change it default=100 that  its possible for the \n\nperson which have not GPU .\n\n3. In This code i removed cuda because a new coder or non-coder or low middle class \n\npeople have not basically have'nt afford GPU so they \n\nalso experience without gpu and seen what are the result or changes or inference are \n\ncome after train and learn a lot.\n\n4. I do some chnges like removal of Cuda because we require GPU (which are costly but \n\nmore effective and give more speed of our system and train a lot more faster than CPU \n\nand taking very less time.)\n\n5. In above ### parameter.py #### file we use or import argparse (# import argpase) and \n\nuse argument to intialize our variable/ argument module (like variable as understandable\n\nterm it may be hyperparameter or anything also) by default value and what it type and \n\nalso give them choice and also automatically generates help and usage messages and issue \n\nerrors when user give the program invalid argument.(pythonforbegginer.com/argparse/argparse.tutorial ).\n\n6. ## super function() ##used in program go this link for explain... \n\n--(https://www.pythonforbeginners.com/super/working-python-super-function)\n\n7. transform function in data_loader.py used for augmentation of dataset.\n\n8. PARENT file ----(https://github.com/heykeetae/Self-Attention-GAN)\n\n where i learn but i do changes and make better for non cuda user please give star on my \n \n link if you like .\n \n 8.Pytorch is a Deep learning framework which are comes mixing of python and \"torch\"\n\ntorch framework comes from  #lua progamming language# basically which are mainly \n\noriginated for research purpose for new model comes and easily deploy and use and find \n\nthe result/INference.\n \n #INFERENCE---------------------------------------------------------------------------------\n\nInference means estimating the values of some (usually hidden random) variable given some observation.\n\n i think there isn\u2019t much of a difference (at least conceptually) between infernce and training.\n \n Deep learning is revolutionizing many areas of machine perception, with the potential to impact the everyday experience of \n \n people everywhere. On a high level, working with deep neural networks is a two-stage process: First, a neural network is \n \n trained: its parameters are determined using labeled examples of inputs and desired output. Then, the network is deployed to run \n \n inference, using its previously trained parameters to classify, recognize and process unknown inputs.\n \n https://devblogs.nvidia.com/wp-content/uploads/2015/08/training_inference1.png\n \n Deep Neural Network Training vs. Inference\n\nFigure 1: Deep learning training compared to inference. In training, many inputs, often in large batches, are used to train a \n\ndeep neural network. In inference, the trained network is used to discover information within new inputs that are fed through the \n\nnetwork in smaller batches.\n\nhttps://devblogs.nvidia.com/inference-next-step-gpu-accelerated-deep-learning/\n\nhttps://www.quora.com/What-is-the-difference-between-inference-and-prediction-in-machine-learning"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9999998531289515
      ],
      "excerpt": "Han Zhang, Ian Goodfellow, Dimitris Metaxas and Augustus Odena, \"Self-Attention Generative Adversarial Networks.\" arXiv preprint arXiv:1805.08318 (2018). \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ankitAMD/Self-Attention-GAN-master_pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-08-09T20:09:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-04T06:08:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9961963160457894,
        0.8410893780530032
      ],
      "excerpt": "This repository provides a PyTorch implementation of SAGAN. Both wgan-gp and wgan-hinge loss are ready, but note that wgan-gp is somehow not compatible with the spectral normalization. Remove all the spectral normalization at the model for the adoption of wgan-gp. \nSelf-attentions are applied to later two layers of both discriminator and generator. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9895372280706561
      ],
      "excerpt": "Per-pixel attention result of SAGAN on LSUN church-outdoor dataset. It shows that unsupervised training of self-attention module still works, although it is not interpretable with the attention map itself. Better results with regard to the generated images will be added. These are the visualization of self-attention in generator layer3 and layer4, which are in the size of 16 x 16 and 32 x 32 respectively, each for 64 images. To visualize the per-pixel attentions, only a number of pixels are chosen, as shown on the leftmost and the rightmost numbers indicate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8052654499173924
      ],
      "excerpt": "SOME IMPORTANT POINTS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8957543331574902
      ],
      "excerpt": "code->> parser.add_argument('--total_step', type=int, default=100, help='how many times to update the generator')  which are used in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8111735557399044,
        0.9274355612432084,
        0.856045395041288
      ],
      "excerpt": "I do some chnges like removal of Cuda because we require GPU (which are costly but  \nmore effective and give more speed of our system and train a lot more faster than CPU  \nand taking very less time.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8070727257029022
      ],
      "excerpt": "originated for research purpose for new model comes and easily deploy and use and find  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9987999762026136,
        0.8991111901555372
      ],
      "excerpt": "Deep learning is revolutionizing many areas of machine perception, with the potential to impact the everyday experience of  \npeople everywhere. On a high level, working with deep neural networks is a two-stage process: First, a neural network is  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8412172601025125,
        0.9225098714206076
      ],
      "excerpt": "Figure 1: Deep learning training compared to inference. In training, many inputs, often in large batches, are used to train a  \ndeep neural network. In inference, the trained network is used to discover information within new inputs that are fed through the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": " Pytorch implementation of Self-Attention Generative Adversarial Networks (SAGAN) of non-cuda user s and its also used by cuda user.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ankitAMD/Self-Attention-GAN-master_pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 15:55:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ankitAMD/Self-Attention-GAN-master_pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ankitAMD/Self-Attention-GAN-master_pytorch",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ankitAMD/Self-Attention-GAN-master_pytorch/master/download.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ bash download.sh CelebA\nor\n$ bash download.sh LSUN\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9829922351927534,
        0.9355532161343175
      ],
      "excerpt": "$ git clone https://github.com/heykeetae/Self-Attention-GAN.git \n$ cd Self-Attention-GAN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9003729605384843
      ],
      "excerpt": "$ cd samples/sagan_celeb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9003729605384843
      ],
      "excerpt": "$ cd samples/sagan_lsun \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8142393839771336
      ],
      "excerpt": "I do some chnges like removal of Cuda because we require GPU (which are costly but  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8532985168798555
      ],
      "excerpt": "<p align=\"center\"><img width=\"100%\" src=\"image/main_model.PNG\" /></p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8532985168798555
      ],
      "excerpt": "<p align=\"center\"><img width=\"100%\" src=\"image/sagan_attn.png\" /></p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8423250384747827
      ],
      "excerpt": "<p align=\"center\"><img width=\"80%\" src=\"image/sagan_celeb.png\" /></p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8907305294368035
      ],
      "excerpt": "$ python python main.py --batch_size 64 --imsize 64 --dataset celeb --adv_loss hinge --version sagan_celeb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8907305294368035
      ],
      "excerpt": "$ python python main.py --batch_size 64 --imsize 64 --dataset lsun --adv_loss hinge --version sagan_lsun \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8166863788385804
      ],
      "excerpt": "parameter.py file and  update the generator after 100 --totalstep but in its parent file  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ankitAMD/Self-Attention-GAN-master_pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Self-Attention GAN for NON-CUDA / NON-GPU USER in Pytorch (i talking about code )#####",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Self-Attention-GAN-master_pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ankitAMD",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ankitAMD/Self-Attention-GAN-master_pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [Python 3.5+](https://www.continuum.io/downloads)\n* [PyTorch 0.3.0](http://pytorch.org/)\n\n&nbsp;\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Sat, 25 Dec 2021 15:55:13 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "sagan",
      "pytorch",
      "deeplearning-framework",
      "python",
      "jupyter-notebook",
      "machine-learning",
      "deep-learning",
      "celeba-dataset",
      "lsun-dataset"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "use argument to intialize our variable/ argument module (like variable as understandable\n\nterm it may be hyperparameter or anything also) by default value and what it type and \n\nalso give them choice and also automatically generates help and usage messages and issue \n\nerrors when user give the program invalid argument.(pythonforbegginer.com/argparse/argparse.tutorial ).\n\n6. ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "--(https://www.pythonforbeginners.com/super/working-python-super-function)\n\n7. transform function in data_loader.py used for augmentation of dataset.\n\n8. PARENT file ----(https://github.com/heykeetae/Self-Attention-GAN)\n\n where i learn but i do changes and make better for non cuda user please give star on my \n \n link if you like .\n \n 8.Pytorch is a Deep learning framework which are comes mixing of python and \"torch\"\n\ntorch framework comes from  #lua progamming language",
      "technique": "Header extraction"
    }
  ]
}