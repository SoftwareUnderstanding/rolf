{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1907.11692",
      "https://arxiv.org/abs/1910.01108"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Paper you can cite for the  Transformers library:\n```\n@article{Wolf2019HuggingFacesTS,\n  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},\n  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.03771}\n}\n```",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Wolf2019HuggingFacesTS,\n  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},\n  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.03771}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9186398560642831
      ],
      "excerpt": "BERT from the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(https://arxiv.org/abs/1810.04805) released by Google. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/knuddj1/op_text",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-21T01:09:38Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-11T08:40:18Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9795393532068207
      ],
      "excerpt": "A wrapper around the popular transformers  machine learning library, by the HuggingFace team. OP Text provides a simplified, Keras like, interface for fine-tuning, evaluating and inference of popular pretrained BERT models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.893528081112636,
        0.8249809263885474
      ],
      "excerpt": "Currently the available models are: \nBERT from the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(https://arxiv.org/abs/1810.04805) released by Google. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8554286247814465,
        0.9120184680636204
      ],
      "excerpt": "DistilBERT from the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter released by HuggingFace. \nEach model has contains a list of the available pretrained models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828787945422049
      ],
      "excerpt": "Loading a model is achieved in one line of code. The string can either be the name of a pretrained model or a path to a local fine-tuned model on disk.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = Bert(\"bert-base-uncased\", num_labels=2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9753696879337924,
        0.9293037399518749
      ],
      "excerpt": "Supply num_labels when using a pretrained model, as an untrained classification head is added when using this one of the DOWNLOADABLE strings. \nFinetuning a model is as simple as loading a dataset, instantiating a model and then passing it to the models fit function.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.860059181823877
      ],
      "excerpt": "model = Bert('bert-base-uncased', num_labels=2) \nmodel.fit(X_train, y_train) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = Bert('bert-base-uncased', num_labels=2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9571433495210571,
        0.860059181823877
      ],
      "excerpt": "Model evaluation is basically the same as model training. Load a dataset, instantiate a model and but instead call the evaluate function. This returns a number between 0 and 1 which is the percentage of predictions the model got correct. \nmodel.evalaute(X_test, y_test) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9884139562411614,
        0.9027373628313262
      ],
      "excerpt": "Predict the label of a piece of text/s by passing a list of strings to the models  \npredict function.  This returns a list of tuples, one for each piece of text to be predicted. These tuples contain the models confidence scores for each class and the numerical label of the predicted class. If a label converter is supplied, a string label of the predicted class is also included in each output tuple. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/knuddj1/op_text/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 12:07:47 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/knuddj1/op_text/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "knuddj1/op_text",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "PyTorch is required as a prerequisite before installing OP Text. Head on over to the [getting started  page](https://pytorch.org/get-started/locally/) of their website and follow the installation instructions for your version of Python. \n\n>!Currently only Python versions 3.6 and above are supported\n\nUse one of the following commands to install OP Text:\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.999746712887969,
        0.9995234057802402
      ],
      "excerpt": "pip install op_text \nconda install op_text \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8113020759983942,
        0.8900486270063179
      ],
      "excerpt": "Loading a model is achieved in one line of code. The string can either be the name of a pretrained model or a path to a local fine-tuned model on disk.  \nfrom op_text.models import Bert \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8900486270063179
      ],
      "excerpt": "from models.import Bert \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778487586960795,
        0.8778487586960795
      ],
      "excerpt": "    \"Example sentence 1\" \n    \"Example sentence 2\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8540506940369198,
        0.8900486270063179
      ],
      "excerpt": "At the conclusion of training you will most likely want to save your model to disk. Simply call the the models save function and supply an output directory and name for the model. \nfrom models.import Bert \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8537102065372528
      ],
      "excerpt": "model.save(\"path/to/output/dir/\", \"example_save_name\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9416522774131079
      ],
      "excerpt": "from op_text.utils import LabelConverter \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8525645461651434
      ],
      "excerpt": "model.predict(to_predict, label_converter=converter) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/knuddj1/op_text/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Joel Barmettler\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "OP Text",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "op_text",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "knuddj1",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/knuddj1/op_text/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 12:07:47 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The entire purpose of this package is to allow users to leverage the power of the transformer models available in HuggingFace's library without needing to understand how to use PyTorch.\n\n",
      "technique": "Header extraction"
    }
  ]
}