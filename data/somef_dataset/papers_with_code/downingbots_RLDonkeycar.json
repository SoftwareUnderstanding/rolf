{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "First thanks to Will Roscoe and Adam Conway for the initial donkeycar code, donkeycar design, and website (donkeycar.com). Will and Adam built the \"hello world\" of neural nets and real-world robotics. This repository tries to extend their work to Reinforcement Learning... but the RL coding isn't nearly as pretty :-) and may never be ready to be integrated into the main donkeycar branch.\n\nThanks to Carlos Uranga at [DeepRacing](http://deepracing.com/). His efforts have created a space to develop and test autonomous RC cars on a regular basis. As of April 2019, DeepRacing meets bi-weekly at [TheShop.Build in San Jose]( https://theshop.build/San-Jose )\n\nThanks to Chris Anderson who hosts [DIYRobocars](https://diyrobocars.com/ ). These quarterly events draw hundreds of spectators and a dozen or more cars. It is fascinating to watch cars that would normally circumnavigate the track fail to handle to the change of scenery due to the number of spectators.\n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "* SWITCH_TO_NN = 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "* PORT_CONTROLPI = \"10.0.0.4:5558\" \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/downingbots/RLDonkeycar",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-02T02:20:31Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-17T08:07:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9779173395292436,
        0.9811005363127397,
        0.8190123497090103,
        0.9528852820373774,
        0.9643435463088645,
        0.9947470861883945,
        0.9191441853634955,
        0.9544051296267467,
        0.8936315385000179
      ],
      "excerpt": "The high-level goal is to plop down a donkeycar (with minor hardware enhancements) on a track and watch the donkeycar improve its lap time each time it drives around the track. In its current form, the RLDonkeycar needs a human minder as it drives around the track, and the ability to use reinforcement learning to improve track-times is hardware-limited. \nIn a little more detail, the goal of the RLDonkeycar software is: \n* To plop the donkeycar down on the center of a track, even tracks it had never seen before.  The track should have a dashed or solid yellow center line and white lane lines. \n* Slowly self-drive around the track using on-car real-time automated control of donkeycar. The automated driving is based upon OpenCV line-following or execution of a Keras Neural Net on a Raspberry Pi 3B+ (aka the \"Control Pi\") \n* Use the opencv line-following to do on-car real-time imitation learning or reinforcement learning (RL) on the second Raspberry Pi 3B+ (aka RLPi.) The Neural Net (NN) is similar to the small convolutional NN in the original donkeycar code but enhanced to support RL. \n* Uses Keras RL implementation based on OpenAI's Proximal Policy Optimization (PPO paper). A good introduction to PPO is available on YouTube by Arxiv Insights. Briefly, PPO is a gradient descent algorithm with an actor-critic component and a reward function based upon a history of results.  \n* Periodically update the Control Pi's Keras model weights as computed at RLPi to support real-time learning as the RLDonkeycar drives continuously along the track. \n* Push Raspberry Pi's to their limit to see what is possible and minimize the changes from the original donkeycar design. The original donkeycar does remarkably well with its simple convolutional NN on a raspberry pi, but can raspberry pi's do real-time RL? \n* Use the same code to run on the enhanced Donkey Car simulator. The simulator is especially useful for debugging on a laptop without requiring access to a real car or track. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8723479501546081,
        0.9246307492754543,
        0.8529625635819587,
        0.9814345764047394
      ],
      "excerpt": "* an additional Raspberry Pi 3B+ with SD card \n* an additional battery (fits side-to-side with the other battery under the pi's and their 3-D printed plate) \n* offsets to stack the raspberry pi's. The Control Pi should be on top and host the camera. \n* connect the raspberry pis with a 6\" Ethernet cable. Tie the cable back so it is outside the field of view of the camera. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8171694801193659
      ],
      "excerpt": "Initially the donkeycar is placed on or near the center yellow line (optionally dashed) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9868602072987772,
        0.9896391681226502,
        0.9501665842561614,
        0.9311857455469074,
        0.9914711517707823,
        0.9723078365471115,
        0.9891052104546413,
        0.8868484084886702,
        0.8662597804777568,
        0.9896647021583524,
        0.8254740567631422,
        0.8461931259297921,
        0.921976499243122,
        0.9172546950545066,
        0.9841099665862552,
        0.934525248771619,
        0.9578607861860514,
        0.9526842169223334,
        0.8390738754942729,
        0.9707753253102149,
        0.95568339259417,
        0.9870608812285386,
        0.924954235004602,
        0.9494870288835905,
        0.9346146759620773
      ],
      "excerpt": "Once moving, the Control Pi uses open-CV and a simple Hough-Transform algorithm to follow the center dashed yellow line. The white/yellow lines and their colors plus lane widths are dynamically computed by identifying vanishing points and then tracking these lines/colors.  A human typically follows the car as it makes its way around the track. If the car leaves the track, the human picks up the car and resets it to the center of the track. Mixing things up like driving direction, lighting, rerunning at failure points are recommended. The slow speed of travel is required due to the frame-per-second (fps) limitations of the raspberry pi despite the simple OpenCV algorithm. To see much more complicated OpenCV algorithms, look at Udacity student blogs. \nThe OpenCV-based throttle and steering values plus the image are sent to the RLPi to perform imitation learning of the Keras model. \nOnce sufficient imitation learning has been trained at the RLPi, the weights of the Keras model are sent to the Control Pi and loaded there. \nThe Control Pi now uses the Keras Model to compute the throttle and steering instead of the Open-CV line-following.  Every few images (configurable), a random bounded change to the computed throttle and steering are made. This randomness increases the RL search-space. \nThe Keras model's throttle and steering values plus the image are sent to the RLPi. The RLPi caches this message and runs OpenCV on the image to compute the frame's reward as a function of throttle and distance from center. If it is determined that the car has left the track, a mini-batch is completed allowing the total reward to be computed by accumulating the frame rewards within the mini-batch with a time-decay function. Very small mini-batches are ignored as these are typically caused by manual resetting of the car.  If a mini-batch reaches a length of 20, then we end the batch because the cause-and-effect of actions 20 images apart is very small in autonomous racing (unlike in video games where there can be sparse large rewards) and because we want to limit the cache size and because we want to facilitate real-time learning. \nOnce a certain number of message are cached and the mini-batch has completed, the Keras model is trained on the images and data contained in these messages and these cached messages are discarded. \nPeriodically, the revised weights of the Keras PPO model are sent to the Control Pi and loaded there. \nThe Control Pi now uses the revised Keras Model to compute the throttle and steering instead of the Open-CV line-following.  Goto step 6. \nThe RLDonkeycar code was cloned back in March 2018 and then enhanced with new RL code. The RLDonkeycar can still use the imitation learning provided for the original donkeycar. Just invoke the code as documented on donkeycar.com and use a remote control to drive the car around the track. \nThe imitation learning in the original donkeycar code could learn how to autonomously drive around the track after training on data obtained from as few as 3 manually driven trips around the track and could drive at decent speed. With relatively little training data, the trained car would effectively memorize its way around the course by observing features off the track like posts, cones, chairs, etc. Such over-fitting on features would result in poor performance at events with large numbers of spectators, when such features could be obfuscated. More training in different scenarios and lighting conditions could greatly improve the intelligence, reliability and performance of the donkeycar. The better the human driver, the better the results. \nInstead of a human using a remote control, the RLDonkeycar is trained by a line-following program using OpenCV. The OpenCV line-following ended up with the following minimal set of features: \nThe OpenCV line-following is slower than the NN in Frames-per-second (fps) in the original donkeycar code. Even with the simplest line-following algorithm, the donkeycar could only get around the track at the slowest speed the car could drive. If the speed was increased only slightly, the car would frequently drive off the track or not handle sharp turns. Sharp turns require a higher throttle in order to provide the additional torque required. \nThe slowest speed was determined by detecting movement of the car by optical flow. Movement by spectators could fool optical flow into thinking the car was moving. Such false positives can be reduced via manual tuning of the optical flow parameters that require higher thresholds to detect movement but may also result in a higher minimum speed. Periodically, optical flow would be used to ensure that movement was still happening as a constant throttle results in battery drainage and the car slowing down. In its minimalistic design, the donkeycar does not have a wheel encoder to determine speed and only has a camera as sensor input. \nOnce moving, the Control Pi uses open-CV and a simple Hough-Transform algorithm to follow the Center Dashed yellow line. The white lines, the white/yellow colors, and lane widths are also tracked by identifying \"vanishing points\". On some tracks, using gray-scale worked better and other tracks color images were better. Lighting makes a huge difference, and dynamically computing colors and their standard deviation worked best. \nBy initially doing imitation learning, RL can skip the long awkward phase learning by doing random movements. Random movements are still used during reinforcement learning, but they are bounded so that the RLDonkeycar will have the opportunity to recover from a bad move and still stay on the track, while still allowing the car to from learn better moves.  The RL is done asynchronously on the \"RLPi\" while the Control Pi drives. This attempt to do incremental real-time learning ended up with the following implementation: \nUnfortunately, early attempts at simple RL algorithms did not do well. After Switching to PPO (implemented using Keras), the RLDonkeycar succeeded in learning line-following instead of overfitting on the training data. The reward function is a function of speed and distance from the middle dashed line of the track as determined by the OpenCV code. \nPPO uses a history of inputs and moves for training.  The RLPi stores frames in a cache.  Rewards are accumulated over a set of consecutive images as long as openCV determines that the car remains on the track (up to a maximum of 20 consecutive images.) Manual resetting a car that goes off the track could take one or two consecutive images so very short runs were ignored for training purposes. \nAfter the total accumulated reward for an image has been computed, the image is eligible to be dequeued and used to train PPO. Training on a batch of images is more efficient, so the current implementation uses a batch of at least 50 images (configurable to trade off cache size and timeliness). \nThe Raspberry Pi 3B+ could only run the PPO algorithm at about 1 FPS. At this frame rate, the donkeycar couldn't get around the track much faster than the OpenCV line-following algorithm. Like the original donkeycar NN, the success of the NN was tied to the speed that it was trained at. \nThe PPO seemed to converge to a usable model on a real track in fewer training images than the simulated \"generated track\" which is relatively featureless. \nPeriodically, the Control Pi's Keras model weights would be updated with those recently computed in real-time at the RLPi.  When the RL weights are updated, the NN would typically increase the throttle in order to increase the reward. Unfortunately, the battery power would decrease resulting in the throttle decreasing before the next RL weight update. The net result in throttle gain was negligible using the current tuning (see the THROTTLE_BOOST paramenter descriptions below.) Such real-world issues don't show up in the simulations. The throttle gain can be tweaked by changing the reward function, but can easily result in the throttle exceeding what the raspberry pi can handle. Instead of fine-tuning, I intend to follow my \"Next Steps.\" \nTo address the battery issues, the next step is to add a rotary encoder and pid based upon the work done by Alan Wells. This will enable the donkeycar to travel at the desired speed despite a draining battery without making the neural net more complex. \nUse one or two NVidia Jetson Nanos and a raspberry pi 3b+. The raspberry pi 3b+ would provide the wi-fi. Ideally the Jetson Nano will be used to run the Control Pi as parallelism can be exploited during execution the PPO neural net. Improved frame rates should increase the achievable throttle speed.  Using the raspberry pi for training is acceptable as training is done asynchronously and can shed loads by strategically dropping frames if it can't keep up with the Jetson nano. Alternatively, another level of functional parallelism can be added by using the raspberry pi for low-level control and handling the encoder while using one nano for running the NN for control and the other nano for training.  \nThere's more that can be done with the minimalist 2 raspberry pi design. For example, instead of using a NN inspired by the simple convolutional NN used so well by the original donkeycar, a 2-level fully connected NN should be tried with PPO. A simplified NN should result in more FPS and faster throughput. Plenty of fine-tuning remains to improve performance. \nFirst, you must create your own donkeycar and follow the instructions on donkeycar.com to play with the unmodified software. After gaining experience with the the unmodified donkeycar, it's time to enhance the donkey car by buying and assembling the raspberry pi and peripherals as outlined earlier. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8678533058343391
      ],
      "excerpt": "To run with the Unity-based simulator, use the version of the simulator designed to run with the OpenAI Gym. RLDonkeycar code has been changed to use version 18.9 that supports a closed-circuit donkeycar track and the ability to reset the car.  The drive script has been modified to accept running the following at the ControlPi: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8037921576397146
      ],
      "excerpt": "If using a physical car, put the car down near the middle of the track and start training in real-time.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9915946305242626
      ],
      "excerpt": "The donkeycar code was cloned somewhere around March 20, 2018. As my laptop was dealing with incompatible versions of some of the dependencies like keras, some of the code were customized for my laptop. There have been hundreds of commits to the donkeycar github repository since the clone.  Most of the new code in the RLDonkeycar repository to support reinforcement learning was separated into new files with names beginning with RL in the parts directory. A few integration points like manage.py were also modified. So, it is likely possible to update the code to the latest donkeycar version and integrate the code changes, but this has not been done at this point in time. The immediate plans is to work around the main limitations in the current hardware - upgrading to use a Jetson Nano and to support an encoder - at which point, the code might just diverge.  Before such divergence, this repository checkpoints the RL code so it can be run on a donkeycar with minimal changes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9334790831345707
      ],
      "excerpt": "RLPi.py: the main code executed at the RLPi. Loops around receiving and caching messages from the ControlPi, processing batches of messages ready for PPO, and sending the updated weights to the ControlPi. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9466992690108382
      ],
      "excerpt": "RLPPO.py: the Keras PPO algorithm as implemented by others with minimal changes. The code is derived from the initial framework and the PPO implementation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8469975214149161,
        0.8676584013702761,
        0.9170973049758725,
        0.9244638934837672,
        0.9359342985902358
      ],
      "excerpt": "* donkeygymsim.py: supports the newest openGym-compatible Unity simulator. \n* tcp_server.py: a tcp socket server (by Tawn Kramer) to talk to the unity donkey simulator. \nThe d2/model directory is the repository for state stored between runs. \nThe PPO models for the actor are stored in: rlpilot_actor  rlpilot_critic \ndonkeystate.json is a human-readable and editable json file storing track-specific state and throttle starting points.  Track-specific state includes white/yellow color computations and lane width. The \"minThrottle\" is the initial throttle when using Optical Flow to determine whether moving. In the current implementation, the value can change dramatically based upon whether you are running in simulation (set to near-zero), or running with a bad or drained battery.  If you change batteries, you probably need to manually edit the file to reset the minThrottle (to around 30 with my car / battery.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9496199533070518
      ],
      "excerpt": "To tune PPO processing in batches, tune the values for: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9267694077848617
      ],
      "excerpt": "The current reward function for an individual message/image is reward(distance_from_center) * (1 + cfg.THROTTLE_BOOST * throttle). See code in RLKeras.py for details. Feel free to experiment with tweaking THROTTLE_BOOST or replacing the reward function altogether: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Real-time reinforcement learning for a donkeycar with two raspberry pi's",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/downingbots/RLDonkeycar/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Tue, 28 Dec 2021 03:45:20 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/downingbots/RLDonkeycar/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "downingbots/RLDonkeycar",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/downingbots/RLDonkeycar/tree/master/donkeycar/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8830230748734117
      ],
      "excerpt": "To run with the Unity-based simulator, use the version of the simulator designed to run with the OpenAI Gym. RLDonkeycar code has been changed to use version 18.9 that supports a closed-circuit donkeycar track and the ability to reset the car.  The drive script has been modified to accept running the following at the ControlPi: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8527977259513921
      ],
      "excerpt": "To run the RL code on the RLPi or with the simulator run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8156790782911917
      ],
      "excerpt": "Otherwise, you need to bump up the OPTFLOWTHRESH: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8849382175042902
      ],
      "excerpt": "To run on your laptop with the simulation, you need to change the following params: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8024843022069515
      ],
      "excerpt": "* python manage.py drive --model rl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "* python RLPi.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/downingbots/RLDonkeycar/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "JavaScript",
      "HTML",
      "CSS"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 Will Roscoe\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "RLDonkeycar",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "RLDonkeycar",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "downingbots",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/downingbots/RLDonkeycar/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 15,
      "date": "Tue, 28 Dec 2021 03:45:20 GMT"
    },
    "technique": "GitHub API"
  }
}