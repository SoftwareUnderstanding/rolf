{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.08566",
      "https://arxiv.org/abs/609.02907v4, 2017.</li>\n<li>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint https://arxiv.org/abs/1704.01212, 2017.</li>\n<li>Vinyals, Oriol, Bengio, Samy, and Kudlur, Manjunath. Order matters: Sequence to sequence for sets. arXiv preprint https://arxiv.org/abs/1511.06391, 2015.</li> \n<li>Kristof Sch\u00fctt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Rober M\u00fcller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems (NIPS) 30, pages 992\u20131002. Curran Associates, Inc., https://arxiv.org/abs/ 1706.08566v5, 2017.</li>\n<li>Predicting Molecular Properties. https://www.kaggle.com/c/champs-scalar-coupling/data</li>\n</ol>\n\n\n\n",
      "https://arxiv.org/abs/1704.01212, 2017.</li>\n<li>Vinyals, Oriol, Bengio, Samy, and Kudlur, Manjunath. Order matters: Sequence to sequence for sets. arXiv preprint https://arxiv.org/abs/1511.06391, 2015.</li> \n<li>Kristof Sch\u00fctt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Rober M\u00fcller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems (NIPS) 30, pages 992\u20131002. Curran Associates, Inc., https://arxiv.org/abs/ 1706.08566v5, 2017.</li>\n<li>Predicting Molecular Properties. https://www.kaggle.com/c/champs-scalar-coupling/data</li>\n</ol>\n\n\n\n",
      "https://arxiv.org/abs/1511.06391, 2015.</li> \n<li>Kristof Sch\u00fctt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Rober M\u00fcller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems (NIPS) 30, pages 992\u20131002. Curran Associates, Inc., https://arxiv.org/abs/ 1706.08566v5, 2017.</li>\n<li>Predicting Molecular Properties. https://www.kaggle.com/c/champs-scalar-coupling/data</li>\n</ol>\n\n\n\n",
      "https://arxiv.org/abs/ 1706.08566v5, 2017.</li>\n<li>Predicting Molecular Properties. https://www.kaggle.com/c/champs-scalar-coupling/data</li>\n</ol>\n\n\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<ol type=\"disc\">\n<li>Sun, M., Zhao, S., Gilvary, C., Elemento, O., Zhou, J., & Wang, F. (2019). Graph convolutional networks for computational drug development and discovery. Briefings in Bioinformatics, 21(3), 919-935. doi:10.1093/bib/bbz042</li>\n<li>Kipf, Thomas N. and Welling, Max. Semi-Supervised Classification with Graph Convolutional Networks. International Conference on Learning Representations (ICLR), arXiv:609.02907v4, 2017.</li>\n<li>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.</li>\n<li>Vinyals, Oriol, Bengio, Samy, and Kudlur, Manjunath. Order matters: Sequence to sequence for sets. arXiv preprint arXiv:1511.06391, 2015.</li> \n<li>Kristof Sch\u00fctt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Rober M\u00fcller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems (NIPS) 30, pages 992\u20131002. Curran Associates, Inc., arXiv: 1706.08566v5, 2017.</li>\n<li>Predicting Molecular Properties. https://www.kaggle.com/c/champs-scalar-coupling/data</li>\n</ol>\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9593299683604384
      ],
      "excerpt": " | 0.10 | -1.32 |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "| Atom ID | Predicted Scalar Coupling Constant |  Actual Scalar Coupling Constant |               \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9243188970772274,
        0.8955886365383559
      ],
      "excerpt": " | 239 | -12.34 | -12.09 | \n | 240 | -11.36 | -11.28 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jmg764/Molecular-Scalar-Coupling-Constant-Prediction-using-SchNet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-24T01:23:56Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-14T20:09:08Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Many of the deep learning models designed to aid in drug discovery show improvement over traditional machine learning methods, but are limited due to two main reasons: first, they rely on hand-crafted features which prevents structural information to be learned directly from raw inputs, and, second, the existing architectures are not conducive for use on structured data such as molecules. Extraction of relevant features from images have already proven highly successful using convolutional neural networks (CNNs). Molecules can be represented as fully connected graphs in which atoms and bonds can be represented as nodes and edges, respectively. Graphs are irregularly shaped thereby making CNNs, which rely on convolution on regular grid-like structures, unsuitable for feature extraction [1]. \n\nEfforts have been made to generalize the convolution operation for graphs, resulting in the development of graph convolutional neural networks (GCNs). As Kipf and Welling describe in their seminal paper [2], the idea behind graph convolutional neural networks (GCNs), as shown in Fig. 1, is to perform convolutions on a graph by aggregating (through sum, average, etc) each node\u2019s neighborhood feature vectors. This new aggregated vector is then passed through a neural network layer, and the output is the new vector representation of the node. Additional neural network layers repeat this same process, except the input is the updated vectors from the first layer. \n\n<p align=\"center\">\n  <img src=\"images/Figure 1.png\"  alt=\"drawing\" width=\"600\"/>\n</p>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The drug discovery process is one of the most challenging and expensive endeavors in biomedicine. While there are about <img src=\"https://render.githubusercontent.com/render/math?math=10^56\"> atoms in the solar system, there are about <img src=\"https://render.githubusercontent.com/render/math?math=10^60\"> chemical compounds with drug-like features that can be made. Since it is unfeasible for chemists to synthesize and evaluate every molecule, they\u2019ve grown to rely on virtual screening to narrow down promising candidates. However, the challenge of searching this almost infinite space of potential molecules is the perfect substrate for deep learning techniques to improve the drug discovery process even further. While the growing number of large datasets for molecules has already enabled the creation of several useful models, the application of deep learning to drug discovery is still in its infancy. Some useful predictions that could expedite drug discovery include toxicity, ability to bind with a given protein, and quantum properties.\n\nResearchers commonly use Nuclear Magnetic Resonance (NMR) to gain insight into a molecule\u2019s structure and dynamics. NMR\u2019s functionality largely depends on its ability to accurately predict scalar couplings which are the strength of magnetic interactions between pairs of atoms in a given molecule. It is possible to compute scalar couplings on an inputted 3D molecular structure using advanced quantum mechanical simulation methods such as Density Functional Theory (DFT) which approximate Schr\u00f6dinger\u2019s equation. However, these methods are limited by their high computational cost, and are therefore reserved for use on small systems, or other, less approximate, methods are adopted instead. My goal for this project was to develop a fast, reliable, and cheaper method to perform this task through the use of a graph convolutional neural network (GCN). In particular, I focused on implementing and optimizing SchNet:\u00a0a novel GCN that has been shown to achieve state-of-the-art performance on quantum chemical property benchmarks. As a byproduct, I hoped to learn more about GCNs and how they could be used for chemical applications.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9907343556489528,
        0.9970945877305479,
        0.9957994495191231
      ],
      "excerpt": "This final project for the CS-GY-9223 Deep Learning course at NYU Tandon implements SchNet, based on the paper by Sch\u00fctt et al., for prediction of molecular scalar coupling constants. \nIn 2017, Gilmer et al. [3] released a paper focusing on the specific use of neural networks for predicting quantum properties of molecules. They noted that the symmetries of atomic systems require graph neural networks that are invariant to graph isomorphism, and therefore reformulated existing models that fall into this category, including Kipf and Welling\u2019s GCN, into a common framework called Message Passing Neural Networks (MPNNs). The \u201cmessage passing\u201d refers to the aggregation of neighborhood vector features described earlier. The MPNN that Gilmer et al. developed, called enn-s2s, managed to achieve state-of-the-art performance on an important molecular property benchmark using QM9: a dataset consisting of 130k molecules with 13 properties for each molecule as approximated by DFT. The neighborhood messages generated used both bond types and interatomic distances followed by a set2set model from Vinyals et al. [4].  \nLater on, Schutt et al. pointed out that enn-s2s was limited by the fact that atomic positions are discretized, and therefore the filter learned was also discrete which rendered it incapable of capturing the gradual positional changes of atoms [5]. In order to remedy this, Schutt et al. proposed a different method of graph convolution with continuous filters that mapped an atomic position to a corresponding filter value. This is advantageous in that it doesn't require atomic position data to lie on a grid, thereby resulting in smooth, rather than discrete energy predictions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8560428656334221,
        0.981611016785341,
        0.8430836021112528
      ],
      "excerpt": "SchNet demonstrated superior performance over enn-s2s in predicting molecular energies and atomic forces on three different datasets. Fig. 3 provides an overview of the SchNet architecture. Molecules input into the model can be uniquely represented by a certain set of nuclear charges <img src=\"https://render.githubusercontent.com/render/math?math=Z = (Z_1, ..., Z_n)\"> and atomic positions <img src=\"https://render.githubusercontent.com/render/math?math=R = ({\\rm r}_1, ... , {\\rm r}_n)\"> where n is the number of atoms. At each layer, the atoms in a given molecule are represented as a tuple of features: <img src=\"https://render.githubusercontent.com/render/math?math=X^l = ({\\rm x}^l_1, ..., {\\rm x}_n^l)\"> with <img src=\"https://render.githubusercontent.com/render/math?math={\\rm x}_i^l \\in \\mathbb R^F\"> where <img src=\"https://render.githubusercontent.com/render/math?math=l\"> and <img src=\"https://render.githubusercontent.com/render/math?math=F\"> are the number of layers, and feature maps, respectively. This representation is analogous to pixels in an image. In the embedding layer, the representation of each atom  is initialized at random using an embedding dependent on the atom type <img src=\"https://render.githubusercontent.com/render/math?math=Z_i\"> which is optimized during training: <img src=\"https://render.githubusercontent.com/render/math?math=x^0_i = {\\rm a}_{Z_i}\"> where <img src=\"https://render.githubusercontent.com/render/math?math=a_Z\"> is the atom type embedding.  \nAtom-wise layers, a recurring building block in this architecture, are dense layers that are applied to each representation <img src=\"https://render.githubusercontent.com/render/math?math={\\rm x}_i^{l}\"> of atom. These layers are responsible for the recombination of feature maps with shared weights across all atoms which allows the architecture to be scaled with respect to the size of the molecule.  \nInteractions between atoms are modeled by three interaction blocks: as shown above, the sequence of atom-wise, interatomic continuous-filter convolution (cfconv), and two more atom-wise layers separated by a softplus non-linearity produces <img src=\"https://render.githubusercontent.com/render/math?math={\\rm v}_i^l\">. The cfconv layer uses a radial basis function that acts as a continuous filter generator. Additionally, the residual connection between <img src=\"https://render.githubusercontent.com/render/math?math=({\\rm x}^l_1, ..., {\\rm x}_n^l)\"> and <img src=\"https://render.githubusercontent.com/render/math?math=({\\rm v}^l_1, ..., {\\rm v}_n^l)\"> allows for the incorporation of interactions between atoms and previously computed feature maps.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8998247141941546
      ],
      "excerpt": "I used the CHAMPS Scalar Coupling dataset which was provided for a Kaggle competition with a similar objective [6], and consists of the following:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8277479918150197,
        0.8742234362073426
      ],
      "excerpt": "  <li> train.csv \u2014 the training set which contains four columns: (1) the name of the molecule where the coupling constant originates, (2) and (3) the atom indices of the atom-pair which create the coupling, (4) the scalar coupling type, (5) the scalar coupling constant that we want to predict.</li>  \n  <li> scalar_coupling_contributions.csv \u2014 the scalar coupling constants in train.csv are a sum of four terms: Fermi contact, spin-dipolar, paramagnetic spin-orbit, and diamagnetic spin-orbit contributions which are contained in this file. It is organized into the following columns: (1) molecule name; (2) and (3) the atom indices of each atom-pair; (4) the type of coupling; and (5), (6), (7), and (8) are the four aforementioned terms.</li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9352853169515112
      ],
      "excerpt": "structures.csv contains cartesian coordinates, so additional preprocessing was required in order to create graph representations of each molecule. In order to do so, I created a Graph class whose objects store the distances between each atom and an adjacency matrix. The fully processed train, validation, and test datasets for input into SchNet were dictionaries consisting of Graph objects for each molecule and associated scalar coupling contributions for each atom pair within that molecule. As an example of how each molecule is represented graphically, Fig. 4 displays the first molecule in the dataset, CH4 (methane). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8035227517750994,
        0.8030103565334145,
        0.9971393758522281
      ],
      "excerpt": "I constructed, trained, evaluated, and optimized the SchNet model using the Chainer Chemistry library which uses the same architecture described by Schutt et al. In addition, however, I added batch normalization using Chainer Chemistry\u2019s GraphBatchNormalization.  \nThe loss function used is log mean absolute error (Log MAE): <img src=\"https://render.githubusercontent.com/render/math?math={\\rm loss} = \\frac{1}{T} \\sum_{t=1}^{T} {\\rm log} \\Big (\\frac{1}{n_t} \\sum_{i=1}^{n_t} |y_i - \\hat y_i| \\Big )\"> where <img src=\"https://render.githubusercontent.com/render/math?math=T\"> is the number of scalar coupling types, <img src=\"https://render.githubusercontent.com/render/math?math=n_t\"> is the number of observations of type <img src=\"https://render.githubusercontent.com/render/math?math=t\">, <img src=\"https://render.githubusercontent.com/render/math?math=y_i\"> is the actual scalar coupling constant for the observation, and <img src=\"https://render.githubusercontent.com/render/math?math=\\hat y_i\"> is the predicted scalar coupling constant for the observation. It is calculated for each scalar coupling type, and then averaged across tips, so that a 1% decrease in error for one type provides the same improvement in score as a decrease for another type. \nI tested out a variety of hyperparameter configurations in order to optimize the model; each of them used Adam optimization with a batch size of 4 over the course of 25 epochs. I was particularly focused on discovering the optimal radial basis function hyperparameters within the cfconv layer. The radial basis function is expressed as <img src=\"https://render.githubusercontent.com/render/math?math=e_k({\\bf r}_i - {\\bf r}_j) = {\\rm exp}(-\\gamma \\|d_{ij} - \\mu_k \\|^2)\"> located at centers 0\u00c5 \u2264 \u00b5_k \u2264 30\u00c5 every 0.1\u00c5 with \u03b3 = 10\u00c5, and 300 is the interatomic distance used as input for the filter network. This translates to hyperparameter values num_rbf=300, radius_resolution=0.1, and gamma=10.0 in Chainer Chemistry implementation of cfconv. In general, these values are chosen such that all distances occurring in the datasets are covered by the cfconv filters. Choosing fewer centers corresponds to reducing the resolution of the filter, while restricting the range of the centers corresponds to the filter size in a usual convolutional layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9633453691908666
      ],
      "excerpt": "Using this optimal learning rate, I then ran the model using different values of radius resolution, and found that the lowest validation Log MAE was achieved with a radius resolution of 0.10: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9236231293166791
      ],
      "excerpt": "Below is a sample of the 1,400,457 predicted scalar coupling constants predicted on the test dataset using these optimal learning rate and radius resolution values: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This final project for the CS-GY-9223 Deep Learning course at NYU Tandon implements SchNet, based on the paper by Sch\u00fctt et al., for prediction of molecular scalar coupling constants",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jmg764/Molecular-Scalar-Coupling-Constant-Prediction-using-SchNet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 16:30:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jmg764/Molecular-Scalar-Coupling-Constant-Prediction-using-SchNet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jmg764/Molecular-Scalar-Coupling-Constant-Prediction-using-SchNet",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jmg764/Molecular-Scalar-Coupling-Constant-Prediction-using-SchNet/main/Molecular_Scalar_Coupling_Constant_Prediction_using_SchNet.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.9100184412152189
      ],
      "excerpt": "  <img src=\"images/Figure 2.png\"  alt=\"drawing\" width=\"500\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9100184412152189
      ],
      "excerpt": "  <img src=\"images/Figure 3.png\"  alt=\"drawing\" width=\"600\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9100184412152189
      ],
      "excerpt": "  <img src=\"images/Figure 4.png\"  alt=\"drawing\" width=\"325\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8702815774991463
      ],
      "excerpt": " | 237 | 1.89 | 1.45 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8702815774991463
      ],
      "excerpt": " | 243 | 8.45 | 8.26 | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jmg764/Molecular-Scalar-Coupling-Constant-Prediction-using-SchNet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Molecular Scalar Coupling Constant Prediction using SchNet",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Molecular-Scalar-Coupling-Constant-Prediction-using-SchNet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jmg764",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jmg764/Molecular-Scalar-Coupling-Constant-Prediction-using-SchNet/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 16:30:19 GMT"
    },
    "technique": "GitHub API"
  }
}