{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this package useful in your research, please consider citing:\n\n    @article{mao2014deep,\n      title={Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)},\n      author={Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},\n      journal={ICLR},\n      year={2015}\n    }\n    \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Created by Junhua Mao\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{mao2014deep,\n  title={Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)},\n  author={Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},\n  journal={ICLR},\n  year={2015}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mjhucla/mRNN-CR",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2015-06-08T05:24:34Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-11T13:35:32Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This package provides a simple way to boost the performance of image captioning task by reranking the hypotheses sentences according to the captions of nearest neighbor images in the training set.\nWe denote this method as consensus reranking for the rest of this document.\n\nIt also provides images features (both refined by the [m-RNN model](www.stat.ucla.edu/~junhua.mao/m-RNN.html) and the original [VGG feature](http://arxiv.org/abs/1409.1556) ) on MS COCO Train2014, Val2014, and Test2014 dataset. We take the [m-RNN model](www.stat.ucla.edu/~junhua.mao/m-RNN.html) as an example to generate the hypotheses descriptions of an image.\n\nThe details is described in Section 8 of the latest version of the m-RNN paper: [Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)](http://arxiv.org/abs/1412.6632).\nThe work is inspired by the [nearest neighbor captions retrieval method](http://arxiv.org/abs/1505.04467).\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mjhucla/mRNN-CR/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 42,
      "date": "Thu, 23 Dec 2021 07:16:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mjhucla/mRNN-CR/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mjhucla/mRNN-CR",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mjhucla/mRNN-CR/master/cr_mRNN_demo.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mjhucla/mRNN-CR/master/download_mRNN_cache.sh",
      "https://raw.githubusercontent.com/mjhucla/mRNN-CR/master/download_VGG_orignial_feature.sh",
      "https://raw.githubusercontent.com/mjhucla/mRNN-CR/master/setup.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. install [MS COCO caption toolkit](https://github.com/tylin/coco-caption)\n\n2. Suppose that toolkit is install on $PATH_COCOCap and this package is install at $PATH_mRNN_CR. Create a soft link to COCOCap as follows:\n  ```Shell\n  cd $PATH_mRNN_CR\n  ln -sf ./external/coco-caption $PATH_COCOCap\n  ```\n  \n3. Setup the package (will download necessary data, including refined features of m-RNN, ~1.5G)\n  ```Shell\n  bash setup.sh\n  ```\n  \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8570400487975717
      ],
      "excerpt": "run download_VGG_orignial_feature.sh to get the extracted feature from original VGGnet on MS COCO train, val and test2014 (totally ~2.1G) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8306721212568767
      ],
      "excerpt": "run download_mRNN_cache.sh to get the consensus reranking sample results on MS COCO crVal set (should be the same after running cr_mRNN_demo.py) and the nearest images index for MS COCO test 2014. (totally ~1.1G) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mjhucla/mRNN-CR/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Nearest Neighbor as Reference: A Simple Way to Boost the Performance of Image Captioning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "mRNN-CR",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mjhucla",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mjhucla/mRNN-CR/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- python 2.7 (Need ackages of numpy, scipy, nltk. All included in [Anaconda](https://store.continuum.io/cshop/anaconda/))\n- java 1.8.0\n- [MS COCO caption toolkit](https://github.com/tylin/coco-caption)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 76,
      "date": "Thu, 23 Dec 2021 07:16:14 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. install [MS COCO caption toolkit](https://github.com/tylin/coco-caption)\n\n2. Suppose that toolkit is install on $PATH_COCOCap and this package is install at $PATH_mRNN_CR. Create a soft link to COCOCap as follows:\n  ```Shell\n  cd $PATH_mRNN_CR\n  ln -sf ./external/coco-caption $PATH_COCOCap\n  ```\n  \n3. Setup the package (will download necessary data, including refined features of m-RNN, ~1.5G)\n  ```Shell\n  bash setup.sh\n  ```\n  \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Run cr_mRNN_demo.py or view cr_mRNN_demo.ipynb\n\nYou can set up the file path, dir or hyperparamters in the script here: ./concensus_reranking_utils/conf_cr.py\n\n",
      "technique": "Header extraction"
    }
  ]
}