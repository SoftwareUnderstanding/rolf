{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1804.00062",
      "https://arxiv.org/abs/1810.11714}\n}\n```\n\n[![Training Frankenstein's Creature To Stack: HyperTree Architecture Search](https://img.youtube.com/vi/1MV7slHnMX0/0.jpg",
      "https://arxiv.org/abs/1804.00062",
      "https://arxiv.org/abs/1810.11714}\n}\n```"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[Visual Robot Task Planning](https://arxiv.org/abs/1804.00062)\n\n```\n@article{paxton2018visual,\n  author    = {Chris Paxton and\n               Yotam Barnoy and\n               Kapil D. Katyal and\n               Raman Arora and\n               Gregory D. Hager},\n  title     = {Visual Robot Task Planning},\n  journal   = {ArXiv},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1804.00062},\n  archivePrefix = {arXiv},\n  eprint    = {1804.00062},\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-00062},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n[Training Frankenstein's Creature To Stack: HyperTree Architecture Search](https://sites.google.com/view/hypertree-renas/home)\n\n```\n@article{hundt2018hypertree,\n    author = {Andrew Hundt and Varun Jain and Chris Paxton and Gregory D. Hager},\n    title = \"{Training Frankenstein's Creature to Stack: HyperTree Architecture Search}\",\n    journal = {ArXiv},\n    archivePrefix = {arXiv},\n    eprint = {1810.11714},\n    year = 2018,\n    month = Oct,\n    url = {https://arxiv.org/abs/1810.11714}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{hundt2018hypertree,\n    author = {Andrew Hundt and Varun Jain and Chris Paxton and Gregory D. Hager},\n    title = \"{Training Frankenstein's Creature to Stack: HyperTree Architecture Search}\",\n    journal = {ArXiv},\n    archivePrefix = {arXiv},\n    eprint = {1810.11714},\n    year = 2018,\n    month = Oct,\n    url = {https://arxiv.org/abs/1810.11714}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{paxton2018visual,\n  author    = {Chris Paxton and\n               Yotam Barnoy and\n               Kapil D. Katyal and\n               Raman Arora and\n               Gregory D. Hager},\n  title     = {Visual Robot Task Planning},\n  journal   = {ArXiv},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1804.00062},\n  archivePrefix = {arXiv},\n  eprint    = {1804.00062},\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-00062},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{hundt2019costar,\n    title={The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints},\n    author={Andrew Hundt and Varun Jain and Chia-Hung Lin and Chris Paxton and Gregory D. Hager},\n    journal = {Intelligent Robots and Systems (IROS), 2019 IEEE International Conference on},\n    year = 2019,\n    url = {https://arxiv.org/abs/1810.11714}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8317916609503867
      ],
      "excerpt": "Code for the paper Visual Robot Task Planning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9713679853829249
      ],
      "excerpt": "CTP Visual: visual robot task planner \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jhu-lcsr/costar_plan",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This code is maintained by:\n\n - Chris Paxton (cpaxton@jhu.edu).\n - Andrew Hundt (ATHundt@gmail.com)\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-02-19T01:14:13Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T16:00:10Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9684316776644925,
        0.8097803792730421,
        0.9498907868446198
      ],
      "excerpt": "CoSTAR Plan is for deep learning with robots divided into two main parts: The CoSTAR Task Planner (CTP) library and CoSTAR Hyper. \nCode for the paper Visual Robot Task Planning. \nCode for the paper The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9953648705476995,
        0.9815434673719748,
        0.9903381151913878
      ],
      "excerpt": "The CoSTAR Planner is part of the larger CoSTAR project. It integrates some learning from demonstration and task planning capabilities into the larger CoSTAR framework in different ways. \nSpecifically it is a project for creating task and motion planning algorithms that use machine learning to solve challenging problems in a variety of domains. This code provides a testbed for complex task and motion planning search algorithms. \nThe goal is to describe example problems where the actor must move around in the world and plan complex interactions with other actors or the environment that correspond to high-level symbolic states. Among these is our Visual Task Planning project, in which robots learn representations of their world and use these to imagine possible futures, then use these for planning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9840705057768145
      ],
      "excerpt": "About this repository: CTP is a single-repository project. As such, all the custom code you need should be in one place: here. There are exceptions, such as the CoSTAR Stack for real robot execution, but these are generally not necessary. The minimal installation of CTP is just to install the costar_models package as a normal python package ignoring everything else. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8655854116367668,
        0.8906128909950556
      ],
      "excerpt": "Early version, deprecated in lieu of the full CoSTAR Block Stacking Dataset. \n0. Introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095237712429106,
        0.8521097922978089
      ],
      "excerpt": "3.1 Data collection: data collection with a real or simulated robot \n3.2 MARCC instructions: learning models using JHU's MARCC cluster \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.987679341475395,
        0.8142643739006004
      ],
      "excerpt": "3.4 SLURM Utilities: tools for using slurm on MARCC \n4. Creating and training a custom task: overview of task representations \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.80882306474767
      ],
      "excerpt": "4.2 Task Learning: specific details \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8466300655299859
      ],
      "excerpt": "5.2 PyBullet Sim: an alternative to Gazebo that may be preferrable in some situations \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8833405926758174
      ],
      "excerpt": "5.4 Adding a robot to the ROS code: NOT using Bullet sim \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.917072694627068,
        0.9520187130394837
      ],
      "excerpt": "7.1 TOM Data: data necessary for TOM \n7.2 The Real TOM: details about parts of the system for running on the real TOM \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9749418049955361,
        0.8874502475897079
      ],
      "excerpt": "CoSTAR Models: tools for learning deep neural networks \nCTP Tom: specific bringup and scenarios for the TOM robot from TU Munich \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8055008574942305
      ],
      "excerpt": "slurm: contains SLURM scripts for running on MARCC \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.98789581305059
      ],
      "excerpt": "docs: markdown files for information that is not specific to a particular ROS package but to all of CTP \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9273898117807865
      ],
      "excerpt": "learning_planning_msgs: ROS messages for data collection when doing learning from demonstration in ROS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Integrating learning and task planning for robots with Keras, including simulation, real robot, and multiple dataset support.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jhu-lcsr/costar_plan/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 22,
      "date": "Tue, 28 Dec 2021 00:48:18 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jhu-lcsr/costar_plan/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jhu-lcsr/costar_plan",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/jhu-lcsr/costar_plan/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/tests.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/setup/travis_setup.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/setup/reset_ros.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/setup/setup_marcc.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/setup/setup_indigo.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/setup/python3.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/setup/experimental.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/setup/init_marcc.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/setup/setup_kinetic.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/setup/setup_tom.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/setup/reinstall_ros_indigo.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/setup/setup_indigo_test.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/setup/setup_docker.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/setup/experimental_travis_tests.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/husky/husky/husky_description/env-hooks/50.husky_description.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/husky/husky_simulator/husky_gazebo/env-hooks/50.husky_gazebo.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/costar_simulation/robot/transformURDF.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/costar_simulation/robot/jaco_description/transformURDF.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/compare_suturing.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/compare_stack.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/ctp_secondary.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/ctp_bn.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/compare_husky.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/stack_policy.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/ctp_train_image.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/start_secondary.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/purge_pngs.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/ctp_actor.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/ctp_husky.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/start_ctp_gans.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/ctp.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/all.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/start_ctp_suturing.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/start_ctp_husky.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/start_policies.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/run_husky_comparison.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/start_ctp_predictors.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/start_multi_actor.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/pretrain_sampler.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/start_ctp_stacks.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/pretrain_image.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/ctp_predictor.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/slurm_utils.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/ctp_conditional_image_gan.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/ctp_gan.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/multi_actor.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/ctp_suturing.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/stack_no_disc.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/start_actors.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/ff.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/old_experiments/compare_batchnorm.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/old_experiments/start_ctp_conditional_image_gan.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/slurm/old_experiments/run_comparison.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/multi_conditional_image_gan.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/jigsaws_goal_discriminator.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/jigsaws_pretrain_image_no_ros.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/husky_discriminator.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/jigsaws_discriminator.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/run_multi_conv_lstm.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/hidden_husky.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/switch_to_saved_weights_goal_sampler.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/multi_goal_discriminator.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/run_multi_lstm.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/run_multi_tcn.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/jigsaws_pretrain_image_gan.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/husky_pretrain_image.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/switch_to_saved_weights.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/train_multi_ff.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/husky_goal_discriminator.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/multi_secondaries_no_ros.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/husky_policy.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/multi_pretrain_sampler.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/multi_pretrain_image_gan.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/jigsaws_conditional_image.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/collect_stack1_goals.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/train_multi_conv_lstm.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/switch_stack.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/learn_predictor.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/jigsaws_pretrain_image.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/train_multi_tcn.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/multi_discriminator.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/multi_pose.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/multi_secondaries.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/jigsaws_conditional_image_gan.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/train_multi_lstm.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/husky_secondaries_no_ros.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/run_multi_ff.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/husky_conditional_image.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/commands/husky_pretrain_image_gan.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/costar_hyper/single_prediction_fine_tune.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/costar_hyper/cornell_hyperopt.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/costar_hyper/pixelwise_single_prediction_grasp_vrep.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/costar_hyper/single_prediction_grasp_train.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/costar_hyper/cornell_grasp_train_regression.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/costar_hyper/keras_distributed_single_prediction_grasp_train.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/costar_hyper/pixelwise_prediction_grasp_train.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/costar_hyper/cornell_grasp_classification_fine_tune.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/costar_hyper/test_pixelwise_attempt_train.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/costar_hyper/pixelwise_single_prediction_grasp_pretrain.sh",
      "https://raw.githubusercontent.com/jhu-lcsr/costar_plan/master/costar_hyper/test_single_attempt_train.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.800643409234543
      ],
      "excerpt": "To run deep learning examples, you will need TensorFlow and Keras, plus a number of Python packages. To run robot experiments, you'll need a simulator (Gazebo or PyBullet), and ROS Indigo or Kinetic. Other versions of ROS may work but have not been tested. If you want to stick to the toy examples, you do not need to use this as a ROS package. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8658272931085065
      ],
      "excerpt": "PyBullet Block Stacking download tar.gz \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.912778786951646
      ],
      "excerpt": "1. Installation Guide \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8076215463769565
      ],
      "excerpt": "3.2 MARCC instructions: learning models using JHU's MARCC cluster \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8532274961852876
      ],
      "excerpt": "setup: contains setup scripts \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8537958765462417
      ],
      "excerpt": "5.3 costar_bullet quick start: How to run tasks, generate datasets, train models, and extend costar_bullet with your own components. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8224807774569491
      ],
      "excerpt": "photos: example images \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jhu-lcsr/costar_plan/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "Shell",
      "CMake",
      "Lua"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright 2017-2018, Chris Paxton, Andrew Hundt, Yotam Barnoy, Ding Yu, The Johns Hopkins University. All rights reserved.\\n\\n\\nApache License\\n\\nVersion 2.0, January 2004\\n\\nhttp://www.apache.org/licenses/\\n\\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n1. Definitions.\\n\\n\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.\\n\\n\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.\\n\\n\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.\\n\\n\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\\n\\n\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\\n\\n\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).\\n\\n\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.\\n\\n\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.\\n\\n2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\\n\\n3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.\\n\\n4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\\n\\nYou must give any other recipients of the Work or Derivative Works a copy of this License; and\\nYou must cause any modified files to carry prominent notices stating that You changed the files; and\\nYou must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and\\nIf the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.\\n\\nYou may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.\\n5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.\\n\\n6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.\\n\\n7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.\\n\\n8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\\n\\n9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.\\n\\nEND OF TERMS AND CONDITIONS\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "CoSTAR Plan",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "costar_plan",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jhu-lcsr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jhu-lcsr/costar_plan/blob/master/Readme.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "ahundt",
        "body": "\r\n### [CoSTAR Hyper](https://github.com/jhu-lcsr/costar_plan/tree/master/costar_hyper)\r\n\r\nCode for the paper [Training Frankenstein's Creature To Stack: HyperTree Architecture Search](https://sites.google.com/view/hypertree-renas/home).\r\n\r\n[![Training Frankenstein's Creature To Stack: HyperTree Architecture Search](https://img.youtube.com/vi/1MV7slHnMX0/0.jpg)](https://youtu.be/1MV7slHnMX0 \"Training Frankenstein's Creature To Stack: HyperTree Architecture Search\")\r\n\r\nDataset collection code and integration with the [CoSTAR Block Stacking Dataset](https://sites.google.com/site/costardataset).\r\n![2018-06-21-23-21-49_example000004 success_tiled](https://user-images.githubusercontent.com/55744/47169252-ff1e3380-d2d0-11e8-97ed-1d747d97ea11.jpg)\r\n\r\n[HyperTree Architecture Search Code and Plotting Utilities](https://github.com/jhu-lcsr/costar_plan/tree/master/costar_hyper)\r\n![2018_10_18 hypertree 1 epoch comparison](https://user-images.githubusercontent.com/55744/47176379-778df000-d2e3-11e8-800d-f3d62540781a.png)\r\n\r\n[HyperTree Pretrained weights](https://github.com/ahundt/costar_dataset/releases) are available for download.\r\n\r\nThe attached binary files are helpful with viewing the google grasping dataset with V-REP as detailed in [CoSTAR Hyper](https://github.com/jhu-lcsr/costar_plan/tree/master/costar_hyper).\r\n\r\n### Costar Task Planning (CTP)\r\n\r\nThere have been some updates so that the [CoSTAR Block Stacking Dataset](https://sites.google.com/site/costardataset) can be loaded.",
        "dateCreated": "2018-10-19T03:29:27Z",
        "datePublished": "2018-10-18T19:35:06Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.7.1",
        "name": "CoSTAR Plan Initial Public Release",
        "tag_name": "v0.7.1",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.7.1",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/13513431",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.7.1"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "Adds support for training the new conditional image models on CoSTAR data, including new tools for saving data. Some refactoring has been done, and we have deleted more dead code.\r\n\r\nThe `simdata.tar.gz` file contains the entire dataset used to train our conditional image models on the simulated block stacking task with obstacle avoidance. Now anyone should be able to reproduce results from our papers.",
        "dateCreated": "2018-03-30T19:10:16Z",
        "datePublished": "2018-03-30T19:12:22Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.6.0",
        "name": "CoSTAR data support",
        "tag_name": "v0.6.0",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.6.0",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/10336594",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.6.0"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "Adding a bunch of fixes including more testing options and an improved planning script",
        "dateCreated": "2018-02-23T02:57:41Z",
        "datePublished": "2018-02-23T02:58:34Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.5.4",
        "name": "Script changes, GAN fixes, and more",
        "tag_name": "v0.5.4",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.5.4",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/9800252",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.5.4"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "Changes:\r\n  - batchnorm now replaced with instance normalization\r\n  - added extra features and layers\r\n  - added permanent dropout - not currently used but can help GANs\r\n",
        "dateCreated": "2018-02-16T18:44:25Z",
        "datePublished": "2018-02-16T18:45:28Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.5.3",
        "name": "Updates to visual task planning models",
        "tag_name": "v0.5.3",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.5.3",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/9712896",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.5.3"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "So this version is working very nicely for our simulated tasks, but not as nicely for surgical robots -- so we're adding in multiple hypothesis support.",
        "dateCreated": "2018-01-25T22:07:20Z",
        "datePublished": "2018-01-25T22:11:00Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.5.2",
        "name": "MHP support; beautiful pictures for the simulation task",
        "tag_name": "v0.5.2",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.5.2",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/9393551",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.5.2"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "We added support for the conditional GANs described in the paper [Image to Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004). We can use this for learning a representation space for our predictive models or for the predictions themselves.",
        "dateCreated": "2018-01-11T02:02:07Z",
        "datePublished": "2018-01-11T02:04:09Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.5.1",
        "name": "GAN implementation for CTP goal prediction",
        "tag_name": "v0.5.1",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.5.1",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/9189109",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.5.1"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "This is the first release with a functioning version of the CTP task parser. Given a set of labeled data, this builds probabilistic skill models (including dynamic movement primitives) and uses them to construct a whole graph of various actions.\r\n\r\nImportant notes:\r\n  - `ctp_tom` includes code and scenarios for the TOM robot at ICS\r\n  - `ctp_tom` contains the `parse.py` tool, which lets you try out the parser on multiple ROS bags",
        "dateCreated": "2017-12-15T08:37:51Z",
        "datePublished": "2017-12-15T08:40:38Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.5.0",
        "name": "Task Parser",
        "tag_name": "v0.5.0",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.5.0",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/8918505",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.5.0"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "This release contains a preliminary version of our proposed predictor network architecture for predictive modeling of task structure. You can train such a model end to end via a command such as:\r\n\r\n```\r\nrosrun costar_models ctp_model_tool \\\r\n  --data_file rpy.npz \\\r\n  --model predictor \\\r\n  -e 1000 \\\r\n  --features multi \\\r\n  --batch_size 24  \\\r\n  --optimizer adam \\\r\n  --lr 0.001 \\\r\n  --upsampling conv_transpose \\\r\n  --use_noise true \\\r\n  --noise_dim 32  \\\r\n  --steps_per_epoch 300 \\\r\n  --dropout_rate 0.2 \\\r\n  --skip_connections 1\r\n```\r\nWhich constructs an end-to-end trainable version of the network that can predict 4 hypotheses.\r\n",
        "dateCreated": "2017-11-07T18:36:06Z",
        "datePublished": "2017-11-07T18:47:50Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.4.1",
        "name": "Predictor Net Updates",
        "tag_name": "v0.4.1",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.4.1",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/8420780",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.4.1"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "Predictor net performance has improved dramatically; they now see fairly low prediction error, and seem to get very nice results w.r.t. predicting future scenes and grasp poses, thanks to richer dense representations.",
        "dateCreated": "2017-10-26T20:37:19Z",
        "datePublished": "2017-10-26T20:38:54Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.4.0",
        "name": "Predictor Nets",
        "tag_name": "v0.4.0",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.4.0",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/8278556",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.4.0"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "This is the \"first pass\" version of the predictor neural nets, with nice, reliable data collection for the stacking task.\r\n\r\nData collection example:\r\n```\r\nrosrun costar_bullet start --robot ur5 --task stack1 --agent task -i 50 --features multi  --verbose --seed 0 --success_only  --cpu --save --data_file small.npz\r\n```\r\n\r\nLearning example:\r\n```\r\nrosrun costar_models ctp_model_tool --data_file small.npz --model predictor -e 1000 --features multi --batch_size 32  --load_model --optimizer adam\r\n```",
        "dateCreated": "2017-09-18T17:16:33Z",
        "datePublished": "2017-09-18T17:19:06Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.3.0",
        "name": "Predictor neural nets for planning",
        "tag_name": "v0.3.0",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.3.0",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/7795671",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "Finalizing neural net models and architectures for learning+planning experiments.",
        "dateCreated": "2017-08-13T22:37:47Z",
        "datePublished": "2017-08-13T22:38:51Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.2.2",
        "name": "Neural net execution and hierarchical models",
        "tag_name": "v0.2.2",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.2.2",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/7375517",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.2.2"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "Follow instructions in `docs/learning.md` to run a whole bunch of learning experiments and go over data generation using CTP.\r\n\r\nThis adds:\r\n  - autoencoder nets\r\n  - generative adversarial nets (GANs)\r\n  - various recurrent networks\r\n\r\nWe can train these things on images or whatever.",
        "dateCreated": "2017-07-21T16:46:45Z",
        "datePublished": "2017-07-21T16:48:21Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.2.1",
        "name": "Neural net training and setup",
        "tag_name": "v0.2.1",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.2.1",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/7126041",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.2.1"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "Turns the current sim tool into something that can be used for a variety of learning problems; adds the framework and options necessary to get these experiments moving.\r\n\r\nExample command:\r\n```\r\nrosrun costar_bullet start --robot ur5 --agent null --features multi --load --model ff_regression --learning_rate 0.0001 --optimizer adam\r\n```\r\n\r\n**NOTE THAT TRAVIS IS CURRENTLY BROKEN.** As of time of release, the Travis build was failing in master -- before any changes. This is likely because of an update on their end, or to the various ROS repositories on which we depend.",
        "dateCreated": "2017-07-12T22:49:46Z",
        "datePublished": "2017-07-12T22:54:48Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.2.0",
        "name": "Learning support",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.2.0",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/7021284",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "Create datasets containing joint state, gripper state, and actions. Supports ground truth data set generation for robot motion planning problems with neural nets. Currently only tested with the ur5.\r\n\r\nFixes bugs:\r\n- #96 timing now works properly\r\n\r\nAdds features:\r\n- #93 via generating trajectories of points to follow\r\n- #94 with added functions to robot interface to convert \"NONE\" actions appropriately\r\n- #95 via trajectory generation (again)\r\n- #47 create data sets for part of the blocks task\r\n\r\nExample command to run:\r\n```\r\nrosrun costar_bullet start --robot ur5 --task blocks --agent task --features multi --save -i 10\r\n```\r\nThis will create a data file `data.npz` containing 10 pick-and-place actions in the blocks task, storing joint states, images, and commanded goals.",
        "dateCreated": "2017-07-06T17:44:45Z",
        "datePublished": "2017-07-06T17:47:37Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.1.2",
        "name": "Data set generation and utilities",
        "tag_name": "v0.1.2",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.1.2",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/6954997",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.1.2"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "Final set of fixes for the task agent so that we can properly manipulate objects in different scenes. Data generation now works, for the most part.",
        "dateCreated": "2017-07-03T23:35:58Z",
        "datePublished": "2017-07-03T23:39:43Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.1.1",
        "name": "Manipulation Demo",
        "tag_name": "v0.1.1",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.1.1",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/6918578",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.1.1"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "This version comes with some tweaks to the Blocks task to allow our task agent to generate supervised training data. The goal of the task agent is to move to a world where we use effective, traditional RL techniques (or optimization in general) to generate training data for our newer models.\r\n\r\nThis fixes a couple issues, and also changes the way we define high-level \"Options\" that make up our task plans. More changes in the works.",
        "dateCreated": "2017-06-28T06:17:33Z",
        "datePublished": "2017-06-28T06:21:51Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.1.0",
        "name": "Task Agent for Blocks",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.1.0",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/6859293",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.1.0"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "This adds in the trajectory (dmp) optimization tool and a bunch of changes and improvements to feature and reward computation.\r\n\r\nIn particular, we now have distributions over parameterized policies, which can be useful when doing continuous-space planning or optimization.\r\n\r\nResolves issues:\r\n    #50 with the --save, --capture, --show_images flags\r\n    #40 with the optimize_tom_actions.py and optimize tools\r\n    #49 with the --randomize_colors flag (preliminary; may not work with current object textures)\r\n    #34 and #43 fixed data set to use joint states from data instead of guessing at fingertip offset\r\n\r\nCloses:\r\n    #57 since we don't actually want to do this\r\n",
        "dateCreated": "2017-06-19T15:16:15Z",
        "datePublished": "2017-06-19T15:20:17Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.0.8",
        "name": "Camera support, optimization, fixes for TOM robot demo",
        "tag_name": "v0.0.8",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.0.8",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/6759812",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.0.8"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "Adding more support here. This is pre-deletion of Gazebo simulation materials. In the future we will not need these at all.",
        "dateCreated": "2017-06-09T01:42:05Z",
        "datePublished": "2017-06-09T01:43:58Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.0.7",
        "name": "Patches to DMP and Task Execution",
        "tag_name": "v0.0.7",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.0.7",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/6656461",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.0.7"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "So we finally have this set up with OpenAI gym for new tasks. Unfortunately reward/feature functions are not yet in place.\r\n\r\nTo execute TOM:\r\n```\r\nroslaunch costar_task_plan tom.launch\r\nrosrun costar_task_plan tom_test.py\r\n```\r\n\r\nTo start the bullet sim:\r\n```\r\nroslaunch costar_bullet start --gui --task sorting --agent random\r\n```\r\n\r\nThe random agent just executes random commands and does not learn. It's terrible.",
        "dateCreated": "2017-05-23T15:14:22Z",
        "datePublished": "2017-05-23T15:23:15Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.0.6",
        "name": "TOM Execution and OpenAI Gym",
        "tag_name": "v0.0.6",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.0.6",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/6477952",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.0.6"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "Unfortunately for now you must control the gripper via the `Ur5RobotiqRobotInterface.gripper()` function. The \"standard\" `robot.gripper()` function takes as arguments:\r\n  - `cmd`: a floating point value\r\n  - `mode`: command mode (should usually be `pybullet.POSITION_CONTROL`\r\n\r\n",
        "dateCreated": "2017-05-21T21:35:03Z",
        "datePublished": "2017-05-21T21:37:19Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.0.5",
        "name": "Fix issues with Robotiq gripper in gazebo simulation",
        "tag_name": "v0.0.5",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.0.5",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/6456612",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.0.5"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "Execution with dmps. Setting up environments:\r\n  * mug: pick up a single mug and manipulate it properly\r\n  * sorting: two arms have to collaborate on pulling different colored balls from a central bin. Still need to add the balls to this one.\r\n  * blocks: stack blocks to build the tallest possible tower.\r\n  * clutter: clean up objects from a pile\r\n\r\nExecution has been tested a little more. Need to add some trajectory optimization; we really should be able to fit DMPs better than we currently are.",
        "dateCreated": "2017-05-19T21:11:27Z",
        "datePublished": "2017-05-19T21:16:38Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.0.4",
        "name": "New test environments and TOM robot execution",
        "tag_name": "v0.0.4",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.0.4",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/6448127",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.0.4"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "Bring up robots easily with:\r\n```\r\nrosrun costar_simulation start\r\nrosrun costar_bullet start\r\n```\r\n\r\nWith `costar_bullet`, you can start different tasks, like `mug`, `blocks`, or `clutter`. These are all still works in progress. We've settled on using bullet directly, because it has an awesome set of tools for deformable materials and the like. ",
        "dateCreated": "2017-05-18T03:43:31Z",
        "datePublished": "2017-05-18T03:48:52Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.0.3",
        "name": "DMP trajectory execution fixed, first pass at pybullet3 integration",
        "tag_name": "v0.0.3",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.0.3",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/6424329",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.0.3"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "Fake sim, more tests and learning methods, TOM stuff, data set tools, things from the old [lfd planner](https://github.com/cpaxton/grid_planning) are integrated...\r\n\r\nStill a few things to do as far as getting this version of the planner working.",
        "dateCreated": "2017-03-23T03:54:04Z",
        "datePublished": "2017-03-23T03:57:36Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.0.2",
        "name": "Things are coming together",
        "tag_name": "v0.0.2",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.0.2",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/5840228",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.0.2"
      },
      {
        "authorType": "User",
        "author_name": "cpaxton",
        "body": "We can read in the TOM dataset, create arbitrary task plans composed of options, compute features, and visualize. To get this to work on the robots, we need:\r\n  - feature extraction function\r\n  - DMP/GMM integration for motion primitives\r\n  - learning setup (previous work)",
        "dateCreated": "2017-03-11T02:25:48Z",
        "datePublished": "2017-03-11T02:28:43Z",
        "html_url": "https://github.com/jhu-lcsr/costar_plan/releases/tag/v0.0.1",
        "name": "Preliminary Support",
        "tag_name": "v0.0.1",
        "tarball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/tarball/v0.0.1",
        "url": "https://api.github.com/repos/jhu-lcsr/costar_plan/releases/5709905",
        "zipball_url": "https://api.github.com/repos/jhu-lcsr/costar_plan/zipball/v0.0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 60,
      "date": "Tue, 28 Dec 2021 00:48:18 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "learning",
      "learning-from-demonstration",
      "lfd",
      "robotics",
      "ros",
      "ur5",
      "simulation",
      "planning",
      "task-planning",
      "planner",
      "motion-planning",
      "deep-learning",
      "keras",
      "keras-tensorflow",
      "tensorflow",
      "deep-neural-networks",
      "deep-neural-nets",
      "architecture-search",
      "grasping",
      "neural-architecture-search"
    ],
    "technique": "GitHub API"
  }
}