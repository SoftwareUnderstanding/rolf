{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1905.02423"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. [**Deep residual learning for image recognition**](https://arxiv.org/pdf/1512.03385.pdf)\n2. [**Enet: A deep neural network architecture for real-time semantic segmentation**](https://arxiv.org/pdf/1606.02147.pdf)\n3. [**Erfnet: Efficient residual factorized convnet for real-time semantic segmentation**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8063438)\n4. [**Shufflenet: An extremely efficient convolutional neural network for mobile devices**](https://arxiv.org/pdf/1707.01083.pdf)\n\n<!--\n[![python-image]][python-url]\n[![pytorch-image]][pytorch-url]\n[![lic-image]][lic-url]\n-->\n\n[python-image]: https://img.shields.io/badge/Python-3.x-ff69b4.svg\n[python-url]: https://www.python.org/\n[pytorch-image]: https://img.shields.io/badge/PyTorch-1.0-2BAF2B.svg\n[pytorch-url]: https://pytorch.org/\n[lic-image]: https://img.shields.io/aur/license/pac.svg\n[lic-url]: https://github.com/xiaoyufenfei/LEDNet/blob/master/LICENSE",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this code useful for your research, please use the following BibTeX entry.\n\n```\n @article{wang2019lednet,\n  title={LEDNet: A Lightweight Encoder-Decoder Network for Real-time Semantic Segmentation},\n  author={Wang, Yu and Zhou, Quan and Liu, Jia and Xiong\uff0cJian and Gao, Guangwei and Wu, Xiaofu, and Latecki Jan Longin},\n  journal={arXiv preprint arXiv:1905.02423},\n  year={2019}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{wang2019lednet,\n  title={LEDNet: A Lightweight Encoder-Decoder Network for Real-time Semantic Segmentation},\n  author={Wang, Yu and Zhou, Quan and Liu, Jia and Xiong\uff0cJian and Gao, Guangwei and Wu, Xiaofu, and Latecki Jan Longin},\n  journal={arXiv preprint arXiv:1905.02423},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9212900890963637
      ],
      "excerpt": "<a href='#Resuming-training-if-decoder-part-broken'>Resuming training</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8562515826458484,
        0.8654671031158477
      ],
      "excerpt": "<a href='#Results'>Results</a> \n<a href='#Citation'>Reference</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8773975323175497
      ],
      "excerpt": "Please refer to our article for more details. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alalagong/LEDNet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-13T07:19:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-27T16:18:21Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project contains the code (Note: The code is test in the environment with python=3.6, cuda=9.0, PyTorch-0.4.1, also support Pytorch-0.4.1+) for:  [**LEDNet: A Lightweight Encoder-Decoder Network for Real-time Semantic Segmentation**](https://arxiv.org/pdf/1905.02423.pdf)  by [Yu Wang](https://github.com/xiaoyufenfei).\n\n<p align=\"center\"><img width=\"100%\" src=\"./images/LEDNet_overview.png\" /></p>\n\nThe extensive computational burden limits the usage of CNNs in mobile devices for dense estimation tasks, a.k.a semantic segmentation. In this paper, we present a lightweight network to address this problem, namely **LEDNet**, which employs an asymmetric encoder-decoder architecture for the task of real-time semantic segmentation.More specifically, the encoder adopts a ResNet as backbone network, where two new operations, channel split and shuffle, are utilized in each residual block to greatly reduce computation cost while maintaining higher segmentation accuracy. On the other hand, an attention pyramid network (APN) is employed in the decoder to further lighten the entire network complexity. Our model has less than 1M parameters, and is able to run at over 71 FPS on a single GTX 1080Ti GPU card. The comprehensive experiments demonstrate that our approach achieves state-of-the-art results in terms of speed and accuracy trade-off on Cityscapes dataset. and becomes an effective method for real-time semantic segmentation tasks.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.879404401835238,
        0.8496407630900236
      ],
      "excerpt": "<a href='#Introduction'>Introduction</a> \n<a href='#Project-Structure'>Project Structure</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9815790226166331
      ],
      "excerpt": "\u251c\u2500\u2500 datasets  #: contains all datasets for the project \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9223041614584597
      ],
      "excerpt": "Please refer to our article for more details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.804118801855894
      ],
      "excerpt": "Limited by GPU resources, the project results need to be further improved... \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Small changed LEDNet",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alalagong/LEDNet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 07:28:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alalagong/LEDNet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "alalagong/LEDNet",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3.6.x. Recommended using [Anaconda3](https://www.anaconda.com/distribution/)\n- Set up python environment\n\n```\npip3 install -r requirements.txt\n```\n\n- Env: PyTorch_0.4.1; cuda_9.0; cudnn_7.1; python_3.6, \n\n- Clone this repository.\n\n```\ngit clone https://github.com/xiaoyufenfei/LEDNet.git\ncd LEDNet-master\n```\n\n- Install [Visdom](https://github.com/facebookresearch/visdom).\n- Install [torchsummary](https://github.com/sksq96/pytorch-summary)\n- Download the dataset by following the **Datasets** below.\n- Note: For training, we currently support [cityscapes](https://www.cityscapes-dataset.com/) , aim to add [Camvid](https://github.com/alexgkendall/SegNet-Tutorial/tree/master/CamVid)  and  [VOC](http://host.robots.ox.ac.uk/pascal/VOC/)  and  [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K/)  dataset\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9287467277036058
      ],
      "excerpt": "<a href='#Installation'>Installation</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8730639939481598
      ],
      "excerpt": "For help on the optional arguments you can run: python main.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8730639939481598
      ],
      "excerpt": "for help on the optional arguments you can run: python main.py -h \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9285065137266004
      ],
      "excerpt": "\u589e\u52a0\u5bf9KITTI\u6570\u636e\u96c6\u7684\u8bfb\u53d6\u7c7b, \u4e8c\u6b21\u8bad\u7ec3\u6587\u4ef6train/main (kitti).py; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8892622509546634
      ],
      "excerpt": "<p align=\"center\"><img width=\"80%\" src=\"./images/rec-19-07-31-15-07-32.png\" /></p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8475977787284243
      ],
      "excerpt": "<p align=\"center\"><img width=\"80%\" src=\"./images/rec-19-08-03-11-45-43.png\" /></p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9008335956509239
      ],
      "excerpt": "<a href='#Training-LEDNet'>Train</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8797519857942414
      ],
      "excerpt": "<a href='#Testing'>Test</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924976426181745,
        0.8539082929308045,
        0.9009974393458172
      ],
      "excerpt": "\u251c\u2500\u2500 utils \n|  \u2514\u2500\u2500 dataset.py #: dataloader for cityscapes dataset \n|  \u2514\u2500\u2500 iouEval.py #: for test 'iou mean' and 'iou per class' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.9285065137266004,
        0.8589534893990137
      ],
      "excerpt": "|  \u2514\u2500\u2500 lednet_imagenet.py #:  \n|  \u2514\u2500\u2500 main.py #:  \n\u251c\u2500\u2500 train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9172968585851939,
        0.8633989807152664,
        0.9152750849795331,
        0.8051091626028818
      ],
      "excerpt": "|  \u2514\u2500\u2500 main.py #: train model scripts \n\u251c\u2500\u2500 test \n|  |  \u2514\u2500\u2500 dataset.py  \n|  |  \u2514\u2500\u2500 lednet.py #: model definition \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9127569099268988
      ],
      "excerpt": "|  |  \u2514\u2500\u2500 eval_cityscapes_color.py #: Test the results to generate RGB images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8584464053392292,
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "|  |  \u2514\u2500\u2500 eval_forward_time.py #: Test model inference time \n|  |  \u2514\u2500\u2500 eval_iou.py  \n|  |  \u2514\u2500\u2500 iouEval.py  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8104209871936175
      ],
      "excerpt": "You can download CityscapesScripts, and convert the dataset to 19 categories. It should have this basic structure. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "\u2502   \u251c\u2500\u2500 train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "\u2502   \u2514\u2500\u2500 test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "\u2502   \u251c\u2500\u2500 train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "\u2502   \u2514\u2500\u2500 test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "\u2502   \u251c\u2500\u2500 train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8889065588611057
      ],
      "excerpt": "For help on the optional arguments you can run: python main.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.924879286126762,
        0.9382600739040826,
        0.8889065588611057,
        0.9250295842579302
      ],
      "excerpt": "To train LEDNet using the train/main.py script the parameters listed in main.py as a flag or manually change them. \npython main.py --savedir logs --model lednet --datadir path/root_directory/  --num-epochs xx --batch-size xx ... \nfor help on the optional arguments you can run: python main.py -h \npython main.py --savedir logs --name lednet --datadir path/root_directory/  --num-epochs xx --batch-size xx --decoder --state \"../save/logs/model_best_enc.pth.tar\"... \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8953731654942318,
        0.8890100598090223
      ],
      "excerpt": "qualitative segmentation result examples: \n<p align=\"center\"><img width=\"100%\" src=\"./images/LEDNet_demo.png\" /></p> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alalagong/LEDNet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Yu Wang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# \u66f4\u6539",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "LEDNet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "alalagong",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alalagong/LEDNet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Thu, 23 Dec 2021 07:28:45 GMT"
    },
    "technique": "GitHub API"
  }
}