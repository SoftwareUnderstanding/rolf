{
  "citation": [
    {
      "confidence": [
        0.9989121489969736
      ],
      "excerpt": "BERT encoder source: https://arxiv.org/pdf/1810.04805.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rohan2002/BERT-Twitter",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-01-06T04:13:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-18T02:07:43Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Using natural language processing, I built a text pipeline to clean text by removing special characters, lemmatization and extensively use the nltk package for preprocessing. Moreover, I encode the clean English text and use BERT encoding to make the text ready for the neural network. Finally, I feed the text into a custom-built feed-forward model I build on top of a pre-trained BERT (Bidirectional Encoder Text Representation) model made by Google. I also tune my custom-built model using Keras Tuner and adjust the dropout regularization rate, number of neurons in the hidden layer, learning rate alpha.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9991871433370088,
        0.9868183461482929
      ],
      "excerpt": "This project is part of a Kaggle NLP competition to classify whether a series of tweets is related to a natural disaster or not. I achieved a test accuracy of 82% and validation accuracy of 83%, which placed me in the top 25% of all kagglers in the competition. (In reality, my position is much higher because many submissions by bots have an unfeasible accuracy of 1.0 and get ranked high on the leaderboard). \nBERT is a Transformers based NLP technique used for various problems such as Seq2Seq, Seq2Vec, Vec2Seq problems. It's a pretrained model developed by Google on a sizeable textual database scraped from the internet. Transformers have an architecture composed of an encoder and a decoder, where an encoder helps the model learn the semantic meaning of a whole sentence. It's \"bidirectional Context\" captures the entire context of the sentence. This makes BERT a popular choice among NLP tasks as learning a complete sentence(not word by word) with parallelization and context awareness makes the model Robust. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8570133698864809
      ],
      "excerpt": "My Keras generated Model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This project is part of a Kaggle NLP competition to classify whether a series of tweets are related to an unfortunate disaster or not.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rohan2002/BERT-natural-disaster-tweet-prediction/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The model uses a pre-trained BERT layer from Google with L=12 hidden layers (i.e., Transformer blocks), a hidden size of H=768, and A=12 attention heads. On top of that, a custom feed-forward model is built, which has two hidden layers with activation \"ELU\" and one classification layer with the Sigmoid activation function. The model uses the ADAM optimization algorithm and uses three epochs to train the model with a model batch size of 32(recommended by the Google Research team).\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 30 Dec 2021 09:10:34 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Rohan2002/BERT-Twitter/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rohan2002/BERT-Twitter",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Rohan2002/BERT-natural-disaster-tweet-prediction/master/lib/notebooks/disaster-classification.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8987531253380272,
        0.9612373635383447,
        0.9952971449203664,
        0.97037904315318
      ],
      "excerpt": "Since the model file is over 1GB, you might need git lfs to pull the project via CLI. If not, you can manually try to download it by clicking the download repo button on the Github Repo. \nRecommended to use Anaconda as an isolated package environment. To create a env with the necessary packages run conda create --name &lt;env_name&gt; --file requirements.txt \nIf you don't have conda(not recommended), run pip3 install -r requirements.txt to install the packages and follow step 4. \nIMPORTANT (source: https://stackoverflow.com/a/57164633/10016132): In your Conda env or local env, make sure to create env variable export PYTHONPATH=\"${PYTHONPATH}:/path/to/your/project/\" for Linux/OS X or set PYTHONPATH=%PYTHONPATH%;C:\\path\\to\\your\\project\\ for Windows \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8613557977512993
      ],
      "excerpt": "My model image generated from Keras plot utils: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Rohan2002/BERT-Twitter/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Fine-Tuned BERT model to Classify Natural-Disaster-Tweets",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERT-Twitter",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rohan2002",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rohan2002/BERT-Twitter/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 30 Dec 2021 09:10:34 GMT"
    },
    "technique": "GitHub API"
  }
}