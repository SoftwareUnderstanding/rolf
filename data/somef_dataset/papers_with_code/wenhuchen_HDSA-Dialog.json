{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We sincerely thank University of Cambridge and PolyAI for releasing the dataset and [code](https://github.com/budzianowski/multiwoz)\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wenhuchen/HDSA-Dialog",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-28T21:37:15Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-08T17:24:15Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8602545469801633,
        0.8764254525065431
      ],
      "excerpt": "This is the code and data for ACL 2019 long paper \"Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention\". The up-to-date version is in http://arxiv.org/abs/1905.12866. \nThe full architecture is displayed as below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.832188210661841
      ],
      "excerpt": "- Dialog act predictor (Fine-tuned BERT model) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9898540153417265
      ],
      "excerpt": "The basic idea of the paper is to do enable controlled reponse generation under the Transformer framework, where we construct a dialog act graph to represent the semantic space in MultiWOZ tasks. Then we particularly specify different heads in different levels to a specific node in the dialog act graph. For example, the picture above demonstrates the merge of two dialog acts \"hotel->inform->location\" and \"hotel->inform->name\". The generated sentence is controlled to deliever message about the name and location of a recommended hotel. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8984287308937853,
        0.8402416450598432
      ],
      "excerpt": "preprocessing: the code for pre-processing the database and original downloaded data \nThis module is used to predict the next-step dialog acts based on the conversation history. Here we adopt the state-of-the-art NLU module BERT to get the best prediction accuracy. Make sure that you install the Pytorch-pretrained-BERT beforehand, which will automatically download pre-trained model into your tmp folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8316304611932525,
        0.8138792443077913
      ],
      "excerpt": "The output values are saved in data/BERT_dev_prediction.json and data/BERT_dev_prediction.json, these two files need to be kept for the generator training. \nThis module is used to control the language generation based on the output of the pre-trained act predictor. The training data is already preprocessed and put in data/ folder (train.json, val.json and test.json). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8436416093345863
      ],
      "excerpt": "We release the pre-trained predictor model in checkpoints/predictor, you can put the zip file into checkpoints/predictor and unzip it to get the save_step_15120 folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code and Data for ACL 2019 \"Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention\"",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nsh collect_data.sh\n```\n###\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wenhuchen/HDSA-Dialog/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 37,
      "date": "Thu, 23 Dec 2021 08:41:05 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/wenhuchen/HDSA-Dialog/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "wenhuchen/HDSA-Dialog",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/wenhuchen/HDSA-Dialog/master/collect_data.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\npython preprocess_data_for_predictor.py\n```\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "<img src=\"resource/full_architecture.png\" width=\"800\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8254967280842863
      ],
      "excerpt": "data: all the needed training/evaluation/testing data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8590729260667338,
        0.9331690376783541,
        0.8517438938355842,
        0.8356092335979298,
        0.8762858059177049,
        0.8770130552131159,
        0.9284546371132865
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python3.5 train_predictor.py --do_eval --test_set dev --load_dir /tmp/output/save_step_xxx \nCUDA_VISIBLE_DEVICES=0 python3.5 train_predictor.py --do_eval --test_set test --load_dir /tmp/output/save_step_xxx \nThe output values are saved in data/BERT_dev_prediction.json and data/BERT_dev_prediction.json, these two files need to be kept for the generator training. \nThis module is used to control the language generation based on the output of the pre-trained act predictor. The training data is already preprocessed and put in data/ folder (train.json, val.json and test.json). \nCUDA_VISIBLE_DEVICES=0 python3.5 train_generator.py --option train --model BERT_dim128_w_domain_exp --batch_size 512 --max_seq_length 50 --field \nCUDA_VISIBLE_DEVICES=0 python3.5 train_generator.py --option test --model BERT_dim128_w_domain_exp --batch_size 512 --max_seq_length 50 --field \nCUDA_VISIBLE_DEVICES=0 python3.5 train_generator.py --option postprocess --output_file /tmp/results.txt.pred.BERT_dim128_w_domain_exp.pred --model BERT --non_delex \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9331690376783541
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python3.5 train_predictor.py --do_eval --test_set test --load_dir /tmp/output/save_step_15120 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/wenhuchen/HDSA-Dialog/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 wenhu chen\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "HDSA-Dialog",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "HDSA-Dialog",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "wenhuchen",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wenhuchen/HDSA-Dialog/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3.5\n- [Pytorch 1.0](https://pytorch.org/)\n- [Pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)\n\nPlease see the instructions to install the required packages before running experiments.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 131,
      "date": "Thu, 23 Dec 2021 08:41:05 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nrm -r checkpoints/predictor/\nCUDA_VISIBLE_DEVICES=0 python3.5 train_predictor.py --do_train --do_eval --train_batch_size 6 --eval_batch_size 6\n```\n",
      "technique": "Header extraction"
    }
  ]
}