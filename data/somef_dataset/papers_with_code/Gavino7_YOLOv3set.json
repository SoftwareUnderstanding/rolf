{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1910.01271",
      "https://arxiv.org/abs/1911.08287",
      "https://arxiv.org/abs/2004.10934",
      "https://arxiv.org/abs/1910.13302",
      "https://arxiv.org/abs/2005.03572",
      "https://arxiv.org/abs/2004.10934",
      "https://arxiv.org/abs/1612.08242"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite keras-YOLOv3-model-set in your publications if it helps your research:\n```\n@article{MobileNet-Yolov3,\n     Author = {Adam Yang},\n     Year = {2018}\n}\n@article{keras-yolo3,\n     Author = {qqwweee},\n     Year = {2018}\n}\n@article{YAD2K,\n     title={YAD2K: Yet Another Darknet 2 Keras},\n     Author = {allanzelener},\n     Year = {2017}\n}\n@article{yolov4,\n     title={YOLOv4: Optimal Speed and Accuracy of Object Detection},\n     author={Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao},\n     journal = {arXiv},\n     year={2020}\n}\n@article{yolov3,\n     title={YOLOv3: An Incremental Improvement},\n     author={Redmon, Joseph and Farhadi, Ali},\n     journal = {arXiv},\n     year={2018}\n}\n@article{redmon2016yolo9000,\n  title={YOLO9000: Better, Faster, Stronger},\n  author={Redmon, Joseph and Farhadi, Ali},\n  journal={arXiv preprint arXiv:1612.08242},\n  year={2016}\n}\n@article{Focal Loss,\n     title={Focal Loss for Dense Object Detection},\n     author={Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Doll\u00e1r},\n     journal = {arXiv},\n     year={2017}\n}\n@article{GIoU,\n     title={Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression},\n     author={Hamid Rezatofighi, Nathan Tsoi1, JunYoung Gwak1, Amir Sadeghian, Ian Reid, Silvio Savarese},\n     journal = {arXiv},\n     year={2019}\n}\n@article{DIoU Loss,\n     title={Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression},\n     author={Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, Dongwei Ren},\n     journal = {arXiv},\n     year={2020}\n}\n@inproceedings{tide-eccv2020,\n  author    = {Daniel Bolya and Sean Foley and James Hays and Judy Hoffman},\n  title     = {TIDE: A General Toolbox for Identifying Object Detection Errors},\n  booktitle = {ECCV},\n  year      = {2020},\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{tide-eccv2020,\n  author    = {Daniel Bolya and Sean Foley and James Hays and Judy Hoffman},\n  title     = {TIDE: A General Toolbox for Identifying Object Detection Errors},\n  booktitle = {ECCV},\n  year      = {2020},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DIoU Loss,\n     title={Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression},\n     author={Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, Dongwei Ren},\n     journal = {arXiv},\n     year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{GIoU,\n     title={Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression},\n     author={Hamid Rezatofighi, Nathan Tsoi1, JunYoung Gwak1, Amir Sadeghian, Ian Reid, Silvio Savarese},\n     journal = {arXiv},\n     year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Focal Loss,\n     title={Focal Loss for Dense Object Detection},\n     author={Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Doll\u00e1r},\n     journal = {arXiv},\n     year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{redmon2016yolo9000,\n  title={YOLO9000: Better, Faster, Stronger},\n  author={Redmon, Joseph and Farhadi, Ali},\n  journal={arXiv preprint arXiv:1612.08242},\n  year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{yolov3,\n     title={YOLOv3: An Incremental Improvement},\n     author={Redmon, Joseph and Farhadi, Ali},\n     journal = {arXiv},\n     year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{yolov4,\n     title={YOLOv4: Optimal Speed and Accuracy of Object Detection},\n     author={Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao},\n     journal = {arXiv},\n     year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{YAD2K,\n     title={YAD2K: Yet Another Darknet 2 Keras},\n     Author = {allanzelener},\n     Year = {2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{keras-yolo3,\n     Author = {qqwweee},\n     Year = {2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{MobileNet-Yolov3,\n     Author = {Adam Yang},\n     Year = {2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "[x] DIoU localization loss (paper) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9821914767258737
      ],
      "excerpt": "   usage: voc_annotation.py [-h] [--dataset_path DATASET_PATH] [--year YEAR] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9981949811229696
      ],
      "excerpt": "     --year YEAR           subset path of year (2007/2012), default will cover \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "                        size, default=10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "                        default=10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8707921115408231,
        0.8100655823024339,
        0.8707921115408231,
        0.8707921115408231,
        0.9446287103301529,
        0.9674153744090779,
        0.8100655823024339
      ],
      "excerpt": "| YOLOv4 Efficientnet(B1) | 512x512 | VOC07+12 | VOC07 | 82.39% | 62.02G | 65.32M | 251MB | 44ms | Keras on Titan XP | \n| Tiny YOLOv3 Lite-MobilenetV3Small | 416x416 | VOC07+12 | VOC07 | 65.09% | 731.64M | 1.50M | 6.5MB | 110ms | MNN on ARM Cortex-A53 * 4 | \n| YOLOv3 Lite-Mobilenet | 320x320 | VOC07+12 | VOC07 | 73.47% | 4.51G | 7.77M | 31.8MB | 17ms | Keras on Titan XP | \n| YOLOv3 Lite-Mobilenet | 416x416 | VOC07+12 | VOC07 | 76.55% | 7.60G | 7.77M | 31.8MB | 20ms | Keras on Titan XP | \n| YOLOv3 Lite-SPP-Mobilenet | 416x416 | VOC07+12 | VOC07 | 76.32% | 7.98G | 8.81M | 34MB | 22ms | Keras on Titan XP | \n| Tiny YOLOv3 Lite-Mobilenet | 320x320 | VOC07+12 | VOC07 | 69.10% | 2.93G | 4.92M | 20.1MB | 9ms | Keras on Titan XP | \n| Tiny YOLOv3 Lite-Mobilenet | 416x416 | VOC07+12 | VOC07 | 72.90% | 4.95G | 4.92M | 20.1MB | 11ms | Keras on Titan XP | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8707921115408231,
        0.9446287103301529
      ],
      "excerpt": "| YOLOv3 Nano | 416x416 | VOC07+12 | VOC07 | 69.55% | 6.40G | 4.66M | 19MB | 29ms | Keras on Titan XP | \n| YOLOv3-Xception | 512x512 | VOC07+12 | VOC07 | 79.15% | 147.30G | 104.72M | 419.8MB | 48ms | Keras on Titan XP | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8707921115408231,
        0.8707921115408231
      ],
      "excerpt": "| YOLOv3-Mobilenet | 320x320 | VOC07+12 | VOC07 | 74.56% |  |  |  | 29fps | Keras on 1080Ti | \n| YOLOv3-Mobilenet | 416x416 | VOC07+12 | VOC07 | 76.82% |  |  |  | 25fps | Keras on 1080Ti | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| MobileNet-SSD | 300x300 | VOC07+12 | VOC07 | 68% |  |  | 22MB |  |  | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Gavino7/YOLOv3set",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-01T17:41:45Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-16T10:32:31Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "A general YOLOv4/v3/v2 object detection pipeline inherited from [keras-yolo3-Mobilenet](https://github.com/Adamdad/keras-YOLOv3-mobilenet)/[keras-yolo3](https://github.com/qqwweee/keras-yolo3) and [YAD2K](https://github.com/allanzelener/YAD2K). Implement with tf.keras, including data collection/annotation, model training/tuning, model evaluation and on device deployment. Support different architecture and different technologies:\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8924212690395703,
        0.8950643645000704
      ],
      "excerpt": "[x] VGG16 \n[x] YOLOv4 (Lite) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8950643645000704
      ],
      "excerpt": "[x] YOLOv3 (Lite, SPP) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8045662531679588,
        0.8950643645000704,
        0.8045662531679588
      ],
      "excerpt": "[x] Tiny YOLOv3 (Lite) \n[x] YOLOv2 (Lite) \n[x] Tiny YOLOv2 (Lite) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "[x] Mosaic data augmentation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8973553931915454
      ],
      "excerpt": "[x] Pruned model training (only valid for TF 1.x) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8964212326416718
      ],
      "excerpt": "[x] Tensorflow-Lite Float32/UInt8 model inference \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8350360121109714
      ],
      "excerpt": "                           default is ./ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8350360121109714
      ],
      "excerpt": "                           default is ./ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9226458496316349
      ],
      "excerpt": "                        Freeze level of the model in transfer training stage. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8055374667023428
      ],
      "excerpt": "                        Number of iteration(batches) interval to rescale input \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8695451713969243
      ],
      "excerpt": "                        Assign multiple anchors to single ground truth \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9401533315797893
      ],
      "excerpt": "  --model_pruning       Use model pruning for optimization, only for TF 1.x \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.941026341042734
      ],
      "excerpt": "We need to dump out inference model from training checkpoint for eval or demo. Following script cmd work for that. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8058999581236133,
        0.8421598367343522
      ],
      "excerpt": "NOTE: Now you can dump out a non-square input shape (e.g. using --model_image_size=320x416) model and do inference as normal, but the input height & weights must be multiples of 32. \nUse eval.py to do evaluation on the inference model with your test data. It support following metrics: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8557864687032882
      ],
      "excerpt": "evaluate TIDE dAP with tidecv \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8435588619052704,
        0.8195325039493727,
        0.840189686441903,
        0.8458266092658722
      ],
      "excerpt": "| YOLOv3 Lite-Mobilenet | 320x320 | train2017 | val2017 | 19.40 | 38.58 | 4.76G | 8.09M | 32MB | 14.6ms | Keras on Titan XP | \n| YOLOv3 Lite-Mobilenet | 416x416 | train2017 | val2017 | 22.69 | 43.61 | 8.04G | 8.09M | 32MB | 16.9ms | Keras on Titan XP | \n| Tiny YOLOv3 Lite-Mobilenet | 320x320 | train2017 | val2017 | 16.41 | 34.17 | 3.04G | 5.19M | 21MB | 8.7ms | Keras on Titan XP | \n| Tiny YOLOv3 Lite-Mobilenet | 416x416 | train2017 | val2017 | 19.28 | 39.36 | 5.13G | 5.19M | 21MB | 9.3ms | Keras on Titan XP | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837965700369634,
        0.8779086284857728,
        0.864842967506361,
        0.8779086284857728
      ],
      "excerpt": "| Tiny YOLOv3 Lite-MobilenetV3Small | 416x416 | VOC07+12 | VOC07 | 65.09% | 731.64M | 1.50M | 6.5MB | 110ms | MNN on ARM Cortex-A53 * 4 | \n| YOLOv3 Lite-Mobilenet | 320x320 | VOC07+12 | VOC07 | 73.47% | 4.51G | 7.77M | 31.8MB | 17ms | Keras on Titan XP | \n| YOLOv3 Lite-Mobilenet | 416x416 | VOC07+12 | VOC07 | 76.55% | 7.60G | 7.77M | 31.8MB | 20ms | Keras on Titan XP | \n| YOLOv3 Lite-SPP-Mobilenet | 416x416 | VOC07+12 | VOC07 | 76.32% | 7.98G | 8.81M | 34MB | 22ms | Keras on Titan XP | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8489694833120595,
        0.9045063818000985
      ],
      "excerpt": "| Tiny YOLOv3 Lite-Mobilenet | 416x416 | VOC07+12 | VOC07 | 72.90% | 4.95G | 4.92M | 20.1MB | 11ms | Keras on Titan XP | \n| Tiny YOLOv3 Lite-Mobilenet with GIoU loss | 416x416 | VOC07+12 | VOC07 | 72.92% | 4.95G | 4.92M | 20.1MB | 11ms | Keras on Titan XP | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8855962429146914,
        0.9840720034314154
      ],
      "excerpt": "NOTE: mAP/AP is evaluated with \"Weighted-Distance-Cluster-NMS\" post process, which has better performance than Traditional NMS \nUsing keras_to_tensorflow.py to convert the tf.keras .h5 model to tensorflow frozen pb model (only for TF 1.x): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9635074990134594
      ],
      "excerpt": "Using keras_to_onnx.py to convert the tf.keras .h5 model to ONNX model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9820850170468253
      ],
      "excerpt": "See on-device inference for TFLite & MNN model deployment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8696234657279613
      ],
      "excerpt": "[ ] provide more imagenet pretrained backbone (e.g. shufflenet, shufflenetv2), see Training backbone \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Gavino7/YOLOv3set/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 16:24:52 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Gavino7/YOLOv3set/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Gavino7/YOLOv3set",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Gavino7/YOLOv3set/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Gavino7/YOLOv3set/master/inference/eval_inference.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8432094244452
      ],
      "excerpt": "   #: cd tools/dataset_converter/ && python voc_annotation.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801083591072351
      ],
      "excerpt": "                           both \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8221133486263911
      ],
      "excerpt": "   You can merge these train &amp; val annotation file as your need. For example, following cmd will creat 07/12 combined trainval dataset: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8432094244452
      ],
      "excerpt": "   #: cd tools/dataset_converter/ && python coco_annotation.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8240275054072005
      ],
      "excerpt": "If you want to download PascalVOC or COCO dataset, refer to Dockerfile for cmd \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8238021420342173
      ],
      "excerpt": "  --gpu_num GPU_NUM     Number of GPU to use, default=1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918757940400692
      ],
      "excerpt": "You can also use Tensorboard to monitor the loss trend during train: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.841363006841768
      ],
      "excerpt": "If you're evaluating with MSCOCO dataset, you can further use pycoco_eval.py with the generated txt detection result and COCO GT annotation to get official COCO AP with pycocotools: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8638225223402811
      ],
      "excerpt": "The test environment is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "Python 3.6.8 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8443717674145305
      ],
      "excerpt": "[x] tf.keras batch-wise YOLOv3/v2 postprocess layer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8722908556576813
      ],
      "excerpt": "[x] Eliminate grid sensitivity (numpy/C++, from YOLOv4) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8250330109371733
      ],
      "excerpt": "[x] Pruned model training (only valid for TF 1.x) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9416858854010595,
        0.891170600867993
      ],
      "excerpt": "Generate train/val/test annotation file and class names file. \nData annotation file format: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8211967361428026,
        0.8476539125579702,
        0.8043630205773916
      ],
      "excerpt": "* Here is an example: \npath/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 \npath/to/img2.jpg 120,300,250,600,2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8130739720218199
      ],
      "excerpt": "1. For VOC style dataset, you can use voc_annotation.py to convert original dataset to our annotation file: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.800394356372325
      ],
      "excerpt": "   #: cd tools/dataset_converter/ && python voc_annotation.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8529281484036173
      ],
      "excerpt": "convert PascalVOC dataset annotation to txt annotation file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8244499061050882,
        0.8633989807152664
      ],
      "excerpt": "     --set SET             convert data set, default will cover train, val and \n                           test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8014833420786232
      ],
      "excerpt": "                           output path for generated annotation txt files, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8928230318842049
      ],
      "excerpt": "   By default, the VOC convert script will try to go through both VOC2007/VOC2012 dataset dir under the dataset_path and generate train/val/test annotation file separately, like: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8221863219217665
      ],
      "excerpt": "   You can merge these train &amp; val annotation file as your need. For example, following cmd will creat 07/12 combined trainval dataset: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.800394356372325,
        0.9456292920265406
      ],
      "excerpt": "   #: cd tools/dataset_converter/ && python coco_annotation.py -h \n   usage: coco_annotation.py [-h] [--dataset_path DATASET_PATH] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8125324372661558
      ],
      "excerpt": "convert COCO dataset annotation to txt annotation file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8014833420786232
      ],
      "excerpt": "                           output path for generated annotation txt files, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8606286258848052
      ],
      "excerpt": "For class names file format, refer to  coco_classes.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785,
        0.9632270575298659
      ],
      "excerpt": ": python train.py -h \nusage: train.py [-h] [--model_type MODEL_TYPE] [--anchors_path ANCHORS_PATH] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8226763253166589
      ],
      "excerpt": "                        YOLO model type: yolo3_mobilenet_lite/tiny_yolo3_mobil \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8602141860168777
      ],
      "excerpt": "                        Initial model image input size as <height>x<width>, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135285178429218
      ],
      "excerpt": "                        Pretrained model/weights file for fine tune \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8805339773852844
      ],
      "excerpt": "                        train annotation txt file, default=trainval.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8606332122043401
      ],
      "excerpt": "                        val annotation txt file, default=None \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8634184914973622
      ],
      "excerpt": "                        Batch size for train, default=16 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8765448335600885
      ],
      "excerpt": "                        Total training epochs, default=250 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8243033319417674
      ],
      "excerpt": "                        size, default=10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8309188525477454
      ],
      "excerpt": "                        enhance data augmentation type (None/mosaic), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8981524067900627
      ],
      "excerpt": "  --elim_grid_sense     Eliminate grid sensitivity \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "                        training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.837264209975536
      ],
      "excerpt": ": python train.py --model_type=yolo3_mobilenet_lite --anchors_path=configs/yolo3_anchors.txt --annotation_file=trainval.txt --classes_path=configs/voc_classes.txt --eval_online --save_eval_checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567364438045971,
        0.8467123023894444
      ],
      "excerpt": "Loss type couldn't be changed from CLI options. You can try them by changing params in loss.py(v3) or loss.py(v2) \nPostprocess type (SoftNMS/DIoU-NMS/Cluster-NMS/WBF) could be configured in yolo_postprocess_np.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8486155923014458
      ],
      "excerpt": ": python yolo.py --model_type=yolo3_mobilenet_lite --weights_path=logs/000/<checkpoint>.h5 --anchors_path=configs/yolo3_anchors.txt --classes_path=configs/voc_classes.txt --model_image_size=416x416 --dump_model --output_model_file=model.h5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8600530148588378
      ],
      "excerpt": "Pascal VOC mAP: will generate txt detection result result/detection_result.txt, draw rec/pre curve for each class and AP/mAP result chart in \"result\" dir with default 0.5 IOU or specified IOU, and optionally save all the detection result on evaluation dataset as images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633787374079541
      ],
      "excerpt": ": python eval.py --model_path=model.h5 --anchors_path=configs/yolo3_anchors.txt --classes_path=configs/voc_classes.txt --model_image_size=416x416 --eval_type=VOC --iou_threshold=0.5 --conf_threshold=0.001 --annotation_file=2007_test.txt --save_result \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9456292920265406
      ],
      "excerpt": "usage: pycoco_eval.py [-h] --result_txt RESULT_TXT --coco_annotation_json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8141116865980981
      ],
      "excerpt": "                        txt detection result file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8824610271238611
      ],
      "excerpt": "                        output coco json result file, default is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9265516773156276
      ],
      "excerpt": ": python pycoco_eval.py --result_txt=../../result/detection_result.txt --coco_annotation_json=./instances_val2017.json --coco_result_json=coco_result.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9456292920265406
      ],
      "excerpt": "usage: tide_eval.py [-h] --coco_annotation_json COCO_ANNOTATION_JSON \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.841100253143215,
        0.8967713307829016
      ],
      "excerpt": "                        coco json result file \n: python tide_eval.py --coco_annotation_json=./instances_val2017.json --coco_result_json=coco_result.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8065676479889494,
        0.8013232639937973
      ],
      "excerpt": "If you enable \"--eval_online\" option in train.py, a default Pascal VOC mAP evaluation on validation dataset will be executed during training. But that may cost more time for train process. \nFollowing is a sample result trained on Mobilenet YOLOv3 Lite model with PascalVOC dataset (using a reasonable score threshold=0.1): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8689571785636104,
        0.9202509175650415
      ],
      "excerpt": "  <img src=\"assets/mAP.jpg\"> \n  <img src=\"assets/COCO_AP.jpg\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.896697476153979
      ],
      "excerpt": "Using keras_to_tensorflow.py to convert the tf.keras .h5 model to tensorflow frozen pb model (only for TF 1.x): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": ": python keras_to_tensorflow.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8960149134623286
      ],
      "excerpt": "Using keras_to_onnx.py to convert the tf.keras .h5 model to ONNX model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": ": python keras_to_onnx.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8204644885613888,
        0.8123763140827432
      ],
      "excerpt": "tensorflow 2.0.0/tensorflow 1.15.0 \ntf.keras 2.2.4-tf \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Gavino7/YOLOv3set/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "HTML",
      "Python",
      "C",
      "C++",
      "Dockerfile",
      "CMake",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 david8862\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "TF Keras YOLOv4/v3/v2 Modelset",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "YOLOv3set",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Gavino7",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Gavino7/YOLOv3set/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 16:24:52 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Install requirements on Ubuntu 16.04/18.04:\n\n```\n#: apt install python3-opencv\n#: pip install Cython\n#: pip install -r requirements.txt\n```\n\n2. Download Related Darknet/YOLOv2/v3/v4 weights from [YOLO website](http://pjreddie.com/darknet/yolo/) and [AlexeyAB/darknet](https://github.com/AlexeyAB/darknet).\n3. Convert the Darknet YOLO model to a Keras model.\n4. Run YOLO detection on your image or video, default using Tiny YOLOv3 model.\n\n```\n#: wget -O weights/darknet53.conv.74.weights https://pjreddie.com/media/files/darknet53.conv.74\n#: wget -O weights/darknet19_448.conv.23.weights https://pjreddie.com/media/files/darknet19_448.conv.23\n#: wget -O weights/yolov3.weights https://pjreddie.com/media/files/yolov3.weights\n#: wget -O weights/yolov3-tiny.weights https://pjreddie.com/media/files/yolov3-tiny.weights\n#: wget -O weights/yolov3-spp.weights https://pjreddie.com/media/files/yolov3-spp.weights\n#: wget -O weights/yolov2.weights http://pjreddie.com/media/files/yolo.weights\n#: wget -O weights/yolov2-voc.weights http://pjreddie.com/media/files/yolo-voc.weights\n#: wget -O weights/yolov2-tiny.weights https://pjreddie.com/media/files/yolov2-tiny.weights\n#: wget -O weights/yolov2-tiny-voc.weights https://pjreddie.com/media/files/yolov2-tiny-voc.weights\n\n#:#:#: manually download csdarknet53-omega_final.weights from https://drive.google.com/open?id=18jCwaL4SJ-jOvXrZNGHJ5yz44g9zi8Hm\n#: wget -O weights/yolov4.weights https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights\n\n#: python tools/model_converter/convert.py cfg/yolov3.cfg weights/yolov3.weights weights/yolov3.h5\n#: python tools/model_converter/convert.py cfg/yolov3-tiny.cfg weights/yolov3-tiny.weights weights/yolov3-tiny.h5\n#: python tools/model_converter/convert.py cfg/yolov3-spp.cfg weights/yolov3-spp.weights weights/yolov3-spp.h5\n#: python tools/model_converter/convert.py cfg/yolov2.cfg weights/yolov2.weights weights/yolov2.h5\n#: python tools/model_converter/convert.py cfg/yolov2-voc.cfg weights/yolov2-voc.weights weights/yolov2-voc.h5\n#: python tools/model_converter/convert.py cfg/yolov2-tiny.cfg weights/yolov2-tiny.weights weights/yolov2-tiny.h5\n#: python tools/model_converter/convert.py cfg/yolov2-tiny-voc.cfg weights/yolov2-tiny-voc.weights weights/yolov2-tiny-voc.h5\n#: python tools/model_converter/convert.py cfg/darknet53.cfg weights/darknet53.conv.74.weights weights/darknet53.h5\n#: python tools/model_converter/convert.py cfg/darknet19_448_body.cfg weights/darknet19_448.conv.23.weights weights/darknet19.h5\n\n#: python tools/model_converter/convert.py cfg/csdarknet53-omega.cfg weights/csdarknet53-omega_final.weights weights/cspdarknet53.h5\n\n#:#:#: make sure to reorder output tensors for YOLOv4 cfg and weights file\n#: python tools/model_converter/convert.py --yolo4_reorder cfg/yolov4.cfg weights/yolov4.weights weights/yolov4.h5\n\n\n#:#:#: Yolo-Fastest\n#: wget -O weights/yolo-fastest.weights https://github.com/dog-qiuqiu/Yolo-Fastest/blob/master/Yolo-Fastest/yolo-fastest.weights?raw=true\n#: wget -O weights/yolo-fastest-xl.weights https://github.com/dog-qiuqiu/Yolo-Fastest/blob/master/Yolo-Fastest/yolo-fastest-xl.weights?raw=true\n\n#: python tools/model_converter/convert.py cfg/yolo-fastest.cfg weights/yolo-fastest.weights weights/yolo-fastest.h5\n#: python tools/model_converter/convert.py cfg/yolo-fastest-xl.cfg weights/yolo-fastest-xl.weights weights/yolo-fastest-xl.h5\n\n\n#: python yolo.py --image\n#: python yolo.py --input=<your video file>\n```\nFor other model, just do in a similar way, but specify different model type, weights path and anchor path with `--model_type`, `--weights_path` and `--anchors_path`.\n\nImage detection sample:\n\n<p align=\"center\">\n  <img src=\"assets/dog_inference.jpg\">\n  <img src=\"assets/kite_inference.jpg\">\n</p>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. [yolo.py](https://github.com/david8862/keras-YOLOv3-model-set/blob/master/yolo.py)\n> * Demo script for trained model\n\nimage detection mode\n```\n#: python yolo.py --model_type=yolo3_mobilenet_lite --weights_path=model.h5 --anchors_path=configs/yolo3_anchors.txt --classes_path=configs/voc_classes.txt --model_image_size=416x416 --image\n```\nvideo detection mode\n```\n#: python yolo.py --model_type=yolo3_mobilenet_lite --weights_path=model.h5 --anchors_path=configs/yolo3_anchors.txt --classes_path=configs/voc_classes.txt --model_image_size=416x416 --input=test.mp4\n```\nFor video detection mode, you can use \"input=0\" to capture live video from web camera and \"output=<video name>\" to dump out detection result to another video\n\n",
      "technique": "Header extraction"
    }
  ]
}