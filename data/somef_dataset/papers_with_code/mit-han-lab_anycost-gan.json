{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We thank Taesung Park, Zhixin Shu, Muyang Li, and Han Cai for the helpful discussion. Part of the work is supported by NSF CAREER Award #1943349, Adobe, SONY, Naver Corporation, and MIT-IBM Watson AI Lab.\n\nThe codebase is build upon a PyTorch implementation of StyleGAN2: [rosinality/stylegan2-pytorch](https://github.com/rosinality/stylegan2-pytorch). For editing direction extraction, we refer to [InterFaceGAN](https://github.com/genforce/interfacegan).\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2103.03243",
      "https://arxiv.org/abs/2103.03243"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code for your research, please cite our paper.\n\n```\n@inproceedings{lin2021anycost,\n  author    = {Lin, Ji and Zhang, Richard and Ganz, Frieder and Han, Song and Zhu, Jun-Yan},\n  title     = {Anycost GANs for Interactive Image Synthesis and Editing},\n  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year      = {2021},\n}\n```\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{lin2021anycost,\n  author    = {Lin, Ji and Zhang, Richard and Ganz, Frieder and Han, Song and Zhu, Jun-Yan},\n  title     = {Anycost GANs for Interactive Image Synthesis and Editing},\n  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year      = {2021},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9999053604528395,
        0.8520815120463098,
        0.8132852241448368
      ],
      "excerpt": "Ji Lin, Richard Zhang, Frieder Ganz, Song Han, Jun-Yan Zhu \nMIT, Adobe Research, CMU \nIn CVPR 2021 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mit-han-lab/anycost-gan",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-04T05:39:06Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T15:33:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide a jupyter notebook example to show how to use the anycost generator for image synthesis at diverse costs: `notebooks/intro.ipynb`.\n\nWe also provide a colab version of the notebook: [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mit-han-lab/anycost-gan/blob/master/notebooks/intro_colab.ipynb). Be sure to select the GPU as the accelerator in runtime options.\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9224021375945964
      ],
      "excerpt": "We also provide the face attribute classifier (which is general for different generators) for computing the editing directions. You can get it by running: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.896414015924676,
        0.8884320195457621
      ],
      "excerpt": "Currently, we provide the following pre-trained generators, encoders, and editing directions. We will add more in the future. \nFor Anycost generators, by default, we refer to the uniform setting. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9921838935622285,
        0.8609676181817508
      ],
      "excerpt": "We provide the code to evaluate some metrics presented in the paper. Some of the code is written with horovod to support distributed evaluation and reduce the cost of inter-GPU communication, which greatly improves the speed. Check its website for a proper installation. \nBefore evaluating the FIDs, you need to compute the inception features of the real images using scripts like: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8931367674473262
      ],
      "excerpt": "Similary, evaluting the PPL with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411944140065915
      ],
      "excerpt": "To evaluate the performance of the encoder, run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9719439929550983
      ],
      "excerpt": "The training of original StyleGAN2 is time-consuming. We recommend downloading the converted checkpoints from here and place it under checkpoint/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9714588837292585
      ],
      "excerpt": "Note that after each epoch, we evaluate the FIDs of two resolutions (1024&512) to better monitor the training progress. We also apply distillation to accelearte the convergence, which is not used for the ablation in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "[CVPR 2021] Anycost GANs for Interactive Image Synthesis and Editing",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mit-han-lab/anycost-gan/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 72,
      "date": "Fri, 24 Dec 2021 22:01:20 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mit-han-lab/anycost-gan/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mit-han-lab/anycost-gan",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mit-han-lab/anycost-gan/master/notebooks/intro.ipynb",
      "https://raw.githubusercontent.com/mit-han-lab/anycost-gan/master/notebooks/intro_colab.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mit-han-lab/anycost-gan/master/scripts/train_stylegan2_ffhq.sh",
      "https://raw.githubusercontent.com/mit-han-lab/anycost-gan/master/scripts/train_anycost_multires_adach_ffhq.sh",
      "https://raw.githubusercontent.com/mit-han-lab/anycost-gan/master/scripts/train_anycost_multires_ffhq.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.811567875266229
      ],
      "excerpt": "or you can download the pre-computed inceptions from here and put it under assets/inceptions. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9075801600570756
      ],
      "excerpt": "import models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8900486270063179
      ],
      "excerpt": "from models.dynamic_channel import set_uniform_channel_ratio, reset_generator \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.828344218455681
      ],
      "excerpt": "python tools/calc_inception.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468
      ],
      "excerpt": "horovodrun -np N_GPU \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468
      ],
      "excerpt": "horovodrun -np N_GPU \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468
      ],
      "excerpt": "horovodrun -np N_GPU \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mit-han-lab/anycost-gan/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 MIT HAN Lab\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Anycost GAN",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "anycost-gan",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mit-han-lab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mit-han-lab/anycost-gan/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 628,
      "date": "Fri, 24 Dec 2021 22:01:20 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "computer-vision",
      "deep-learning",
      "computer-graphics",
      "generative-adversarial-network",
      "gan",
      "image-generation",
      "image-manipulation",
      "image-editing",
      "gans",
      "pytorch",
      "stylegan2"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<a href=\"https://youtu.be/_yEziPl9AkM?t=90\"><img src='assets/figures/demo.gif' width=600></a>\n\nHere, we can use the Anycost generator for **interactive image editing**. A full generator takes **~3s** to render an image, which is too slow for editing. While with Anycost generator, we can provide a visually similar preview at **5x faster speed**. After adjustment, we hit the \"Finalize\" button to synthesize the high-quality final output. Check [here](https://youtu.be/_yEziPl9AkM?t=90) for the full demo.\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Clone this repo:\n\n```bash\ngit clone https://github.com/mit-han-lab/anycost-gan.git\ncd anycost-gan\n```\n\n- Install PyTorch 1.7 and other dependeinces.\n\nWe recommend setting up the environment using Anaconda: `conda env create -f environment.yml`\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide an interactive demo showing how we can use anycost GAN to enable interactive image editing. To run the demo:\n\n```bash\npython demo.py\n```\n\nIf your computer contains a CUDA GPU, try running with:\n```bash\nFORCE_NATIVE=1 python demo.py\n```\n\nYou can find a video recording of the demo [here](https://youtu.be/_yEziPl9AkM?t=90).\n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}