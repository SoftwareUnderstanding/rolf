{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762\n- https://arxiv.org/abs/1810.04805\n- https://arxiv.org/abs/1906.08237\n",
      "https://arxiv.org/abs/1810.04805\n- https://arxiv.org/abs/1906.08237\n",
      "https://arxiv.org/abs/1906.08237\n"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/2miatran/Natural-Language-Processing",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-24T06:50:13Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-11T14:44:18Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9082318332732812,
        0.8223307436613855,
        0.8223307436613855
      ],
      "excerpt": "I used 2 models: BERT and ULMFiT, but there are others to try out (XLM, ALBERT, ...) \n- Accuracy for BERT 0.79 \n- Accuracy for ULMFiT 0.73 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978497877812938,
        0.9726653607520781,
        0.9614530325870682,
        0.9624077908724069
      ],
      "excerpt": "BERT requires input tokens to be truncated to 512 (510 to be specific, excluding CLS and SEP tokens). Most of the clinical notes are more than 1000 words in length (before bert tokenization), and note that the subword tokenization aldo reduced the numbers of input words (as same words can be splitted to subwords). Clinical notes do have a lot of the subword (e.g. ##os, ##es). One way to make use of all data, maybe to  \nTry classification task using either the first part, middle part, or last part to see which one is best. The intuiation is some time, info at the begining may be more important than the end, or vice versus.    \nInstead of using one segmentation to work on, segment the text into many parts that maybe allow some overlap, then make classification and average the results.  \nFor ULMFiT model: During the language model training, spacy tokenizer and ADW LSTM. We can try using transformer language models instead, and then passing the leanred encoder for the second classification networks.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9973570537178026
      ],
      "excerpt": "The codes is attributable to Maximilien Roberti (BERT) and fastai lecture (ULMFiT) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "BERT, ULMFiT",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/2miatran/Natural-Language-Processing/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 10:53:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/2miatran/Natural-Language-Processing/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "2miatran/Natural-Language-Processing",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/2miatran/Natural-Language-Processing/master/NLP.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/2miatran/Natural-Language-Processing/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Natural-Language-Processing",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Natural-Language-Processing",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "2miatran",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/2miatran/Natural-Language-Processing/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Thu, 23 Dec 2021 10:53:29 GMT"
    },
    "technique": "GitHub API"
  }
}