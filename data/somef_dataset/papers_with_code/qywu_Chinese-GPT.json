{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1810.04805"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": " [OpenAI GPT-2](https://openai.com/blog/better-language-models/)\n \n [BERT](https://arxiv.org/abs/1810.04805)\n \n [AllenNLP](https://github.com/allenai/allennlp/)\n \n [Pytorch BERT](https://github.com/huggingface/pytorch-pretrained-BERT)\n \n [NLP Chinese Corpus](https://github.com/brightmart/nlp_chinese_corpus)\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/qywu/Chinese-GPT",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-04T03:54:01Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T14:47:34Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9603225682039609,
        0.8752260919243735,
        0.9850017475343504,
        0.8325843424918357
      ],
      "excerpt": "This project is unidirectional transformer GPT model (117M) trained on a large corpus dataset following the approach OpenAI GPT-2. Due to limited computational resources, we did not train our model from scratch. Instead, we take the advantage of BERT and use its weights as initialization to train our Chinese GPT. This makes the training possible on 4 x 1080Ti. \nHowever, please notice that currently the performance still cannot match the original English GPT-2 model for various reasons. This can be that OpenAI has done better text filtering and has a dataset with better quality. Also, they have trained their model for about 300 GPU days at least. But the model here can be a good starting point if you want to apply it for substream tasks. \nThis repository contains a rewritten cached BERT model, which is the same technique used in GPT-2 implementation. It can cache the intermediate keys and values, and therefore save the compuation time and memory during the decoding stage. \nOne thing to take care of is that text filtering. Since Bert Chinese tokenizer doesn't include some punctuations. You might want to use the following code to clean your data first: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Chinese Transformer Generative Pre-Training Model",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/qywu/Chinese-GPT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Tue, 28 Dec 2021 07:09:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/qywu/Chinese-GPT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "qywu/Chinese-GPT",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/qywu/Chinese-GPT/master/tutorials/Training.ipynb",
      "https://raw.githubusercontent.com/qywu/Chinese-GPT/master/tutorials/Model%20Weights%20Conversion%28%2AOptional%29.ipynb",
      "https://raw.githubusercontent.com/qywu/Chinese-GPT/master/tutorials/Data%20Processing%20Example.ipynb",
      "https://raw.githubusercontent.com/qywu/Chinese-GPT/master/tutorials/Decode.ipynb",
      "https://raw.githubusercontent.com/qywu/Chinese-GPT/master/tutorials/Beam%20Search%20and%20Sampling%20Search.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To train GPT, it requires a dataset from a wide range of sources.\n\nWe collected data from [NLP Chinese Corpus](https://github.com/brightmart/nlp_chinese_corpus)\n\nIn details, we used:\n\n- \u793e\u533a\u95ee\u7b54json\u7248(webtext2019zh) \uff1a\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\n- \u767e\u79d1\u7c7b\u95ee\u7b54json\u7248(baike2018qa)\n- \u65b0\u95fb\u8bed\u6599json\u7248(news2016zh)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Before using it, you might want to install the requirements first.\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n\nYou can also install it via `pip`.\n\n   ```bash\n   pip install chinese-gpt\n   ```\n   \n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/qywu/Chinese-GPT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 \\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Chinese-GPT \u4e2d\u6587GPT\u9884\u8bad\u7ec3\u6a21\u578b",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Chinese-GPT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "qywu",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/qywu/Chinese-GPT/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 40,
      "date": "Tue, 28 Dec 2021 07:09:49 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Check [tutorials](https://github.com/qywu/Chinese-GPT/tree/master/tutorials) for details.\n\nI have also included a colab for demo: https://colab.research.google.com/drive/1cvBSt2uF7hYL1feDGt0dkCxIeaVXQs5x\n\nEncoder Weights: https://drive.google.com/file/d/1Mr2-x_qT2hgyo0RalPjc09NmyNi6a_gs/view?usp=sharing\n\nDecoder Weights: https://drive.google.com/file/d/1W6n7Kv6kvHthUX18DhdGSzBYkyzDvxYh/view?usp=sharing\n\n",
      "technique": "Header extraction"
    }
  ]
}