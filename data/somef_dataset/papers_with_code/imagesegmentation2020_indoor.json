{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1505.04597\r\n\r\n[3]: Accuracy Improvement of UNet Based on Dilated Convolution. https://iopscience.iop.org/article/10.1088/1742-6596/1345/5/052066\r\n\r\n[4]: Bitcoin Insider: Smart Audit.Using AI in International Trade. https://www.bitcoininsider.org/article/48604/smart-audit-using-ai-international-trade\r\n\r\n[5]: Attention U-Net: Learning Where to Look for the Pancreas. Ozan Oktay and others. https://arxiv.org/abs/1804.03999\r\n\r\n[6]: Attention Gated Networks (Image Classification & Segmentation",
      "https://arxiv.org/abs/1804.03999\r\n\r\n[6]: Attention Gated Networks (Image Classification & Segmentation"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1]: NYU depth V2 dataset. https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html\r\n\r\n[2]: Olaf Ronneberger, Philipp Fischer, Thomas Brox. \"U-Net: Convolutional Networks for Biomedical Image Segmentation\". CVPR, 2015. https://arxiv.org/abs/1505.04597\r\n\r\n[3]: Accuracy Improvement of UNet Based on Dilated Convolution. https://iopscience.iop.org/article/10.1088/1742-6596/1345/5/052066\r\n\r\n[4]: Bitcoin Insider: Smart Audit.Using AI in International Trade. https://www.bitcoininsider.org/article/48604/smart-audit-using-ai-international-trade\r\n\r\n[5]: Attention U-Net: Learning Where to Look for the Pancreas. Ozan Oktay and others. https://arxiv.org/abs/1804.03999\r\n\r\n[6]: Attention Gated Networks (Image Classification & Segmentation). https://github.com/ozan-oktay/Attention-Gated-Networks",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Institute: Universitat Polit\u00e8cnica de Catalunya. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| Mean IOU       | 42.54%       | 30.60%            | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906175765576931
      ],
      "excerpt": "Our research was fructiferous and we obtain a paper Accuracy Improvement of UNet Based on Dilated Convolution from Shengyuan Piao and Jiaming Liu, from the University of Beijing, published at November, 2019. [3] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "    filters = [32, 64, 128, 256, 512] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8096653902102603
      ],
      "excerpt": "Experiment 10: Comparison between Unet, Attention Unet and Deep Dilated Unet \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/imagesegmentation2020/indoor",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-12T14:54:58Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-27T09:05:40Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9477399100500472,
        0.8103101534240367,
        0.8370603478590362,
        0.9735379888830579,
        0.8546156931672925
      ],
      "excerpt": "      - metrics: methods to calculate our metrics and show results. \n      - transformations: classes and methods to be available the images and labels to be transformed \n      - models: U-net model \n      - utils: Other methods for miscellaneous goals, as save the current state of the model, select the colors to show labels. \n  - Folder Experiments: Contains the model and checkpoints for each model evaluated on every experiment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.837390111143349,
        0.8944223591797553
      ],
      "excerpt": "  - Model testing: Google colab to load the model and test them. \n  - Weights calculation: Google Colab to calculate the weights for the CrossEntropy weighted loss. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.891165609747666
      ],
      "excerpt": "  - Tensorboard Deeplab Experiments: Google Colab to read results of previous executions, when using Depplab Model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9486708075075907,
        0.9330483895381568,
        0.951569900158593,
        0.9944891231318553,
        0.9913672052744061,
        0.9426348195391017,
        0.9711821068651724,
        0.9675189592726612,
        0.8649439993095681,
        0.994282135614105,
        0.9513663821428956
      ],
      "excerpt": "We decided to pursue an indoor image segmentation task, which is one of the key problems in the field of computer vision. \nOne of the main reasons we chose this topic is because we are interested in object detection and semantic segmentation, and after analyze the datasets available for free, we found a dataset available for semantic segmantation. \nThe architectures normally used in this field also caught our attention. \nWe were excited to learn the most about deep learning, its implementation, and observe how far our creativity can go to improve the performance of our model. \nBesides learning about Deep Learning, we seeked to learn the most about working as a team on a project of this type. \nAnalyze and pre-process the data adapting it to the network. \nLearn how to code a semantic segmentation neural network from scratch, with the help of existing papers of U-Net. \nMitigate the class imbalance to achieve a better performance of the model. \nKnow about how the different loss functions work. \nLearn to implement the metrics to quantify the performance of the model. \nIntroduce to methods to reduce the overfitting, for example, using data augmentation and regularization techniques. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9801951819714257
      ],
      "excerpt": "Implement the functions to calculate the metrics and the loss to evaluate the performance of the model during training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9251803701776122,
        0.8396781616590686,
        0.9954383974714762,
        0.9438415187142264
      ],
      "excerpt": "Improve the performance of the network by changing the hyperparameters. \nGenerate the report, presenting the results obtained with its conclusions. \nIn this project we used Google Colaboratory, which is a free platform where the users can write text and code, so it is executed and saved in the cloud. We made a Google account to store the results of the experiments. \nGoogle colaboratory assigns a random GPU when running, which it sometimes has different computing power. It is a Nvidia GPU, which specifications are described in the following image. There are also daily time limitations of execution of the GPU. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9546945461034317,
        0.98127297225741,
        0.9361352982762122
      ],
      "excerpt": "The size of raw images, which were captured by an RGB camera, is 640 x 480 x 3 channels. The size of the labelled images is 640 x 480 x 1 channel. Containing only the values of the object class for each one of the pixels. \nThis dataset had already defined the splitting of the dataset in training and test sets of images. The training and validation splits are generated randomly with proportionality 80% training and 20% validation using the given training split. The aim of that is to be able to compare the results and the performance of the network with other people who have used this dataset for the image segmentation task as well. \nThe splitting of the dataset is shown at the following table: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9820124246540209,
        0.8464513401663029
      ],
      "excerpt": "By default, the objects labelled in the NYU Depth V2 dataset are classified in 894 different classes. To reduce the complexity of the project and achieve a better understanding of the dataset, a class reduction has been applied, grouping the 894 different classes in 13 classes, that contained more generalizing information of the objects. To grouping the classes a relation to convert 894 to 40 classes and 40 classes to 13 is given by the dataset. For example, the objects \u201cshelf\u201d, \u201cwardrobe\u201d and \u201cdesk\u201d have been grouped into the class \u201cfurniture\u201d. \nIn the following image we can observe the apparition of the different classes in the dataset, for train, validation, and test sets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9468790024802805,
        0.9194993231713834,
        0.9017511993912984
      ],
      "excerpt": "We have adapted the labelled images values so they go between 0 and 12, and we also assigned to the \u201cno label\u201d pixels a value of 255 instead of 0. \nColorization of the labelled images has also been implemented in order to visualize the classes by looking the labelled image: \nA custom dataset class of the loading images and targets was created, in order to automatically resize them to fit them in the network, and also apply some data augmentation techniques. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9756471804734816,
        0.9916941761059866,
        0.9139015221227815,
        0.9265789920536804,
        0.9558357698575557,
        0.9956075320809277,
        0.9347653867650056,
        0.9206640969992113,
        0.9422667906644151,
        0.8344556375963635
      ],
      "excerpt": "This dataset class also applies some transformations like random cropping and center cropping used for data augmentation, controlling that the random is applied at same time for both the image and the target. \nFor data augmentation, we added some noise to the data and modified contrast, saturation and brightness, and also applied horizontal flip to the images, to try to improve the performance of the network and its generalization. \nThe values of the RGB images and its labels have been normalized to facilitate the training of the network, so all the values are between 0 and 1. \nThe architecture selected for our task is the U-Net, one of the most popular networks for computer vision, and specially for segmentation tasks. \nThis model was born in 2015 to solve image segmentation needs for biomedical applications [2]. It consist on a encoder, a bottleneck and a decoder and would be detailed at the paragraph \u201cDisclosing the U-net\u201d \nOur model consists of using the U-Net to convert 256x256 RGB images to get a 256x256 pixels segmentation map of 13 categories. To accomplish that we follow the same structure, but adapt the steps of the original model to our needs. (Mainly modifying the convolutional part). \nThe main difference between U-net and other network architectures is that every pooling layer is mirrored by an up-sampling layer. \nThe mirror permits to reduce the size of the image in the encoder part, reducing the number of parameters that should be calculated. \nFor the previous reason, and also as the number of filters can be parallelized using GPUs is it possible to train the network fastly. \nIt is possible to define the U-net as a convolutional Encoder-Decoder architecture because it is made by an encoder, a decoder and a Neckbottle. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8663952509092815,
        0.9749119823967378,
        0.896380240404022,
        0.982226447452154,
        0.8746143635840946
      ],
      "excerpt": "The encoder part has the goal to obtain information about the input image. It is made by four convolutional blocks and four maxpooling steps. \nEvery Convolutional block is constructed by two blocks of Conv2D, BatchNormalization and ReLU as we can see on the following image: \nThe different elements of the Conv2D has the following goals: \n  - Conv2D: Get information of the input image using different kind of filters. \n  - Batch Normalization: Block to modify the layer, re-centering and re-scaling them to made faster and more stable. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9923882985848208
      ],
      "excerpt": "The maxpooling select the highest value of every 2x2 cell at the end of every convolutional block. The goal of maxpooling is to retain only the important features from each region and throw away non relevant information. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9947712186078657,
        0.9880042818750429
      ],
      "excerpt": "The bottleneck block has the goal to force the model to learn a compression of the input data. The idea is that this model only learns relevant information to try to reconstruct the image. \nThe bottleneck is made by a convolutional block. It can also be used to represent the inputs with reduced dimensionality, because it contains all the information of the input image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9798360412492197
      ],
      "excerpt": "The goal of the decoder is to predict the labels of the input image, using the information obtained by the Encoder and Bottleneck part. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8960417313955129,
        0.9879592462133664,
        0.9393300210754562
      ],
      "excerpt": "The elements of the Decoder part have the following goals: \n  - Unpooling: Increase the size of the features, to try to recover a label with the same dimensions of the input image. \n  - ConvBlock: Use the unpooling features and the higher resolution feature maps from the encoder network to reconstruct features. The feature maps from the encoder network help the decoder part generate reconstructions with accurate details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8845391512937506,
        0.9418397618598836,
        0.9704450200051716,
        0.8386435119614886,
        0.8683183912711478,
        0.9048096294530161
      ],
      "excerpt": "  - Conv2D: Assign a pseudo probability for every input pixel to pertain to a certain class. \nThe output of the Conv2D of the last layer should have the same number of features of the number of classes that should predict. \nThe loss function has the goal to measure how far the prediction of the network is with respect to the expected results. \nThe Cross Entropy is the simplest and most common loss function used in semantic segmentation. This loss examines each pixel individually, comparing the class prediction to our one-hot encoded target vector. \nMathematically is calculated using the following formule: \n  is the target vector. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9909091077040009,
        0.8845933098772102,
        0.9760208167276444,
        0.966664975376165,
        0.9281283702709737,
        0.9940152872759278
      ],
      "excerpt": " is the CNN score of the class 'i' \nDue to the pixels that are not labeled in the target image can\u2019t be predicted, they are not considered in this formula. \nOne of the problems of the Cross Entropy loss is the class imbalance of the dataset, because it learns better classes that appear more than classes that appear less. \nTo reduce this effect, it is possible use the \"Weighted Cross Entropy\" that consists of considering more relevant pixels that appear less. This relevance should have an inverse proportion with the number of pixels. For this reason it is necessary to analyze the number of pixels of each class in the training dataset: \nTo calculate the weights that should be used in the weighted loss we need to calculate the inverted frequency. These are the weights for our train dataset. \nWith this loss ideally it is possible to increase the mean intersection over union, but as it is explained on the experiments, it won\u2019t improve them because the weights of the training are not the weights that maximizes the results of the validation dataset. One of the reasons is because of the size of the dataset, which is not so big. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9406158341506079,
        0.9224264358951372
      ],
      "excerpt": "Where 13 is the number of classses \n is the number of pixels of class 'i' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9895135555291101,
        0.9811489044721544,
        0.9519612459802113,
        0.9142755142010365,
        0.8232983539899711
      ],
      "excerpt": "Metrics are used in semantic segmentation to evaluate results of the training of the network. In our application the goal is maximize the metric called mean intersection over union. \nBy using the metrics, parameters of the model can be selected to maximize the needs of the application. \nTo evaluate the metrics, pixels that are not labeled on the ground truth are not considered. \nThe procedure to calculate the metrics in every epoch are the following one: \n  - Every batch the values of pixel accuracy per class and intersection over union per class are calculated and stored in a dictionary. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8473513907595547
      ],
      "excerpt": "     - Mean pixel accuracy is calculated averaging the pixel accuracy per class of every epoch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8975661487605072
      ],
      "excerpt": "     - Mean intersection over union is calculated averaging the intersection over union per class of every epoch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9203973186689501
      ],
      "excerpt": "Consist on evaluating for each class the percentage of pixels that are well labeled. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9429461363496205,
        0.9700300345293772,
        0.9454811203123573
      ],
      "excerpt": " is the pixel accuracy on batch 'b' of class 'i' \n is the number of pixels of class 'i' of the batch 'b' well labeled \n is the number of pixels of class 'i' on batch 'b' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186215876720722,
        0.9448513341678904
      ],
      "excerpt": " is the pixel accuracy of class 'i' \n is the number of batch that contains the class 'i' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8668295187627404,
        0.8539331441199062
      ],
      "excerpt": "The mean pixel accuracy, as his name defines, compute the mean of the pixel accuracy of the classes calculated in the previous paragraph. \nThe formula used to calculate them is the next one: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9032479533708948,
        0.8323082054586532
      ],
      "excerpt": "The intersection over union per class is calculated as the relation between the pixels well labeled of the class divided by the number of pixels predicted as the class and the number of pixels of the class that are not predicted. \nGraphically is calculated as: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9627017179048742,
        0.9700300345293772,
        0.9206952140628172,
        0.9206952140628172
      ],
      "excerpt": " is the intersection over union on batch 'b' of class 'i'. \n is the number of pixels of class 'i' of the batch 'b' well labeled. \n is the number of pixels of class 'i' of the batch 'b' labeled as class 'i' but pertaining to other class. \n is the number of pixels of class 'i' of the batch 'b' labeled as other class but pertaining to class 'i'. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9527812279344638,
        0.9448513341678904
      ],
      "excerpt": " is the intersection over union of class 'i'. \n is the number of batch that contains the class 'i' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9379390507838162,
        0.9053434692782786,
        0.8979892168108902,
        0.9977008287886631,
        0.8539331441199062,
        0.9939621699239576,
        0.9926964503517675,
        0.8389660554031046
      ],
      "excerpt": "The mean intersection over union, compute the mean of the intersection over union of the classes calculated in the previous paragraph. \nThis metric helps to reduce the effect of the class imbalance, for the following reasons: \n  - Mean pixel accuracy doesn't consider the false positives, and therefore when the network has doubts about predicting the class tends to select one of the classes that appears the most. \n  - In mean intersection over union, the classes with less apparitions have the same influence in the metric than classes with higher apparitions, due to the nature of the calculation of this metric. This makes it a good metric to evaluate unbalanced datasets, as is the case of the dataset in this project. \nThe formula used to calculate them is the next one: \nWe trained the model several times with different optimizers and hyperparameters, aiming to get the best Mean IOU. Which is the metric that best defines the performance of the results, as it is explained above.  \nThe main criterion to select which model is the best for our application is based on the simulation that gets the best mean IOU in any epoch of its simulation. Other factors can be taken into account, for example theoretical reasons or hardware limitations. \nIn every experiment the mean IOU, the Pixel Accuracy and the loss are graphically evaluated. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8935526387033211,
        0.8586735146045786
      ],
      "excerpt": "The ADAM optimizer was selected to our model due to his adaptive gradient and his computacional efficiency. \nIn this experiment two of the typical LR for ADAM are compared: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9326481900163792
      ],
      "excerpt": "We observe that the one with learning rate 1e-3 converges faster, but slightly better results in metrics are obtained with learning rate 5e-4.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9532560938127908,
        0.9803457517022587
      ],
      "excerpt": "The second parameter to evaluate is weight decay regularization. \nUsing the learning rate with the best IOU of the previous step (5e-4), we are going to simulate the difference between using Weight decay 1e-4 and not using it.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9370286370698145
      ],
      "excerpt": "The  regularization does not reduce the overfitting effect but improves slightly the results obtaining an increment of IOU about 1% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9307117088995931
      ],
      "excerpt": "The third experiment consists of evaluating a train between the best optimizer in the previous step (ADAM with Learning rate 5e-4 and weight decay 1e-4) with a SGD with momentum with similar conditions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "SGD with momentum \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "|                | ADAM Decay 0 | SGD with Momentum | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9974066308520635,
        0.9919521216321665
      ],
      "excerpt": "Visualizing the graphics is easy to select ADAM as the best optimizer for the model. ADAM Optimizer is faster than SGD with Momentum, and due to this reason, when the system is trained with a limited number of epochs ADAM get better results. \nFor this reason, because the number of epochs due to restrictions of hardware and time to evaluate, we selected the ADAM optimizer although the IOU of the SGD continues increasing at the last epoch simulated. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8741819578262017
      ],
      "excerpt": "Before the experiment we expect dropout helps to reduce overfitting and therefore improve results with dropout. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8741650821368109,
        0.9279161265424782,
        0.9515133206040135
      ],
      "excerpt": "A dropout with a probability p=0.5 (50%) does not increase the metric performance. \nInstead, a dropout with a probability p=0.2 (20%) seems to get similar metric performance. \nAccording to deep learning theory, the experiments with dropout should provide better results. Even though the results are very similar, we obtained slightly better results with dropout 0,2, so we kept this dropout in the following experiments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9790297012403445,
        0.9409372704180644,
        0.9714292114464536
      ],
      "excerpt": "As it is explained on paragraph \"interpretation of the dataset\" our dataset is imbalanced \nDuring training, it is possible to appreciate that using standard CrossEntropy, classes that appear more in the dataset are learnt better than classes that appear less. \nFor this reason the following experiment consists of force to the system to attend more to classes that appear less on the images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.94600813541007
      ],
      "excerpt": "Previous at the experiment we expect improve results on intersection over union, and reduce also the class imbalance, improving the results of classes with less apparitions on the dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9570966343727969,
        0.8609487934228387
      ],
      "excerpt": "The weighted loss does not improve the mean IOU. The explanation to that fact may improve the results in the ideal case that the distribution of categories in the training split were similar to the distribution of categories in the validation split. \nThe results of the IOU per class are the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9785050825670334
      ],
      "excerpt": "Despite the mean IOU is not better than not using weighted loss, we can see that the classes with less appearance have increased their IOU, and the classes more relevant have decreased. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9306383843344389,
        0.9090885968231586
      ],
      "excerpt": "To try to make the system more robust to different brightness levels, we added different random brightness, contrast and saturation. \nTo do that it is possible using a function called ColorJitter, that consist on add noise, modifying brightness, contrast and saturation randomly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "With ColorJitter \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9373766643947943,
        0.860282697451818
      ],
      "excerpt": "We expect improve the results adding random brightness noise to make the system more robust with different kind of noisy images. \n|                | Without ColorJitter | With ColorJitter | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9364789633160701,
        0.9751173574232551
      ],
      "excerpt": "After the results, we can see as a conclusi\u00f3n that adding noise in our dataset doesn't improve the results, probably because the pictures are in similar brightness conditions  \nUsing colorJitter for data augmentation, stabilizes the loss in the validation split, but the results obtained in the metrics are worse. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9147310301843157
      ],
      "excerpt": "In the following experiment we are going to compare the results using horizontal flip or without using them. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "With Horizontal Flip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9848759026705289,
        0.860282697451818
      ],
      "excerpt": "For this reason it is reasonable that we can do a kind of data augmentation doing horizontal flip on training to try to decrease the overfitting. \n|                | Without Horizontal Flip | With Horizontal Flip | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9815518576613093,
        0.9563145141917505
      ],
      "excerpt": "As we expected, the results on validation are better using Horizontal Flip and also the loss and the metrics are stabilized. \nAfter analyse the results of the experiments, several hypothesis are presented to improve the model, reduce overfitting, and maximize Intersection over Union (Iou) and Pixel Accuracy(Acc). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9000510062266499,
        0.9123823417261434,
        0.9666527218524767,
        0.9251173179413222
      ],
      "excerpt": "Reduce the number of features could improve the overfitting. \nA bigger size in bottleneck features could improve the metric results. \nOur research was fructiferous and we obtain a paper Accuracy Improvement of UNet Based on Dilated Convolution from Shengyuan Piao and Jiaming Liu, from the University of Beijing, published at November, 2019. [3] \nWe show here the modificated Unet model proposed : Deep Dilated Unet with a Parallel Dilated Convolution bottleneck module. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9611386634775368,
        0.9753898620779413,
        0.971231214574451,
        0.9165528935295658
      ],
      "excerpt": "As seen in this model, the number of skipconnections are reduced to 3. The module proposed as bottleneck consists in a 7 dilated convolutions. \nEach one of them works with features from size 32x32x512. This is 512 features with mapping size of 32x32. The dilations rate increases quadratically, from 1 to 32. \nTo adapt this proposed Deep Dilated Unet, to our already implemented Unet working with our dataset, in addition with the quadratic rate convolutions, \nwe approach several prototypes. The problem we found is a trade off among skipconnections number, the number of convolutions in the bottleneck, and the number of channels and size of the features at the bottleneck. We start from RGB images at dataset from size 256x256x3. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9120529492806104
      ],
      "excerpt": "The prototype 1 (Deep Dilated Unet 1-16), has 3 skip connections, 5 dilated convolutions at the bottleneck which dilation rate is from 1 to 16, and features size 32x32x256. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9677014093400739
      ],
      "excerpt": "The prototype 2, (Deep Dilated Unet 1-8) has 4 skip connections, 4 dilated convolutions at the bottleneck which dilation rate is from 1 to 8, and features size 16x16x512. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9776714895355679,
        0.9912382829526718,
        0.8987708369319426,
        0.9710758277497017
      ],
      "excerpt": "We got surprising results:  Prototype 1 (Deep Dilated Unet 1-16), with 3 skip connections, got better results than Prototype 2 (Deep Dilated Unet 1-8) with the four skip connections as our Unet model. But Unet model still has much more better results then the presented prototypes. \nWe concluded that we should improve prototype 1 (Deep Dilated Unet 1-16), because 3 skip connections is best than 4. But the clue is to have, for one side features with mapping space 32x32 with 512 channels at the bottleneck, at in the other side complete the 7 dilated convolution structure somehow. \nOur research was fructiferous and we obtain a document with the final solution. \nAt the webpage Bitcoin Insider: Smart Audit: Using AI in International Trade. [4] we found the model prototype that inspired us to our final solution.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9654295387498794
      ],
      "excerpt": "This model has 3 skip connections, 6 dilated convolutions at the bottleneck which dilation rate is from 1 to 6, increased lineally, and features size 32x32x512 at the bottleneck. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9785310299586579,
        0.8667698868481178
      ],
      "excerpt": "At the first convolution of the Unet, instead to produce feature maps of 32 channels, it produces feature maps of 64 channels. This results in features size 32x32x512 at the bottleneck  \nDilations rate increases lineally instead of quadratically. This give us the possibility to complete the Parallel Dilated Convolution bottleneck module from [3] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8667599137164643
      ],
      "excerpt": "Our Deep Dilated Unet developed : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9868056341625379
      ],
      "excerpt": "Our Deep Dilated Unet model has 3 skip connections, 6 dilated convolutions at the bottleneck which dilation rate is from 1 to 6, increased lineally, and features size 32x32x512 at the bottlenck. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9810439584536048
      ],
      "excerpt": "The results shown there is still overfitting. But we got a quite significally increase of the Intersection Over Union (mIoU) and Pixel Accuracy (mAcc). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9563145141917505
      ],
      "excerpt": "After analyse the results of the experiments, several hypothesis are presented to improve the model, reduce overfitting, and maximize Intersection over Union (Iou) and Pixel Accuracy(Acc). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8565685237187862,
        0.9827551469776987
      ],
      "excerpt": "An attention mechanism in the Unet could improve the metric results. \nTo implement the reduction size of features, we are gonna construct an array filters like this : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.904691407694432,
        0.9645130575721192,
        0.8520870534961787,
        0.9979592737036479,
        0.9957151630454956,
        0.9512123473747482,
        0.8592400786873863
      ],
      "excerpt": "    filters = [int(x / feature_scale) for x in filters] \nThe parameter feature_scale will affect the size of the filters. Passed to the model, it will build one with reduced size in features model.         \nWe encounter, that  \n  - For our classic Unet model, the overfitting is significantly reduced with feature_scale = 4. \n  - For the Deep Dilated Unet model, the overfitting is significantly reduced with feature_scale = 8. \nOur research was fructiferous and we obtain a paper Attention U-Net: Learning Where to Look for the Pancreas from Ozan Oktay and others. [5] \nWe show here the modificated Unet model proposed : Attention U-Net with attention gate (AG) unit. [5] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8907453509244883
      ],
      "excerpt": "The implementation is adapted from the code Attention Gated Networks (Image Classification & Segmentation) [6]. We reuse and adapt some python code modules. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9276718518861788
      ],
      "excerpt": "In the following experiment we are going to compare the results using feature_scale, attention at skipconnections and dilated convolutions at bottleneck. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.95914334757328,
        0.9884829243809936
      ],
      "excerpt": "The results shown there is much less overfitting with feature_scale parameter.  \nWe got a a better results in overfitting with Attention Unet, since validation loss and training loss are close in more epochs. But instead we observe better performance in Intersection Over Union (mIoU) and Pixel Accuracy (mAcc) metrics with Deep Dilated Unet. We observe a peak in metrics that gives Attention Unet to overpass Deep Dilated Unet at the last epochs in simulations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.986387062158751
      ],
      "excerpt": "In the following experiment we are going to compare the results using feature_scale, and Attention Unet single model with Attention + Deep Dilated Unet combined model.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.95914334757328,
        0.9751272526985968
      ],
      "excerpt": "The results shown there is much less overfitting with feature_scale parameter.  \nWe got a better results in overfitting with Attention Unet, since validation loss and training loss are close in more epochs. But instead we observe better performance in Intersection Over Union (mIoU) metric with Attention Deep Dilated Unet combined model. Instead Pixel Accuracy (mAcc) metric has been reduced compared with a  Deep Dilated Unit single model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9194465240036198,
        0.9825192654413342
      ],
      "excerpt": "In the following experiment we are going to compare the results using feature_scale=1 ; this means at the original scale. \nThe models to compare are Deep Dilated Unet single model with Attention + Deep Dilated Unet combined model.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8526303852218546,
        0.9342808194864048,
        0.8991460065978605
      ],
      "excerpt": "The results shown very similar results with differences: \nThere is better response at reducing overfitting with Attention Deep Dilated Unet combined model, since validation loss and training loss are close in more epochs. We observe a faster convergence in loss at training and validation final values. \nWe got a similar results at performance in Intersection Over Union (mIoU) and Pixel Accuracy (mAcc) metrics in both models. Pixel Accuracy (mAcc) metric has some improvement in the Attention Deep Dilated Unit combined model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9291249939707024
      ],
      "excerpt": "In the following experiment we are going to compare the results using a pretrained Network, using two different optimizers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8022655254624435
      ],
      "excerpt": "We expect increase results because the network was pretrained with a huge Dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9445372711093514,
        0.8591956064820979,
        0.9486143742460909
      ],
      "excerpt": "As we expected, the results on validation are better using a model pretrained with a huge dataset than a small dataset. \nSurprisingly for ours, SGD configuration has significantly better results than ADAM. \nAfter select the best model as the best mean intersection over union of all experiments, we can see the following results in testing: \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/imagesegmentation2020/indoor/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 16:37:44 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/imagesegmentation2020/indoor/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "imagesegmentation2020/indoor",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/imagesegmentation2020/indoor/main/Model%20Deeplabv3-Resnet101%20Training.ipynb",
      "https://raw.githubusercontent.com/imagesegmentation2020/indoor/main/Tensorboard%20DeepLab%20Experiments.ipynb",
      "https://raw.githubusercontent.com/imagesegmentation2020/indoor/main/Tensorboard%20DeepDilatedUnet%20Experiments.ipynb",
      "https://raw.githubusercontent.com/imagesegmentation2020/indoor/main/Model%20Training.ipynb",
      "https://raw.githubusercontent.com/imagesegmentation2020/indoor/main/Tensorboard%20Unet%20Experiments.ipynb",
      "https://raw.githubusercontent.com/imagesegmentation2020/indoor/main/Weights%20calculation.ipynb",
      "https://raw.githubusercontent.com/imagesegmentation2020/indoor/main/Tensorboard%20AttentionUnet%20Experiments.ipynb",
      "https://raw.githubusercontent.com/imagesegmentation2020/indoor/main/Model%20Testing.ipynb",
      "https://raw.githubusercontent.com/imagesegmentation2020/indoor/main/Data%20Preprocessing.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8330384949876687
      ],
      "excerpt": "The repository has the following structure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8088767936602097
      ],
      "excerpt": "Instead, a dropout with a probability p=0.2 (20%) seems to get similar metric performance. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9093427579815482
      ],
      "excerpt": "      - datas: class to read images from a CSV file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577510771173124
      ],
      "excerpt": "  - Folder Images: Zip file with all images and labels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8400346790581915
      ],
      "excerpt": "  - Model testing: Google colab to load the model and test them. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.852486936649923
      ],
      "excerpt": "  - Data preprocessing: Google colab to read the dataset and generate images and labels for training, test and validation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137,
        0.925193092977907
      ],
      "excerpt": "| Train      | 636              | 44%        | \n| Test       | 654              | 45%        | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.821080998323074
      ],
      "excerpt": "| Mean Pixel Acc | 67.27%       | 67.50%          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314375690990653
      ],
      "excerpt": "| Mean Pixel Acc | 67.50%       | 58.90%            | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8132419885544244
      ],
      "excerpt": "| Mean Pixel Acc | 67.50%     | 68.58%      | 68.60%      | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.880051300153077
      ],
      "excerpt": "<img src=\"Images/Figures/Prototype1-2_Loss.png\" alt=\"Loss\" /><img src=\"Images/Figures/Prototype1-2_mIoU.png\" alt=\"mIou\" /><img src=\"Images/Figures/Prototype1-2_mAcc.png\" alt=\"mAcc\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8075977199235965
      ],
      "excerpt": "| Mean IOU       | 45.70%              | 43.56%  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8059565119367058,
        0.8482889136278727,
        0.9238353911890825
      ],
      "excerpt": "We show here the modificated Unet model proposed : Attention U-Net with attention gate (AG) unit. [5] \n<img src=\"Images/Figures/Attention U-Net.png\" alt=\"Attention U-Net model\" /> \n<img src=\"Images/Figures/attention gate (AG).png\" alt=\"Attention Gate (AG) unit\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.880051300153077
      ],
      "excerpt": "<img src=\"Images/Figures/Attention_Loss.png\" alt=\"Loss\" /><img src=\"Images/Figures/Attention_mIoU.png\" alt=\"mIou\" /><img src=\"Images/Figures/Attention_mAcc.png\" alt=\"mAcc\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.880051300153077
      ],
      "excerpt": "<img src=\"Images/Figures/Attention_PDUnet_Loss.png\" alt=\"Loss\" /><img src=\"Images/Figures/Attention_PDUnet_mIoU.png\" alt=\"mIou\" /><img src=\"Images/Figures/Attention_PDUnet_mAcc.png\" alt=\"mAcc\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.880051300153077
      ],
      "excerpt": "<img src=\"Images/Figures/Attention_Best_Loss.png\" alt=\"Loss\" /><img src=\"Images/Figures/Attention_Best_mIoU.png\" alt=\"mIou\" /><img src=\"Images/Figures/Attention_Best_mAcc.png\" alt=\"mAcc\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": " Train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8642568393101823
      ],
      "excerpt": "| Mean IOU       | 45.94% | 56.44% | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017168178864091
      ],
      "excerpt": "Mean IOU: 34.09% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540771561731
      ],
      "excerpt": "Example of qualitative results: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/imagesegmentation2020/indoor/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Repository structure",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "indoor",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "imagesegmentation2020",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/imagesegmentation2020/indoor/blob/main/Readme.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 16:37:44 GMT"
    },
    "technique": "GitHub API"
  }
}