{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lonePatient/Bert-Multi-Label-Text-Classification",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-10T15:19:42Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T13:16:39Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9767865281252386,
        0.9652771208610523
      ],
      "excerpt": "This repo contains a PyTorch implementation of the pretrained BERT and XLNET model for multi-label text classification. \nAt the root of the project, you will see: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "|  \u2514\u2500\u2500 model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8596982069667689
      ],
      "excerpt": "|  \u2514\u2500\u2500 output #:save the ouput of model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314910279460742
      ],
      "excerpt": "|  \u2514\u2500\u2500 common #: a set of utility functions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.974376041305329,
        0.9528725823047942
      ],
      "excerpt": "When converting the tensorflow checkpoint into the pytorch, it's expected to choice the \"bert_model.ckpt\", instead of \"bert_model.ckpt.index\", as the input file. Otherwise, you will see that the model can learn nothing and give almost same random outputs for any inputs. This means, in fact, you have not loaded the true ckpt for your model \nWhen using multiple GPUs, the non-tensor calculations, such as accuracy and f1_score, are not supported by DataParallel instance \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This repo contains a PyTorch implementation of a pretrained BERT model for multi-label text classification.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lonePatient/Bert-Multi-Label-Text-Classification/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 173,
      "date": "Wed, 29 Dec 2021 07:40:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lonePatient/Bert-Multi-Label-Text-Classification/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lonePatient/Bert-Multi-Label-Text-Classification",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "|  |  \u2514\u2500\u2500 lrscheduler.py\u3000\u3000 \n|  |  \u2514\u2500\u2500 trainingmonitor.py\u3000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369245672576114
      ],
      "excerpt": "|  |  \u2514\u2500\u2500 basic_config.py #:a configuration file for storing model parameters \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9152750849795331,
        0.9336801098518991
      ],
      "excerpt": "|  |  \u2514\u2500\u2500 dataset.py\u3000\u3000 \n|  |  \u2514\u2500\u2500 data_transformer.py\u3000\u3000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8277380791614312
      ],
      "excerpt": "|  \u2514\u2500\u2500 output #:save the ouput of model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8653523522410159,
        0.828520898273279
      ],
      "excerpt": "|  \u2514\u2500\u2500 train #:used for training a model \n|  |  \u2514\u2500\u2500 trainer.py  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "\u251c\u2500\u2500 run_bert.py \n\u251c\u2500\u2500 run_xlnet.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8768003128815295
      ],
      "excerpt": "training result: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lonePatient/Bert-Multi-Label-Text-Classification/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 lonePatinet\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Bert multi-label text classification by PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Bert-Multi-Label-Text-Classification",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lonePatient",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lonePatient/Bert-Multi-Label-Text-Classification/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n- csv\r\n- tqdm\r\n- numpy\r\n- pickle\r\n- scikit-learn\r\n- PyTorch 1.1+\r\n- matplotlib\r\n- pandas\r\n- transformers=2.5.1\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 620,
      "date": "Wed, 29 Dec 2021 07:40:26 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "pytorch-implmention",
      "nlp",
      "multi-label-classification",
      "text-classification",
      "bert",
      "fine-tuning",
      "xlnet",
      "transformers",
      "albert"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nyou need download pretrained bert model and xlnet model.\r\n\r\n<div class=\"note info\"><p> BERT:  bert-base-uncased</p></div>\r\n\r\n<div class=\"note info\"><p> XLNET:  xlnet-base-cased</p></div>\r\n\r\n1. Download the Bert pretrained model from [s3](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin) \r\n2. Download the Bert config file from [s3](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json) \r\n3. Download the Bert vocab file from [s3](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt) \r\n4. Rename:\r\n\r\n    - `bert-base-uncased-pytorch_model.bin` to `pytorch_model.bin`\r\n    - `bert-base-uncased-config.json` to `config.json`\r\n    - `bert-base-uncased-vocab.txt` to `bert_vocab.txt`\r\n5. Place `model` ,`config` and `vocab` file into  the `/pybert/pretrain/bert/base-uncased` directory.\r\n6. `pip install pytorch-transformers` from [github](https://github.com/huggingface/pytorch-transformers).\r\n7. Download [kaggle data](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) and place in `pybert/dataset`.\r\n    -  you can modify the `io.task_data.py` to adapt your data.\r\n8. Modify configuration information in `pybert/configs/basic_config.py`(the path of data,...).\r\n9. Run `python run_bert.py --do_data` to preprocess data.\r\n10. Run `python run_bert.py --do_train --save_best --do_lower_case` to fine tuning bert model.\r\n11. Run `run_bert.py --do_test --do_lower_case` to predict new data.\r\n\r\n",
      "technique": "Header extraction"
    }
  ]
}