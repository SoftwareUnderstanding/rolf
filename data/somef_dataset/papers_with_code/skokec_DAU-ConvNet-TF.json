{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite our CVPR 2018 paper when using the DAU code/model:\n\n```\n@inproceedings{Tabernik2018,\n\ttitle = {{Spatially-Adaptive Filter Units for Deep Neural Networks}},\n\tauthor = {Tabernik, Domen and Kristan, Matej and Leonardis, Ale{\\v{s}}},\n\tbooktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n\tyear = {2018}\n\tpages = {9388--9396}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{Tabernik2018,\n    title = {{Spatially-Adaptive Filter Units for Deep Neural Networks}},\n    author = {Tabernik, Domen and Kristan, Matej and Leonardis, Ale{\\v{s}}},\n    booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n    year = {2018}\n    pages = {9388--9396}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/skokec/DAU-ConvNet-TF",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-02T16:46:11Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-03T09:39:34Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8025017878785243,
        0.9838112266242472,
        0.8834945362650369
      ],
      "excerpt": "Python/TensorFlow implementation of the Displaced Aggregation Units for Convolutional Networks from CVPR 2018 paper titled \"Spatially-Adaptive Filter Units for Deep Neural Networks\" that was developed as part of Deep Compositional Networks. \nThis code is a less efficient version of DAU-ConvNet that is implemented using only Python/TensorFlow operations and results in: \nfully learnable sigma/standard deviation of DAU (independently for each DAU as done in our ICPR 2016 paper), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222583964196076,
        0.810838892451173
      ],
      "excerpt": "is suitable for prototyping due to its flexibility, and \nis easy to use (only Python code) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9615040782038505,
        0.9791950239014606
      ],
      "excerpt": "performance is dependent on the kernel size based on max displacements, \nis slightly slower code for large displacements, and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Fully Python/TensorFlow implementation of DAU-ConvNet",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/skokec/DAU-ConvNet-TF/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 12:57:08 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/skokec/DAU-ConvNet-TF/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "skokec/DAU-ConvNet-TF",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/skokec/DAU-ConvNet-TF/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DAU-ConvNet in pure TensorFlow",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DAU-ConvNet-TF",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "skokec",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/skokec/DAU-ConvNet-TF/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "skokec",
        "body": "",
        "dateCreated": "2019-09-03T09:30:11Z",
        "datePublished": "2019-09-03T09:38:22Z",
        "html_url": "https://github.com/skokec/DAU-ConvNet-TF/releases/tag/v1.0",
        "name": "Main release",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/skokec/DAU-ConvNet-TF/tarball/v1.0",
        "url": "https://api.github.com/repos/skokec/DAU-ConvNet-TF/releases/19712198",
        "zipball_url": "https://api.github.com/repos/skokec/DAU-ConvNet-TF/zipball/v1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 12:57:08 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Basically this implementation converts DAUs into a single K x K kernel, and then uses a standard conv2d operation, which can be simply done by:\n\n```Python\n    \n    F = 128 #: num output channels\n    G = 4 #: num DAUs per channel\n    S = 64 #: num input channels    \n    max_kernel_size = 17\n    \n    dau_w = tf.Variable(shape=(1,S,G,F))\n    dau_mu1 = tf.Variable(shape=(1,S,G,F))\n    dau_mu2 = tf.Variable(shape=(1,S,G,F))\n    dau_sigma = tf.Variable(shape=(1,S,G,F)) \n    \n    [X,Y] = np.meshgrid(np.arange(max_kernel_size),np.arange(max_kernel_size))\n    \n    X = np.reshape(X,(max_kernel_size*max_kernel_size,1,1,1)) - int(max_kernel_size/2)\n    Y = np.reshape(Y,(max_kernel_size*max_kernel_size,1,1,1)) - int(max_kernel_size/2)\n        \n    #: Gaussian kernel\n    gauss_kernel = tf.exp(-1* (tf.pow(X - dau_mu1,2.0) + tf.pow(Y - dau_mu2,2.0)) / (2.0*tf.pow(dau_sigma,2.0)),name='gauss_kernel')\n    gauss_kernel_sum = tf.reduce_sum(gauss_kernel,axis=0, keep_dims=True,name='guass_kernel_sum')\n    gauss_kernel_norm = tf.divide(gauss_kernel, gauss_kernel_sum ,name='gauss_kernel_norm')\n    \n    #: normalize to sum of 1 and add weight\n    gauss_kernel_norm = tf.multiply(dau_w, gauss_kernel_norm,name='gauss_kernel_weight')\n    \n    #: sum over Gaussian units\n    gauss_kernel_norm = tf.reduce_sum(gauss_kernel_norm, axis=2, keep_dims=True,name='gauss_kernel_sum_units')\n    \n    #: convert to [Kw,Kh,S,F] shape\n    gauss_kernel_norm = tf.reshape(gauss_kernel_norm, (max_kernel_size, max_kernel_size, gauss_kernel_norm.shape[1], gauss_kernel_norm.shape[3]),name='gauss_kernel_reshape')\n      \n    output = tf.nn.conv2d(inputs, gauss_kernel_norm)\n    \n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide a wrapper based on `tf.contrib.layer.conv2d()` API, that is also compatible/interchangeable with the `dau_conv2d` from [DAU-ConvNet](http://github.com/skokec/DAU-ConvNet). \n\nInstall using pip:\n```bash\nsudo pip3 install https://github.com/skokec/DAU-ConvNet-TF/releases/download/v1.0/dau_conv_tf-1.0-py3-none-any.whl  \n```\n\nThere are two available methods to use: \n\n```python\nfrom dau_conv_tf import dau_conv2d_tf\n\ndau_conv2d_tf(inputs,\n             filters, #: number of output filters\n             dau_units, #: number of DAU units per image axis, e.g, (2,2) for 4 DAUs per filter \n             max_kernel_size, #: maximal possible size of kernel that limits the offset of DAUs (highest value that can be used=17)  \n             stride=1, #: only stride=1 supported \n             mu_learning_rate_factor=500, #: additional factor for gradients of mu1 and mu2\n             dau_unit_border_bound=1,\n             data_format=None,\n             activation_fn=tf.nn.relu,\n             normalizer_fn=None,\n             normalizer_params=None,\n             weights_initializer=tf.random_normal_initializer(stddev=0.1), \n             weights_regularizer=None,\n             mu1_initializer=None, \n             mu1_regularizer=None, \n             mu2_initializer=None,\n             mu2_regularizer=None,\n             sigma_initializer=None,\n             sigma_regularizer=None,\n             biases_initializer=tf.zeros_initializer(),\n             biases_regularizer=None,\n             reuse=None,\n             variables_collections=None,\n             outputs_collections=None,\n             trainable=True,\n             scope=None)\n```\n \n```python\nfrom dau_conv_tf import DAUConv2dTF\n\nDAUConv2dTF(filters, #: number of output filters\n           dau_units, #: number of DAU units per image axis, e.g, (2,2) for 4 DAUs total per one filter\n           max_kernel_size, #: maximal possible size of kernel that limits the offset of DAUs (highest value that can be used=17)\n           strides=1, #: only stride=1 supported\n           data_format='channels_first', #: supports only 'channels_last' \n           activation=None,\n           use_bias=True,\n           weight_initializer=tf.random_normal_initializer(stddev=0.1),\n           mu1_initializer=None, \n           mu2_initializer=None, \n           sigma_initializer=None,\n           bias_initializer=tf.zeros_initializer(),\n           weight_regularizer=None,\n           mu1_regularizer=None,\n           mu2_regularizer=None,\n           sigma_regularizer=None,\n           bias_regularizer=None,\n           activity_regularizer=None,\n           weight_constraint=None,\n           mu1_constraint=None,\n           mu2_constraint=None,\n           sigma_constraint=None,\n           bias_constraint=None,\n           trainable=True,\n           mu_learning_rate_factor=500, #: additional factor for gradients of mu1 and mu2\n           dau_unit_border_bound=1,  \n           unit_testing=False, #: for competability between CPU and GPU version (where gradients of last edge need to be ignored) during unit testing\n           name=None)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}