{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1708.03888",
      "https://arxiv.org/abs/1708.03888",
      "https://arxiv.org/abs/1901.08256"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888)\n\n[Large-Batch Training for LSTM and Beyond](https://arxiv.org/abs/1901.08256)\n\nhttps://www.comp.nus.edu.sg/~youy/lars_optimizer.py\n\nhttps://github.com/tensorflow/tpu/blob/5f71c12a020403f863434e96982a840578fdd127/models/official/efficientnet/lars_optimizer.py\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.934079999077463,
        0.9030859728368266,
        0.9030859728368266,
        0.858613587748793
      ],
      "excerpt": "|         512         |    64     |  2<sup>2</sup>  | 10/2<sup>6</sup> |  1e-5   |  77.02%  |    Light blue     | \n|        1024         |    128    | 2<sup>2.5</sup> | 10/2<sup>5</sup> |  1e-5   |  76.96%  |       Brown       | \n|        4096         |    128    | 2<sup>3.5</sup> | 10/2<sup>3</sup> |  1e-5   |  77.38%  |      Orange       | \n|        8192         |    128    |  2<sup>4</sup>  | 10/2<sup>2</sup> |  1e-5   |  77.14%  |     Deep Blue     | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NUS-HPC-AI-Lab/LARS-ImageNet-PyTorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-02-28T09:28:25Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-02T06:40:38Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9920962754686891,
        0.9454787153436541
      ],
      "excerpt": "This is the code for the paper \"Large Batch Training of Convolutional Networks\", which implements a large batch deep learning optimizer called LARS using PyTorch. Although the optimizer has been released for some time and has an official TensorFlow version implementation, as far as we know, there is no reliable PyTorch version implementation, so we try to complete this work. We use Horovod to implement distributed data parallel training and provide accumulated gradient and NVIDIA DALI dataloader as options. \nWe verified the implementation on the complete ImageNet-1K (ILSVRC2012) data set. The parameters and performance as follows. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8899092689691226,
        0.8390790770549811
      ],
      "excerpt": "Accumulated gradient  When the GPUs is insufficient, the accumulated gradient technology can be used, which can simulate larger effective batch size using limited GPUs, although it maybe extend the running time to some extent. To use it, you just need add --batches-per-allreduce N in above command, where N is the scale factor. For example, set N = 4 here can simulate effective batch size 4096 using only  8 GPUs.  \nDALI dataloader NVIDIA DALI can accelerate data loading and pre-processing using GPU rather than CPU, although with GPU memory tradeoff. It can also avoid some potential conflicts between MPI libraries and Horovod on some GPU clusters. To use it, please use 'pytorch_imagenet_resnet_dali.py' with '--data-dir' rather than 'train/val-dir'. For '--data-dir', it requires ImageNet-1k data in TFRecord format in the following structure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Accuracy 77%. Large batch deep learning optimizer LARS for ImageNet with PyTorch and ResNet, using Horovod for distribution. Optional accumulated gradient and NVIDIA DALI dataloader.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NUS-HPC-AI-Lab/LARS-ImageNet-PyTorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Sun, 26 Dec 2021 17:27:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/NUS-HPC-AI-Lab/LARS-ImageNet-PyTorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "NUS-HPC-AI-Lab/LARS-ImageNet-PyTorch",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8532609700891199
      ],
      "excerpt": "We set epochs = 90, weight decay = 0.0001, model = resnet50 and use NVIDIA Tesla V100/P100 GPU for all experiments. We do not finetune the hyperparameters, maybe you can get better performance using others. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8406656370926594
      ],
      "excerpt": "train-recs 'path/train/*'  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8406656370926594
      ],
      "excerpt": "train-idx 'path/idx_files/train/*'  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8055993971367016,
        0.8006390320936013,
        0.8015023243036071
      ],
      "excerpt": "|         512         |    64     |  2<sup>2</sup>  | 10/2<sup>6</sup> |  1e-5   |  77.02%  |    Light blue     | \n|        1024         |    128    | 2<sup>2.5</sup> | 10/2<sup>5</sup> |  1e-5   |  76.96%  |       Brown       | \n|        4096         |    128    | 2<sup>3.5</sup> | 10/2<sup>3</sup> |  1e-5   |  77.38%  |      Orange       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8551928889545153
      ],
      "excerpt": "train-recs 'path/train/*'  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8551928889545153
      ],
      "excerpt": "train-idx 'path/idx_files/train/*'  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/NUS-HPC-AI-Lab/LARS-ImageNet-PyTorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 binmakeswell\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Implementation of LARS for ImageNet with PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "LARS-ImageNet-PyTorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "NUS-HPC-AI-Lab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NUS-HPC-AI-Lab/LARS-ImageNet-PyTorch/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This code is validated to run with Python 3.6.10, PyTorch 1.5.0, Horovod 0.21.1, CUDA 10.0/1, CUDNN 7.6.4, and NCCL 2.4.7.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 21,
      "date": "Sun, 26 Dec 2021 17:27:23 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nfrom lars import *\n...\noptimizer = create_optimizer_lars(model=model, lr=args.base_lr, epsilon=args.epsilon,\n                                momentum=args.momentum, weight_decay=args.wd,\n                                bn_bias_separately=args.bn_bias_separately)\n...\nlr_scheduler = PolynomialWarmup(optimizer, decay_steps=args.epochs * num_steps_per_epoch,\n                                warmup_steps=args.warmup_epochs * num_steps_per_epoch,\n                                end_lr=0.0, power=lr_power, last_epoch=-1)\n...\n```\n\nNote that we recommend using create_optimizer_lars and setting bn_bias_separately=True, instead of using class Lars directly, which helps LARS skip parameters in BatchNormalization and bias, and has better performance in general. Polynomial Warmup learning rate decay is also helpful for better performance in general.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Example scripts for training with 8 GPUs and 1024 effective batch size on ImageNet-1k are provided.\n\n```\n$ mpirun -np 8 \\\npython pytorch_imagenet_resnet.py  \\\n--batch-size 128 \\\n--warmup-epochs 0.3125 \\\n--train-dir=your path/ImageNet/train/ \\\n--val-dir=your path/ImageNet/val \\\n--base-lr 5.6568542494924 \\\n--base-op lars \\\n--bn-bias-separately \\\n--wd 0.0001 \\\n--lr-scaling keep\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}