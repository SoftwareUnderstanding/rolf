{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1609.04802",
      "https://arxiv.org/abs/1903.09922 ).\n\n[6] Datasets Link:\n1. DIV2K Dataset [Dataset-link](https://data.vision.ee.ethz.ch/cvl/DIV2K/)\n2. MS-COCO Dataset [Dataset-link](http://cocodataset.org/#download)\n3. Vid4 Dataset [Dataset-link](https://xinntao.github.io/open-videorestoration/rst_src/datasets_sr.html)\n4. LFW:[Dataset-link](https://www.tensorflow.org/datasets/catalog/lfw)\n5. Set5 and Set14: [Dataset-link](https://www.kaggle.com/ll01dm/set-5-14-super-resolution-dataset)\n\n[7] Agustsson, Eirikur and Timofte, Radu. \u201cNTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study\u201d, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, July 2017.\n\n[8] C.Y. Yang, C. Ma, and M.H. Yang. Single-image super-resolution:A benchmark. In European Conference on Computer Vision (ECCV),pages 372\u2013386. Springer, 2014.\n\n[9] C.Y. Yang, C. Ma, and M.H. Yang. Single-image super-resolution:A benchmark. In European Conference on Computer Vision (ECCV), pages 372\u2013386. Springer, 2014.\n\n[10] [Tensorflow-Documentation](https://www.tensorflow.org/)"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] I.Goodfellow, J.Pouget-Abadie, M.Mirza, B.Xu, D.Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), pages 2672\u20132680, 2014.\n\n[2] C. Ledig, L. Theis, F. Husza \u0301r, J. Caballero, A. Cunningham, A. Acosta, A. P. Aitken, A. Tejani, J. Totz, Z. Wang et al., \u201cPhoto-realistic single image super-resolution using a generative adversarial network,\u201d in CVPR, 2017. [(SRGAN)](https://arxiv.org/abs/1609.04802)\n\n[3] Zhihao Wang, Jian Chen, Steven C.H. Hoi, Fellow, \"Deep Learning for Image Super-resolution: A Survey\", IEEE, 2020.\n\n[4] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.\n\n[5] Nao Takano and Gita Alaghband. \u201cSRGAN: Training Dataset Matters\u201d, 2019. ( arXiv:1903.09922 ).\n\n[6] Datasets Link:\n1. DIV2K Dataset [Dataset-link](https://data.vision.ee.ethz.ch/cvl/DIV2K/)\n2. MS-COCO Dataset [Dataset-link](http://cocodataset.org/#download)\n3. Vid4 Dataset [Dataset-link](https://xinntao.github.io/open-videorestoration/rst_src/datasets_sr.html)\n4. LFW:[Dataset-link](https://www.tensorflow.org/datasets/catalog/lfw)\n5. Set5 and Set14: [Dataset-link](https://www.kaggle.com/ll01dm/set-5-14-super-resolution-dataset)\n\n[7] Agustsson, Eirikur and Timofte, Radu. \u201cNTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study\u201d, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, July 2017.\n\n[8] C.Y. Yang, C. Ma, and M.H. Yang. Single-image super-resolution:A benchmark. In European Conference on Computer Vision (ECCV),pages 372\u2013386. Springer, 2014.\n\n[9] C.Y. Yang, C. Ma, and M.H. Yang. Single-image super-resolution:A benchmark. In European Conference on Computer Vision (ECCV), pages 372\u2013386. Springer, 2014.\n\n[10] [Tensorflow-Documentation](https://www.tensorflow.org/)\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-25T22:31:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-04T22:59:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Super-resolution (SR) of images refers to the process of generating or reconstructing the high- resolution (HR) images from low-resolution images (LR). This project mainly focuses on dealing with this problem of super-resolution using the generative adversarial network, named SRGAN, a deep learning framework. In this project, SRGAN was trained and evaluated using 'DIV2K', \u2018MS-COCO\u2019 and \u2018VID4\u2019 [6] which are the popular datasets for image resolution tasks.\n\nIn total, datasets were merged to form: \n\n1. 5800 training images\n\n2. 100 validation images\n\n3. 4 videos for testing\n\nApart from the datasets mentioned above, \u2018LFW\u2019, \u2018Set5\u2019 and \u2018Set14\u2019 datasets [6] were used to get inferences and compare the performance of models implemented in this project with the models from Ledig et al. [2].\n\nMost of this project is built upon the ideas of Ledig et al [2]. Apart from that, I did some research on comparing the results obtained using different objective functions available in TensorFlow\u2019s \u201cTFGAN\u201d library for loss optimizations of SRGAN. Different model implementations were evaluated for pixel quality through the peak signal-to-noise ratio (PSNR) scores as a metric. Intuitively, this metric does not capture the essence of the perceptual quality of an image. However, it is comparatively easy to use PSNR when evaluating the performance while training the model compared to mean-opinion-score (MOS) that has been used by Ledig et al [2]. To evaluate the perceptual quality of images, I have compared the generated images from both the models. This paper also proposes a method of super-resolution using SRGAN with \u201cPer-Pix loss\u201d which I defined in the losses section of this paper. Based on results from [2] and [5], I have combined both MSE and VGG losses, named it \u201cPer-Pix loss\u201d that stands for \u2018Perceptual and Pixel\u2019 qualities of the image, which resulted in preserving the pixel quality besides improving the perceptual quality of images. Finally, I have compared the models built in this project with the models from Ledig et al. [2] to know the performance and effectiveness of models implemented in this project.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9934972221512273,
        0.9932761885509049,
        0.9927110940962072,
        0.9917090976270854
      ],
      "excerpt": "In first phase of this project, I have implemented the SRGAN which is a GAN-based model using TensorFlow, Keras and other Machine learning APIs. I choose Peak signal-to-noise-ratio PSNR as the key metric to evaluate the model's performance. Proposed a new loss, namely 'Per-Pix' loss, for SRGAN model training and observed significant improvement in PSNR values with fewer iterations of training compared to model trained with 'Perceptual Loss'. \nNow, in second phase of this project, I pickup from the first phase results and focus on comparing the model performances trained separately with 'Per-Pix', 'Perceptual' and 'MSE' losses through 'PSNR' metric. Apart from this, I will do research on using various other model architectures. There is also a great need for proper metric to evaluate the image quality. For this, currently, I found the paper [8] which detailed about different metrics that can be used for evaluating the image resolution quality. In this paper, they have described how various metrics are related to the perceptual quality. So, I will study further on papers [3] and [8] to get deeper understanding and arrive at right approaches in order to solve super-resolution problem. If I find any reasonable approach or ideas that would impact the performance, I will incorporate those into the project. \nComing to training stage of this project, it requires a huge effort to train these massive models. Thus, all of the training will be done using the Google cloud platform (GCP) AI services and products. During training, I make use of NVIDIA Tesla P100 GPUs with CUDA and cuDNN toolkits to leverage faster training offered by GPUs. As a part of training procedure, I will also create a visualization dashboard consisting of model architecture and results using TensorBoard to better interpret the results while training is in progress. After the training stage, the model will be deployed using google cloud AI platform for future predictions. Also, the best model will be used to super-resolve the videos using a new data pipeline to process the videos. Further, I am planning to deploy the model as an application to real-world users using TensorFlow Lite. Overall, in phase 2, I primarily concentrate on training and deploying the SRGAN model besides doing further research. \nInitially, I have implemented the image preprocessing part of project so that images data fits to our model seamlessly. The steps that I have followed are as follows : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9490111887174826,
        0.9089090015905235
      ],
      "excerpt": "Now, discriminator input will be of shape (256, 256, 3) and generator input is (64, 64, 3) which is downsampled version of discriminator input using \"bicubic\" kernel with factor of \"4\"    \nSome of the sample low and high resolution images that are obtained from image preprocessing stage are as shown in below figure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8077507278559276,
        0.9917835141550423,
        0.9698996063168089
      ],
      "excerpt": "Models implemented were evaluated in terms of \"Pixel quality\" and \"Perceptual quality\" through \"PSNR\" and \"Visualization of Images\". \nTo better understand how efficient the models perform, I have compared the results of models implemented in this project with corresponding model results from Ledig et al. [2] using the same datasets, that is, Set5 and Set14 datasets and results were tabulated in the below figure. While comparing, MSE and PERPIX trained models in this project are compared with MSE and VGG54 models from Ledig et al. [2] respectively. \nFrom the tables of below figure, we can observe that the models implemented here in this project are performing well-enough considering the number of training steps that models have been trained for. PSNR values obtained were close to the results from Ledig et al. [2] and with further training, these values will improve. From the below figure, another noticeable point is that PSNR value was slightly dropped between MSE and VGG54 trained models (from Ledig et al. [2]) on both datasets. With the PERPIX loss, since we are preserving the pixel quality through the MSE loss component besides the VGG component, we can observe a slight increase in PSNR between MSE and PERPIX trained models (in this project) on both datasets. Thus, the SRGAN model trained with PERPIX loss can be considered as the better choice over the VGG54 (from Ledig et al. [2]) in terms of pixel quality that we have measured through PSNR. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9051525002715974
      ],
      "excerpt": "<p align=\"center\"> Figure: Comparison tables of PSNR values (in each table cell) on Set5 and Set14 datasets <p align=\"center\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8695197229418324
      ],
      "excerpt": "Note: In the case of PERPIX trained model, this difference can be reduced with further iterations as it can be observed from Ledig et al. [2] that it took almost 2 \u2217 105 training steps to reach notifiable perceptual quality. We can also observe from Ledig et al. [2] that it took 20k iterations to actually diverge from the pre-trained SR-ResNet model (trained with MSE loss) and start learning the high-frequency details. Thus, FURTHER TRAINING IS REQUIRED. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9778364506747026
      ],
      "excerpt": "From figures below, it can be observed that the generated images from MSE trained model are consisting of pixelated boxes, whereas PERPIX trained model\u2019s generated images are more photo-realistic and contain the high-frequency details such as corners around the objects like cups, hands. Also, there are no pixelated boxes as compared to generated images from MSE trained model. Thus, we can infer that the SRGAN model trained with PERPIX loss has started learning the high-frequency details in less number of training steps with more weight towards the VGG loss component of the PERPIX loss. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9947251212711521,
        0.8672487129399602
      ],
      "excerpt": "To conclude, both in terms of pixel and perceptual qualities, the PERPIX trained model performs better and considered as the best model among two models implemented in this project and showed satisfactory results compared to models from Ledig et al. [2] ***(but requires further training)***. PERPIX trained SRGAN model has the potential to balance between smoothening and high-frequency details of images, which is the desirable property in real-world scenarios.  \n***Benefits of PERPIX loss:*** \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.952513679925093,
        0.8287093362032132,
        0.9417731705206832,
        0.8345618850053063,
        0.9324695089624347
      ],
      "excerpt": "2. There is no requirement of a pre-trained model for the generator network \n3. Flexibility to train the model with different weights giving importance towards pixel and perceptual qualities \nUsing PERPIX trained model, I have done super-resolution inferences on faces and videos, and the link to access those is provided below. \nSuper-Resolution of Videos \nNow, the trained models (best models in case of MSE and PERPIX losses) were deployed in the Google Cloud AI platform and can be used for future predictions from almost any application through a REST API call sending JSON payload of shape 4- Dimensional array or tensor, where the first dimension corresponds to the batch size of images and fourth dimension represents the number of color channels of images. The link to the deployment website has been provided below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9872872351943102,
        0.9937736990325866
      ],
      "excerpt": "I have trained the models using the Adam optimizer [4] with \u03b21 = 0.9 and learning rate of 0.0001 for generator and discriminator respectively. For each training step, model training was alternated between the generator and discriminator that is k=1 as described in [1]. In total, I have trained both models (MSE loss and PERPIX loss trained models) for 3.5 * 10^4 steps (where each step is training over a mini-batch of training data). The figure illustrating the basic workflow of this project is provided below. To describe the workflow, both the models were trained in parallel with NVIDIA Tesla P100 GPUs, one on Google Colaboratory and the other on Google Cloud\u2019s AI platform Deep Learning virtual machine. \nDuring phase 1 of this project, the model was built and trained for a few training steps. In phase 2, GPU training, TensorBoard, other code implementations, and bug fixes were done. Later, the model was trained and deployed in the Google Cloud Platform. The following figure demonstrates the workflow of this project involving various tools. For Google Colab training, the data is taken from Google Drive for training. On the other hand, for Google cloud AI Deep learning VM, data was fetched from Google cloud storage. All the models were implemented using TensorFlow (python) and are compatible to run on different platforms. The trained models were then deployed on to the Google Cloud AI platform. Those deployed models have been used for performing inferences and can be used for future predictions of super- resolution on images or videos. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8027392553096159,
        0.9982633423916127,
        0.8558636112476213,
        0.9530725995881689
      ],
      "excerpt": "<p align=\"center\"> Figure: Project Workflow <p align=\"center\"> \nAll of the mentioned project goals were accomplished successfully and currently working on application development part of this project, and research with deep learning techniques to improve performance is ongoing . \n- It saves the storage space of images and provides high resolution images whenever needed \nAdapts to new hardware upgrades(like improved screen resolution of TV, Theatre, etc) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This Repository implements the variant of \" SRGAN \" model, a 'Generative Adversarial Network', to solve super-resolution problem with newly proposed \" PERPIX \" loss",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 14:30:12 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN/master/deploy_model_perpix.ipynb",
      "https://raw.githubusercontent.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN/master/predictions_sets.ipynb",
      "https://raw.githubusercontent.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN/master/SRGAN_GPU_SRGANLOSS.ipynb",
      "https://raw.githubusercontent.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN/master/TF2_SRGAN_GPU_MSELOSS.ipynb",
      "https://raw.githubusercontent.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN/master/SRGAN_GPU_PERPIXLOSS.ipynb",
      "https://raw.githubusercontent.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN/master/deploy_model_mse.ipynb",
      "https://raw.githubusercontent.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN/master/SRGAN_GPU_MSELOSS.ipynb",
      "https://raw.githubusercontent.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN/master/.ipynb_checkpoints/SRGAN-Initial%20model-checkpoint.ipynb",
      "https://raw.githubusercontent.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN/master/.ipynb_checkpoints/SRGAN_GPU_MSELOSS-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8842428593688703
      ],
      "excerpt": "<img src =\"downloaded images/image_preprocess/low_res1.png\" width = \"400\" height = \"400\" /> <img src =\"downloaded images/image_preprocess/high_res1.png\" width = \"400\" height = \"400\" />  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.892323548927039
      ],
      "excerpt": "<img src =\"downloaded images/psnr_results/set5_comparison.png\" width = \"400\" height = \"250\" /> <img src =\"downloaded images/psnr_results/set14_comparison.png\" width = \"400\" height = \"250\" />  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8990996803548087,
        0.8990996803548087,
        0.8864226560541208
      ],
      "excerpt": "<img src =\"downloaded images/sets_results_images/set5 results/result_images1.png\" width = \"800\" height = \"300\" />  \n<img src =\"downloaded images/sets_results_images/set5 results/result_images2.png\" width = \"800\" height = \"300\" />  \n<img src =\"downloaded images/sets_results_images/set14 results/result_images8.png\" width = \"800\" height = \"300\" />  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8905619255337077
      ],
      "excerpt": "<img src =\"downloaded images/tensorboard/mse real.png\" width = \"400\" height = \"500\" /> <img src =\"downloaded images/tensorboard/mse gen.png\" width = \"400\" height = \"500\" />  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8905619255337077
      ],
      "excerpt": "<img src =\"downloaded images/tensorboard/perpix real.png\" width = \"400\" height = \"500\" /> <img src =\"downloaded images/tensorboard/perpix gen.png\" width = \"400\" height = \"500\" />  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8032399341489906
      ],
      "excerpt": "<img src =\"downloaded images/workflow.png\" width = \"900\" height = \"500\" align = \"center\" />  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2020, Srikanth Babu Mandru\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\n3. Neither the name of the copyright holder nor the names of its\\n   contributors may be used to endorse or promote products derived from\\n   this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Title: Super Resolution of images/videos using SRGAN",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Super-Resolution-of-Images-Videos-using-SRGAN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "srikanthmandru",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/srikanthmandru/Super-Resolution-of-Images-Videos-using-SRGAN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 14:30:12 GMT"
    },
    "technique": "GitHub API"
  }
}