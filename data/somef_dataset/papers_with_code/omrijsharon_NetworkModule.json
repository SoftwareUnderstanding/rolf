{
  "citation": [
    {
      "confidence": [
        0.9910494787647849
      ],
      "excerpt": "Paper: https://arxiv.org/pdf/1706.02515.pdf , TL;DR: like leaky ReLU, without the problems of exploding/vanishing gradients. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/omrijsharon/NetworkModule",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-03T10:21:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-09T16:01:47Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9131818041753809,
        0.8033759431102128,
        0.8479484165456702
      ],
      "excerpt": "An easy way to create a fully connected network with pytorch.  \nThis module contains an additional function modules that can be used with pytorch Sequential. \nThis module is an extension to torch.nn module.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "An easy way to create a fully connected network with pytorch. Contains additional module functions that can be added to Sequential.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/omrijsharon/NetworkModule/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 22:04:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/omrijsharon/NetworkModule/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "omrijsharon/NetworkModule",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/omrijsharon/NetworkModule/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "NetworkModule",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "NetworkModule",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "omrijsharon",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/omrijsharon/NetworkModule/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 22:04:04 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The module gets a list of layers and a list of activation functions.\nIn the layer's list, each element corresponds to the number of nodes in the layer, and the length of the list is the number of layers in the network.\n\ni.e.: the code\n```\nL = [16, * 2 * [8] , 4]\nactivation_func = [*(len(L)-2) * [functional.SeLU()], functional.Identity()]\n```\nwill help us to create a network with:\n1. input layer: 16 nodes\n2. hidden layer: 8 nodes, with SeLU as activation function \n3. hidden layer: 8 nodes, with SeLU as activation function \n4. output layer: 4 nodes, with a Identity activation function\n\nnotice that Identity() is a linear activation function. It is exacly like not putting any activation function on the layer. Yet, it is necessary that each layer will have an activation function, except for the 1st/Input layer. In other words, *when no activation function is needed, use functional.Identity()*.\n\nAlso notice that the length of the activation_func list is always smaller by 1 than the layers' list length , because the 1st layer never gets an activation function.\n\n\n\n**example:**\n\n```\nfrom NetworkModule import Network\nfrom NetworkModule import functional as functional\nimport torch.nn as nn\n\ninput_dim = 16\noutput_dim = 4\nhidden_layers = 2*[8]\nL = [input_dim, *hidden_layers, output_dim]\nactivation_func = [functional.SeLU(), functional.Sin(), nn.Softmax(dim=1)]\nnet = Network(L, activation_func, dropout=0.5)\n```\n\nThe network can also use dropout. In this example, the dropout probability is set to 0.5.\n- IMPORTANT: dropout should not be used on the weights between the last 2 layers. In the last example we have 4 layers. Dropout will be activated only on the weights between layers 1-2 and 2-3.\n\n",
      "technique": "Header extraction"
    }
  ]
}