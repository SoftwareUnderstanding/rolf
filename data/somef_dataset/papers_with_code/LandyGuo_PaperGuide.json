{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2105.14103\n\n# \u591a\u6a21\u6001Transformers\n- E2E-VLP: https://arxiv.org/abs/2106.01804 \n  - \u95ee\u9898: \u8bba\u6587\u4e2dtransformer decoder generation\u5b58\u5728\u8f93\u5165\u4fe1\u606f\u6cc4\u6f0f\n  - \u6f5c\u5728\u63d0\u5347\u70b9: \n    - \u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u7684\u89e3\u51b3\u89c6\u9891 \u548c \u6587\u672c\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u7684\u7edf\u4e00\u7684Framework, \u89c6\u9891Encoder-decoder\u89e3\u51b3\u89c6\u9891\u76f8\u5173\u7684\u4efb\u52a1\uff0c \u89c6\u9891\u662f\u5426\u5339\u914d\n    - \u56fe\u50cf\u4efb\u52a1\u5206\u79bb, image Encoder-decoder(\u76ee\u6807\u68c0\u6d4b, caption generation, \u5206\u7c7b\u7b49, \u628aimagenet \u7684\u56fe\u50cflabel\u4e5f\u5229\u7528\u8d77\u6765",
      "https://arxiv.org/abs/2106.01804 \n  - \u95ee\u9898: \u8bba\u6587\u4e2dtransformer decoder generation\u5b58\u5728\u8f93\u5165\u4fe1\u606f\u6cc4\u6f0f\n  - \u6f5c\u5728\u63d0\u5347\u70b9: \n    - \u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u7684\u89e3\u51b3\u89c6\u9891 \u548c \u6587\u672c\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u7684\u7edf\u4e00\u7684Framework, \u89c6\u9891Encoder-decoder\u89e3\u51b3\u89c6\u9891\u76f8\u5173\u7684\u4efb\u52a1\uff0c \u89c6\u9891\u662f\u5426\u5339\u914d\n    - \u56fe\u50cf\u4efb\u52a1\u5206\u79bb, image Encoder-decoder(\u76ee\u6807\u68c0\u6d4b, caption generation, \u5206\u7c7b\u7b49, \u628aimagenet \u7684\u56fe\u50cflabel\u4e5f\u5229\u7528\u8d77\u6765",
      "https://arxiv.org/abs/2011.14503\n  - Transformer\u5728\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u4e2d\u7684\u5e94\u7528 \n\n- TAP: Text-Aware Pre-training for Text-VQA and Text-Caption\n  - \u8f93\u5165: image, roi, ocr \u6a21\u6001\uff0c\u53ef\u4ee5\u4f5c\u4e3aroi_model\u7684\u53c2\u8003\n\n# \u89c6\u9891\u9884\u8bad\u7ec3\n- Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling\n  - \u67b6\u6784\u548cattrim-mmbt\u975e\u5e38\u7c7b\u4f3c\n  - \u7528\u79bb\u6563\u91c7\u6837\u7684video-clip, \u7528\u56fe\u7247\u7279\u5f81\u62bd\u53d6\u4ee3\u66ff\u89c6\u9891\u7279\u5f81\u62bd\u53d6 \n- Video Transformer Network\n  - \u5355\u72ec\u8bbe\u8ba1\u4e00\u4e2a\u65f6\u5e8f\u6a21\u5757transformer\uff0c\u7528\u4e8e\u5b66\u4e60\u89c6\u9891\u65f6\u95f4\u5e8f\u5217 \n- VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text\n  - \u76f4\u63a5\u4ece\u89c6\u9891\u50cf\u7d20\u8fdb\u884c\u5b66\u4e60\n\n# Transformer\u540c\u65f6\u505a\u89c6\u89c9\u548c\u591a\u6a21\u6001\u4efb\u52a1\n- UniT: Multimodal Multitask Learning with a Unified Transformer: https://arxiv.org/abs/2102.10772\n\n# \u6587\u6863\u9884\u8bad\u7ec3\n- DocFormer: End-to-End Transformer for Document Understanding\n  - \u589e\u52a0\u4e00\u4e2a\u65b0\u7684\u56fe\u50cf\u91cd\u5efa\u9884\u8bad\u7ec3\u4efb\u52a1\n",
      "https://arxiv.org/abs/2102.10772\n\n# \u6587\u6863\u9884\u8bad\u7ec3\n- DocFormer: End-to-End Transformer for Document Understanding\n  - \u589e\u52a0\u4e00\u4e2a\u65b0\u7684\u56fe\u50cf\u91cd\u5efa\u9884\u8bad\u7ec3\u4efb\u52a1\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9597829878518437,
        0.9912556242864674
      ],
      "excerpt": "An Attention Free Transformer: https://arxiv.org/abs/2105.14103 \nE2E-VLP: https://arxiv.org/abs/2106.01804  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8340683530535242,
        0.842790493796475
      ],
      "excerpt": "\u56fe\u50cf\u4efb\u52a1\u5206\u79bb, image Encoder-decoder(\u76ee\u6807\u68c0\u6d4b, caption generation, \u5206\u7c7b\u7b49, \u628aimagenet \u7684\u56fe\u50cflabel\u4e5f\u5229\u7528\u8d77\u6765),  text\u4efb\u52a1\u5206\u79bb MLM,  \u591a\u6a21\u6001Encoder(image-text matching) \n\u6240\u6709\u53ef\u4ee5\u83b7\u53d6\u7684\u6570\u636e\u653e\u4e00\u8d77\u505a\u9884\u8bad\u7ec3(COCO/Visual Genome/Conceptual Captions )  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "CLIP: https://github.com/openai/CLIP \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9976093797263226
      ],
      "excerpt": "End-to-End Video Instance Segmentation with Transformers:https://arxiv.org/abs/2011.14503 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8414101574813954
      ],
      "excerpt": "TAP: Text-Aware Pre-training for Text-VQA and Text-Caption \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.89292487927795
      ],
      "excerpt": "Video Transformer Network \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8483913307023051
      ],
      "excerpt": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9670492833072164
      ],
      "excerpt": "UniT: Multimodal Multitask Learning with a Unified Transformer: https://arxiv.org/abs/2102.10772 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/LandyGuo/PaperGuide",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-08T03:05:48Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-14T02:27:05Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9354760107083896
      ],
      "excerpt": "Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8992424301208307
      ],
      "excerpt": "DocFormer: End-to-End Transformer for Document Understanding \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/LandyGuo/PaperGuide/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 18:47:41 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/LandyGuo/PaperGuide/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "LandyGuo/PaperGuide",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8918974083095406
      ],
      "excerpt": "CLIP: https://github.com/openai/CLIP \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8133481352184462
      ],
      "excerpt": "TAP: Text-Aware Pre-training for Text-VQA and Text-Caption \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/LandyGuo/PaperGuide/issues{/number}",
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Transformer\u6548\u7387\u4f18\u5316",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PaperGuide",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "LandyGuo",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/LandyGuo/PaperGuide/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 18:47:41 GMT"
    },
    "technique": "GitHub API"
  }
}