{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We thank Ming-Yu Liu for an early review, Timo Viitanen for his help with code release, and Tero Kuosmanen for compute infrastructure.\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.08500",
      "https://arxiv.org/abs/1606.03498",
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/1904.06991"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@article{Karras2019stylegan2,\n  title   = {Analyzing and Improving the Image Quality of {StyleGAN}},\n  author  = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},\n  journal = {CoRR},\n  volume  = {abs/1912.04958},\n  year    = {2019},\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Karras2019stylegan2,\n  title   = {Analyzing and Improving the Image Quality of {StyleGAN}},\n  author  = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},\n  journal = {CoRR},\n  volume  = {abs/1912.04958},\n  year    = {2019},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9941533744942845
      ],
      "excerpt": "Paper: http://arxiv.org/abs/1912.04958<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8334710283794773,
        0.8753150968738145
      ],
      "excerpt": "For business inquiries, please contact researchinquiries@nvidia.com<br> \nFor press and other inquiries, please contact Hector Marinez at hmarinez@nvidia.com<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8956259629933797
      ],
      "excerpt": "| &boxvr;&nbsp; stylegan2-video.mp4 | High-quality version of the video \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8422862053358879
      ],
      "excerpt": "| pr50k3    | 0.689 / 0.492  | 26 min | 17 min  | 12 min | Precision and Recall \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/b4nn3d/stylegan2encoder",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-11T11:46:29Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-15T18:55:48Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9635540871549964
      ],
      "excerpt": "This is a port of Puzer/stylegan-encoder for NVlabs/stylegan2, plus a modified StyleGAN2 projector. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8108731727373031
      ],
      "excerpt": "Replace step 2 with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.968443920163481
      ],
      "excerpt": "This is usually preferable. It also allows you to render a video of the optimization process. To see all available options, type: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8802695721951891
      ],
      "excerpt": "Analyzing and Improving the Image Quality of StyleGAN<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9835764315336535
      ],
      "excerpt": "Abstract: The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent vectors to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably detect if an image is generated by a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051988754586414
      ],
      "excerpt": "| &boxvr;&nbsp; stylegan2-paper.pdf | High-quality version of the paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043984993190003
      ],
      "excerpt": "Projecting images to latent space: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8885141820379509
      ],
      "excerpt": "We have verified that the results match the paper when training with 1, 2, 4, or 8 GPUs. Note that training FFHQ at 1024&times;1024 resolution requires GPU(s) with at least 16 GB of memory. The following table lists typical training times using NVIDIA DGX-1 with 8 Tesla V100 GPUs: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8950816142632219,
        0.8826059627549617
      ],
      "excerpt": "For other configurations, see the StyleGAN2 Google Drive folder. \nNote that the metrics are evaluated using a different random seed each time, so the results will vary between runs. In the paper, we reported the average result of running each metric 10 times. The following table lists the available metrics along with their expected runtimes and random variation: \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/b4nn3d/stylegan2encoder/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 17:33:55 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/b4nn3d/stylegan2encoder/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "b4nn3d/stylegan2encoder",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/b4nn3d/stylegan2encoder/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/b4nn3d/stylegan2encoder/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Datasets are stored as multi-resolution TFRecords, similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory, e.g., `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections, the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments, e.g., `--dataset=ffhq --data-dir=~/datasets`.\n\n**FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords, run:\n\n```.bash\npushd ~\ngit clone https://github.com/NVlabs/ffhq-dataset.git\ncd ffhq-dataset\npython download_ffhq.py --tfrecords\npopd\npython dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq\n```\n\n**LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords, run:\n\n```.bash\npython dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384\npython dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256\npython dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256\npython dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256\n```\n\n**Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords, run:\n\n```.bash\npython dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images\npython dataset_tool.py display ~/datasets/my-custom-dataset\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9961428138875512,
        0.9893272198983933,
        0.9906248903846466
      ],
      "excerpt": "pip install tensorflow-gpu==1.14 \ngit clone https://github.com/rolux/stylegan2encoder.git \ncd stylegan2encoder \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python align_images.py raw_images/ aligned_images/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python encode_images.py aligned_images/ generated_images/ latent_representations/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python project_images.py aligned_images/ generated_images/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python project_images.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8114488624004937
      ],
      "excerpt": "| StyleGAN2 | Main Google Drive folder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8027133024627905
      ],
      "excerpt": ": Generate uncurated car images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8344311767521461
      ],
      "excerpt": "  --dataset=car --data-dir=~/datasets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9054017614873153,
        0.8782615320428928,
        0.9054017614873153,
        0.8761283009744154,
        0.9054017614873153,
        0.8761283009744154,
        0.9054017614873153,
        0.896198593558758,
        0.9054017614873153,
        0.896198593558758
      ],
      "excerpt": "python run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\ \n  --dataset=ffhq --mirror-augment=true \npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\ \n  --dataset=car --total-kimg=57000 \npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\ \n  --dataset=cat --total-kimg=88000 \npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\ \n  --dataset=church --total-kimg 88000 --gamma=100 \npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\ \n  --dataset=horse --total-kimg 100000 --gamma=100 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9226509042444692
      ],
      "excerpt": "python run_generator.py generate-images --seeds=0-999 --truncation-psi=1.0 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/b4nn3d/stylegan2encoder/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# StyleGAN2 &mdash; Encoder/Projector for Official TensorFlow Implementation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "stylegan2encoder",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "b4nn3d",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/b4nn3d/stylegan2encoder/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Both Linux and Windows are supported. Linux is recommended for performance and compatibility reasons.\n* 64-bit Python 3.6 installation. We recommend Anaconda3 with numpy 1.14.3 or newer.\n* TensorFlow 1.14 or 1.15 with GPU support. The code does not support TensorFlow 2.0.\n* On Windows, you need to use TensorFlow 1.14 &mdash; TensorFlow 1.15 will not work.\n* One or more high-end NVIDIA GPUs, NVIDIA drivers, CUDA 10.0 toolkit and cuDNN 7.5. To reproduce the results reported in the paper, you need an NVIDIA GPU with at least 16 GB of DRAM.\n* Docker users: use the [provided Dockerfile](./Dockerfile) to build an image with the required library dependencies.\n\nStyleGAN2 relies on custom TensorFlow ops that are compiled on the fly using [NVCC](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html). To test that your NVCC installation is working correctly, run:\n\n```.bash\nnvcc test_nvcc.cu -o test_nvcc -run\n| CPU says hello.\n| GPU says hello.\n```\n\nOn Windows, the compilation requires Microsoft Visual Studio to be in `PATH`. We recommend installing [Visual Studio Community Edition](https://visualstudio.microsoft.com/vs/) and adding into `PATH` using `\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"`.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 28 Dec 2021 17:33:55 GMT"
    },
    "technique": "GitHub API"
  }
}