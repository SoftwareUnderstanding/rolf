{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1907.11692",
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9783707574233336
      ],
      "excerpt": "[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9126419181843836,
        0.9698483046903941,
        0.9977994744046882,
        0.985735709622531
      ],
      "excerpt": "[4] https://www.kdnuggets.com/2018/12/bert-sota-nlp-model-explained.html \n[5] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, \nRoBERTa: A Robustly Optimized BERT Pretraining Approach, (https://arxiv.org/abs/1907.11692) \n[6] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le, XLNet: Generalized Autoregressive Pretraining for Language Understanding, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "[9] https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bluejurand/Kaggle_QA_Google_Labeling",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-24T11:28:33Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-12T08:20:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Question-answering problem is currenlty one of the most chellenging task in Natural Language Processing domain. In purpose to solve it transfer\nlearning is state of the art method. Thanks to huggingface-transformers which made avaiable pretrained NLP most advanced models (like: BERT,\n GPT-2, XLNet, RoBERTa, DistilBERT) relatively easy to be used in different language tasks.  \nOriginal [akensert](https://www.kaggle.com/akensert/quest-bert-base-tf2-0) code was tested with different parameters and changed base models.\nFrom both implemented (XLNet, RoBERTa) the second one resulted in better score. Further improvement ccould be made by implementation of\ncombined model version. For example it could consists of BERT, RoBERTa and XLNet.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Change of main algorithm from BERT to RoBERTa was justified by the fact that second one is an improved version of the first one. The expansion\nof the algorithm name is Robustly Optimized BERT Pretraining Approach, it modifications consists of [5]:\n- training the model longer, with bigger batches, over more data; \n- removing the next sentence prediction objective; \n- training on longer sequences; \n- dynamically changing the masking pattern applied to the training data.\n\nUse of RoBERTa consequently causes the need of configuration change and implementation of RoBERTa sepcific tokenizer.\nIt constructs a RoBERTa BPE tokenizer, derived from the GPT-2 tokenizer, using byte-level Byte-Pair-Encoding.\nWhich works in that order:\n1. Prepare a large enough training data (i.e. corpus)\n2. Define a desired subword vocabulary size\n3. Split word to sequence of characters and appending suffix \u201c</w>\u201d to end of word with word frequency. So the basic unit is\ncharacter in this stage. For example, the frequency of \u201clow\u201d is 5, then we rephrase it to \u201cl o w </w>\u201d: 5\n4. Generating a new subword according to the high frequency occurrence.\n5. Repeating step 4 until reaching subword vocabulary size which is defined in step 2 or the next highest frequency pair is 1.\n\nValues of the tuning parameters (folds, epochs, batch_size) was mostly implicated by the kaggle GPU power and competition constrain of kernel\ncomputation limitation to 2 hours run-time.\n\nFinal step was calculation of predicitons taking into acount results averaged results for folds.\nWeights have been assigned by empricialy tring different values. The change of particular ones was based on the prediction score.\nLimitation was only the summing up of weights to one.\nChange of arithmetic mean of folds predictions to weighted average improved results in public leaderboard from 0.38459 to 0.38798.\nOn the other hand as the final scores on the private leaderboard showed it was not good choice. Finally, soo strictly assignment of\nweights caused the decrease in final result from 0.36925 to 0.36724.\n\nXLNet was also tested (to show it, the code with it was left commented).\nIn theory XLNet should overcome BERT limitations. Relying on corrupting the input with masks, BERT neglects dependency between the masked\npositions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons XLNet, which is characterised by:\n- learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order;\n- overcomes the limitations of BERT thanks to its autoregressive formulation;\n- integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.\nEmpirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering,\nnatural language inference, sentiment analysis, and document ranking [6].  \n\nAfter all public score for XLNet version was lower (0.36310) than score (0.37886) for base BERT model and was rejected.  \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\"In this competition, you\u2019re challenged to use this new dataset to build predictive algorithms for different subjective aspects of\nquestion-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion.\nOur raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts.\nAs such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task.\nBy lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set.\"[1]  \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9556935210289095,
        0.9800046624125056,
        0.8126146635985848
      ],
      "excerpt": "My changes consisted of (indicated by commented out parts of the code): \n- change model from BERT to RoBERTa (modifications of: tokenizer, model itself, different configuration of model inter alia, vocalbury size, \nmaximal position of embedding); \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9558341196307333,
        0.9943026789494441
      ],
      "excerpt": "- change of arithmetic mean of epochs predictions to weighted average. \nTo practice deep learning in keras enviroment, transfer learning and get familiar with state of the art of Natural Language Processing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8503829381739865
      ],
      "excerpt": "Deep Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.837806216476862
      ],
      "excerpt": "It uses Huggingface transformer library implementation of BERT to solve this question-answering problem. Name BERT stands for Bidirectional \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9037587921738517,
        0.9501344514463582,
        0.994570917933026,
        0.9478766470019198,
        0.8354789014027051,
        0.9704273048920282,
        0.9203303037859495
      ],
      "excerpt": "As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads \nthe entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that \nit\u2019s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings \n(left and right of the word).[3] \nTraining of BERT  \nBefore feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict \nthe original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. In technical terms, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9707777013054807,
        0.9641092859720458,
        0.9727721398232596,
        0.9266411189369175,
        0.9980536375483287,
        0.9697415944338418
      ],
      "excerpt": "Calculating the probability of each word in the vocabulary with softmax. \nIn case of next sentence prediction algorithm is like this: \n1. A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence. \n2. A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token \nembeddings with a vocabulary of 2. \n3. A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9426486193544259
      ],
      "excerpt": "5. The output of the [CLS] token is transformed into a 2\u00d71 shaped vector, using a simple classification layer (learned matrices of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9828166011691677,
        0.972495508785708,
        0.8557453739096383
      ],
      "excerpt": "6. Calculating the probability of IsNextSequence with softmax. \nTo implement BERT fine-tuning is required, it consists of: \n1. Classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification layer on top of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9474195732996525,
        0.9141927868768909,
        0.9077136283637627
      ],
      "excerpt": "2. In Question Answering tasks (e.g. SQuAD v1.1), the software receives a question regarding a text sequence and is required to mark the answer \nin the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer. \n3. In Named Entity Recognition (NER), the software receives a text sequence and is required to mark the various types of entities (Person, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278930638015495
      ],
      "excerpt": "The original English-language BERT model used two corpora in pre-training: BookCorpus and English Wikipedia.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8957688785170662,
        0.9125523579408461
      ],
      "excerpt": "language processing (NLP) model called Hierarchical Multi-Task Learning (HMTL), managed a library of pre-trained NPL models under \nPyTorch-Transformers [7] and also in last time implemented them in Tensorflow 2.0.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9482834718602394
      ],
      "excerpt": "Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.849142965830465,
        0.9731829532423475,
        0.9019084705047843
      ],
      "excerpt": "As huggingface documentation [8] states BertTokenizer() constructs a BERT tokenizer, based on WordPiece. \nIt relies on the initialization the vocabulary to every character present in the corpus and progressively \nlearn a given number of merge rules, it doesn\u2019t choose the pair that is the most frequent but the one that will \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9228671018043538,
        0.9781108280474116,
        0.9210633187440198,
        0.9101459464812902,
        0.8927793648375625,
        0.8497580898479555,
        0.945526590934036
      ],
      "excerpt": "It means that only merge \u2018u\u2019 and \u2018g\u2019 if the probability of having \u2018ug\u2019 divided by \nthe probability of having \u2018u\u2019 then \u2018g\u2019 is greater than for any other pair of symbols. \nThis tokenizer inherits from PreTrainedTokenizer which contains most of the methods. \nPreTrainedTokenizer is a base class for all slow tokenizers. \nHandle all the shared methods for tokenization and special tokens as well as methods downloading/caching/loading pretrained tokenizers as well \nas adding tokens to the vocabulary. \nThis class also contain the added tokens in a unified way on top of all tokenizers so it does not requires to handle the specific vocabulary \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9815133791073936
      ],
      "excerpt": "Model creation starts with loading tensorflow BERT model. Subsequently call of this model is used to generate question and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807636094961956
      ],
      "excerpt": "1. input_ids - indices of input sequence tokens in the vocabulary; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9513823941885174
      ],
      "excerpt": "1 for tokens that are NOT MASKED, 0 for MASKED tokens; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9792259756067191,
        0.8486070539429006,
        0.90454190340543,
        0.976816765153097,
        0.9764083217502432
      ],
      "excerpt": "in [0, 1]: 0 corresponds to a sentence A token, 1 corresponds to a sentence B token. \nNext step is one dimensional global average pooling performed on the embeddings, which are concatenated. \nSubsequently dropout with rate 0.2 and finally dense layer performed to get 30 target labels for questions and aswers. \nOn each epoch end the spearmen corelation is calculated in order to have information of score type values the same as it is used in competition. \nSpearman's rank correlation coefficient is a nonparametric measure of rank correlation (statistical dependence between the rankings of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9265410003316119
      ],
      "excerpt": "[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8972053889277957
      ],
      "excerpt": "[3] https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8069216424114248
      ],
      "excerpt": "[6] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le, XLNet: Generalized Autoregressive Pretraining for Language Understanding, \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bluejurand/Kaggle_QA_Google_Labeling/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 22:08:27 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bluejurand/Kaggle_QA_Google_Labeling/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "bluejurand/Kaggle_QA_Google_Labeling",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/bluejurand/Kaggle_QA_Google_Labeling/master/q-a-labeling-bert-base-huggingface-transformer.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Python is a requirement (Python 3.3 or greater, or Python 2.7). Recommended enviroment is Anaconda distribution to install Python and Jupyter (https://www.anaconda.com/download/).\n\n__Installing dependencies__  \nTo install can be used pip command in command line.  \n  \n\tpip install -r requirements.txt\n\n__Installing python libraries__  \nExemplary commands to install python libraries:\n \n\tpip install numpy  \n\tpip install pandas  \n\tpip install xgboost  \n\tpip install seaborn \n\t\nAdditional requirement is Tensorflow GPU support. Process of configuiring it is described [here](https://www.tensorflow.org/install/gpu).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8044890189769122
      ],
      "excerpt": "[7] https://golden.com/wiki/Hugging_Face-39P6RJJ \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8535355481847942,
        0.8535355481847942,
        0.8535355481847942,
        0.8535355481847942
      ],
      "excerpt": "#q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.float32) \n    #a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.float32) \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32) \n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8509548777984706,
        0.8973550643270167
      ],
      "excerpt": "test_predictions = [np.average(test_predictions[i], axis=0, weights=[1./18, 1./6, 2./9, 2./9, 1./3]) for i in range(len(test_predictions))] \ntest_predictions = np.mean(test_predictions, axis=0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8260293383555951
      ],
      "excerpt": "df_sub.to_csv('submission.csv', index=False) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815944027936717
      ],
      "excerpt": "Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bluejurand/Kaggle_QA_Google_Labeling/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Kaggle Q&A Google Labeling competition",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Kaggle_QA_Google_Labeling",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "bluejurand",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bluejurand/Kaggle_QA_Google_Labeling/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 22:08:27 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\tdef create_model():\n\t\tq_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\t\ta_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\t\t\n\t\tq_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\t\ta_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\t\t\n\t\t",
      "technique": "Header extraction"
    }
  ]
}