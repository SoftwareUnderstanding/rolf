{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2106.08261},\n        archivePrefix = {arXiv},\n        eprint = {2106.08261},\n        Year = {2021}\n    }\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this codebase useful in your research, please consider citing:\n\n    @inproceedings{bear2021physion,\n        Title={Physion: Evaluating Physical Prediction from Vision in Humans and Machines},\n        author= {Daniel M. Bear and\n               Elias Wang and\n               Damian Mrowca and\n               Felix J. Binder and\n               Hsiao{-}Yu Fish Tung and\n               R. T. Pramod and\n               Cameron Holdaway and\n               Sirui Tao and\n               Kevin A. Smith and\n               Fan{-}Yun Sun and\n               Li Fei{-}Fei and\n               Nancy Kanwisher and\n               Joshua B. Tenenbaum and\n               Daniel L. K. Yamins and\n               Judith E. Fan},\n        url = {https://arxiv.org/abs/2106.08261},\n        archivePrefix = {arXiv},\n        eprint = {2106.08261},\n        Year = {2021}\n    }\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{bear2021physion,\n    Title={Physion: Evaluating Physical Prediction from Vision in Humans and Machines},\n    author= {Daniel M. Bear and\n           Elias Wang and\n           Damian Mrowca and\n           Felix J. Binder and\n           Hsiao{-}Yu Fish Tung and\n           R. T. Pramod and\n           Cameron Holdaway and\n           Sirui Tao and\n           Kevin A. Smith and\n           Fan{-}Yun Sun and\n           Li Fei{-}Fei and\n           Nancy Kanwisher and\n           Joshua B. Tenenbaum and\n           Daniel L. K. Yamins and\n           Judith E. Fan},\n    url = {https://arxiv.org/abs/2106.08261},\n    archivePrefix = {arXiv},\n    eprint = {2106.08261},\n    Year = {2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9999999453580998
      ],
      "excerpt": "Daniel M. Bear, Elias Wang, Damian Mrowca, Felix J. Binder, Hsiao-Yu Fish Tung, R.T. Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, Li Fei-Fei, Nancy Kanwisher, Joshua B. Tenenbaum, Daniel L.K. Yamins, Judith E. Fan \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9970843225568294
      ],
      "excerpt": "Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B. Tenenbaum, Antonio Torralba ** \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/htung0101/Physion-particles",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-21T02:25:35Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-10T06:22:04Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9928227472268023,
        0.9592044532967136
      ],
      "excerpt": "This is the official implementation of particle-based models (GNS and DPI-Net) on the Physion dataset. \nThe code is built based on the original implementation of DPI-Net (https://github.com/YunzhuLi/DPI-Net). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9303555001080315
      ],
      "excerpt": "Learning to Simulate Complex Physics with Graph Networks  [paper] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9220524079299228
      ],
      "excerpt": "Rigid Bodies, Deformable Objects, and Fluids  [website] [paper] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9809530519212043
      ],
      "excerpt": "holds data for the particle-based models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8473974987542507
      ],
      "excerpt": "You can visualize the original videos and the generated particle scenes with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8226245814782441
      ],
      "excerpt": "3) For evalution on the red-hits-yellow prediciton, we can get the binary red-hits-yellow label txt file from the test dataset with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9863162916185126,
        0.9902487724092792,
        0.9866328390625613
      ],
      "excerpt": "Our implementation is different from the original DPI paper in 2 ways: (1) our model takes as inputs relative \npositions as opposed to absolute positions, (2) our model is trained with injected noise. These two features \nare suggested in the GNS paper, and we found them to be critcial for the models to generalize well to unseen \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8039013575721133
      ],
      "excerpt": "Models and model logs are saved under [out_dir]/dump/dump_TDWdominoes. You can visualize the training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8704430776405275
      ],
      "excerpt": "You can get the prediction txt file under eval/eval_TDWdominoes/[MODEL_NAME], e.g., test-Drape.txt, which contains results of testing the model on the Drape scenario. You can visualize the results with additional argument --vis 1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9024125711458021
      ],
      "excerpt": "Here we provide some example of evaluating on arbitray models trained on all scenarios. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9419845345589848
      ],
      "excerpt": "We should see a gif for the original RGB videos, and another gif for the side-by-side comparison of gt particle scenes and the predicted \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Official implementation of particle-based models (GNS and DPI-Net) on the Physion dataset.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/htung0101/Physion-particles/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 12:31:32 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/htung0101/Physion-particles/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "htung0101/Physion-particles",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/upload.sh",
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/run_preprocessing_tdw_cheap.sh",
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/run_get_label_txt.sh",
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/scripts/eval_all_gns_ransac.sh",
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/scripts/train_dpi.sh",
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/scripts/eval_all_gns.sh",
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/scripts/eval_all_dpi.sh",
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/scripts/eval_gns.sh",
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/scripts/vis_gns.sh",
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/scripts/train_gns.sh",
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/scripts/train_all.sh",
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/scripts/eval_gns_ransac.sh",
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/scripts/eval_dpi.sh",
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/scripts/vis_dpi.sh",
      "https://raw.githubusercontent.com/htung0101/Physion-particles/master/scripts/vis_gns_ransac.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "open **paths.yaml** and write your own path there.\nYou can set up different paths for different machines under different user name.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We use binvox to transform object mesh into particles. To use binvox, please download **binvox**\nfrom https://www.patrickmin.com/binvox/, put it under ./bin, and include it in your path with\n```\nexport PATH=$PATH:$PWD/bin.\n```\nYou might need to do **chmod 777 binvox** in order to execute the file.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "For Conda users, we provide an installation script:\n```\nbash ./scripts/conda_deps.sh\npip install pyyaml\n```\n\n\n\nTo use tensorboard for training visualization\n```\npip install tensorboardX\npip install tensorboard\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Clone this repo:\n```\ngit clone https://github.com/htung0101/DPI-Net-p.git\ncd DPI-Net-p\ngit submodule update --init --recursive\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9359831126561968,
        0.987661277436341
      ],
      "excerpt": "bash run_preprocessing_tdw_cheap.sh [SCENARIO_NAME] [MODE] \ne.g., bash run_preprocessing_tdw_cheap.sh Dominoes train SCENARIO_NAME can be one of the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.845897444940461,
        0.9331632367618001
      ],
      "excerpt": "3) For evalution on the red-hits-yellow prediciton, we can get the binary red-hits-yellow label txt file from the test dataset with \nbash run_get_label_txt.sh [SCENARIO_NAME] test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9541214504957959
      ],
      "excerpt": "Ok, now we are ready to start training the models.You can use the following command to train from scratch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936863904247939,
        0.9414102165092624
      ],
      "excerpt": "bash scripts/train_gns.sh [SCENARIO_NAME] [GPU_ID] \nSCENARIO_NAME can be one of the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936863904247939
      ],
      "excerpt": "bash scripts/train_dpi.sh [SCENARIO_NAME] [GPU_ID] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9272266856785002
      ],
      "excerpt": "You can also train with more than one scenarios by adding different scenario to the argument dataf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.875458402755927
      ],
      "excerpt": "bash scripts/eval_gns.sh [TRAIN_SCENARIO_NAME] [EPOCH] [ITER] [Test SCENARIO_NAME] [GPU_ID] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.875458402755927
      ],
      "excerpt": "bash scripts/eval_gns_ransac.sh [TRAIN_SCENARIO_NAME] [EPOCH] [ITER] [Test SCENARIO_NAME] [GPU_ID] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.875458402755927
      ],
      "excerpt": "bash scripts/eval_dpi.sh [TRAIN_SCENARIO_NAME] [EPOCH] [ITER] [Test SCENARIO_NAME] [GPU_ID] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8559266769387087,
        0.8559266769387087,
        0.8559266769387087
      ],
      "excerpt": "bash eval_all_gns.sh [EPOCH] [ITER] [Test SCENARIO_NAME] [GPU_ID] \nbash eval_all_dpi.sh [EPOCH] [ITER] [Test SCENARIO_NAME] [GPU_ID] \nbash eval_all_gns_ransac.sh [EPOCH] [ITER] [Test SCENARIO_NAME] [GPU_ID] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8559266769387087
      ],
      "excerpt": "bash vis_gns.sh [EPOCH] [ITER] [Test SCENARIO_NAME] [GPU_ID] \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.840977161653111
      ],
      "excerpt": "1) We need to convert the mesh scenes into particle scenes. This line will generate a separate folder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8548166654479674
      ],
      "excerpt": "python preprocessing_tdw_cheap.py --scenario Dominones --mode \"train\" --visualization 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8046435552974375,
        0.9246227682586091
      ],
      "excerpt": "2) Then, try generate a train.txt and valid.txt files that indicates the trials you want to use for training and validaiton. \npython create_train_valid.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021156783284755
      ],
      "excerpt": "bash run_get_label_txt.sh [SCENARIO_NAME] test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "Train GNS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815764638198152
      ],
      "excerpt": "bash scripts/eval_gns.sh [TRAIN_SCENARIO_NAME] [EPOCH] [ITER] [Test SCENARIO_NAME] [GPU_ID] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815764638198152
      ],
      "excerpt": "bash scripts/eval_gns_ransac.sh [TRAIN_SCENARIO_NAME] [EPOCH] [ITER] [Test SCENARIO_NAME] [GPU_ID] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815764638198152
      ],
      "excerpt": "bash scripts/eval_dpi.sh [TRAIN_SCENARIO_NAME] [EPOCH] [ITER] [Test SCENARIO_NAME] [GPU_ID] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8598660865583957,
        0.8598660865583957,
        0.8598660865583957
      ],
      "excerpt": "bash eval_all_gns.sh [EPOCH] [ITER] [Test SCENARIO_NAME] [GPU_ID] \nbash eval_all_dpi.sh [EPOCH] [ITER] [Test SCENARIO_NAME] [GPU_ID] \nbash eval_all_gns_ransac.sh [EPOCH] [ITER] [Test SCENARIO_NAME] [GPU_ID] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8598660865583957
      ],
      "excerpt": "bash vis_gns.sh [EPOCH] [ITER] [Test SCENARIO_NAME] [GPU_ID] \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/htung0101/Physion-particles/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Physion: Evaluating Physical Prediction from Vision in Humans and Machines [[paper]](https://arxiv.org/pdf/2106.08261.pdf)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Physion-particles",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "htung0101",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/htung0101/Physion-particles/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For Conda users, we provide an installation script:\n```\nbash ./scripts/conda_deps.sh\npip install pyyaml\n```\n\n\n\nTo use tensorboard for training visualization\n```\npip install tensorboardX\npip install tensorboard\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Sat, 25 Dec 2021 12:31:32 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Rollout from our learned model (left is ground truth, right is prediction)\n\n**Dominoes**\n![](imgs/dominoes.gif)\n**Roll**\n![](imgs/rollingSliding.gif)\n**Contain**\n![](imgs/containment.gif)\n**Drape**\n![](imgs/clothSagging.gif)\n\n",
      "technique": "Header extraction"
    }
  ]
}