{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1406.2661\n\n## WGAN\n\n**title**\n\nWasserstein GAN\n\n**Times**\n\n2017\n\n**Authors**\n\nMartin Arjovsky, Soumith Chintala, L\u00e9on Bottou\n\n**Abstract**\n\nWe introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.\n\n**Content introduction**\n\nhttps://blog.csdn.net/qq_41076797/article/details/116898649\n\n**Paper address**\n\nhttps://arxiv.org/abs/1701.07875\n\n## WGAN-GP\n\n**title**\n\nImproved Training of Wasserstein GANs\n\n**Times**\n\n2017\n\n**Authors**\n\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron \nCourville\n\n**Abstract**\n\nGenerative Adversarial Networks (GANs",
      "https://arxiv.org/abs/1701.07875\n\n## WGAN-GP\n\n**title**\n\nImproved Training of Wasserstein GANs\n\n**Times**\n\n2017\n\n**Authors**\n\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron \nCourville\n\n**Abstract**\n\nGenerative Adversarial Networks (GANs",
      "https://arxiv.org/abs/1704.00028\n\n\n\n## LargeScaleOT\n\n**title**\n\nLarge scale optimal transport and mapping estimation\n\n**Times**\n\n2018\n\n**Authors**\n\nVivien Seguy\u3001Bharath Bhushan Damodaran\u3001R\u00e9mi Flamary\u3001Nicolas Courty\u3001Antoine Rolet\u3001Mathieu Blondel\n\n**Abstract**\n\nThis paper presents a novel two-step approach for the fundamental problem of \nlearning an optimal map from one distribution to another. First, we learn an \noptimal transport (OT",
      "https://arxiv.org/abs/1711.02283\n\n## JCPOT\n\n**title**\n\nOptimal Transport for Multi-source Domain Adaptation under Target\nShift\n\n**Times**\n\n2019\n\n**Authors**\n\nIevgen Redko \u3001Nicolas Courty \u3001R\u00e9mi Flamary \u3001Devis Tuia\n\n**Abstract**\n\nIn this paper, we tackle the problem of reducing discrepancies between multiple \ndomains, i.e. multi-source domain adaptation, and consider it under the target \nshift assumption: in all domains we aim to solve a classification problem with \nthe same output classes, but with different labels proportions. This problem, \ngenerally ignored in the vast majority of domain adaptation papers, is \nnevertheless critical in real-world applications, and we theoretically show its \nimpact on the success of the adaptation. Our proposed method is based on optimal \ntransport, a theory that has been successfully used to tackle adaptation \nproblems in machine learning. The introduced approach, Joint Class Proportion \nand Optimal Transport (JCPOT",
      "https://arxiv.org/abs/1803.10081\n\n## DCWD\n\n**title**\n\nDomain-attention Conditional Wasserstein Distance\nfor Multi-source Domain Adaptation\n\n**Times**\n\n2020\n\n**Authors**\n\nHANRUI WU \u3001YUGUANG YAN  \u3001 MICHAEL K. NG \u3001QINGYAO WU\n\n**Abstract**\n\nMulti-source domain adaptation has received considerable attention due to its \neffectiveness of leveraging the knowledge from multiple related sources with \ndifferent distributions to enhance the learning performance. One of the \nfundamental challenges in multi-source domain adaptation is how to determine the \namount of knowledge transferred from each source domain to the target domain. To \naddress this issue, we propose a new algorithm, called Domain-attention \nConditional Wasserstein Distance (DCWD",
      "https://arxiv.org/abs/1412.3474\n\n## DAN\n\n**title**\n\nLearning Transferable Features with Deep Adaptation Networks\n\n**Times**\n\n2015\n\n**Authors**\n\nMingsheng Long  Yue Cao  Jianmin Wang  Michael I. Jordan\n\n**Abstract**\n\nRecent studies reveal that a deep neural network can learn transferable features \nwhich generalize well to novel tasks for domain adaptation. However, as deep \nfeatures eventually transition from general to specific along the network, the \nfeature transferability drops significantly in higher layers with increasing \ndomain discrepancy. Hence, it is important to formally reduce the dataset bias \nand enhance the transferability in task-specific layers. In this paper, we \npropose a new Deep Adaptation Network (DAN",
      "https://arxiv.org/abs/1902.00415\n\n## WDAN\n\n**title**\n\nMind the Class Weight Bias: Weighted Maximum Mean Discrepancy for Unsupervised \nDomain Adaptation\n\n**Times**\n\n2017\n\n**Authors**\n\nHongliang Yan, Y ukang Ding, Peihua Li, Qilong Wang, Y ong Xu, Wangmeng Zuo\n\n**Abstract**\n\nIn domain adaptation, maximum mean discrepancy (MMD",
      "https://arxiv.org/abs/1705.00609\n\n## MCDA\n\n**title**\n\nDeep multi-Wasserstein unsupervised domain adaptation\n\n**Times**\n\n2019\n\n**Authors**\n\nTien-Nam Le , Amaury Habrard , Marc Sebban\n\n**Abstract**\n\nIn unsupervised domain adaptation (DA",
      "https://arxiv.org/abs/1812.01754\n\n## CMSS\n\n**title**\n\nCurriculum manager for source selection in multi- source domain adaptation\n\n**Times**\n\n2020\n\n**Authors**\n\nLuyu Yang, Yogesh Balaji, Ser-Nam Lim, Abhinav Shrivastava\n\n**Abstract**\n\nThe performance of Multi-Source Unsupervised Domain Adaptation depends significantly on the effectiveness of transfer from labeled source domain samples. In this paper, we proposed an adversarial agent that learns a dynamic curriculum for source samples, called Curriculum Manager for Source Selection (CMSS",
      "https://arxiv.org/abs/2007.01261\n\n## LtC-MSDA\n\n**title**\n\nLearning to Combine: Knowledge Aggregation for Multi-Source Domain Adaptation\n\n**Times**\n\n2020\n\n**Authors**\n\nHang Wang , Minghao Xu , Bingbing Ni , and Wenjun Zhang\n\n**Abstract**\n\nTransferring knowledges learned from multiple source domains to target domain is a more practical and challenging task than conventional single-source domain adaptation. Furthermore, the increase of modalities brings more difficulty in aligning feature distributions among multiple domains. To mitigate these problems, we propose a Learning to Combine for Multi-Source Domain Adaptation (LtC-MSDA",
      "https://arxiv.org/abs/2007.08801\n\n## Dirt-T\n\n**title**\n\nA DIRT-T approach to unsupervised domain adaptation\n\n**Times**\n\n2018\n\n**Authors**\n\nRui Shu, Hung H. Bui, Hirokazu Narui, & Stefano Ermon\n\n**Abstract**\n\nDomain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky,2015"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Generative Adversarial Nets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9449187648369126
      ],
      "excerpt": "2014 NIPS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9109170852450571
      ],
      "excerpt": "Optimal Transport for Multi-source Domain Adaptation under Target \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8745679699601063
      ],
      "excerpt": "domains, i.e. multi-source domain adaptation, and consider it under the target  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9090589062165046,
        0.9742490452670491
      ],
      "excerpt": "problems in machine learning. The introduced approach, Joint Class Proportion  \nand Optimal Transport (JCPOT), performs multi-source adaptation and target shift  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9237727085035603
      ],
      "excerpt": "for Multi-source Domain Adaptation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9990108094887582
      ],
      "excerpt": "HANRUI WU \u3001YUGUANG YAN  \u3001 MICHAEL K. NG \u3001QINGYAO WU \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9989902221627448
      ],
      "excerpt": "Jian Shen, Yanru Qu, Weinan Zhang\u2217, Y ong Yu \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9312907016997455
      ],
      "excerpt": "Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, Trevor Darrell \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9412230998982531
      ],
      "excerpt": "previously published results on a standard benchmark visual domain adaptation  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554441738822752
      ],
      "excerpt": "Mingsheng Long  Yue Cao  Jianmin Wang  Michael I. Jordan \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9970973501678682
      ],
      "excerpt": "Mingsheng Long  Han Zhu  Jianmin Wang  Michael I. Jordan \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9891605818130551
      ],
      "excerpt": "Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9898689759446914
      ],
      "excerpt": "Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, Daniel Ulbricht \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9737097586946903,
        0.9468141635371006
      ],
      "excerpt": "effectiveness and genericness of our method on digit and sign recognition, image  \nclassification, semantic segmentation, and object detection. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151442409103197
      ],
      "excerpt": "Joint Partial Optimal Transport for Open Set Domain Adaptation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9992727543373428
      ],
      "excerpt": "Renjun Xu, Pelen Liu, Yin Zhang, Fang Cai, Jindong Wang, Shuoying Liang, Heting \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9972781719882056
      ],
      "excerpt": "Hongliang Yan, Y ukang Ding, Peihua Li, Qilong Wang, Y ong Xu, Wangmeng Zuo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.842790493796475
      ],
      "excerpt": "Coupled generative adversarial networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9969389849313013
      ],
      "excerpt": "Ming-Yu Liu , Oncel Tuzel \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9726805786299247
      ],
      "excerpt": "Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9237727085035603
      ],
      "excerpt": "Moment Matching for Multi-Source Domain Adaptation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9996350469817206
      ],
      "excerpt": "Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, Bo Wang \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8887093677713733
      ],
      "excerpt": "Curriculum manager for source selection in multi- source domain adaptation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702105236373249
      ],
      "excerpt": "Luyu Yang, Yogesh Balaji, Ser-Nam Lim, Abhinav Shrivastava \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9810232764722594
      ],
      "excerpt": "Learning to Combine: Knowledge Aggregation for Multi-Source Domain Adaptation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.993027276580589
      ],
      "excerpt": "Hang Wang , Minghao Xu , Bingbing Ni , and Wenjun Zhang \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CtrlZ1/Domain-Adaptation-Algorithms",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-05T10:55:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T02:32:33Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9731529571265919,
        0.9198623139686889
      ],
      "excerpt": "Welcome to visit my work space, I'm Yiyang Li, at least in the next three years (2021-2024), I will be here to record what I studied in graduate student stage about Domain Adaptation, such as literature introduction and code implementation, etc. I look forward to working with you scholars and experts in communication and begging your comments.  \nPS. This code base is based on models, which is more convenient for learning a single model. If you want to avoid cumbersome conventional code (such as data reading, etc.), you can visit the following link:https://github.com/CtrlZ1/Domain-Adaptive-CodeBase. It presents various domain adaptive codes in the form of projects. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8427431600031599
      ],
      "excerpt": "We propose a new framework for estimating generative models via an adversarial  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9658525869177588
      ],
      "excerpt": "captures the data distribution, and a discriminative model D that estimates the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9817390618578464,
        0.9918598418510449,
        0.8371490516836214,
        0.893772109164259
      ],
      "excerpt": "training procedure for G is to maximize the probability of D making a mistake.  \nThis framework corresponds to a minimax two-player game. In the space of  \narbitrary functions G and D, a unique solution exists, with G recovering the  \ntraining data distribution and D equal to1 2everywhere. In the case where G and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8463627163412538
      ],
      "excerpt": "backpropagation. There is no need for any Markov chains or unrolled approximate  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8756605684449089,
        0.8705557101792806,
        0.8906128909950556
      ],
      "excerpt": "demonstrate the potential of the framework through qualitative and quantitative  \nevaluation of the generated samples. \nContent introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9961930051248495,
        0.8906128909950556
      ],
      "excerpt": "We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions. \nContent introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811632236546332,
        0.9801628702613304,
        0.9110277259866307,
        0.986912690670289,
        0.8690540152533487,
        0.9444579706487233,
        0.9398444592228706
      ],
      "excerpt": "only poor samples or fail to converge. We find that these problems are often due  \nto the use of weight clipping in WGAN to enforce a Lipschitz constraint on the  \ncritic, which can lead to undesired behavior. We propose an alternative to  \nclipping weights: penalize the norm of gradient of the critic with respect to  \nits input. Our proposed method performs better than standard WGAN and enables  \nstable training of a wide variety of GAN architectures with almost no  \nhyperparameter tuning, including 101-layer ResNets and language models with  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906128909950556
      ],
      "excerpt": "Content introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9098074465594534
      ],
      "excerpt": "This paper presents a novel two-step approach for the fundamental problem of  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9520272094318142,
        0.8517890875687005,
        0.9589639787480919,
        0.8475973953741599,
        0.9480670280368243,
        0.9656728372474285,
        0.9905164793693997,
        0.8815248658519469,
        0.8711183497762851
      ],
      "excerpt": "the two distributions. To that end, we propose a stochastic dual approach of  \nregularized OT, and show empirically that it scales better than a recent related  \napproach when the amount of samples is very large. Second, we estimate a Monge  \nmap as a deep neural network learned by approximating the barycentric projection  \nof the previously-obtained OT plan. This parameterization allows generalization  \nof the mapping outside the support of the input measure. We prove two  \ntheoretical stability results of regularized OT which show that our estimations  \nconverge to the OT plan and Monge map between the underlying continuous  \nmeasures. We showcase our proposed approach on two applications: domain  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906128909950556
      ],
      "excerpt": "Content introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8394075339097179
      ],
      "excerpt": "In this paper, we tackle the problem of reducing discrepancies between multiple  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9279473361940885
      ],
      "excerpt": "shift assumption: in all domains we aim to solve a classification problem with  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9748486339944662,
        0.8476053006964913,
        0.9960410714836839
      ],
      "excerpt": "generally ignored in the vast majority of domain adaptation papers, is  \nnevertheless critical in real-world applications, and we theoretically show its  \nimpact on the success of the adaptation. Our proposed method is based on optimal  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8847075904696309,
        0.9037549271290307
      ],
      "excerpt": "correction simultaneously by learning the class probabilities of the unlabeled  \ntarget sample and the coupling allowing to align two (or more) probability  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9417911435150914,
        0.9496977585835255,
        0.8906128909950556
      ],
      "excerpt": "image pixel classification) task show the superiority of the proposed method  \nover the state-of-the-art. \nContent introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9215872603355609,
        0.9290629222229614,
        0.9436421186425358,
        0.8384949741586202,
        0.9788069493915491,
        0.9779013252081197,
        0.9457676948637632,
        0.9816352314219816,
        0.8943526236208801,
        0.9616215512640888,
        0.8906128909950556
      ],
      "excerpt": "labels are known. Our work makes the following assumption: there exists a  \nnon-linear transformation between the joint feature/label space distributions of  \nthe two domain Ps and Pt. We propose a solution of this problem with optimal  \ntransport, that allows to recover an estimated target P^f_t= (X, f(X)) by  \noptimizing simultaneously the optimal coupling and f. We show that our method  \ncorresponds to the minimization of a bound on the target error, and provide an  \nefficient algorithmic solution, for which convergence is proved. The versatility  \nof our approach, both in terms of class of hypothesis or loss functions is  \ndemonstrated with real world classification and regression problems, for which  \nwe reach or surpass state-of-the-art results. \nContent introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8716551246562448,
        0.8944601485054445,
        0.947818319430658,
        0.9004340057410453
      ],
      "excerpt": "data sharing similar characteristics (e.g. same classes), but also different  \nlatent data structures (e.g. different acquisition conditions). In such a  \nsituation, the model will perform poorly on the new data, since the classifier  \nis specialized to recognize visual cues specific to the source domain. In this  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9539677519675458,
        0.9171422249586806,
        0.8245520407867869,
        0.9007995261401619,
        0.8774782179778391
      ],
      "excerpt": "measure of discrepancy on joint deep representations/labels based on optimal  \ntransport, we not only learn new data representations aligned between the source  \nand target domain, but also simultaneously preserve the discriminative  \ninformation used by the classifier. We applied DeepJDOT to a series of visual  \nrecognition tasks, where it compares favorably against state-of-the-art deep  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906128909950556
      ],
      "excerpt": "Content introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391084037779758,
        0.9448285421465973
      ],
      "excerpt": "effectiveness of leveraging the knowledge from multiple related sources with  \ndifferent distributions to enhance the learning performance. One of the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8926036478616747
      ],
      "excerpt": "Conditional Wasserstein Distance (DCWD), to learn transferred weights for  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8908481000905535,
        0.822225758585059
      ],
      "excerpt": "compute the transferred weights of different source domains based on their  \nconditional Wasserstein distances to the target domain. After that, the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8919662059587576,
        0.9651278785716463,
        0.8906128909950556
      ],
      "excerpt": "several real-world data sets, and the results demonstrate the effectiveness and  \nefficiency of the proposed method. \nContent introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9290185312156476
      ],
      "excerpt": "different but related data distribution. One solution to domain adaptation is to  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8923164909802913
      ],
      "excerpt": "should also be discriminative in prediction. To learn such representations,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8675763481549896
      ],
      "excerpt": "learning approach to measure and reduce the domain discrepancy, as well as a  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8663578777075299,
        0.9160277653011865
      ],
      "excerpt": "feature extractor network to minimize the estimated Wasserstein distance in an  \nadversarial manner. The theoretical advantages of Wasserstein distance for  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8504386704920579,
        0.9748476228840226
      ],
      "excerpt": "bound. Empirical studies on common sentiment and image classification adaptation  \ndatasets demonstrate that our proposed WDGRL outperforms the state-of-the-art  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906128909950556
      ],
      "excerpt": "Content introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8832560589040753
      ],
      "excerpt": "Recent reports suggest that a generic supervised deep CNN model trained on a  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9779006779861394,
        0.907006762154002,
        0.8722617740603533
      ],
      "excerpt": "amount of data, which for many applications is simply not available. We propose  \na new CNN architecture which introduces an adaptation layer and an additional  \ndomain confusion loss, to learn a representation that is both semantically  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9283985866263316,
        0.8596407359310078,
        0.9179846137309086
      ],
      "excerpt": "metric can be used for model selection to determine the dimension of an  \nadaptation layer and the best position for the layer in the CNN architecture.  \nOur proposed adaptation method offers empirical performance which exceeds  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906128909950556
      ],
      "excerpt": "Content introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9782160209658392
      ],
      "excerpt": "Learning Transferable Features with Deep Adaptation Networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8727320617007682,
        0.8847100557332125,
        0.8081856995191138
      ],
      "excerpt": "Recent studies reveal that a deep neural network can learn transferable features  \nwhich generalize well to novel tasks for domain adaptation. However, as deep  \nfeatures eventually transition from general to specific along the network, the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8512223841942096,
        0.8920412139304563,
        0.8102600859831532
      ],
      "excerpt": "domain discrepancy. Hence, it is important to formally reduce the dataset bias  \nand enhance the transferability in task-specific layers. In this paper, we  \npropose a new Deep Adaptation Network (DAN) architecture, which generalizes deep  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8866441279490559
      ],
      "excerpt": "representations of all task-specific layers are embeddedin a reproducing kernel  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9020735273645539
      ],
      "excerpt": "explicitly matched. The domain discrepancy is further reduced using an optimal  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9208229525948849
      ],
      "excerpt": "transferable features with statistic alguarantees,and can scale linearly by  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906128909950556
      ],
      "excerpt": "Content introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.938338528662953
      ],
      "excerpt": "Deep Transfer Learning with Joint Adaptation Networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9717600757959398
      ],
      "excerpt": "Deep networks have been successfully applied to learn transferable features for  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9135525491166459
      ],
      "excerpt": "paper, we present joint adaptation networks (JAN), which learn a transfer  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802539912416887,
        0.8907992206778969,
        0.898586611936329
      ],
      "excerpt": "across domains based on a joint maximum mean discrepancy (JMMD) criterion.  \nAdversarial training strategy is adopted to maximize JMMD such that the  \ndistributions of the source and target domains are made more distinguishable.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9329016767169871,
        0.9413938254523494,
        0.8906128909950556
      ],
      "excerpt": "computed by back-propagation in linear-time. Experiments testify that our model  \nyields state of the art results on standard datasets. \nContent introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.950946824384928
      ],
      "excerpt": "In this work, we present a method for unsupervised domain adaptation. Many  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8756060460427258
      ],
      "excerpt": "domain classifier only tries to distinguish the features as a source or target  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9229821502559434,
        0.8168875653822054,
        0.9372134531609767,
        0.8640666617770469,
        0.8304594725961228,
        0.9092845235975691,
        0.9869319202944571,
        0.8625734438322108,
        0.8906128909950556
      ],
      "excerpt": "distributions between different domains, which is difficult because of each  \ndomain\u2019s characteristics. To solve these problems, we introduce a new approach  \nthat attempts to align distributions of source and target by utilizing the  \ntask-specific decision boundaries. We propose to maximize the discrepancy  \nbetween two classifiers\u2019 outputs to detect target samples that are far from the  \nsupport of the source. A feature generator learns to generate target features  \nnear the support to minimize the discrepancy. Our method outperforms other  \nmethods on several datasets of image classification and semantic segmentation. \nContent introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8925371381098632
      ],
      "excerpt": "In this work, we connect two distinct concepts for unsupervised domain  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9116304146814065,
        0.9238873769526268,
        0.968009709763716,
        0.8804572192930057
      ],
      "excerpt": "task-specificdecision boundary [57] and the Wasserstein metric [72]. Our  \nproposed sliced Wasserstein discrepancy (SWD) is designed to capture the natural  \nnotion of dissimilarity between the outputs of task-specific classifiers. It  \nprovides a geometrically meaningful guidance to detect target samples that are  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9367235015994445,
        0.9770138559989004
      ],
      "excerpt": "in an end-to-end trainable fashion. In the experiments, we validate the  \neffectiveness and genericness of our method on digit and sign recognition, image  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906128909950556
      ],
      "excerpt": "Content introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8575083212926422,
        0.9204592845350161,
        0.9618361246498198
      ],
      "excerpt": "contains classes that are never observed in the source domain, namely in Open  \nSet Domain Adaptation (OSDA), existing DA methods failed to work because of the  \ninterference of the extra unknown classes. This is a much more challenging  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8127122149243328,
        0.8999196020708994
      ],
      "excerpt": "between the unknown and known classes. Existing researches are susceptible to  \nmisclassification when target domain unknown samples in the feature space  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.910326842608502
      ],
      "excerpt": "utilizing information of not only the labeled source domain but also the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8279588456460073,
        0.9654401282828003,
        0.8857196560872094,
        0.8949983662837108,
        0.8558716840280941
      ],
      "excerpt": "and variance of the unknown class through backpropagation, which remains  \nintractable for previous methods due to the blindness about the structure of the  \nunknown classes. To our best knowledge, this is the first optimal transport  \nmodel for OSDA. Extensive experiments demonstrate that our proposed model can  \nsignificantly boost the performance of open set domain adaptation on standard DA  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906128909950556
      ],
      "excerpt": "Content introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8720316848191574
      ],
      "excerpt": "Normalized Wasserstein for Mixture Distributions with Applications in  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9534560442320053
      ],
      "excerpt": "Understanding proper distance measures between distributions is at the core of  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156081810806604,
        0.9032025906162409
      ],
      "excerpt": "etc. In this work, we focus on mixture distributions that arise naturally in  \nseveral application domains where the data contains different sub-populations. F  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.94488724976449,
        0.8288162273430537,
        0.8683159784453708,
        0.933677582827093
      ],
      "excerpt": "often leads to undesired results in distance-based learning methods for mixture  \ndistributions. In this paper , we resolve this issue by introducing the  \nNormalized Wasserstein measure. The key idea is to introduce mixture proportions  \nas optimization variables, effectively normalizing mixture proportions in the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8954843316763088,
        0.8020314517927558,
        0.8886638515951062
      ],
      "excerpt": "to significant performance gains for mixture distributions with imbalanced  \nmixture proportions compared to the vanilla Wasserstein distance. We demonstrate  \nthe effectiveness of the proposed measure in GANs, domain adaptation and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906128909950556
      ],
      "excerpt": "Content introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8212254640788781
      ],
      "excerpt": "a discrepancy metric between the distributions of source and target domains.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9019709736285032,
        0.9158833355781748
      ],
      "excerpt": "show that MMD cannot account for class weight bias and results in degraded  \ndomain adaptation performance. To address this issue, a weighted MMD model is  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808737623281254
      ],
      "excerpt": "weights into the original MMD for exploiting the class prior probability on  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9871519080394426
      ],
      "excerpt": "target domain is unavailable. To account for it, our proposed weighted MMD model  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8258878075266534
      ],
      "excerpt": "domain, and a classification EM algorithm is suggested by alternating between  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8954662748493214
      ],
      "excerpt": "parameters. Extensive experiments demonstrate the superiority of our weighted  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906128909950556
      ],
      "excerpt": "Content introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8368564877540773
      ],
      "excerpt": "data and fully unlabeled target examples a model with a low error on the target  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8854968388006329
      ],
      "excerpt": "the source and target domains, and (c) the combined error of the ideal joint  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8303665730002009,
        0.847065079859424,
        0.9517795674908283
      ],
      "excerpt": "using deep neural networks \u2013 h a v e focused on the first two terms by using  \ndifferent divergence measures to align the source and target distributions on a  \nshared latent feature space, while ignoring the third term, assuming it is  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9264664118528743
      ],
      "excerpt": "so-called negative transfer . In this paper, we address this issue with a new  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.813452869284623,
        0.9618155388838185,
        0.8906128909950556
      ],
      "excerpt": "efficiently approximate the ideal joint hypothesis. Empirical results show that  \nour approach outperforms state of the art methods. \nContent introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8358091722008054
      ],
      "excerpt": "Adversarial learning methods are a promising approach to training robust deep  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8106427138123802,
        0.9077430327106936
      ],
      "excerpt": "(GANs) show compelling visualizations, they are not optimal on discriminative  \ntasks and can be limited to smaller shifts. On the other hand, discriminative  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8360421074687041
      ],
      "excerpt": "state-of-the-art approaches as special cases, and use this generalized view to  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9287750237147661
      ],
      "excerpt": "of our general framework which combines discriminative modeling, untied weight  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452479963761007,
        0.9755580053702673,
        0.9450929263472714
      ],
      "excerpt": "Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler  \nthan competing domainadversarial methods, and demonstrate the promise of our  \napproach by exceeding state-of-the-art unsupervised adaptation results on  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906128909950556
      ],
      "excerpt": "Content introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8524301530198436
      ],
      "excerpt": "which require tuples of corresponding images in different domains in the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8100401973264977
      ],
      "excerpt": "from the marginal distributions. This is achieved by enforcing a weight-sharing  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8663595982975121
      ],
      "excerpt": "solution over a product of marginal distributions one. We apply CoGAN to several  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906128909950556
      ],
      "excerpt": "Content introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8889526749926795
      ],
      "excerpt": "Adversarial learning has been embedded into deep networks to learn disentangled  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8306874281317977
      ],
      "excerpt": "(CDANs) are designed with two novel conditioning strategies: multilinear  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9328819706582491,
        0.9793702800341118,
        0.9436429356107905,
        0.958384155757567,
        0.8906128909950556
      ],
      "excerpt": "and classifier predictions to improve the discriminability, and entropy  \nconditioning that controls the uncertainty of classifier predictions to  \nguarantee the transferability. With theoretical guarantees and a few lines of  \ncodes, the approach has exceeded state-of-the-art results on five datasets. \nContent introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8317834471100165
      ],
      "excerpt": "sampled from a single domain. This neglects the more practical scenario where  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8435017985020606
      ],
      "excerpt": "which contains six domains and about 0.6 million images distributed among 345  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.878385081732886
      ],
      "excerpt": "research. Second, we propose a new deep learning approach, Moment Matching for  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8263231567690831,
        0.8167219883882184
      ],
      "excerpt": "dynamically aligning moments of their feature distributions. Third, we provide  \nnew theoretical insights specifically for moment matching approaches in both  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9596527280119123
      ],
      "excerpt": "conducted to demonstrate the power of our new dataset in benchmarking  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9709614658623594
      ],
      "excerpt": "advantage of our proposed model. Dataset and Code are available at  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906128909950556
      ],
      "excerpt": "Content introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9925361264144313,
        0.8906128909950556
      ],
      "excerpt": "The performance of Multi-Source Unsupervised Domain Adaptation depends significantly on the effectiveness of transfer from labeled source domain samples. In this paper, we proposed an adversarial agent that learns a dynamic curriculum for source samples, called Curriculum Manager for Source Selection (CMSS). The Curriculum Manager, an independent network module, constantly updates the curriculum during training, and iteratively learns which domains or samples are best suited for aligning to the target. The intuition behind this is to force the Curriculum Manager to constantly re-measure the transferability of latent domains over time to adversarially raise the error rate of the domain discriminator. CMSS does not require any knowledge of the domain labels, yet it outperforms other methods on four well-known benchmarks by significant margins. We also provide interpretable results that shed light on the proposed method. \nContent introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9990269651305321,
        0.8906128909950556
      ],
      "excerpt": "Transferring knowledges learned from multiple source domains to target domain is a more practical and challenging task than conventional single-source domain adaptation. Furthermore, the increase of modalities brings more difficulty in aligning feature distributions among multiple domains. To mitigate these problems, we propose a Learning to Combine for Multi-Source Domain Adaptation (LtC-MSDA) framework via exploring interactions among domains. In the nutshell, a knowledge graph is constructed on the prototypes of various domains to realize the information propagation among semantically adjacent representations. On such basis, a graph model is learned to predict query samples under the guidance of correlated prototypes. In addition, we design a Relation Alignment Loss (RAL) to facilitate the consistency of categories\u2019 relational interdependency and the compactness of features, which boosts features\u2019 intra-class invariance and inter-class separability. Comprehensive results on public benchmark datasets demonstrate that our approach outperforms existing methods with a remarkable margin. Our code is available athttps://github.com/ChrisAllenMing/LtC-MSDA. \nContent introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.986669203080574,
        0.8906128909950556
      ],
      "excerpt": "Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky,2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes violation of the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T)1 model, which takes the V ADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on the digit, traffic sign, and Wi-Fi recognition domain adaptation benchmarks. \nContent introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Record my work in transfer learning during my postgraduate period.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CtrlZ1/Domain-Adaptation-Algorithms/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 16:40:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/CtrlZ1/Domain-Adaptation-Algorithms/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "CtrlZ1/Domain-Adaptation-Algorithms",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n$ cd yourfolder\n$ git clone https://github.com/CtrlZ1/Domain-Adaptation-Algorithms.git\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/CtrlZ1/Domain-Adaptation-Algorithms/issues{/number}",
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 CtrlZ1\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Domain-Adaptation-Algorithms",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Domain-Adaptation-Algorithms",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "CtrlZ1",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CtrlZ1/Domain-Adaptation-Algorithms/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Thu, 23 Dec 2021 16:40:26 GMT"
    },
    "technique": "GitHub API"
  }
}