{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code is based on [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN).\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1802.08435v1"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mkotha/WaveRNN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-25T10:21:18Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-29T19:46:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9855499422255644
      ],
      "excerpt": "This is a Pytorch implementation of WaveRNN. Currently 3 top-level networks are \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9189978621613133
      ],
      "excerpt": "A VQ-VAE implementation with a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8785457187871489,
        0.9462955924161977,
        0.8999531193718416,
        0.8119473266644903
      ],
      "excerpt": "I deviated from the papers in some details, sometimes because I was lazy, and \nsometimes because I was unable to get good results without it. Below is a \n(probably incomplete) list of deviations. \nAll models: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.854633230898563,
        0.8429767091403526,
        0.9124350067198248
      ],
      "excerpt": "  of the embedding vectors. \nIn the early stage of training, I scale with a small number the penalty term \n  that apply to the input to the VQ layer. Without this, the input very often \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8486933521777352
      ],
      "excerpt": "During training, the target audio signal (which is also the input signal) is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9061413722968034,
        0.9439332356050371,
        0.9410568188481462
      ],
      "excerpt": "  Gaussian noise is also applied to each audio sample. Without these types of \n  noise, the feature captured by the model tended to be very sensitive to small \n  purterbations to the input, and the subjective quality of the model output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9943207707700006,
        0.825059156920169,
        0.9322063413841916,
        0.9292237775514578,
        0.9387334090312462
      ],
      "excerpt": "The decoder is based on WaveRNN instead of WaveNet. See the next section for \n  details about this network. \nThe VQ-VAE implementation uses a WaveRNN-based decoder instead of a WaveNet- \nbased decoder found in the paper. This is a WaveRNN network augmented \nwith a context stack to extend the receptive field.  This network is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A WaveRNN implementation",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mkotha/WaveRNN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 50,
      "date": "Wed, 29 Dec 2021 00:03:57 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mkotha/WaveRNN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mkotha/WaveRNN",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mkotha/WaveRNN/master/preload.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can skip this section if you don't need a single-speaker dataset.\n\n1. Download and uncompress [the LJ speech dataset](\n  https://keithito.com/LJ-Speech-Dataset/).\n2. `python preprocess16.py /path/to/dataset/LJSpeech-1.1/wavs\n  /path/to/output/directory`\n3. In `config.py`, set `single_speaker_data_path` to point to the output\n  directory.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You can skip this section if you don't need a multi-speaker dataset.\n\n1. Download and uncompress [the VCTK dataset](\n  https://datashare.is.ed.ac.uk/handle/10283/2651).\n2. `python preprocess_multispeaker.py /path/to/dataset/VCTK-Corpus/wav48\n  /path/to/output/directory`\n3. In `config.py`, set `multi_speaker_data_path` to point to the output\n  directory.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.925968641921108
      ],
      "excerpt": "It has been tested with the following datasets. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9055586456026614
      ],
      "excerpt": "cp config.py.example config.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8160243266191944
      ],
      "excerpt": "  kept descreasing after a certain point in training. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mkotha/WaveRNN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "WaveRNN + VQ-VAE",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "WaveRNN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mkotha",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mkotha/WaveRNN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python 3.6 or newer\n* PyTorch with CUDA enabled\n* [librosa](https://github.com/librosa/librosa)\n* [apex](https://github.com/NVIDIA/apex) if you want to use FP16 (it probably\n  doesn't work that well).\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 196,
      "date": "Wed, 29 Dec 2021 00:03:57 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "`wavernn.py` is the entry point:\n\n```\n$ python wavernn.py\n```\n\nBy default, it trains a VQ-VAE model. The `-m` option can be used to tell the\nthe script to train a different model.\n\nTrained models are saved under the `model_checkpoints` directory.\n\nBy default, the script will take the latest snapshot and continues training\nfrom there. To train a new model freshly, use the `--scratch` option.\n\nEvery 50k steps, the model is run to generate test audio outputs. The output\ngoes under the `model_outputs` directory.\n\nWhen the `-g` option is given, the script produces the output using the saved\nmodel, rather than training it.\n\n",
      "technique": "Header extraction"
    }
  ]
}