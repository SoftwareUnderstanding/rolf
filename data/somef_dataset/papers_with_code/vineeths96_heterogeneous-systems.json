{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2110.08941>View Paper</a>\n  </p>\n\n\n\n\n</p>\n\n> tags : distributed optimization, large-scale machine learning, heterogenous systems, edge learning, federated learning, deep learning, pytorch \n\n\n\n### Code for the paper [Distributed Optimization using Heterogeneous Compute Systems](https://arxiv.org/abs/2110.08941",
      "https://arxiv.org/abs/2110.08941",
      "https://arxiv.org/abs/1512.03385",
      "https://arxiv.org/abs/1409.1556",
      "https://arxiv.org/abs/2110.08941"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8283216015784888
      ],
      "excerpt": "| Without Dynamic Partitioning | Without Dynamic Partitioning | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vineeths96/Heterogeneous-Systems",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Vineeth S - vs96codes@gmail.com\n\nProject Link: [https://github.com/vineeths96/Heterogeneous-Systems](https://github.com/vineeths96/Heterogeneous-Systems)\n\n\n\n\n\n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n\n[contributors-shield]: https://img.shields.io/github/contributors/vineeths96/Heterogeneous-Systems.svg?style=flat-square\n[contributors-url]: https://github.com/vineeths96/Heterogeneous-Systems/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/vineeths96/Heterogeneous-Systems.svg?style=flat-square\n[forks-url]: https://github.com/vineeths96/Heterogeneous-Systems/network/members\n[stars-shield]: https://img.shields.io/github/stars/vineeths96/Heterogeneous-Systems.svg?style=flat-square\n[stars-url]: https://github.com/vineeths96/Heterogeneous-Systems/stargazers\n[issues-shield]: https://img.shields.io/github/issues/vineeths96/Heterogeneous-Systems.svg?style=flat-square\n[issues-url]: https://github.com/vineeths96/Heterogeneous-Systems/issues\n[license-shield]: https://img.shields.io/badge/License-MIT-yellow.svg\n[license-url]: https://github.com/vineeths96/Heterogeneous-Systems/blob/master/LICENSE\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=flat-square&logo=linkedin&colorB=555\n[linkedin-url]: https://linkedin.com/in/vineeths\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-08T04:38:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-14T11:32:15Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9740529229038164,
        0.9976968581622495,
        0.9696340467506461
      ],
      "excerpt": "<!-- ABOUT THE PROJECT --> \nHardware compute power has been growing at an unprecedented rate in recent years. The utilization of such advancements plays a key role in producing better results in less time -- both in academia and industry. However, merging the existing hardware with the latest hardware within the same ecosystem poses a challenging task. One of the key challenges, in this case, is varying compute power. In this paper, we consider the training of deep neural networks on a distributed system of workers with varying compute power. A naive implementation of synchronous distributed training will result in the faster workers waiting for the slowest worker to complete processing. To mitigate this issue, we propose to dynamically adjust the data assigned for each worker at every epoch during the training. We assign each worker a partition of total data proportional to its computing power. By adjusting the data partition to the workers, we directly control the workload on the workers. We assign the partitions, and hence the workloads, such that the time taken to process the data partition is almost uniform across the workers. We empirically evaluate the performance of the dynamic partitioning by training deep neural networks on the CIFAR10 dataset. We examine the performance of training ResNet50 (computation-heavy) model and VGG16 (computation-light) model with and without the dynamic partitioning algorithms. Our experiments show that dynamically adjusting the data partition helps to improve the utilization of the system and significantly reduces the time taken for training. \nThis project was built with  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9039984891541929
      ],
      "excerpt": "The environment used for developing this project is available at environment.yml. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.93408484363683
      ],
      "excerpt": "We conducted experiments on ResNet50 architecture and VGG16 architecture. Refer the original papers for more information about the models. We use publicly available implementations from GitHub for reproducing the models.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9677251570781675,
        0.9816424809337918,
        0.8924212690395703
      ],
      "excerpt": "We highly recommend to read through the paper before proceeding to this section. The paper explains the dynamic partitioning schemes we propose and contains many more analysis & results than what is presented here.  \nWe begin with an explanation of the notations used for the plot legends in this section. Sync-SGD corresponds to the default gradient aggregation provided by PyTorch. DP-SGD and EDP-SGD corresponds to Dynamic Partitioning and Enhanced Dynamic Partitioning respectively. We artificially simulate heterogeneity by adding time delays to a subset of workers. We evaluate the algorithms for a low level of heterogeneity and a high level of heterogeneity. \n|                           ResNet50                           |                            VGG16                             | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "We present an algorithm to dynamically adjust the data assigned for each worker at every epoch during the training in a heterogeneous cluster. We empirically evaluate the performance of the dynamic partitioning by training deep neural networks on the CIFAR10 dataset.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vineeths96/heterogeneous-systems/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 10:53:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vineeths96/Heterogeneous-Systems/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "vineeths96/Heterogeneous-Systems",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8837680365796365,
        0.8477062130654064
      ],
      "excerpt": "python v3.7.6 \nPyTorch v1.7.1 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vineeths96/Heterogeneous-Systems/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 vineeths96\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## Code for the paper [Distributed Optimization using Heterogeneous Compute Systems](https://arxiv.org/abs/2110.08941).",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Heterogeneous-Systems",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "vineeths96",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vineeths96/Heterogeneous-Systems/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Create a new conda environment and install all the libraries by running the following command\n\n```shell\nconda env create -f environment.yml\n```\n\nThe dataset used in this project (CIFAR 10) will be automatically downloaded and setup in `data` directory during execution.\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The training of the models can be performed on a distributed cluster with multiple machines and multiple worker GPUs. We make use of `torch.distributed.launch` to launch the distributed training. More information is available [here](https://pytorch.org/tutorials/beginner/dist_overview.html).\n\nTo launch distributed training on a single machine with multiple workers (GPUs), \n\n```shell\npython -m torch.distributed.launch --nproc_per_node=<num_gpus> trainer.py --local_world_size=<num_gpus> \n```\n\n To launch distributed training on multiple machine with multiple workers (GPUs), \n\n```sh\nexport NCCL_SOCKET_IFNAME=ens3\n\npython -m torch.distributed.launch --nproc_per_node=<num_gpus> --nnodes=<num_machines> --node_rank=<node_rank> --master_addr=<master_address> --master_port=<master_port> trainer.py --local_world_size=<num_gpus>\n```\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 10:53:04 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "distributed-optimization",
      "large-scale",
      "machine-learning",
      "deep-learning",
      "heterogeneous-parallel-programming",
      "federated-learning",
      "pytorch"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Clone the repository into a local machine using,\n\n```shell\ngit clone https://github.com/vineeths96/Heterogeneous-Systems\ncd Heterogeneous-Systems/\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}