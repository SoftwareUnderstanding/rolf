{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2101.11986",
      "https://arxiv.org/abs/2101.11986"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this repo useful, please consider citing:\n```\n@article{yuan2021tokens,\n  title={Tokens-to-token vit: Training vision transformers from scratch on imagenet},\n  author={Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},\n  journal={arXiv preprint arXiv:2101.11986},\n  year={2021}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{yuan2021tokens,\n  title={Tokens-to-token vit: Training vision transformers from scratch on imagenet},\n  author={Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},\n  journal={arXiv preprint arXiv:2101.11986},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8025038807982551
      ],
      "excerpt": "The results look like: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Shaunlipy/T2T_VIT",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-10T03:37:40Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-01T16:48:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8595433602387665
      ],
      "excerpt": "2021/03/02: update our new results. Now T2T-ViT-14 with 21.5M parameters can reach 81.5% top1-acc by training from scratch on ImageNet.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9902436460989538
      ],
      "excerpt": "2021/01/28: init codes and upload most of the pretrained models of T2T-ViT. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.96943561827828
      ],
      "excerpt": "Our codes are based on the official imagenet example by PyTorch and pytorch-image-models by Ross Wightman \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9920529930459461
      ],
      "excerpt": "The three lite variant of T2T-ViT (Comparing with MobileNets): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9888152922201394
      ],
      "excerpt": "Top-1 accuracy of the model is: 81.5% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8351829970101771
      ],
      "excerpt": "Visualize the image features of ResNet50, you can open and run the visualization_resnet.ipynb file in jupyter notebook or jupyter lab; some results are given as following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8351829970101771
      ],
      "excerpt": "Visualize the image features of ViT, you can open and run the visualization_vit.ipynb file in jupyter notebook or jupyter lab; some results are given as following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8122199204195946
      ],
      "excerpt": "Visualize attention map, you can refer to this file. A simple example by visualizing the attention map in attention block 4 and 5 is: \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Shaunlipy/T2T_VIT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 28 Dec 2021 07:11:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Shaunlipy/T2T_VIT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Shaunlipy/T2T_VIT",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Shaunlipy/T2T_VIT/main/visualization_vit.ipynb",
      "https://raw.githubusercontent.com/Shaunlipy/T2T_VIT/main/visualization_resnet.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Shaunlipy/T2T_VIT/main/distributed_train.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8886563876056374
      ],
      "excerpt": "| Model    | T2T Transformer | Top1 Acc | #params | MACs |  Download| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801608852907022
      ],
      "excerpt": "| T2T-ViT_t-24 | Transformer |   82.6   |  64.1M  | 15.0G| here |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8886563876056374
      ],
      "excerpt": "| Model    | T2T Transformer | Top1 Acc | #params | MACs |  Download| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.827797439604211
      ],
      "excerpt": "| T2T-ViT-10   |  Performer  |   75.2   |  5.9M   | 1.8G  | here|  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9210188757447015
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python main.py path/to/data --model T2t_vit_14 -b 100 --eval_checkpoint path/to/checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.849436359944601,
        0.9210188757447015
      ],
      "excerpt": "Download the T2T-ViT-7, T2T-ViT-10 or T2T-ViT-12, then test it by running: \nCUDA_VISIBLE_DEVICES=0 python main.py path/to/data --model T2t_vit_7 -b 100 --eval_checkpoint path/to/checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8214858730342897
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,1,2,3 ./distributed_train.sh 4 path/to/data --model T2t_vit_7 -b 128 --lr 1e-3 --weight-decay .03 --amp --img-size 224 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8380762698219618
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 ./distributed_train.sh 8 path/to/data --model T2t_vit_14 -b 64 --lr 5e-4 --weight-decay .05 --amp --img-size 224 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8416281603013129
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 ./distributed_train.sh 8 path/to/data --model T2t_vit_19 -b 64 --lr 5e-4 --weight-decay .065 --amp --img-size 224 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8221155366440788
      ],
      "excerpt": "<img src=\"https://github.com/yitu-opensource/T2T-ViT/blob/main/images/attention_visualization.png\" width=\"600\" height=\"400\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Shaunlipy/T2T_VIT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/Shaunlipy/T2T_VIT/main/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The Clear BSD License\\n\\nCopyright (c) [2012]-[2021] Shanghai Yitu Technology Co., Ltd.\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification, are permitted (subject to the limitations in the disclaimer below) provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n* Neither the name of Shanghai Yitu Technology Co., Ltd. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\n\\nNO EXPRESS OR IMPLIED LICENSES TO ANY PARTY\\'S PATENT RIGHTS ARE GRANTED BY THIS LICENSE. THIS SOFTWARE IS PROVIDED BY SHANGHAI YITU TECHNOLOGY CO., LTD. AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL SHANGHAI YITU TECHNOLOGY CO., LTD. OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet, [arxiv](https://arxiv.org/abs/2101.11986)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "T2T_VIT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Shaunlipy",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Shaunlipy/T2T_VIT/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[timm](https://github.com/rwightman/pytorch-image-models), pip install timm\n\ntorch>=1.4.0\n\ntorchvision>=0.5.0\n\npyyaml\n\ndata prepare: ImageNet with the following folder structure:\n\n```\nimagenet/\n....train/\n........calss1/\n............/ img1\n........class2/\n............/ img2\n....val/\n........calss1/\n............/ img1\n........class2/\n............/ img2\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 28 Dec 2021 07:11:07 GMT"
    },
    "technique": "GitHub API"
  }
}