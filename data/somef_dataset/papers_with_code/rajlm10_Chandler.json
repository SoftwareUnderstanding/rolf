{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1908.07414"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1](https://arxiv.org/pdf/1706.03762.pdf) Vaswani, Ashish & Shazeer, Noam & Parmar, Niki & Uszkoreit, Jakob & Jones, Llion & Gomez, Aidan & Kaiser, Lukasz & Polosukhin, Illia, \u201cAttention is all you need\u201d , 2017.\n\n[2](https://arxiv.org/abs/1810.04805) Jacob Devlin,Ming-Wei Chang, Kenton Lee, Kristina Toutanova, \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" ,**v2 2019** \n\n[3](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270) Rani Horev, \u201cBERT Explained: State of the art language model for NLP\u201d blog post, 2018.\n\n[4](http://jalammar.github.io/illustrated-transformer/) Jay Alammar, \u201cThe Ilustrated Transformer\u201d blog post, 2018.\n\n[5](http://peterbloem.nl/blog/transformers) Peter Bloem, \u201cTransformers from scratch\u201d blog post, 2019.\n\n[6](https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634) Eduardo Munoz, \u201cAttention is all you need: Discovering the Transformer paper\u201d blog post, 2020. **Big Thank you!**\n\n[7](https://stackoverflow.com/questions/58123393/how-to-use-transformers-for-text-classification) Jindrich's stackoverflow answer , 2019.\n\n\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@book{book,\nauthor = {Misra, Rishabh and Grover, Jigyasa},\nyear = {2021},\nmonth = {01},\npages = {},\ntitle = {Sculpting Data for ML: The first act of Machine Learning},\nisbn = {978-0-578-83125-1}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{misra2019sarcasm,\n  title={Sarcasm Detection using Hybrid Neural Network},\n  author={Misra, Rishabh and Arora, Prahal},\n  journal={arXiv preprint arXiv:1908.07414},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "<a id=\"jumpto\"></a> Multi-Headed-Attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8820807460429729
      ],
      "excerpt": "| I would kill for a Nobel Peace Prize.                             | Sarcastic         | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rajlm10/Chandler",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-10T14:44:44Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-28T13:48:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.998559115808361,
        0.9378029485353175
      ],
      "excerpt": "Sarcasm is tough to detect sometimes even at a human level making it even tougher for machines to detect. I wanted to incorporate state of the art techniques to address this problem. The code in this repo is solely in tensorflow and the models have been created from scratch to understand the workings of an Encoder thouroughly. The implementation is based on the paper titled. Attention is all you need (Vaswani et al). Here is the link to the original paper. Some insights were also derived from the paper titled BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google AI Language). This paper can be found here. \nI will try and expain each section of the code here and also provide excerpts from the paper/some medium blogs which helped me understand the code better. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9935160863661997
      ],
      "excerpt": "For those of you who solely want to achieve high performance I would recommend using a pre-trained BERT model from Hugging Face and fine-tuning it on the dataset I have used. However this repo aims at understanding the Encoder model better from scratch. Also note that the final model in production is very light (29 MB Weights + 7MB Tokenizer) whereas BERT or even DISTILBERT for that matter consumes much more space but at the same time is much more robust since it has more Encoder Layers, Feed Forward Network Units and is trained on huge data using different objectives and has a larger vocabulary size. The BERT paper linked above will tell you more about this. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8787782034875317
      ],
      "excerpt": "The dataset contains news headline from two news website. TheOnion which aims at producing sarcastic versions of current events and   non-sarcastic news headlines from HuffPost. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8398843037498357
      ],
      "excerpt": "Here are some stats from the official repo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8946113851689268
      ],
      "excerpt": "Self-attention is a sequence-to-sequence operation: a sequence of vectors goes in, and a sequence of vectors comes out. Let\u2019s call the input vectors x1, x2,\u2026, xt and the corresponding output vectors y1, y2,\u2026, yt. The vectors all have dimension k. To produce output vector yi, the self attention operation simply takes a weighted average over all the input vectors, the simplest option is the dot product.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9444332647578842,
        0.8505802046936021,
        0.8729575132246831,
        0.8947153015013308,
        0.9541567508720183,
        0.9886645096936172
      ],
      "excerpt": "Every input vector is used in three different ways in the self-attention mechanism: the Query, the Key and the Value. \nIn every role, it is compared to the other vectors to get its own output yi(Query), to get the j-th output yj(Key) and to compute each output vector once the weights have been established (Value). \nTo obtain this roles, we need three weight matrices of dimensions d_model x d_model and compute three linear transformation for each xi: \nThese three matrices are usually known as K, Q and V, three learnable weight layers that are applied to the same encoded input. Consequently, as each of these three matrices come from the same input, we can apply the attention mechanism of the input vector with itself, a \u201cself-attention\u201d. \nHere is the equation from the paper. This is called scaled dot-product attention* \nWe basically parallelize the scaled dot product attention discussed in the previous section. Instead of having one attention operation do this n times , n being the number of attention heads. Each head gets a part (equally) of the query, key and value. In the previous description the attention scores are focused on the whole sentence at a time, this would produce the same results even if two sentences contain the same words in a different order. Instead, we would like to attend to different segments of the words. We can give the self attention greater power of discrimination, by combining several self attention heads, dividing the words vectors into a fixed number (h, number of heads) of chunks, and then self-attention is applied on the corresponding chunks, using Q, K and V sub-matrices.[http://peterbloem.nl/blog/transformers]. Later attention scores from these various heads are again concatenated and multiplied by a set of trainable weights. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9950206672427361
      ],
      "excerpt": "The problem with Encoders is that they are fed a sequence all at once rather than sequentially. This is why they are also called \"Bi-Directional\". Now a  problem arises that if we only use word embeddings we are not taking into account the position of each word. In summary any two sentences with the same words in different orders will be viewed the same by our encoder. To make the encoder aware about the position of these words we need an additional embedding which is concatenated with the word embeddings. One approach would be to use a position embedding, similar to word embedding, coding every known position with a vector. \u201cIt would requiere sentences of all accepted positions during the training loop but positional encoding allow the model to extrapolate to sequence lengths longer than the ones encountered during training\u201d. Moreover while training we would need sentences of all lengths (upto maxlen) to make sure the positional embedding layer learns. However the paper addresses this by using a sinusodial function. (The code contains more insights to the equation) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.996946912937092
      ],
      "excerpt": "The best resource to learn about the Encoder is surely Jay Alammar's The illustrated transformer. I would strongly suggest new learners to go through the entire blog. It also illustrates the Decoder which we won't be using for our project. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030810469677935
      ],
      "excerpt": "Let us go through each of the model variations listed in the above section visually. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.958161681345163,
        0.9761677888505915
      ],
      "excerpt": "All the three models were trained on a GPU on Google Colab. All of them were trained against the Binary Cross Entropy loss function. According to the transformer paper Adam was used as an optimizer with a variable learning rate varying according to the model dimensions as per this equation below. \nAll of them were trained for 5 epochs on the GPU with a batch size of 128. By the end these were the training accuracies and losses for each model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9200766324275832
      ],
      "excerpt": "Here are the detailed validation details about the three models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "| Model             | ROC_AUC   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9734224104428513,
        0.9502669232446295,
        0.9765767105203136,
        0.8422951933133369
      ],
      "excerpt": "Although on first glance all three models have comparable metrics on further testing I found that the pure Encoder generalises the best. Again it's always best to choose a model based on an objective you want to maximize in the confusion matrix. In my case I was ready to compromise a bit on predicting sarcastic sentences if that meant I could decrease false positives. The Encoder+LSTM model predicted sarcastic sentences very well but had high false positives. I created many sentences both sarcastic and otherwise and found that the pure Encoder generalises better than the other two.  \nI took alot of sarcastic sentences from this website. \nHere is the ROC curve of the final model in deployment. \nHere are some results from the web app.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9649876030074646,
        0.9839965428190722,
        0.8968912153687325
      ],
      "excerpt": "Today is such a beautiful day , the weather is perfect to sit inside . \nToday is such a beautiful day , the weather is perfect for football . \nHere are some more results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.902779063278878
      ],
      "excerpt": "| I work forty hours a week for me to be this poor.                 | Sarcastic         | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9158069876183995
      ],
      "excerpt": "| Marriage. Because your crappy day doesn\u2019t have to end at work.    | Sarcastic         | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9262892357286094
      ],
      "excerpt": "| It's nothing to joke about.                                       | Not Sarcastic     | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A sarcasm detector based on state of the art NLP techniques. The implementation is based on the Encoder model and incorporates a few variations. The website uses a backend made using FLASK and deployed using Heroku .The readme contains comprehensive details about the project!",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rajlm10/Chandler/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The encoder in the paper was used for a machine translation task. It is important to know this because we will be using the encoder for a binary classification task. The last layer from the encoder outputs a tensor of shape **BATCH_SIZE X MAX_SEQ_LEN X EMBEDDING_DIM** . In essence this can be considered a hidden state for each token since we have MAX_SEQ_LEN of these tokens. However to use it as a sequence classifier we need to reduce this tensor to a shape **BATCH_SIZE X EMBEDDING_DIM** before passing it through a Dense layer (containing n target classes) . This [stackoverflow](https://stackoverflow.com/questions/58123393/how-to-use-transformers-for-text-classification) answer helps offer some choices. Here are some choices to play around with \n\n1. Average all the hidden states along the axis **MAX_SEQ_LEN**\n2.  According to the **BERT** paper prepend a **[CLS]** token to each sentence and use the hidden state of this during classification. Note that using a single token like this requires extensive training as the token must learn as much from context as possible.\n3.  Use a Conv2D layer and then a Flatten Layer to make sure the dimensions are **BATCH_SIZE X EMBEDDING_DIM**\n4.  Use an LSTM layer before the final Dense layer\n\nI will be using techniques listed in 1,3 and 4 to create models and will compare them. The diagrams for each are listed in the next section.\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 17:54:47 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rajlm10/Chandler/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rajlm10/Chandler",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rajlm10/Chandler/main/Sarcasm_Final.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8991827813922305
      ],
      "excerpt": "| It\u2019s okay if you don\u2019t like me. Not everyone has good taste.      | Sarcastic         | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rajlm10/Chandler/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "HTML",
      "CSS"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Chandler",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Chandler",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rajlm10",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rajlm10/Chandler/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 17:54:47 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "machine-learning",
      "nlp",
      "sarcasm-detection",
      "transformer",
      "encoder",
      "sarcasm-prediction",
      "news-headline-classify"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I would like to reiterate that the model is far from perfect. For best performance I would suggest you use examples which do not have any conversational dependence or history. They should summarize emotions in a single sentence (2 at maximum ). Please try to give as much context as possible since phrases such as **Sure!** , **What a day!** could be interpreted as both sarcastic and otherwise depending on the context. \n\nI will try to improve the model by training it on more examples in the future. The goal of this repo is only to implement and understand Encoders. For **practical usage using a pre-trained model such as BERT obviously yields better performance**\n\n",
      "technique": "Header extraction"
    }
  ]
}