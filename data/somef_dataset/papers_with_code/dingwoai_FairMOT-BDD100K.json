{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "A large part of the code is borrowed from [Zhongdao/Towards-Realtime-MOT](https://github.com/Zhongdao/Towards-Realtime-MOT) and [xingyizhou/CenterNet](https://github.com/xingyizhou/CenterNet). Thanks for their wonderful works.\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.01888"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@article{zhang2020simple,\n  title={A Simple Baseline for Multi-Object Tracking},\n  author={Zhang, Yifu and Wang, Chunyu and Wang, Xinggang and Zeng, Wenjun and Liu, Wenyu},\n  journal={arXiv preprint arXiv:2004.01888},\n  year={2020}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhang2020simple,\n  title={A Simple Baseline for Multi-Object Tracking},\n  author={Zhang, Yifu and Wang, Chunyu and Wang, Xinggang and Zeng, Wenjun and Liu, Wenyu},\n  journal={arXiv preprint arXiv:2004.01888},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9604675413581302,
        0.9746139683026899,
        0.9997089747468236,
        0.9904682582335301
      ],
      "excerpt": "A simple baseline for one-shot multi-object tracking: \nA Simple Baseline for Multi-Object Tracking,           \nYifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, Wenyu Liu,       \narXiv technical report (arXiv 2004.01888) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dingwoai/FairMOT-BDD100K",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-22T05:10:57Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T10:25:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9186971835967312
      ],
      "excerpt": "This is a fork of FairMOT used to do MOT(multi-object tracking) on BDD100K Dataset and can also be modified to other customized datasets. You can refer to origin fork \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.839826758897499
      ],
      "excerpt": "Change '--data_cfg' in the opts, change '--data_dir' in the opts to '/save_path/data/det' (same as in Data Preparation) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8894029941185991
      ],
      "excerpt": "Change '--data_cfg' in the opts, change '--data_dir' in the opts to '/save_path/data/MOT' (same as in Data Preparation) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9752483642891783
      ],
      "excerpt": "There has been remarkable progress on object detection and re-identification in recent years which are the core components for multi-object tracking. However, little attention has been focused on accomplishing the two tasks in a single network to improve the inference speed. The initial attempts along this path ended up with degraded results mainly because the re-identification branch is not appropriately learned. In this work, we study the essential reasons behind the failure, and accordingly present a simple baseline to addresses the problems. It remarkably outperforms the state-of-the-arts on the MOT challenge datasets at 30 FPS. We hope this baseline could inspire and help evaluate new ideas in this field. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144965960308795
      ],
      "excerpt": "HRNetV2 ImageNet pretrained model: HRNetV2-W18 official, HRNetV2-W32 official. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9308437247302681
      ],
      "excerpt": "to see the tracking results (76.1 MOTA using the DLA-34 baseline model). You can also set save_images=True in src/track.py to save the visualization results of each frame.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9578257783900481
      ],
      "excerpt": "to see the tracking results (76.6 MOTA using the HRNetV2-W18 baseline model). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8058105383201016
      ],
      "excerpt": "To get the SOTA results of 2DMOT15 and MOT20, you need to finetune the baseline model on the specific dataset because our training set do not contain them. You can run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8758515090420119
      ],
      "excerpt": "Results of the test set all need to be evaluated on the MOT challenge server. You can see the tracking results on the training set by setting --val_motxx True and run the tracking code. We set 'conf_thres' 0.4 for MOT16 and MOT17. We set 'conf_thres' 0.3 for 2DMOT15 and MOT20. You can also use the SOTA MOT20 pretrained model here [Google], [Baidu],code:mqnz: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A fork of FairMOT used to do MOT on BDD100K Dataset",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dingwoai/FairMOT-BDD100K/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Tue, 28 Dec 2021 17:34:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dingwoai/FairMOT-BDD100K/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dingwoai/FairMOT-BDD100K",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/dingwoai/FairMOT-BDD100K/master/build/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/dingwoai/FairMOT-BDD100K/master/experiments/all_hrnet.sh",
      "https://raw.githubusercontent.com/dingwoai/FairMOT-BDD100K/master/experiments/bdd100k.sh",
      "https://raw.githubusercontent.com/dingwoai/FairMOT-BDD100K/master/experiments/ft_mot15_dla34.sh",
      "https://raw.githubusercontent.com/dingwoai/FairMOT-BDD100K/master/experiments/all_dla34.sh",
      "https://raw.githubusercontent.com/dingwoai/FairMOT-BDD100K/master/experiments/all_res50.sh",
      "https://raw.githubusercontent.com/dingwoai/FairMOT-BDD100K/master/experiments/ft_mot20_dla34.sh",
      "https://raw.githubusercontent.com/dingwoai/FairMOT-BDD100K/master/src/lib/models/networks/DCNv2/make.sh",
      "https://raw.githubusercontent.com/dingwoai/FairMOT-BDD100K/master/src/lib/models/networks/DCNv2_new/make.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We use the same training data as [JDE](https://github.com/Zhongdao/Towards-Realtime-MOT). Please refer to their [DATA ZOO](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md) to download and prepare all the training data including Caltech Pedestrian, CityPersons, CUHK-SYSU, PRW, ETHZ, MOT17 and MOT16. \n\n[2DMOT15](https://motchallenge.net/data/2D_MOT_2015/) and [MOT20](https://motchallenge.net/data/MOT20/) can be downloaded from the official webpage of MOT challenge. After downloading, you should prepare the data in the following structure:\n```\nMOT15\n   |\u2014\u2014\u2014\u2014\u2014\u2014images\n   |        \u2514\u2014\u2014\u2014\u2014\u2014\u2014train\n   |        \u2514\u2014\u2014\u2014\u2014\u2014\u2014test\n   \u2514\u2014\u2014\u2014\u2014\u2014\u2014labels_with_ids\n            \u2514\u2014\u2014\u2014\u2014\u2014\u2014train(empty)\nMOT20\n   |\u2014\u2014\u2014\u2014\u2014\u2014images\n   |        \u2514\u2014\u2014\u2014\u2014\u2014\u2014train\n   |        \u2514\u2014\u2014\u2014\u2014\u2014\u2014test\n   \u2514\u2014\u2014\u2014\u2014\u2014\u2014labels_with_ids\n            \u2514\u2014\u2014\u2014\u2014\u2014\u2014train(empty)\n```\nThen, you can change the seq_root and label_root in src/gen_labels_15.py and src/gen_labels_20.py and run:\n```\ncd src\npython gen_labels_15.py\npython gen_labels_20.py\n```\nto generate the labels of 2DMOT15 and MOT20. The seqinfo.ini files of 2DMOT15 can be downloaded here [[Google]](https://drive.google.com/open?id=1kJYySZy7wyETH4fKMzgJrYUrTfxKlN1w), [[Baidu],code:8o0w](https://pan.baidu.com/s/1zb5tBW7-YTzWOXpd9IzS0g).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "* Clone this repo, and we'll call the directory that you cloned as ${FAIRMOT_ROOT}\n* Install dependencies. We use python 3.7 and pytorch >= 1.2.0\n```\nconda create -n FairMOT\nconda activate FairMOT\nconda install pytorch==1.2.0 torchvision==0.4.0 cudatoolkit=10.0 -c pytorch\ncd ${FAIRMOT_ROOT}\npip install -r requirements.txt\ncd src/lib/models/networks/DCNv2_new sh make.sh\n```\n* We use [DCNv2](https://github.com/CharlesShang/DCNv2) in our backbone network and more details can be found in their repo. \n* In order to run the code for demos, you also need to install [ffmpeg](https://www.ffmpeg.org/).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src=\"assets/MOT15.gif\" width=\"400\"/>   <img src=\"assets/MOT16.gif\" width=\"400\"/>\n<img src=\"assets/MOT17.gif\" width=\"400\"/>   <img src=\"assets/MOT20.gif\" width=\"400\"/>\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "| Dataset    |  MOTA | IDF1 | IDS | MT | ML | FPS |\n|--------------|-----------|--------|-------|----------|----------|--------|\n|2DMOT15  | 59.0 | 62.2 |  582 | 45.6% | 11.5% | 30.5 |\n|MOT16       | 68.7 | 70.4 | 953 | 39.5% | 19.0% | 25.9 |\n|MOT17       | 67.5 | 69.8 | 2868 | 37.7% | 20.8% | 25.9 |\n|MOT20       | 58.7 | 63.7 | 6013 | 66.3% | 8.5% | 13.2 |\n\n All of the results are obtained on the [MOT challenge](https://motchallenge.net) evaluation server under the \u201cprivate detector\u201d protocol. We rank first among all the trackers on 2DMOT15, MOT17 and the recently released (2020.02.29) MOT20. Note that our IDF1 score remarkably outperforms other one-shot MOT trackers by more than **10 points**. The tracking speed of the entire system can reach up to **30 FPS**.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- First you need to transform the BDD100K dataset to the formation of MOT format, like folder 'images' and 'labels with ids'.\n\nFor training MOT tracker:\n```\npython bdd2mot/bdd2mot.py --img_dir '/bdd_root/bdd100k/images/track' --label_dir '/bdd_root/bdd100k/labels-20/box-track' --save_path '/save_path/data/MOT'\n```\n\nFor training detector:\n```\npython bdd2mot/bdd2det.py --img_dir '/bdd_root/bdd100k/images/100k' --label_dir '/bdd_root/bdd100k/labels' --save_path '/save_path/data/det'\n```\nPlease modify the path accordingly.\n\n- Then, you need to generate the paths of the training images like the files in src/data and add your json file for training in src/lib/cfg. \n```\ncd bdd2mot\npython gen_path.py --img_train_dir '/save_path/data/MOT/images/train' --img_val_dir '/save_path/data/MOT/images/train' --save_dir '../src/data/' --save_name 'bdd100k'  \n```\nfor tracking. And for detection:\n```\ncd bdd2mot\npython gen_path.py --img_train_dir '/save_path/data/det/images/train' --img_val_dir '/save_path/data/det/images/train' --save_dir '../src/data/' --save_name 'bdd100k' \n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9309435651573079,
        0.9452797457628369
      ],
      "excerpt": "You can then train detector on bdd dataset: \ncd src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164640256136145
      ],
      "excerpt": "sh experiments/bdd100k.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164640256136145,
        0.8086588881330743,
        0.9452797457628369
      ],
      "excerpt": "sh experiments/all_dla34.sh \nThe default settings run tracking on the validation dataset from 2DMOT15. Using the DLA-34 baseline model, you can run: \ncd src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452797457628369
      ],
      "excerpt": "cd src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452797457628369
      ],
      "excerpt": "cd src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164640256136145,
        0.9164640256136145
      ],
      "excerpt": "sh experiments/ft_mot15_dla34.sh \nsh experiments/ft_mot20_dla34.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452797457628369
      ],
      "excerpt": "cd src \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8629400568140789,
        0.9308699766224975
      ],
      "excerpt": "cd src \nCUDA_VISIBLE_DEVICES=0,1 python -W ignore train_det.py det --exp_id bdd100k_det --gpus '0,1' --print_iter 100 --batch_size 16 --load_model '../models/ctdet_coco_dla_2x.pth' --data_cfg './lib/cfg/bdd100k_det.json' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9220986107601614
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python -W ignore track_bdd.py mot --val_bdd100k True --load_model /data/BDD100K/bdd_track/FairMOT/exp/mot/bdd_hrnet/model_30.pth --conf_thres 0.4 --arch 'hrnet_32' --reid_dim 128 --exp_id bdd_hrnet --K 256 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8245539886860519
      ],
      "excerpt": "Pretrained models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8952823871208778,
        0.8630739888074163
      ],
      "excerpt": "Download the training data \nChange the dataset root directory 'root' in src/lib/cfg/data.json and 'data_dir' in src/lib/opts.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8097269094842118,
        0.8629400568140789,
        0.9300244143002546
      ],
      "excerpt": "The default settings run tracking on the validation dataset from 2DMOT15. Using the DLA-34 baseline model, you can run: \ncd src \npython track.py mot --load_model ../models/all_dla34.pth --conf_thres 0.6 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629400568140789,
        0.9359196317249231
      ],
      "excerpt": "cd src \npython track.py mot --load_model ../models/all_hrnet_v2_w18.pth --conf_thres 0.6 --arch hrnet_18 --reid_dim 128 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629400568140789,
        0.9544536113645911,
        0.9544536113645911
      ],
      "excerpt": "cd src \npython track.py mot --test_mot17 True --load_model ../models/all_dla34.pth --conf_thres 0.4 \npython track.py mot --test_mot16 True --load_model ../models/all_dla34.pth --conf_thres 0.4 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629400568140789,
        0.9516623759856101,
        0.9516623759856101
      ],
      "excerpt": "cd src \npython track.py mot --test_mot15 True --load_model your_mot15_model.pth --conf_thres 0.3 \npython track.py mot --test_mot20 True --load_model your_mot20_model.pth --conf_thres 0.3 --K 500 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9572001618458429
      ],
      "excerpt": "python track.py mot --test_mot20 True --load_model ../models/mot20_dla34.pth --reid_dim 128 --conf_thres 0.3 --K 500 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dingwoai/FairMOT-BDD100K/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "C",
      "Dockerfile",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2019, Charles Shang\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\n3. Neither the name of the copyright holder nor the names of its\\n   contributors may be used to endorse or promote products derived from\\n   this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "FairMOT-BDD",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "FairMOT-BDD100K",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dingwoai",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dingwoai/FairMOT-BDD100K/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 18,
      "date": "Tue, 28 Dec 2021 17:34:03 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src=\"assets/MOT15.gif\" width=\"400\"/>   <img src=\"assets/MOT16.gif\" width=\"400\"/>\n<img src=\"assets/MOT17.gif\" width=\"400\"/>   <img src=\"assets/MOT20.gif\" width=\"400\"/>\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You can input a raw video and get the demo video by running src/demo.py and get the mp4 format of the demo video:\n```\ncd src\npython demo.py mot --load_model ../models/all_dla34.pth --conf_thres 0.4\n```\nYou can change --input-video and --output-root to get the demos of your own videos.\n\nIf you have difficulty building DCNv2 and thus cannot use the DLA-34 baseline model, you can run the demo with the HRNetV2_w18 baseline model (don't forget to comment lines with 'dcn' in src/libs/models/model.py if you do not build DCNv2): \n```\ncd src\npython demo.py mot --load_model ../models/all_hrnet_v2_w18.pth --arch hrnet_18 --reid_dim 128 --conf_thres 0.4\n```\n--conf_thres can be set from 0.3 to 0.7 depending on your own videos.\n\n",
      "technique": "Header extraction"
    }
  ]
}