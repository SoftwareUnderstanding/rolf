{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1907.11692\">Roberta: A robustly optimized BERT pretraining approach</a> --- Roberta--- by<i> Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov\n</a>  (<a href=\"https://github.com/pytorch/fairseq\">Github</a>"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8520815120463098
      ],
      "excerpt": "Future Research Challenges \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999999835690973
      ],
      "excerpt": "<a href=\"https://arxiv.org/pdf/2008.00364.pdf\">A Survey on Text Classification: From Shallow to Deep Learning,2020</a> by<i> Qian Li, Hao Peng, Jianxin Li, Congying Xia, Renyu Yang, Lichao Sun, Philip S. Yu, Lifang He \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9920403885227626,
        0.9878817550225492
      ],
      "excerpt": "<a href=\"https://transacl.org/ojs/index.php/tacl/article/view/1853\">Spanbert: Improving pre-training by representing and predicting spans</a>  --- SpanBERT--- by<i> Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, Omer Levy \n</a>(<a href=\"https://github.com/facebookresearch/SpanBERT\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.996550442158424,
        0.9831840704037338
      ],
      "excerpt": "<a href=\"https://openreview.net/forum?id=H1eA7AEtvS\">ALBERT: A lite BERT for self-supervised learning of language representations</a> --- ALBERT--- by<i> Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut \n</a> (<a href=\"https://github.com/google-research/ALBERT\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8539451384573938
      ],
      "excerpt": "\u5728\u5bf9\u81ea\u7136\u8bed\u8a00\u8868\u793a\u8fdb\u884c\u9884\u8bad\u7ec3\u65f6\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u901a\u5e38\u4f1a\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5728\u67d0\u79cd\u7a0b\u5ea6\u4e0a\u7531\u4e8eGPU/TPU\u7684\u5185\u5b58\u9650\u5236\u548c\u8bad\u7ec3\u65f6\u95f4\u7684\u589e\u957f\uff0c\u8fdb\u4e00\u6b65\u7684\u63d0\u5347\u6a21\u578b\u89c4\u6a21\u53d8\u5f97\u66f4\u52a0\u56f0\u96be\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u53c2\u6570\u7f29\u51cf\u6280\u672f\u6765\u964d\u4f4e\u5185\u5b58\u6d88\u8017\uff0c\u5e76\u63d0\u9ad8BERT\u7684\u8bad\u7ec3\u901f\u5ea6\u3002\u5168\u9762\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u80fd\u591f\u8ba9\u6a21\u578b\u5728\u89c4\u6a21\u53ef\u4f38\u7f29\u6027\u65b9\u9762\u8fdc\u4f18\u4e8eBERT\u3002\u672c\u6587\u8fd8\u4f7f\u7528\u4e86\u4e00\u79cd\u5bf9\u53e5\u5b50\u95f4\u8fde\u8d2f\u6027\u8fdb\u884c\u5efa\u6a21\u7684\u81ea\u76d1\u7763\u635f\u5931\u51fd\u6570\uff0c\u5e76\u8bc1\u660e\u8fd9\u79cd\u65b9\u6cd5\u5bf9\u591a\u53e5\u5b50\u8f93\u5165\u7684\u4e0b\u6e38\u4efb\u52a1\u786e\u5b9e\u6709\u5e2e\u52a9\u3002\u672c\u6587\u7684\u6700\u4f73\u6a21\u578b\u5728GLUE, RACE\u548cSQuAD\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f73\u6548\u679c\uff0c\u5e76\u4e14\u53c2\u6570\u91cf\u4f4e\u4e8eBERT-large\u3002\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u7ecf\u5f00\u6e90\u5230github\uff1ahttps://github.com/google-research/ALBERT. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997583412485235,
        0.9106346948061492
      ],
      "excerpt": "<a href=\"https://arxiv.org/abs/1907.11692\">Roberta: A robustly optimized BERT pretraining approach</a> --- Roberta--- by<i> Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov \n</a>  (<a href=\"https://github.com/pytorch/fairseq\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9912189466374434,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding\">Xlnet: Generalized autoregressive pretraining for language understanding</a> --- Xlnet--- by<i> Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R. Salakhutdinov, Quoc V. Le \n</a>  (<a href=\"https://github.com/zihangdai/xlnet\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998799806924774,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://www.aclweb.org/anthology/P19-1441/\">Multi-task deep neural networks for natural language understanding</a> --- MT-DNN--- by<i> Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao \n</a>  (<a href=\"https://github.com/namisan/mt-dnn\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999595581587511,
        0.9831840704037338
      ],
      "excerpt": "<a href=\"https://doi.org/10.18653/v1/n19-1423\">BERT: pre-training of deep bidirectional transformers for language understanding</a> --- BERT--- by<i> Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova \n</a> (<a href=\"https://github.com/google-research/bert\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998944008055906
      ],
      "excerpt": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u8a00\u8868\u793a\u6a21\u578bBERT\uff0c\u5b83\u8868\u793aTransformers\u7684\u53cc\u5411\u7f16\u7801\u5668\u8868\u793a\u3002\u4e0e\u6700\u8fd1\u7684\u8bed\u8a00\u8868\u793a\u6a21\u578b\u4e0d\u540c(Peters et al., 2018; Radford et al., 2018)\uff0cBERT\u901a\u8fc7\u5728\u6240\u6709\u5c42\u7684\u4e0a\u4e0b\u6587\u8054\u5408\u8c03\u8282\u6765\u9884\u8bad\u7ec3\u6df1\u5c42\u53cc\u5411\u8868\u793a\u3002\u56e0\u6b64\uff0c\u53ea\u9700\u4e00\u4e2a\u989d\u5916\u7684\u8f93\u51fa\u5c42\u5c31\u53ef\u4ee5\u5bf9\u9884\u5148\u8bad\u7ec3\u597d\u7684BERT\u8868\u793a\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u4fbf\u4e3a\u5404\u79cd\u4efb\u52a1\u521b\u5efa\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u4f8b\u5982\u95ee\u7b54\u548c\u8bed\u8a00\u63a8\u65ad\uff0c\u800c\u65e0\u9700\u57fa\u672c\u7684\u4efb\u52a1\u7279\u5b9a\u67b6\u6784\u4fee\u6539\u3002BERT\u6982\u5ff5\u7b80\u5355\uff0c\u7ecf\u9a8c\u4e30\u5bcc\u3002\u5b83\u572811\u9879\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u83b7\u5f97\u4e86\u6700\u65b0\u7684\u6280\u672f\u6210\u679c\uff0c\u5305\u62ec\u5c06GLUE\u7684\u57fa\u51c6\u503c\u63d0\u9ad8\u523080.4%(7.6%\u7684\u7edd\u5bf9\u6539\u8fdb)\u3001\u591a\u9879\u51c6\u786e\u7387\u63d0\u9ad8\u523086.7%(5.6%\u7684\u7edd\u5bf9\u6539\u8fdb)\u3001\u5c06SQuAD v1.1\u7684\u95ee\u7b54\u6d4b\u8bd5F1\u57fa\u51c6\u503c\u63d0\u9ad8\u523093.2(1.5\u7684\u7edd\u5bf9\u6539\u8fdb)\uff0c\u4ee5\u53ca\u5c06SQuAD v2.0\u6d4b\u8bd5\u7684F1\u57fa\u51c6\u503c\u63d0\u9ad8\u523083.1\uff085.1\u7684\u7edd\u5bf9\u6539\u8fdb\uff09\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9915953143439705,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4725\">Graph convolutional networks for text classification</a> --- TextGCN---  by<i> Liang Yao, Chengsheng Mao, Yuan Luo \n</a>(<a href=\"https://github.com/yao8839836/text_gcn\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9987747154885248,
        0.9106346948061492
      ],
      "excerpt": "<a href=\"https://doi.org/10.18653/v1/d18-1380\">Multi-grained attention network for aspect-level sentiment classification</a> --- MGAN --- by<i> Feifan Fan, Yansong Feng, Dongyan Zhao \n</a>(<a href=\"https://github.com/songyouwei/ABSA-PyTorch\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999781603949043,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://doi.org/10.18653/v1/d18-1350\">Investigating capsule networks with dynamic routing for text classification</a> --- TextCapsule --- by<i> Min Yang, Wei Zhao, Jianbo Ye, Zeyang Lei, Zhou Zhao, Soufei Zhang \n</a>(<a href=\"https://github.com/andyweizhao/capsule_text_classification\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999997678875004,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://doi.org/10.24963/ijcai.2018/584\">Constructing narrative event evolutionary graph for script event prediction</a> --- SGNN ---  by<i> Zhongyang Li, Xiao Ding, Ting Liu \n</a>(<a href=\"https://github.com/eecrazy/ConstructingNEEG_IJCAI_2018\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999998936776308,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://www.aclweb.org/anthology/C18-1330/\">SGM: sequence generation model for multi-label classification</a> --- SGM ---  by<i> Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma, Wei Wu, Houfeng Wang \n</a>(<a href=\"https://github.com/lancopku/SGM\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999996807761503,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://www.aclweb.org/anthology/P18-1216/\">Joint embedding of words and labelsfor text classification</a> --- LEAM ---  by<i> Guoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe Zhang, Dinghan Shen, Xinyuan Zhang, Ricardo Henao, Lawrence Carin \n</a>(<a href=\"https://github.com/guoyinwang/LEAM\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8029948720864918
      ],
      "excerpt": "</a>(<a href=\"http://nlp.fast.ai/category/classification.html\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999994182870453,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://dl.acm.org/doi/10.1145/3178876.3186005\">Large-scale hierarchical text classification withrecursively regularized deep graph-cnn</a> --- DGCNN --- by<i> Hao Peng, Jianxin Li, Yu He, Yaopeng Liu, Mengjiao Bao, Lihong Wang, Yangqiu Song, Qiang Yang \n</a>(<a href=\"https://github.com/HKUST-KnowComp/DeepGraphCNNforTexts\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9996069617326243,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://doi.org/10.18653/v1/n18-1202\">Deep contextualized word rep-resentations</a> --- ELMo --- by<i> Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer \n</a>(<a href=\"https://github.com/flairNLP/flair\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999013618933363,
        0.9106346948061492
      ],
      "excerpt": "<a href=\"https://www.aclweb.org/anthology/D17-1047/\">Recurrent Attention Network on Memory for Aspect Sentiment Analysis</a> --- RAM --- by<i> Peng Chen, Zhongqian Sun, Lidong Bing, Wei Yang \n</a>(<a href=\"https://github.com/songyouwei/ABSA-PyTorch\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9827268296650306,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://doi.org/10.18653/v1/d17-1169\">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</a> --- DeepMoji --- by<i> Bjarke Felbo, Alan Mislove, Anders S\u00f8gaard, Iyad Rahwan, Sune Lehmann \n</a>(<a href=\"https://github.com/bfelbo/DeepMoji\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999243250531353,
        0.9106346948061492
      ],
      "excerpt": "<a href=\"https://www.ijcai.org/Proceedings/2017/568\">Interactive attention networks for aspect-level sentiment classification</a> --- IAN --- by<i> Dehong Ma, Sujian Li, Xiaodong Zhang, Houfeng Wang \n</a>(<a href=\"https://github.com/songyouwei/ABSA-PyTorch\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999972581445452,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://doi.org/10.18653/v1/P17-1052\">Deep pyramid convolutional neural networks for text categorization</a> --- DPCNN --- by<i> Rie Johnson, Tong Zhang \n</a>(<a href=\"https://github.com/Cheneng/DPCNN\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8887368974344244,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://openreview.net/forum?id=rJbbOLcex\">Topicrnn: A recurrent neural network with long-range semantic dependency</a> --- TopicRNN ---  by<i> Adji B. Dieng, Chong Wang, Jianfeng Gao, John Paisley \n</a>(<a href=\"https://github.com/dangitstam/topic-rnn\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9966129799112964
      ],
      "excerpt": "<a href=\"https://openreview.net/forum?id=r1X3g2_xl\">Adversarial training methods for semi-supervised text classification</a> --- Miyato et al. ---  by<i> Takeru Miyato, Andrew M. Dai, Ian Goodfellow \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9996998451955476,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://doi.org/10.18653/v1/e17-2068\">Bag of tricks for efficient text classification</a> --- FastText ---  by<i> Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov \n</a>(<a href=\"https://github.com/SeanLee97/short-text-classification\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999682336170169,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://doi.org/10.18653/v1/d16-1053\">Long short-term memory-networks for machine reading</a> --- LSTMN ---  by<i> Jianpeng Cheng, Li Dong, Mirella Lapata \n</a>(<a href=\"https://github.com/JRC1995/Abstractive-Summarization\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9917542978790073,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://www.ijcai.org/Abstract/16/408\">Recurrent neural network for text classification with multi-task learning</a> --- Multi-Task --- by<i> Pengfei Liu, Xipeng Qiu, Xuanjing Huang \n</a>(<a href=\"https://github.com/baixl/text_classification\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999779663023346,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://doi.org/10.18653/v1/n16-1174\">Hierarchical attention networks for document classification</a> --- HAN --- by<i> Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy \n</a>(<a href=\"https://github.com/richliao/textClassifier\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9698834407712852
      ],
      "excerpt": "</a>(<a href=\"https://github.com/mhjabreel/CharCNN\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9989911663470156,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://doi.org/10.3115/v1/p15-1150\">Improved semantic representations from tree-structured long short-term memory networks</a> --- Tree-LSTM ---  by<i> Kai Sheng Tai, Richard Socher, Christopher D. Manning \n</a>(<a href=\"https://github.com/stanfordnlp/treelstm\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "\u7531\u4e8e\u5177\u6709\u8f83\u5f3a\u7684\u5e8f\u5217\u957f\u671f\u4f9d\u8d56\u4fdd\u5b58\u80fd\u529b\uff0c\u5177\u6709\u66f4\u590d\u6742\u7684\u8ba1\u7b97\u5355\u5143\u7684\u957f\u77ed\u65f6\u8bb0\u5fc6\u7f51\u7edc\uff08LSTM\uff09\u5728\u5404\u79cd\u5e8f\u5217\u5efa\u6a21\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u63a2\u7d22\u8fc7\u7684\u552f\u4e00\u5e95\u5c42LSTM\u7ed3\u6784\u662f\u7ebf\u6027\u94fe\u3002\u7531\u4e8e\u81ea\u7136\u8bed\u8a00\u5177\u6709\u53e5\u6cd5\u5c5e\u6027\uff0c \u56e0\u6b64\u53ef\u4ee5\u81ea\u7136\u5730\u5c06\u5355\u8bcd\u4e0e\u77ed\u8bed\u7ed3\u5408\u8d77\u6765\u3002 \u672c\u6587\u63d0\u51fa\u4e86Tree-LSTM\uff0c\u5b83\u662fLSTM\u5728\u6811\u5f62\u62d3\u6251\u7f51\u7edc\u7ed3\u6784\u4e0a\u7684\u6269\u5c55\u3002 Tree-LSTM\u5728\u4e0b\u9762\u4e24\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u6a21\u578b\u4ee5\u53ca\u5f3a\u5927\u7684LSTM\u57fa\u51c6\u65b9\u6cd5\uff1a\u9884\u6d4b\u4e24\u4e2a\u53e5\u5b50\u7684\u8bed\u4e49\u76f8\u5173\u6027\uff08SemEval 2014\uff0c\u4efb\u52a11\uff09\u548c\u60c5\u611f\u5206\u7c7b\uff08Stanford\u60c5\u611f\u6811\u5e93\uff09\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9955116254782576,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://doi.org/10.3115/v1/p15-1162\">Deep unordered composition rivals syntactic methods for text classification</a> --- DAN ---  by<i> Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, Hal Daum\u00e9 III \n</a>(<a href=\"https://github.com/miyyer/dan\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.992256901906255,
        0.9106346948061492
      ],
      "excerpt": "<a href=\"http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745\">Recurrent convolutional neural networks for text classification</a> --- TextRCNN ---  by<i> Siwei Lai, Liheng Xu, Kang Liu, Jun Zhao \n</a>(<a href=\"https://github.com/roomylee/rcnn-text-classification\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9071299061032697,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"http://proceedings.mlr.press/v32/le14.html\">Distributed representations of sentences and documents</a> --- Paragraph-Vec ---  by<i> Quoc Le, Tomas Mikolov \n</a>(<a href=\"https://github.com/inejc/paragraph-vectors\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9965227327852304,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://doi.org/10.3115/v1/p14-1062\">A convolutional neural network for modelling sentences</a> --- DCNN ---  by<i> Nal Kalchbrenner, Edward Grefenstette, Phil Blunsom \n</a>(<a href=\"https://github.com/kinimod23/ATS_Project\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9213314193787535
      ],
      "excerpt": "</a>(<a href=\"https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9698834407712852
      ],
      "excerpt": "</a>(<a href=\" https://github.com/pondruska/DeepSentiment\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9902818506350933
      ],
      "excerpt": "</a>(<a href=\"https://github.com/github-pengge/MV_RNN\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9777173565264698,
        0.9213314193787535
      ],
      "excerpt": "<a href=\"https://www.aclweb.org/anthology/D11-1014/\">Semi-supervised recursive autoencoders forpredicting sentiment distributions</a> --- RAE --- by<i> Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, Christopher D. Manning \n</a>(<a href=\"https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999954520000751,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"http://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree\">Lightgbm: A highly efficient gradient boosting decision tree</a> --- LightGBM --- by<i> Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu \n</a>(<a href=\"https://github.com/creatist/text_classify\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9995563977112556
      ],
      "excerpt": "<a href=\"https://dl.acm.org/doi/10.1145/2939672.2939785\">Xgboost: A scalable tree boosting system</a> --- XGBoost ---  by<i> Tianqi Chen, Carlos Guestrin \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9967367517072127,
        0.9430484695340363
      ],
      "excerpt": "<a href=\"https://link.springer.com/article/10.1023%2FA%3A1010933404324\"> --- Random Forests (RF) --- by<i> Leo Breiman  \n</a></a> (<a href=\"https://github.com/hexiaolang/RandomForest-In-text-classification\">{Github}</a>) </summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9698834407712852
      ],
      "excerpt": "</a> (<a href=\"https://github.com/Gunjitbedi/Text-Classification\">{Github}</a>) </summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999276776863486,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://link.springer.com/article/10.1007/BF00993309\">C4.5: Programs for Machine Learning (C4.5)</a> by<i> Steven L. Salzberg  \n</a>  (<a href=\"https://github.com/Cater5009/Text-Classify\">{Github}</a>) </summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9850261894179057,
        0.8888676439986835
      ],
      "excerpt": "<a href=\"https://dblp.org/img/paper.dark.empty.16x16.png\">Classification and Regression Trees (CART)</a> by<i> Chyon-HwaYeh \n</a> (<a href=\"https://github.com/sayantann11/all-classification-templetes-for-ML\">{Github}</a>) </summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999916947741552,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://dl.acm.org/doi/10.1145/321075.321084\">Nearest neighbor pattern classification (k-nearest neighbor classification,KNN)</a> by<i> M. E. Maron \n</a> (<a href=\"https://github.com/raimonbosch/knn.classifier\">{Github}</a>) </summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9898463483355563,
        0.9698834407712852
      ],
      "excerpt": "<a href=\"https://dl.acm.org/doi/10.1145/321075.321084\">Automatic indexing: An experimental inquiry</a> by<i> M. E. Maron \n</a> (<a href=\"https://github.com/Gunjitbedi/Text-Classification\">{Github}</a>) </summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/kdd/DiaoQWSJW14\">IMDB reviews IMDB\u8bc4\u8bba</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/emnlp/TangQL15\">Yelp reviews Yelp\u8bc4\u8bba</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/conf/cncl/SunQXH19.bib\">Sogou News (Sogou) \u641c\u72d7\u65b0\u95fb</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/journals/semweb/LehmannIJJKMHMK15.bib\">DBpedia</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702105236373249
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">Yahoo answers (YahooA) \u96c5\u864e\u95ee\u7b54</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9321597499656605
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">Stanford Question Answering Dataset (SQuAD\uff09 \u65af\u5766\u798f\u95ee\u7b54\u6570\u636e\u96c6</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9537267708315463
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">MS MARCO</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702105236373249
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">TREC-QA</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702105236373249
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">WikiQA</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9162165611683208
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">The Stanford Natural Language Inference (SNLI)</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9924850221410872
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">Multi-Genre Natural Language Inference (MNLI)</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "  Multi-NLI\u662fSNLI\u7684\u6269\u5c55\uff0c\u6db5\u76d6\u4e86\u66f4\u5927\u8303\u56f4\u7684\u4e66\u9762\u548c\u53e3\u5934\u6587\u5b57\u7c7b\u578b\u3002 \u5b83\u5305\u62ec433,000\u4e2a\u53e5\u5b50\u5bf9\uff0c\u5e76\u5e26\u6709\u6587\u672c\u662f\u5426\u8574\u542b\u7684\u6807\u7b7e\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9880157813353811
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">Sentences Involving Compositional Knowledge (SICK)</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9943942403619029
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">Microsoft Research Paraphrase (MSRP)</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9850821515827285
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">Dialog State Tracking Challenge 4 (DSTC 4)</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702105236373249
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">ICSI Meeting Recorder Dialog Act (MRDA)</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702105236373249
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">Switchboard Dialog Act (SwDA)</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702105236373249
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">Reuters news</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9321597499656605
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">Patent Dataset</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9927475169765758
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">Reuters Corpus Volume I (RCV1) and RCV1-2K</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9654939460080199
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">Web of Science (WOS-11967)</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.99793919739415
      ],
      "excerpt": "<a href=\"https://dblp.org/rec/bib/conf/nips/ZhangZL15\">Arxiv Academic Paper Dataset (AAPD)</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "<summary><a href=\"https://github.com/Tencent/NeuralNLP-NeuralClassifier\">NeuralClassifier</a></summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "<summary><a href=\"https://github.com/nocater/baidu_nlp_project2\">baidu_nlp_project2</a></summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9809621424453162
      ],
      "excerpt": "<summary><a href=\"https://github.com/TianWuYuJiangHenShou/textClassifier\">Multi-label</a></summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xiaoqian19940510/text-classification-surveys",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-07T05:18:09Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T06:40:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9934996662080545,
        0.873105325388281
      ],
      "excerpt": "This repository contains resources for Natural Language Processing (NLP) with a focus on the task of Text Classification. The content is mainly from paper \u300aA Survey on Text Classification: From Shallow to Deep Learning\u300b \n\uff08\u8be5repository\u4e3b\u8981\u603b\u7ed3\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u7684\u8d44\u6599\u3002\u5185\u5bb9\u4e3b\u8981\u6765\u81ea\u6587\u672c\u5206\u7c7b\u7efc\u8ff0\u8bba\u6587\u300aA Survey on Text Classification: From Shallow to Deep Learning\u300b\uff09 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.864734543539211
      ],
      "excerpt": "Deep Learning Models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9658732805324127
      ],
      "excerpt": "Tools and Repos \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9975243494087841
      ],
      "excerpt": "Text classification is the most fundamental and essential task in natural language processing. The last decade has seen a surge of research in this area due to the unprecedented success of deep learning. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This paper fills the gap by reviewing the state of the art approaches from 1961 to 2020, focusing on models from shallow to deep learning. We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions. A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey. Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9992278133433284
      ],
      "excerpt": "We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERT-Large, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8992882707167645
      ],
      "excerpt": "<a href=\"https://openreview.net/forum?id=H1eA7AEtvS\">ALBERT: A lite BERT for self-supervised learning of language representations</a> --- ALBERT--- by<i> Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9973874886410807
      ],
      "excerpt": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9989860151160167
      ],
      "excerpt": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8501420816787081
      ],
      "excerpt": "<a href=\"http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding\">Xlnet: Generalized autoregressive pretraining for language understanding</a> --- Xlnet--- by<i> Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R. Salakhutdinov, Quoc V. Le \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9990152194192438
      ],
      "excerpt": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9963513408715556
      ],
      "excerpt": "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9908173616900161
      ],
      "excerpt": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9991572594097263
      ],
      "excerpt": "Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only a limited number of studies have explored the more flexible graph convolutional neural networks (convolution on non-grid, e.g., arbitrary graph) for the task. In this work, we propose to use graph convolutional networks for text classification. We build a single text graph for a corpus based on word co-occurrence and document word relations, then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings. In addition, experimental results show that the improvement of Text GCN over state-of-the-art comparison methods become more prominent as we lower the percentage of training data, suggesting the robustness of Text GCN to less training data in text classification. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9556787082485652
      ],
      "excerpt": "We propose a novel multi-grained attention network (MGAN) model for aspect level sentiment classification. Existing approaches mostly adopt coarse-grained attention mechanism, which may bring information loss if the aspect has multiple words or larger context. We propose a fine-grained attention mechanism, which can capture the word-level interaction between aspect and context. And then we leverage the fine-grained and coarse-grained attention mechanisms to compose the MGAN framework. Moreover, unlike previous works which train each aspect with its context separately, we design an aspect alignment loss to depict the aspect-level interactions among the aspects that have the same context. We evaluate the proposed approach on three datasets: laptop and restaurant are from SemEval 2014, and the last one is a twitter dataset. Experimental results show that the multi-grained attention network consistently outperforms the state-of-the-art methods on all three datasets. We also conduct experiments to evaluate the effectiveness of aspect alignment loss, which indicates the aspect-level interactions can bring extra useful information and further improve the performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9949069909359011
      ],
      "excerpt": "In this study, we explore capsule networks with dynamic routing for text classification. We propose three strategies to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain \u201cbackground\u201d information or have not been successfully trained. A series of experiments are conducted with capsule networks on six text classification benchmarks. Capsule networks achieve state of the art on 4 out of 6 datasets, which shows the effectiveness of capsule networks for text classification. We additionally show that capsule networks exhibit significant improvement when transfer single-label to multi-label text classification over strong baseline methods. To the best of our knowledge, this is the first work that capsule networks have been empirically investigated for text modeling. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9972405726020862
      ],
      "excerpt": "Script event prediction requires a model to predict the subsequent event given an existing event context. Previous models based on event pairs or event chains cannot make full use of dense event connections, which may limit their capability of event prediction. To remedy this, we propose constructing an event graph to better utilize the event network information for script event prediction. In particular, we first extract narrative event chains from large quantities of news corpus, and then construct a narrative event evolutionary graph (NEEG) based on the extracted chains. NEEG can be seen as a knowledge base that describes event evolutionary principles and patterns. To solve the inference problem on NEEG, we present a scaled graph neural network (SGNN) to model event interactions and learn better event representations. Instead of computing the representations on the whole graph, SGNN processes only the concerned nodes each time, which makes our model feasible to large-scale graphs. By comparing the similarity between input context event representations and candidate event representations, we can choose the most reasonable subsequent event. Experimental results on widely used New York Times corpus demonstrate that our model significantly outperforms state-of-the-art baseline methods, by using standard multiple choice narrative cloze evaluation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9918425583608428
      ],
      "excerpt": "Multi-label classification is an important yet challenging task in natural language processing. It is more complex than single-label classification in that the labels tend to be correlated. Existing methods tend to ignore the correlations between labels. Besides, different parts of the text can contribute differently for predicting different labels, which is not considered by existing models. In this paper, we propose to view the multi-label classification task as a sequence generation problem, and apply a sequence generation model with a novel decoder structure to solve it. Extensive experimental results show that our proposed methods outperform previous work by a substantial margin. Further analysis of experimental results demonstrates that the proposed methods not only capture the correlations between labels, but also select the most informative words automatically when predicting different labels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9993791632615534
      ],
      "excerpt": "Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences. We propose to view text classification as a label-word joint embedding problem: each label is embedded in the same space with the word vectors. We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels. The attention is learned on a training set of labeled samples to ensure that, given a text sequence, the relevant words are weighted higher than the irrelevant ones. Our method maintains the interpretability of word embeddings, and enjoys a built-in ability to leverage alternative sources of information, in addition to input text sequences. Extensive results on the several large text datasets show that the proposed framework outperforms the state-of-the-art methods by a large margin, in terms of both accuracy and speed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9967481339221704,
        0.9076758408987082
      ],
      "excerpt": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code. \n\u5f52\u7eb3\u8fc1\u79fb\u5b66\u4e60\u5728CV\u9886\u57df\u5927\u653e\u5f02\u5f69\uff0c\u4f46\u5e76\u672a\u5e7f\u6cdb\u5e94\u7528\u4e8eNLP\u9886\u57df\uff0cNLP\u9886\u57df\u7684\u73b0\u6709\u65b9\u6cd5\u4ecd\u7136\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u6a21\u578b\u4fee\u6539\u5e76\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u3002 \u56e0\u6b64\u672c\u6587\u63d0\u51fa\u4e86\u901a\u7528\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\uff08Universal Language Model Fine-tuning \uff0cULMFiT\uff09\uff0c\u4e00\u79cd\u53ef\u4ee5\u5e94\u7528\u4e8eNLP\u4e2d\u4efb\u4f55\u4efb\u52a1\u7684\u9ad8\u6548\u7387\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u4ecb\u7ecd\u4e86\u5bf9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u5173\u952e\u6280\u672f\u3002 \u672c\u6587\u7684\u65b9\u6cd5\u5728\u516d\u4e2a\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u660e\u663e\u4f18\u4e8eSOTA\u6280\u672f\uff0c\u5728\u5927\u591a\u6570\u6570\u636e\u96c6\u4e0a\u7684\u9519\u8bef\u7387\u964d\u4f4e\u4e8618-24\uff05\u3002 \u6b64\u5916\uff0c\u5728\u53ea\u6709100\u4e2a\u5e26\u6807\u7b7e\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u80fd\u4e8e\u5728100\u500d\u6570\u636e\u4e0a\u4ece\u5934\u8bad\u7ec3\u7684\u6027\u80fd\u76f8\u5339\u914d\u3002\u76f8\u5173\u7684\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9994429968057986,
        0.9120073990341322
      ],
      "excerpt": "Text classification to a hierarchical taxonomy of topics is a common and practical problem. Traditional approaches simply use bag-of-words and have achieved good results. However, when there are a lot of labels with different topical granularities, bag-of-words representation may not be enough. Deep learning models have been proven to be effective to automatically learn different levels of representations for image data. It is interesting to study what is the best way to represent texts. In this paper, we propose a graph-CNN based deep learning model to first convert texts to graph-of-words, and then use graph convolution operations to convolve the word graph. Graph-of-words representation of texts has the advantage of capturing non-consecutive and long-distance semantics. CNN models have the advantage of learning different level of semantics. To further leverage the hierarchy of labels, we regularize the deep architecture with the dependency among labels. Our results on both RCV1 and NYTimes datasets show that we can significantly improve large-scale hierarchical text classification over traditional hierarchical text classification and existing deep models. \n\u5c06\u6587\u672c\u5206\u7c7b\u6309\u4e3b\u9898\u8fdb\u884c\u5c42\u6b21\u5206\u7c7b\u662f\u4e00\u4e2a\u5e38\u89c1\u4e14\u5b9e\u9645\u7684\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u4ec5\u4f7f\u7528\u5355\u8bcd\u888b\uff08bag-of-words\uff09\u5e76\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002\u4f46\u662f\uff0c\u5f53\u6709\u8bb8\u591a\u5177\u6709\u4e0d\u540c\u7684\u4e3b\u9898\u7c92\u5ea6\u6807\u7b7e\u65f6\uff0c\u8bcd\u888b\u7684\u8868\u5f81\u80fd\u529b\u53ef\u80fd\u4e0d\u8db3\u3002\u9274\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u6709\u6548\u5730\u81ea\u52a8\u5b66\u4e60\u56fe\u50cf\u6570\u636e\u7684\u4e0d\u540c\u8868\u793a\u5f62\u5f0f\uff0c\u56e0\u6b64\u503c\u5f97\u7814\u7a76\u54ea\u79cd\u65b9\u6cd5\u662f\u6587\u672c\u8868\u5f81\u5b66\u4e60\u7684\u6700\u4f73\u65b9\u6cd5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8egraph-CNN\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u9996\u5148\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5355\u8bcd\u56fe\uff0c\u7136\u540e\u4f7f\u7528\u56fe\u5377\u79ef\u8fd0\u7b97\u5bf9\u8bcd\u56fe\u8fdb\u884c\u5377\u79ef\u3002\u5c06\u6587\u672c\u8868\u793a\u4e3a\u56fe\u5177\u6709\u6355\u83b7\u975e\u8fde\u7eed\u548c\u957f\u8ddd\u79bb\u8bed\u4e49\u4fe1\u606f\u7684\u4f18\u52bf\u3002 CNN\u6a21\u578b\u7684\u4f18\u52bf\u5728\u4e8e\u53ef\u4ee5\u5b66\u4e60\u4e0d\u540c\u7ea7\u522b\u7684\u8bed\u4e49\u4fe1\u606f\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u5229\u7528\u6807\u7b7e\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u672c\u6587\u4f7f\u7528\u6807\u7b7e\u4e4b\u95f4\u7684\u4f9d\u8d56\u6027\u6765\u5bf9\u6df1\u5ea6\u7f51\u7edc\u7ed3\u6784\u8fdb\u884c\u6b63\u5219\u5316\u3002\u5728RCV1\u548cNYTimes\u6570\u636e\u96c6\u4e0a\u7684\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u5206\u5c42\u6587\u672c\u5206\u7c7b\u548c\u73b0\u6709\u7684\u6df1\u5ea6\u6a21\u578b\u76f8\u6bd4\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u7684\u5206\u5c42\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9992250716609077
      ],
      "excerpt": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999893125763988
      ],
      "excerpt": "We propose a novel framework based on neural networks to identify the sentiment of opinion targets in a comment/review. Our framework adopts multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information. The results of multiple attentions are non-linearly combined with a recurrent neural network, which strengthens the expressive power of our model for handling more complications. The weighted-memory mechanism not only helps us avoid the labor-intensive feature engineering work, but also provides a tailor-made memory for different opinion targets of a sentence. We examine the merit of our model on four datasets: two are from SemEval2014, i.e. reviews of restaurants and laptops; a twitter dataset, for testing its performance on social media data; and a Chinese news comment dataset, for testing its language sensitivity. The experimental results show that our model consistently outperforms the state-of-the-art methods on different types of data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8064778852184278
      ],
      "excerpt": "<a href=\"https://doi.org/10.18653/v1/d17-1169\">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</a> --- DeepMoji --- by<i> Bjarke Felbo, Alan Mislove, Anders S\u00f8gaard, Iyad Rahwan, Sune Lehmann \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9988509049940099
      ],
      "excerpt": "NLP tasks are often limited by scarcity of manually annotated data. In social media sentiment analysis and related tasks, researchers have therefore used binarized emoticons and specific hashtags as forms of distant supervision. Our paper shows that by extending the distant supervision to a more diverse set of noisy labels, the models can learn richer representations. Through emoji prediction on a dataset of 1246 million tweets containing one of 64 common emojis we obtain state-of-the-art performance on 8 benchmark datasets within emotion, sentiment and sarcasm detection using a single pretrained model. Our analyses confirm that the diversity of our emotional labels yield a performance improvement over previous distant supervision approaches. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9964725079155025
      ],
      "excerpt": "Aspect-level sentiment classification aims at identifying the sentiment polarity of specific target in its context. Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling thier contexts via generating target-specific representations. However, these studies always ignore the separate modeling of targets. In this paper, we argue that both targets and contexts deserve special treatment and need to be learned their own representations via interactive learning. Then, we propose the interactive attention networks (IAN) to interactively learn attentions in the contexts and targets, and generate the representations for targets and contexts separately. With this design, the IAN model can well represent a target and its collocative context, which is helpful to sentiment classification. Experimental results on SemEval 2014 Datasets demonstrate the effectiveness of our model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9984763168454323
      ],
      "excerpt": "This paper proposes a low-complexity word-level deep convolutional neural network (CNN) architecture for text categorization that can efficiently represent long-range associations in text. In the literature, several deep and complex neural networks have been proposed for this task, assuming availability of relatively large amounts of training data. However, the associated computational complexity increases as the networks go deeper, which poses serious challenges in practical applications. Moreover, it was shown recently that shallow word-level CNNs are more accurate and much faster than the state-of-the-art very deep nets such as character-level CNNs even in the setting of large training data. Motivated by these findings, we carefully studied deepening of word-level CNNs to capture global representations of text, and found a simple network architecture with which the best accuracy can be obtained by increasing the network depth without increasing computational cost by much. We call it deep pyramid CNN. The proposed model with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic categorization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9981858617262761
      ],
      "excerpt": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9903018186411666
      ],
      "excerpt": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9917628210389019
      ],
      "excerpt": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9979969247232835
      ],
      "excerpt": "In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9950958519108468
      ],
      "excerpt": "Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, in most previous works, the models are learned based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we use the multi-task learning framework to jointly learn across multiple related tasks. Based on recurrent neural network, we propose three different mechanisms of sharing information to model text with task-specific and shared layers. The entire network is trained jointly on all these tasks. Experiments on four benchmark text classification tasks show that our proposed models can improve the performance of a task with the help of other related tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9844833183355991
      ],
      "excerpt": "We propose a hierarchical attention networkfor document classification.  Our model hastwo distinctive characteristics: (i) it has a hier-archical structure that mirrors the hierarchicalstructure of documents; (ii) it has two levelsof attention mechanisms applied at the word-and sentence-level, enabling it to attend dif-ferentially to more and less important con-tent when constructing the document repre-sentation. Experiments conducted on six largescale text classification tasks demonstrate thatthe proposed architecture outperform previousmethods by a substantial margin. Visualiza-tion of the attention layers illustrates that themodel selects qualitatively informative wordsand sentences. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.992257446063324,
        0.9120073990341322
      ],
      "excerpt": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks. \n\u672c\u6587\u63d0\u51fa\u4e86\u901a\u8fc7\u5b57\u7b26\u7ea7\u5377\u79ef\u7f51\u7edc\uff08ConvNets\uff09\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u7684\u5b9e\u8bc1\u7814\u7a76\u3002 \u672c\u6587\u6784\u5efa\u4e86\u51e0\u4e2a\u5927\u578b\u6570\u636e\u96c6\uff0c\u4ee5\u8bc1\u660e\u5b57\u7b26\u7ea7\u5377\u79ef\u7f51\u7edc\u53ef\u4ee5\u8fbe\u5230SOTA\u7ed3\u679c\u6216\u8005\u5f97\u5230\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002 \u53ef\u4ee5\u4e0e\u4f20\u7edf\u6a21\u578b\uff08\u4f8b\u5982bag of words\uff0cn-grams \u53ca\u5176 TFIDF\u53d8\u4f53\uff09\u4ee5\u53ca\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u4f8b\u5982\u57fa\u4e8e\u5355\u8bcd\u7684ConvNets\u548cRNN\uff09\u8fdb\u884c\u6bd4\u8f83\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9929743299891497
      ],
      "excerpt": "Because  of  their  superior  ability  to  pre-serve   sequence   information   over   time,Long  Short-Term  Memory  (LSTM)  net-works,   a  type  of  recurrent  neural  net-work with a more complex computationalunit, have obtained strong results on a va-riety  of  sequence  modeling  tasks.Theonly underlying LSTM structure that hasbeen  explored  so  far  is  a  linear  chain.However,  natural  language  exhibits  syn-tactic properties that would naturally com-bine words to phrases.  We introduce theTree-LSTM, a generalization of LSTMs totree-structured network topologies.  Tree-LSTMs  outperform  all  existing  systemsand strong LSTM baselines on two tasks:predicting the semantic relatedness of twosentences  (SemEval  2014,  Task  1)  andsentiment  classification  (Stanford  Senti-ment Treebank). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9992711817295112
      ],
      "excerpt": "Many  existing  deep  learning  models  fornatural language processing tasks focus onlearning thecompositionalityof their in-puts, which requires many expensive com-putations. We present a simple deep neuralnetwork that competes with and, in somecases,  outperforms  such  models  on  sen-timent  analysis  and  factoid  question  an-swering tasks while taking only a fractionof the training time.  While our model issyntactically-ignorant, we show significantimprovements over previous bag-of-wordsmodels by deepening our network and ap-plying a novel variant of dropout.  More-over, our model performs better than syn-tactic models on datasets with high syn-tactic variance.  We show that our modelmakes similar errors to syntactically-awaremodels, indicating that for the tasks we con-sider, nonlinearly transforming the input ismore important than tailoring a network toincorporate word order and syntax. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9981062724626467
      ],
      "excerpt": "Text classification is a foundational task in many NLP applications. Traditional text classifiers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classification without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classification to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044232732231226
      ],
      "excerpt": "<a href=\"http://proceedings.mlr.press/v32/le14.html\">Distributed representations of sentences and documents</a> --- Paragraph-Vec ---  by<i> Quoc Le, Tomas Mikolov \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9988770543705721
      ],
      "excerpt": "Many machine learning algorithms require the input to be represented as a fixed length feature vector. When it comes to texts, one of the most common representations is bag-of-words. Despite their popularity, bag-of-words models have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose an unsupervised algorithm that learns vector representations of sentences and text documents. This algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that our technique outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9949460730427471
      ],
      "excerpt": "The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.995205151695221,
        0.9627487715650112
      ],
      "excerpt": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification. \n\u672c\u6587\u7814\u7a76\u4e86\u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u4e0a\u8fdb\u884c\u7684\u4e00\u7cfb\u5217\u5b9e\u9a8c\uff0c\u8fd9\u4e9b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u9488\u5bf9\u53e5\u5b50\u7ea7\u522b\u5206\u7c7b\u4efb\u52a1\u7684\u9884\u8bad\u7ec3\u5355\u8bcd\u5411\u91cf\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002 \u5b9e\u9a8c\u8bc1\u660e\uff0c\u51e0\u4e4e\u6ca1\u6709\u8d85\u53c2\u6570\u8c03\u6574\u548c\u9759\u6001\u77e2\u91cf\u7684\u7b80\u5355CNN\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5747\u80fd\u5b9e\u73b0\u51fa\u8272\u7684\u7ed3\u679c\u3002 \u901a\u8fc7\u5fae\u8c03\u6765\u5b66\u4e60\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u5355\u8bcd\u5411\u91cf\u53ef\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002 \u6b64\u5916\uff0c\u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u5bf9\u4f53\u7cfb\u7ed3\u6784\u8fdb\u884c\u7b80\u5355\u7684\u4fee\u6539\uff0c\u4ee5\u8ba9\u6a21\u578b\u80fd\u540c\u65f6\u4f7f\u7528\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u5355\u8bcd\u5411\u91cf\u548c\u9759\u6001\u5411\u91cf\u3002 \u672c\u6587\u8ba8\u8bba\u7684CNN\u6a21\u578b\u57287\u4e2a\u4efb\u52a1\u4e2d\u76844\u4e2a\u4e0a\u8d85\u8fc7\u4e86\u73b0\u6709\u7684SOTA\u6548\u679c\uff0c\u5176\u4e2d\u5305\u62ec\u60c5\u611f\u5206\u6790\u548c\u95ee\u9898\u5206\u7c7b\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9992849632901127
      ],
      "excerpt": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment composition-ality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9988893716599794
      ],
      "excerpt": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9990044259634334
      ],
      "excerpt": "We introduce a novel machine learning frame-work based on recursive autoencoders for sentence-level prediction of sentiment labeldistributions. Our method learns vector spacerepresentations for multi-word phrases. In sentiment prediction tasks these represen-tations outperform other state-of-the-art ap-proaches on commonly used datasets, such asmovie reviews, without using any pre-definedsentiment lexica or polarity shifting rules. Wealso  evaluate  the  model\u2019s  ability to predict sentiment distributions on a new dataset basedon confessions from the experience project. The dataset consists of personal user storiesannotated with multiple labels which, whenaggregated, form a multinomial distributionthat captures emotional reactions. Our algorithm can more accurately predict distri-butions over such labels compared to severalcompetitive baselines. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997853489047068
      ],
      "excerpt": "Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: \\emph{Gradient-based One-Side Sampling} (GOSS) and \\emph{Exclusive Feature Bundling} (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB \\emph{LightGBM}. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9962486318354461
      ],
      "excerpt": "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.998656108850186
      ],
      "excerpt": "  Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.900858013194623
      ],
      "excerpt": "<a href=\"https://xueshu.baidu.com/usercenter/paper/show?paperid=58aa6cfa340e6ae6809c5deadd07d88e&site=xueshu_se\">Text categorization with Support Vector Machines: Learning with many relevant features (SVM)</a>  by<i> JOACHIMS,T. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9911922871340161
      ],
      "excerpt": "  This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9939345559097618
      ],
      "excerpt": "  The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of errorRof such a rule must be at least as great as the Bayes probability of errorR^{\\ast}--the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in theM-category case thatR^{\\ast} \\leq R \\leq R^{\\ast}(2 --MR^{\\ast}/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9928167819110495
      ],
      "excerpt": "  This inquiry examines a technique for automatically classifying (indexing) documents according to their subject content. The task, in essence, is to have a computing machine read a document and on the basis of the occurrence of selected clue words decide to which of many subject categories the document in question belongs. This paper describes the design, execution and evaluation of a modest experimental study aimed at testing empirically one statistical technique for automatic indexing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9892245968173173,
        0.855382220164282
      ],
      "excerpt": "SA is the process of analyzing and reasoning the subjective text withinemotional color. It is crucial to get information on whether it supports a particular point of view fromthe text that is distinct from the traditional text classification that analyzes the objective content ofthe text. SA can be binary or multi-class. Binary SA is to divide the text into two categories, includingpositive and negative. Multi-class SA classifies text to multi-level or fine-grained labels.  \n\u60c5\u611f\u5206\u6790\uff08Sentiment Analysis\uff0cSA\uff09\u662f\u5728\u60c5\u611f\u8272\u5f69\u4e2d\u5bf9\u4e3b\u89c2\u6587\u672c\u8fdb\u884c\u5206\u6790\u548c\u63a8\u7406\u7684\u8fc7\u7a0b\u3002 \u901a\u8fc7\u5206\u6790\u6587\u672c\u6765\u5224\u65ad\u4f5c\u8005\u662f\u5426\u652f\u6301\u7279\u5b9a\u89c2\u70b9\u7684\u4fe1\u606f\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u4e0e\u5206\u6790\u6587\u672c\u5ba2\u89c2\u5185\u5bb9\u7684\u4f20\u7edf\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e0d\u540c\u3002 SA\u53ef\u4ee5\u662f\u4e8c\u5206\u7c7b\u4e5f\u53ef\u4ee5\u662f\u591a\u5206\u7c7b\u3002 Binary SA\u5c06\u6587\u672c\u5206\u4e3a\u4e24\u7c7b\uff0c\u5305\u62ec\u80af\u5b9a\u548c\u5426\u5b9a\u3002 \u591a\u7c7bSA\u5c06\u6587\u672c\u5206\u7c7b\u4e3a\u591a\u7ea7\u6216\u7ec6\u7c92\u5ea6\u66f4\u9ad8\u7684\u4e0d\u540c\u6807\u7b7e\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9346725833415578
      ],
      "excerpt": "The MR is a movie review dataset, each of which correspondsto a sentence. The corpus has 5,331 positive data and 5,331 negative data. 10-fold cross-validationby random splitting is commonly used to test MR.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.938267985272663
      ],
      "excerpt": "The SST [175] is an extension of MR. It has two cate-gories. SST-1 with fine-grained labels with five classes. It has 8,544 training texts and 2,210 testtexts, respectively. Furthermore, SST-2 has 9,613 texts with binary labels being partitioned into6,920 training texts, 872 development texts, and 1,821 testing texts. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8857987879210649
      ],
      "excerpt": "The MPQA is an opinion dataset. It has two class labels and also an MPQA dataset of opinion polarity detection sub-tasks.MPQA includes 10,606 sentences extracted from news articles from various news sources. It shouldbe noted that it contains 3,311 positive texts and 7,293 negative texts without labels of each text. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9415614527165832
      ],
      "excerpt": "The IMDB review is developed for binary sentiment classification of filmreviews with the same amount in each class. It can be separated into training and test groups onaverage, by 25,000 comments per group. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9204973236968825
      ],
      "excerpt": "The Yelp review is summarized from the Yelp Dataset Challenges in 2013,2014, and 2015. This dataset has two categories. Yelp-2 of these were used for negative and positiveemotion classification tasks, including 560,000 training texts and 38,000 test texts. Yelp-5 is used todetect fine-grained affective labels with 650,000 training and 50,000 test texts in all classes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955448778358444
      ],
      "excerpt": "The AM is a popular corpus formed by collecting Amazon websiteproduct reviews [190]. This dataset has two categories. The Amazon-2 with two classes includes 3,600,000 training sets and 400,000 testing sets. Amazon-5, with five classes, includes 3,000,000 and650,000 comments for training and testing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9690874789350274
      ],
      "excerpt": "News content is one of the most crucial information sources which hasa critical influence on people. The NC system facilitates users to get vital knowledge in real-time.News classification applications mainly encompass: recognizing news topics and recommendingrelated news according to user interest. The news classification datasets include 20NG, AG, R8, R52,Sogou, and so on. Here we detail several of the primary datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9062870702341144
      ],
      "excerpt": "The AG News is a search engine for news from academia, choosingthe four largest classes. It uses the title and description fields of each news. AG contains 120,000texts for training and 7,600 texts for testing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8831706343250291
      ],
      "excerpt": "The Sogou News combines two datasets, including SogouCA andSogouCS news sets. The label of each text is the domain names in the URL. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9946491078105699
      ],
      "excerpt": "The topic analysis attempts to get the meaning of the text by defining thesophisticated text theme. The topic labeling is one of the essential components of the topic analysistechnique, intending to assign one or more subjects for each document to simplify the topic analysis. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9635513053008261
      ],
      "excerpt": "The Ohsumed belongs to the MEDLINE database. It includes 7,400 texts andhas 23 cardiovascular disease categories. All texts are medical abstracts and are labeled into one ormore classes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9389393319445334
      ],
      "excerpt": "The YahooA is a topic labeling task with 10 classes. It includes140,000 training data and 5,000 test data. All text contains three elements, being question titles,question contexts, and best answers, respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9853613727840775
      ],
      "excerpt": "The QA task can be divided into two types: the extractive QA and thegenerative QA. The extractive QA gives multiple candidate answers for each question to choosewhich one is the right answer. Thus, the text classification models can be used for the extractiveQA task. The QA discussed in this paper is all extractive QA. The QA system can apply the textclassification model to recognize the correct answer and set others as candidates. The questionanswering datasets include SQuAD, MS MARCO, TREC-QA, WikiQA, and Quora [209]. Here wedetail several of the primary datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9647612044523474
      ],
      "excerpt": "The SQuAD is a set of question and answer pairs obtained from Wikipedia articles. The SQuAD has two categories. SQuAD1.1 contains 536 pairs of 107,785 Q&A items. SQuAD2.0 combines 100,000 questions in SQuAD1.1 with morethan 50,000 unanswerable questions that crowd workers face in a form similar to answerable questions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8619800641806701
      ],
      "excerpt": "The MS MARCO contains questions and answers. The questions and part ofthe answers are sampled from actual web texts by the Bing search engine. Others are generative. Itis used for developing generative QA systems released by Microsoft. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8379344358778605
      ],
      "excerpt": "The WikiQA dataset includes questions with no correct answer, which needs toevaluate the answer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.990191658563943
      ],
      "excerpt": "NLI is used to predict whether the meaning of one text canbe deduced from another. Paraphrasing is a generalized form of NLI. It uses the task of measuringthe semantic similarity of sentence pairs to decide whether one sentence is the interpretation ofanother. The NLI datasets include SNLI, MNLI, SICK, STS, RTE, SciTail, MSRP, etc. Here we detailseveral of the primary datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.963629420100891
      ],
      "excerpt": "The SNLI is generally applied toNLI tasks. It contains 570,152 human-annotated sentence pairs, including training, development,and test sets, which are annotated with three categories: neutral, entailment, and contradiction. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9896354664756708
      ],
      "excerpt": "The Multi-NLI is an expansion of SNLI, embracing a broader scope of written and spoken text genres. It includes 433,000 sentencepairs annotated by textual entailment labels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8688834598603122
      ],
      "excerpt": "The SICK contains almost10,000 English sentence pairs. It consists of neutral, entailment and contradictory labels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9316740064690275
      ],
      "excerpt": "The MSRP consists of sentence pairs, usuallyfor the text-similarity task. Each pair is annotated by a binary label to discriminate whether theyare paraphrases. It respectively includes 1,725 training and 4,076 test sets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.998939252525397
      ],
      "excerpt": "A dialog act describes an utterance in a dialog based on semantic,pragmatic, and syntactic criteria. DAC labels a piece of a dialog according to its category of meaningand helps learn the speaker\u2019s intentions. It is to give a label according to dialog. Here we detailseveral of the primary datasets, including DSTC 4, MRDA, and SwDA. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8287146700648396
      ],
      "excerpt": "The DSTC 4 is used for dialog act classi-fication. It has 89 training classes, 24,000 training texts, and 6,000 testing texts. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8388046803604519
      ],
      "excerpt": "The MRDA is used for dialog act classifi-cation. It has 5 training classes, 51,000 training texts, 11,000 testing texts, and 11,000 validation texts. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8722440090435166
      ],
      "excerpt": "The SwDA is used for dialog act classification. It has43 training classes, 1,003,000 training texts, 19,000 testing texts and 112,000 validation texts. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9066157108081448
      ],
      "excerpt": "In multi-label classification, an instance has multiple labels, and each la-bel can only take one of the multiple classes. There are many datasets based on multi-label textclassification. It includes Reuters, Education, Patent, RCV1, RCV1-2K, AmazonCat-13K, BlurbGen-reCollection, WOS-11967, AAPD, etc. Here we detail several of the main datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9686190250550923
      ],
      "excerpt": "The Reuters is a popularly used dataset for text classification fromReuters financial news services. It has 90 training classes, 7,769 training texts, and 3,019 testingtexts, containing multiple labels and single labels. There are also some Reuters sub-sets of data,such as R8, BR52, RCV1, and RCV1-v2. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8861536262821913
      ],
      "excerpt": "The Patent Dataset is obtained from USPTO1, which is a patent system gratingU.S. patents containing textual details such title and abstract. It contains 100,000 US patents awardedin the real-world with multiple hierarchical categories. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9705733209919264
      ],
      "excerpt": "The RCV1 is collected from Reuters News articles from 1996-1997, which is human-labeled with 103 categories. It consists of 23,149 training and 784,446 testing texts, respectively. The RCV1-2K dataset has the same features as the RCV1. However, the label set of RCV1-2K has been expanded with some new labels. It contains2456 labels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9889582231029301,
        0.9968029537584643
      ],
      "excerpt": "The WOS-11967 is crawled from the Web of Science,consisting of abstracts of published papers with two labels for each example. It is shallower, butsignificantly broader, with fewer classes in total. \nWOS-11967\u662f\u4eceWeb of Science\u722c\u53d6\u7684\uff0c\u5b83\u7531\u5df2\u53d1\u8868\u8bba\u6587\u7684\u6458\u8981\u7ec4\u6210\uff0c\u6bcf\u4e2a\u793a\u4f8b\u5e26\u6709\u4e24\u4e2a\u6807\u7b7e\u3002 \u8be5\u6570\u636e\u96c6\u6837\u672c\u6570\u8f83\u5c11\uff0c\u4f46\u8986\u76d6\u9762\u660e\u663e\u66f4\u5e7f\u6cdb\uff0c\u603b\u5171\u6709\u8f83\u5c11\u7684\u7c7b\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9591951943937739
      ],
      "excerpt": "The AAPD is a large dataset in the computer science field for the multi-label text classification from website2. It has 55,840 papers, including the abstract and the corresponding subjects with 54 labels in total. The aim is to predict the corresponding subjects of each paper according to the abstract. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9719190298480713
      ],
      "excerpt": "There are some datasets for other applications, such as Geonames toponyms, Twitter posts,and so on. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9801615989804964
      ],
      "excerpt": "In terms of evaluating text classification models, accuracy and F1 score are the most used to assessthe text classification methods. Later, with the increasing difficulty of classification tasks or theexistence of some particular tasks, the evaluation metrics are improved. For example, evaluationmetrics such as P@K and Micro-F1 are used to evaluate multi-label text classification performance,and MRR is usually used to estimate the performance of QA tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8962479539057638
      ],
      "excerpt": "Single-label text classification divides the text into one of the most likelycategories applied in NLP tasks such as QA, SA, and dialogue systems [9]. For single-label textclassification, one text belongs to just one catalog, making it possible not to consider the relationsamong labels. Here we introduce some evaluation metrics used for single-label text classificationtasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.842625957454396
      ],
      "excerpt": "Accuracy and Error Rate are the fundamental metrics for a text classification model. The Accuracy and Error Rate are respectively defined as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "<a >Precision, Recall and F1</a></summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8904738010771339
      ],
      "excerpt": "These are vital metrics utilized for unbalanced test sets regardless ofthe standard type and error rate. For example, most of the test samples have a class label. F1 is theharmonic average of Precision and Recall. Accuracy, Recall, and F1 as defined \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9869261851749266
      ],
      "excerpt": "The EM is a metric for QA tasks measuring the prediction that matches all theground-truth answers precisely. It is the primary metric utilized on the SQuAD dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9655391054970387
      ],
      "excerpt": "The MRR is usually applied for assessing the performance of ranking algorithms on QA and Information Retrieval (IR) tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9664713406602634
      ],
      "excerpt": "The HL assesses the score of misclassified instance-label pairs wherea related label is omitted or an unrelated is predicted. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9146642812507463
      ],
      "excerpt": "Compared with single-label text classification, multi-label text classifica-tion divides the text into multiple category labels, and the number of category labels is variable. These metrics are designed for single label text classification, which are not suitable for multi-label tasks. Thus, there are some metrics designed for multi-label text classification. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9857329614880302
      ],
      "excerpt": "The Micro\u2212F1 is a measure that considers the overall accuracy and recall of alllabels. The Micro\u2212F1is defined as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9542901710915815
      ],
      "excerpt": "The Macro\u2212F1 calculates the average F1 of all labels. Unlike Micro\u2212F1, which setseven weight to every example, Macro\u2212F1 sets the same weight to all labels in the average process. Formally, Macro\u2212F1is defined as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8517618488271302
      ],
      "excerpt": "In addition to the above evaluation metrics, there are some rank-based evaluation metrics forextreme multi-label classification tasks, including P@K and NDCG@K. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9664999583800932
      ],
      "excerpt": "The P@K is the precision at the top k. ForP@K, each text has a set of L ground truth labels Lt={l0,l1,l2...,lL\u22121}, in order of decreasing probability Pt=p0,p1,p2...,pQ\u22121.The precision at k is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.959444779061272
      ],
      "excerpt": "The NDCG at k is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "\u6587\u672c\u5206\u7c7b\u8d44\u6e90\u6c47\u603b\uff0c\u5305\u62ec\u6df1\u5ea6\u5b66\u4e60\u6587\u672c\u5206\u7c7b\u6a21\u578b\uff0c\u5982SpanBERT\u3001ALBERT\u3001RoBerta\u3001Xlnet\u3001MT-DNN\u3001BERT\u3001TextGCN\u3001MGAN\u3001TextCapsule\u3001SGNN\u3001SGM\u3001LEAM\u3001ULMFiT\u3001DGCNN\u3001ELMo\u3001RAM\u3001DeepMoji\u3001IAN\u3001DPCNN\u3001TopicRNN\u3001LSTMN \u3001Multi-Task\u3001HAN\u3001CharCNN\u3001Tree-LSTM\u3001DAN\u3001TextRCNN\u3001Paragraph-Vec\u3001TextCNN\u3001DCNN\u3001RNTN\u3001MV-RNN\u3001RAE\u7b49\uff0c\u6d45\u5c42\u5b66\u4e60\u6a21\u578b\uff0c\u5982LightGBM \u3001SVM\u3001XGboost\u3001Random Forest\u3001C4.5\u3001CART\u3001KNN\u3001NB\u3001HMM\u7b49\u3002\u4ecb\u7ecd\u6587\u672c\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u5982MR\u3001SST\u3001MPQA\u3001IMDB\u3001Yelp\u300120NG\u3001AG\u3001R8\u3001DBpedia\u3001Ohsumed\u3001SQuAD\u3001SNLI\u3001MNLI\u3001MSRP\u3001MRDA\u3001RCV1\u3001AAPD\uff0c\u8bc4\u4ef7\u6307\u6807\uff0c\u5982accuracy\u3001Precision\u3001Recall\u3001F1\u3001EM\u3001MRR\u3001HL\u3001Micro-F1\u3001Macro-F1\u3001P@K\uff0c\u548c\u6280\u672f\u6311\u6218\uff0c\u5305\u62ec\u591a\u6807\u7b7e\u6587\u672c\u5206\u7c7b\u3002",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://xgboost.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xiaoqian19940510/text-classification-/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 73,
      "date": "Fri, 24 Dec 2021 14:01:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xiaoqian19940510/text-classification-surveys/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "xiaoqian19940510/text-classification-surveys",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/xiaoqian19940510/text-classification-/master/ChineseTextClassification/GCN_based/text_gcn/train.sh",
      "https://raw.githubusercontent.com/xiaoqian19940510/text-classification-/master/ChineseTextClassification/GCN_based/src/test.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8979602292317721
      ],
      "excerpt": "</a>  (<a href=\"https://github.com/pytorch/fairseq\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979602292317721
      ],
      "excerpt": "</a>(<a href=\"https://github.com/songyouwei/ABSA-PyTorch\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979602292317721
      ],
      "excerpt": "</a>(<a href=\"https://github.com/songyouwei/ABSA-PyTorch\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979602292317721
      ],
      "excerpt": "</a>(<a href=\"https://github.com/songyouwei/ABSA-PyTorch\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8516828110319933
      ],
      "excerpt": "</a>(<a href=\"https://github.com/tensorflow/models/tree/master/adversarial_text\">Github</a>)</summary><blockquote><p align=\"justify\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8359299706379749
      ],
      "excerpt": "\u6587\u672c\u5206\u7c7b\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u800c\u7ecf\u5178\u7684\u95ee\u9898\u3002\u5df2\u7ecf\u6709\u5f88\u591a\u7814\u7a76\u5c06\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (\u89c4\u5219\u7f51\u683c\u4e0a\u7684\u5377\u79ef\uff0c\u4f8b\u5982\u5e8f\u5217) \u5e94\u7528\u4e8e\u5206\u7c7b\u3002\u7136\u800c\uff0c\u53ea\u6709\u4e2a\u522b\u7814\u7a76\u63a2\u7d22\u4e86\u5c06\u66f4\u7075\u6d3b\u7684\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(\u5728\u975e\u7f51\u683c\u4e0a\u5377\u79ef\uff0c\u5982\u4efb\u610f\u56fe)\u5e94\u7528\u5230\u8be5\u4efb\u52a1\u4e0a\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u6765\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u3002\u57fa\u4e8e\u8bcd\u7684\u5171\u73b0\u5173\u7cfb\u548c\u6587\u6863\u8bcd\u7684\u5173\u7cfb\uff0c\u672c\u6587\u4e3a\u6574\u4e2a\u8bed\u6599\u5e93\u6784\u5efa\u5355\u4e2a\u6587\u672c\u56fe\uff0c\u7136\u540e\u5b66\u4e60\u7528\u4e8e\u8bed\u6599\u5e93\u7684\u6587\u672c\u56fe\u5377\u79ef\u7f51\u7edc(text GCN)\u3002\u672c\u6587\u7684text-GCN\u9996\u5148\u5bf9\u8bcd\u8bed\u548c\u6587\u672c\u4f7f\u7528one-hot\u7f16\u7801\u8fdb\u884c\u521d\u59cb\u5316\uff0c\u7136\u540e\u5728\u5df2\u77e5\u6587\u6863\u7c7b\u6807\u7b7e\u7684\u76d1\u7763\u4e0b\u8054\u5408\u5b66\u4e60\u5355\u8bcd\u548c\u6587\u672c\u7684\u5d4c\u5165\uff08\u901a\u8fc7GCN\u7f51\u7edc\u4f20\u64ad\uff09\u3002\u6211\u4eec\u7684\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e00\u4e2a\u6ca1\u6709\u4efb\u4f55\u5916\u90e8\u8bcd\u6216\u77e5\u8bc6\u5d4c\u5165\u7684\u666e\u901atext-GCN\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u3002\u53e6\u4e00\u65b9\u9762\uff0cText -GCN\u4e5f\u5b66\u4e60\u8bcd\u7684\u9884\u6d4b\u548c\u6587\u6863\u5d4c\u5165\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u964d\u4f4e\u8bad\u7ec3\u6570\u636e\u7684\u767e\u5206\u6bd4\u65f6\uff0c\u6587\u672cGCN\u76f8\u5bf9\u4e8e\u73b0\u6709\u6bd4\u8f83\u65b9\u6cd5\u7684\u6539\u8fdb\u66f4\u52a0\u663e\u8457\uff0c\u8bf4\u660e\u5728\u6587\u672c\u5206\u7c7b\u4e2d\uff0c\u6587\u672cGCN\u5bf9\u8f83\u5c11\u7684\u8bad\u7ec3\u6570\u636e\u5177\u6709\u9c81\u68d2\u6027\u3002 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xiaoqian19940510/text-classification-surveys/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Text Classification papers/surveys\uff08\u6587\u672c\u5206\u7c7b\u8d44\u6599\u7efc\u8ff0\u603b\u7ed3\uff09\u66f4\u65b0\u4e2d...",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "text-classification-surveys",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "xiaoqian19940510",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xiaoqian19940510/text-classification-surveys/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 357,
      "date": "Fri, 24 Dec 2021 14:01:01 GMT"
    },
    "technique": "GitHub API"
  }
}