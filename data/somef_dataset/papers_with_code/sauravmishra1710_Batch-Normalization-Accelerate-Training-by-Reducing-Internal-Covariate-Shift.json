{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1502.03167\n\n[2] **How Does Batch Normalization Help Optimization?**<br>\nPaper Link @ https://arxiv.org/abs/1805.11604\n\nA high level paper summary of both the papers are discussed below - \n\n## Batch Normalization: Accelerate Training by Reducing Internal Covariate Shift\nOne complication with training deep neural networks is always related to the distribution of inputs. The distribution of input to each layer changes during the training phase since the parameters of the layers change. This phenomenon is known as ***INTERNAL CO-VARIATE SHIFT*** and is known to slow down the learning process. \n\n**Batch Normalization** is a technique to reduce the internal co-variate shift and improve the training speed, performance and stability of deep neural networks. Batch Normalization make normalization a part of the model architecture and considers normalization for each ***mini-batch*** during the training phase. \n\nBatch Normalization advantages - \n\n- Allows us to use much higher learning rates and worry less about initialization. \n- Ocassionally acts as a regularizer, in some cases eliminating the need for Dropout.\n- Attempts to solve the vanishing gradient problem.\n- Makes the network more stable to the initialization of the weights.\n\n## How Does Batch Normalization Help Optimization?\n\nThis paper discusses more in detail of how the batch normalization works. The authors point that input distributions of layer inputs has little to do with the success of BatchNorm.  Instead, a more fundamental impact of Batch Normalization on the training process is that it **makes the optimization landscape significantly smoother**. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.",
      "https://arxiv.org/abs/1805.11604\n\nA high level paper summary of both the papers are discussed below - \n\n## Batch Normalization: Accelerate Training by Reducing Internal Covariate Shift\nOne complication with training deep neural networks is always related to the distribution of inputs. The distribution of input to each layer changes during the training phase since the parameters of the layers change. This phenomenon is known as ***INTERNAL CO-VARIATE SHIFT*** and is known to slow down the learning process. \n\n**Batch Normalization** is a technique to reduce the internal co-variate shift and improve the training speed, performance and stability of deep neural networks. Batch Normalization make normalization a part of the model architecture and considers normalization for each ***mini-batch*** during the training phase. \n\nBatch Normalization advantages - \n\n- Allows us to use much higher learning rates and worry less about initialization. \n- Ocassionally acts as a regularizer, in some cases eliminating the need for Dropout.\n- Attempts to solve the vanishing gradient problem.\n- Makes the network more stable to the initialization of the weights.\n\n## How Does Batch Normalization Help Optimization?\n\nThis paper discusses more in detail of how the batch normalization works. The authors point that input distributions of layer inputs has little to do with the success of BatchNorm.  Instead, a more fundamental impact of Batch Normalization on the training process is that it **makes the optimization landscape significantly smoother**. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9991295298092048
      ],
      "excerpt": "Paper Link @ https://arxiv.org/abs/1502.03167 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9991295298092048
      ],
      "excerpt": "Paper Link @ https://arxiv.org/abs/1805.11604 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sauravmishra1710/Batch-Normalization-and-Internal-Covariate-Shift",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-12T13:31:38Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-17T13:52:56Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8771565275867904
      ],
      "excerpt": "This repository discusses the following two papers on Batch Normalization -  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8958344728235409,
        0.9853952718515863,
        0.9269824759981536
      ],
      "excerpt": "A high level paper summary of both the papers are discussed below - \nOne complication with training deep neural networks is always related to the distribution of inputs. The distribution of input to each layer changes during the training phase since the parameters of the layers change. This phenomenon is known as INTERNAL CO-VARIATE SHIFT and is known to slow down the learning process.  \nBatch Normalization is a technique to reduce the internal co-variate shift and improve the training speed, performance and stability of deep neural networks. Batch Normalization make normalization a part of the model architecture and considers normalization for each mini-batch during the training phase.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057235046564356
      ],
      "excerpt": "Ocassionally acts as a regularizer, in some cases eliminating the need for Dropout. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Batch Normalization: Accelerate Training by Reducing Internal Covariate Shift - Paper Review",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sauravmishra1710/Batch-Normalization-Accelerate-Training-by-Reducing-Internal-Covariate-Shift/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 14:26:55 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sauravmishra1710/Batch-Normalization-and-Internal-Covariate-Shift/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "sauravmishra1710/Batch-Normalization-and-Internal-Covariate-Shift",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/sauravmishra1710/Batch-Normalization-Accelerate-Training-by-Reducing-Internal-Covariate-Shift/main/How%20Does%20Batch%20Norm%20Help%20Optimization.ipynb",
      "https://raw.githubusercontent.com/sauravmishra1710/Batch-Normalization-Accelerate-Training-by-Reducing-Internal-Covariate-Shift/main/Batch%20Normalization%20-%20Accelerate%20Training%20by%20Reducing%20Internal%20Covariate%20Shift.ipynb",
      "https://raw.githubusercontent.com/sauravmishra1710/Batch-Normalization-Accelerate-Training-by-Reducing-Internal-Covariate-Shift/main/.ipynb_checkpoints/Batch%20Normalization%20-%20Accelerate%20Training%20by%20Reducing%20Internal%20Covariate%20Shift-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sauravmishra1710/Batch-Normalization-and-Internal-Covariate-Shift/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Batch Normalization : Internal Covariate Shift",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Batch-Normalization-and-Internal-Covariate-Shift",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "sauravmishra1710",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sauravmishra1710/Batch-Normalization-and-Internal-Covariate-Shift/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 14:26:55 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This paper discusses more in detail of how the batch normalization works. The authors point that input distributions of layer inputs has little to do with the success of BatchNorm.  Instead, a more fundamental impact of Batch Normalization on the training process is that it **makes the optimization landscape significantly smoother**. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.\n",
      "technique": "Header extraction"
    }
  ]
}