# Merlin Bioassay Data Analyzer

The purpose of this program is to analyze the images taken from the Biotek Cytation5 platereader. The program also has the ability to perform dose-response curve analysis and calculate LC values. Platereader images are preprocessed, and larvae are identified, classified, and counted using a modification of the [matterport implementation](https://github.com/matterport/Mask_RCNN)[^2] of Mask R-CNN[^3]. Dose-response data is determined using a Bayesian approach to generate dose-response curves, estimated LC values, and corresponding credible intervals. 

## Use
For Windows 10, the recommended method is to use the included batch file to invoke the program. The PYTHON_HOME variable should be set to the computer's path to the python executable. Once the program is started, the GUI can be used to enter all the required data. See the [Graphical User Interface section](#gui) for details. 

Alternatively, one may invoke the program directly with `main.py`, which takes no command line arguments. 

All variables used by the program are set in either `./config.txt` or in `./stats/analysis_config.txt`; the former contains arguments for the GUI and general program options and filepaths, while the latter contains options related to data analysis, statistical options, and filepaths related to the statistical analysis output. The [`./docs/stats_config`](./docs/stats_config.md) file describes the permitted variables and their actions for the statistical analysis and plotting options, while [`./docs/general_config`](./docs/general_config.md) describes the permitted options for the general program. 

## Requirements

This program requires the use of Python 3. The testing was performed with Python 3.8. The Anaconda distribution is recommended for ease; otherwise, `scikit-image`, `numpy`, `scipy`, `tensorflow`, `tkinter`, and other packages may be needed. 

The GUI does require the addition of `tkcalendar`, which is easily installed using pip. 

Testing has been performed on Ubuntu 20.04 and Windows 10 using Python 3.8.8.

Tensorflow is used for analyzing the images with the Mask R-CNN algorithm. The process is highly parallelized, and runs fastest with a CUDA-enabled NVIDIA graphics card; however, images can also be processed on multiple CPU cores.

The statistics module, which analyzes live/dead count data, contains several functions that are written in both C and python. The C code must be compiled and requires the [GNU Scientific Library](https://www.gnu.org/software/gsl/), and must be linked to this library during compilation into a `.so` or `.dll` file on Linux/MacOS or Windows, respectively. See the statistic readme for more details. If the C library is not compiled, full functionality is maintained using python functions; however, the C library provides substantial speed-up. 

## <a name="gui"></a>Graphical User Interface

![An image of the GUI on Ubuntu 20.04. The GUI is better scaled on Windows 10.](./docs/img/Program.png) 

The graphical user interface (GUI) allows the user to browse directories to find the appropriate folder containing the Gen5 images. If the appropriate parent directory is selected in the `config.txt` file using the `IMG_DIR` keyword, then the `Image folder path` is autopopulated with the most recent directory of images. The default `Ouput parent directory` defaults to the value in given in the `config.txt` file with the `OUT_DIR` keyword. The GUI also allows the user to select which images are created for the output. When the corresponding check box is checked, the program generates intermediate imates (INTMD images), masked images, or bounding box images. See below for examples. By default, the date is is selected to be the day that the plate was prepared, i.e., one day before the plate is read in the platereader. This is determined using the directory name generated by Gen5, which includes the date. This default behavior can be changed in the `config.txt` file. 

The Plate Configuration section of the GUI allows the user to input the compound code, compound name, and maximum concentration information, which is used to fill out the output .csv file. This data is best entered before starting the run--if this information is not entered prior to running the image analysis, it will have to be entered manually into the output file. 

The image analysis/larva counting process is initiated by hitting the "Run Image Analysis" button. The status bar shows the progress of the program, and an estimated percentage completion is show. If there are any errors, this information is also showed above the status bar. 

Statistical anlysis can also be performed from the GUI by hitting the "Run Statistics" button. 


## Data Collection and Analysis Pipeline

The pipeline from image collection to data output is as follows:

1. The platereader takes three high-resolution, 2048x2048 pixel .tiff images of the well. Images are spaced approximately 0.2 seconds apart, and are saved locally. 

![Example of the three images of a single well, taken fractions of a second apart. Note the movement of some larvae near the top of the image.](./docs/img/initial_images.jpg) 

<!-- <figure>
  <img src="./docs/img/B2_04.jpg" width="200" />
  <img src="./docs/img/B2_05.jpg" width="200" /> 
  <img src="./docs/img/B2_06.jpg" width="200" />
  <figcaption aria-hidden="true">Example of the three images of a single well, taken fractions of a second apart. Note the movement of some larvae near the top of the image.</figcaption>
</figure> -->
2. For each well, these three images are used to make a composite image with six channels as follows:

	1. Red channel of the first image
	2. Green channel of the first image
	3. Blue channel of the first image
	4. Prewitt edge filter of guassian blurred grayscale version of the first image
	5. Grayscale of the second image
	6. Grayscale of the third image

3. The grayscale of the first image is also subjected to a Circle Hough Transform to determine the edge of the well. The area outside the well is set to mean value of the inside of the well for each image channel. The image is also cropped and centered on the center of the well, and downsized to 800x800 pixels and single precision. Setting the area outside the well to a uniform color removes any reflections of the larvae and ensures that objects are not detected in this area. 
4. The trained Mask R-CNN algorithm processes each image to classify larvae as dead, alive, moribund, L2 larvae, egg, long-dead, or artifact. 
5. The number of each class is counted, and, if the user has selected the option, composite images are made. Options for composite images include a stack of the three grayscale images for the well as the red, green, and blue channels; a grayscale of the first image with bounding boxes, or a grayscle of the first image with colored masks. In images with bounding boxes or masks shown, the following colors are used:

	- <span style="color:LimeGreen">live: green </span>
	- <span style="color:Crimson">dead: red </span>
	- <span style="color:Gold">moribund: yellow </span>
	- <span style="color:BlueViolet">L2 larvae: purple</span>
	- <span style="color:blue">egg: blue </span>
	- <span style="color:SaddleBrown">long-dead: brown </span>
	- <span style="color:Turquoise">artifact: cyan </span>


![The RGB composite, bounding box, and masked output images corresponding to the three raw images shown above. Note that the images are centered on the well, and the area outside the well is a uniform gray. Artifact bounding boxes are not shown in the bounding box image to prevent visual crowding in wells with significant debris. Note that this well contains objects identified as live, dead, morbund, and L2, as well as several small bubbles classified as artifacts.](./docs/img/processed.jpg) 

<!-- <figure>
  <img src="./docs/img/B2_comp.png" width="200" />
  <img src="./docs/img/B2_bbox.png" width="200" /> 
  <img src="./docs/img/B2_splash.png" width="200" />
  <figcaption aria-hidden="true">The RGB composite, bounding box, and masked output images corresponding to the three raw images shown above. Note that the images are centered on the well, and the area outside the well is a uniform gray. Artifact bounding boxes are not shown in the bounding box image to prevent visual crowding in wells with significant debris. Note that this well contains objects identified as live, dead, morbund, and L2, as well as several small bubbles classified as artifacts. </figcaption>
</figure> -->
6. A .csv file containing the count data for each well is produced. Only the live, dead, and moribund larvae are counted; the moribund larvae are included with the dead larvae. The other four groups are not included in the count data; however, if there are more than five total objects not classified as live, dead, or moribund, a note is included in the .csv file stating the number of eggs, L2 larvae, long-dead larvae, and artifacts in the well. 

## Training Set

The Mask R-CNN algorithm was trained using a set of 800 hand-classified wells. This training set included 6032 live larvae, 942 moribund larvae, 2178 dead larvae, 164 eggs, 290 aged dead larvae, 173 L2 larve, and 328 artifacts. Labelling of images was performed using the [VGG Image Annotator (VIA) version 2](https://www.robots.ox.ac.uk/~vgg/software/via/).[^1] 

The data set was augmented to produce 48,000 training images and 8,000 validation images. Augmentation was performed using `misc/image_augment.py`, which applies random rotations, scaling, gamma adjustments, blurring, and flipping about the y-axis to generate new images and corresponding json images. The augmentation program also creates a new annotation `.json` file for all the transformed labels for the augmented images. Note that the `.json` annotation file must match that of the VIA output annotation to be properly adjusted during the augmentation process.


## Data Analysis

A more-comprehensive treatment of the statistical analysis is provided in the `./docs/statistical_overview.pdf` file or its corresponding $\LaTeX$ file. 

Briefly, the live/dead count data for each well is used to generate a beta distribution posterior distribution, with the beta prior set using the `BETA_PRIOR` paramter in the `./stats/analysis_config.txt` file. Each replicate of dose-response data is correlated by some unknown amount (based simply on the fact that each concentration is produced through serial dilutions of the initial concentration, and that all larvae in a replicate come from the same cohort). The `RHO` parameter is used to set the correlation betwee adjacent concentrations in a single replicate; the correlation between different cohorts of larvae is assumed to be zero. See `./docs/statistical_overview.pdf` for details on the correlation matrix. The correlation matrix is then used to generated correlated MVN variables, and a copula is used to convert these variable to correlated beta variables sampled from the posterior beta distributions of each live/dead sample. This process is repeated `BOOTSTRAP_ITERS` times to produce a sample of bootstrapped data. 

The default behavior is to fit the bootstrapped data to a three-parameter dose-response curve *s*(*x*) = *b*<sub>2</sub>/(1 + exp(*b*<sub>0</sub> + *b*<sub>1</sub>*x*), where *s(x)* is the survival probability at log-concentration *x*, *b<sub>2</sub>* is the background survival rate (which is typically somewhat less than 1 due to background mortality), *b<sub>1</sub>* is related to the slope/steepness of the dose-response curve, and the ratio *-b<sub>0</sub>*/*b<sub>1</sub>* is equal to the LC<sub>50</sub>.These curves are then used to generate posterior distributions of LC values to permit the determination of the LC value and corresponding credible intervals, and sampling these curves at various concentrations allows us to create credible intervals for the curve itself. 



[^1]: Abhishek Dutta and Andrew Zisserman. **2019**. *The VIA Annotation Software for Images, Audio and Video*. In *Proceedings of the 27th ACM International Conference on Multimedia (MM ’19)*, October 21–25, 2019, Nice, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3343031.3350535.
[^2]: Waleed Abdulla. **2017**. *Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow*. GitHub repository. https://github.com/matterport/Mask_RCNN
[^3]: Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick. **2017**. *Mask R-CNN*. arXiv:1703.06870. https://arxiv.org/abs/1703.06870
