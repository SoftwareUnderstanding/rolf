{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1707.06347\n\nBoth program designed good interfaces to pre-process and pass in data, calibrate hyper-parameters, save/restore agent specifications and performance visualization.\n\nThe program is easy and clear to use. Here are some hints that might help you to understand and recall some basic information mentioned above.\n\nThe program is composed of several blocks:\n## Environment Setup\n### Action & Rewards\nChoose and call the action and rewards modules for the program.\n\nHere DirectProfitStrategy and FutureActionStrategy should be called and used.\n\n### Feature_pipeline\nData pre-process module that is currently unused but remained in the interface for customized calling.\n\n### Data Input(Exchanges"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9384552127166319,
        0.8111036989382164
      ],
      "excerpt": "Songhao Li \nModified from https://github.com/notadamking/tensortrade  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EconomistGrant/HTFE-tensortrade",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-06T10:01:33Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-19T06:05:35Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9838238904048653,
        0.91191115541398,
        0.9416395895984693,
        0.9217329011604904
      ],
      "excerpt": "Tensortrade is a modulized quantitative reinforcement learning project. According to my adjustment, this project has achieved self-provided data input, futures market environment configuration, and deployment of tensorforce agents incluing PPO and A2C. \nFor a preview of the learning results using simple_ppo.py program in the project (PPO algorithm: https://arxiv.org/abs/1707.06347) \nThe reward(profits) of the agent in an episode, with the initial investment $10,000.  \nThe learning curve of the agent over episodes. After 600 episodes (trials), the rewards apparently converge. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8318804184842473
      ],
      "excerpt": "Modules Overview  | Running the Program | Details of Modules | Case Analysis (PPO/A2C) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.893165218249792
      ],
      "excerpt": "1. Exchanges (state space) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8409765022373101,
        0.9497472749089373
      ],
      "excerpt": "This four sub-moduls have been expanded with new classed to fit futures market environment; there will be specified introduction later in this document. \nLearning agent is managed by strategies.tensorforce_trading_strategy to call and configure a tensorforce learning agent. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8116807302137188
      ],
      "excerpt": "Call agent.observe: judging whether to update networks; recording reward; going into next timestep \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9070826700577611
      ],
      "excerpt": "Exchage module is the \"market environment\" that we normally understand. This module consists of data input of each episode, and obervation generation for each step \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8211270425804342,
        0.9751461366909213
      ],
      "excerpt": "Determines whether the current position (amount of securities hold) will be included into obs and observed by the agent. \nThis is theoritically very important to make the agent more \"sensible\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8169296993746966,
        0.8254491591603793,
        0.9337953215586146,
        0.9689888669419544,
        0.9491649402792236,
        0.9186019564725022,
        0.8155730623832492
      ],
      "excerpt": "1. obs: the observations that will be regarded as input to the agent \n2. price: the price that will be used to calculate profit and reward \nSuch separation makes the program more robust to future data issues \nThe method to judge whether a trade decision generated by the agent is valid in the market environment \nCurrently it is defined as: \n1. Amount of secureites hold <= 1, and \n2. Value of securities hold <= net value of the account \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9150039183116457,
        0.8792367046787585
      ],
      "excerpt": "Before reset, I transform Performance to pandas.dataframe and print the tail of it to the console. \nThis is modified further based on future_exchange.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.808525587788275,
        0.9193299841638262,
        0.9830264350325844,
        0.9139075027755718
      ],
      "excerpt": "since each action at its timestep is the amount of securities hold, there shouldn't be any restriction on such decision. It will return True everytime. \nTo judge the action(the amount of securities hold) at this timestep, the program will observe price at the next timestep to give reward. \nNotice that this is only used in reward module and the future price will not be passed on to the agent, so there is no future data issues by using this method. \nAction module is responsible to generate the action space for the reinforement learning environment, and interpret the action (typically a natural number) from the core of agent to a trade class \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8033412317959783,
        0.8638953299879121
      ],
      "excerpt": "Each action corresponds to the amount of securities hold. \nIf n_action = 5, there will be five kinds of actions: 0 | 1 | 2 | 3 | 4 that are generated by the agent, which correspond to five kinds of security holdings: -1 | -0.5 | 0 | 0.5 | 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9911453361586697
      ],
      "excerpt": "Methods of last_position and next_price, to  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8583651616484091,
        0.8877266600655057,
        0.9635107758272291,
        0.9601687533809103,
        0.9606997349282793
      ],
      "excerpt": "2. read price of next timestep to evaluate the action made at this timestep (coherent with next_price in future_exchange_position.py) \nNotice that this suits only update by episode but not updates by timesteps \nReward of each episode is calculate by: \nsecurity holdings after the action of the last timestep * (price of this timestep - price of the last timestep) - trade fees incured for the action of this timestep \nThis reward can not evaluate the action made at this timestep, but over a whole episode, the sum of rewards equals to the total profit of the agent in this episode, so this is called \"DirectProfitStrategy\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8914517846240337,
        0.9378264371656798
      ],
      "excerpt": "To support generation and the data flow of trade instance \nTrade instances flow among different modules to pass on the specifications of trade action \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9055351525861568,
        0.9118064872369253,
        0.9067105523728577
      ],
      "excerpt": "Both program designed good interfaces to pre-process and pass in data, calibrate hyper-parameters, save/restore agent specifications and performance visualization. \nThe program is easy and clear to use. Here are some hints that might help you to understand and recall some basic information mentioned above. \nThe program is composed of several blocks: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9739619133804029
      ],
      "excerpt": "Data pre-process module that is currently unused but remained in the interface for customized calling. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Tensortrade project for reinforcement learning in futures market",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EconomistGrant/HTFE-tensortrade/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 16,
      "date": "Thu, 30 Dec 2021 02:55:15 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/EconomistGrant/HTFE-tensortrade/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "EconomistGrant/HTFE-tensortrade",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Merge all environment settings together by TradingEnvironment and TensorForceTradingStrategy\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.827500681962232
      ],
      "excerpt": "Modified from https://github.com/notadamking/tensortrade  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8365220162126692
      ],
      "excerpt": "There are two major modules: trading environment and learning agent. Trading environment is managed by environment.TradingEnvironment, including the following sub-modules: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801032478201142
      ],
      "excerpt": "Substitute Pandas procedures with Numpy to improve calculation efficiency \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8475009402458092
      ],
      "excerpt": "I wrote future_exchange.py and future_exchange_position.py based on simulated_exchange.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8475009402458092
      ],
      "excerpt": "I wrote future_action_strategy.py and future_position_strategy.py based on discrete_action_strategy.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8693503248200412
      ],
      "excerpt": "Data format: please check Data/TA.csv as an example (futures market data on PTA) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/EconomistGrant/HTFE-tensortrade/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "HTML"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Tesortrade",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "HTFE-tensortrade",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "EconomistGrant",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EconomistGrant/HTFE-tensortrade/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Two tutorails using sample codes/data in the project will be offered; the runner of the program is set up in strategies.tensorforce_trading_strategies.run. \n\nBasically, the program iterates on every episodes, and in each episode the program iterates on every timesteps\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "simple_PPO.py is the simplest program while PPO_learn configured in-out-sample analysis and save/load of the agent\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 39,
      "date": "Thu, 30 Dec 2021 02:55:15 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "simple_PPO.py is the simplest program while PPO_learn configured in-out-sample analysis and save/load of the agent\n",
      "technique": "Header extraction"
    }
  ]
}