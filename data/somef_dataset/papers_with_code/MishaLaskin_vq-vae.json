{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1711.00937",
      "https://arxiv.org/abs/1512.03385"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MishaLaskin/vqvae",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-06T19:40:57Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T16:37:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8474438791022325
      ],
      "excerpt": "This is a PyTorch implementation of the vector quantized variational autoencoder (https://arxiv.org/abs/1711.00937).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8237042847414902
      ],
      "excerpt": "A VectorQuantizer class which transform the encoder output into a discrete one-hot vector that is the index of the closest embedding vector z_e -&gt; z_q \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8674442051909835
      ],
      "excerpt": "The Encoder / Decoder classes are convolutional and inverse convolutional stacks, which include Residual blocks in their architecture see ResNet paper. The residual models are defined by the ResidualLayer and ResidualStack classes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9605985935642228
      ],
      "excerpt": "To sample from the latent space, we fit a PixelCNN over the latent pixel values z_ij. The trick here is recognizing that the VQ VAE maps an image to a latent space that has the same structure as a 1 channel image. For example, if you run the default VQ VAE parameters you'll RGB map images of shape (32,32,3) to a latent space with shape (8,8,1), which is equivalent to an 8x8 grayscale image. Therefore, you can use a PixelCNN to fit a distribution over the \"pixel\" values of the 8x8 1-channel latent space. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9103660012826962
      ],
      "excerpt": "Use saved VQ VAE parameters to encode your dataset and save discrete latent space representations with np.save API. In the quantizer.py this is the min_encoding_indices variable.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A pytorch implementation of the vector quantized variational autoencoder (https://arxiv.org/abs/1711.00937)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MishaLaskin/vq-vae/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 29,
      "date": "Sat, 25 Dec 2021 18:30:54 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MishaLaskin/vqvae/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "MishaLaskin/vqvae",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/MishaLaskin/vq-vae/master/visualization.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To install dependencies, create a conda or virtual environment with Python 3 and then run `pip install -r requirements.txt`. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9077690950705612
      ],
      "excerpt": "To train the PixelCNN on latent representations, you first need to follow these steps: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python pixelcnn/gated_pixelcnn.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MishaLaskin/vqvae/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Vector Quantized Variational Autoencoder",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "vqvae",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "MishaLaskin",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MishaLaskin/vqvae/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To install dependencies, create a conda or virtual environment with Python 3 and then run `pip install -r requirements.txt`. \n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run the VQ-VAE simply run `python3 main.py`. Make sure to include the `-save` flag if you want to save your model. You can also add parameters in the command line. The default values are specified below:\n\n```python\nparser.add_argument(\"--batch_size\", type=int, default=32)\nparser.add_argument(\"--n_updates\", type=int, default=5000)\nparser.add_argument(\"--n_hiddens\", type=int, default=128)\nparser.add_argument(\"--n_residual_hiddens\", type=int, default=32)\nparser.add_argument(\"--n_residual_layers\", type=int, default=2)\nparser.add_argument(\"--embedding_dim\", type=int, default=64)\nparser.add_argument(\"--n_embeddings\", type=int, default=512)\nparser.add_argument(\"--beta\", type=float, default=.25)\nparser.add_argument(\"--learning_rate\", type=float, default=3e-4)\nparser.add_argument(\"--log_interval\", type=int, default=50)\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 145,
      "date": "Sat, 25 Dec 2021 18:30:54 GMT"
    },
    "technique": "GitHub API"
  }
}