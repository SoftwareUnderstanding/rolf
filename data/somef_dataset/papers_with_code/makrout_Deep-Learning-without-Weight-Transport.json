{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@article{akrout2019deep,\n  title={Deep Learning without Weight Transport.},\n  author={Akrout, Mohamed and Wilson, Collin and Humphreys, Peter C and Lillicrap, Timothy P and Tweed, Douglas B},\n  journal={CoRR, abs/1904.05391},\n  year={2019}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The backpropagation code uses the same function structure of the backpagation code of Michael Nielsen's [repository](https://github.com/mnielsen/neural-networks-and-deep-learning). However, we added different code refactoring, batch learning and the two new algorithms we proposed in the paper.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{akrout2019deep,\n  title={Deep Learning without Weight Transport.},\n  author={Akrout, Mohamed and Wilson, Collin and Humphreys, Peter C and Lillicrap, Timothy P and Tweed, Douglas B},\n  journal={CoRR, abs/1904.05391},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.944783340987309
      ],
      "excerpt": "Preprint here, feedback welcome! Contact Mohamed and Douglas: makrout@cs.toronto.edu, douglas.tweed@utoronto.ca \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/makrout/Deep-Learning-without-Weight-Transport",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-27T20:21:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-15T13:02:45Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9962222833815809
      ],
      "excerpt": "Current algorithms for deep learning probably cannot run in the brain because they rely on weight transport, where forward-path neurons transmit their synaptic weights to a feedback path, in a way that is likely impossible biologically. In this work, we present two new mechanisms which let the feedback path learn appropriate synaptic weights quickly and accurately even in large networks, without weight transport or complex wiring. One mechanism is a neural circuit called a weight mirror, which learns without sensory input, and so could tune feedback paths in the pauses between physical trials of a task, or even in sleep or in utero. The other mechanism is based on a 1994 algorithm of Kolen and Pollack. Their method worked by transporting weight changes, which is no more biological than transporting the weights themselves, but we have shown that a simple circuit lets forward and feedback synapses compute their changes separately, based on local information, and still evolve as in the Kolen-Pollack algorithm. Tested on the ImageNet visual-recognition task, both the weight mirror and the Kolen-Pollack circuit outperform other recent proposals for biologically feasible learning \u2014 feedback alignment and the sign-symmetry method \u2014 and nearly match backprop, the standard algorithm of deep learning, which uses weight transport. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391605228237601
      ],
      "excerpt": "delta angles between Backprop and the algorithms: feedback alignment, weight mirrors and Kolen-Pollack. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391605228237601
      ],
      "excerpt": "Weight angles between Backprop and the algorithms: feedback alignment, weight mirrors and Kolen-Pollack. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9362087150999083
      ],
      "excerpt": "Weight Mirrors (WM): it represents the the second learning mode alternating with the engaged mode during the training. This algorithm suggests that neurons can discharge noisily their signals and adjust the feedback weights so they mimic the forward ones. Here is a pseudo-code of this method: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8806591977424095
      ],
      "excerpt": "    #: generate the noise of the forward neurons \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9495035579767509
      ],
      "excerpt": "    #: update the backward weight matrices using the equation 7 of the paper manuscript \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9736890617421932
      ],
      "excerpt": "- Kolen-Pollack algorithm (KP): it solves the weight transport problem by transporting the changes in weights. At every time step, the forward and backward weights undergo identical adjustments and apply identical weight-decay factors as described in the equations 16 and 17 of the paper manuscript. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Deep Learning without Weight Transport",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/makrout/Deep-Learning-without-Weight-Transport/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Thu, 23 Dec 2021 07:28:52 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/makrout/Deep-Learning-without-Weight-Transport/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "makrout/Deep-Learning-without-Weight-Transport",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/makrout/Deep-Learning-without-Weight-Transport/master/run.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If this is your first time running the code, follow these steps:\n\n1. Run `script/up` to create a virtual environment `.venv` with the required packages\n2. Activate the virtual environment by running `source .venv/bin/activate`\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8274086401038624
      ],
      "excerpt": "| --dataset         |     Dataset's name      |  Choose from {mnist, cifar10} | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8157298581652734,
        0.8638016694668396
      ],
      "excerpt": "| --n_epochs   | Number of epochs to run     | 400 (default)    | \n| --batch_size     | Batch size   | 128 (default)      | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8723881753749236,
        0.8815474807652428
      ],
      "excerpt": "    <img src=\"figures/mnist/delta_angles.png\" alt=\"delta angles on MNIST\" width=\"400\"/> \n    <img src=\"figures/cifar10/delta_angles.png\" alt=\"delta angles on CIFAR10\" width=\"400\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8723881753749236,
        0.8815474807652428
      ],
      "excerpt": "    <img src=\"figures/mnist/weight_angles.png\" alt=\"weight angles on MNIST\" width=\"400\"/> \n    <img src=\"figures/cifar10/weight_angles.png\" alt=\"weight angles on CIFAR10\" width=\"400\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836391184100474
      ],
      "excerpt": "    noise_x = noise_amplitude * (np.random.rand(forward_weight_size, batch_size) - 0.5) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8224121324368083
      ],
      "excerpt": "    noise_y = self.sigmoid(np.matmul(forward_weight, noise_x) + bias) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468
      ],
      "excerpt": "    backward_weight += mirror_learning_rate * np.matmul(noise_x, noise_y.T) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/makrout/Deep-Learning-without-Weight-Transport/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep-Learning-without-Weight-Transport",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep-Learning-without-Weight-Transport",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "makrout",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/makrout/Deep-Learning-without-Weight-Transport/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 30,
      "date": "Thu, 23 Dec 2021 07:28:52 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n#: Run backpropagation (BP)\npython load_and_run.py --dataset=mnist --algo=bp --n_epochs=400 --size_hidden_layers 500 --batch_size=128 --learning_rate=0.2 --test_frequency=1\n\n#: Run feedback alignment (FA)\npython load_and_run.py --dataset=mnist --algo=fa --n_epochs=400 --size_hidden_layers 500 --batch_size=128 --learning_rate=0.2 --test_frequency=1\n\n#: Run weight mirrors (WM)\npython load_and_run.py --dataset=mnist --algo=wm --n_epochs=400 --size_hidden_layers 500 --batch_size=128 --learning_rate=0.05 --test_frequency=1\n\n#: Run the Kolen-Pollack (KP) algorithm\npython load_and_run.py --dataset=mnist --algo=kp --n_epochs=400 --size_hidden_layers 500 --batch_size=128 --learning_rate=0.3 --test_frequency=1\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}