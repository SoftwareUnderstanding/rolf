{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1509.06461, 2015.](https://arxiv.org/pdf/1509.06461.pdf)\n03. [T. Schaul et al., \"Prioritized Experience Replay.\" arXiv preprint https://arxiv.org/abs/1511.05952, 2015.](https://arxiv.org/pdf/1511.05952.pdf)\n04. [Z. Wang et al., \"Dueling Network Architectures for Deep Reinforcement Learning.\" arXiv preprint https://arxiv.org/abs/1511.06581, 2015.](https://arxiv.org/pdf/1511.06581.pdf)\n05. [R. S. Sutton, \"Learning to predict by the methods of temporal differences.\" Machine learning, 3(1):9\u201344, 1988.](http://incompleteideas.net/papers/sutton-88-with-erratum.pdf)\n\n<hr />\n\n## Author\n\n  - Soheil Changizi ( [@cocolico14](https://github.com/cocolico14) )\n\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details\n\n",
      "https://arxiv.org/abs/1511.05952, 2015.](https://arxiv.org/pdf/1511.05952.pdf)\n04. [Z. Wang et al., \"Dueling Network Architectures for Deep Reinforcement Learning.\" arXiv preprint https://arxiv.org/abs/1511.06581, 2015.](https://arxiv.org/pdf/1511.06581.pdf)\n05. [R. S. Sutton, \"Learning to predict by the methods of temporal differences.\" Machine learning, 3(1):9\u201344, 1988.](http://incompleteideas.net/papers/sutton-88-with-erratum.pdf)\n\n<hr />\n\n## Author\n\n  - Soheil Changizi ( [@cocolico14](https://github.com/cocolico14) )\n\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details\n\n",
      "https://arxiv.org/abs/1511.06581, 2015.](https://arxiv.org/pdf/1511.06581.pdf)\n05. [R. S. Sutton, \"Learning to predict by the methods of temporal differences.\" Machine learning, 3(1):9\u201344, 1988.](http://incompleteideas.net/papers/sutton-88-with-erratum.pdf)\n\n<hr />\n\n## Author\n\n  - Soheil Changizi ( [@cocolico14](https://github.com/cocolico14) )\n\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9675901596201627,
        0.9675901596201627
      ],
      "excerpt": "conv2d_2 (Conv2D)               (None, 11, 10, 64)   32832       conv2d_1[0][0]                    \nconv2d_3 (Conv2D)               (None, 11, 10, 64)   36928       conv2d_2[0][0]                    \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999664721034568,
        0.9999698296041944,
        0.9999940083563547
      ],
      "excerpt": "van Hasselt et al., \"Deep Reinforcement Learning with Double Q-learning.\" arXiv preprint arXiv:1509.06461, 2015. \nT. Schaul et al., \"Prioritized Experience Replay.\" arXiv preprint arXiv:1511.05952, 2015. \nZ. Wang et al., \"Dueling Network Architectures for Deep Reinforcement Learning.\" arXiv preprint arXiv:1511.06581, 2015. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cocolico14/N-step-Dueling-DDQN-PER-Pacman",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-18T23:03:42Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-04T10:42:56Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "DeepMind published its famous paper Playing Atari with Deep Reinforcement Learning, in which a new algorithm called DQN was implemented. It showed that an AI agent could learn to play games by simply watching the screen without any prior knowledge about the game. Also, I have added a few enhancement to the vanilla DQN from various papers and tested it on the MsPacman-v4 OpenAI's environment.\n\n\n<img src=\"./Figure_1.png\" align=\"middle\">\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9646519337722552,
        0.8716200745847167,
        0.9148129809673633
      ],
      "excerpt": "Using N-step dueling DDQN with PER for learning how to play a Pacman game \nFor keeping the downsampled image from distortion, I have dilated the pixels with a (3,3) kernel two times. \nDownsample the game image to the size 88x80. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8133281672623176
      ],
      "excerpt": "Instead of stacking four images and feeding them to the network, I'm taking an average of 4 recent images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336005288966698
      ],
      "excerpt": "The replay buffer size is 100000. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577302739126875
      ],
      "excerpt": "dense_2 (Dense)                 (None, 5)            35205       flatten_1[0][0]                   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577302739126875
      ],
      "excerpt": "dense_1 (Dense)                 (None, 1)            7041        flatten_1[0][0]                   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9117493133612748
      ],
      "excerpt": "For updating the Q values in the max operator, DQN uses the same values both to select and to evaluate an action. This makes it more likely to select overestimated values, resulting in overoptimistic value estimates. In order to solve this issue, we can use the target network as a value estimator and the main network as an action selector. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9466704667762463,
        0.9823489642364167
      ],
      "excerpt": "The dueling architecture can learn which states are valuable for each state without learning the effect of each action. This is particularly useful in states where its actions in no relevant way affect the environment. Also, for a more stable optimization, we use an average baseline for Q evaluation. \nLastly, we can prioritize the episode by the magnitude of a transition\u2019s TD error. Moreover, to overcome the issue of replaying a subset of transitions more frequently, we will use a stochastic sampling method that interpolates between pure greedy prioritization and uniform random sampling. I used a min-heap and chose about 60% by the TD error priority and 40% uniformly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9441137020680721
      ],
      "excerpt": "R. S. Sutton, \"Learning to predict by the methods of temporal differences.\" Machine learning, 3(1):9\u201344, 1988. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Using N-step dueling DDQN with PER for playing Pacman game",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cocolico14/N-step-Dueling-DDQN-PER-Pacman/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 13:01:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cocolico14/N-step-Dueling-DDQN-PER-Pacman/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "cocolico14/N-step-Dueling-DDQN-PER-Pacman",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.856989540636594
      ],
      "excerpt": "Layer (type)                    Output Shape         Param #:     Connected to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934745846565956
      ],
      "excerpt": "Total params: 114,086 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cocolico14/N-step-Dueling-DDQN-PER-Pacman/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Soheil Changizi\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# N-step-Dueling-DDQN-PER-Pacman\n> Using N-step dueling DDQN with PER for learning how to play a Pacman game\n\n## Summary\n\nDeepMind published its famous paper Playing Atari with Deep Reinforcement Learning, in which a new algorithm called DQN was implemented. It showed that an AI agent could learn to play games by simply watching the screen without any prior knowledge about the game. Also, I have added a few enhancement to the vanilla DQN from various papers and tested it on the MsPacman-v4 OpenAI's environment.\n\n\n<img src=\"./Figure_1.png\" align=\"middle\">\n\n## Demo\n\n<img src=\"./overview.gif\" width=\"256\" align=\"middle\">\n\n<hr />\n\n## Preprocessing\n\n  - For keeping the downsampled image from distortion, I have dilated the pixels with a (3,3) kernel two times.\n  - Downsample the game image to the size 88x80.\n  - Change the color of Pacman's pixels for precise observation.\n\n## DQN\n\n  - Instead of stacking four images and feeding them to the network, I'm taking an average of 4 recent images.\n  - The main network is being updated each 1000 steps.\n  - The replay buffer size is 100000.\n  - The network architecture is:\n  ```\n  __________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "N-step-Dueling-DDQN-PER-Pacman",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "cocolico14",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cocolico14/N-step-Dueling-DDQN-PER-Pacman/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10,
      "date": "Wed, 29 Dec 2021 13:01:06 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "dqn",
      "ddqn",
      "prioritized-experience-replay",
      "n-step",
      "dueling-dqn",
      "keras-tensorflow",
      "pacman-game"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src=\"./overview.gif\" width=\"256\" align=\"middle\">\n\n<hr />\n\n",
      "technique": "Header extraction"
    }
  ]
}