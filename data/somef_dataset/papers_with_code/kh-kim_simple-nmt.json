{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1512.02433",
      "https://arxiv.org/abs/1707.00415",
      "https://arxiv.org/abs/1512.02433",
      "https://arxiv.org/abs/1609.08144",
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1707.00415"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [[Luong et al., 2015](http://aclweb.org/anthology/D15-1166)] Effective Approaches to Attention-based Neural Machine Translation\n- [[Shen et al., 2015](https://arxiv.org/abs/1512.02433)] Minimum Risk Training for Neural Machine Translation\n- [[Sennrich et al., 2016](http://www.aclweb.org/anthology/P16-1162)] Neural Machine Translation of Rare Words with Subword Units\n- [[Wu et al, 2016](https://arxiv.org/abs/1609.08144)] Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\n- [[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)] Attention is All You Need\n- [[Xia et al., 2017](https://arxiv.org/abs/1707.00415)] Dual Supervised Learning\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8206588280033469
      ],
      "excerpt": "Please, refer those sites for further information. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "\\end{gathered}$$ --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "\\end{gathered}$$ --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "\\end{gathered}$$ --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "|Transformer (DSL)|35.48|32.80| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8631132759125981
      ],
      "excerpt": "                        Learning rate decay start at. Default=10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "                        Default=10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "--gpu_id 0 --batch_size 128 --n_epochs 30 --max_length 100 --dropout .2 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "--gpu_id 0 --batch_size 128 --n_epochs 30 --max_length 100 --dropout .2 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "--dsl_n_warmup_epochs 30 --dsl_lambda 1e-2 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369514598000786
      ],
      "excerpt": "  --lang LANG           Source language and target language. Example: enko \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kh-kim/simple-nmt",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-01T03:26:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T13:26:18Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9094886103489276
      ],
      "excerpt": "This repo contains a simple source code for advanced neural machine translation based on Sequence-to-Sequence and Transformer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9538659698968525
      ],
      "excerpt": "Also, this repo is for lecture and book, what I conduct. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9752858185576989
      ],
      "excerpt": "This repo provides many features, and many of those codes were written from scratch. (e.g. Transformer and Beam search) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8001859980895589
      ],
      "excerpt": "Reinforcement learning for fine-tuning like Minimum Risk Training (MRT) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8128928735270164
      ],
      "excerpt": "Beam search with mini-batch in parallel \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8488033058825993
      ],
      "excerpt": "Note that it was unable to run MRT on Transformer, due to lack of memory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8336613151495407
      ],
      "excerpt": "Table shows that beam search improve BLEU score without data adding and model change. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8526608138026486
      ],
      "excerpt": "                        Hidden size of LSTM. Default=768 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8942910935221287
      ],
      "excerpt": "                        Number of epochs for reinforcement learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9600168690874294
      ],
      "excerpt": "                        Maximum number of tokens to calculate BLEU for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This repo contains a simple source code for advanced neural machine translation based on sequence-to-sequence.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kh-kim/simple-nmt/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 50,
      "date": "Wed, 29 Dec 2021 16:28:28 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kh-kim/simple-nmt/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kh-kim/simple-nmt",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/kh-kim/simple-nmt/master/test/run_enko.sh",
      "https://raw.githubusercontent.com/kh-kim/simple-nmt/master/test/run_koen.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In order to evaluate this project, I used public dataset from [AI-HUB](https://aihub.or.kr/), which provides 1,600,000 pairs of sentence.\nI randomly split this data into train/valid/test set by following number of lines each.\nIn fact, original test set, which has about 200000 lines, is too big to take bunch of evaluations, I reduced it to 1,000 lines.\n(In other words, you can get better model, if you put removed 199,000 lines into training set.)\n\n|set|lang|#lines|#tokens|#characters|\n|-|-|-|-|-|\n|train|en|1,200,000|43,700,390|367,477,362|\n||ko|1,200,000|39,066,127|344,881,403|\n|valid|en|200,000|7,286,230|61,262,147|\n||ko|200,000|6,516,442|57,518,240|\n|valid-1000|en|1,000|36,307|305,369|\n||ko|1,000|32,282|285,911|\n|test-1000|en|1,000|35,686|298,993|\n||ko|1,000|31,720|280,126|\n\nEach dataset is tokenized with Mecab/MosesTokenizer and BPE.\nAfter preprocessing, each language has vocabulary size like as below:\n\n|en|ko|\n|-|-|\n|20,525|29,411|\n\nAlso, we have following hyper-parameters for each model to proceed a evaluation.\nNote that both architectures have small number of parameters, because I don't have enough corpus.\nYou need to increase the number of parameters, if you have more corpus.\n\n|parameter|seq2seq|transformer|\n|-|-|-|\n|batch_size|320|4096|\n|word_vec_size|512| - |\n|hidden_size|768|768|\n|n_layers|4|4|\n|n_splits| - |8|\n|n_epochs|30|30|\n\nBelow is a table for hyper-parameters for each algorithm.\n\n|parameter|MLE|MRT|DSL|\n|-|-|-|-|\n|n_epochs|30|30 + 40|30 + 10|\n|optimizer|Adam|SGD|Adam|\n|lr|1e-3|1e-2|1e-2|\n|max_grad_norm|1e+8|5|1e+8 $\\rightarrow$ 5|\n\nPlease, note that MRT has different optimization setup.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8245311590490573
      ],
      "excerpt": "I believe that this repo has minimal features to build NMT system. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8445664242666031
      ],
      "excerpt": "  --gpu_id GPU_ID       GPU ID to train. Currently, GPU parallel is not \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918763032919761
      ],
      "excerpt": "  --gpu_id GPU_ID       GPU ID to use. -1 for CPU. Default=-1 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9503189345333785,
        0.8825828764272149
      ],
      "excerpt": "python train.py -h \nusage: train.py [-h] --model_fn MODEL_FN --train TRAIN --valid VALID --lang \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8717884560234187,
        0.9160407707045309,
        0.8589534893990137
      ],
      "excerpt": "                        be annotated to the file name. \n  --train TRAIN         Training set file name except the extention. (ex: \n                        train.en --> train) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8324273402132306
      ],
      "excerpt": "  --n_epochs N_EPOCHS   Number of epochs to train. Default=20 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8259557163625612
      ],
      "excerpt": "                        continue training. Default=1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184891977590839
      ],
      "excerpt": "                        Maximum length of the training sequence. Default=100 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925619830435868
      ],
      "excerpt": "example usage: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8813707756549166,
        0.8758812316357435
      ],
      "excerpt": "python train.py --train ./data/corpus.shuf.train.tok.bpe --valid ./data/corpus.shuf.valid.tok.bpe --lang enko \\ \n--gpu_id 0 --batch_size 128 --n_epochs 30 --max_length 100 --dropout .2 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8318549392002752
      ],
      "excerpt": "--model_fn ./model.pth \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.903739844475289
      ],
      "excerpt": "python continue_train.py --load_fn ./model.pth --model_fn ./model.rl.pth \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8813707756549166,
        0.8758812316357435
      ],
      "excerpt": "python train.py --train ./data/corpus.shuf.train.tok.bpe --valid ./data/corpus.shuf.valid.tok.bpe --lang enko \\ \n--gpu_id 0 --batch_size 128 --n_epochs 30 --max_length 100 --dropout .2 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8318549392002752
      ],
      "excerpt": "--model_fn ./model.pth \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "LM Training: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8663485624778443,
        0.8056244852123754
      ],
      "excerpt": "python lm_train.py --train ./data/corpus.shuf.train.tok.bpe --valid ./data/corpus.shuf.valid.tok.bpe --lang enko \\ \n--gpu_id 0 --batch_size 256 --n_epochs 20 --max_length 64 --dropout .2 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8663485624778443
      ],
      "excerpt": "python dual_train.py --train ./data/corpus.shuf.train.tok.bpe --valid ./data/corpus.shuf.valid.tok.bpe --lang enko \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8318549392002752
      ],
      "excerpt": "--model_fn ./model.pth \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8510820389754454
      ],
      "excerpt": "usage: translate.py [-h] --model_fn MODEL_FN [--gpu_id GPU_ID] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860588576311044
      ],
      "excerpt": "  --model_fn MODEL_FN   Model file name to use \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.825618086948489
      ],
      "excerpt": "  --n_best N_BEST       Number of best inference result per sample. Default=1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925619830435868
      ],
      "excerpt": "example usage: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241089777282525
      ],
      "excerpt": "python translate.py --model_fn ./model.pth --gpu_id 0 --lang enko < test.txt > test.result.txt \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kh-kim/simple-nmt/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Perl",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Simple Neural Machine Translation (Simple-NMT)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "simple-nmt",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kh-kim",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kh-kim/simple-nmt/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3.6 or higher\n- PyTorch 1.6 or higher\n- TorchText 0.5 or higher\n- PyTorch Ignite\n- [torch-optimizer 0.0.1a15](https://pypi.org/project/torch-optimizer/)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 103,
      "date": "Wed, 29 Dec 2021 16:28:28 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I recommend to use corpora from [AI-Hub](http://www.aihub.or.kr/), if you are trying to build Kor/Eng machine translation.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Below table shows that result from both MLE and MRT in Korean-English translation task.\n\n|INPUT|REF|MLE|MRT|\n|-|-|-|-|\n|\uc6b0\ub9ac\ub294 \ub610\ud55c \uadf8 \uc9c0\uc5ed\uc758 \uc0dd\uc120 \uac00\uacf5 \uacf5\uc7a5\uc5d0\uc11c \uc2ec\ud55c \uc545\ucde8\ub97c \ub0b4\uba70 \uc369\uc5b4\uac00\ub294 \uc5c4\uccad\ub09c \uc591\uc758 \uc0dd\uc120\ub3c4 \uce58\uc6e0\uc2b5\ub2c8\ub2e4.|We cleared tons and tons of stinking, rotting fish carcasses from the local fish processing plant.|We also had a huge stink in the fish processing plant in the area, smelling havoc with a huge amount of fish.|We also cleared a huge amount of fish that rot and rot in the fish processing factory in the area.|\n|\ud68c\uc0ac\ub97c \uc774\uc804\ud560 \uc774\uc0c1\uc801\uc778 \uc7a5\uc18c\uc774\ub2e4.|It is an ideal place to relocate the company.|It's an ideal place to transfer the company.|It's an ideal place to transfer the company.|\n|\ub098\ub294 \uc774\uac83\ub4e4\uc774 \ub0b4 \uc0b6\uc744 \ubc14\uafb8\uac8c \ud558\uc9c0 \uc54a\uaca0\uc5b4.|I won't let this thing alter my life.|I'm not gonna let these things change my life.|I won't let these things change my life.|\n|\uc0ac\ub78c\ub4e4\uc774 \uc2ac\ud37c\ubcf4\uc778\ub2e4.|Their faces appear tearful.|People seem to be sad.|People seem to be sad.|\n|\uc544\ub0d0, \uadf8\ub7f0\ub370 \ub10c \uadf8\ub807\ub2e4\uace0 \uc0dd\uac01\ud574.|No, but I think you do.|No, but I think you do.|No, but you think it's.|\n|\ud558\uc9c0\ub9cc, \ub098\ub294 \ub098\uc911\uc5d0 \uace7 \uc7a0\ub4e4\uc5c8\ub2e4.|But I fell asleep shortly afterwards.|However, I fell asleep in a moment.|However, I fell asleep soon afterwards.|\n|\ud558\uc9c0\ub9cc 1997\ub144 \uc544\uc2dc\uc544\uc5d0 \uc678\ud658\uc704\uae30\uac00 \ubd88\uc5b4\ub2e5\ucce4\ub2e4.|But Asia was hit hard by the 1997 foreign currency crisis.|In 1997, however, the financial crisis in Asia has become a reality for Asia.|But in 1997, the foreign currency crisis was swept in Asia.|\n|\uba54\uc774\uc800 \ub9ac\uadf8 \uacf5\uc2dd \uc6f9\uc0ac\uc774\ud2b8\uc5d0 \ub530\ub974\uba74, 12\uc6d4 22\uc77c, \ucd94\uc528\ub294 \ud14d\uc0ac\uc2a4 \ub808\uc778\uc838\uc2a4\uc640 7\ub144 \uacc4\uc57d\uc744 \ub9fa\uc5c8\ub2e4.|According to Major League Baseball's official website, on Dec. 22, Choo signed a seven year contract with the Texas Rangers.|According to the Major League official website on December 22, Choo signed a seven-year contract with Texas Rangers in Texas|According to the Major League official website on December 22, Choo made a seven-year contract with Texas Rangers.|\n|\ud55c \uac1c\uc778.|a private individual|a person of personal importance|a personal individual|\n|\ub3c4\ub85c\uc5d0 \ucc28\uac00 \uaf2c\ub9ac\ub97c \ubb3c\uace0 \ub298\uc5b4\uc11c\uc788\ub2e4.|The traffic is bumper to bumper on the road.|The road is on the road with a tail.|The road is lined with tail on the road.|\n|\ub0b4\uac00 \uadf8\ub807\uac8c \ub299\uc9c0 \uc54a\uc558\ub2e4\ub294 \uc810\uc744 \uc9c0\uc801\ud574\ub3c4 \ub420\uae4c\uc694.|Let me point out that I'm not that old.|You can point out that I'm not that old.|You can point out that I'm not that old.|\n|\ub2d0\uc2a8 \uc2dc\uccad\ub960\uc740 15\ubd84 \ub2e8\uc704 \uc99d\uac10\uc73c\ub85c \uc2dc\uccad\ub960\uc744 \uce21\uc815\ud558\ubbc0\ub85c, ABC, NBC, CBS \uc640 Fox \uc758 \uc21c\uc704\ub97c \uc815\ud558\uc9c0 \uc54a\uc558\ub2e4.|Nielsen had no ratings for ABC, NBC, CBS and Fox because it measures their viewership in 15-minute increments.|The Nielsen ratings measured the viewer's ratings with increments for 15-minute increments, so they did not rank ABC, NBC, CBS and Fox.|Nielson ratings measured ratings with 15-minute increments, so they did not rank ABC, NBC, CBS and Fox.|\n|\ub2e4\uc2dc\ub9d0\ud574\uc11c, \ud559\uad50\ub294 \uad50\uc0ac \ubd80\uc871\uc774\ub2e4.|In other words, the school is a teacher short.|In other words, school is a teacher short of a teacher.|In other words, school is a lack of teacher.|\n|\uadf8 \ub2e4\uc74c \uba87 \uc8fc \ub3d9\uc548\uc5d0 \uc0ac\ud0dc\uac00 \uadf9\uc801\uc73c\ub85c \uc804\ud658\ub418\uc5c8\ub2e4.|Events took a dramatic turn in the weeks that followed.|The situation has been dramatically changed for the next few weeks.|The situation was dramatically reversed for the next few weeks.|\n|\uc80a\uc740\uc774\ub4e4\uc744 \ubb3c\ub9ac\ud559\uc5d0 \ub300\ud574 \ud765\ubbf8\ub97c \ubd99\uc77c\uc218 \uc788\uac8c \ud560\uc218 \uc788\ub294 \uac00\uc7a5 \uc88b\uc740 \uc0ac\ub78c\uc740 \uc878\uc5c5\uc0dd \ubb3c\ub9ac\ud559\uc790\uc774\ub2e4.|The best possible person to excite young people about physics is a graduate physicist.|The best person to be able to make young people interested in physics is a self-thomac physicist.|The best person to make young people interested in physics is a graduate physicist.|\n|5\uc6d4 20\uc77c, \uc778\ub3c4\ub294 \ud314\ub85c\ub514 \ub9c8\uc744\uc5d0\uc11c \ucda9\uaca9\uc801\uc778 \uae30\uc628\uc778 \uc12d\uc528 51\ub3c4\ub97c \ub2ec\uc131\ud558\uba70, \uac00\uc7a5 \ub354\uc6b4 \ub0a0\uc528\ub97c \uae30\ub85d\ud588\uc2b5\ub2c8\ub2e4.|On May 20, India recorded its hottest day ever in the town of Phalodi with a staggering temperature of 51 degrees Celsius.|On May 20, India achieved its hottest temperatures, even 51 degrees Celsius, in the Palrody village, and recorded the hottest weather.|On May 20, India achieved 51 degrees Celsius, a devastating temperature in Paldydy town, and recorded the hottest weather.|\n|\ub0b4\ub9d0\uc740, \uac00\ub054 \ubc14\ub098\ub294 \uadf8\ub0e5 \ubc14\ub098\ub098\uc57c.|I mean, sometimes a banana is just a banana.|I mean, sometimes a banana is just a banana.|I mean, sometimes a banana is just a banana.|\n\n",
      "technique": "Header extraction"
    }
  ]
}