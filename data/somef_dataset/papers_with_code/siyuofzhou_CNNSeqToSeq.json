{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1705.03122",
      "https://arxiv.org/abs/1611.02344",
      "https://arxiv.org/abs/1508.07909"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use the code in your paper, then please cite it as:\n\n```\n@article{gehring2017convs2s,\n  author          = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},\n  title           = \"{Convolutional Sequence to Sequence Learning}\",\n  journal         = {ArXiv e-prints},\n  archivePrefix   = \"arXiv\",\n  eprinttype      = {arxiv},\n  eprint          = {1705.03122},\n  primaryClass    = \"cs.CL\",\n  keywords        = {Computer Science - Computation and Language},\n  year            = 2017,\n  month           = May,\n}\n```\n\nand\n\n```\n@article{gehring2016convenc,\n  author          = {Gehring, Jonas and Auli, Michael and Grangier, David and Dauphin, Yann N},\n  title           = \"{A Convolutional Encoder Model for Neural Machine Translation}\",\n  journal         = {ArXiv e-prints},\n  archivePrefix   = \"arXiv\",\n  eprinttype      = {arxiv},\n  eprint          = {1611.02344},\n  primaryClass    = \"cs.CL\",\n  keywords        = {Computer Science - Computation and Language},\n  year            = 2016,\n  month           = Nov,\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{gehring2016convenc,\n  author          = {Gehring, Jonas and Auli, Michael and Grangier, David and Dauphin, Yann N},\n  title           = \"{A Convolutional Encoder Model for Neural Machine Translation}\",\n  journal         = {ArXiv e-prints},\n  archivePrefix   = \"arXiv\",\n  eprinttype      = {arxiv},\n  eprint          = {1611.02344},\n  primaryClass    = \"cs.CL\",\n  keywords        = {Computer Science - Computation and Language},\n  year            = 2016,\n  month           = Nov,\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{gehring2017convs2s,\n  author          = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},\n  title           = \"{Convolutional Sequence to Sequence Learning}\",\n  journal         = {ArXiv e-prints},\n  archivePrefix   = \"arXiv\",\n  eprinttype      = {arxiv},\n  eprint          = {1705.03122},\n  primaryClass    = \"cs.CL\",\n  keywords        = {Computer Science - Computation and Language},\n  year            = 2017,\n  month           = May,\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9865374311292784,
        0.9278824608274014
      ],
      "excerpt": "H   -0.068684287369251  Pourquoi est-il rare de d\u00e9couvrir de nouvelles esp\u00e8ces de mammif\u00e8res marins ? \nA   1 1 4 4 6 6 7 11 9 9 9 12 13 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "$ fairseq preprocess -sourcelang de -targetlang en \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| BLEU4 = 43.43, 68.2/49.2/37.4/28.8 (BP=0.996, ratio=1.004, sys_len=92087, ref_len=92448) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/siyuofzhou/CNNSeqToSeq/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/siyuofzhou/CNNSeqToSeq",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to fairseq (Lua)\nWe want to make contributing to this project as easy and transparent as\npossible.\nPull Requests\nFirst, note that there is now a PyTorch version\nfairseq-py of this toolkit and\nnew development efforts will focus on it. That being said, we actively welcome\nyour pull requests:\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nCoding Style\n\n4 spaces for indentation rather than tabs\n80 character line length\nCamelCase; capitalized class names and lower-case function names\n\nLicense\nBy contributing to fairseq, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-19T07:59:17Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-23T13:12:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is fairseq, a sequence-to-sequence learning toolkit for [Torch](http://torch.ch/) from Facebook AI Research tailored to Neural Machine Translation (NMT).\nIt implements the convolutional NMT models proposed in [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) and [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344) as well as a standard LSTM-based model.\nIt features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU.\nWe provide pre-trained models for English to French, English to German and English to Romanian translation.\n\nNote, there is now a PyTorch version [fairseq-py](https://github.com/facebookresearch/fairseq-py) of this toolkit and new development efforts will focus on it.\n\n![Model](fairseq.gif)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.826338129739459
      ],
      "excerpt": "This will unpack vocabulary files and a serialized model for English to French translation to wmt14.en-fr.fconv-cuda/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9352025323402063
      ],
      "excerpt": "This model uses a Byte Pair Encoding (BPE) vocabulary, so we'll have to apply the encoding to the source text. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.95216182185485,
        0.880522896826705
      ],
      "excerpt": "@@ is used as a continuation marker and the original text can be easily recovered with e.g. sed s/@@ //g. \nPrior to BPE, input text needs to be tokenized using tokenizer.perl from mosesdecoder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9608370950550695,
        0.9608370950550695,
        0.9608370950550695
      ],
      "excerpt": "Why is it rare to discover new marine mam@@ mal species ? \nS   Why is it rare to discover new marine mam@@ mal species ? \nO   Why is it rare to discover new marine mam@@ mal species ? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9588298614533357,
        0.8959166619866832
      ],
      "excerpt": "This generation script produces four types of output: a line prefixed with S shows the supplied source sentence after applying the vocabulary; O is a copy of the original source sentence; H is the hypothesis along with an average log-likelihood and A are attention maxima for each word in the hypothesis (including the end-of-sentence marker which is omitted from the text). \nCheck below for a full list of pre-trained models available. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047,
        0.8273969389908572
      ],
      "excerpt": "the IWSLT14 German-English corpus. \nPre-process and binarize the data as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "  -model conv -nenclayer 6 -dropout 0.2 -dropout_hid 0 -savedir trainings/convenc \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9314247363883542,
        0.8280551827442427
      ],
      "excerpt": "Once your model is trained, you can translate with it using fairseq generate (for binarized data) or fairseq generate-lines (for text). \nHere, we'll do it for a fully convolutional model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9806517750592484
      ],
      "excerpt": "H   -0.23804219067097   a language is expression of human mind . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9863259525149165
      ],
      "excerpt": "H   -0.23861141502857   a language is expression of the human mind . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": ": Convert to float \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9806517750592484
      ],
      "excerpt": "H   -0.2380430996418    a language is expression of human mind . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9863259525149165
      ],
      "excerpt": "H   -0.23861189186573   a language is expression of the human mind . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8058788892275457
      ],
      "excerpt": "  This model was trained on the original WMT bitext as well as back-translated data provided by Rico Sennrich. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8228422372620685
      ],
      "excerpt": "Google group: https://groups.google.com/forum/#!forum/fairseq-users \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/siyuofzhou/CNNSeqToSeq/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 22 Dec 2021 23:31:41 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/siyuofzhou/CNNSeqToSeq/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "siyuofzhou/CNNSeqToSeq",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/siyuofzhou/CNNSeqToSeq/master/data/prepare-iwslt14.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* A computer running macOS or Linux\n* For training new models, you'll also need a NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl)\n* A [Torch installation](http://torch.ch/docs/getting-started.html). For maximum speed, we recommend using LuaJIT and [Intel MKL](https://software.intel.com/en-us/intel-mkl).\n* A recent version [nn](https://github.com/torch/nn). The minimum required version is from May 5th, 2017. A simple `luarocks install nn` is sufficient to update your locally installed version.\n\nInstall fairseq by cloning the GitHub repository and running\n```\nluarocks make rocks/fairseq-scm-1.rockspec\n```\nLuaRocks will fetch and build any additional dependencies that may be missing.\nIn order to install the CPU-only version (which is only useful for translating new data with an existing model), do\n```\nluarocks make rocks/fairseq-cpu-scm-1.rockspec\n```\n\nThe LuaRocks installation provides a command-line tool that includes the following functionality:\n* `fairseq preprocess`: Data pre-processing: build vocabularies and binarize training data\n* `fairseq train`: Train a new model on one or multiple GPUs\n* `fairseq generate`: Translate pre-processed data with a trained model\n* `fairseq generate-lines`: Translate raw text with a trained model\n* `fairseq score`: BLEU scoring of generated translations against reference translations\n* `fairseq tofloat`: Convert a trained model to a CPU model\n* `fairseq optimize-fconv`: Optimize a fully convolutional model for generation. This can also be achieved by passing the `-fconvfast` flag to the generation scripts.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9454746135759499
      ],
      "excerpt": "$ curl https://s3.amazonaws.com/fairseq/models/wmt14.en-fr.fconv-cuda.tar.bz2 | tar xvjf - \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8645139392565148
      ],
      "excerpt": "$ curl https://s3.amazonaws.com/fairseq/models/wmt14.en-fr.fconv-float.tar.bz2 | tar xvjf - \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8203019823690298
      ],
      "excerpt": "A   1 1 4 4 6 6 7 11 9 9 9 12 13 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8080002376980483,
        0.922398499773593,
        0.9906248903846466
      ],
      "excerpt": "$ cd data/ \n$ bash prepare-iwslt14.sh \n$ cd .. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902627162932362
      ],
      "excerpt": "$ mkdir -p trainings/blstm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902627162932362
      ],
      "excerpt": "$ mkdir -p trainings/fconv \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902627162932362
      ],
      "excerpt": "$ mkdir -p trainings/convenc \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8479585501696961
      ],
      "excerpt": "Use the CUDA_VISIBLE_DEVICES environment variable to select specific GPUs or -ngpus to change the number of GPU devices that will be used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8941083980170715
      ],
      "excerpt": "wmt14.en-fr.fconv-float.tar.bz2: CPU version of the above \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382240013774961
      ],
      "excerpt": "wmt14.en-de.fconv-float.tar.bz2: CPU version of the above \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8941083980170715
      ],
      "excerpt": "wmt16.en-ro.fconv-float.tar.bz2: CPU version of the above \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8218273080005066
      ],
      "excerpt": "| Timings: setup 0.1s (0.1%), encoder 1.9s (1.4%), decoder 108.9s (79.9%), search_results 0.0s (0.0%), search_prune 12.5s (9.2%) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9150198703535596
      ],
      "excerpt": "First, download a pre-trained model along with its vocabularies: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.899798830109176
      ],
      "excerpt": "  -trainpref $TEXT/train -validpref $TEXT/valid -testpref $TEXT/test \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8011103870803743
      ],
      "excerpt": "Use fairseq train to train a new model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8022661796773425
      ],
      "excerpt": "  -model fconv -nenclayer 4 -nlayer 3 -dropout 0.2 -optim nag -lr 0.25 -clip 0.1 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8562539004375309
      ],
      "excerpt": "$ fairseq generate -sourcelang en -targetlang fr -datadir data-bin/wmt14.en-fr -dataset newstest2014 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/siyuofzhou/CNNSeqToSeq/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Lua",
      "C++",
      "Python",
      "Shell",
      "CMake"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/siyuofzhou/CNNSeqToSeq/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD License\\n\\nFor fairseq software\\n\\nCopyright (c) 2017-present, Facebook, Inc. All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification,\\nare permitted provided that the following conditions are met:\\n\\n * Redistributions of source code must retain the above copyright notice, this\\n    list of conditions and the following disclaimer.\\n\\n * Redistributions in binary form must reproduce the above copyright notice,\\n    this list of conditions and the following disclaimer in the documentation\\n       and/or other materials provided with the distribution.\\n\\n * Neither the name Facebook nor the names of its contributors may be used to\\n    endorse or promote products derived from this software without specific\\n       prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CNNSeqToSeq",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "siyuofzhou",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/siyuofzhou/CNNSeqToSeq/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* A computer running macOS or Linux\n* For training new models, you'll also need a NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl)\n* A [Torch installation](http://torch.ch/docs/getting-started.html). For maximum speed, we recommend using LuaJIT and [Intel MKL](https://software.intel.com/en-us/intel-mkl).\n* A recent version [nn](https://github.com/torch/nn). The minimum required version is from May 5th, 2017. A simple `luarocks install nn` is sufficient to update your locally installed version.\n\nInstall fairseq by cloning the GitHub repository and running\n```\nluarocks make rocks/fairseq-scm-1.rockspec\n```\nLuaRocks will fetch and build any additional dependencies that may be missing.\nIn order to install the CPU-only version (which is only useful for translating new data with an existing model), do\n```\nluarocks make rocks/fairseq-cpu-scm-1.rockspec\n```\n\nThe LuaRocks installation provides a command-line tool that includes the following functionality:\n* `fairseq preprocess`: Data pre-processing: build vocabularies and binarize training data\n* `fairseq train`: Train a new model on one or multiple GPUs\n* `fairseq generate`: Translate pre-processed data with a trained model\n* `fairseq generate-lines`: Translate raw text with a trained model\n* `fairseq score`: BLEU scoring of generated translations against reference translations\n* `fairseq tofloat`: Convert a trained model to a CPU model\n* `fairseq optimize-fconv`: Optimize a fully convolutional model for generation. This can also be achieved by passing the `-fconvfast` flag to the generation scripts.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 23:31:41 GMT"
    },
    "technique": "GitHub API"
  }
}