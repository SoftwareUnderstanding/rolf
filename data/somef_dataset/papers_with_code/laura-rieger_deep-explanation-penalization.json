{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1909.13584\">pdf</a>",
      "https://arxiv.org/abs/1901.04592",
      "https://arxiv.org/abs/2003.01926",
      "https://arxiv.org/abs/1905.07631"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- feel free to use/share this code openly\n- if you find this code useful for your research, please cite the following:\n\n```r\n@inproceedings{rieger2020interpretations,\n  title={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\n  author={Rieger, Laura and Singh, Chandan and Murdoch, William and Yu, Bin},\n  booktitle={International Conference on Machine Learning},\n  pages={8116--8126},\n  year={2020},\n  organization={PMLR}\n}\n```\n\n  \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{rieger2020interpretations,\n  title={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\n  author={Rieger, Laura and Singh, Chandan and Murdoch, William and Yu, Bin},\n  booktitle={International Conference on Machine Learning},\n  pages={8116--8126},\n  year={2020},\n  organization={PMLR}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9460073726822703
      ],
      "excerpt": "ACD (ICLR 2019 pdf, github) - extends CD to CNNs / arbitrary DNNs, and aggregates explanations into a hierarchy \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/laura-rieger/deep-explanation-penalization",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-12T21:26:08Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-12T08:21:50Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8838099479002469
      ],
      "excerpt": "  - the model must be replaced with your model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8458586764898561,
        0.9747365561097491
      ],
      "excerpt": "PDR framework (PNAS 2019 pdf) - an overarching framewwork for guiding and framing interpretable machine learning \nTRIM (ICLR 2020 workshop pdf, github) - using simple reparameterizations, allows for calculating disentangled importances to transformations of the input (e.g. assigning importances to different frequencies) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for using CDEP from the paper \"Interpretations are useful: penalizing explanations to align neural networks with prior knowledge\" https://arxiv.org/abs/1909.13584",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- fully-contained data/models/code for reproducing and experimenting with CDEP\n- the [src](src) folder contains the core code for running and penalizing contextual decomposition\n- in addition, we run experiments on 4 datasets, each of which are located in their own folders\n  - notebooks in these folders show demos for different kinds of text\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/laura-rieger/deep-explanation-penalization/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Sat, 25 Dec 2021 00:48:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/laura-rieger/deep-explanation-penalization/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "laura-rieger/deep-explanation-penalization",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/laura-rieger/deep-explanation-penalization/master/compas/CDEP%20on%20COMPAS.ipynb",
      "https://raw.githubusercontent.com/laura-rieger/deep-explanation-penalization/master/text/analyze_sst.ipynb",
      "https://raw.githubusercontent.com/laura-rieger/deep-explanation-penalization/master/mnist/analyze_mnist.ipynb",
      "https://raw.githubusercontent.com/laura-rieger/deep-explanation-penalization/master/mnist/analyze_colormnist.ipynb",
      "https://raw.githubusercontent.com/laura-rieger/deep-explanation-penalization/master/isic-skin-cancer/analyze_isic.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8941754951168903
      ],
      "excerpt": "1. run CD/ACD on your model. Specifically, 3 things must be altered: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8713119658514014
      ],
      "excerpt": "  - the pred_ims function must be replaced by a function you write using your own trained model. This function gets predictions from a model given a batch of examples. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/laura-rieger/deep-explanation-penalization/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Laura Rieger\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "documentation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "deep-explanation-penalization",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "laura-rieger",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/laura-rieger/deep-explanation-penalization/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 94,
      "date": "Sat, 25 Dec 2021 00:48:31 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "interpretability",
      "neural-network",
      "machine-learning",
      "convolutional-neural-network",
      "pytorch",
      "explainability",
      "deep-learning",
      "explainable-ai",
      "artificial-intelligence",
      "ml",
      "ai",
      "python",
      "data-science",
      "jupyter-notebook",
      "feature-importance",
      "recurrent-neural-network",
      "fairness",
      "fairness-ml",
      "cdep",
      "interpretable-deep-learning"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[ISIC skin-cancer classification](isic-skin-cancer) - using CDEP, we can learn to avoid spurious patches present in the training set, improving test performance!\n\n<p align=\"center\">\n  <img width=\"60%\" src=\"isic-skin-cancer/results/gradCAM.png\"></img>\n</p>\n\nThe segmentation maps of the patches can be downloaded [here](https://drive.google.com/drive/folders/1Er2PQMwmDSmg3BThyeu-JKX442OkQJit?usp=sharing)\n\n[ColorMNIST](mnist) - penalizing the contributions of individual pixels allows us to teach a network to learn a digit's shape instead of its color, improving its test accuracy from 0.5% to 25.1%\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"mnist/results/ColorMNIST_examples.png\"></img>\n</p>\n\n[Fixing text gender biases](text) - CDEP can help to learn spurious biases in a dataset, such as gendered words\n\n<p align=\"center\">\n  <img width=\"50%\" src=\"text/results/data_example.png\"></img>\n</p>\n\n",
      "technique": "Header extraction"
    }
  ]
}