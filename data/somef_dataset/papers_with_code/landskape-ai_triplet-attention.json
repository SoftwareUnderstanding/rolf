{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2010.03045\" alt=\"ArXiv\">\r\n        <img src=\"https://img.shields.io/badge/Preprint-arXiv-blue.svg\" /></a>\r\n  <a href=\"https://openaccess.thecvf.com/content/WACV2021/papers/Misra_Rotate_to_Attend_Convolutional_Triplet_Attention_Module_WACV_2021_paper.pdf\" alt=\"PDF\">\r\n          <img src=\"https://img.shields.io/badge/WACV-PDF-neon.svg\" /></a>\r\n  <a href=\"https://openaccess.thecvf.com/content/WACV2021/supplemental/Misra_Rotate_to_Attend_WACV_2021_supplemental.pdf\" alt=\"Supp\">\r\n          <img src=\"https://img.shields.io/badge/WACV-Supp-pink.svg\" /></a>\r\n  <a href=\"https://landskapeai.github.io/publication/triplet/\" alt=\"Project\">\r\n          <img src=\"https://img.shields.io/badge/Project-Website-green.svg\" /></a>\r\n  <a href=\"https://landskapeai.github.io/slides/triplet/#/\" alt=\"Slides\">\r\n          <img src=\"https://img.shields.io/badge/WACV-Slides-yellow.svg\" /></a>\r\n  <a href=\"https://youtu.be/ZW9_2bNF1zo\" alt=\"Video\">\r\n          <img src=\"https://img.shields.io/badge/WACV-Video-maroon.svg\" /></a>\r\n</p>\r\n\r\n*Abstract - Benefiting from the capability of building inter-dependencies among channels or spatial locations, attention mechanisms have been extensively studied and broadly used in a variety of computer vision tasks recently. In this paper, we investigate light-weight but effective attention mechanisms and present triplet attention, a novel method for computing attention weights by capturing cross-dimension interaction using a three-branch structure. For an input tensor, triplet attention builds inter-dimensional dependencies by the rotation operation followed by residual transformations and encodes inter-channel and spatial information with negligible computational overhead. Our method is simple as well as efficient and can be easily plugged into classic backbone networks as an add-on module. We demonstrate the effectiveness of our method on various challenging tasks including image classification on ImageNet-1k and object detection on MSCOCO and PASCAL VOC datasets. Furthermore, we provide extensive in-sight into the performance of triplet attention by visually inspecting the GradCAM and GradCAM++ results. The empirical evaluation of our method supports our intuition on the importance of capturing dependencies across dimensions when computing attention weights.*\r\n\r\n<p float=\"left\">\r\n  <img src =\"figures/triplet.png\"  width=\"1000\"/>\r\n</p>\r\n<p>\r\n    <em>Figure 1. Structural Design of Triplet Attention Module. </em>\r\n</p>\r\n\r\n<p float=\"left\">\r\n  <img src =\"figures/comp.png\"  width=\"1000\"/>\r\n</p>\r\n<p>\r\n    <em>Figure 2. (a",
      "https://arxiv.org/abs/2010.03045",
      "https://arxiv.org/abs/2010.03045v1"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n```\r\n@InProceedings{Misra_2021_WACV,\r\n    author    = {Misra, Diganta and Nalamada, Trikay and Arasanipalai, Ajay Uppili and Hou, Qibin},\r\n    title     = {Rotate to Attend: Convolutional Triplet Attention Module},\r\n    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},\r\n    month     = {January},\r\n    year      = {2021},\r\n    pages     = {3139-3148}\r\n}\r\n```\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{Misra_2021_WACV,\n    author    = {Misra, Diganta and Nalamada, Trikay and Arasanipalai, Ajay Uppili and Hou, Qibin},\n    title     = {Rotate to Attend: Convolutional Triplet Attention Module},\n    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},\n    month     = {January},\n    year      = {2021},\n    pages     = {3139-3148}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9273716284741519,
        0.8345534538202741
      ],
      "excerpt": "|ResNet-18 + Triplet Attention (k = 3)|11.69 M|1.823|29.67%|10.42%|Google Drive| \n|ResNet-18 + Triplet Attention (k = 7)|11.69 M|1.825|28.91%|10.01%|Google Drive| \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/landskape-ai/triplet-attention",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-14T17:58:40Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-18T14:41:54Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8555128307665404,
        0.8555128307665404
      ],
      "excerpt": "|ResNet-50 + Triplet Attention (k = 7)|25.56 M|4.169|22.52%|6.326%|Google Drive| \n|ResNet-50 + Triplet Attention (k = 3)|25.56 M|4.131|23.88%|6.938%|Google Drive| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8799013953567729
      ],
      "excerpt": "All models are trained with 1x learning schedule. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9323729243789385,
        0.9397244443768564,
        0.8319866147669099
      ],
      "excerpt": "|ResNet-50 + Triplet Attention (k = 7)|Faster R-CNN|39.2|60.8|42.3|23.3|42.5|50.3|Google Drive| \n|ResNet-50 + Triplet Attention (k = 7)|RetinaNet|38.2|58.5|40.4|23.4|42.1|48.7|Google Drive| \n|ResNet-50 + Triplet Attention (k = 7)|Mask RCNN|39.8|61.6|42.8|24.3|42.9|51.3|Google Drive| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8229812125816096
      ],
      "excerpt": "|ResNet-50 + Triplet Attention (k = 7)|Keypoint RCNN|64.7|85.9|70.4|60.3|73.1|Google Drive| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9323729243789385,
        0.8608429686011371,
        0.9108601653970201,
        0.9569954402667161
      ],
      "excerpt": "|ResNet-50 + Triplet Attention (k = 7)|Faster R-CNN|39.3|60.8|42.7|23.4|42.8|50.3|Google Drive| \n|ResNet-50 + Triplet Attention (k = 7)|RetinaNet|37.6|57.3|40.0|21.7|41.1|49.7|Google Drive| \nThe Triplet Attention layer is implemented in triplet_attention.py. Since triplet attention is a dimentionality-preserving module, it can be inserted between convolutional layers in most stages of most networks. We recommend using the model definition provided here with our imagenet training repo to use the fastest and most up-to-date training scripts. \nHowever, this repository includes all the code required to recreate the experiments mentioned in the paper. This sections provides the instructions required to run these experiments. Imagenet training code is based on the official PyTorch example. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9720618372694302
      ],
      "excerpt": "The default learning rate schedule starts at 0.1 and decays by a factor of 10 every 30 epochs. This is appropriate for ResNet and models with batch normalization, but too high for AlexNet and VGG. Use 0.01 as the initial learning rate for AlexNet or VGG: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9356601498368972
      ],
      "excerpt": "Note, however, that we do not provide model defintions for AlexNet, VGG, etc. Only the ResNet family and MobileNetV2 are officially supported. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Official PyTorch Implementation for \"Rotate to Attend: Convolutional Triplet Attention Module.\"  [WACV 2021]",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/landskape-ai/triplet-attention/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 36,
      "date": "Thu, 23 Dec 2021 01:54:41 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/landskape-ai/triplet-attention/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "landskape-ai/triplet-attention",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/landskape-ai/triplet-attention/master/gradcam.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/landskape-ai/triplet-attention/master/scripts/train_imagenet_mobilenetv2_triplet_attention.sh",
      "https://raw.githubusercontent.com/landskape-ai/triplet-attention/master/scripts/train_coco_mask_rcnn_resnet50_triplet_attention.sh",
      "https://raw.githubusercontent.com/landskape-ai/triplet-attention/master/scripts/train_coco_retinanet_resnet50_triplet_attention.sh",
      "https://raw.githubusercontent.com/landskape-ai/triplet-attention/master/scripts/train_coco_faster_rcnn_resnet50_triplet_attention.sh",
      "https://raw.githubusercontent.com/landskape-ai/triplet-attention/master/scripts/train_coco_panoptic_resnet50_triplet_attention.sh",
      "https://raw.githubusercontent.com/landskape-ai/triplet-attention/master/scripts/train_cityscapes_resnet50_triplet_attention.sh",
      "https://raw.githubusercontent.com/landskape-ai/triplet-attention/master/scripts/resume_imagenet_resnet50_triplet_attention.sh",
      "https://raw.githubusercontent.com/landskape-ai/triplet-attention/master/scripts/train_voc_resnet50_triplet_attention.sh",
      "https://raw.githubusercontent.com/landskape-ai/triplet-attention/master/scripts/resume_imagenet_mobilenetv2_triplet_attention.sh",
      "https://raw.githubusercontent.com/landskape-ai/triplet-attention/master/scripts/train_coco_keypoint_resnet50_triplet_attention.sh",
      "https://raw.githubusercontent.com/landskape-ai/triplet-attention/master/scripts/train_imagenet_resnet50_triplet_attention.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8003727092893909
      ],
      "excerpt": "|ResNet-50 + Triplet Attention (k = 3)|25.56 M|4.131|23.88%|6.938%|Google Drive| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8600390629676424
      ],
      "excerpt": "To train a model on ImageNet, run train_imagenet.py with the desired model architecture and the path to the ImageNet dataset: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9110033972638539
      ],
      "excerpt": "python train_imagenet.py -a resnet18 [imagenet-folder with train and val folders] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9564982163984996
      ],
      "excerpt": "python main.py -a alexnet --lr 0.01 [imagenet-folder with train and val folders] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8457915567869898
      ],
      "excerpt": "python train_imagenet.py -a resnet50 --dist-url 'tcp://127.0.0.1:FREEPORT' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 [imagenet-folder with train and val folders] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8305890333401166
      ],
      "excerpt": "python train_imagenet.py -a resnet50 --dist-url 'tcp://IP_OF_NODE0:FREEPORT' --dist-backend 'nccl' --multiprocessing-distributed --world-size 2 --rank 0 [imagenet-folder with train and val folders] \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/landskape-ai/triplet-attention/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell",
      "TeX"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 LandskapeAI\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "/\" alt=\"Slides\">",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "triplet-attention",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "landskape-ai",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/landskape-ai/triplet-attention/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 267,
      "date": "Thu, 23 Dec 2021 01:54:41 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "computer-vision",
      "deep-learning",
      "attention-mechanism",
      "convolutional-neural-networks",
      "attention-mechanisms",
      "gradcam",
      "detection",
      "triplet-attention",
      "imagenet",
      "paper",
      "arxiv"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n|Backbone|Detectors|AP|AP<sub>50</sub>|AP<sub>75</sub>|AP<sub>S</sub>|AP<sub>M</sub>|AP<sub>L</sub>|Weights|\r\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\r\n|ResNet-50 + Triplet Attention (k = 7)|Mask RCNN|**35.8**|**57.8**|**38.1**|**18**|**38.1**|**50.7**|[Google Drive](https://drive.google.com/file/d/18hFlWdziAsK-FB_GWJk3iBRrtdEJK7lf/view?usp=sharing)|\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n```\r\nusage: train_imagenet.py  [-h] [--arch ARCH] [-j N] [--epochs N] [--start-epoch N] [-b N]\r\n                          [--lr LR] [--momentum M] [--weight-decay W] [--print-freq N]\r\n                          [--resume PATH] [-e] [--pretrained] [--world-size WORLD_SIZE]\r\n                          [--rank RANK] [--dist-url DIST_URL]\r\n                          [--dist-backend DIST_BACKEND] [--seed SEED] [--gpu GPU]\r\n                          [--multiprocessing-distributed]\r\n                          DIR\r\n\r\nPyTorch ImageNet Training\r\n\r\npositional arguments:\r\n  DIR                   path to dataset\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  --arch ARCH, -a ARCH  model architecture: alexnet | densenet121 |\r\n                        densenet161 | densenet169 | densenet201 |\r\n                        resnet101 | resnet152 | resnet18 | resnet34 |\r\n                        resnet50 | squeezenet1_0 | squeezenet1_1 | vgg11 |\r\n                        vgg11_bn | vgg13 | vgg13_bn | vgg16 | vgg16_bn | vgg19\r\n                        | vgg19_bn (default: resnet18)\r\n  -j N, --workers N     number of data loading workers (default: 4)\r\n  --epochs N            number of total epochs to run\r\n  --start-epoch N       manual epoch number (useful on restarts)\r\n  -b N, --batch-size N  mini-batch size (default: 256), this is the total\r\n                        batch size of all GPUs on the current node when using\r\n                        Data Parallel or Distributed Data Parallel\r\n  --lr LR, --learning-rate LR\r\n                        initial learning rate\r\n  --momentum M          momentum\r\n  --weight-decay W, --wd W\r\n                        weight decay (default: 1e-4)\r\n  --print-freq N, -p N  print frequency (default: 10)\r\n  --resume PATH         path to latest checkpoint (default: none)\r\n  -e, --evaluate        evaluate model on validation set\r\n  --pretrained          use pre-trained model\r\n  --world-size WORLD_SIZE\r\n                        number of nodes for distributed training\r\n  --rank RANK           node rank for distributed training\r\n  --dist-url DIST_URL   url used to set up distributed training\r\n  --dist-backend DIST_BACKEND\r\n                        distributed backend\r\n  --seed SEED           seed for initializing training.\r\n  --gpu GPU             GPU id to use.\r\n  --multiprocessing-distributed\r\n                        Use multi-processing distributed training to launch N\r\n                        processes per node, which has N GPUs. This is the\r\n                        fastest way to use PyTorch for either single node or\r\n                        multi node data parallel training\r\n```\r\n\r\n",
      "technique": "Header extraction"
    }
  ]
}