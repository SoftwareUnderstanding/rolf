{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1910.02054",
      "https://arxiv.org/abs/1910.02054",
      "https://arxiv.org/abs/1904.00962",
      "https://arxiv.org/abs/1910.02054"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "if step % args.save_interval: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8303083769691317
      ],
      "excerpt": "worker-2 and GPUs 0 and 1 on worker-3: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9699535689076293
      ],
      "excerpt": "multi-node/multi-gpu training jobs. If you prefer to launch your training job \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222383658450612
      ],
      "excerpt": "| Article                                                                                        | Description                                  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8687179211148074
      ],
      "excerpt": "| CIFAR-10 Tutorial                                              |  Getting started with CIFAR-10 and DeepSpeed | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8706498213570278
      ],
      "excerpt": "DeepSpeed welcomes your contributions! Please see our \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/meltnur/speed/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/meltnur/speed",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing\nDeepSpeed welcomes your contributions!\nPrerequisites\nDeepSpeed uses pre-commit to ensure that formatting is\nconsistent across DeepSpeed. First, ensure that pre-commit is installed from either\ninstalling DeepSpeed or pip install pre-commit. Next, the pre-commit hooks must be\ninstalled once before commits can be made:\nbash\npre-commit install\nAfterwards, our suite of formatting tests run automatically before each git commit. You\ncan also run these manually:\nbash\npre-commit run --all-files\nIf a formatting test fails, it will fix the modified code in place and abort\nthe git commit. After looking over the changes, you can git add &lt;modified files&gt;\nand then repeat the previous git commit command.\nTesting\nDeepSpeed tracks two types of tests: unit tests and more costly model convergence tests.\nThe model convergence tests train\nDeepSpeedExamples and measure\nend-to-end convergence and related metrics. Unit tests are found in tests/unit/ and\nthe model convergence tests are found in tests/model/.\nUnit Tests\nPyTest is used to execute tests. PyTest can be\ninstalled from PyPI via pip install pytest. Simply invoke pytest --forked to run the\nunit tests:\nbash\npytest --forked tests/unit/\nYou can also provide the -v flag to pytest to see additional information about the\ntests. Note that pytest-forked and the\n--forked flag are required to test CUDA functionality in distributed tests.\nModel Tests\nTo execute model tests, first install DeepSpeed. The\nDeepSpeedExamples repository is cloned\nas part of this process. Next, execute the model test driver:\nbash\ncd tests/model/\npytest run_sanity_check.py\nNote that the --forked flag is not necessary for the model tests.\nContributor License Agreement\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\nCode of Conduct\nThis project has adopted the Microsoft Open Source Code of\nConduct. For more information see the\nCode of Conduct FAQ or contact\nopencode@microsoft.com with any additional questions or\ncomments.",
    "technique": "File Exploration"
  },
  "contributor": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-10T03:05:36Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-02T21:43:21Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9899219360696059
      ],
      "excerpt": "| Contributing           |  Instructions for contributing to DeepSpeed | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333769817186591,
        0.9175423955855755
      ],
      "excerpt": "Training advanced deep learning models is challenging. Beyond model design, \nmodel scientists also need to set up the state-of-the-art training techniques \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8982750200382874,
        0.9949901874433842,
        0.8699759846391634
      ],
      "excerpt": "performance and convergence rate. Large model sizes are even more challenging: \na large model easily runs out of memory with pure data parallelism and it is \ndifficult to use model parallelism. DeepSpeed addresses these challenges to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9618191992500631
      ],
      "excerpt": "The DeepSpeed API is a lightweight wrapper on PyTorch. This \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9888589610366169
      ],
      "excerpt": "platform. In addition, DeepSpeed manages all of the boilerplate state-of-the-art \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9445639497724616
      ],
      "excerpt": "DeepSpeed to boost speed and scale with just a few lines of code changes to your PyTorch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8972746406044196
      ],
      "excerpt": "DeepSpeed trains BERT-large to parity in 14 hours using 64 GPUs (4 DGX-2 boxes) and in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8167207040083355
      ],
      "excerpt": "DeepSpeed provides memory-efficient data parallelism and enables training models without \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9070441982565356
      ],
      "excerpt": "PyTorch's Distributed Data Parallel) run out of memory with 1.5 billion parameter models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874769138271518,
        0.8666136523704866,
        0.9431129248712083,
        0.8392027697682727,
        0.9482299612188899
      ],
      "excerpt": "replicated across data-parallel processes, ZeRO partitions model states to save \nsignificant memory. The current implementation (stage 1 of ZeRO) reduces memory by up to \n4x relative to the state-of-art. You can read more about ZeRO in our paper. \nWith this impressive memory reduction, early adopters of DeepSpeed have already \nproduced  a language model (LM) with over 17B parameters called \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9235134564707205,
        0.9507858653619622
      ],
      "excerpt": "establishing a new SOTA in the LM category. \nDeepSpeed supports efficient data parallelism, model parallelism, and their \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8993901074938371,
        0.859254952601827,
        0.9544259560300294,
        0.9757290552820004,
        0.9808185202362899,
        0.8945726939934083
      ],
      "excerpt": "  10x larger than the state-of-art (8 billion NVIDIA GPT, 11 billion Google T5). \n* DeepSpeed can run large models more efficiently, up to 6x faster for models with \n  various sizes spanning 1.5B to 100B.  More specifically, the data parallelism powered by ZeRO \n  is complementary and can be combined with different types of model parallelism.  It allows \n  DeepSpeed to fit models using lower degree of model parallelism and higher batch size, offering \n  significant performance gains compared to using model parallelism alone. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865,
        0.908925214220865
      ],
      "excerpt": "  and GPT tutorial. \n  <!-- and [QANet tutorial](../../Tutorials/QANetTutorial.md). --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9799409182652217
      ],
      "excerpt": "<em>The figure depicts system throughput improvements of DeepSpeed (combining ZeRO-powered data parallelism with model parallelism of NVIDIA Megatron-LM) over using Megatron-LM alone.</em> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8774368119042177,
        0.8269423109006566,
        0.9468121113530321,
        0.854342404181016
      ],
      "excerpt": "DeepSpeed supports advanced hyperparameter tuning and large batch size \noptimizers such as LAMB. These improve the \neffectiveness of model training and reduce the number of samples required to \nconvergence to desired accuracy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9966878443716578,
        0.878151521971628
      ],
      "excerpt": "Only a few lines of code changes are needed to enable a PyTorch model to use DeepSpeed and ZeRO. Compared to current model parallelism libraries, DeepSpeed does not require a code redesign or model refactoring. It also does not put limitations on model dimensions (such as number of attention heads, hidden sizes, and others), batch size, or any other training parameters. For models of up to six billion parameters, you can use ZeRO-powered data parallelism conveniently without requiring model parallelism, while in contrast, standard data parallelism will run out of memory for models with more than 1.3 billion parameters. In addition, DeepSpeed conveniently supports flexible combination of ZeRO-powered data parallelism with custom model parallelisms, such as tensor slicing of NVIDIA's Megatron-LM. \nBelow we provide a brief feature list, see our detailed feature \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.9103754479321569
      ],
      "excerpt": "Model Parallelism \nSupport for Custom Model Parallelism \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "The Zero Redundancy Optimizer (ZeRO) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8104426079870416,
        0.8520517412160609,
        0.8967403971132363,
        0.9745134634784969
      ],
      "excerpt": "Performance Analysis and Debugging \nDeepSpeed model training is accomplished using the DeepSpeed engine. The engine \ncan wrap any arbitrary model of type torch.nn.module and has a minimal set of APIs \nfor training and checkpointing the model. Please see the tutorials for detailed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.82932183982513
      ],
      "excerpt": "To initialize the DeepSpeed engine: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "                                                     model=model, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9442041279454365
      ],
      "excerpt": "deepspeed.inialize ensures that all of the necessary setup required for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9364468717212198,
        0.854989323555329,
        0.962883366754369
      ],
      "excerpt": "appropriately under the hood.  In addition to wrapping the model, DeepSpeed can \nconstruct and manage the training optimizer, data loader, and the learning rate \nscheduler based on the parameters passed to deepspeed.initialze and the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.885925996747002
      ],
      "excerpt": "model using three simple APIs for forward propagation (()), backward \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8309506487949138
      ],
      "excerpt": "for step, batch in enumerate(data_loader): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.891808036706038
      ],
      "excerpt": "required for distributed data parallel training, in mixed precision, with a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8163899217778334,
        0.9023362870041343,
        0.859556598546517
      ],
      "excerpt": "  rate when step is executed. \nSaving and loading the training state is handled via the save_checkpoint and \nload_checkpoint API in DeepSpeed which takes two arguments to uniquely \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.911333334169639
      ],
      "excerpt": ":advance data loader to ckpt step \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8309506487949138
      ],
      "excerpt": "for step, batch in enumerate(data_loader): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8681842926862705,
        0.8248716673632324,
        0.9143189198241036,
        0.876359675482716
      ],
      "excerpt": "DeepSpeed can automatically save and restore the model, optimizer, and the \nlearning rate scheduler states while hiding away these details from the user. \nHowever, the user may want to save other data in addition to these that are \nunique to a given model training. To support these items, save_checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9328800658496833
      ],
      "excerpt": "the step value is stored as part of the client_sd. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9915465897241252
      ],
      "excerpt": "is shown below. For a full set of features see core API \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8734024767871401
      ],
      "excerpt": "prior to training. The user can simply add these variables to a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8925500893873347
      ],
      "excerpt": "We illustrate an example usage of DeepSpeed with the following assumptions: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9018081109426139,
        0.9271033869619315,
        0.908925214220865,
        0.9514883200706653,
        0.909663863000182
      ],
      "excerpt": "ds_config.json is the configuration file for DeepSpeed \nDeepSpeed configures multi-node compute resources with hostfiles that are compatible with \nOpenMPI and Horovod. \nA hostfile is a list of hostnames (or SSH aliases), which are machines accessible via passwordless \nSSH, and slot counts, which specify the number of GPUs available on the system. For \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9550854317868718
      ],
      "excerpt": "DeepSpeed queries the number of GPUs on the local machine to discover the number of local \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9319405839763483
      ],
      "excerpt": "Alternatively, DeepSpeed allows you to restrict distributed training of your model to a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8971894977872404
      ],
      "excerpt": "worker-2 and GPUs 0 and 1 on worker-3: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8775605545449664
      ],
      "excerpt": "using MPI (e.g., mpirun), we provide support for this. It should be noted that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.820048603443141
      ],
      "excerpt": "rank, world size) and properly initialize torch distributed for training. In this \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9776424598865588
      ],
      "excerpt": "In the case that we are only running on a single node (with one or more GPUs) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9470859083262667,
        0.814663668568442,
        0.8239003597221727
      ],
      "excerpt": "local machine to discover the number of slots available. The --include and \n--exclude arguments work as normal, but the user should specify 'localhost' \nas the hostname. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8930901044020226
      ],
      "excerpt": "| DeepSpeed Features                                                       |  DeepSpeed features                          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8272945003240391,
        0.8807518949129635,
        0.9234539619149897
      ],
      "excerpt": "| 1Cycle Tutorial                                                  |  SOTA learning schedule in DeepSpeed         | \nDeepSpeed welcomes your contributions! Please see our \ncontributing guide for more details on formatting, testing, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8399748571878369,
        0.9177762152211659,
        0.8373310029600464
      ],
      "excerpt": "This project has adopted the Microsoft Open Source Code of \nConduct. For more information see the \nCode of Conduct FAQ or contact \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/meltnur/speed/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 10:56:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/meltnur/speed/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "meltnur/speed",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/meltnur/speed/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/meltnur/speed/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/meltnur/speed/master/install.sh",
      "https://raw.githubusercontent.com/meltnur/speed/master/azure/create_vms.sh",
      "https://raw.githubusercontent.com/meltnur/speed/master/azure/attach.sh",
      "https://raw.githubusercontent.com/meltnur/speed/master/azure/azure_ssh.sh",
      "https://raw.githubusercontent.com/meltnur/speed/master/azure/build_docker_image.sh",
      "https://raw.githubusercontent.com/meltnur/speed/master/azure/setup_vms.sh",
      "https://raw.githubusercontent.com/meltnur/speed/master/azure/setup_docker.sh",
      "https://raw.githubusercontent.com/meltnur/speed/master/azure/start_container.sh",
      "https://raw.githubusercontent.com/meltnur/speed/master/azure/shutdown_vms.sh",
      "https://raw.githubusercontent.com/meltnur/speed/master/tests/model/BingBertSquad/run_BingBertSquad_sanity.sh",
      "https://raw.githubusercontent.com/meltnur/speed/master/tests/model/BingBertSquad/run_BingBertSquad.sh",
      "https://raw.githubusercontent.com/meltnur/speed/master/tests/model/BingBertSquad/run_tests.sh",
      "https://raw.githubusercontent.com/meltnur/speed/master/tests/model/Megatron_GPT2/ds_gpt2_test.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Please see our [Azure tutorial](docs/azure.md) to get started with DeepSpeed on Azure!\n* If you're not on Azure, we recommend using our docker image via `docker pull deepspeed/deepspeed:latest` which contains a pre-installed version of DeepSpeed and all the necessary dependencies.\n* If you want to install DeepSpeed manually, we provide an install script [install.sh](install.sh) to help install on a local machine or across an entire cluster.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8339749365308032
      ],
      "excerpt": "| Contributing           |  Instructions for contributing to DeepSpeed | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8897142725240411
      ],
      "excerpt": "deepspeed.inialize ensures that all of the necessary setup required for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8092064060474253
      ],
      "excerpt": "propagate all NCCL and PYTHON related environment variables that are set. If \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8585567598093745
      ],
      "excerpt": "DeepSpeed will then make sure that these environment variables are set when \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8331407610465424
      ],
      "excerpt": "--exclude flags. For example, to use all available resources except GPU 0 on node \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8509840299663464
      ],
      "excerpt": "mpi4py to discover the MPI environment (e.g., \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9978942625814468
      ],
      "excerpt": "installed via pip install mpi4py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8222325963015485
      ],
      "excerpt": "as the hostname. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8014336232771484
      ],
      "excerpt": "<!---*Read more*: [BERT tutorial](../../Tutorials/bert_pretraining/deepspeed_bert_training.md)--> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8473115912663577
      ],
      "excerpt": "model parallelism. For example, DeepSpeed can train models with up to 6 billion parameters on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "Training Optimizers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "Training Agnostic Checkpointing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060372108626078
      ],
      "excerpt": "                                                     model_parameters=params) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8686326385887355
      ],
      "excerpt": "file that should be specified as args.deepspeed_config. A sample config file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060372108626078
      ],
      "excerpt": "    \"params\": { \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  \"zero_optimization\": true \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8117072807040038
      ],
      "excerpt": "the deepspeed launcher, here is an example: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/meltnur/speed/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Cuda",
      "C++",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'    MIT License\\r\\n\\r\\n    Copyright (c) Microsoft Corporation.\\r\\n\\r\\n    Permission is hereby granted, free of charge, to any person obtaining a copy\\r\\n    of this software and associated documentation files (the \"Software\"), to deal\\r\\n    in the Software without restriction, including without limitation the rights\\r\\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\r\\n    copies of the Software, and to permit persons to whom the Software is\\r\\n    furnished to do so, subject to the following conditions:\\r\\n\\r\\n    The above copyright notice and this permission notice shall be included in all\\r\\n    copies or substantial portions of the Software.\\r\\n\\r\\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\r\\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\r\\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\r\\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\r\\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\r\\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\r\\n    SOFTWARE\\r\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Table of Contents",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "speed",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "meltnur",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/meltnur/speed/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 10:56:02 GMT"
    },
    "technique": "GitHub API"
  }
}