{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2011.09533",
      "https://arxiv.org/abs/2103.01955",
      "https://arxiv.org/abs/1706.02275"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/anonymous-ICLR22/Trust-Region-Methods-in-Multi-Agent-Reinforcement-Learning",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-04T08:15:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-29T16:15:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9849988635106698
      ],
      "excerpt": "Anonymous code release for ICLR 22 paper submission, named \"Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning\". This repository develops Heterogeneous Agent Trust Region Policy Optimisation (HATRPO) and Heterogeneous-Agent Proximal Policy Optimisation (HAPPO) algorithms on the bechmarks of SMAC and Multi-agent MUJOCO. HATRPO and HAPPO are the first trust region methods for multi-agent reinforcement learning with theoretically-justified monotonic improvement guarantee. Performance wise, it is the new state-of-the-art algorithm against its rivals such as IPPO, MAPPO and MADDPG \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/anonymous-iclr22/trust-region-in-multi-agent-reinforcement-learning/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 00:50:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/anonymous-ICLR22/Trust-Region-Methods-in-Multi-Agent-Reinforcement-Learning/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "anonymous-ICLR22/Trust-Region-Methods-in-Multi-Agent-Reinforcement-Learning",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/anonymous-iclr22/trust-region-in-multi-agent-reinforcement-learning/master/install_sc2.sh",
      "https://raw.githubusercontent.com/anonymous-iclr22/trust-region-in-multi-agent-reinforcement-learning/master/scripts/train_mujoco.sh",
      "https://raw.githubusercontent.com/anonymous-iclr22/trust-region-in-multi-agent-reinforcement-learning/master/scripts/train_smac.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.954950449355536,
        0.9770335174395833,
        0.9979947896609701,
        0.9955995167213997,
        0.9492425951526664
      ],
      "excerpt": "conda create -n env_name python=3.9 \nconda activate env_name \npip install -r requirements.txt \nconda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c nvidia \nFollowing the instructions in https://github.com/openai/mujoco-py and https://github.com/schroederdewitt/multiagent_mujoco to setup a mujoco environment. In the end, remember to set the following environment variables: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9465718491881494,
        0.9846747649829007
      ],
      "excerpt": "bash install_sc2.sh \nOr you could install them manually to other path you like, just follow here: https://github.com/oxwhirl/smac. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/anonymous-ICLR22/Trust-Region-Methods-in-Multi-Agent-Reinforcement-Learning/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Tianshou contributors\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Trust-Region-Methods-in-Multi-Agent-Reinforcement-Learning",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "anonymous-ICLR22",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/anonymous-ICLR22/Trust-Region-Methods-in-Multi-Agent-Reinforcement-Learning/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "When your environment is ready, you could run shell scripts provided. For example:\n``` Bash\ncd scripts\n./train_mujoco.sh  #: run with HAPPO/HATRPO on Multi-agent MuJoCo\n./train_smac.sh  #: run with HAPPO/HATRPO on StarCraft II\n```\n\nIf you would like to change the configs of experiments, you could modify sh files or look for config files for more details. And you can change algorithm by modify **algo=happo** as **algo=hatrpo**.\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Thu, 23 Dec 2021 00:50:58 GMT"
    },
    "technique": "GitHub API"
  }
}