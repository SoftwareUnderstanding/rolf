{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1607.04606",
      "https://arxiv.org/abs/1607.01759",
      "https://arxiv.org/abs/1612.03651",
      "https://arxiv.org/abs/1612.03651"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite [1](#enriching-word-vectors-with-subword-information) if using this code for learning word representations or [2](#bag-of-tricks-for-efficient-text-classification) if using for text classification.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite below work if using this code for extreme classification.\n\nM. Wydmuch, K. Jasinska, M. Kuznetsov, R. Busa-Fekete, K. Dembczy\u0144ski. [*A no-regret generalization of hierarchical softmax to extreme multi-label classification*](http://papers.nips.cc/paper/7872-a-no-regret-generalization-of-hierarchical-softmax-to-extreme-multi-label-classification). Advances in Neural Information Processing Systems 31, 2018.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{joulin2016fasttext,\n  title={FastText.zip: Compressing text classification models},\n  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\\'e}gou, H{\\'e}rve and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1612.03651},\n  year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{joulin2017bag,\n  title={Bag of Tricks for Efficient Text Classification},\n  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},\n  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},\n  month={April},\n  year={2017},\n  publisher={Association for Computational Linguistics},\n  pages={427--431},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{bojanowski2017enriching,\n  title={Enriching Word Vectors with Subword Information},\n  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n  journal={Transactions of the Association for Computational Linguistics},\n  volume={5},\n  year={2017},\n  issn={2307-387X},\n  pages={135--146}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8568217992087697
      ],
      "excerpt": "sigmoid loss for multi-label classification, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "$ export MACOSX_DEPLOYMENT_TARGET=10.9 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8929030112788902
      ],
      "excerpt": "Word vectors for 157 languages trained on Wikipedia and Crawl. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9422100272984922
      ],
      "excerpt": "$ wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mwydmuch/extremeText",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to fastText\nWe want to make contributing to this project as easy and transparent as possible.\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is clear and has sufficient instructions to be able to reproduce the issue.\nReproducing issues\nPlease make sure that the issue you mention is not a result of one of the existing third-party libraries. For example, please do not post an issue if you encountered an error within a third-party Python library. We can only help you with errors which can be directly reproduced either with our C++ code or the corresponding Python bindings. If you do find an error, please post detailed steps to reproduce it. If we can't reproduce your error, we can't help you fix it.\nPull Requests\nPlease post an Issue before submitting a pull request. This might save you some time as it is possible we can't support your contribution, albeit we try our best to accomodate your (planned) work and highly appreciate your time. Generally, it is best to have a pull request emerge from an issue rather than the other way around.\nTo create a pull request:\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nTests\nFirst, you will need to make sure you have the required data. For that, please have a look at the fetch_test_data.sh script under tests. Next run the tests using the runtests.py script passing a path to the directory containing the datasets.\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nLicense\nBy contributing to fastText, you agree that your contributions will be licensed under its BSD license.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-08-16T01:04:26Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-17T06:38:54Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9816549644572102
      ],
      "excerpt": "extremeText is an extension of fastText library for multi-label classification including extreme cases with hundreds of thousands and millions of labels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9391767134256334,
        0.8671484895448803,
        0.9683454051111621,
        0.9293381352122214
      ],
      "excerpt": "L2 regularization for all losses, \nensemble of loss layers with bagging, \ncalculation of hidden (document) vector as a weighted average of the word vectors, \ncalculation of TF-IDF weights for words. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8273099514127742,
        0.8555139264671839
      ],
      "excerpt": "This will produce object files for all the classes as well as the main binary extremetext. \nThe easiest way to get extremeText is to use pip. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241465669537728
      ],
      "excerpt": "Rewrite vanilla fastText losses as extremeText loss layers to support all new features. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8712700522911636
      ],
      "excerpt": "Obtaining word vectors for out-of-vocabulary words \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8965799723048201
      ],
      "excerpt": "Bag of Tricks for Efficient Text Classification \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.915712009163892,
        0.804878140569991
      ],
      "excerpt": "The preprocessed YFCC100M data used in [2]. \nWe also provide a cheatsheet full of useful one-liners. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8273099514127742,
        0.8445442627931573,
        0.8610864278537756
      ],
      "excerpt": "This will produce object files for all the classes as well as the main binary fasttext. \nIf you do not plan on using the default system-wide compiler, update the two macros defined at the beginning of the Makefile (CC and INCLUDES). \nFor now this is not part of a release, so you will need to clone the master branch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9190149997775718,
        0.8610864278537756
      ],
      "excerpt": "This will create the fasttext binary and also all relevant libraries (shared, static, PIC). \nFor now this is not part of a release, so you will need to clone the master branch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8776028636598163,
        0.9156813420755983
      ],
      "excerpt": "For further information and introduction see python/README.md \nIn order to learn word vectors, as described in 1, do: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9483541808483614
      ],
      "excerpt": "At the end of optimization the program will save two files: model.bin and model.vec. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9781224015614493,
        0.894958727550187,
        0.8852187335675773
      ],
      "excerpt": "model.bin is a binary file containing the parameters of the model along with the dictionary and all hyper parameters. \nThe binary file can be used later to compute word vectors or to restart the optimization. \nThe previously trained model can be used to compute word vectors for out-of-vocabulary words. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8846431569672244
      ],
      "excerpt": "By default, we assume that labels are words that are prefixed by the string __label__. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9614742823684236,
        0.8272065922480798
      ],
      "excerpt": "The argument k is optional, and is equal to 1 by default. \nIn order to obtain the k most likely labels for a piece of text, use: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8317909397572478
      ],
      "excerpt": "or use predict-prob to also get the probability for each label \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186981690125441
      ],
      "excerpt": "The argument k is optional, and equal to 1 by default. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230558135694459
      ],
      "excerpt": "This assumes that the text.txt file contains the paragraphs that you want to get vectors for. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.838895537599196
      ],
      "excerpt": "This will create a .ftz file with a smaller memory footprint. All the standard functionality, like test or predict work the same way on the quantized models: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8965799723048201
      ],
      "excerpt": "[2] A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, Bag of Tricks for Efficient Text Classification \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Library for fast text representation and extreme classification.",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Invoke a command without arguments to list available arguments and their default values:\n\n```\n$ ./fasttext supervised\nEmpty input or output path.\n\nThe following arguments are mandatory:\n  -input              training file path\n  -output             output file path\n\nThe following arguments are optional:\n  -verbose            verbosity level [2]\n\nThe following arguments for the dictionary are optional:\n  -minCount           minimal number of word occurences [1]\n  -minCountLabel      minimal number of label occurences [0]\n  -wordNgrams         max length of word ngram [1]\n  -bucket             number of buckets [2000000]\n  -minn               min length of char ngram [0]\n  -maxn               max length of char ngram [0]\n  -t                  sampling threshold [0.0001]\n  -label              labels prefix [__label__]\n\nThe following arguments for training are optional:\n  -lr                 learning rate [0.1]\n  -lrUpdateRate       change the rate of updates for the learning rate [100]\n  -dim                size of word vectors [100]\n  -ws                 size of the context window [5]\n  -epoch              number of epochs [5]\n  -neg                number of negatives sampled [5]\n  -loss               loss function {ns, hs, softmax} [softmax]\n  -thread             number of threads [12]\n  -pretrainedVectors  pretrained word vectors for supervised learning []\n  -saveOutput         whether output params should be saved [0]\n\nThe following arguments for quantization are optional:\n  -cutoff             number of words and ngrams to retain [0]\n  -retrain            finetune embeddings if a cutoff is applied [0]\n  -qnorm              quantizing the norm separately [0]\n  -qout               quantizing the classifier [0]\n  -dsub               size of each sub-vector [2]\n```\n\nDefaults may vary by mode. (Word-representation modes `skipgram` and `cbow` use a default `-minCount` of 5.)\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mwydmuch/extremeText/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can find [answers to frequently asked questions](https://fasttext.cc/docs/en/faqs.html#content) on our [website](https://fasttext.cc/).\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 15,
      "date": "Wed, 22 Dec 2021 04:33:11 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mwydmuch/extremeText/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mwydmuch/extremeText",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/mwydmuch/extremeText/tree/master/docs",
      "https://github.com/mwydmuch/extremeText/tree/master/website/static/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/classification-results.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/classification-example.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/word-vector-example.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/get-wikimedia.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/quantization-example.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/scripts/kbcompletion/data.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/scripts/kbcompletion/svo.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/scripts/kbcompletion/fb15k237.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/scripts/kbcompletion/fb15k.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/scripts/kbcompletion/wn18.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/scripts/quantization/quantization-results.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/tests/fetch_test_data.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_Wiki10-31K_ensemble.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_AmazonCat-13K.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_WikipediaLarge-500K.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_EURLex-4K.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_Delicious-200K.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_xml.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_Delicious-200K_ensemble.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_AmazonCat-13K_ensemble.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_WikiLSHTC-325K.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_WikiLSHTC-325K_ensemble.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_EURLex-4K_ensemble.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_Amazon-3M.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_WikipediaLarge-500K_ensemble.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_Wiki10-31K.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_Amazon-3M_ensmeble.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_Amazon-670K.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/xml_experiments/run_Amazon-670K_ensemble.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/.circleci/gcc_test.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/.circleci/setup_circleimg.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/.circleci/setup_debian.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/.circleci/pull_data.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/.circleci/cmake_test.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/.circleci/run_locally.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/.circleci/python_test.sh",
      "https://raw.githubusercontent.com/mwydmuch/extremeText/master/.circleci/pip_test.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9296675346880667,
        0.9893272198983933,
        0.9906248903846466,
        0.8446577173439814,
        0.8474895321345809
      ],
      "excerpt": "extremeText like fastText can be build as executable using Make (recommended) or/and CMake: \n$ git clone https://github.com/mwydmuch/extremeText.git \n$ cd extremeText \n(optional) $ cmake . \n$ make \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8786030359925244,
        0.999746712887969,
        0.8084334251881404
      ],
      "excerpt": "The easiest way to get extremeText is to use pip. \n$ pip install extremetext \nInstalling on MacOS may require setting MACOSX_DEPLOYMENT_TARGET=10.9 first: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999746712887969,
        0.9850086664289751,
        0.9893272198983933,
        0.9906248903846466,
        0.999746712887969,
        0.9834327952880133
      ],
      "excerpt": "$ pip install extremetext \nThe latest version of extremeText can be build from sources using pip or alternatively setuptools. \n$ git clone https://github.com/mwydmuch/extremeText.git \n$ cd extremeText \n$ pip install . \n(or) $ python setup.py install \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8197654322448317
      ],
      "excerpt": "Merge with the latest changes from fastText. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8058015025548094
      ],
      "excerpt": "Getting the source code \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8606438138370869
      ],
      "excerpt": "$ wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466,
        0.8474895321345809
      ],
      "excerpt": "$ cd fastText-0.1.0 \n$ make \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.982061580294172,
        0.9906248903846466,
        0.9920524742609089,
        0.98163424234106
      ],
      "excerpt": "$ git clone https://github.com/facebookresearch/fastText.git \n$ cd fastText \n$ mkdir build &amp;&amp; cd build &amp;&amp; cmake .. \n$ make &amp;&amp; make install \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.982061580294172,
        0.9906248903846466,
        0.999746712887969
      ],
      "excerpt": "$ git clone https://github.com/facebookresearch/fastText.git \n$ cd fastText \n$ pip install . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8147438040199454
      ],
      "excerpt": "You can also quantize a supervised model to reduce its memory usage with the following command: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9133368656218674
      ],
      "excerpt": "import extremeText \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8514016617147449
      ],
      "excerpt": "FastText.zip: Compressing text classification models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8082022887640018
      ],
      "excerpt": "$ unzip v0.1.0.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.901453818582758,
        0.9103756700490212
      ],
      "excerpt": "$ ./fasttext skipgram -input data.txt -output model \nwhere data.txt is a training file containing UTF-8 encoded text. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9070631331869214
      ],
      "excerpt": "model.vec is a text file containing the word vectors, one per line. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8553814334179051
      ],
      "excerpt": "This will output word vectors to the standard output, one vector per line. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8643354341371444
      ],
      "excerpt": "$ cat queries.txt | ./fasttext print-word-vectors model.bin \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8839158252682724,
        0.9437964632054884
      ],
      "excerpt": "$ ./fasttext supervised -input train.txt -output model \nwhere train.txt is a text file containing a training sentence per line along with the labels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8271340404619316
      ],
      "excerpt": "This will output two files: model.bin and model.vec. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8951381859294457
      ],
      "excerpt": "$ ./fasttext test model.bin test.txt k \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9118148969249582
      ],
      "excerpt": "$ ./fasttext predict model.bin test.txt k \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9118148969249582,
        0.8919829784350816,
        0.855185380383668
      ],
      "excerpt": "$ ./fasttext predict-prob model.bin test.txt k \nwhere test.txt contains a piece of text to classify per line. \nDoing so will print to the standard output the k most likely labels for each line. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8530417207217321
      ],
      "excerpt": "$ ./fasttext print-sentence-vectors model.bin &lt; text.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906678155209428
      ],
      "excerpt": "The program will output one vector representation per line in the file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.857129346874625
      ],
      "excerpt": "$ ./fasttext quantize -output model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8910782061406535
      ],
      "excerpt": "$ ./fasttext test model.ftz test.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.861059532256563
      ],
      "excerpt": "run the script quantization-example.sh for an example. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8514016617147449
      ],
      "excerpt": "[3] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J\u00e9gou, T. Mikolov, FastText.zip: Compressing text classification models \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mwydmuch/extremeText/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "HTML",
      "C++",
      "JavaScript",
      "Python",
      "Shell",
      "CSS",
      "CMake",
      "Makefile",
      "Perl"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/mwydmuch/extremeText/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD License\\n\\nFor fastText software\\n\\nCopyright (c) 2016-present, Facebook, Inc. All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification,\\nare permitted provided that the following conditions are met:\\n\\n * Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n * Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\n * Neither the name Facebook nor the names of its contributors may be used to\\n   endorse or promote products derived from this software without specific\\n   prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "extremeText",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "extremeText",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mwydmuch",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mwydmuch/extremeText/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "mwydmuch",
        "body": "",
        "dateCreated": "2018-11-29T02:45:21Z",
        "datePublished": "2018-11-29T07:10:30Z",
        "html_url": "https://github.com/mwydmuch/extremeText/releases/tag/0.8.3",
        "name": "v0.8.3",
        "tag_name": "0.8.3",
        "tarball_url": "https://api.github.com/repos/mwydmuch/extremeText/tarball/0.8.3",
        "url": "https://api.github.com/repos/mwydmuch/extremeText/releases/14249256",
        "zipball_url": "https://api.github.com/repos/mwydmuch/extremeText/zipball/0.8.3"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We are continously building and testing our library, CLI and Python bindings under various docker images using [circleci](https://circleci.com/).\n\nGenerally, **fastText** builds on modern Mac OS and Linux distributions.\nSince it uses some C++11 features, it requires a compiler with good C++11 support.\nThese include :\n\n* (g++-4.7.2 or newer) or (clang-3.3 or newer)\n\nCompilation is carried out using a Makefile, so you will need to have a working **make**.\nIf you want to use **cmake** you need at least version 2.8.9.\n\nOne of the oldest distributions we successfully built and tested the CLI under is [Debian wheezy](https://www.debian.org/releases/wheezy/).\n\nFor the word-similarity evaluation script you will need:\n\n* Python 2.6 or newer\n* NumPy & SciPy\n\nFor the python bindings (see the subdirectory python) you will need:\n\n* Python version 2.7 or >=3.4\n* NumPy & SciPy\n* [pybind11](https://github.com/pybind/pybind11)\n\nOne of the oldest distributions we successfully built and tested the Python bindings under is [Debian jessie](https://www.debian.org/releases/jessie/).\n\nIf these requirements make it impossible for you to use fastText, please open an issue and we will try to accommodate you.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 139,
      "date": "Wed, 22 Dec 2021 04:33:11 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "extreme-classification",
      "multi-label-classification",
      "extreme-multi-label-cassification",
      "text-classification",
      "fasttext",
      "extremetext",
      "machine-learning"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "extremeText adds new options for fastText supervised command:\n\n```\n$ ./extremetext supervised\n\nNew losses for multi-label classification:\n  -loss sigmoid       \n  -loss plt           (Probabilistic Labels Tree)\n\nWith the following optional arguments:\n  General:\n  -l2                 L2 regularization (default = 0)\n  -tfidfWeights       calculate TF-IDF weights for words\n  -wordsWeights       read word weights from file (format: <word>:<weights>)\n  -weight             document weight prefix (default = __weight__; format: <weight prefix>:<document weight>)\n  -tag                tags prefix (default = __tag__), tags are ignored words, that are outputed with prediction\n  -eosWeight          weight of EOS token (default = 1.0)\n  -freezeVectors      freeze pretrained word vectors for supervised learning\n  \n  PLT (Probabilistic Labels Tree):\n  -treeType           type of PLT: complete, huffman, kmeans (default = kmeans)\n  -arity              arity of PLT (default = 2)\n  -maxLeaves          maximum number of leaves (labels) in one internal node of PLT (default = 100)\n  -kMeansEps          stopping criteria for k-means clustering (default = 0.001)\n  \n  Ensemble:\n  -ensemble           size of the ensemble (default = 1)\n  -bagging            bagging ratio (default = 1.0)\n```\n\nextremeText also adds new commands and makes other to work in parallel:\n```\n$ ./extremetext predict[-prob] <model> <test-data> [<k>] [<th>] [<output>] [<thread>]\n$ ./extremetext get-prob <model> <input> [<th>] [<output>] [<thread>]\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.\n\nThere is also the master branch that contains all of our most recent work, but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This library has two main use cases: word representation learning and text classification.\nThese were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).\n\n",
      "technique": "Header extraction"
    }
  ]
}