{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1711.00937 and https://arxiv.org/abs/1901.08810.\n\nAlso see [vq-vae-melgan](https://github.com/DongyaoZhu/Real-Time-Accent-Conversion/tree/master/voice_cloning",
      "https://arxiv.org/abs/1901.08810.\n\nAlso see [vq-vae-melgan](https://github.com/DongyaoZhu/Real-Time-Accent-Conversion/tree/master/voice_cloning",
      "https://arxiv.org/abs/1901.08810\n\nParameters can be found in `Encoder/encoder.py` and `model_parameters.json`.\n\n#### VQ\n\nThere are 2 ways to train the embedding:\n- train $z_e$ and $e_k$ separately, as described in original paper (default"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py\n- https://github.com/deepmind/sonnet/blob/master/sonnet/examples/vqvae_example.ipynb\n- https://github.com/tensorflow/magenta/tree/master/magenta/models/nsynth/wavenet\n- https://github.com/ibab/tensorflow-wavenet\n- https://github.com/JeremyCCHsu/vqvae-speech\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9875460765120729
      ],
      "excerpt": "This is a TensorFlow implementation of vqvae with wavenet decoder, based on https://arxiv.org/abs/1711.00937 and https://arxiv.org/abs/1901.08810. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9954018478929674
      ],
      "excerpt": "- 2019 the one described in https://arxiv.org/abs/1901.08810 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DongyaoZhu/VQ-VAE-WaveNet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-01T18:52:45Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-03T03:16:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9914545701035804,
        0.9121859697720188
      ],
      "excerpt": "Also see vq-vae-melgan using melgan in place of wavenet for real-time application. \nThe folder results contains some reconstructed audio. Speaker conversion works well, but encoder (local condition) needs some more tuning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8742124284718332
      ],
      "excerpt": "This could be turned off as well, in which case an AE is trained. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8663265130543782
      ],
      "excerpt": "Supports VCTK (default) and LibriSpeech.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.930663519510811
      ],
      "excerpt": "Note that the speaker embedding separated gender almost perfectly (upload the vec and meta files to http://projector.tensorflow.org, then search for #f# or #m#). Also q(z|x) did slowly converge to the assumed uniform prior distribution. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8000170423071773
      ],
      "excerpt": "- At each frame of encoder output, instead of predicting a vector and find nearest neighbour and use the index as a one-hot categorical distribution, I make the last encoder channel = k, then apply a softmax so it represents a k-way softmax distribution, whose KL-divergence with a uniform prior is the same as a cross entropy loss. Add this loss in addition to the original 3 losses. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "TensorFlow implementation of VQ-VAE with WaveNet decoder, based on https://arxiv.org/abs/1711.00937 and https://arxiv.org/abs/1901.08810",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DongyaoZhu/VQ-VAE-WaveNet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Thu, 30 Dec 2021 09:30:08 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DongyaoZhu/VQ-VAE-WaveNet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "DongyaoZhu/VQ-VAE-WaveNet",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/DongyaoZhu/VQ-VAE-WaveNet/master/data/aishell_info/convert_aishell.sh",
      "https://raw.githubusercontent.com/DongyaoZhu/VQ-VAE-WaveNet/master/data/librispeech_info/convert_librispeech.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8603524511256402,
        0.8004696040558207,
        0.925619830435868,
        0.8803789792717766
      ],
      "excerpt": "Download data and put the unzipped folders 'VCTK-Corpus' or 'LibriSpeech' in the folder data. \nTo train from custom datasets, refer to dataset.py for making iterators. \nexample usage:  \npython3 train.py -dataset VCTK -length 6656 -batch 8 -step 100000 -save saved_model/weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925619830435868,
        0.9145146994275923
      ],
      "excerpt": "example usage: \npython3 generate.py -restore saved_model/weights-110640 -audio data/VCTK-Corpus/wav48/p225/p225_001.wav -speakers p225 p226 p227 p228 -mode sample \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925619830435868
      ],
      "excerpt": "example usage: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DongyaoZhu/VQ-VAE-WaveNet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "VQ-VAE-WaveNet",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "VQ-VAE-WaveNet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "DongyaoZhu",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DongyaoZhu/VQ-VAE-WaveNet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "TensorFlow r1.12 / r1.14, numpy, librosa, scipy, tqdm\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 44,
      "date": "Thu, 30 Dec 2021 09:30:08 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "vq-vae",
      "wavenet",
      "tensorflow",
      "voice-cloning"
    ],
    "technique": "GitHub API"
  }
}