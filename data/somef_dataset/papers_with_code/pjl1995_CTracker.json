{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n- Part of codes are borrowed from the [pytorch retinanet implementation](https://github.com/yhenon/pytorch-retinanet)\r\n- The NMS module used is from the [simpledet](https://github.com/TuSimple/simpledet)\r\n\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2007.14557"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nIf you find CTracker is useful in your project, please consider citing us:\r\n\r\n```BibTeX\r\n@inproceedings{peng2020ctracker,\r\n  title={Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End Joint Multiple-Object Detection and Tracking},\r\n  author={Peng, Jinlong and Wang, Changan and Wan, Fangbin and Wu, Yang and Wang, Yabiao and Tai, Ying and Wang, Chengjie and Li, Jilin and Huang, Feiyue and Fu, Yanwei},\r\n  booktitle={Proceedings of the European Conference on Computer Vision},\r\n  year={2020},\r\n}\r\n```\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{peng2020ctracker,\n  title={Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End Joint Multiple-Object Detection and Tracking},\n  author={Peng, Jinlong and Wang, Changan and Wan, Fangbin and Wu, Yang and Wang, Yabiao and Tai, Ying and Wang, Chengjie and Li, Jilin and Huang, Feiyue and Fu, Yanwei},\n  booktitle={Proceedings of the European Conference on Computer Vision},\n  year={2020},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9648896722461423
      ],
      "excerpt": "Official implementation in PyTorch of Chained-Tracker as described in Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End Joint Multiple-Object Detection and Tracking. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pjl1995/CTracker",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-10T03:22:00Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T03:47:53Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9650868277061295,
        0.9926473821422407,
        0.902640050718899
      ],
      "excerpt": "Official implementation in PyTorch of Chained-Tracker as described in Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End Joint Multiple-Object Detection and Tracking. \nThe introduction video of CTracker is uploaded to Youtube. \nThe codes is tested with PyTorch 0.4.1. It may not run with other versions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8403161022946263
      ],
      "excerpt": "We provide the two CSV files for MOT17 with codes in the CTRACKER_ROOT/data, you should copy them to MOT17_ROOT before starting training. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pjl1995/CTracker/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 42,
      "date": "Wed, 29 Dec 2021 00:09:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pjl1995/CTracker/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pjl1995/CTracker",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pjl1995/CTracker/master/lib/build.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Clone this repo into a directory named CTRACKER_ROOT\r\n* Install the required packages\r\n```\r\napt-get install tk-dev python-tk\r\n```\r\n* Install Python dependencies. We use python 3.6.5 and pytorch 0.4.1\r\n```\r\nconda create -n CTracker\r\nconda activate CTracker\r\nconda install pytorch=0.4.1 cuda90 -c pytorch\r\ncd ${CTRACKER_ROOT}\r\npip install -r requirements.txt\r\nsh lib/build.sh\r\n```\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src=\"demos/MOT17-03.gif\" width=\"400\"/>   <img src=\"demos/MOT17-07.gif\" width=\"400\"/>\r\n<img src=\"demos/MOT17-08.gif\" width=\"400\"/>   <img src=\"demos/MOT17-12.gif\" width=\"400\"/>\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8380864095764049,
        0.8275848300855979
      ],
      "excerpt": "The codes is tested with PyTorch 0.4.1. It may not run with other versions. \nMOT17 dataset can be downloaded at MOTChallenge. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8692508731223322
      ],
      "excerpt": "We uses two CSV files to organize the MOT17 dataset: one file containing annotations and one file containing a class name to ID mapping.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9175119970183425
      ],
      "excerpt": "The CSV file with annotations should contain one annotation per line. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8476999535042485,
        0.9246227682586091
      ],
      "excerpt": "The MOT17 CSV file can be generated by generate_csv.py: \npython generate_csv.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8555811575201808
      ],
      "excerpt": "The class name to ID mapping file should contain one mapping per line. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216270093103228
      ],
      "excerpt": "For example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.812373165122338,
        0.9316857653369599,
        0.8211051381599109
      ],
      "excerpt": "The network can be trained using the train.py script. For training on MOT17, use \nCUDA_VISIBLE_DEVICES=0 python train.py --root_path MOT17_ROOT --model_dir ./ctracker/ --depth 50 \nBy default, testing will start immediately after training finished. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pjl1995/CTracker/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "CTracker (ECCV2020 Spotlight)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CTracker",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pjl1995",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pjl1995/CTracker/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 235,
      "date": "Wed, 29 Dec 2021 00:09:38 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src=\"demos/MOT17-03.gif\" width=\"400\"/>   <img src=\"demos/MOT17-07.gif\" width=\"400\"/>\r\n<img src=\"demos/MOT17-08.gif\" width=\"400\"/>   <img src=\"demos/MOT17-12.gif\" width=\"400\"/>\r\n\r\n",
      "technique": "Header extraction"
    }
  ]
}