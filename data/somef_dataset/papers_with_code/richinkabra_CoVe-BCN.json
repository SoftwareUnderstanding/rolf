{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1708.00107",
      "https://arxiv.org/abs/1705.03122",
      "https://arxiv.org/abs/1408.5882",
      "https://arxiv.org/abs/1802.05365"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/richinkabra/CoVe-BCN",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing\nThanks for considering contributing!  We want AllenNLP to be the way to do cutting-edge NLP research, but we cannot\nget there without community support.\nHow Can I Contribute?\nDid you find a bug?\nFirst, do a quick search to see whether your issue has already been reported.\nIf your issue has already been reported, please comment on the existing issue.\nOtherwise, open a new GitHub issue.  Be sure to include a clear title\nand description.  The description should include as much relevant information as possible.  The description should\nexplain how to reproduce the erroneous behavior as well as the behavior you expect to see.  Ideally you would include a\ncode sample or an executable test case demonstrating the expected behavior.\nDid you write a fix for a bug?\nOpen a new GitHub pull request with the fix.  Make sure you have a clear\ndescription of the problem and the solution, and include a link to relevant issues.\nOnce your pull request is created, our continuous build system will check your pull request.  Continuous\nbuild will test that:\n\npytest All tests pass\npylint accepts the code style (our guidelines are based on PEP8)\nmypy typechecks the Python code\nThe docs can be generated successfully\nTest coverage remains high.  Please add unit tests so we maintain our code coverage.\n\nIf your code fails one of these checks, you will be expected to fix your pull request before it is considered.\nYou can run most of these tests locally with ./scripts/verify.py, which will be faster than waiting for\ncloud systems to run tests.\nDo you have a suggestion for an enhancement?\nWe use GitHub issues to track enhancement requests.  Before you create an enhancement request:\n\n\nMake sure you have a clear idea of the enhancement you would like.  If you have a vague idea, consider discussing\nit first on the users list.\n\n\nCheck the documentation to make sure your feature does not already exist.\n\n\nDo a quick search to see whether your enhancement has already been suggested.\n\n\nWhen creating your enhancement request, please:\n\n\nProvide a clear title and description.\n\n\nExplain why the enhancement would be useful.  It may be helpful to highlight the feature in other libraries.\n\n\nInclude code examples to demonstrate how the enhancement would be used.\n\n\nDo you have a new state-of-the-art model?\nWe are always looking for new models to add to our collection.  If you have trained a model and would like to include it in \nAllenNLP, please create a pull request that includes:\n\nAny code changes needed to support your new model.\nA link to the model itself.  Please do not check your model into the GitHub repository, but instead upload it in the \nPR conversation or provide a link to it at an external location.\n\nIn the description of your PR, please clearly explain the task your model performs along with precision and recall statistics \non an established dataset.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-23T15:12:17Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-23T17:03:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9900726753778705
      ],
      "excerpt": "Codebase to generate contextualized word vectors by training a sequence-to-sequence model based on a two-layer bidirectional LSTM for machine translation (MT) task. The hidden state output of the second layer of the machine translation model\u2019s encoder, called CoVe (Context Vectors) in McCann et al. 2017, is used to represent useful context-based information about text. To show the improvement in accuracy in downstream sentiment and question classification tasks (SST-2, SST-5, IMDb, TREC-6, and TREC-50 datasets), a Biattentive Classification Network (BCN) is used. The BCN results show that using CoVe has a higher test accuracy than random, GloVe, or character embeddings. A further improvement in accuracy is obtained if a weighted sum of all the hidden states of a several layer bidirectional LSTM encoder, called ELMo (Embeddings from Language Models) in Peters et al. 2017, is used. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/richinkabra/CoVe-BCN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 03:50:46 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/richinkabra/CoVe-BCN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "richinkabra/CoVe-BCN",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/richinkabra/CoVe-BCN/master/Sentence%20Classification/allennlp-forked/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/richinkabra/CoVe-BCN/master/Sentence%20Classification/allennlp-forked/scripts/mypy.sh",
      "https://raw.githubusercontent.com/richinkabra/CoVe-BCN/master/Sentence%20Classification/allennlp-forked/scripts/pylint.sh",
      "https://raw.githubusercontent.com/richinkabra/CoVe-BCN/master/Sentence%20Classification/allennlp-forked/scripts/compile_coref_data.sh",
      "https://raw.githubusercontent.com/richinkabra/CoVe-BCN/master/Sentence%20Classification/allennlp-forked/scripts/check_large_files.sh",
      "https://raw.githubusercontent.com/richinkabra/CoVe-BCN/master/Sentence%20Classification/allennlp-forked/scripts/ai2-internal/resumable_train.sh",
      "https://raw.githubusercontent.com/richinkabra/CoVe-BCN/master/MTLSTM/scripts/train_base_IWSLT.sh",
      "https://raw.githubusercontent.com/richinkabra/CoVe-BCN/master/MTLSTM/scripts/train_ext_multi30k.sh",
      "https://raw.githubusercontent.com/richinkabra/CoVe-BCN/master/MTLSTM/scripts/train_base_multi30k.sh"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/richinkabra/CoVe-BCN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jsonnet",
      "HTML",
      "Perl",
      "C",
      "Makefile",
      "Scilab",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'This is free and unencumbered software released into the public domain.\\n\\nAnyone is free to copy, modify, publish, use, compile, sell, or\\ndistribute this software, either in source code form or as a compiled\\nbinary, for any purpose, commercial or non-commercial, and by any\\nmeans.\\n\\nIn jurisdictions that recognize copyright laws, the author or authors\\nof this software dedicate any and all copyright interest in the\\nsoftware to the public domain. We make this dedication for the benefit\\nof the public at large and to the detriment of our heirs and\\nsuccessors. We intend this dedication to be an overt act of\\nrelinquishment in perpetuity of all present and future rights to this\\nsoftware under copyright law.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\\nIN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR\\nOTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\\nARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\\nOTHER DEALINGS IN THE SOFTWARE.\\n\\nFor more information, please refer to https://unlicense.org/\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Transfer Learning in NLP - Contextualized Word Vectors",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CoVe-BCN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "richinkabra",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/richinkabra/CoVe-BCN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 03:50:46 GMT"
    },
    "technique": "GitHub API"
  }
}