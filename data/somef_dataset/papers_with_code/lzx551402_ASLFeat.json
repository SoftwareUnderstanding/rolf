{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n1. The backbone networks and the learning scheme are heavily borrowed from [D2-Net](https://github.com/mihaidusmanu/d2-net).\r\n\r\n2. We thank for the authors of [R2D2](https://github.com/naver/r2d2) for sharing their evaluation results on HPatches that helped us plot Fig.1. The updated results of R2D2 are even more excited.\r\n\r\n3. We refer to the public implementation of [SuperPoint](https://github.com/rpautrat/SuperPoint) for organizing the code and implementing the evaluation metrics.\r\n\r\n4. We implement the modulated DCN referring to [this](https://github.com/DHZS/tf-deformable-conv-layer/blob/master/nets/deformable_conv_layer.py). The current implementation is not efficient, and we expect a native implementation in TensorFlow to be available in the future. (update: this [contribution](https://github.com/tensorflow/addons/pull/1129) would be extremely useful once it is integrated!)\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2003.10071",
      "https://arxiv.org/abs/2002.10857"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{luo2020aslfeat,\n  title={ASLFeat: Learning Local Features of Accurate Shape and Localization},\n  author={Luo, Zixin and Zhou, Lei and Bai, Xuyang and Chen, Hongkai and Zhang, Jiahui and Yao, Yao and Li, Shiwei and Fang, Tian and Quan, Long},\n  journal={Computer Vision and Pattern Recognition (CVPR)},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9972362079258619
      ],
      "excerpt": "TensorFlow implementation of ASLFeat for CVPR'20 paper \"ASLFeat: Learning Local Features of Accurate Shape and Localization\", by Zixin Luo, Lei Zhou, Xuyang Bai, Hongkai Chen, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang and Long Quan. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9994380975146042
      ],
      "excerpt": "Please cite libvot if you find it useful. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lzx551402/ASLFeat",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-12T07:57:00Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T09:09:31Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9487853446858024,
        0.9562414221865928,
        0.91078433701825
      ],
      "excerpt": "TensorFlow implementation of ASLFeat for CVPR'20 paper \"ASLFeat: Learning Local Features of Accurate Shape and Localization\", by Zixin Luo, Lei Zhou, Xuyang Bai, Hongkai Chen, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang and Long Quan. \nThis paper presents a joint learning framework of local feature detectors and descriptors. Two aspects are addressed to learn a powerful feature: 1) shape-awareness of feature points, and 2) the localization accuracy of keypoints. If you find this project useful, please cite: \nWe here release ASLFeat with post-CVPR update, which we find to perform consistently better among the evaluations. The model can be accessed by: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9607542723392347,
        0.9569968160445805
      ],
      "excerpt": "On HPatches dataset, the MMA@3 is improved from 72.29 to 74.31 in single scale prediction, while the multi-scale prediction now achieves 75.26. The major difference comes from 1) using blended images and rendered depths, which is proposed in BlendedMVS and integrated in GL3D, 2) using circle loss and 3) conducting early stopping. Details can be found in the updated arxiv paper. The above implementation is also available in TFMatch. \nWe release the training scripts in a separate project, TFMatch, which also contains our previous research works (GeoDesc, ECCV'18 and ContextDesc, CVPR'19). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9536596029461112
      ],
      "excerpt": "At the end of running, we report the average number of features, repeatability, precision, matching score, recall and mean matching accuracy (a.k.a. MMA). The evaluation results will be displayed as: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8221330449564902
      ],
      "excerpt": "0 /data/hpatches-sequences-release/v_abstract \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8221330449564902
      ],
      "excerpt": "1 /data/hpatches-sequences-release/v_adam \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8976059764188804
      ],
      "excerpt": "When multi-scale (MS) inference is enabled, the results become: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9960518501266833
      ],
      "excerpt": "The results for repeatability and matching score is different from what we have reported in the paper, as we now apply a symmetric check when counting the number of covisible features (referring to SuperPoint). This change may not influence the conclusion in the section of ablation study, but would be useful for making comparision with other relavant papers. We thank for Sida Peng for pointing this out when reproducing this work. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8204633463655914
      ],
      "excerpt": "The cached results of ASLFeat can be reached here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9189749255119013
      ],
      "excerpt": "Finally, refer to the evaluation script to generate and submit the results to the challenge website. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.922953744572954
      ],
      "excerpt": "We use Bag-of-Words (BoW) method for image retrieval. To do so, clone and compile libvot: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.917132917453234
      ],
      "excerpt": "To extract the features. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of CVPR'20 paper - ASLFeat: Learning Local Features of Accurate Shape and Localization",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lzx551402/ASLFeat/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 24,
      "date": "Wed, 29 Dec 2021 14:37:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lzx551402/ASLFeat/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lzx551402/ASLFeat",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8478999832311983
      ],
      "excerpt": "cd /local/aslfeat &amp;&amp; python hseq_eval.py --config configs/hseq_eval.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9893272198983933
      ],
      "excerpt": "git clone https://github.com/lzx551402/FM-Bench.git \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8478999832311983
      ],
      "excerpt": "cd /local/aslfeat &amp;&amp; python evaluations.py --config configs/fmbench_eval.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8478999832311983
      ],
      "excerpt": "cd /local/aslfeat &amp;&amp; python evaluations.py --config configs/aachen_eval.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902627162932362,
        0.9906248903846466
      ],
      "excerpt": "mkdir Oxford5k &amp;&amp; \\ \ncd Oxford5k &amp;&amp; \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8478999832311983
      ],
      "excerpt": "cd /local/aslfeat &amp;&amp; python evaluations.py --config configs/oxford_eval.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466,
        0.9893272198983933,
        0.9620406150340135,
        0.9944375700570437
      ],
      "excerpt": "cd Oxford5k &amp;&amp; \\ \ngit clone https://github.com/hlzz/libvot.git &amp;&amp; \\ \nmkdir build &amp;&amp; \\ \ncd build &amp;&amp; \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd Oxford5k &amp;&amp; \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8478999832311983
      ],
      "excerpt": "cd /local/aslfeat &amp;&amp; python evaluations.py --config configs/imw2020_eval.yaml \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8313940312814669
      ],
      "excerpt": "cd /local/aslfeat &amp;&amp; python hseq_eval.py --config configs/hseq_eval.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8313940312814669
      ],
      "excerpt": "cd /local/aslfeat &amp;&amp; python evaluations.py --config configs/fmbench_eval.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8313940312814669
      ],
      "excerpt": "cd /local/aslfeat &amp;&amp; python evaluations.py --config configs/aachen_eval.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8744163264716114
      ],
      "excerpt": "Take Oxford Buildings dataset as an example. First, download the evaluation data and (parsed) groundtruth files: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8313940312814669
      ],
      "excerpt": "cd /local/aslfeat &amp;&amp; python evaluations.py --config configs/oxford_eval.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8536842656864143
      ],
      "excerpt": "python benchmark.py --method_name aslfeat_ms \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8397453974429769
      ],
      "excerpt": "cda /local/aslfeat &amp;&amp; python evaluations.py --config configs/eth_eval.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8551173607683867
      ],
      "excerpt": "Download the data (validation/test) Link, then configure configs/imw2020_eval.yaml, finally call: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8313940312814669
      ],
      "excerpt": "cd /local/aslfeat &amp;&amp; python evaluations.py --config configs/imw2020_eval.yaml \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lzx551402/ASLFeat/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Zixin Luo\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ASLFeat implementation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ASLFeat",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lzx551402",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lzx551402/ASLFeat/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nPlease use Python 3.7, install NumPy, OpenCV (3.4.2) and TensorFlow (1.15.2). Refer to [requirements.txt](requirements.txt) for some other dependencies.\r\n\r\nIf you are using conda, you may configure ASLFeat as:\r\n\r\n```bash\r\nconda create --name aslfeat python=3.7 -y && \\\r\npip install -r requirements.txt && \\\r\nconda activate aslfeat\r\n```\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 213,
      "date": "Wed, 29 Dec 2021 14:37:31 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nClone the repo and download the pretrained model:\r\n```bash\r\ngit clone https://github.com/lzx551402/aslfeat.git && \\\r\ncd ASLFeat/pretrained && \\\r\nwget https://research.altizure.com/data/aslfeat_models/aslfeat.tar && \\\r\ntar -xvf aslfeat.tar\r\n```\r\n\r\nA quick example for image matching can be called by:\r\n\r\n```bash\r\ncd /local/aslfeat && python image_matching.py --config configs/matching_eval.yaml\r\n```\r\n\r\nYou will be able to see the matching results by displaying [disp.jpg](imgs/disp.jpg).\r\n\r\nYou may configure ``configs/matching_eval.yaml`` to test images of your own.\r\n\r\n",
      "technique": "Header extraction"
    }
  ]
}