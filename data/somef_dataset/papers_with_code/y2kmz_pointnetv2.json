{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1612.00593",
      "https://arxiv.org/abs/1710.04954\" target=\"_blank\">PCPNET: Learning Local Shape Properties from Raw Point Clouds</a> by Guerrero et al. (arXiv",
      "https://arxiv.org/abs/1711.06396\" target=\"_blank\">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a> by Zhou et al. from Apple (arXiv",
      "https://arxiv.org/abs/1711.08488\" target=\"_blank\">Frustum PointNets for 3D Object Detection from RGB-D Data</a> by Qi et al. (arXiv",
      "https://arxiv.org/abs/1612.00593"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find our work useful in your research, please consider citing:\n\n\t@article{qi2016pointnet,\n\t  title={PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},\n\t  author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},\n\t  journal={arXiv preprint arXiv:1612.00593},\n\t  year={2016}\n\t}\n   \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{qi2016pointnet,\n  title={PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},\n  author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},\n  journal={arXiv preprint arXiv:1612.00593},\n  year={2016}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/y2kmz/pointnetv2",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-27T16:17:12Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-22T02:17:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593), which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.\n\nPoint cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input.  Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective.\n\nIn this repository, we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes, as well as for training a part segmentation network on ShapeNet Part dataset.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "pointnet\u4ee3\u7801\uff0c\u5347\u7ea7\u81f3\u9002\u7528\u4e8eTF2.1\u548cpython3.7\uff0c\u52a0\u5165\u5927\u91cf\u7528\u4e8e\u5b66\u4e60\u7684\u6ce8\u91ca\u3002",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/y2kmz/pointnetv2/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Wed, 22 Dec 2021 23:04:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/y2kmz/pointnetv2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "y2kmz/pointnetv2",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/y2kmz/pointnetv2/master/part_seg/download_data.sh",
      "https://raw.githubusercontent.com/y2kmz/pointnetv2/master/sem_seg/download_data.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Install <a href=\"https://www.tensorflow.org/get_started/os_setup\" target=\"_blank\">TensorFlow</a>. You may also need to install h5py. The code has been tested with Python 2.7, TensorFlow 1.0.1, CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04.\n\nIf you are using PyTorch, you can find a third-party pytorch implementation <a href=\"https://github.com/fxia22/pointnet.pytorch\" target=\"_blank\">here</a>.\n\nTo install h5py for Python:\n```bash\nsudo apt-get install libhdf5-dev\nsudo pip install h5py\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Created by <a href=\"http://charlesrqi.com\" target=\"_blank\">Charles R. Qi</a>, <a href=\"http://ai.stanford.edu/~haosu/\" target=\"_blank\">Hao Su</a>, <a href=\"http://cs.stanford.edu/~kaichun/\" target=\"_blank\">Kaichun Mo</a>, <a href=\"http://geometry.stanford.edu/member/guibas/\" target=\"_blank\">Leonidas J. Guibas</a> from Stanford University.\n\n![prediction example](https://github.com/charlesq34/pointnet/blob/master/doc/teaser.png)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9906248903846466,
        0.9023697225149864
      ],
      "excerpt": "cd part_seg \nsh download_data.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.860197485310198
      ],
      "excerpt": "To train a model for object part segmentation, firstly download the data: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/y2kmz/pointnetv2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/y2kmz/pointnetv2/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.\\n\\nCopyright (c) 2017, Geometric Computation Group of Stanford University\\n\\nThe MIT License (MIT)\\n\\nCopyright (c) 2017 Charles R. Qi\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\nPointNet\\xef\\xbc\\x9a\\xe9\\x92\\x88\\xe5\\xaf\\xb93D\\xe5\\x88\\x86\\xe7\\xb1\\xbb\\xe5\\x92\\x8c\\xe5\\x88\\x86\\xe5\\x89\\xb2\\xe7\\x9a\\x84\\xe7\\x82\\xb9\\xe9\\x9b\\x86\\xe6\\xb7\\xb1\\xe5\\xba\\xa6\\xe5\\xad\\xa6\\xe4\\xb9\\xa0\\xe3\\x80\\x82\\n\\n\\xe6\\x96\\xaf\\xe5\\x9d\\xa6\\xe7\\xa6\\x8f\\xe5\\xa4\\xa7\\xe5\\xad\\xa6\\xe5\\x87\\xa0\\xe4\\xbd\\x95\\xe8\\xae\\xa1\\xe7\\xae\\x97\\xe5\\xb0\\x8f\\xe7\\xbb\\x84\\xef\\xbc\\x88c\\xef\\xbc\\x892017\\xe7\\x89\\x88\\xe6\\x9d\\x83\\xe6\\x89\\x80\\xe6\\x9c\\x89\\n\\nMIT\\xe8\\xae\\xb8\\xe5\\x8f\\xaf\\xe8\\xaf\\x81\\xef\\xbc\\x88MIT\\xef\\xbc\\x89\\n\\n\\xe7\\x89\\x88\\xe6\\x9d\\x83\\xe6\\x89\\x80\\xe6\\x9c\\x89\\xef\\xbc\\x88c\\xef\\xbc\\x892017 Charles R.Qi\\n\\n\\xe7\\x89\\xb9\\xe6\\xad\\xa4\\xe6\\x8e\\x88\\xe4\\xba\\x88\\xe8\\x8e\\xb7\\xe5\\xbe\\x97\\xe5\\x89\\xaf\\xe6\\x9c\\xac\\xe7\\x9a\\x84\\xe4\\xbb\\xbb\\xe4\\xbd\\x95\\xe4\\xba\\xba\\xe5\\x85\\x8d\\xe8\\xb4\\xb9\\xe7\\x9a\\x84\\xe8\\xae\\xb8\\xe5\\x8f\\xaf\\n\\xe8\\xbd\\xaf\\xe4\\xbb\\xb6\\xe5\\x92\\x8c\\xe7\\x9b\\xb8\\xe5\\x85\\xb3\\xe6\\x96\\x87\\xe6\\xa1\\xa3\\xe6\\x96\\x87\\xe4\\xbb\\xb6\\xef\\xbc\\x88\\xe4\\xbb\\xa5\\xe4\\xb8\\x8b\\xe7\\xae\\x80\\xe7\\xa7\\xb0\\xe2\\x80\\x9c\\xe8\\xbd\\xaf\\xe4\\xbb\\xb6\\xe2\\x80\\x9d\\xef\\xbc\\x89\\xe7\\x9a\\x84\\xe4\\xba\\xa4\\xe6\\x98\\x93\\n\\xe5\\x9c\\xa8\\xe8\\xbd\\xaf\\xe4\\xbb\\xb6\\xe4\\xb8\\xad\\xe4\\xb8\\x8d\\xe5\\x8f\\x97\\xe9\\x99\\x90\\xe5\\x88\\xb6\\xef\\xbc\\x8c\\xe5\\x8c\\x85\\xe6\\x8b\\xac\\xe4\\xbd\\x86\\xe4\\xb8\\x8d\\xe9\\x99\\x90\\xe4\\xba\\x8e\\xe6\\x9d\\x83\\xe5\\x88\\xa9\\n\\xe4\\xbd\\xbf\\xe7\\x94\\xa8\\xef\\xbc\\x8c\\xe5\\xa4\\x8d\\xe5\\x88\\xb6\\xef\\xbc\\x8c\\xe4\\xbf\\xae\\xe6\\x94\\xb9\\xef\\xbc\\x8c\\xe5\\x90\\x88\\xe5\\xb9\\xb6\\xef\\xbc\\x8c\\xe5\\x8f\\x91\\xe5\\xb8\\x83\\xef\\xbc\\x8c\\xe5\\x88\\x86\\xe5\\x8f\\x91\\xef\\xbc\\x8c\\xe5\\x86\\x8d\\xe8\\xae\\xb8\\xe5\\x8f\\xaf\\xe5\\x92\\x8c/\\xe6\\x88\\x96\\xe5\\x87\\xba\\xe5\\x94\\xae\\n\\xe6\\x9c\\xac\\xe8\\xbd\\xaf\\xe4\\xbb\\xb6\\xe7\\x9a\\x84\\xe5\\x89\\xaf\\xe6\\x9c\\xac\\xef\\xbc\\x8c\\xe5\\xb9\\xb6\\xe5\\x85\\x81\\xe8\\xae\\xb8\\xe6\\x9c\\xac\\xe8\\xbd\\xaf\\xe4\\xbb\\xb6\\xe6\\x89\\x80\\xe9\\x92\\x88\\xe5\\xaf\\xb9\\xe7\\x9a\\x84\\xe4\\xba\\xba\\n\\xe5\\x85\\xb7\\xe5\\xa4\\x87\\xe4\\xbb\\xa5\\xe4\\xb8\\x8b\\xe6\\x9d\\xa1\\xe4\\xbb\\xb6\\xef\\xbc\\x9a\\n\\n\\xe4\\xbb\\xa5\\xe4\\xb8\\x8a\\xe7\\x89\\x88\\xe6\\x9d\\x83\\xe5\\xa3\\xb0\\xe6\\x98\\x8e\\xe5\\x92\\x8c\\xe6\\xad\\xa4\\xe8\\xae\\xb8\\xe5\\x8f\\xaf\\xe5\\xa3\\xb0\\xe6\\x98\\x8e\\xe5\\xba\\x94\\xe5\\x8c\\x85\\xe5\\x90\\xab\\xe5\\x9c\\xa8\\xe6\\x89\\x80\\xe6\\x9c\\x89\\n\\xe5\\xa4\\x8d\\xe5\\x88\\xb6\\xe6\\x88\\x96\\xe5\\xae\\x9e\\xe8\\xb4\\xa8\\xe6\\x80\\xa7\\xe7\\x9a\\x84\\xe8\\xbd\\xaf\\xe4\\xbb\\xb6\\xe9\\x83\\xa8\\xe5\\x88\\x86\\xe3\\x80\\x82\\n\\n\\xe6\\x9c\\xac\\xe8\\xbd\\xaf\\xe4\\xbb\\xb6\\xe6\\x8c\\x89\\xe2\\x80\\x9c\\xe5\\x8e\\x9f\\xe6\\xa0\\xb7\\xe2\\x80\\x9d\\xe6\\x8f\\x90\\xe4\\xbe\\x9b\\xef\\xbc\\x8c\\xe4\\xb8\\x8d\\xe6\\x8f\\x90\\xe4\\xbe\\x9b\\xe4\\xbb\\xbb\\xe4\\xbd\\x95\\xe5\\xbd\\xa2\\xe5\\xbc\\x8f\\xe7\\x9a\\x84\\xe6\\x98\\x8e\\xe7\\xa4\\xba\\xe6\\x88\\x96\\xe6\\x98\\x8e\\xe7\\xa4\\xba\\xe4\\xbf\\x9d\\xe8\\xaf\\x81\\xe3\\x80\\x82\\n\\xe6\\x9a\\x97\\xe7\\xa4\\xba\\xef\\xbc\\x88\\xe5\\x8c\\x85\\xe6\\x8b\\xac\\xe4\\xbd\\x86\\xe4\\xb8\\x8d\\xe9\\x99\\x90\\xe4\\xba\\x8e\\xe9\\x80\\x82\\xe9\\x94\\x80\\xe6\\x80\\xa7\\xe7\\x9a\\x84\\xe4\\xbf\\x9d\\xe8\\xaf\\x81\\xef\\xbc\\x89\\xef\\xbc\\x8c\\n\\xe9\\x80\\x82\\xe7\\x94\\xa8\\xe4\\xba\\x8e\\xe7\\x89\\xb9\\xe5\\xae\\x9a\\xe7\\x9b\\xae\\xe7\\x9a\\x84\\xe5\\x92\\x8c\\xe9\\x9d\\x9e\\xe4\\xbe\\xb5\\xe6\\x9d\\x83\\xe3\\x80\\x82\\xe5\\x9c\\xa8\\xe4\\xbb\\xbb\\xe4\\xbd\\x95\\xe6\\x83\\x85\\xe5\\x86\\xb5\\xe4\\xb8\\x8b\\xe9\\x83\\xbd\\xe4\\xb8\\x8d\\xe4\\xbc\\x9a\\n\\xe4\\xbd\\x9c\\xe8\\x80\\x85\\xe6\\x88\\x96\\xe7\\x89\\x88\\xe6\\x9d\\x83\\xe6\\x8c\\x81\\xe6\\x9c\\x89\\xe4\\xba\\xba\\xe5\\xaf\\xb9\\xe4\\xbb\\xbb\\xe4\\xbd\\x95\\xe7\\xb4\\xa2\\xe8\\xb5\\x94\\xef\\xbc\\x8c\\xe6\\x8d\\x9f\\xe5\\xae\\xb3\\xe6\\x88\\x96\\xe5\\x85\\xb6\\xe4\\xbb\\x96\\xe8\\xb4\\xa3\\xe4\\xbb\\xbb\\n\\xe6\\x97\\xa0\\xe8\\xae\\xba\\xe6\\x98\\xaf\\xe7\\x94\\xb1\\xe4\\xba\\x8e\\xe5\\x90\\x88\\xe5\\x90\\x8c\\xef\\xbc\\x8c\\xe4\\xbe\\xb5\\xe6\\x9d\\x83\\xe6\\x88\\x96\\xe5\\x85\\xb6\\xe4\\xbb\\x96\\xe5\\xbd\\xa2\\xe5\\xbc\\x8f\\xe7\\x9a\\x84\\xe8\\xaf\\x89\\xe8\\xae\\xbc\\xe8\\x80\\x8c\\xe5\\xbc\\x95\\xe8\\xb5\\xb7\\xe7\\x9a\\x84\\xe8\\xb4\\xa3\\xe4\\xbb\\xbb\\xef\\xbc\\x8c\\n\\xe4\\xb8\\x8e\\xe8\\xbd\\xaf\\xe4\\xbb\\xb6\\xe6\\x88\\x96\\xe8\\xbd\\xaf\\xe4\\xbb\\xb6\\xe7\\x9a\\x84\\xe4\\xbd\\xbf\\xe7\\x94\\xa8\\xe6\\x88\\x96\\xe5\\x85\\xb6\\xe4\\xbb\\x96\\xe4\\xba\\xa4\\xe6\\x98\\x93\\xe6\\x97\\xa0\\xe5\\x85\\xb3\\xe6\\x88\\x96\\xe4\\xb8\\x8e\\xe4\\xb9\\x8b\\xe6\\x9c\\x89\\xe5\\x85\\xb3\\n\\xe8\\xbd\\xaf\\xe4\\xbb\\xb6\\xe3\\x80\\x82'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# PointNet: *Deep Learning on Point Sets for 3D Classification and Segmentation*",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pointnetv2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "y2kmz",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/y2kmz/pointnetv2/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Wed, 22 Dec 2021 23:04:59 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To train a model to classify point clouds sampled from 3D shapes:\n\n    python train.py\n\nLog files and network parameters will be saved to `log` folder in default. Point clouds of <a href=\"http://modelnet.cs.princeton.edu/\" target=\"_blank\">ModelNet40</a> models in HDF5 files will be automatically downloaded (416MB) to the data folder. Each point cloud contains 2048 points uniformly sampled from a shape surface. Each cloud is zero-mean and normalized into an unit sphere. There are also text files in `data/modelnet40_ply_hdf5_2048` specifying the ids of shapes in h5 files.\n\nTo see HELP for the training script:\n\n    python train.py -h\n\nWe can use TensorBoard to view the network architecture and monitor the training progress.\n\n    tensorboard --logdir log\n\nAfter the above training, we can evaluate the model and output some visualizations of the error cases.\n\n    python evaluate.py --visu\n\nPoint clouds that are wrongly classified will be saved to `dump` folder in default. We visualize the point cloud by rendering it into three-view images.\n\nIf you'd like to prepare your own data, you can refer to some helper functions in `utils/data_prep_util.py` for saving and loading HDF5 files.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "* <a href=\"http://stanford.edu/~rqi/pointnet2/\" target=\"_blank\">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a> by Qi et al. (NIPS 2017) A hierarchical feature learning framework on point clouds. The PointNet++ architecture applies PointNet recursively on a nested partitioning of the input point set. It also proposes novel layers for point clouds with non-uniform densities.\n* <a href=\"http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w13/Engelmann_Exploring_Spatial_Context_ICCV_2017_paper.pdf\" target=\"_blank\">Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</a> by Engelmann et al. (ICCV 2017 workshop). This work extends PointNet for large-scale scene segmentation.\n* <a href=\"https://arxiv.org/abs/1710.04954\" target=\"_blank\">PCPNET: Learning Local Shape Properties from Raw Point Clouds</a> by Guerrero et al. (arXiv). The work adapts PointNet for local geometric properties (e.g. normal and curvature) estimation in noisy point clouds.\n* <a href=\"https://arxiv.org/abs/1711.06396\" target=\"_blank\">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a> by Zhou et al. from Apple (arXiv) This work studies 3D object detection using LiDAR point clouds. It splits space into voxels, use PointNet to learn local voxel features and then use 3D CNN for region proposal, object classification and 3D bounding box estimation.\n* <a href=\"https://arxiv.org/abs/1711.08488\" target=\"_blank\">Frustum PointNets for 3D Object Detection from RGB-D Data</a> by Qi et al. (arXiv) A novel framework for 3D object detection with RGB-D data. The method proposed has achieved first place on KITTI 3D object detection benchmark on all categories (last checked on 11/30/2017).\n",
      "technique": "Header extraction"
    }
  ]
}