{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2003.03384",
      "https://arxiv.org/abs/2003.03384",
      "https://arxiv.org/abs/2003.03384",
      "https://arxiv.org/abs/2003.03384",
      "https://arxiv.org/abs/2003.03384"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use the code in your research, please cite:\n\n```\n@article{real2020automl,\n  title={AutoML-Zero: Evolving Machine Learning Algorithms From Scratch},\n  author={Real, Esteban and Liang, Chen and So, David R and Le, Quoc V},\n  journal={arXiv preprint arXiv:2003.03384},\n  year={2020}\n}\n```\n\n&nbsp;\n\n<sup><sub>\nSearch keywords: machine learning, neural networks, evolution,\nevolutionary algorithms, regularized evolution, program synthesis,\narchitecture search, NAS, neural architecture search,\nneuro-architecture search, AutoML, AutoML-Zero, algorithm search,\nmeta-learning, genetic algorithms, genetic programming, neuroevolution,\nneuro-evolution.\n</sub></sup>\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{real2020automl,\n  title={AutoML-Zero: Evolving Machine Learning Algorithms From Scratch},\n  author={Real, Esteban and Liang, Chen and So, David R and Le, Quoc V},\n  journal={arXiv preprint arXiv:2003.03384},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9558636900827701,
        0.8654671031158477
      ],
      "excerpt": "Open source code for the paper: \\\"AutoML-Zero: Evolving Machine Learning Algorithms From Scratch\" \n| Introduction | Quick Demo| Reproducing Search Baselines | Citation | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/KnollFrank/automl_zero",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-06T09:57:12Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-29T23:51:31Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9905837445383023,
        0.9931500892600611,
        0.8281193878915896
      ],
      "excerpt": "AutoML-Zero aims to automatically discover computer programs that can solve machine learning tasks, starting from empty or random programs and using only basic math operations. The goal is to simultaneously search for all aspects of an ML algorithm&mdash;including the model structure and the learning strategy&mdash;while employing minimal human bias. \nDespite AutoML-Zero's challenging search space, evolutionary search shows promising results by discovering linear regression with gradient descent, 2-layer neural networks with backpropagation, and even algorithms that surpass hand designed baselines of comparable complexity. The figure above shows an example sequence of discoveries from one of our experiments, evolving algorithms to solve binary classification tasks. Notably, the evolved algorithms can be interpreted. Below is an analysis of the best evolved algorithm: the search process \"invented\" techniques like bilinear interactions, weight averaging, normalized gradient, and data augmentation (by adding noise to the inputs). \nMore examples, analysis, and details can be found in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8867798377136061
      ],
      "excerpt": "Section 9 (\"Baselines\") with the \"Basic\" method on 1 process (1 CPU). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9815383007388166,
        0.9298743859036851
      ],
      "excerpt": "platform. A platform-agnostic description of what we did is given in our paper. \nNote we left out of this directory upgrades for the \"Full\" method that are \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/KnollFrank/automl_zero/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 03:46:09 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/KnollFrank/automl_zero/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "KnollFrank/automl_zero",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/KnollFrank/automl_zero/master/run_demo_sort_task_loop.sh",
      "https://raw.githubusercontent.com/KnollFrank/automl_zero/master/run_demo_sort_task.sh",
      "https://raw.githubusercontent.com/KnollFrank/automl_zero/master/run_demo.sh",
      "https://raw.githubusercontent.com/KnollFrank/automl_zero/master/run_integration_test_linear.sh",
      "https://raw.githubusercontent.com/KnollFrank/automl_zero/master/setup.sh",
      "https://raw.githubusercontent.com/KnollFrank/automl_zero/master/run_integration_tests.sh",
      "https://raw.githubusercontent.com/KnollFrank/automl_zero/master/run_integration_test_nonlinear.sh",
      "https://raw.githubusercontent.com/KnollFrank/automl_zero/master/run_integration_test_projected_binary_datasets.sh",
      "https://raw.githubusercontent.com/KnollFrank/automl_zero/master/run_demo_unit_test_custom_task.sh",
      "https://raw.githubusercontent.com/KnollFrank/automl_zero/master/run_baseline.sh",
      "https://raw.githubusercontent.com/KnollFrank/automl_zero/master/run_demo_unit_test_fixed_task.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9552862416559362
      ],
      "excerpt": "First install bazel, following the instructions here (bazel>=2.2.0 and g++>=9 are required), then follow the instructions below to reproduce the results in Supplementary \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.819811067898397,
        0.9246227682586091
      ],
      "excerpt": "First, generate the projected binary CIFAR10 datasets by running \npython generate_datasets.py --data_dir=binary_cifar10_data \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/KnollFrank/automl_zero/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Python",
      "Shell",
      "Starlark"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "AutoML-Zero",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "automl_zero",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "KnollFrank",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/KnollFrank/automl_zero/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Mon, 27 Dec 2021 03:46:09 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "As a miniature \"AutoML-Zero\" experiment, let's try to automatically discover programs to solve linear regression tasks.\n\nTo get started, first install `bazel` following instructions [here](https://docs.bazel.build/versions/master/install.html) (bazel>=2.2.0 and g++>=9 are required), then run the demo with:\n\n```\ngit clone https://github.com/google-research/google-research.git\ncd google-research/automl_zero\n./run_demo.sh\n```\n\nThis script runs evolutionary search on 10 linear tasks (*T<sub>search</sub>* in the paper). After each experiment, it evaluates the best algorithm discovered on 100 new linear tasks (*T<sub>select</sub>* in the paper). Once an algorithm attains a fitness (1 - RMS error) greater than 0.9999, it is selected for a final evaluation on 100 *unseen tasks*. To conclude, the demo prints the results of the final evaluation and shows the code for the automatically discovered algorithm.\n\nTo make this demo quick, we use a much smaller search space than in the [paper](https://arxiv.org/abs/2003.03384): only the math operations necessary to implement linear regression are allowed and the programs are constrained to a short, fixed length. Even with these limitations, the search space is quite sparse, as random search experiments show that only ~1 in 10<sup>8</sup> algorithms in the space can solve the tasks with the required accuracy. Nevertheless, this demo typically discovers programs similar to linear regression by gradient descent in under 5 minutes using 1 CPU (Note that the runtime may vary due to random seeds and hardware). We have seen similar and more interesting discoveries in the unconstrained search space (see more details in the [paper](https://arxiv.org/abs/2003.03384)).\n\nYou can compare the automatically discovered algorithm with the solution from a human ML researcher (one of the authors):\n\n```\ndef Setup():\n  s2 = 0.001  #: Init learning rate.\n\ndef Predict():  #: v0 = features\n  s1 = dot(v0, v1)  #: Apply weights\n\ndef Learn():  #: v0 = features; s0 = label\n  s3 = s0 - s1  #: Compute error.\n  s4 = s3 * s2  #: Apply learning rate.\n  v2 = v0 * s4  #: Compute gradient.\n  v1 = v1 + v2  #: Update weights.\n```\n\nIn this human designed program, the ```Setup``` function establishes a learning rate, the ```Predict``` function applies a set of weights to the inputs, and the ```Learn``` function corrects the weights in the opposite direction to the gradient; in other words, a linear regressor trained with gradient descent. The evolved programs may look different even if they have the same functionality due to redundant instructions and different ordering, which can make them challenging to interpret. See more details about how we address these problems in the [paper](https://github.com/google-research/google-research/tree/master/automl_zero#automl-zero).\n\n&nbsp;\n\n",
      "technique": "Header extraction"
    }
  ]
}