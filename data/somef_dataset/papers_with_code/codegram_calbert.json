{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is part of the applied research we do at [Codegram](https://codegram.com) (who is to thank for the time and the compute!).\n\nThis would have been a ton of pain to build without [Huggingface](http://huggingface.co)'s powerful [transformers](http://github.com/huggingface/transformers) and [tokenizers](http://github.com/huggingface/tokenizers) libraries. Thank you for making NLP actually nice to work with!\n\nAlso, thanks to Google Research for creating and open-sourcing [ALBERT](https://github.com/google-research/ALBERT) in the first place.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.855313318686353
      ],
      "excerpt": ": 1-hot encode and add special starting and end tokens \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/codegram/calbert",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-11T11:05:47Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-24T13:46:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9676939583262685,
        0.9700910786139088
      ],
      "excerpt": "A Catalan ALBERT (A Lite BERT), Google's take on self-supervised learning of language representations. \nIt's trained on a corpus of 19.557.475 sentence pairs (containing 729 million unique words) extracted from the Catalan subset of Inria's OSCAR dataset. We use the a validation set of 833.259 sentence pairs to evaluate the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8458928283165856,
        0.9450369801536308
      ],
      "excerpt": "They are available at HuggingFace's Model Hub page \n| Model                               | Arch.          | Training data          | Play with it                                                              | Visualize it                                                                                                                                                      | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9155015792111943,
        0.9806476015915715
      ],
      "excerpt": "| codegram / calbert-tiny-uncased | Tiny (uncased) | OSCAR (4.3 GB of text) | Card on Model Hub | Visualize in exBERT | \n| codegram / calbert-base-uncased | Base (uncased) | OSCAR (4.3 GB of text) | Card on Model Hub | Visualize in exBERT | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9527638904963389
      ],
      "excerpt": "Another use case is Natural Language Understanding --using these vectors as abstract representations of documents/sentences that can be used as input to other downstream models such as classifiers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8940399283240205
      ],
      "excerpt": ": Tokenize in sub-words with SentencePiece \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8972329959498299
      ],
      "excerpt": ": 1-hot encode and add special starting and end tokens \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "embeddings, _ = model(encoded_sentence) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9838034670753307,
        0.9808156398651716,
        0.9577477852251941
      ],
      "excerpt": "ALBERT is a Language Model, that is, a neural network that can learn sequences with certain structure, such as sentences in natural language (but not only natural language!). \nBut how do they learn language? Different language models are trained with different pretext tasks, namely challenges that you give them so that they can learn how language works. The idea is that in order to get reaosnably good at this one task they must indirectly learn the grammar of the language, and even its semantics and style. \nTraditional (also known as causal) language models are usually trained with the task of predicting the next word in a sequence, like this: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9972775896755381
      ],
      "excerpt": "However, ALBERT is of another family called masked language models. In this family, the pretext task they have to learn is similar, but instead of always predicting the last word in a sequence, some words in the sentence are randomly turned into blanks (or masked), like this: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9144271954194145,
        0.8517313617846615,
        0.9163276473894826,
        0.9690281048290753,
        0.9520849824395835,
        0.8331657834952738,
        0.9362853717980744,
        0.84284562270366
      ],
      "excerpt": "This task is a little more difficult, and more importantly, requires understanding the context surrounding a blank much better. \nTurns out, once a language model gets really, really good at this rather pointless pretext task, it can be easily repurposed for much more interesting tasks. \nOnce a language learns grammar and semantics, it can become a very good classifier of sentences, and even whole documents, for example. \nIf you then teach it to classify tweets or documents into categories (or identify sentiment, or toxicity for example) it no longer sees just a bunch of confusing characters, but rather it's \"reading\" the document at a much more abstract level, so it can \"make sense\" of it much more readily. (Note the air quotes, this is not magic but it is probably the closest thing.) \nBecause there are no language models in Catalan! And there's a lot of Catalan text to be processed. (In Catalonia). \nAll config lives under config. There you can control parameters related to training, tokenizing, and everything, and even choose which version of the model to train. \nAll configuration is overridable, since it's Hydra configuration. Check their docs. \nThe pretrained tokenizers are at dist/tokenizer-{cased,uncased}. They are trained only on the full training set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Catalan ALBERT (A Lite BERT for self-supervised learning of language representations)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/codegram/calbert/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Sat, 25 Dec 2021 07:26:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/codegram/calbert/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "codegram/calbert",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/codegram/calbert/master/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For dependency management we use [Poetry](https://python-poetry.org) (and Docker of course).\n\n```bash\npip install -U poetry\npoetry install\npoetry shell\n```\n\nThe production image to train the model is under `docker/`, and it's called `codegram/calbert`. It contains all the latest dependencies, but no code -- Deepkit will ship the code in every experiment (read on to learn more about Deepkit).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8376497906612891
      ],
      "excerpt": ": NB: Can be done in one step : tokenize.encode(\"M'\u00e9s una mica igual\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8520517767447615
      ],
      "excerpt": "To train the cased one, just override the appropriate Hydra configuration: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.933113204129504
      ],
      "excerpt": "Configure a cluster in your local Deepkit with at least one machine with a GPU. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8289669050403863
      ],
      "excerpt": "Correct output: \"quickly\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289669050403863
      ],
      "excerpt": "Correct output: \"dog\", \"quickly\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9248925118229055
      ],
      "excerpt": "python -m calbert train_tokenizer --input-file dataset/train.txt --out-dir tokenizer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9359643182351021
      ],
      "excerpt": "python -m calbert train_tokenizer --input-file dataset/train.txt --out-dir tokenizer vocab.lowercase=False \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.851227829413455
      ],
      "excerpt": "deepkit run test.deepkit.yml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008331685760428
      ],
      "excerpt": "deepkit run --cluster \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/codegram/calbert/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Dockerfile",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "calbert ![](https://github.com/codegram/calbert/workflows/Tests/badge.svg)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "calbert",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "codegram",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/codegram/calbert/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We use [Deepkit](https://deepkit.ai) to run and keep track of experiments. Download it for free for your platform of choice if you'd like to run locally, or check their docs to run against their free community server.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\nmake test\n```\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 13,
      "date": "Sat, 25 Dec 2021 07:26:04 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "albert",
      "nlp",
      "bert",
      "transformers",
      "language-model"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You just need the `transformers` library. Nothing else to clone or install.\n\nTo choose which model version to use (`tiny`, or `base`), consider that smaller models are less powerful, but nimbler and less resource-hungry to run.\n\n```bash\npip install transformers\n```\n\n```python\nfrom transformers import AutoModel, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"codegram/calbert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"codegram/calbert-base-uncased\")\n\nmodel.eval() #: disable dropout\n```\n\nNow onto the two main use cases that you can do.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "A tiny subset of the dataset lives under `dist/data` so that you can train a small model and do quick experiments locally.\n\nTo download the full dataset and automatically split it in training / validation, just run this command:\n\n```bash\npython -m calbert download_data --out-dir dataset\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}