{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nThis work was done by my intern [@raleighhan](https://github.com/RaleighHan)(\u6496\u671d\u6da6) during his internship at NLP Group of Tencent AI Lab.\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1907.11692",
      "https://arxiv.org/abs/1810.04805, 2018.\r\n\r\nRoBERTa: Liu Y, Ott M, Goyal N, et al. Roberta: A robustly optimized bert pretraining approach[J]. arXiv preprint https://arxiv.org/abs/1907.11692, 2019.\r\n\r\nBERT-small: Turc I, Chang M W, Lee K, et al. Well-read students learn better: On the importance of pre-training compact models[J]. arXiv preprint https://arxiv.org/abs/1908.08962, 2019.\r\n\r",
      "https://arxiv.org/abs/1907.11692, 2019.\r\n\r\nBERT-small: Turc I, Chang M W, Lee K, et al. Well-read students learn better: On the importance of pre-training compact models[J]. arXiv preprint https://arxiv.org/abs/1908.08962, 2019.\r\n\r",
      "https://arxiv.org/abs/1908.08962, 2019.\r\n\r"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nHuggingFace Transformers: https://github.com/huggingface/transformers\r\n\r\nBERT: Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.\r\n\r\nRoBERTa: Liu Y, Ott M, Goyal N, et al. Roberta: A robustly optimized bert pretraining approach[J]. arXiv preprint arXiv:1907.11692, 2019.\r\n\r\nBERT-small: Turc I, Chang M W, Lee K, et al. Well-read students learn better: On the importance of pre-training compact models[J]. arXiv preprint arXiv:1908.08962, 2019.\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.842790493796475
      ],
      "excerpt": "| UMBC WebBase Corpus | 18G         | web pages            | ~180M      | ~3B    | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/haisongzhang/roberta-tiny-cased",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-12T07:08:47Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T02:20:40Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8507442644015248
      ],
      "excerpt": "A small case-preserving RoBERTa model pre-trained for production use. Feel free to download our model from Baidu Disk (Extraction Code: yhmq) or Google Drive or from HuggingFace model hub. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9497441790553057
      ],
      "excerpt": "We used a 43G corpus consists of Wikipedia, BookCorpus and UMBC WebBase Corpus. Except BookCorpus, all pre-training data is case-preserving. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934342326747609
      ],
      "excerpt": "We used code from Transformers to pre-train RoBERTa-tiny. Datasets library was used to provide fast and efficient access to disk data. During pre-training, we followed the setting from RoBERTa and only used MLM loss as pre-training objective. However, we used Wordpiece as tokenizer, while BPE was used in original RoBERTa. Input data was organized in FULL-SENTENCES format. The whole pre-training took about 5 days on 8 V100 GPUs for 20 epochs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.98220467096327
      ],
      "excerpt": "We fine-tuned our RoBERTa-tiny (cased) model on all tasks from GLUE (Task descriptions are listed below), and compared the test set results with BERT-small, an uncased BERT model with same structure released by Google.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9648729098351673
      ],
      "excerpt": "For RTE, STS, MRPC and QNLI, we found it helpful to finetune starting from the MNLI single-task model, rather than the baseline pretrained RoBERTa. For each task, we selected the best fine-tuning hyperparameters from the lists below, and trained for 4 epochs: \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/haisongzhang/roberta-tiny-cased/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 01:05:41 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/haisongzhang/roberta-tiny-cased/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "haisongzhang/roberta-tiny-cased",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/haisongzhang/roberta-tiny-cased/issues{/number}",
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "RoBERTa-tiny-cased",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "roberta-tiny-cased",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "haisongzhang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/haisongzhang/roberta-tiny-cased/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Thu, 23 Dec 2021 01:05:41 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nOur pre-trained model is specially suitable for low latency applications. Combined with knowledge distillation and task-specific fine-tuning, our model can achieve high inference speed while keeping similar performance with larger models. HuggingFace Transformers is recommended when loading our model for further fine-tuning:\r\n\r\n```python\r\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"haisongzhang/roberta-tiny-cased\")\r\n\r\nmodel = AutoModelWithLMHead.from_pretrained(\"haisongzhang/roberta-tiny-cased\")\r\n```\r\n\r\n**Note**: When loading the tokenizer in transformers, use BertTokenizer instead of RobertaTokenizer since Wordpiece was used in this model. \r\n\r\n",
      "technique": "Header extraction"
    }
  ]
}