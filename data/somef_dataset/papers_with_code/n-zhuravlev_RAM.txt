# RAM
Recurrent attentional module


##### Hyperparams:
    - Number of layers 1, but it should work better with 4 layers and don't bring huge parameters overhead
    - Activation: PReLU
    - Reduction ratio: vanilla 16
    - Normalization: BatchNorm but I think that it will work better with LayerNorm or GroupNorm 
    - Statistics: 'max', 'mean' and we should try out 'min'
    - use_mapping_act: False, but we should check, whether we should use 'Tanh' activation after summation or not.

##### Structure:
    - experiments_workbook.ipynb notebook for different experiments and accuracy graphics.
    - src/
        - cbam.py implementation of [CBAM](https://arxiv.org/pdf/1807.06521.pdf) module  
        - recurrent_channel_gate.py implementation of new recurrent module only for channels
        - channel_attention.py wrapper for all channel attentions for convenient switching during training
        - resnet.py implementation of resnet models zoo
        - utils.py common tools
   
   
