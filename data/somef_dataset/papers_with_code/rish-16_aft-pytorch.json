{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@misc{attention-free-transformer,\ntitle = {An Attention Free Transformer},\nauthor = {Shuangfei Zhai and Walter Talbott and Nitish Srivastava and Chen Huang and Hanlin Goh and Ruixiang Zhang and Josh Susskind},\nyear = {2021},\nURL = {https://arxiv.org/pdf/2105.14103.pdf}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{attention-free-transformer,\ntitle = {An Attention Free Transformer},\nauthor = {Shuangfei Zhai and Walter Talbott and Nitish Srivastava and Chen Huang and Hanlin Goh and Ruixiang Zhang and Josh Susskind},\nyear = {2021},\nURL = {https://arxiv.org/pdf/2105.14103.pdf}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9372151656983196,
        0.8326585122547251
      ],
      "excerpt": "x = torch.rand(32, 10, 512) \ny = layer(x) #: [32, 10, 512] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372151656983196,
        0.8326585122547251
      ],
      "excerpt": "x = torch.rand(32, 10, 512) \ny = layer(x) #: [32, 10, 512] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372151656983196,
        0.8326585122547251
      ],
      "excerpt": "x = torch.rand(32, 10, 512) \ny = layer(x) #: [32, 10, 512] \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rish-16/aft-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-01T08:42:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T10:10:28Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8054510352908986
      ],
      "excerpt": "Unofficial PyTorch implementation of Attention Free Transformer's layers by Zhai, et al. [abs, pdf] from Apple Inc. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8888959611262811
      ],
      "excerpt": ": a batch of sequences with 10 timesteps of length 512 each \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8888959611262811
      ],
      "excerpt": ": a batch of sequences with 10 timesteps of length 512 each \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8888959611262811
      ],
      "excerpt": ": a batch of sequences with 10 timesteps of length 512 each \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8824898858348871
      ],
      "excerpt": "This layer wrapper is a 'plug-and-play' with your existing networks / Transformers. You can swap out the Self-Attention layer with the available layers in this package with minimal changes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Unofficial PyTorch implementation of Attention Free Transformer (AFT) layers by Apple Inc.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rish-16/aft-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 15,
      "date": "Sat, 25 Dec 2021 13:27:36 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rish-16/aft-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rish-16/aft-pytorch",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can install `aft-pytorch` via `pip`:\n\n```bash\npip install aft-pytorch\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8600175624665045
      ],
      "excerpt": "<img src=\"https://github.com/rish-16/aft-pytorch/raw/main/pic.png\" width=650> \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from aft_pytorch import AFTFull \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from aft_pytorch import AFTSimple \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from aft_pytorch import AFTLocal \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rish-16/aft-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Rishabh Anand\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "aft-pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "aft-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rish-16",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rish-16/aft-pytorch/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 161,
      "date": "Sat, 25 Dec 2021 13:27:36 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can import the **AFT-Full** or **AFT-Simple** layer (as described in the paper) from the package like so:\n\n",
      "technique": "Header extraction"
    }
  ]
}