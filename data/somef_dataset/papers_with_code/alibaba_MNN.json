{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "MNN participants: Taobao Technology Department, Search Engineering Team, DAMO Team, Youku and other Alibaba Group employees.\n\nMNN refers to the following projects:\n- [Caffe](https://github.com/BVLC/caffe)\n- [flatbuffer](https://github.com/google/flatbuffers)\n- [gemmlowp](https://github.com/google/gemmlowp)\n- [Google Vulkan demo](http://www.github.com/googlesamples/android-vulkan-tutorials)\n- [Halide](https://github.com/halide/Halide)\n- [Mace](https://github.com/XiaoMi/mace)\n- [ONNX](https://github.com/onnx/onnx)\n- [protobuffer](https://github.com/protocolbuffers/protobuf)\n- [skia](https://github.com/google/skia)\n- [Tensorflow](https://github.com/tensorflow/tensorflow)\n- [ncnn](https://github.com/Tencent/ncnn)\n- [paddle-mobile](https://github.com/PaddlePaddle/paddle-mobile)\n- [stb](https://github.com/nothings/stb)\n- [rapidjson](https://github.com/Tencent/rapidjson)\n- [pybind11](https://github.com/pybind/pybind11)\n- [pytorch](https://github.com/pytorch/pytorch)\n- [bolt](https://github.com/huawei-noah/bolt)\n- [libyuv](https://chromium.googlesource.com/libyuv/libyuv)\n- [libjpeg](https://github.com/libjpeg-turbo/libjpeg-turbo)\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{alibaba2020mnn,\n  author = {Jiang, Xiaotang and Wang, Huan and Chen, Yiliu and Wu, Ziqi and Wang, Lichuan and Zou, Bin and Yang, Yafeng and Cui, Zongyang and Cai, Yu and Yu, Tianhang and Lv, Chengfei and Wu, Zhihua},\n  title = {MNN: A Universal and Efficient Inference Engine},\n  booktitle = {MLSys},\n  year = {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9999999999915588,
        0.9833155603401413,
        0.9713345300859536,
        0.9664456561658856
      ],
      "excerpt": "  author = {Jiang, Xiaotang and Wang, Huan and Chen, Yiliu and Wu, Ziqi and Wang, Lichuan and Zou, Bin and Yang, Yafeng and Cui, Zongyang and Cai, Yu and Yu, Tianhang and Lv, Chengfei and Wu, Zhihua}, \n  title = {MNN: A Universal and Efficient Inference Engine}, \n  booktitle = {MLSys}, \n  year = {2020} \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alibaba/MNN",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "../../CONTRIBUTING.md",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-15T07:40:18Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T10:08:40Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9951898531741867
      ],
      "excerpt": "MNN is a highly efficient and lightweight deep learning framework. It supports inference and training of deep learning models, and has industry leading performance for inference and training on-device. At present, MNN has been integrated in more than 20 apps of Alibaba Inc, such as Taobao, Tmall, Youku, Dingtalk, Xianyu and etc., covering more than 70 usage scenarios such as live broadcast, short video capture, search recommendation, product searching by image, interactive marketing, equity distribution, security risk control. In addition, MNN is also used on embedded devices, such as IoT. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9280888933505724,
        0.8397007657188905,
        0.9710465250911055,
        0.8844546786450868,
        0.8590374383230895
      ],
      "excerpt": "Implements core computing with lots of optimized assembly code to make full use of the ARM CPU. \nFor iOS, GPU acceleration (Metal) can be turned on, which is faster than Apple's native CoreML. \nFor Android, OpenCL, Vulkan, and OpenGL are available and deep tuned for mainstream GPUs (Adreno and Mali). \nConvolution and transposition convolution algorithms are efficient and stable. The Winograd convolution algorithm is widely used to better symmetric convolutions such as 3x3 -> 7x7. \nTwice speed increase for the new architecture ARM v8.2 with FP16 half-precision calculation support. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9790582809511369,
        0.9425381475847175,
        0.9838363002856877,
        0.9856601592102623,
        0.9389434564127155
      ],
      "excerpt": "iOS platform: static library size for armv7+arm64 platforms is about 5MB, size increase of linked executables is about 620KB, and metallib file is about 600KB. \nAndroid platform: core so size is about 400KB, OpenCL so is about 400KB, Vulkan so is about 400KB. \nSupports Tensorflow, Caffe, ONNX, and supports common neural networks such as CNN, RNN, GAN. \nMNN model converter supports 149 Tensorflow OPs, 58 TFLite OPs, 47 Caffe OPs and 74 ONNX OPs; Number of OPs by different MNN hardware backends: 111 for CPU, 6 for ARM V8.2, 55 for Metal, 43 for OpenCL, and 32 for Vulkan. \nSupports iOS 8.0+, Android 4.3+ and embedded devices with POSIX interface. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "MNN is a blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "MNN's docs are in placed in [Yuque docs here](https://www.yuque.com/mnn/en).\n\nMNN Workbench could be downloaded from [MNN's homepage](http://www.mnn.zone), which provides pretrained models, visualized training tools, and one-click deployment of models to devices.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alibaba/MNN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1300,
      "date": "Mon, 27 Dec 2021 13:07:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alibaba/MNN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "alibaba/MNN",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/alibaba/MNN/master/tools/script/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/alibaba/MNN/tree/master/3rd_party/flatbuffers/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/alibaba/MNN/master/test.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/docker_run.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/reflection/generate_code.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/src/clang-format.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/tests/TypeScriptTest.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/tests/GoTest.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/tests/PythonTest.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/tests/JavaScriptTest.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/tests/phpUnionVectorTest.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/tests/TestAll.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/tests/DartTest.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/tests/JavaTest.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/tests/generate_code.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/tests/RustTest.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/tests/FlatBuffers.Test/NetTest.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/grpc/build_grpc.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/grpc/tests/java-grpc-test.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/samples/java_sample.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/samples/python_sample.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/samples/android_sample.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/samples/php_sample.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/samples/javascript_sample.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/samples/csharp_sample.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/samples/dart_sample.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/samples/go_sample.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/.travis/build-and-run-docker-test-containers.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/.travis/deploy-python.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/3rd_party/flatbuffers/.travis/check-generate-code.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/schema/generate.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/cross-compile/build.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/android/updateTest.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/android/build_32_shared.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/android/build_32_stl_shared.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/android/build_32_arm82.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/android/pullResult.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/android/rTest.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/android/testCommon.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/android/build_64.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/android/speedTest.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/android/build_32.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/android/testBasic.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/android/build_gnu_32.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/android/build_32_ndk14.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/android/run.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/mac/buildMacOsX86.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/project/ios/buildiOS.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/source/backend/vulkan/compiler/getSpirv.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/source/backend/metal/schema/generate.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/source/backend/opencl/schema/generate.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/source/backend/tensorrt/execution/schema/generate.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/benchmark/bench_android.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/tools/script/copy2Resource.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/tools/script/pull_all_output.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/tools/script/runMNNBasic.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/tools/script/get_model.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/tools/script/pull_output.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/tools/script/test_android.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/tools/script/cat_uuid.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/tools/script/forconvert/convertTf.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/tools/script/forconvert/convertOnnx.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/tools/script/forconvert/convertMNN.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/tools/script/forconvert/convertTflite.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/tools/script/forconvert/convertCaffe.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/ciscripts/parse_options.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/ciscripts/build.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/ciscripts/iOS/Xcode.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/ciscripts/iOS/CMake.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/ciscripts/Linux/CL_OMP_Vulkan.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/ciscripts/Linux/CL_ThreadPool_Vulkan.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/ciscripts/Android/64.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/ciscripts/Android/32OMP.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/ciscripts/Android/64OMP.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/ciscripts/Android/Gradle.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/ciscripts/Android/32.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/ciscripts/macOS/CPU_Metal.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/ciscripts/macOS/CPU.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/codegen/build.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/package_scripts/mac/build_lib.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/package_scripts/mac/build_bridge.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/package_scripts/mac/build_whl.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/package_scripts/mac/build_tools.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/package_scripts/linux/build_lib.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/package_scripts/linux/build_bridge.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/package_scripts/linux/build_whl.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/package_scripts/linux/build.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/package_scripts/linux/build_tools.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/pymnn/update_mnn_wrapper_assets.sh",
      "https://raw.githubusercontent.com/alibaba/MNN/master/pymnn/pip_package/build_manylinux2014.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8587000349647055
      ],
      "excerpt": "For iOS, GPU acceleration (Metal) can be turned on, which is faster than Apple's native CoreML. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alibaba/MNN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "C",
      "Assembly",
      "Python",
      "Objective-C++",
      "Cuda",
      "CMake",
      "Metal",
      "Shell",
      "GLSL",
      "PowerShell",
      "Objective-C",
      "Ruby",
      "Batchfile",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright 2008 Google Inc.  All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are\\nmet:\\n\\n    * Redistributions of source code must retain the above copyright\\nnotice, this list of conditions and the following disclaimer.\\n    * Redistributions in binary form must reproduce the above\\ncopyright notice, this list of conditions and the following disclaimer\\nin the documentation and/or other materials provided with the\\ndistribution.\\n    * Neither the name of Google Inc. nor the names of its\\ncontributors may be used to endorse or promote products derived from\\nthis software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\nCode generated by the Protocol Buffer compiler is owned by the owner\\nof the input file used when generating it.  This code is not\\nstandalone and requires a support library to be linked with it.  This\\nsupport library is itself covered by the above license.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Intro",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "MNN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "alibaba",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alibaba/MNN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "jxt1234",
        "body": "1. Fix deconvolution multi -batch multi-input compute error.\r\n2. Fix bug for compute size for stridedslice when beginShape << input.dim\r\n3. Add fp16 for Gridsample\r\n4. Fix bug for PoolGrad\r\n5. Fix compile error for not comparability asm",
        "dateCreated": "2021-07-26T08:41:52Z",
        "datePublished": "2021-07-29T03:30:25Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/1.2.1",
        "name": "Bugfix / Feature",
        "tag_name": "1.2.1",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/1.2.1",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/46946575",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/1.2.1"
      },
      {
        "authorType": "User",
        "author_name": "hush-alibaba",
        "body": "# \u4e00\u3001\u6846\u67b6\u901a\u7528\u6027\r\n\r\n\r\n# 1.1 \u65b0\u589eTorchscript\u6a21\u578b\u683c\u5f0f\u652f\u6301\r\n\r\n\r\n\u6211\u4eec\u6ce8\u610f\u5230\uff0c\u5927\u91cf\u7684\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u5e08\u5728\u4eceTensorFlow\u5f80PyTorch\u8fc1\u79fb\u3002\u63a8\u7406\u5f15\u64ce\u5bf9\u4e8ePyTorch\u6a21\u578b\u7684\u539f\u751f\u652f\u6301\u5c24\u4e3a\u91cd\u8981\u3002\u867d\u7136MNN\u5df2\u7ecf\u652f\u6301\u4e86ONNX\u683c\u5f0f\u7684\u6a21\u578b\uff0c\u4f46\u662f\u8003\u8651\u5230PyTorch\u81ea\u8eab\u957f\u671f\u7684\u53d1\u5c55\u8d8b\u52bf\uff0c\u57fa\u4e8eTorchscript\u683c\u5f0f\u7684\u6a21\u578b\u6bd4ONNX\u66f4\u5177\u901a\u7528\u6027\u3002\r\n\r\n\r\n\u73b0\u5728\uff0cMNNConvert\u652f\u6301\u5728Mac\u3001Windows\u3001Linux\u5e73\u53f0\u4e0b\u5c06\u6240\u6709\u7684 [TorchVision\u89c6\u89c9\u6a21\u578b](https://pytorch.org/vision/stable/index.html) \u8f6c\u6362\u5230MNN\u683c\u5f0f\u3002\r\n\r\n\r\n# 1.2 \u65b0\u589eARM BF16\u540e\u7aef\r\n\r\n\r\nBF16 \u53ef\u4ee5\u7ed9\u4e2d\u4f4e\u7aef\u624b\u673a\u548c\u9ad8\u7aef\u673a\u7684\u5c0f\u6838\u5e26\u6765\u6027\u80fd\u6536\u76ca\uff0c\u5e76\u4e14\u964d\u4f4e\u5185\u5b58\u5360\u7528\u3002\u7ecfMNN\u56e2\u961f\u5185\u90e8\u6d4b\u8bd5\uff0cBF16\u76f8\u5bf9\u4e8eFP32\u5728\u4e0d\u540c\u673a\u578b\u7684\u4e2d\u4f4e\u7aef\u6838\u5fc3(A53 A55 A53kyro A55kyro)\u4e0a\uff0c\u4e0d\u540c\u6a21\u578b\u6709 5%- 30%\u7684\u4f18\u5316\u3002\r\n![1623327886911-992fe908-e27e-4951-8942-cf3431b17f80](https://user-images.githubusercontent.com/56632408/121664073-acc15f00-cad9-11eb-9db0-2ce898bad04f.png)\r\n\r\n\r\nBF16\u4f7f\u7528\u65b9\u6cd5\uff1a\r\n1. \u7f16\u8bd1MNN\u65f6\uff0c\u6307\u5b9a-DMNN_SUPPORT_BF16=ON\r\n2. BackendConfig\u4e2d\u6307\u5b9aPrecisionMode \u4e3a Precision_Low\r\n\r\n\r\n# 1.3 \u65b0\u589eCoreML\u540e\u7aef\r\n\u57fa\u4e8e\u51e0\u4f55\u8ba1\u7b97\uff0cMNN\u6dfb\u52a0\u4e86CoreML\u7684\u652f\u6301\u3002\u5728iPhone X\u4e4b\u540e\uff0c\u501f\u52a9\u4e8eApple Neural Engine\uff0c\u76f8\u6bd4\u4e8eCPU\uff0cCoreML (ANE) \u5728\u89c6\u89c9\u6a21\u578b\u4e2d\u7ea6\u67095-6\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002\r\n\r\n# 1.4 \u51e0\u4f55\u8ba1\u7b97\u7684\u6f14\u8fdb\r\n\r\n\r\n\u57281.1.0\u7248\u672c\u7684\u51e0\u4f55\u8ba1\u7b97\u7684\u57fa\u7840\u4e0a\uff0c\u672c\u6b21\u53d1\u5e03\u4e2d\u300e\u51e0\u4f55\u8ba1\u7b97\u300f\u589e\u52a0\u4e86\u5bf9\u4e8e\u5faa\u73af\u7b97\u5b50\uff08\u5982Gather\u3001BatchMatMul\u3001LSTM\uff09\u7684GPU\u540e\u7aef\u652f\u6301\u3002\r\n\r\n\r\n# \u4e8c\u3001\u6a21\u578b\u538b\u7f29\r\n\r\n\r\n# 2.1 ARM \u6d6e\u70b9\u7a00\u758f\u7b97\u5b50\u5b9e\u73b0\r\n\r\n\r\n\u968f\u7740CPU\u6027\u80fd\u4f18\u5316\u7684\u8fb9\u9645\u6536\u76ca\u7684\u964d\u4f4e\uff0c\u4e3a\u4e86\u83b7\u5f97\u66f4\u9ad8\u7684\u6027\u80fd\uff0c\u9700\u8981\u4ece\u6a21\u578b\u7ed3\u6784\u672c\u8eab\u7740\u624b\uff0c\u8bbe\u8ba1\u3001\u88c1\u526a\u51fa\u5408\u9002\u76ee\u6807\u786c\u4ef6\u548c\u63a8\u7406\u5f15\u64ce\u7684\u6a21\u578b\u7ed3\u6784\uff0c\u4ee5\u83b7\u5f97\u6700\u4f73\u7684\u7cbe\u5ea6\u548c\u6027\u80fd\u3002\r\n\u57fa\u4e8e\u6b64\uff0cMNN\u6dfb\u52a0\u4e86\u968f\u673a\u7a00\u758f\u548c\u534a\u7ed3\u6784\u5316\u7a00\u758f\u7b97\u5b50\u7684ARM\u6d6e\u70b9\u5b9e\u73b0  \uff08\u539f\u7406\u89c1 \u201d [Fast Conv Nets](https://openaccess.thecvf.com/content_CVPR_2020/papers/Elsen_Fast_Sparse_ConvNets_CVPR_2020_paper.pdf) \u201d \uff09\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a\r\n![1623327945195-777985a8-dd6d-4cb0-a9b6-b7a7c8351fd0](https://user-images.githubusercontent.com/56632408/121664114-b8148a80-cad9-11eb-9b6a-e64d9942557e.png)\r\n\r\n\r\n\u7ecf\u8fc7MNN\u5185\u90e8\u5728\u5404\u7c7b\u673a\u578b\u548c\u6a21\u578b\u5b9e\u6d4b\uff0c\u968f\u673a\u7a00\u758f\u7387,   1x4\u534a\u7ed3\u6784\u7a00\u758f\u7387 (\u6cbf\u8f93\u51fa\u901a\u9053OC\u5206\u5757\uff0cblockOC=4) \u5206\u522b\u4e3a0.6\u3001 0.3\u65f6\uff0c\u63a8\u7406\u6027\u80fd\u5c06\u5927\u4e8e\u7a20\u5bc6\u5b9e\u73b0\u6027\u80fd\u3002\r\n\r\n\r\n\u968f\u673a\u7a00\u758f\u73870.9\u65f6\uff0cMobileNet\u3001NasNet\u3001SqueezeNet\u5404\u7c7b\u6a21\u578b\u4e2d\uff0c\u5728\u9ad8\u3001\u4e2d\u3001\u4f4e\u7aef\u624b\u673a\u4e0a\u7684\u52a0\u901f\u6bd4\u4e3a1.7\u500d ~ 4.5\u500d\uff1b1x4\u534a\u7ed3\u6784\u7a00\u758f\u73870.9\u65f6\uff0c\u52a0\u901f\u6bd4\u4e3a1.8\u500d ~ 6.1\u500d\u3002\r\n\r\n\r\n# 2.2 \u79bb\u7ebf\u91cf\u5316\u7cbe\u5ea6\u63d0\u5347\r\n\r\n\r\n\u79bb\u7ebf\u91cf\u5316\u5de5\u5177\u4e2d\u6dfb\u52a0\u4e86\u6fc0\u6d3b\u975e\u5bf9\u79f0\u7684\u652f\u6301\uff0c\u5e76\u4e14\u901a\u8fc7\u91cf\u5316\u65f6\u66f4\u65b0BN\u53c2\u6570\uff0c\u79bb\u7ebf\u91cf\u5316\u7cbe\u5ea6\u83b7\u5f97\u666e\u904d\u63d0\u5347\u3002\u7ed3\u5408\u4f7f\u7528\u975e\u5bf9\u79f0\u91cf\u5316+BN\u53c2\u6570\u66f4\u65b0\uff0cMobileNet V2\u91cf\u5316\u6a21\u578b\u7cbe\u5ea6\u4ece71.05%\u63d0\u9ad8\u523071.73%\uff0c\u8fdb\u4e00\u6b65\u903c\u8fd1\u6d6e\u70b9\u6a21\u578b\uff0871.90%\uff09\u3002\r\n\u4f7f\u7528\u65b9\u6cd5\u89c1\uff1a [MNN/tools/MNNPythonOfflineQuant at master \u00b7 alibaba/MNN \u00b7 GitHub](https://github.com/alibaba/MNN/tree/master/tools/MNNPythonOfflineQuant) \r\n\r\n\r\n# \u4e09\u3001\u6027\u80fd\u4f18\u5316\r\n\r\n\r\n# 3.1 ARM \u540e\u7aef\r\n\r\n\u652f\u6301ARMv8.2\u6307\u4ee4\u7684\u8bbe\u5907\u5360\u6709\u7387\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u9010\u6e10\u4e0a\u5347\uff0c\u662fMNN\u4f18\u5316\u7684\u91cd\u70b9\u4e4b\u4e00\u3002\u76f8\u6bd4\u4e8eMNN 1.1.x\u7248\u672c\uff0cMNN 1.2.0\u7684ARMv8.2\u6027\u80fd\u5728\u5404\u7c7b\u89c6\u89c9\u6a21\u578b\u4e2d\u67095% ~ 20%\u7684\u63d0\u5347\uff0c\u4e14\u4e0e\u4e1a\u754c\u4e3b\u6d41\u5f15\u64ce\u5bf9\u6bd4\u5904\u4e8e\u9886\u5148\u5730\u4f4d\u3002\r\n![1623328023799-3a5590e1-1ae3-423e-bdcf-53efecceaddb](https://user-images.githubusercontent.com/56632408/121664143-c06cc580-cad9-11eb-9d18-88197cbcdf83.png)\r\n\r\n\r\n\r\n\r\n# 3.2 X86 \u540e\u7aef\r\n\r\nMNN\u96c6\u4e2d\u4f18\u5316\u4e86X86-AVX2\u4e0a\u7684\u6027\u80fd\u3002\u76ee\u524d\u5728\u4e3b\u6d41\u7684\u89c6\u89c9\u3001\u65f6\u5e8f\u6a21\u578b\u4e2d\uff0cMNN-AVX2\u540e\u7aef\u76f8\u6bd4\u4e8eOpenVINO\u753120%\u5230440%\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u4e0eONNXRuntime\u76f8\u6bd4\uff0c\u5219\u670918%\u523060%\u7684\u6027\u80fd\u4f18\u52bf (\u4ec5MobileNet V2\u7565\u900a\u4e8eOpenVINO/ONNXRuntime)\u3002\r\n\r\n![1623328041437-ef9f1bbe-2bd4-4228-a9b0-cf949e6e1926](https://user-images.githubusercontent.com/56632408/121664164-c498e300-cad9-11eb-8214-130d44d2ff61.png)\r\n\u53d6\u5f97\u5982\u6b64\u6027\u80fd\u6210\u7ee9\uff0c\u4e14\u901a\u7528\u6027\u6301\u5e73\u6216\u66f4\u80dc\u4e00\u7b79\u7684\u524d\u63d0\u4e0b\uff0c\u76f8\u6bd4\u4e8e Onnx / OpenVino \u51e0\u5341\u81f3\u51e0\u767eM \u7684\u5927\u5c0f\uff0cMNN \u5e93\u7684\u4f53\u79ef\u5374\u5f88\u5c0f\uff0c\u4ec5 3M \u4e0d\u5230\u3002 \u6b64\u5916\uff0cMNN \u652f\u6301\u4e86 AVX512 / AVX512VNNI\uff0c\u76f8\u5bf9\u4e8e AVX2 \uff0c\u5728\u6d6e\u70b9\u77e9\u9635\u4e58\u6cd5\u6709 60% \u52a0\u901f\uff0cInt8\u77e9\u9635\u4e58\u5219\u6709 200% \u7684\u52a0\u901f\u3002\r\n# 3.3 OpenCL\u540e\u7aef\r\n\r\n\u968f\u7740\u79fb\u52a8App\u4e2d\u7684\u90e8\u7f72\u7684\u5404\u7c7b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6570\u91cf\u589e\u591a\uff0cMNN\u56e2\u961f\u53d1\u73b0\uff0cCPU\u5360\u7528\u7387\u5c45\u9ad8\u4e0d\u4e0b\uff0c\u4f1a\u5f71\u54cdApp\u7a33\u5b9a\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002\u57fa\u4e8e\u6b64\u5224\u65ad\uff0c\u6211\u4eec\u91cd\u70b9\u4f18\u5316OpenCL\u540e\u7aef\u7684\u6027\u80fd\uff08\u4e0e\u4e3b\u6d41\u5f15\u64ce\u76f8\u6bd4\u5df2\u5904\u4e8e\u9886\u5148\u5730\u4f4d\uff09\uff0c\u5e76\u4e14\u4e0e\u5185\u90e8\u4e1a\u52a1\u65b9\u5171\u540c\u8bbe\u8ba1\u9762\u5411GPU\u6a21\u578b\uff0c\u8fbe\u5230\u6027\u80fd\u3001\u7cbe\u5ea6\u53cc\u9ad8\u7684\u6a21\u578b\u3002\u6027\u80fd\u6570\u636e\u5982\u4e0b\u56fe\uff1a\r\n![1623328057517-031b6250-5e2a-4886-8f05-5dbfc53cd7e3](https://user-images.githubusercontent.com/56632408/121664186-cbbff100-cad9-11eb-937e-a96a104768b4.png)\r\n\r\n![1623328074312-ec3bcf02-f8ef-42d7-99f1-aa175e8f618c](https://user-images.githubusercontent.com/56632408/121664195-cfec0e80-cad9-11eb-8f84-1cbce315c8f4.png)\r\n\r\n# \u56db\u3001\u5176\u4ed6\r\n### \u529f\u80fd\r\n1. \u65b0\u5efa MNN \u8868\u8fbe\u5f0f\u63a5\u53e3\u76f8\u5173 demo\uff0c\u89c1pymnn/examples/MNNExpr/mobilenet_demo.py\u548cdemo/exec/{pictureRecognition_module.cpp, transformerDemo.cpp}\r\n2. \u5bf9\u79bb\u7ebf\u91cf\u5316\u5de5\u5177\u8fdb\u884c\u4e86\u91cd\u6784\uff0c\u51cf\u5c11 int8 / float \u4e92\u8f6c\u635f\u8017\uff0c\u4ee5 shufflenet \u4e3a\u4f8b\u53ef\u51cf\u5c11 20% \u8017\u65f6\r\n3. \u5b8c\u5584\u6a21\u578b\u6821\u9a8c\u5de5\u5177 \uff08tools/script/fastTest \u7cfb\u5217\uff09\r\n4. \u589e\u52a0 Onnx \u6240\u6709\u4e0e MNN \u76f8\u5339\u914d\u7684\u5355\u76ee / \u53cc\u76ee\u7b97\u7b26\u652f\u6301\r\n\r\n\r\n### \u56fe\u4f18\u5316\uff08\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\uff09\r\n1. \u65b0\u589e Gelu / Hardswish \u7b97\u5b50\u878d\u5408\r\n2. \u589e\u52a0 Layernorm \u7b97\u5b50\u878d\u5408\u7684\u8303\u56f4\r\n3. \u65b0\u589e MatMul + Bias \u878d\u5408\uff0c\u589e\u52a0\u5176\u8f6c\u6362\u4e3a\u5377\u79ef\u7684\u8303\u56f4\r\n4. \u65b0\u589e Tensorflow / Tflite \u7684 Dilate Convolution \u7b97\u5b50\u878d\u5408\uff08SpaceToBatch + Conv + BatchToSpace\uff09\r\n\r\n\r\n### Bugfix\uff08\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\uff09:\r\n1. \u4fee\u6b63 StridedSlice \u5728 newAxis \u548c begin << inputShape \u60c5\u51b5\u4e0b\u7684\u5b9e\u73b0\u9519\u8bef\r\n2. \u4fee\u6b63 Eltwise MAX \u7684\u6c42\u5bfc\u9519\u8bef\r\n3. \u79fb\u9664 MNNConvert \u5bf9 regex \u7684\u4f9d\u8d56\uff08\u6b64\u95ee\u9898\u5bfc\u81f4\u5728 gcc 4.8 \u73af\u5883\u4e0b\u65e0\u6cd5\u8fd0\u884c Converter\uff09\r\n4. \u4fee\u6b63 CPU Raster \u7b97\u5b50\u5bf9 dimension = 1 \u7684 NC4HW4 \u6570\u636e\u683c\u5f0f\u5904\u7406\u9519\u8bef\u7684\u95ee\u9898\r\n5. \u79fb\u9664 MNN Python wheel \u4e2d\u610f\u4e49\u4e0d\u5927\u7684 mnnops \uff08\u91cc\u9762\u663e\u793a\u7684 Op \u5217\u8868\u4e0d\u51c6\u786e\uff09\r\n\r\n",
        "dateCreated": "2021-06-11T09:17:13Z",
        "datePublished": "2021-06-18T10:08:42Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/1.2.0",
        "name": "MNN 1.2.0 Release Notes",
        "tag_name": "1.2.0",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/1.2.0",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/44467257",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/1.2.0"
      },
      {
        "authorType": "User",
        "author_name": "jxt1234",
        "body": "Features:\r\n1. Rewrite GRU Op for Onnx\r\n2. Support deconvolution for OpenCL Buffer mode\r\n\r\nBugfix / Refractor:\r\n1. Move NN from MNN_Express to MNN_Train \r\n2. Fix bug for LSTM model crash \r\n3. Add HardSwish Fuse\r\n4. Add Onnx::Not, Onnx::GatherElement\r\n5. Support TF::FusedBatchNormal when is_training = True\r\n6. Fix compute bug for Pooling3D\r\n7. Fix bug for CUDA backend convolution depthwise with multi-input crash",
        "dateCreated": "2021-04-16T07:43:14Z",
        "datePublished": "2021-04-21T01:55:40Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/1.1.6",
        "name": "More Op and bugfix",
        "tag_name": "1.1.6",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/1.1.6",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/41749106",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/1.1.6"
      },
      {
        "authorType": "User",
        "author_name": "jxt1234",
        "body": "Features:\r\n1. Add BF16 Support, only optimized for ARMv7a / ARMv8, set MNN_SUPPORT_BF16 to open it.\r\n2. Refract FP16 (ARM82) , speed up 10%-20% for benchmark model, support deconvolution / matmul.\r\n3. Support full Int8 compute, remove int8-float convert for data reorder Ops  (In Test)\r\n4. Support AVX512 for float compute\r\n\r\nOptimize / Bugfix:\r\n1. Fix bug for HIAI-NPU Backend's multi output\r\n2. Add GatherV2 grad for Training\r\n3. Add fastTestTflite.py for test tflite -> MNN correctly\r\n4. Add Dup Op Fuse in Graph Optimization\r\n5. Add GridSample for CPU / Metal\r\n6. Fix compile bug for Vulkan shader by glslang 4.5",
        "dateCreated": "2021-04-08T10:48:43Z",
        "datePublished": "2021-04-21T01:50:44Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/1.1.5",
        "name": "Support BF16, Speed Up FP16, AVX512, Full Quantization",
        "tag_name": "1.1.5",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/1.1.5",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/41748993",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/1.1.5"
      },
      {
        "authorType": "User",
        "author_name": "jxt1234",
        "body": "Features:\r\n1. Support OpenCL Backend for Buffer, user can use numberThread / mode to control it : https://www.yuque.com/mnn/cn/create_session\r\n2. Remove unuseful code for OpenCL Backend\r\n3. Support tflite's Binary with Activation\r\n\r\nBugfix:\r\n1. Fix bug for convolution compute zero shape tensor\r\n",
        "dateCreated": "2021-03-13T03:54:32Z",
        "datePublished": "2021-04-20T10:07:01Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/1.1.4",
        "name": "OpenCL Speed Up",
        "tag_name": "1.1.4",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/1.1.4",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/41699364",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/1.1.4"
      },
      {
        "authorType": "User",
        "author_name": "jxt1234",
        "body": "Features:\r\n1. Add HIAI Backend\r\n2. Refract the control flow support for tensorflow, support dynamic LSTM from TF\r\n3. Add several op for TensorRT Backend (GatherV2, Interp, ScatterND, DetectionPostProcess, BatchMatMul)\r\n\r\nSpeed up / Bugfix\r\n1. Add Prelu fuse for Tensorflow\r\n2. Fix some op bug for CUDA\r\n3. Fix some op bug for Metal (Relu6 / Not register)",
        "dateCreated": "2021-02-07T05:39:04Z",
        "datePublished": "2021-04-20T10:00:00Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/1.1.3",
        "name": "Add HIAI-NPU, While/Loop refract",
        "tag_name": "1.1.3",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/1.1.3",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/41698995",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/1.1.3"
      },
      {
        "authorType": "User",
        "author_name": "jxt1234",
        "body": "Features:\r\n1. Add OneDNN for Convolution Opt (In Test)\r\n2. Add Codegen for Fuse Op (In Test)\r\n3. Add TensorArray support (In Test)\r\n\r\nSpeed up / Bugfix:\r\n1. Support more binary op for ARMv8.2\r\n2. Optimize Int8 compute for AVX2 and Neon\r\n3. Fix bug for region fuse for spectial case\r\n4. Fix bug for Vulkan Convolution Winograd run error for feature map is large\r\n",
        "dateCreated": "2021-01-11T06:04:25Z",
        "datePublished": "2021-04-20T09:53:45Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/1.1.2",
        "name": "Add oneDNN, TensorArray support, Bugfix ",
        "tag_name": "1.1.2",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/1.1.2",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/41698639",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/1.1.2"
      },
      {
        "authorType": "User",
        "author_name": "jxt1234",
        "body": "Bugfix:\r\n1. Fix bug for memory leak when create session for some model.\r\n2. Fix metal backend's serveral bug.\r\n3. Small bugfix for ios demo\r\n4. Fix bug for 3d BatchNormal Module don't work for MNNTrain\r\n5. Fix memory leak for CPUInterp\r\n6. Fix bug for stack error for MNNPackedMatMulRemain.S\r\n7. Fix bug for SSE branch use AVX instruction\r\n\r\nOptimize:\r\n1. Reduce buffer create for metal execute.\r\n2. Reduce memory copy in CPUBatchMatMul.\r\n3. Reduce memory copy for Module\r\n4. Use neon to optimize CPUTopKV2\r\n5. Reduce memory usage for CUDA Backend by split and merge\r\n\r\nFeature:\r\n1. Support multi-instance for Module.\r\n2. Serveral CI scripts.\r\n3. More op for ARM82 Backend ()\r\n4. More op for CUDA (ArgMax, BatchMatMul, GatherV2, LayerNorm ......)\r\n",
        "dateCreated": "2020-12-20T14:10:04Z",
        "datePublished": "2021-01-06T08:20:33Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/1.1.1",
        "name": "Bugfix and speed up",
        "tag_name": "1.1.1",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/1.1.1",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/36034881",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/1.1.1"
      },
      {
        "authorType": "User",
        "author_name": "hush-alibaba",
        "body": "## \u4e00\u3001\u6846\u67b6\u901a\u7528\u6027\r\n### 1.1 \u51e0\u4f55\u8ba1\u7b97\r\n\u51e0\u4f55\u8ba1\u7b97\u662f\u672c\u6b21\u53d1\u5e03\u4e2d\u5927\u89c4\u6a21\u7684\u6846\u67b6\u91cd\u6784\u3002\u5b83\u5c06\u5927\u90e8\u5206\u7b97\u5b50\u7684\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u4e0e\u786c\u4ef6\u540e\u7aef\u65e0\u5173\u90e8\u5206\uff08\u5f62\u72b6\u8ba1\u7b97\u548c\u51e0\u4f55\u8ba1\u7b97\uff09\u5265\u79bb\u51fa\u6765\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e86\u5f02\u6784\u540e\u7aef\u7b97\u5b50\u5b9e\u73b0\u7684\u6210\u672c\u3002\u57fa\u4e8e\u51e0\u4f55\u8ba1\u7b97\uff0cMNN\u91cd\u5199\u4e86\u76ee\u524d\u6240\u6709\u7684\u786c\u4ef6\u540e\u7aef\u3002\u7531\u4e8e\u5f15\u5165\u51e0\u4f55\u8ba1\u7b97\u4e4b\u540eGPU\u540e\u7aef\u7b97\u5b50\u7684\u8986\u76d6\u7387\u7684\u589e\u52a0\uff0c\u5728\u963f\u91cc\u5df4\u5df4\u5185\u90e8\u7684\u4e1a\u52a1\u6a21\u578b\u4e2d\uff0cMNN GPU\u540e\u7aef\u6027\u80fd\u666e\u904d\u83b7\u5f97\u7ea620%\u63d0\u5347\u3002\r\n\r\n### 1.2 \u65b0\u589e\u540e\u7aef\r\n\u57fa\u4e8e\u51e0\u4f55\u8ba1\u7b97\u673a\u5236\uff0cMNN\u65b0\u589e\u4e86TensorRT\u548cCUDA\u540e\u7aef\u3002\u76ee\u524d\u5df2\u7ecf\u652f\u6301\u5e38\u7528CV\u6a21\u578b\u4e0eRNN\u6a21\u578b\u3002\r\n\r\n### 1.3 ASR\u6a21\u578b\u652f\u6301\r\n\u9664\u4e86\u4e1a\u52a1\u5e94\u7528\u5e7f\u6cdb\u7684CV\u6a21\u578b\uff0cMNN\u5728\u8fd9\u6b21\u53d1\u5e03\u4e2d\u6dfb\u52a0\u4e86\u5bf9\u57fa\u4e8eTransformer\u7ed3\u6784\u7684ASR\u6a21\u578b\u7684\u652f\u6301\u3002\u8fd9\u7c7b\u6a21\u578b\u7ed3\u6784\u8981\u6c42\u63a8\u7406\u5f15\u64ce\u652f\u6301Control Flow\u3001Dynamic Shape\u548cZero Shape\u7b49\u7279\u6027\u3002MNN\u5728\u6846\u67b6\u5c42\u9762\u5bf9\u8fd9\u4e9b\u7279\u6027\u8fdb\u884c\u4e86\u652f\u6301\u548c\u5b8c\u5584\uff1a\r\n\u2022 \u91cd\u6784Control Flow\u652f\u6301\u65b9\u6848\uff0c\u63d0\u4f9b\u7528\u6237\u900f\u660e\u7684functional control flow\u5b9e\u73b0\uff0c\u5e76\u652f\u6301TF1.x\u7684\u63a7\u5236\u6d41\u6a21\u578b\u8f6c\u6362\u3002\r\n\u2022 \u6dfb\u52a0Dynamic Shape\u7684\u652f\u6301\uff0cMNN\u5c06\u6574\u56fe\u6309\u7167\u52a8\u6001\u5f62\u72b6\u7b97\u5b50\u5212\u5206\u4e3a\u591a\u4e2a\u5206\u6bb5\u5b50\u56fe\u3002\u5728\u4ee3\u7801\u5c42\u9762\uff0c\u4e00\u4e2a\u5b50\u56fe\u5bf9\u5e94\u4e00\u4e2aModule\uff0cModule\u652f\u6301\u5d4c\u5957\uff0c\u5373\u6574\u56fe\u88ab\u8868\u8fbe\u4e3a\u4e00\u4e2a\u7531Module\u7ec4\u6210\u7684\u8c03\u7528\u6811\uff0c\u6811\u7684\u53f6\u5b50\u8282\u70b9\u53ef\u4ee5\u4f7f\u7528Session\u6765\u6267\u884c\uff0cSession\u6bcf\u6b21\u6267\u884c\u524dResize\uff0c\u91cd\u65b0\u8fdb\u884c\u5f62\u72b6\u63a8\u7406\u548c\u9884\u5206\u914d\u5185\u5b58\u3002\r\n\u2022 Zero Shape\u6307\u7684\u662f\u6a21\u578b\u4e2d\u67d0\u4e9bTensor\u7684shape\u5b58\u57280\u503c\uff0c\u6bd4\u5982 (1, 0, 256\uff09\uff0c\u8fd9\u79cd\u60c5\u51b5\u5927\u591a\u662f\u4e3a\u4e86\u7ed9while-loop\u4e2d\u67d0\u4e9b\u5faa\u73af\u53d8\u91cf\u63d0\u4f9b\u521d\u59cb\u503c\u800c\u5f15\u5165\u7684\u3002MNN\u5728\u5bf9\u5f62\u72b6\u63a8\u7406\u548c\u6267\u884c\u903b\u8f91\u4e0a\u5bf9Zero Shape\u8fdb\u884c\u4e86\u652f\u6301\u3002\r\n\r\n## \u4e8c\u3001\u6a21\u578b\u538b\u7f29\r\n\u65b0\u6dfb\u6a21\u578b\u538b\u7f29\u7684[\u4ec5\u6743\u503c\u91cf\u5316](https://www.yuque.com/mnn/cn/model_convert)\uff08MNNConvert --weightQuantBits\uff09\u3002\u6b64\u529f\u80fd\u4ec5\u5bf9conv/matmul/LSTM\u7684float32\u6743\u503c\u8fdb\u884c\u91cf\u5316\uff0c\u4ec5\u4f18\u5316\u6a21\u578b\u5927\u5c0f\uff0c\u52a0\u8f7d\u6a21\u578b\u540e\u4f1a\u89e3\u7801\u4e3afloat32\uff0c\u91cf\u5316\u4f4d\u5bbd\u53ef\u90092~8\uff0c\u8fd0\u884c\u901f\u5ea6\u548cfloat32\u6a21\u578b\u4e00\u81f4\u3002\u7ecf\u5185\u90e8\u6d4b\u8bd58bit\u65f6\u7cbe\u5ea6\u57fa\u672c\u65e0\u635f\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c0f4\u500d\u3002\r\n\r\n## \u4e09\u3001\u6027\u80fd\u4f18\u5316\r\n### ARM\u540e\u7aef\r\n\u5728\u4eca\u5e745\u6708\uff0cMNN\u5728ARM CPU\u4e0a\u7684\u6027\u80fd\u5df2\u7acb\u4e8e\u4e1a\u754c\u524d\u5217\uff08https://zhuanlan.zhihu.com/p/151666822\uff09\u3002\u5728\u6b64\u4e4b\u540e\uff0cMNN\u6301\u7eed\u6295\u5165ARM CPU\u6027\u80fd\u4f18\u5316\uff0c\u5728\u5404\u6a21\u578b\u548c\u82af\u7247\u4e0a\u53c8\u83b7\u5f97\u4e8610%~20%\u7684\u6027\u80fd\u63d0\u5347\u3002\u6027\u80fd\u63d0\u5347\u4e4b\u8def\u6c38\u65e0\u6b62\u5883\u3002\r\n![1](https://user-images.githubusercontent.com/56632408/98210106-04637f00-1f7b-11eb-9df5-8c31ecd11035.png)\r\n![2](https://user-images.githubusercontent.com/56632408/98210183-1cd39980-1f7b-11eb-8f09-cd29cd62ebef.png)\r\n\r\n### OpenCL\u540e\u7aef\r\n\u5f00\u542fAutoTuning\u7b49\u4e00\u7cfb\u5217\u4f18\u5316\u540e\uff0cMNN\u57281.0.0\u7684\u57fa\u7840\u4e0a\uff0c\u666e\u904d\u670920%~100%\u7684\u6027\u80fd\u63d0\u5347\u3002\u5177\u4f53\u6027\u80fd\u6570\u636e\u5982\u4e0b\uff1a\r\n![3](https://user-images.githubusercontent.com/56632408/98210252-3674e100-1f7b-11eb-8a57-196907dec310.png)\r\n![4](https://user-images.githubusercontent.com/56632408/98210273-3ecd1c00-1f7b-11eb-81b2-373dc6b1fc90.png)\r\n\r\n### x86\u540e\u7aef\r\n5\u6708\u4ee5\u6765\uff0cMNN\u56e2\u961f\u6301\u7eed\u6295\u5165x86\u540e\u7aef\u7684\u4f18\u5316\uff0c\u76ee\u524d\u6d6e\u70b9\u5355\u7ebf\u7a0b\u6027\u80fd\u4e0e\u884c\u4e1a\u6807\u6746OpenVINO\u57fa\u672c\u6301\u5e73\uff0c\u90e8\u5206\u60c5\u51b5 (Squeezenet v1.0) \u8d85\u8d8a\u3002\r\n![5](https://user-images.githubusercontent.com/56632408/98210339-60c69e80-1f7b-11eb-9c77-5dffefd688fc.png)\r\n\r\n## \u56db\u3001\u6846\u67b6\u6613\u7528\u6027\r\nInterpreter::setCacheFile API\r\n\u7531\u4e8eOpenCL\u65b0\u589e\u7684AutoTuning\u673a\u5236\u3001TensorRT\u540e\u7aef\u521d\u6b21\u63a8\u7406\u7684\u8017\u65f6\u8f83\u9ad8\uff0cMNN\u5728Interpreter\u4e0a\u589e\u52a0setCacheFile API\uff0c\u7528\u4e8e\u7f13\u5b58GPU\u540e\u7aef\u7684\u7f16\u8bd1\u4f18\u5316\u4e4b\u540e\u7684\u6a21\u578b\u3002\u7528\u6cd5\u5982\u4e0b\uff1a\r\n```\r\n    std::shared_ptr<MNN::Interpreter> net =\r\n        std::shared_ptr<MNN::Interpreter>(MNN::Interpreter::createFromFile(fileName));\r\n    if (nullptr == net) {\r\n        return 0;\r\n    }\r\n    // Must call it before createSession.\r\n    // If \".tempcache\" file does not exist, Interpreter will go through the \r\n    // regular initialization procedure, after which the compiled model files \r\n    // will be written  to \".tempcache\".\r\n    // If \".tempcache\" file exists, the Interpreter will be created from the\r\n    // cache.\r\n    net->setCacheFile(\".tempcache\");\r\n    \r\n    MNN::ScheduleConfig config;\r\n    // Creates the session after you've called setCacheFile.\r\n    MNN::Session* session = net->createSession(config);\r\n```\r\n\r\n## \u4e94\u3001Bugfixes\r\n\u4fee\u6b63\u4f46\u4e0d\u9650\u4e8e\u5982\u4e0b Bug\uff1a\r\n1. SpaceToBatchND , BatchToSpaceND \u652f\u6301 block size / padding \u4f5c\u4e3a\u8f93\u5165\uff08\u652f\u6301\u5728\u8f93\u5165shape \u672a\u77e5\u60c5\u51b5\u4e0b\u7684 Tensorflow \u7a7a\u6d1e\u5377\u79ef\uff09\r\n2. \u4fee\u6b63 depthToSpace \u548c spaceToDepth \uff0c\u652f\u6301 pixelshuffle\r\n3. \u4fee\u6b63 1x1 \u5377\u79ef\u5bf9\u4e8e batch \u8f83\u5927\uff0cwidth / height \u8f83\u5c0f\u65f6\uff0c\u6027\u80fd\u4e0d\u597d\u7684\u95ee\u9898\r\n4. \u4fee\u6b63 Onnx \u7684 ConvTranspose output padding \u652f\u6301\u95ee\u9898\r\n5. \u4fee\u6b63 Onnx \u7684 Resize \u5728\u67d0\u4e9b\u8f93\u5165\u4e2a\u6570\u4e0b\u4e0d\u652f\u6301\u7684\u95ee\u9898\r\n\r\n## \u516d\u3001One More Thing\r\nMNN\u6709\u81ea\u5df1\u7684\u5b98\u7f51\u5566\uff01[\u5b98\u7f51](https://www.mnn.zone/)\u4e0a\u8fd8\u53ef\u4ee5\u4e0b\u8f7dMNN\u56e2\u961f\u5168\u65b0\u529b\u4f5c\u300cMNN\u5de5\u4f5c\u53f0\u300d\uff0c\u6db5\u76d6\u5f00\u7bb1\u5373\u7528\u6a21\u578b\u3001\u53ef\u89c6\u5316\u8bad\u7ec3\u7b49\u5de5\u5177\uff0c\u66f4\u53ef\u4ee5\u4e00\u952e\u90e8\u7f72\u5230\u591a\u7aef\u8bbe\u5907\u3002\r\n![6](https://user-images.githubusercontent.com/56632408/98215708-121d0280-1f83-11eb-89e8-c2a27d2516fd.png)\r\n",
        "dateCreated": "2020-11-05T08:49:17Z",
        "datePublished": "2020-11-05T09:20:12Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/1.1.0",
        "name": "MNN 1.1.0 Release Notes",
        "tag_name": "1.1.0",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/1.1.0",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/33490851",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/1.1.0"
      },
      {
        "authorType": "User",
        "author_name": "hush-alibaba",
        "body": "As many of you MNN fans may know, MNN was initially released in May 2019 by a  team of  enthusiastic Alibaba engineers who thought existing open source DL frameworks at the time weren\u2019t good enough to handle the particular and demanding requirements of a billion-user mega application such as Taobao and that it was better to roll their own.\r\n\r\nAnd so we did it. We built an inference engine from the ground up and we supported a vast variety of applications inside Alibaba with it. It was many times faster (and smaller!) than other inference engines such as TFLite and NCNN. We open sourced it and you guys loved it. The 3.9k stars on the Github repo are 3.9k votes of confidence from our biggest fans: you. What\u2019s more, we are thrilled to publish our paper on MNN design principles in MLSys 2020 (Read the paper here: https://proceedings.mlsys.org/static/paper_files/mlsys/2020/7-Paper.pdf).\r\n\r\nA year has passed since the initial release and we\u2019ve made a great number of new features, improvements and bug fixes. And today we\u2019re delighted to announce the 1.0.0 official stable release of MNN. In this release, we add training capability to MNN, which means MNN is no longer an \u201cinference\u201d engine, but a fully-fledged DL engine capable of training and inference, with numerous hardware backends including Arm (32bit, 64bit, v8.2), x86, Metal, OpenCL, Vulkan and OpenGL.\r\n\r\nThe following are some release highlights.\r\n\r\n# Highlights\r\nThere are three major new features in this MNN stable release: training and quantization, performance improvements and MNN Python API (currently in BETA).\r\n\r\n# Training and Quantization With the Express API\r\nWe added a new set of C++ APIs (located in the `express/` directory) to dynamically construct a graph, run training and inference, quantize models, etc.  These APIs stand side-by-side with the existing APIs to use MNN and will be the officially recommended API surface once we iron out more details.\r\n\r\nThis new set of C++ APIs allows you to:\r\n1. train a model from scratch using MNN,\r\n2. finetune a model trained by other DL frameworks,\r\n3. and perform quantization-aware training (a.k.a QAT).\r\n\r\nThe documentation has been updated with instructions. ([Quantization-aware Training](https://www.yuque.com/mnn/en/tqr7ft))\r\n\r\nThe following table is a comparison of model accuracy and size between MNN QAT and Tensorflow QAT using MobileNet V2.\r\n\r\n![image](https://user-images.githubusercontent.com/56632408/81364172-a2cf6e80-9117-11ea-8d25-bbeaff499489.png)\r\n\r\n---\r\n**Note 1**\r\n\r\nBoth training and eval use ImageNet. Training batchsize is 32, with 100 iterations, i.e. 3200 images are used in training. Eval phase uses 50000 images.\r\n\r\n**Note 2**\r\n\r\nThe original tensorflow model has an accuracy of 71.8%. Due to minor differences in image preprocessing, the original model in MNN format has a higher accuracy than the TF original.\r\n\r\n---\r\n\r\nOn top of the improved accuracy with MNN QAT, MNN has great training speed on-device and on a PC. The following graph shows the time (in secs) to train a Lenet model with MNIST from scratch.\r\n\r\n![image](https://user-images.githubusercontent.com/56632408/81364461-46208380-9118-11ea-9a9e-7021baf4ec4e.png)\r\n\r\nAs you can see, MNN training performance on a mobile phone (MI6 or Mate 20) is comparable to the performance of PyTorch or Caffe on a 2015 Macbook Pro. What's more, MNN training speed on a Mac is more than 2 times as fast as PyTorch.\r\n\r\n# Performance Improvements\r\nWe have been hard at work pushing the limits of Arm CPU and x86 CPU, and bringing every bit of performance improvement into MNN.\r\n\r\n## Arm V8.2 Performance Improvements\r\nWe utilized asimdhp (Advanced SIMD half precision) extension and asimddp (Advanced SIMD dot product) in ArmV8.2 to increase inference speed by roughy 100%.\r\n\r\n**Inference time (ms) before and after half precision optimization:**\r\nMNN doubles the inference performance without any noticeable drop in accuracy. Details below:\r\n\r\n![armv82fp16](https://user-images.githubusercontent.com/56632408/81364577-94ce1d80-9118-11ea-8276-8a1de8547f84.png)\r\n\r\n**Inference time (ms) before and after SIMD dot product optimization:**\r\nMNN doubles the speed with SIMD dot product optimization, even on devices whose int8 performance is worse than fp32. Details below:\r\n\r\n![armv8 2sdot](https://user-images.githubusercontent.com/56632408/81364629-af07fb80-9118-11ea-8b0c-0fb468a6436c.png)\r\n\r\n## Arm64 Performance Improvements\r\nBy rethinking matrix multiplication tiling, cacheline alignment and cache prefetch, we've made significant performance improvements on mid to low tier chipsets (Qualcomm 652 and 425), which are the types of devices where a sizable perf improvement could mean the difference between smooth 30 fps and a janky unusable experience.\r\n\r\nInference speed (ms) for MobileNetV1 before and after optimization.\r\n![image](https://user-images.githubusercontent.com/56632408/81364657-c0e99e80-9118-11ea-9a94-2a43bf5d6f25.png)\r\n\r\n## X86 Performance Improvements\r\nEven though Arm platform is MNN's \"basecamp\", we've been investing in the x86 platform so as to make MNN training on laptops faster.\r\n\r\nBy improving the weight matrix layout and turning on FMA instruction, we've improved the x86 performance by about 40% ~ 60% single-threaded. (The performance improvement is 100% compared to MNN at initial release).\r\n\r\nThe following table shows the inference performance before and after the optimization.\r\n![image](https://user-images.githubusercontent.com/56632408/81364682-d1017e00-9118-11ea-85c0-9efed2f34978.png)\r\n\r\n# New Python Express API (BETA)\r\nThe aforementioned \"Express API\" in C++ allows you to train a model with MNN, finetune a model and perform quantization-aware training with MNN. We added a set of corresponding Python Express APIs for people with prior experience of Tensorflow or PyTorch. The new Python Express API documentation is [here](https://www.yuque.com/mnn/en/usage_in_python#JoQeY).\r\n\r\nThe new Python Express API is currently in beta as we strive to make it more pythonic and PyTorch-esque. In the mean time, please give us feedback by posting an issue on Github and leave a message in the Dingtalk group.\r\n",
        "dateCreated": "2020-05-07T10:22:11Z",
        "datePublished": "2020-05-08T02:54:21Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/1.0.0",
        "name": "Advancing Our Amazing DL Engine: MNN 1.0.0",
        "tag_name": "1.0.0",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/1.0.0",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/26297189",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/1.0.0"
      },
      {
        "authorType": "User",
        "author_name": "jxt1234",
        "body": "1. Speed up inference ~30% for x86 cpu , support fma by set MNN_FMA_ENABLE = ON\r\n2. Fix compile bug of vulkan / converter in windows\r\n3. Add plugin support\r\n4. Add a few op support for onnx\r\n\r\nAppendix:\r\n\r\n1-thread MacBook Pro (Retina, 15-inch, Mid 2015) | Before opt / ms | After opt / ms\r\n-- | -- | --\r\nResnet18 | 81 | 48\r\nMobilenetv1.10-224 | 37 | 26\r\n\r\n\r\n4-Thread MacBook Pro (Retina, 15-inch, Mid 2015) | Before optimize / ms | After optimize / ms\r\n-- | -- | --\r\nresnet-v2-50.mnn | 54.042 | 39.547\r\nMobileNetV2_224.mnn | 7.748 | 5.915\r\nmobilenet-v1-1.0.mnn | 12.172 | 7.746\r\nSqueezeNetV1.0.mnn | 20.729 | 11.133\r\ninception-v3.mnn | 76.550 | 59.054\r\n\r\n",
        "dateCreated": "2020-04-15T07:34:58Z",
        "datePublished": "2020-04-15T11:04:17Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.2.3",
        "name": "X86-Optimize and Plugin support",
        "tag_name": "0.2.2.3",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.2.3",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/25524581",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.2.3"
      },
      {
        "authorType": "User",
        "author_name": "jxt1234",
        "body": "1\u3001Fix several bug for tflite converter\r\n2\u3001Speed up ARM v8a for A53 / A55 / A72 / A73\r\n\r\n\u5c0f\u7c735\uff0cARM 64 \u5355\u7ebf\u7a0b | \u4f18\u5316\u524d / ms | \u4f18\u5316\u540e / ms\r\n-- | -- | --\r\nresnet-v2-50.mnn | 529.041 | 493.421\r\nMobileNetV2_224.mnn | 65.550 | 64.877\r\ninception-v3 | 826.439 | 778.218\r\nSqueezeNetV1.0.mnn | 135.534 | 130.932\r\nmobilenet-v1-1.0.mnn | 104.304 | 104.232\r\n",
        "dateCreated": "2020-04-03T05:00:09Z",
        "datePublished": "2020-04-07T05:10:37Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.2.2",
        "name": "Bugfix and low level arm speed up.",
        "tag_name": "0.2.2.2",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.2.2",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/25262985",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.2.2"
      },
      {
        "authorType": "User",
        "author_name": "jxt1234",
        "body": "1\u3001Support CPU fp16 and sdot  on ARM v8.2+ (qualcomm snapdragon 845+, qualcomm snapdragon 660+, kirlin 980+, kirlin810+), speed up about 100% on these chipes. You can just open the macro MNN_ARM82 and load libMNN_Arm82.so to enable it.\r\n2\u3001Add a few ops for Vulkan and reduce the prepare cost of vulkan.",
        "dateCreated": "2020-03-23T07:01:40Z",
        "datePublished": "2020-03-23T14:34:19Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.2.1",
        "name": "ARM v8.2+ support and Vulkan backend enhance",
        "tag_name": "0.2.2.1",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.2.1",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/24775206",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.2.1"
      },
      {
        "authorType": "User",
        "author_name": "jxt1234",
        "body": "1\u3001Support  quantization aware training. See https://www.yuque.com/mnn/cn/bhz5eu for detail.\r\n2\u3001Support python API for MNN-Express. See pymnn/examples/MNNTrain for detail.\r\n3\u3001Speed up several case of binary op.",
        "dateCreated": "2020-03-15T10:20:43Z",
        "datePublished": "2020-03-23T04:45:28Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.2.0",
        "name": "Support quantization aware training",
        "tag_name": "0.2.2.0",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.2.0",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/24759101",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.2.0"
      },
      {
        "authorType": "User",
        "author_name": "Naville",
        "body": "",
        "dateCreated": "2020-03-07T09:03:58Z",
        "datePublished": "2020-03-09T06:36:01Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.1.9",
        "name": "0.2.1.9",
        "tag_name": "0.2.1.9",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.1.9",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/24335339",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.1.9"
      },
      {
        "authorType": "User",
        "author_name": "Naville",
        "body": "- Vulkan winograd bug Fix\r\n- Fix Segment Fault issue crash when converting TFLite Models\r\n- Workaround Metal Softmax Op\r\n- Lower required OpenCL version to 1.10 from 2.0 to 1.10\r\n\r\n- vulkan winograd bug \u4fee\u590d\r\n- \u4fee\u6b63 tflite \u8f6c\u6362\u65f6\u6bb5\u9519\u8bef\u95ee\u9898\r\n- metal softmax bug \u6682\u65f6\u89c4\u907f\r\n- OpenCL \u6240\u9700\u8981\u7248\u672c\u7531 2.0 \u964d\u5230 1.10 \uff0c\u63d0\u5347\u517c\u5bb9\u6027",
        "dateCreated": "2020-01-16T08:55:46Z",
        "datePublished": "2020-01-17T12:08:11Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.1.7",
        "name": "0.2.1.7",
        "tag_name": "0.2.1.7",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.1.7",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/22887201",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.1.7"
      },
      {
        "authorType": "User",
        "author_name": "Naville",
        "body": "# Op Support\r\nAdded support for: \r\n- Over 30 TFLite Ops\r\n- Over 20 Onnx Ops\r\n- 9 Caffe Ops\r\n- 24 Tensorflow Ops\r\n\r\n# Project Layout and Engineering Improvements\r\n- CMake Build System Rewrite\r\n- Header Layout Standardization. Public headers are now under ``include/MNN`` and is installed as ``<MNN/>``\r\n\r\n# Inferencing Improvements and Bug Fixes\r\n- OpenCL BinaryOp Bugs\r\n- CPU MatMul Bugs\r\n- Added Unit Testing for over 30 ops\r\n- ImageProcess now supports NV12 input and NV21 / NV12 stride\r\n\r\n# Training (Experimental Feature)\r\n- Optimize ``Express`` Module's Dynamic Graph Execution Policy\r\n- Improve single machine training with a working demo\r\n\r\n\r\n# Op \u8865\u5168\r\n- \u65b0\u589e TFlite 30 + op \u652f\u6301\r\n- \u65b0\u589e Onnx 20 + op \u652f\u6301\r\n- \u65b0\u589e Caffe 9 \u4e2a op \u652f\u6301\r\n- \u65b0\u589e Tensorflow 24 \u4e2a op \u652f\u6301\r\n# \u5de5\u7a0b\u4f18\u5316\r\n- \u5b8c\u5584CMake\u76f8\u5173\u7f16\u8bd1\u914d\u7f6e\r\n- \u5934\u6587\u4ef6\u76ee\u5f55\u89c4\u8303\u5316\uff0c\u539f include \u76ee\u5f55\u4e0b\u7684\u5934\u6587\u4ef6\u79fb\u5230 include/MNN \u4e0b\r\n# \u63a8\u7406\u76f8\u5173\u529f\u80fd\u5b8c\u5584\u4e0eBug\u4fee\u590d\r\n- OpenCL BinaryOp \u76f8\u5173 Bug \u4fee\u590d\r\n- CPU MatMul Bug \u4fee\u590d\r\n- \u5b8c\u5584\u5355\u5143\u6d4b\u8bd5\uff0c\u6dfb\u52a0 30 + op \u5355\u5143\u6d4b\u8bd5\u7528\u4f8b\r\n- ImageProcess \u652f\u6301 NV12 \u8f93\u5165\u4ee5\u53ca NV21 / NV12 stride \u652f\u6301\r\n# \u8bad\u7ec3\u80fd\u529b(\u6574\u4f53\u4ecd\u5904\u8c03\u8bd5\u9636\u6bb5)\r\n- \u4f18\u5316 Express \u6a21\u5757\u52a8\u6001\u56fe\u8fd0\u884c\u673a\u5236\r\n- MNN \u5355\u673a\u8bad\u7ec3\u529f\u80fd\u5b8c\u5584\uff0cDemo\u5b8c\u6210",
        "dateCreated": "2019-12-31T02:36:59Z",
        "datePublished": "2019-12-31T05:04:33Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.1.6",
        "name": "0.2.1.6",
        "tag_name": "0.2.1.6",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.1.6",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/22526634",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.1.6"
      },
      {
        "authorType": "User",
        "author_name": "li-qing",
        "body": "0.2.1.5\r\n\r\n# integration\r\n- add travis CI\r\n- fix building parameters for python\r\n\r\n# converter\r\n- add half storage option for MNN converter\r\n- fix op name lost in converter\r\n- fix converter bug for print input output, identity remove output\r\n\r\n# ops\r\n- add quantized Convolution & Deconvolution support on OpenCL\r\n- add more expression supports\r\n- add DetectionPostProcess Op for TensorFlow Lite (ssd is supported directly now)\r\n- add supports for LSTM & ELU for ONNX\r\n- add support for Convolution that weights is not constant for ONNX\r\n- fix Unary Op compile error on Linux\r\n- fix Metal backend buffer reuse after resize\r\n- fix Metal raw memory access after model releasing\r\n- fix redundant transpose in Winograd generater",
        "dateCreated": "2019-11-15T06:22:45Z",
        "datePublished": "2019-11-15T06:23:42Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.1.5",
        "name": "",
        "tag_name": "0.2.1.5",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.1.5",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/21497091",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.1.5"
      },
      {
        "authorType": "User",
        "author_name": "li-qing",
        "body": "### build\r\n- unify schema building in core and converter;\r\n- add more build script for android;\r\n- add linux build script for python;\r\n\r\n### ops impl\r\n- add floor mod support in binary;\r\n- use eltwise impl in add/max/sub/mul binary for optimization;\r\n- remove fake double support in cast;\r\n- fix 5d support for concat;\r\n- add adjX and adjY support for batch matmul;\r\n- optimize conv2d back prop filter;\r\n- add pad mode support for conv3d;\r\n- fix bug in conv2d & conv depthwise with very small feature map;\r\n- optimize binary without broacast;\r\n- add data types support for gather;\r\n- add gather ND support;\r\n- use uint8 data type in gather v2;\r\n- add transpose support for matmul;\r\n- add matrix band part;\r\n- add dim != 4 support for padding, reshape & tensor convert;\r\n- add pad type support for pool3d;\r\n- make ops based on TensorFlow Lite quantization optional;\r\n- add all & any support for reduction;\r\n- use type in parameter as output type in reduction;\r\n- add int support for unary;\r\n- add variable weight support for conv2d;\r\n- fix conv2d depthwise weights initialization;\r\n- fix type support for transpose;\r\n- fix grad outputs count for reduce grad and reshape grad;\r\n- fix priorbox & detection output;\r\n- fix metal softmax error;\r\n\r\n### python\r\n- add runSessionWithCallBackInfo interface;\r\n- add max nodes limit (1400) for visualization tool;\r\n- fix save error in python3;\r\n- align default dim;\r\n\r\n### convert\r\n- add extra design for optimization;\r\n- add more post converting optimizers;\r\n- add caffe v1 weights blob support;\r\n- add cast, unary, conv transpose support for onnx model;\r\n- optimize batchnorm, conv with variable weights, prelu, reshape, slice, upsample for onnx model;\r\n- add cos/sin/atan/tan support for unary for tensorflow model;\r\n- add any/all support for reduction for tensorflow model;\r\n- add elu, conv3d, pool3d support for tensorflow model;\r\n- optimize argmax, batchnorm, concat, batch to space, conv with variable weights, prelu, slice for tensorflow model;\r\n\r\n### others\r\n- fix size computer lock;\r\n- fix thread pool deadlock;\r\n- add express & parameters in express;\r\n- rewrite blitter chooser without static map;\r\n- add tests for expr;",
        "dateCreated": "2019-10-29T07:11:53Z",
        "datePublished": "2019-10-29T07:14:05Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.1.2",
        "name": "0.2.1.2",
        "tag_name": "0.2.1.2",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.1.2",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/21043943",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.1.2"
      },
      {
        "authorType": "User",
        "author_name": "li-qing",
        "body": "0.2.1.0\r\n- dynamic computation graph (beta)\r\n\t- add supports (/express)\r\n\t- add tests\r\n\t- add benchmarks with it (/benchmark/exprModels)\r\n- Python\r\n\t- MNN engine and tools were submitted to pip\r\n\t- available on Windows/macOS/Linux\r\n- Engine/Converter\r\n\t- add supports for each op benchmarking\r\n\t- refactor optimizer by separating steps\r\n- CPU\r\n\t- add supports for Conv3D, Pool3D, ELU, ReverseSequence\r\n\t- fix ArgMax, Permute, Scale, BinaryOp, Slice, SliceTf\r\n- OpenCL\r\n\t- add half transform in CPU\r\n\t- add broadcast supports for binary\r\n\t- optimize Conv2D, Reshape, Eltwise, Gemm, etc.\r\n- OpenGL\r\n\t- add sub, real div supports for binary\r\n\t- add supports for unary\r\n\t- optimize Conv2D, Reshape\r\n- Vulkan\r\n\t- add max supports for eltwise\r\n- Metal\r\n\t- fix metallib missing problem\r\n- Train/Quantization\r\n\t- use express to refactor training codes",
        "dateCreated": "2019-09-26T13:02:07Z",
        "datePublished": "2019-09-26T13:03:10Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.1.0",
        "name": "0.2.1.0",
        "tag_name": "0.2.1.0",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.1.0",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/20276542",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.1.0"
      },
      {
        "authorType": "User",
        "author_name": "li-qing",
        "body": "beta 0.2.0.9\r\n- fix quantization tool compiling on Windows\r\n- fix converter compiling on Windows\r\n- fix eltwise optimization on Windows\r\n- separate sse & avx for Windows\r\n- add LeakyReLU support for TensorFlow\r\n- fix reshape, const for TensorFlow\r\n- fix dimension format error for ONNX ops\r\n- optimize winograd, ReLU for OpenCL\r\n- add fp16 availability & dimensions size check-up for OpenCL\r\n- optimize GEMM for arm32\r\n- fix ExpandDims shape calculation when inputs size == 1",
        "dateCreated": "2019-09-01T11:25:26Z",
        "datePublished": "2019-09-01T11:25:53Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.0.9",
        "name": "",
        "tag_name": "0.2.0.9",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.0.9",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/19678614",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.0.9"
      },
      {
        "authorType": "User",
        "author_name": "li-qing",
        "body": "beta 0.2.0.8\r\n- add NaN check-up\r\n- add quantification support for ScaleAdd Op\r\n- add binary to eltwise optimization\r\n- add console logs for quantization tool\r\n- better document for quantization tool\r\n- replace redundant dimension flags with dimension format\r\n- optimize performance of TensorFlow Lite Quantized Convolution\r\n- fix axis support for ONNX softmax converting\r\n- fix getPerformance tool compiling error on Windows",
        "dateCreated": "2019-08-22T12:13:46Z",
        "datePublished": "2019-08-22T12:15:56Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.0.8",
        "name": "",
        "tag_name": "0.2.0.8",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.0.8",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/19464072",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.0.8"
      },
      {
        "authorType": "User",
        "author_name": "li-qing",
        "body": "- move docs to http://www.yuque.com/mnn\r\n- fix bugs for CPU ops TopKV2 and quantized convolution\r\n- add enqueue map buffer error handle for OpenCL\r\n- add nullptr protection for extra tensor desc\r\n- add failure protection for memory acquirement\r\n- fix slice shape calculation\r\n- refactor binary shape calculation",
        "dateCreated": "2019-08-15T09:30:39Z",
        "datePublished": "2019-08-15T09:31:50Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.0.7",
        "name": "",
        "tag_name": "0.2.0.7",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.0.7",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/19308755",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.0.7"
      },
      {
        "authorType": "User",
        "author_name": "li-qing",
        "body": "- fix bugs in quantization\r\n- add evaluating tool for quantization\r\n- add ADMM support in quantization\r\n- fix lock in thread pool\r\n- fix fusing for deconv\r\n- fix reshape converting from ONNX to MNN\r\n- turn off blob size checking by default",
        "dateCreated": "2019-08-07T08:44:09Z",
        "datePublished": "2019-08-07T08:47:19Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.0.6",
        "name": "",
        "tag_name": "0.2.0.6",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.0.6",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/19129128",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.0.6"
      },
      {
        "authorType": "User",
        "author_name": "li-qing",
        "body": "beta 0.2.0.5\r\n- CPU\r\n- add support for DepthToSpace & SpaceToDepth ops\r\n- OpenGL\r\n- add Android demo\r\n- add half / float runtime option\r\n- add support for ROIPooling, Squeeze\r\n- fix bugs in conv im2col\r\n- OpenCL\r\n- fix Concat, Eltwise, Reshape bugs\r\n- Tools\r\n- add KL threshold method in quantization tool\r\n- support optimization for graph with multiple rnn",
        "dateCreated": "2019-07-25T05:36:35Z",
        "datePublished": "2019-08-05T02:41:38Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.0.5",
        "name": "",
        "tag_name": "0.2.0.5",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.0.5",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/19068596",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.0.5"
      },
      {
        "authorType": "User",
        "author_name": "li-qing",
        "body": "beta 0.2.0.4\r\n- bug fix for quantization tool\r\n- bug fix/performance update for thread pool\r\n- bug fix for converters\r\n- tutorial/doc update\r\n- more op support",
        "dateCreated": "2019-07-19T09:36:12Z",
        "datePublished": "2019-08-05T02:41:12Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.0.4",
        "name": "",
        "tag_name": "0.2.0.4",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.0.4",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/19068593",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.0.4"
      },
      {
        "authorType": "User",
        "author_name": "li-qing",
        "body": "beta 0.2.0.3\r\n\r\n- add quantization tool & cpu impl & demo/exec\r\n- add thread pool\r\n- add tests\r\n- fix onnx converter tensor name mismatch\r\n- optimize cpu performance with SSE for windows",
        "dateCreated": "2019-07-11T05:56:52Z",
        "datePublished": "2019-07-11T05:58:48Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.0.3",
        "name": "beta 0.2.0.3",
        "tag_name": "0.2.0.3",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.0.3",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/18542907",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.0.3"
      },
      {
        "authorType": "User",
        "author_name": "li-qing",
        "body": "beta 0.2.0.2\r\n- CPU\r\n  - add padding support\r\n  - fix bug in permute when channel % 4 != 0\r\n  - fix bug in exp with extreme value\r\n- OpenCL\r\n  - add protecting logics\r\n- OpenGL\r\n  - add protecting logics\r\n  - support NCHW format in Squeeze and Reshape\r\n- Converter\r\n  - add ShuffleChannel support for Caffe\r\n  - add Clip/Transpose/Unary/Pad supports for ONNX",
        "dateCreated": "2019-07-02T10:01:08Z",
        "datePublished": "2019-07-02T10:01:54Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.0.2",
        "name": "",
        "tag_name": "0.2.0.2",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.0.2",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/18354182",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.0.2"
      },
      {
        "authorType": "User",
        "author_name": "li-qing",
        "body": "beta 0.2.0.1\r\n\r\n- support both armv7/arm64 in podspec (pod version >= 1.5.0 required)\r\n- refactor neg axis support\r\n- fix memory overlap in de-conv\r\n- fix CONVOLUTION_TILED_NUMBER spell error\r\n- fix few warnings\r\n- add binary / interp / permute / relu / reshape / softmax support and optimize conv for OpenGL backend\r\n- add clean in nmake build script",
        "dateCreated": "2019-06-24T03:32:41Z",
        "datePublished": "2019-06-24T03:36:53Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.0.1",
        "name": "0.2.0.1",
        "tag_name": "0.2.0.1",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.0.1",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/18171160",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.0.1"
      },
      {
        "authorType": "User",
        "author_name": "li-qing",
        "body": "beta 0.2.0.0 \r\n- replace FreeImage with stb_image \r\n- warn unicode error in Windows compiling \r\n- separate clang/gcc build script for android \r\n- add default values in fbs \r\n- optimize CPU conv / conv depthwise / deconv / deconv depthwise / lstm / sigmoid \r\n- add sub support in eltwise \r\n- add reciprocal / log1p / log in unary \r\n- add zero like / select / set diff 1d \r\n- add batch support for permute \r\n- add training codes \r\n- fix metal error in dynamic separate storage type handling",
        "dateCreated": "2019-06-17T12:10:35Z",
        "datePublished": "2019-06-17T12:12:20Z",
        "html_url": "https://github.com/alibaba/MNN/releases/tag/0.2.0.0",
        "name": "beta 0.2.0.0",
        "tag_name": "0.2.0.0",
        "tarball_url": "https://api.github.com/repos/alibaba/MNN/tarball/0.2.0.0",
        "url": "https://api.github.com/repos/alibaba/MNN/releases/18035252",
        "zipball_url": "https://api.github.com/repos/alibaba/MNN/zipball/0.2.0.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6308,
      "date": "Mon, 27 Dec 2021 13:07:07 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Scan the following QR codes to join Dingtalk discussion group. The group discussions are predominantly Chinese. But we welcome and will help English speakers.\n\nGroup #1 (Full):\n\n<img src=\"doc/DingTalkQR1.png\" height=\"256\"/>\n\nGroup #2 (Full):\n\n<img src=\"doc/DingTalkQR2.png\" height=\"256\"/>\n\nGroup #3:\n\n<img src=\"doc/DingTalkQR3.png\" height=\"256\"/>\n\n",
      "technique": "Header extraction"
    }
  ],
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "mnn",
      "convolution",
      "vulkan",
      "embedded-devices",
      "winograd-algorithm",
      "machine-learning",
      "deep-learning",
      "ml",
      "deep-neural-networks",
      "arm"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Efficient image processing module, speeding up affine transform and color space transform without libyuv or opencv.\n- Provides callbacks throughout the workflow to extract data or control the execution precisely.\n- Provides options for selecting inference branch and paralleling branches on CPU and GPU.\n- (BETA) MNN Python API helps ML engineers to easily use MNN to build a model, train it and quantize it, without dipping their toes in C++ code.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Scan the following QR codes to join Dingtalk discussion group. The group discussions are predominantly Chinese. But we welcome and will help English speakers.\n\nGroup #1 (Full):\n\n<img src=\"doc/DingTalkQR1.png\" height=\"256\"/>\n\nGroup #2 (Full):\n\n<img src=\"doc/DingTalkQR2.png\" height=\"256\"/>\n\nGroup #3:\n\n<img src=\"doc/DingTalkQR3.png\" height=\"256\"/>\n\n",
      "technique": "Header extraction"
    }
  ]
}