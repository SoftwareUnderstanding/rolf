{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2109.06387",
      "https://arxiv.org/abs/1606.06031",
      "https://arxiv.org/abs/2109.06387",
      "https://arxiv.org/abs/2109.06387",
      "https://arxiv.org/abs/1301.3781"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{vafa2021rationales,\n  title={Rationales for Sequential Predictions},\n  author={Vafa, Keyon and Deng, Yuntian and Blei, David M and Rush, Alexander M},\n  booktitle={Empirical Methods in Natural Language Processing},\n  year={2021}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "cff-version: \"1.2.0\"\ndate-released: 2020-10\nmessage: \"If you use this software, please cite it using these metadata.\"\ntitle: \"Transformers: State-of-the-Art Natural Language Processing\"\nurl: \"https://github.com/huggingface/transformers\"\nauthors: \n  - family-names: Wolf\n    given-names: Thomas\n  - family-names: Debut\n    given-names: Lysandre\n  - family-names: Sanh\n    given-names: Victor\n  - family-names: Chaumond\n    given-names: Julien\n  - family-names: Delangue\n    given-names: Clement\n  - family-names: Moi\n    given-names: Anthony\n  - family-names: Cistac\n    given-names: Perric\n  - family-names: Ma\n    given-names: Clara\n  - family-names: Jernite\n    given-names: Yacine\n  - family-names: Plu\n    given-names: Julien\n  - family-names: Xu\n    given-names: Canwen\n  - family-names: \"Le Scao\"\n    given-names: Teven\n  - family-names: Gugger\n    given-names: Sylvain\n  - family-names: Drame\n    given-names: Mariama\n  - family-names: Lhoest\n    given-names: Quentin\n  - family-names: Rush\n    given-names: \"Alexander M.\"\npreferred-citation:\n  type: inproceedings\n  authors:\n  - family-names: Wolf\n    given-names: Thomas\n  - family-names: Debut\n    given-names: Lysandre\n  - family-names: Sanh\n    given-names: Victor\n  - family-names: Chaumond\n    given-names: Julien\n  - family-names: Delangue\n    given-names: Clement\n  - family-names: Moi\n    given-names: Anthony\n  - family-names: Cistac\n    given-names: Perric\n  - family-names: Ma\n    given-names: Clara\n  - family-names: Jernite\n    given-names: Yacine\n  - family-names: Plu\n    given-names: Julien\n  - family-names: Xu\n    given-names: Canwen\n  - family-names: \"Le Scao\"\n    given-names: Teven\n  - family-names: Gugger\n    given-names: Sylvain\n  - family-names: Drame\n    given-names: Mariama\n  - family-names: Lhoest\n    given-names: Quentin\n  - family-names: Rush\n    given-names: \"Alexander M.\"\n  booktitle: \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\"\n  month: 10\n  start: 38\n  end: 45\n  title: \"Transformers: State-of-the-Art Natural Language Processing\"\n  year: 2020\n  publisher: \"Association for Computational Linguistics\"\n  url: \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\"\n  address: \"Online\"",
      "technique": "File Exploration"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{vafa2021rationales,\n  title={Rationales for Sequential Predictions},\n  author={Vafa, Keyon and Deng, Yuntian and Blei, David M and Rush, Alexander M},\n  booktitle={Empirical Methods in Natural Language Processing},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.911449585530318
      ],
      "excerpt": "fairseq-preprocess --source-lang de --target-lang en \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8852471095218447,
        0.8389959595361606
      ],
      "excerpt": "The results should look like: \n|       Method      | Source Mean | Target Mean | Source Frac. | Target Frac. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507374082549614
      ],
      "excerpt": "| Greedy            |   0.12  |   0.12  |     0.09     |   0.02   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.911449585530318
      ],
      "excerpt": "fairseq-preprocess --source-lang de --target-lang en \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8852471095218447
      ],
      "excerpt": "The results should look like: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372151656983196
      ],
      "excerpt": "| Gradient norms     |  10.2  |  0.82  |  0.30  |  0.16  |  0.63  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384,
        0.8690140377415217
      ],
      "excerpt": "| Last attention     |  10.8  |  0.84  |  0.27  |  0.15  |  0.59  | \n| All attentions     |  10.7  |  0.82  |  0.32  |  0.15  |0.66| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "| Grad x emb         |  33.1  |  6.0  |  0.99  |  0.10  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.9593299683604384
      ],
      "excerpt": "| Integrated grads   |  67.0  |  0.12  |  0.21  | \n| Attention rollout  |  72.6  |  0.10  |  0.18  | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/keyonvafa/sequential-rationales",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq)\nWe want to make contributing to this project as easy and transparent as\npossible.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nLicense\nBy contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq),\nyou agree that your contributions will be licensed under the LICENSE file in\nthe root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-28T21:54:19Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-29T06:50:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8224239944363274,
        0.9089968638323423,
        0.8151452611412692
      ],
      "excerpt": "Source code for the paper: Rationales for Sequential Predictions by Keyon Vafa, Yuntian Deng, David Blei, and Sasha Rush (EMNLP 2021). \nClick here to watch our EMNLP talk. \nannotated_lambada.json is an annotated dataset based on Lambada, containing 107 passages and their annotated rationales.  Each row has three keys:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9881823645870469,
        0.9471587376791257
      ],
      "excerpt": "- text contains the text of the full passage. \n- rationale  contains the human rationales for predicting the final word of the passage. rationale is a list: each entry is a tuple of indices. The first index in each tuple represents the start of an annotation. The second index in each tuple represents the end of the corresponding annotation. The length of the list for each example is the size of its rationale. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9716994022773928
      ],
      "excerpt": "To rationalize your own sequence model, check out the instructions in the Custom Model section. To reproduce the experiments in our paper, jump ahead to Reproduce Experiments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9850471327901299,
        0.9822321207307942
      ],
      "excerpt": "There are two steps: fine-tuning a model for compatibility, and then performing greedy rationalization. We currently support fine-tuning language models and conditional models in fairseq and fine-tuning GPT-2-based models in Hugging Face. Below, we'll walk through fine-tuning and rationalizing a language model using fairseq, but see IWSLT for a conditional model example in fairseq or GPT-2 for fine-tuning GPT-2 in Hugging Face. \nFirst, you'll need to fine-tune your model for compatibility. Unless your model is trained with word-dropout, it is unable to form sensible predictions for incomplete inputs. For example, a pretrained language model may be able to fill in a blank when the sequence has no missing words, like I ate some ice cream because I was ____________, but it's not able to fill in the blank when other words in the sequence are missing, like I XXX some XXX cream because XXX was ____________. Since rationalization requires evaluating incomplete sequences, it's necessary to fine-tune for compatibility by using word dropout. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9701815082956602,
        0.9434346898358695
      ],
      "excerpt": "The command above uses word dropout with probability 0.5. Each time word dropout is being performed, the number of words dropped out is uniformly sampled from 1 to the sequence length. The corresponding number of tokens are dropped out uniformly at random. For machine translation, we recommend setting --word-dropout-type inverse_length. \nThe max-tokens option depends on the size of your model and the capacity of your GPU. We recommend setting it to the maximum number that doesn't result in memory errors.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": ": model checkpoint. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerModel.from_pretrained( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779835666780124
      ],
      "excerpt": "model.model = model.models[0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254749566248652
      ],
      "excerpt": "input_string = \"The Supreme Court on Tuesday\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9645725747583828
      ],
      "excerpt": ": <eos> token to the beginning of generated_sequence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9841723891653026
      ],
      "excerpt": "The rest of this README provides instructions for reproducing all of the experiments from our paper. All of the commands below were run on a single GPU. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9272931382108558
      ],
      "excerpt": "When we're done pretraining the standard model, we can fine-tune for compatibility using word dropout. We first setup the checkpoint for the compatible model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8534116021872802
      ],
      "excerpt": "The experiment with distractor sentences is described in the first paragraph of Section 8.2 in our paper. The experiment involves generating translations from the test set and concatenating random examples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8009512079859675
      ],
      "excerpt": "| All attentions    |     0.58    |     0.80    |     0.08     |     0.12     | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9880425276495857
      ],
      "excerpt": "In the paper, we performed experiments for fine-tuning GPT-2 Large (using sequence lengths of 1024). Since practitioners may not have a GPU that has the memory capacity to train the large model, our replication instructions are for GPT-2 Medium, fine-tuning with a sequence length of 512. This can be done on a single 12GB GPU, and the rationalization performance is similar for both models. If you would like to specifically replicate our results for GPT-2 Large, email me at keyvafa@gmail.com and I can provide you with the fine-tuning instructions/the full fine-tuned model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8020850401435046
      ],
      "excerpt": "This will reproduce Figure 4 of the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method greedy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method gradient_norm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method signed_gradient \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method integrated_gradient \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method attention_rollout \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method all_attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method last_attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method exhaustive \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8009512079859675
      ],
      "excerpt": "| All attentions     |  11.2  |  2.3  |  0.99  |  0.32  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9784533868521781,
        0.9458493872502137
      ],
      "excerpt": "Since these results are for GPT-2 Medium rather than GPT-2 Large, the results in Table 1 of the paper are a little different. \nFor the final experiment, we collected an annotated version of the Lambada dataset. See more details about using the dataset above. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method greedy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method gradient_norm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method signed_gradient \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method integrated_gradient \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method attention_rollout \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method last_attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "    --method all_attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Rationales for Sequential Predictions",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```{bash}\ncd examples/translation/\nbash prepare-iwslt14.sh\ncd ../..\n\n#: Preprocess/binarize the data\nTEXT=examples/translation/iwslt14.tokenized.de-en\nfairseq-preprocess --source-lang de --target-lang en \\\n    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n    --destdir data-bin/iwslt14.tokenized.de-en \\\n    --workers 20\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The other translation experiment involves word alignments. It is described in more detail in Section 8.2 of the paper.\n\nFirst, agree to the license and download the gold alignments from [RWTH Aachen](https://www-i6.informatik.rwth-aachen.de/goldAlignment/). Put the files `en`, `de`, and `alignmentDeEn` in the directory `fairseq/examples/translation/iwslt14.tokenized.de-en/gold_labels`, and, in that same repo, convert to Unicode using\n```{bash}\niconv -f ISO_8859-1 -t UTF8 de > gold.de\niconv -f ISO_8859-1 -t UTF8 en > gold.en\n```\nClean and tokenize the text\n```{bash}\ncd ../..\ncat iwslt14.tokenized.de-en/gold_labels/gold.en | \\\n  perl mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -l en | \\\n  perl mosesdecoder/scripts/tokenizer/lowercase.perl > iwslt14.tokenized.de-en/gold_labels/tmp_gold.en\ncat iwslt14.tokenized.de-en/gold_labels/gold.de | \\\n  perl mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -l de | \\\n  perl mosesdecoder/scripts/tokenizer/lowercase.perl > iwslt14.tokenized.de-en/gold_labels/tmp_gold.de\ncd ../..\n```\n\nApply BPE\n```{bash}\npython subword-nmt/subword_nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code < iwslt14.tokenized.de-en/gold_labels/tmp_gold.en > iwslt14.tokenized.de-en/gold_labels/gold_bpe.en \npython subword-nmt/subword_nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code < iwslt14.tokenized.de-en/gold_labels/tmp_gold.de > iwslt14.tokenized.de-en/gold_labels/gold_bpe.de\n```\n\nSince the original file automatically tokenizes the apostrophes (e.g. `don ' t`) after BPE, sometimes there are incorrect spaces in the tokenization (e.g. `don &apos; t` instead of `don &apos;t`). Since this would change the alignments for these files, you may need to go through the files `gold_bpe.en` and `gold_bpe.de` and change them manually. Keep the spaces for apostrophes but not for plurals, e.g. `man &apos;s office` and `&apos; legal drugs &apos;` are both correct. Delete the new lines at the bottom of `gold.en`, `gold.de`, `gold_bpe.en`, and `gold_bpe.de`. The only other necessary change is changing `\u00e0@@ -@@` to `a-@@` on line 247 since we don't tokenize accents.\n\nIf you've agreed to the license and would like to skip these steps, email me at [keyvafa@gmail.com](mailto:keyvafa@gmail.com) and I can provide you the preprocessed files.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "First go to the [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), a WebText replication corpus provided by Aaron Gokaslan and Vanya Cohen. We only use a single split to train (we used `urlsf_subset09.tar`). Expand all the items and merge the first 998, taking only the first 8 million lines. This will be the training set. We used half of the remaining files as the validation set, and the other half as the test set. Store the files as `webtext_train.txt`, `webtext_valid.txt`, and `webtext_test.txt` in `huggingface/data`.\n\nAlternatively, you can email me at [keyvafa@gmail.com](mailto:keyvafa@gmail.com) and I can send you the raw files (they're a little too large to store on Github).\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/keyonvafa/sequential-rationales/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 23:42:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/keyonvafa/sequential-rationales/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "keyonvafa/sequential-rationales",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/docker/transformers-pytorch-cpu/Dockerfile",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/docker/transformers-pytorch-tpu/Dockerfile",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/docker/transformers-tensorflow-cpu/Dockerfile",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/docker/transformers-tensorflow-gpu/Dockerfile",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/docker/transformers-cpu/Dockerfile",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/docker/transformers-pytorch-gpu/Dockerfile",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/docker/transformers-gpu/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/keyonvafa/sequential-rationales/tree/main/huggingface/docs",
      "https://github.com/keyonvafa/sequential-rationales/tree/main/fairseq/docs",
      "https://github.com/keyonvafa/sequential-rationales/tree/main/fairseq/examples/simultaneous_translation/docs",
      "https://github.com/keyonvafa/sequential-rationales/tree/main/fairseq/examples/speech_text_joint_to_text/docs",
      "https://github.com/keyonvafa/sequential-rationales/tree/main/fairseq/examples/speech_to_text/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/movement-pruning/Saving_PruneBERT.ipynb",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/lxmert/demo.ipynb",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/visual_bert/demo.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/scripts/fsmt/convert-allenai-wmt19.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/scripts/fsmt/eval-allenai-wmt16.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/scripts/fsmt/eval-facebook-wmt19.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/scripts/fsmt/s3-move.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/scripts/fsmt/convert-allenai-wmt16.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/scripts/fsmt/tests-to-run.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/scripts/fsmt/eval-allenai-wmt19.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/scripts/fsmt/convert-facebook-wmt19.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/scripts/tatoeba/upload_models.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/.github/conda/build.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/docker/transformers-pytorch-tpu/docker-entrypoint.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/pytorch/multiple-choice/run_no_trainer.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/pytorch/token-classification/run_no_trainer.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/pytorch/token-classification/run.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/legacy/pytorch-lightning/run_pos.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/legacy/pytorch-lightning/run_glue.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/legacy/pytorch-lightning/run_ner.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/legacy/seq2seq/finetune_tpu.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/legacy/seq2seq/train_distil_marian_enro_tpu.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/legacy/seq2seq/train_mbart_cc25_enro.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/legacy/seq2seq/train_distilbart_cnn.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/legacy/seq2seq/finetune.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/legacy/seq2seq/train_distil_marian_enro.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/legacy/token-classification/run_pos.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/legacy/token-classification/run_chunk.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/legacy/token-classification/run.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/performer/full_script.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/performer/sanity_script.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/deebert/eval_deebert.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/deebert/train_deebert.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/deebert/entropy_eval.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/seq2seq-distillation/distil_marian_no_teacher.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/seq2seq-distillation/finetune_pegasus_xsum.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/seq2seq-distillation/dynamic_bs_example.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/seq2seq-distillation/finetune_bart_tiny.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/seq2seq-distillation/train_mbart_cc25_enro.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/seq2seq-distillation/train_distilbart_cnn.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/seq2seq-distillation/distil_marian_enro_teacher.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/seq2seq-distillation/finetune.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/seq2seq-distillation/finetune_t5.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/seq2seq-distillation/train_distilbart_xsum.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/rag-end2end-retriever/finetune_rag_ray_end2end.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/rag-end2end-retriever/test_run/test_finetune.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/rag-end2end-retriever/test_run/test_rag_new_features.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/rag/finetune_rag.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/rag/finetune_rag_ray.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/wav2vec2/finetune_base_timit_asr.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/wav2vec2/finetune_wav2vec2_xlsr_turkish.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/wav2vec2/finetune_large_lv60_timit_asr.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/wav2vec2/finetune_base_100.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/wav2vec2/finetune_large_xlsr_53_arabic_speech_corpus.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/examples/research_projects/wav2vec2/finetune_large_lv60_100.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/huggingface/.circleci/deploy.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/scripts/compound_split_bleu.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/scripts/sacrebleu.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/scripts/test_fsdp.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/language_model/prepare-wikitext-103.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/multilingual/finetune_multilingual_model.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/multilingual/multilingual_fairseq_gen.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/multilingual/train_multilingual_model.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/multilingual/data_scripts/download_af_xh.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/multilingual/data_scripts/download_wmt20.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/multilingual/data_scripts/download_wat19_my.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/multilingual/data_scripts/download_ML50_v1.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/multilingual/data_scripts/preprocess_ML50_v1.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/multilingual/data_scripts/download_flores_data.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/multilingual/data_scripts/download_lotus.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/multilingual/data_scripts/download_iitb.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/multilingual/data_scripts/download_iwslt_and_extract.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/multilingual/data_scripts/utils/strip_sgm.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/speech_recognition/datasets/prepare-librispeech.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/m2m_100/install_dependecies.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/m2m_100/tok.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/m2m_100/tokenizers/seg_ja.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/m2m_100/tokenizers/seg_ko.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/m2m_100/tokenizers/tokenizer_ar.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/criss/download_and_preprocess_tatoeba.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/criss/download_and_preprocess_flores_test.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/criss/sentence_retrieval/sentence_retrieval_tatoeba.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/criss/unsupervised_mt/eval.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/criss/mining/mine_example.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/byte_level_bpe/get_data.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/roberta/preprocess_GLUE_tasks.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/roberta/preprocess_RACE.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/roberta/commonsense_qa/download_cqa_data.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/backtranslation/sacrebleu.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/backtranslation/tokenized_bleu.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/backtranslation/prepare-wmt18en2de.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/backtranslation/prepare-de-monolingual.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/scripts/binarize_manifest.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/scripts/prepare_timit.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/scripts/prepare_text.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/scripts/prepare_audio.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_word_step1.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/train.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_phone.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_word_step2.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/path.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/cmd.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/score.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/decode.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/show_wer.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/train_subset_lgbeam.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lm.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lang.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/unsup_select_decode_word.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/unsup_select_decode.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lang_word.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_sat.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_deltas.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_lda_mllt.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/translation/prepare-wmt14en2fr.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/translation/prepare-iwslt14.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/translation/prepare-iwslt17-multilingual.sh",
      "https://raw.githubusercontent.com/keyonvafa/sequential-rationales/main/fairseq/examples/translation/prepare-wmt14en2de.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Configure a virtual environment using Python 3.6+ ([instructions here](https://docs.python.org/3.6/tutorial/venv.html)).\nInside the virtual environment, use `pip` to install the required packages:\n\n```{bash}\npip install -r requirements.txt\n```\n\nConfigure [Hugging Face](https://github.com/huggingface/transformers) to be developed locally\n```{bash}\ncd huggingface\npip install --editable ./\ncd ..\n```\n\nDo the same with  [fairseq](https://github.com/pytorch/fairseq)\n```{bash}\ncd fairseq\npip install --editable ./\ncd ..\n```\n\nOptionally, install NVIDIA's [apex](https://github.com/NVIDIA/apex) library to enable faster training\n```{bash}\ncd fairseq\ngit clone https://github.com/NVIDIA/apex\npip install -v --no-cache-dir \\\n  --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" \\\n  --global-option=\"--deprecated_fused_adam\" --global-option=\"--xentropy\" \\\n  --global-option=\"--fast_multihead_attn\" ./\ncd ../..\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9484665158606834
      ],
      "excerpt": "First, make sure all the required packages are installed: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd fairseq \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd fairseq \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057675898656085
      ],
      "excerpt": "    --path $CHECKPOINT_DIR/standard_majority_class/checkpoint_best.pt \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057675898656085
      ],
      "excerpt": "    --path $CHECKPOINT_DIR/compatible_majority_class/checkpoint_best.pt \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108683623975996
      ],
      "excerpt": "cd ../analysis \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd ../fairseq \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902627162932362
      ],
      "excerpt": "mkdir $CHECKPOINT_DIR/compatible_iwslt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057675898656085
      ],
      "excerpt": "    --path $CHECKPOINT_DIR/standard_iwslt/checkpoint_best.pt \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057675898656085
      ],
      "excerpt": "    --path $CHECKPOINT_DIR/compatible_iwslt/checkpoint_best.pt \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902627162932362
      ],
      "excerpt": "mkdir generated_translations \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057675898656085
      ],
      "excerpt": "  --path $CHECKPOINT_DIR/compatible_iwslt/checkpoint_best.pt \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108683623975996
      ],
      "excerpt": "cd ../analysis \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd ../fairseq \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108683623975996
      ],
      "excerpt": "cd ../analysis \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd ../fairseq \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108683623975996
      ],
      "excerpt": "cd ../analysis \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd ../fairseq \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108683623975996
      ],
      "excerpt": "cd ../analysis \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd ../fairseq \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd huggingface \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108683623975996
      ],
      "excerpt": "cd ../analysis \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd ../huggingface \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108683623975996
      ],
      "excerpt": "cd ../analysis \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd ../huggingface \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108683623975996
      ],
      "excerpt": "cd ../analysis \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd ../huggingface \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8651734158513442,
        0.942251427147612,
        0.8359299706379749
      ],
      "excerpt": "df = pd.read_json('annotated_lambada.json', orient='records', lines=True) \n: Print the rationale of the first example \ntext = df['text'].iloc[0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9169499245496534
      ],
      "excerpt": "print([text[sub_rationale[0]:sub_rationale[1]] for sub_rationale in rationale]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8035177299311627
      ],
      "excerpt": "fairseq-train --task language_modeling \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "    --weight-decay 0.01 --clip-norm 0.0 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8552248255445785
      ],
      "excerpt": "    --tokens-per-sample 512 --sample-break-mode eos \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401558704798054,
        0.8900486270063179,
        0.8801854956928516
      ],
      "excerpt": "import os \nfrom fairseq.models.transformer import TransformerModel \nfrom rationalization import rationalize_lm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8048195900317339
      ],
      "excerpt": ": model checkpoint. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8023622814229289
      ],
      "excerpt": "generated_sequence = model.generate(input_ids)[0]['tokens'] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8708467368187395
      ],
      "excerpt": "rationales, log = rationalize_lm(model, generated_sequence, verbose=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9061125722915375
      ],
      "excerpt": "    --trainpref $TEXT/train.tok \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9077522279961474
      ],
      "excerpt": "    --testpref $TEXT/test.tok \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8035177299311627
      ],
      "excerpt": "fairseq-train --task language_modeling \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "    --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8552248255445785
      ],
      "excerpt": "    --tokens-per-sample 512 --sample-break-mode eos \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8071162279884108
      ],
      "excerpt": "fairseq-eval-lm data-bin/majority_class \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8516101378212834
      ],
      "excerpt": "    --tokens-per-sample 20 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8035177299311627
      ],
      "excerpt": "fairseq-train --task language_modeling \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "    --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8552248255445785
      ],
      "excerpt": "    --tokens-per-sample 512 --sample-break-mode eos \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8071162279884108
      ],
      "excerpt": "fairseq-eval-lm data-bin/majority_class \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8516101378212834
      ],
      "excerpt": "    --tokens-per-sample 20 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python plot_majority_class_compatibility.py --checkpoint_dir $CHECKPOINT_DIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "fairseq-train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9076460749310138
      ],
      "excerpt": "    --eval-bleu-print-samples \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "fairseq-train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9076460749310138
      ],
      "excerpt": "    --eval-bleu-print-samples \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.823687576256224
      ],
      "excerpt": "  --batch-size 128 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python create_distractor_iwslt_dataset.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9061125722915375
      ],
      "excerpt": "    --trainpref $TEXT/train  \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617
      ],
      "excerpt": "python evaluate_distractor_rationales.py --baseline gradient_norm \npython evaluate_distractor_rationales.py --baseline signed_gradient \npython evaluate_distractor_rationales.py --baseline integrated_gradient \npython evaluate_distractor_rationales.py --baseline last_attention \npython evaluate_distractor_rationales.py --baseline all_attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9061125722915375
      ],
      "excerpt": "    --trainpref $TEXT/train  \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359299706379749
      ],
      "excerpt": "    --testpref $TEXT/gold_labels/gold_bpe \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python map_alignments_to_bpe.py --checkpoint_dir $CHECKPOINT_DIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_iwslt.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617
      ],
      "excerpt": "python evaluate_alignment_rationales.py --baseline gradient_norm  \npython evaluate_alignment_rationales.py --baseline gradient_norm  --top_1 \npython evaluate_alignment_rationales.py --baseline signed_gradient  \npython evaluate_alignment_rationales.py --baseline signed_gradient  --top_1 \npython evaluate_alignment_rationales.py --baseline integrated_gradient  \npython evaluate_alignment_rationales.py --baseline integrated_gradient  --top_1 \npython evaluate_alignment_rationales.py --baseline last_attention  \npython evaluate_alignment_rationales.py --baseline last_attention  --top_1 \npython evaluate_alignment_rationales.py --baseline all_attention  \npython evaluate_alignment_rationales.py --baseline all_attention  --top_1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python plot_gpt2_the_repeats.py  --checkpoint_dir $CHECKPOINT_DIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python plot_gpt2_rationalization.py  --checkpoint_dir $CHECKPOINT_DIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_analogies.py  --checkpoint_dir $CHECKPOINT_DIR  \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_analogies.py  --checkpoint_dir $CHECKPOINT_DIR  \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_analogies.py  --checkpoint_dir $CHECKPOINT_DIR  \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_analogies.py  --checkpoint_dir $CHECKPOINT_DIR  \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_analogies.py  --checkpoint_dir $CHECKPOINT_DIR  \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_analogies.py  --checkpoint_dir $CHECKPOINT_DIR  \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_analogies.py  --checkpoint_dir $CHECKPOINT_DIR  \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_analogies.py  --checkpoint_dir $CHECKPOINT_DIR  \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617
      ],
      "excerpt": "python evaluate_analogies_rationales.py --baseline gradient_norm \npython evaluate_analogies_rationales.py --baseline signed_gradient \npython evaluate_analogies_rationales.py --baseline integrated_gradient \npython evaluate_analogies_rationales.py --baseline attention_rollout \npython evaluate_analogies_rationales.py --baseline last_attention \npython evaluate_analogies_rationales.py --baseline all_attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_annotated_lambada.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_annotated_lambada.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_annotated_lambada.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_annotated_lambada.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_annotated_lambada.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_annotated_lambada.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python rationalize_annotated_lambada.py --checkpoint_dir $CHECKPOINT_DIR \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617,
        0.8131799934034617
      ],
      "excerpt": "python evaluate_lambada_rationales.py --baseline gradient_norm \npython evaluate_lambada_rationales.py --baseline signed_gradient \npython evaluate_lambada_rationales.py --baseline integrated_gradient \npython evaluate_lambada_rationales.py --baseline attention_rollout \npython evaluate_lambada_rationales.py --baseline last_attention \npython evaluate_lambada_rationales.py --baseline all_attention \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/keyonvafa/sequential-rationales/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Jupyter Notebook",
      "Cuda",
      "C++",
      "JavaScript",
      "Cython",
      "CSS",
      "Dockerfile",
      "Makefile",
      "Lua",
      "Jsonnet",
      "Batchfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) Facebook, Inc. and its affiliates.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rationales for Sequential Predictions",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sequential-rationales",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "keyonvafa",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/keyonvafa/sequential-rationales/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Configure a virtual environment using Python 3.6+ ([instructions here](https://docs.python.org/3.6/tutorial/venv.html)).\nInside the virtual environment, use `pip` to install the required packages:\n\n```{bash}\npip install -r requirements.txt\n```\n\nConfigure [Hugging Face](https://github.com/huggingface/transformers) to be developed locally\n```{bash}\ncd huggingface\npip install --editable ./\ncd ..\n```\n\nDo the same with  [fairseq](https://github.com/pytorch/fairseq)\n```{bash}\ncd fairseq\npip install --editable ./\ncd ..\n```\n\nOptionally, install NVIDIA's [apex](https://github.com/NVIDIA/apex) library to enable faster training\n```{bash}\ncd fairseq\ngit clone https://github.com/NVIDIA/apex\npip install -v --no-cache-dir \\\n  --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" \\\n  --global-option=\"--deprecated_fused_adam\" --global-option=\"--xentropy\" \\\n  --global-option=\"--fast_multihead_attn\" ./\ncd ../..\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 22,
      "date": "Sun, 26 Dec 2021 23:42:26 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Check out our [Colab notebook](https://colab.research.google.com/drive/1l33I0BDOXtPMdQVqB8Y24DJUp7K52qDz#scrollTo=KdN0dxky7nMw), which generates a sequence with GPT-2 and performs greedy rationalization. Our compatible version of GPT-2 is [available on Hugging Face](https://huggingface.co/keyonvafa/compatible-gpt2).\n\n<p align=\"center\">\n<img src=\"https://github.com/keyonvafa/sequential-rationales/blob/main/analysis/figs/sequential_rationale_small.gif\" --width=\"300\" height=\"300\" />\n</p>\n\nThe following code loads our compatible GPT-2 and rationalizes a sampled sequence:\n\n```python\nfrom huggingface.rationalization import rationalize_lm\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n\n#: Load model from Hugging Face\nmodel = AutoModelWithLMHead.from_pretrained(\"keyonvafa/compatible-gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"keyonvafa/compatible-gpt2\")\nmodel.cuda()\nmodel.eval()\n\n#: Generate sequence\ninput_string = \"The Supreme Court on Tuesday\"\ninput_ids = tokenizer(input_string, return_tensors='pt')['input_ids'].to(model.device)\ngenerated_input = model.generate(input_ids=input_ids, max_length=16, do_sample=False)[0]\n  \n#: Rationalize sequence with greedy rationalization\nrationales, rationalization_log = rationalize_lm(model, generated_input, tokenizer, verbose=True)\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This will reproduce Figure 6 from the paper.\n```{bash}\npython plot_iwslt_rationalization.py\ncd ..\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "For the compatible model:\n```{bash}\npython examples/pytorch/language-modeling/run_clm.py \\\n    --model_name_or_path $CHECKPOINT_DIR/compatible_gpt2 \\\n    --output_dir gpt2_test_output/compatible/ \\\n    --do_eval \\\n    --validation_file data/webtext_test.txt \\\n    --block_size 512 \\\n    --per_device_eval_batch_size 4\n```\nThis should give a heldout perplexity of 17.6086.\n\nFor the pretrained model:\n```{bash}\npython examples/pytorch/language-modeling/run_clm.py \\\n    --model_name_or_path gpt2-medium \\\n    --output_dir gpt2_test_output/pretrained/ \\\n    --do_eval \\\n    --validation_file data/webtext_test.txt \\\n    --block_size 512 \\\n    --per_device_eval_batch_size 4\n```\nThis should give a heldout perplexity of 19.9674.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```{bash}\npython compare_rationalization_times.py  --checkpoint_dir $CHECKPOINT_DIR\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}