{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1609.04468",
      "https://arxiv.org/abs/1609.05158\n\n## 6: Use Soft and Noisy Labels\n\n- Label Smoothing, i.e. if you have two target labels: Real=1 and Fake=0, then for each incoming sample, if it is real, then replace the label with a random number between 0.7 and 1.2, and if it is a fake sample, replace it with 0.0 and 0.3 (for example"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9854121748519129
      ],
      "excerpt": "  - Goodfellow et. al (2014) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9944484218006108
      ],
      "excerpt": "PixelShuffle: https://arxiv.org/abs/1609.05158 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jaingaurav3/GAN-Hacks",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-27T03:13:02Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-17T12:59:12Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9618661740576044
      ],
      "excerpt": "fundamental stability of these models, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9863915163441662,
        0.9799125863937105
      ],
      "excerpt": "Here are a summary of some of the tricks. \nHere's a link to the authors of this document \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8761164022235732
      ],
      "excerpt": "Construct different mini-batches for real and fake, i.e. each mini-batch needs to contain only all real images or all generated images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066911099384199
      ],
      "excerpt": "when things are working, D loss has low variance and goes down over time vs having huge variance and spiking \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9029400070642164
      ],
      "excerpt": "It's hard and we've all tried it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.834695533457999
      ],
      "excerpt": "adding gaussian noise to every layer of generator (Zhao et. al. EBGAN) \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jaingaurav3/GAN-Hacks/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 16:01:09 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jaingaurav3/GAN-Hacks/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jaingaurav3/GAN-Hacks",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8216270093103228
      ],
      "excerpt": "For example \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "  train D \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "  train G \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jaingaurav3/GAN-Hacks/issues{/number}",
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "How to Train a GAN? Tips and tricks to make GANs work",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "GAN-Hacks",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jaingaurav3",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jaingaurav3/GAN-Hacks/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Tue, 28 Dec 2021 16:01:09 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Dont sample from a Uniform distribution\n\n![cube](images/cube.png \"Cube\")\n\n- Sample from a gaussian distribution\n\n![sphere](images/sphere.png \"Sphere\")\n\n- When doing interpolations, do the interpolation via a great circle, rather than a straight line from point A to point B\n- Tom White's [Sampling Generative Networks](https://arxiv.org/abs/1609.04468) ref code https://github.com/dribnet/plat has more details\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Label Smoothing, i.e. if you have two target labels: Real=1 and Fake=0, then for each incoming sample, if it is real, then replace the label with a random number between 0.7 and 1.2, and if it is a fake sample, replace it with 0.0 and 0.3 (for example).\n  - Salimans et. al. 2016\n- make the labels the noisy for the discriminator: occasionally flip the labels when training the discriminator\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Experience Replay\n  - Keep a replay buffer of past generations and occassionally show them\n  - Keep checkpoints from the past of G and D and occassionaly swap them out for a few iterations\n- All stability tricks that work for deep deterministic policy gradients\n- See Pfau & Vinyals (2016)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- optim.Adam rules!\n  - See Radford et. al. 2015\n- Use SGD for discriminator and ADAM for generator\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- if you have labels available, training the discriminator to also classify the samples: auxillary GANs\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Provide noise in the form of dropout (50%).\n- Apply on several layers of our generator at both training and test time\n- https://arxiv.org/pdf/1611.07004v1.pdf\n\n\n",
      "technique": "Header extraction"
    }
  ]
}