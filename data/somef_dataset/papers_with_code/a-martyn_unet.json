{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1505.04597",
      "https://arxiv.org/abs/1505.04597",
      "https://arxiv.org/abs/1611.07004",
      "https://arxiv.org/abs/1503.04069"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- animation from [ISBI Challenge](http://brainiac2.mit.edu/isbi_challenge/)\n- title and aproach inspired by [LSTM: A Search Space Odyssey](https://arxiv.org/abs/1503.04069)",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Olaf Ronneberger, Philipp Fischer, Thomas Brox. [*U-Net: Convolutional Networks for Biomedical Image Segmentation*](https://arxiv.org/abs/1505.04597). MICCAI, 2015.\n2. Vladimir Iglovikov, Alexey Shvets. [*TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation*](https://arxiv.org/abs/1505.04597). arXiv, 2018.\n3. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros. [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004) CVPR, 2017\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8189540139458567
      ],
      "excerpt": "Tabulated results from plots above. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "source venv/bin/activate \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/a-martyn/unet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-20T19:33:56Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-21T15:12:23Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8375294287458631,
        0.919509532701739
      ],
      "excerpt": "This repo implements and compares three U-Net architectures trained on the ISBI Challenge dataset. The following architectures are included: \nU-Net: The original implementation as described in [1] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8736923547355259
      ],
      "excerpt": "Before comparing architectures we set a baseline by reproducing the original U-Net experiment (Table 1. in [1]). The U-Net architecture is trained as described in [1] with a few simplifications: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8619765347762739,
        0.8863069382349216,
        0.9585518060505661,
        0.9612043743968917,
        0.9428520987790096,
        0.8974222973310264
      ],
      "excerpt": "Weight map that incurs greater loss for pixels between cell borders is omitted \nData augmentation: use shear to warp images instead of \"random elastic deformations\" described in paper \nThe model achieves a validation set pixel accuracy of 0.9376 which equates to a pixel error of 0.0624. This is similar to the original paper's result of 0.0611 pixel error. \nNow that we know our baseline U-Net experiment is roughly equivalent to the original paper, we can vary model architectures within the same experimental method to compare performance. \nComparison of validation-set pixel accuracy for each model after 20 epochs each of 250 training steps. Each experiment is repeated 20 times (iterations) to allow estimate of mean and variance of model performance. \nSame as above but without pix2pix results to allow fine-grained comparison on a reduced scale. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8626753917300718,
        0.9462848760128624
      ],
      "excerpt": "- unet: as described in original paper [1] \n- unet_upsampled: as above but with bilinear upsampling instead of transposed convolutions in decoder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.816248652787121,
        0.8147670169335928,
        0.9977245508412412,
        0.9425893881237865,
        0.947980382925525,
        0.9681928229330284,
        0.9353553954941225,
        0.8960296087630891,
        0.9638023905454952,
        0.9465232438493663
      ],
      "excerpt": "- ternaus_drop: As ternaus, with dropout added after final two layers of the encoder, as per unet  \n- ternaus_bn: As ternaus, with batch normalisation applied to decoder layers \n- ternaus_dropbn: combination of ternaus_drop and ternaus_bn \n- pix2pix_generator: generator for the pix2pix GAN as described in [3] \nThe TernausNet with added dropout achieves the highest mean pixel accuracy of 0.9322. All variations on the TernausNet architecture perform similarly, except for when pre-trained weights are excluded.  \nThe TernausNet without pre-training exhibits lower mean accuracy of 0.927 and higher variance in results, its performance is similar to the U-Net, despite architectural differences. \nUse of bilinear-upsampling, in place of transposed-convolutions, in the U-Net decoder degrades the mean accuracy. \nThe pix2pix generator architecture is the worst performing architecture tested with roughly 10% lower accuracy than other models. This model also has significantly more parameters. \nThe results suggest that pre-training is the most significant factor explaining performance differences between the U-Net and TernausNet architectures. The lower performance of the larger pix2pix model might be due to overfitting. \nWith respect to our initial questions the results from these experiments suggest the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8269133661807342
      ],
      "excerpt": "The dataset is included in this repo. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Comparison of three U-Net architectures on the ISBI Challenge dataset. Keras/Tensorflow",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/a-martyn/unet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Sat, 25 Dec 2021 17:09:37 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/a-martyn/unet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "a-martyn/unet",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/a-martyn/unet/master/model_comparison.ipynb",
      "https://raw.githubusercontent.com/a-martyn/unet/master/data_visualisation.ipynb",
      "https://raw.githubusercontent.com/a-martyn/unet/master/analysis.ipynb",
      "https://raw.githubusercontent.com/a-martyn/unet/master/model/inspect_pretrained_vgg.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9754213707472548,
        0.9911185628344078
      ],
      "excerpt": "CUDA 10 drivers required if running on GPU, installation steps here. If running on CPU, change tensorflow-gpu to tensorflow in requirements.txt. \nSetup python environment: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8211232956927162,
        0.9979947896609701
      ],
      "excerpt": "source venv/bin/activate \npip install -r requirements.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8820234121644812
      ],
      "excerpt": "jupyter notebook \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8599654469282022
      ],
      "excerpt": "All models are implemented in Keras/Tensorflow and are trained on the same dataset of neuronal structure images used in the original U-Net paper. Example input images (left) and target output images (right) are shown in the animation above. This dataset is tiny containing only 30 training samples, and 30 test samples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8095893137545214
      ],
      "excerpt": "Disambiguation of model names: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/a-martyn/unet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "U-Nets: A segment space odyssey",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "unet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "a-martyn",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/a-martyn/unet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 17,
      "date": "Sat, 25 Dec 2021 17:09:37 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "unet",
      "image-segmentation",
      "deep-learning",
      "keras",
      "keras-tensorflow",
      "tensorflow",
      "unet-image-segmentation",
      "unet-keras",
      "paper",
      "machine-learning"
    ],
    "technique": "GitHub API"
  }
}