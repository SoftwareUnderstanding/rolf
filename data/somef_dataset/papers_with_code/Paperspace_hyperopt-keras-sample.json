{
  "citation": [
    {
      "confidence": [
        0.8187756947909643,
        0.8187756947909643
      ],
      "excerpt": "'residual': hp.choice( \n    'residual', [None, hp.quniform( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9805378847049522,
        0.9795187663973746
      ],
      "excerpt": "    'all_conv',  #: All-convolutionnal: https://arxiv.org/pdf/1412.6806.pdf \n    'inception'  #: Inspired from: https://arxiv.org/pdf/1602.07261.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8089227735508986
      ],
      "excerpt": "#: The kernel_size for residual convolutions: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8187756947909643
      ],
      "excerpt": "'residual': 4, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    \"history\": {...}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8187756947909643
      ],
      "excerpt": "        \"residual\": 3.0, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8111036989382164
      ],
      "excerpt": "  --workspaceUrl git+https://github.com/Paperspace/hyperopt-keras-sample \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8111036989382164
      ],
      "excerpt": "  \"workspaceUrl\": \"git+https://github.com/Paperspace/hyperopt-keras-sample\", \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Paperspace/hyperopt-keras-sample",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-24T11:38:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T12:43:34Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9900310594856769
      ],
      "excerpt": "This project acts as both a tutorial and demo for using Paperspace Hyperparameter Tuning based on Hyperopt with Keras, TensorFlow, and TensorBoard. Not only do we try to find the best hyperparameters for the given hyperspace, but we also represent the neural network architecture as hyperparameters that can be tuned. This serves to automate the process of searching for the best neural architecture configuration and hyperparameters. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9654229309907278,
        0.9959855936983396,
        0.9975212084152825,
        0.9396794080982249
      ],
      "excerpt": "Hyperopt is a method for searching through a hyperparameter space. For example, it can use the Tree-structured Parzen Estimator (TPE) algorithm, which intelligently explores the search space while narrowing down to the best estimated parameters. \nIt is thus a good method for meta-optimizing a neural network. Whereas a neural network is an optimization problem that is tuned using gradient descent methods, hyperparameters cannot be tuned using gradient descent methods. That's where Hyperopt comes in and shines: it's useful not only for tuning hyperparameters like learning rate, but also for tuning more sophisticated parameters, and in a flexible way: it can change the number of layers of different types; the number of neurons in one layer or another; or even the type of layer to use at a certain place in the network given an array of choices, each themselves with nested, tunable hyperparameters. \nThis kind of Oriented Random Search is Hyperopt's strength, as opposed to a simpler Grid Search where hyperparameters are pre-established with fixed-step increases. Random Search for Hyperparameter Optimization has proven to be an effective search technique. The paper about this technique sits among the most cited deep learning papers. In summary, it is more efficient to randomly search through values and intelligently narrow the search space, rather than looping on fixed sets of hyperparameter values. \nA parameter is defined with either a certain uniform range or a probability distribution, such as: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9381035213817388
      ],
      "excerpt": "There are also a few quantized versions of those functions, which round the generated values at each step of \"q\": \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.853717509216044,
        0.8266571165504194
      ],
      "excerpt": "It is also possible to use a \"choice\" that can lead to hyperparameter nesting: \nhp.choice(label, [\"list\", \"of\", \"potential\", \"choices\"]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9132715703317671
      ],
      "excerpt": "Also, the results are pickled to results.pkl to be able to resume the TPE meta-optimization process later, which you can do simply by running the program again with python3 hyperopt_optimize.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9619320045220046,
        0.9962841206567983
      ],
      "excerpt": "It's possible that you may achieve better results than what's been achieved here in this repository. Pull requests / contributions are welcome! A couple of suggestions for finding interesting results: 1) try many different initializers for the layers; 2) add SELU activations. To restart the training with new or removed hyperparameters, it is recommended to delete existing results by running ./delete_results.sh. \nBelow is a basic overview of the model. We implemented it in such a way that Hyperopt will try to change the shape of the layers and remove or replace some of them based on some pre-parameterized ideas that we're trying here. In this approach, Hyperopt changes a lot of parameters in addition to the learning rate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254751311447194
      ],
      "excerpt": "space = { \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8145507995419725
      ],
      "excerpt": "    #: it vary exponentially, in a multiplicative fashion rather than in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "    #: Choice of optimizer: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9684915575131307
      ],
      "excerpt": "#: The kernel_size for convolutions: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184876049031471
      ],
      "excerpt": "#: The kernel_size for residual convolutions: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9005522722260962
      ],
      "excerpt": "Here is an excerpt: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9721315147531567,
        0.9569823018085111,
        0.972363844099263,
        0.952047706911873
      ],
      "excerpt": "This could help to redefine the hyperparameters and to narrow them down successively, relaunching the meta-optimization on refined spaces. \nThe final accuracy is 67.61% on average for the 100 fine labels, and is 77.31% on average for the 20 coarse labels. \nThese results are comparable to the ones in the middle of this list, under the CIFAR-100 section. \nThe only image preprocessing that we do is a random flip left-right. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254751311447194
      ],
      "excerpt": "    \"space\": { \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8776505623505683
      ],
      "excerpt": "Note: currently there not a CLI option to pass experimentEnv json to start hyperparameter with specific ENV values. However this functionality is available via the /hyperopt/create_and_start HTTP API call. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Hyperopt on Gradient",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Paperspace/hyperopt-keras-sample/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Mon, 27 Dec 2021 12:57:34 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Paperspace/hyperopt-keras-sample/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Paperspace/hyperopt-keras-sample",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Paperspace/hyperopt-keras-sample/master/local_infra/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Paperspace/hyperopt-keras-sample/master/hyperopt-mongo-worker.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8437033485426174
      ],
      "excerpt": "#: Use one more FC layer at output: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8169276304574682
      ],
      "excerpt": "You can run a hyperparameter tuning experiment on Paperspace using the Gradient CLI. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "  --name HyperoptKerasExperimentCLI1 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8474895321345809,
        0.9153271414091303
      ],
      "excerpt": "  --tuningCommand 'make run_hyperopt' \\ \n  --workerContainer tensorflow/tensorflow:1.13.1-gpu-py3 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8474895321345809
      ],
      "excerpt": "  --workerCommand 'make run_hyperopt_worker' \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8847949311650575
      ],
      "excerpt": "  --workspaceUrl git+https://github.com/Paperspace/hyperopt-keras-sample \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8125056659873306,
        0.8474895321345809,
        0.8474895321345809,
        0.9153271414091303
      ],
      "excerpt": "  \"name\": \"&lt;your-experiment-name&gt;\", \n  \"tuningCommand\": \"make run_hyperopt\", \n  \"workerCommand\": \"make run_hyperopt_worker\", \n  \"workerContainer\": \"tensorflow/tensorflow:1.13.1-gpu-py3\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8847949311650575
      ],
      "excerpt": "  \"workspaceUrl\": \"git+https://github.com/Paperspace/hyperopt-keras-sample\", \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8768801849289117
      ],
      "excerpt": "    'use_BN': hp.choice('use_BN', [False, True]), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    'use_BN': True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "  <img src=\"model_demo.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "  <img src=\"hyperparameters_scatter_matrix.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "        \"use_BN\": true \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "  <img src=\"model_best.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "  --name HyperoptKerasExperimentCLI1 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8977600242162364
      ],
      "excerpt": "Example json: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Paperspace/hyperopt-keras-sample/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Makefile",
      "Dockerfile",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Paperspace\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Paperspace Hyperparameter Tuning for solving CIFAR-100 with a convolutional neural network (CNN) built with Keras and TensorFlow, GPU backend",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "hyperopt-keras-sample",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Paperspace",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Paperspace/hyperopt-keras-sample/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[Download](https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz) dataset for model training and put it to `ml_req` folder with name `cifar-100-python.tar.gz`.\n\nThen from `local_infra` build and run with these commands:\n\n```bash\ndocker-compose build\ndocker-compose up --scale hks_hyperopt_worker=2\n```\n\nNote: you should run hyperopt with at least 2 worker nodes for better reliability. (You might experience issues with unsaved evaluation results when running with a single worker node.)\n\n**Important**\nRemember to set available RAM for docker on mac to at least 4GB (1 GB per docker with assumption to run 2 hyperopt workers).\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 12,
      "date": "Mon, 27 Dec 2021 12:57:34 GMT"
    },
    "technique": "GitHub API"
  }
}