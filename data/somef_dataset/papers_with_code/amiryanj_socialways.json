{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1904.09507",
      "https://arxiv.org/abs/1803.10892",
      "https://arxiv.org/abs/1606.03657"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you are using this code for your work, please cite:\r\n```\r\n@inproceedings{amirian2019social,\r\n  title={Social ways: Learning multi-modal distributions of pedestrian trajectories with GANs},\r\n  author={Amirian, Javad and Hayet, Jean-Bernard and Pettr{\\'e}, Julien},\r\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\r\n  pages={0--0},\r\n  year={2019}\r\n}\r\n```\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{amirian2019social,\n  title={Social ways: Learning multi-modal distributions of pedestrian trajectories with GANs},\n  author={Amirian, Javad and Hayet, Jean-Bernard and Pettr{\\'e}, Julien},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n  pages={0--0},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9949137349832196
      ],
      "excerpt": "Presented at CVPR 2019 in Precognition Workshop ( \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/crowdbotp/socialways",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-23T10:19:08Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-14T01:09:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9506656117581989,
        0.8102336794536806
      ],
      "excerpt": "The pytorch implementation for the paper \n<a href=\"http://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Amirian_Social_Ways_Learning_Multi-Modal_Distributions_of_Pedestrian_Trajectories_With_GANs_CVPRW_2019_paper.pdf\">Social Ways: Learning Multi-Modal Distributions of Pedestrian Trajectories with GANs</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9836766332214147
      ],
      "excerpt": "This work is, theoretically, an improvement of Social-GAN by applying the following changes: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9290420866633023
      ],
      "excerpt": "2. Introducing to use new social features between pair of agents: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8878047004649634,
        0.9231030498990952,
        0.9966474696551649,
        0.9966474696551649
      ],
      "excerpt": "The system is composed of two main components: Trajectory Generator and Trajectory Discriminator. \nFor generating a prediction sample for Pedestrian of Interest (POI), the generator needs the following inputs: \n- the observed trajectory of POI, \n- the observed trajectory of surrounding agents, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693580423826478,
        0.850719131157432
      ],
      "excerpt": "- and the latent codes (c) \nThe Discriminator takes a pair of observation and prediction samples and decides, if the given prediction sample is real or fake. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Social Ways: Learning Multi-Modal Distributions of Pedestrian Trajectories with GANs (CVPR 2019)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amiryanj/socialways/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 40,
      "date": "Fri, 24 Dec 2021 20:46:15 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/crowdbotp/socialways/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "crowdbotp/socialways",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run this code you better to use python >= 3.5.\r\nYou can use pip to install the required packages.\r\n```bash\r\n$ pip install torch torchvision numpy matplotlib tqdm nose\r\n$ pip install seaborn opencv-python   #: to run visualize.py\r\n```\r\n\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8558022358339008
      ],
      "excerpt": "  <img src='figs/block-diagram.png' width='800px'\\> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8366559674709381,
        0.950563948951535
      ],
      "excerpt": "To train the model, please edit the train.py to select the dataset you want to train the model on. The next few lines define some of the most critical parameters values. Then execute: \n$ python3 train.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/crowdbotp/socialways/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Social Ways",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "socialways",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "crowdbotp",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/crowdbotp/socialways/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 95,
      "date": "Fri, 24 Dec 2021 20:46:15 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "trajectory-prediction",
      "human-trajectory-prediction",
      "gan",
      "social-navigation",
      "social-ways",
      "pedestrian-trajectories",
      "pedestrian",
      "social-gan",
      "social-robots",
      "generative-adversarial-network",
      "trajectory-forecasting",
      "self-driving-car",
      "prediction-model",
      "info-gan",
      "crowd-simulation"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We designed the trajectory toy dataset, to assess the capability of generator in preserving modes of trajectory distribution.\r\nThere are six groups of trajectories, all starting from one specific point located along a circle (blue dots). When approaching the circle center, they split into 3 subgroups. Their endpoints are the green dots.\r\n<p align='center'>\r\n  <img src='figs/toy.gif' width='600px'\\>\r\n</p>\r\n\r\nIn order to create the toy example trajectories, you need to run\r\n\r\n```\r\n$ python3 create_toy.py --npz [output file]\r\n```\r\nthis will store the required data into a .npz file. The default parameters are:\r\n```\r\nn_conditions = 8\r\nn_modes = 3\r\nn_samples = 768  \r\n```\r\n\r\nYou can also store the raw trajectories into a .txt file with the following command:\r\n```\r\n$ python3 create_toy.py --txt [output file]\r\n```\r\nFor having fun and seeing the animation of toy agents you can call:\r\n```\r\n$ python3 create_toy.py --anim\r\n```\r\n\r\n\r\n",
      "technique": "Header extraction"
    }
  ]
}