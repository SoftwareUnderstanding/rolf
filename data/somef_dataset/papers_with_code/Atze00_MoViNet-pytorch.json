{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2103.11511",
      "https://arxiv.org/abs/2103.11511"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@article{kondratyuk2021movinets,\n  title={MoViNets: Mobile Video Networks for Efficient Video Recognition},\n  author={Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Matthew Brown, and Boqing Gong},\n  journal={arXiv preprint arXiv:2103.11511},\n  year={2021}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{kondratyuk2021movinets,\n  title={MoViNets: Mobile Video Networks for Efficient Video Recognition},\n  author={Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Matthew Brown, and Boqing Gong},\n  journal={arXiv preprint arXiv:2103.11511},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.995859219758795,
        0.9999227716632646
      ],
      "excerpt": "Pytorch unofficial implementation of MoViNets: Mobile Video Networks for Efficient Video Recognition. <br> \nAuthors: Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew Brown, Boqing Gong (Google Research) <br> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Atze00/MoViNet-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-02T10:11:43Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T16:49:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.914788438746977
      ],
      "excerpt": "It is required to clean the buffer after all the clips of the same video have been processed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9439099324423044
      ],
      "excerpt": "        num_classes is set to 600, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8283456865541465,
        0.9187598482975701,
        0.9554276467747909
      ],
      "excerpt": "*Accuracy reported on the official repository for the dataset kinetics 600, It has not been tested by me. It should be the same since the tf models and the reimplemented pytorch models output the same results [Test]. \nI currently haven't tested the speed of the streaming models, feel free to test and contribute. \nCurrently are available the pretrained models for the following architectures: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8940169903468237
      ],
      "excerpt": "I currently have no plans to include streaming version of A3,A4,A5. Those models are too slow for most mobile applications. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "MoViNets PyTorch implementation: Mobile Video Networks for Efficient Video Recognition; ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Atze00/MoViNet-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 21,
      "date": "Tue, 21 Dec 2021 21:41:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Atze00/MoViNet-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Atze00/MoViNet-pytorch",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Atze00/MoViNet-pytorch/main/movinet_tutorial.ipynb",
      "https://raw.githubusercontent.com/Atze00/MoViNet-pytorch/main/weights/weight_load.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```pip install git+https://github.com/Atze00/MoViNet-pytorch.git```\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8801854956928516,
        0.8600689777569865,
        0.9062991951605445,
        0.9062991951605445
      ],
      "excerpt": "from movinets import MoViNet \nfrom movinets.config import _C \nMoViNetA0 = MoViNet(_C.MODEL.MoViNetA0, causal = True, pretrained = True ) \nMoViNetA1 = MoViNet(_C.MODEL.MoViNetA1, causal = True, pretrained = True ) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8097069122563822
      ],
      "excerpt": "    If pretrained is True: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9035058829369951,
        0.9078701452257918
      ],
      "excerpt": "model = MoViNet(_C.MODEL.MoViNetA0, causal = True, pretrained = True ) \nmodel = MoViNet(_C.MODEL.MoViNetA0, causal = False, pretrained = True ) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8064750830880529
      ],
      "excerpt": "**In streaming mode, the number of frames correspond to the total accumulated \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Atze00/MoViNet-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Atze00\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "MoViNet-pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "MoViNet-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Atze00",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Atze00/MoViNet-pytorch/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 103,
      "date": "Tue, 21 Dec 2021 21:41:03 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "video-recognition",
      "movinets",
      "mobile-video-networks",
      "video",
      "stream-buffer"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Atze00/MoViNet-pytorch/blob/main/movinet_tutorial.ipynb) <br>\nClick on \"Open in Colab\" to open an example of training on HMDB-51 <br> \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Training loop with stream buffer\n```python\ndef train_iter(model, optimz, data_load, n_clips = 5, n_clip_frames=8):\n    \"\"\"\n    In causal mode with stream buffer a single video is fed to the network\n    using subclips of lenght n_clip_frames. \n    n_clips*n_clip_frames should be equal to the total number of frames presents\n    in the video.\n    \n    n_clips : number of clips that are used\n    n_clip_frames : number of frame contained in each clip\n    \"\"\"\n    \n    #:clean the buffer of activations\n    model.clean_activation_buffers()\n    optimz.zero_grad()\n    for i, data, target in enumerate(data_load):\n        #:backward pass for each clip\n        for j in range(n_clips):\n          out = F.log_softmax(model(data[:,:,(n_clip_frames)*(j):(n_clip_frames)*(j+1)]), dim=1)\n          loss = F.nll_loss(out, target)/n_clips\n          loss.backward()\n        optimz.step()\n        optimz.zero_grad()\n        \n        #:clean the buffer of activations\n        model.clean_activation_buffers()\n```\nTraining loop with standard convolutions\n```python\ndef train_iter(model, optimz, data_load):\n\n    optimz.zero_grad()\n    for i, (data,_ , target) in enumerate(data_load):\n        out = F.log_softmax(model(data), dim=1)\n        loss = F.nll_loss(out, target)\n        loss.backward()\n        optimz.step()\n        optimz.zero_grad()\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}