{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1701.07875",
      "https://arxiv.org/abs/1701.07875 <br>\n[2] https://en.wikipedia.org/wiki/Earth_mover%27s_distance <br>\n[3] https://en.wikipedia.org/wiki/Wasserstein_metric <br>\n[4] https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/ <br>\n[5] https://www.alexirpan.com/2017/02/22/wasserstein-gan.html \n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] https://arxiv.org/abs/1701.07875 <br>\n[2] https://en.wikipedia.org/wiki/Earth_mover%27s_distance <br>\n[3] https://en.wikipedia.org/wiki/Wasserstein_metric <br>\n[4] https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/ <br>\n[5] https://www.alexirpan.com/2017/02/22/wasserstein-gan.html \n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.8356013927728488
      ],
      "excerpt": "<td> MNIST generation </td> \n<td> MNIST generation </td> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Mohammad-Rahmdel/WassersteinGAN-Tensorflow",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-01T19:14:08Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-14T12:14:05Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Training GAN is hard. Models may never converge and mode collapses are common. <br>\n\nWhen learning generative models, we assume the data we have comes from some unknown distribution <img src='./readme_images/pr.png' />. (The r stands for real) We want to learn a distribution <img src='./readme_images/ptheta.png' />\u200b\u200b that approximates <img src='./readme_images/pr.png' />, where \u03b8 are the parameters of the distribution. <br>\nYou can imagine two approaches for doing this. <br>\n- Directly learn the probability density function <img src='./readme_images/ptheta.png' />\u200b\u200b. We optimize <img src='./readme_images/ptheta.png' />\u200b\u200b through maximum likelihood estimation.\n\n- Learn a function that transforms an existing distribution Z into <img src='./readme_images/ptheta.png' />\u200b\u200b.\n\n\nThe first approach runs into problems. Given function <img src='./readme_images/ptheta.png' />\u200b\u200b\u200b\u200b, the MLE objective is <br>\n<img src='./readme_images/eqn13.png' />\u200b\u200b <br>\nIn the limit, this is equivalent to minimizing the KL-divergence. <br>\n<img src='./readme_images/eqn14.png' />\u200b\u200b <br>\n<img src='./readme_images/eqn15.png' />\u200b\u200b <br>\n\nVariational Auto-Encoders (VAEs) and Generative Adversarial Networks (GANs) are well known examples of this approach.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9298775026294136
      ],
      "excerpt": "model.add(Dense(128 * 7 * 7, kernel_initializer=init, input_dim=latent_dim)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8341589559722501
      ],
      "excerpt": "    model.add(Reshape((7, 7, 128))) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8341589559722501
      ],
      "excerpt": "    model.add(BatchNormalization()) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8341589559722501
      ],
      "excerpt": "    model.add(BatchNormalization()) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8341589559722501
      ],
      "excerpt": "    model.add(BatchNormalization()) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8341589559722501
      ],
      "excerpt": "    model.add(BatchNormalization())  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8341589559722501,
        0.9298775026294136,
        0.882858313080758
      ],
      "excerpt": "    model.add(Flatten()) \n    model.add(Dense(1)) \nIn Critic's last dense layer there is no activation function(sigmoid). Therefore, its output value is not limited unlike discriminator which predicts the probability of its inputs as being fake or real. That's why the author called this network critic instead of discriminator! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9934440214515639,
        0.9589995977676425,
        0.9413107302363173,
        0.9320232479139985
      ],
      "excerpt": "In statistics, the earth mover's distance (EMD) is a measure of the distance between two probability distributions over a region D. Informally, if the distributions are interpreted as two different ways of piling up a certain amount of dirt over the region D, the EMD is the minimum cost of turning one pile into the other; where the cost is assumed to be the amount of dirt moved times the distance by which it is moved. Calculating the EMD is in itself an optimization problem. There are infinitely many ways to move the earth around, and we need to find the optimal one. \nThe EMD is widely used in content-based image retrieval to compute distances between the color histograms of two digital images. In this case, the region is the RGB color cube, and each image pixel is a parcel of \"dirt\". The same technique can be used for any other quantitative pixel attribute, such as luminance, gradient, apparent motion in a video frame, etc.. <br> \nIn computer science, this metric is widely used to compare discrete distributions, e.g. the color histograms of two digital images. Here are some examples of EMD that help for better understanding. \nIn mathematics, EMD is known as Wasserstein metric. The Wasserstein or Kantorovich\u2013Rubinstein metric or distance is a distance function defined between probability distributions on a given metric space M.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9435169050102573
      ],
      "excerpt": "where <img src='./readme_images/uv.png' /> denotes the collection of all measures on M \u00d7 M with marginals \u03bc and \u03bd on the first and second factors respectively. <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9927157622786279,
        0.9261111118503627
      ],
      "excerpt": "where E[Z] denotes the expected value of a random variable Z and the infimum is taken over all joint distributions of the random variables X and Y with marginals \u03bc and \u03bd respectively and d is a metric.  \nFor p=1, it is proven that the Wasserstein-1 metric in 1D (dimension one), between two cumulative distribution functions(CDF) F1 and F2 on R can be written as the L1 distance: <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9243025844493356
      ],
      "excerpt": "In Wasserstein GAN paper, Arjovsky et al use the Wasserstein-1 metric as a way to improve the original framework of Generative Adversarial Networks (GAN), to alleviate the vanishing gradient and the mode collapse issues. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9672691332564237
      ],
      "excerpt": "every distribution that converges under the KL, reverse-KL, TV, and JS divergences also converges under the Wasserstein divergence. It also proves that a small earth mover distance corresponds to a small difference in distributions. Combined, this shows the Wasserstein distance is a compelling loss function for generative models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8856683696602976
      ],
      "excerpt": "The paper shows how we can compute an approximation of this. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.928266935340697,
        0.8539352901297806,
        0.9853597783882908
      ],
      "excerpt": "where the supremum is taken over all 1-Lipschitz functions. \nf is called k-Lipschitz if |f(x) - f(y)| <= k.|| x-y || for all x, y. <br> \nThe Wasserstein distance is the minimum cost of transporting mass in converting the data distribution q to the data distribution p. The Wasserstein distance for the real data distribution Pr and the generated data distribution Pg is mathematically defined as the greatest lower bound (infimum) for any transport plan (i.e. the cost for the cheapest plan): <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9207373930788184,
        0.9611410459392781
      ],
      "excerpt": "\u03a0(Pr, Pg) denotes the set of all joint distributions \u03b3(x, y) whose marginals are respectively Pr and Pg. \nLet\u2019s look at the two common divergences used in generative models first, namely the KL-Divergence and the JS-Divergence where p is the real data distribution and q is the one estimated from the model. Let\u2019s assume they are Gaussian distributed. In the diagram below, we plot p and a few q having different means. <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9748105849110736
      ],
      "excerpt": "Below, we plot the corresponding KL-divergence and JS-divergence between p and q with means ranging from 0 to 35. As anticipated, when both p and q are the same, the divergence is 0. As the mean of q increases, the divergence increases. The gradient of the divergency will eventually diminish. We have close to a zero gradient, i.e. the generator learns nothing from the gradient descent. <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9146232300983975
      ],
      "excerpt": "Minimizing the GAN objective function with an optimal discriminator is equivalent to minimizing the JS-divergence. As illustrated above, if the generated image has distribution q far away from the ground truth p, the generator barely learns anything. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621762781314194
      ],
      "excerpt": "The network design is almost the same except the critic does not have an output sigmoid function. The major difference is only on the cost function: <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8795541060074918
      ],
      "excerpt": "However, there is one major thing missing. f has to be a 1-Lipschitz function. To enforce the constraint, WGAN applies a very simple clipping to restrict the maximum weight value in f, i.e. the weights of the discriminator must be within a certain range controlled by the hyperparameters c. <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9099643039008728
      ],
      "excerpt": "In GAN, the loss measures how well it fools the discriminator rather than a measure of the image quality. As shown below, the generator loss in GAN does not drop even the image quality improves. Hence, we cannot tell the progress from its value. We need to save the testing images and evaluate them visually. On the contrary, WGAN loss function reflects the image quality which is more desirable. <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8502705393151028
      ],
      "excerpt": "- the generator can still learn when the critic performs well.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9827693291307725,
        0.9373423046498114,
        0.9950827967667912,
        0.9153601889170973,
        0.8411093500855995,
        0.9026765439905334,
        0.9774642956478801
      ],
      "excerpt": "Instead of using a discriminator to classify or predict the probability of generated images as being real or fake, the WGAN changes or replaces the discriminator model with a critic that scores the realness or fakeness of a given image. \nThis change is motivated by a theoretical argument that training the generator should seek a minimization of the distance between the distribution of the data observed in the training dataset and the distribution observed in generated examples. \nThe benefit of the WGAN is that the training process is more stable and less sensitive to model architecture and choice of hyperparameter configurations. Perhaps most importantly, the loss of the discriminator appears to relate to the quality of images created by the generator. \nThe differences in implementation for the WGAN are as follows: \n1. Use a linear activation function in the output layer of the critic model (instead of sigmoid). \nThe DCGAN uses the sigmoid activation function in the output layer of the discriminator to predict the likelihood of a given image being real. \nIn the WGAN, the critic model requires a linear activation to predict the score of \u201crealness\u201d for a given image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9662088071556962
      ],
      "excerpt": "In the DCGAN, these are precise labels that the discriminator is expected to achieve. The WGAN does not have precise labels for the critic. Instead, it encourages the critic to output scores that are different for real and fake images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9763804131361774,
        0.918097399858278,
        0.9906619709806425
      ],
      "excerpt": "The DCGAN trains the discriminator as a binary classification model to predict the probability that a given image is real. \nTo train this model, the discriminator is optimized using the binary Cross-Entropy loss function. The same loss function is used to update the generator model. \nThe primary contribution of the WGAN model is the use of a new loss function that encourages the discriminator to predict a score of how real or fake a given input looks. This transforms the role of the discriminator from a classifier into a critic for scoring the realness or fakeness of images, where the difference between the scores is as large as possible. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9434045130398154
      ],
      "excerpt": "The score is maximizing for real examples and minimizing for fake examples. Given that stochastic gradient descent is a minimization algorithm, we can multiply the class label by the mean score (e.g. -1 for real and 1 for fake which as no effect), which ensures that the loss for real and fake images is minimizing to the network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Tensorflow implementation of WGAN (Wasserstein Generative Adversarial Networks)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Mohammad-Rahmdel/WassersteinGAN-Tensorflow-/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 21 Dec 2021 14:51:21 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Mohammad-Rahmdel/WassersteinGAN-Tensorflow/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Mohammad-Rahmdel/WassersteinGAN-Tensorflow",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Mohammad-Rahmdel/WassersteinGAN-Tensorflow-/master/WGAN.ipynb",
      "https://raw.githubusercontent.com/Mohammad-Rahmdel/WassersteinGAN-Tensorflow-/master/Earth%20Mover%27s%20Distance.ipynb",
      "https://raw.githubusercontent.com/Mohammad-Rahmdel/WassersteinGAN-Tensorflow-/master/.ipynb_checkpoints/Earth%20Mover%27s%20Distance-checkpoint.ipynb",
      "https://raw.githubusercontent.com/Mohammad-Rahmdel/WassersteinGAN-Tensorflow-/master/.ipynb_checkpoints/Untitled-checkpoint.ipynb",
      "https://raw.githubusercontent.com/Mohammad-Rahmdel/WassersteinGAN-Tensorflow-/master/.ipynb_checkpoints/WGAN-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8243583848559142
      ],
      "excerpt": "    model.add(Reshape((7, 7, 128))) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8699780450325489,
        0.8699780450325489
      ],
      "excerpt": "<td> <img src = 'results/3.gif'> \n<td> <img src = 'results/4.gif'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8699780450325489,
        0.8699780450325489
      ],
      "excerpt": "<td> <img src = 'results/mnist.gif'> \n<td> <img src = 'results/mnist_v2.gif'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8183181927103621
      ],
      "excerpt": "<td> epoch 100  </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8884629887036417,
        0.8884629887036417
      ],
      "excerpt": "<td> <img src = 'results/3/generated_plot_0100.png'> \n<td> <img src = 'results/3/generated_plot_1400.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8884629887036417,
        0.8884629887036417
      ],
      "excerpt": "<td> <img src = 'results/3/generated_plot_3400.png'> \n<td> <img src = 'results/3/plot_line_plot_loss.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8884629887036417
      ],
      "excerpt": "    <img src='results/6/generated_plot_8000.png' \\> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8376875059222426,
        0.8836704212480256
      ],
      "excerpt": "The <img src='./readme_images/pth.png' /> Wasserstein distance between two probability measures \u03bc and \u03bd in <img src='./readme_images/ppm.png' /> is defined as: <br> \n<img src='./readme_images/eqn10.png' /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src='./readme_images/eqn11.png' /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src='./readme_images/eqn12.png' /> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src='./readme_images/eqn16.png' />\u200b\u200b <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256,
        0.8836704212480256,
        0.8836704212480256
      ],
      "excerpt": "<img src='./readme_images/eqn17.png' />\u200b\u200b <br> \n<img src='./readme_images/eqn18.png' />\u200b\u200b <br> \n<img src='./readme_images/eqn19.png' />\u200b\u200b <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src='./readme_images/eqn1.png' />\u200b\u200b <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src='./readme_images/eqn2.png' />\u200b\u200b <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src='./readme_images/eqn1.png' /> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8370360397840984
      ],
      "excerpt": "<img src='./readme_images/fig10.jpeg' /> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8370360397840984
      ],
      "excerpt": "<img src='./readme_images/fig10.jpeg' /> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8370360397840984
      ],
      "excerpt": "<img src='./readme_images/wgan.jpeg' /> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8370360397840984
      ],
      "excerpt": "<img src='./readme_images/tables.jpeg' /> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src='./readme_images/eqn31.png' /> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8239984166194723
      ],
      "excerpt": "<img src='./readme_images/wgan-algorithm.png' /> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src='./readme_images/res1.png' /> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8370360397840984
      ],
      "excerpt": "<img src='./readme_images/res2.jpeg' /> <br> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Mohammad-Rahmdel/WassersteinGAN-Tensorflow/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "[Wasserstein GAN](https://arxiv.org/abs/1701.07875)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "WassersteinGAN-Tensorflow",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Mohammad-Rahmdel",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Mohammad-Rahmdel/WassersteinGAN-Tensorflow/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Tue, 21 Dec 2021 14:51:21 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "generative-adversarial-network",
      "tensorflow"
    ],
    "technique": "GitHub API"
  }
}