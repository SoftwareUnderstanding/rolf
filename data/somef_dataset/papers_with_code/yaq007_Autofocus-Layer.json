{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1603.05959",
      "https://arxiv.org/abs/1606.00915",
      "https://arxiv.org/abs/1606.00915"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find the code or the models implemented here are useful, please cite our paper:\n\n[Autofocus Layer for Semantic Segmentation](https://arxiv.org/pdf/1805.08403.pdf). \n[Y. Qin](http://cseweb.ucsd.edu/~yaq007/), K. Kamnitsas, S. Ancha, J. Nanavati, G. Cottrell, A. Criminisi, A. Nori, MICCAI 2018.\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yaq007/Autofocus-Layer",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you have any problems when using our codes or models, please feel free to contact me via e-mail: yaq007@eng.ucsd.edu.\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-05-21T16:24:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-16T07:26:38Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is a PyTorch implementation of the autofocus convolutional layer proposed for semantic segmentation with the objective of enhancing the capabilities of neural networks for multi-scale processing.\nAutofocus layers adaptively change the size of the effective receptive field \nbased on the processed context to generate more powerful features.\nThe proposed autofocus layer can be easily integrated into existing networks to improve a model's representational power. \n\nHere we apply the autofocus convolutional layer to deep neural networks for 3D semantic segmentation. We run experiments on the [Brain Tumor Image Segmentation dataset (BRATS2015)](https://www.smir.ch/BRATS/Start2015) as an example to show how the models work. In addition, we also implement a series of deep learning based models used for 3D Semantic Segmentation. The details of all the models implemented here can be found in our paper: [Autofocus Layer for Semantic Segmentation](https://arxiv.org/pdf/1805.08403.pdf).\n\n<img src=\"./src/autofocus.png\" width=\"900\"/>\n\nFigure 1. An autofocus convolutional layer with four candidate dilation rates. (a) The attention model. (b) A weighted summation of activations from parallel dilated convolutions. (c) An example of attention maps for a small (r^1) and large (r^2) dilation rate. The first row is the input and the segmentation result of AFN6. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8976157948088741,
        0.8398277418190442,
        0.8912178674475786
      ],
      "excerpt": "- Provide a mask including the Region of Interest (RoI) as one of the input image. For example, in the BRATS dataset, the region outside the brain should be masked out with the provided mask. \n- The intensity of the data within the RoI must be normalized to be zero-mean, unit-variance. For the BRATS dataset, each image must be normalized independently other than doing the normalization with the mean and variance of the whole training dataset. \n- Make sure the ground-truth labels for training and testing represent the background with zero. For example, we have four different  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9403867294357943,
        0.8157613278813797,
        0.950410122061728,
        0.9575996313537869,
        0.9363795319688077,
        0.9363795319688077
      ],
      "excerpt": "We provide the example codes for data preprocessing, including converting the data format, generating the masks and normalizing the input image. The corresponding text file is also provided to show the directory where the image are saved. You can create your own text file to save the image data path and change the corresponding code in the python scripts. The data normalization code is mainly derived from DeepMedic. \nA small subset of the BRATS dataset (after all the above data pre-processing) is provided here to run the preset examples. \nPlease refer \"Autofocus Layer for Semantic Segmentation\" for the details of all the supported models. \n- Basic Model: half pathway of DeepMedic with the last 6 layers with dilation rates equal 2. \n- ASPP-c: adding an ASPP module on top of Basic model (parallel features are merged via concatenation). \n- ASPP-s: adding an ASPP module on top of Basic model (parallel features are merged via summation). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.894543138770577
      ],
      "excerpt": "The code is developed under the follwing configurations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9381464651939406
      ],
      "excerpt": "- PyTorch 0.3.0 or higher is required to run the codes.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333956102471876,
        0.9819332443441887
      ],
      "excerpt": "- test_full.py The input of the network is a full image rather than a smaller image segment. \nThere are small differences of these two different testing methods due to the padding in the convolutions. For the performance that we report above, we use the test.py to get all the results.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Autofocus Layer for Semantic Segmentation",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yaq007/Autofocus-Layer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 33,
      "date": "Sat, 25 Dec 2021 14:47:20 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yaq007/Autofocus-Layer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "yaq007/Autofocus-Layer",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/yaq007/Autofocus-Layer/master/download_sub_dataset.sh",
      "https://raw.githubusercontent.com/yaq007/Autofocus-Layer/master/download_models.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "``` bash\ngit clone https://github.com/yaq007/Autofocus-Layer.git\nconda install pytorch torchvision -c pytorch\npip install nibabel\npip install SimpleITK\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8599075572079108,
        0.9437914241839805
      ],
      "excerpt": "- 1-3 GPUs with at least 12G GPU memories. You can choose the number of GPUs used via [--num_gpus NUM_GPUS]. \n- PyTorch 0.3.0 or higher is required to run the codes.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.981488574814665
      ],
      "excerpt": "For the models like \"Basic\", you may only need one gpu to run the experiments. For the models like \"AFN6\", you may need to increase the number of GPUs to be 2 or 3. This depends on the GPU memory that you are using. Please check all the input arguments via python train.py -h. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.88532055607357
      ],
      "excerpt": "To test, you can simply run  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8540108379913837
      ],
      "excerpt": "You can download the full dataset with training and testing images from https://www.smir.ch/BRATS/Start2015. To run all the models here, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8948061125563042
      ],
      "excerpt": "<img src=\"./src/performance.png\" width=\"900\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186046920996785
      ],
      "excerpt": "python train.py --num_gpus NUM_GPUS --id MODEL_ID \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8478640228666131
      ],
      "excerpt": "python val.py --num_gpus NUM_GPUS --id MODEL_ID \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.944230767782244
      ],
      "excerpt": "python test.py/test_full.py --num_gpus NUM_GPUS --id MODEL_ID --test_epoch NUM_OF_TEST_EPOCH \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yaq007/Autofocus-Layer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Yao Qin\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Autofocus Layer for Semantic Segmentation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Autofocus-Layer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "yaq007",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yaq007/Autofocus-Layer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 179,
      "date": "Sat, 25 Dec 2021 14:47:20 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "First, you need to download the provided subset of BRATS dataset and all the trained models. Please run\n``` bash\nchmod +x download_sub_dataset.sh\n./download_sub_daset.sh\n\nchmod +x download_models.sh\n./download_models.sh\n```\nThen you can run the following script to choose a model and do the testing. Here we use AFN1 as an example.\n```bash\npython test.py --num_gpus 1 --id AFN1 --test_epoch 390 \n```\nYou can change the number of used GPUs via ```[--num_gpus NUM_GPUS]``` and choose the tested model that you want via ```[--id MODEL_ID]```. Make sure the test epoch is included in the downloaded directory \"saved_models\".\n\nYou can check all the input arguments via ```python test.py -h```.\n\n",
      "technique": "Header extraction"
    }
  ]
}