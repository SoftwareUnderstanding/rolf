{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/angelvillar96/FaceEmoji",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-01T08:02:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-14T06:27:55Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9901148798144956
      ],
      "excerpt": "This project uses image processing techniques to perform face detection and deep learning in order to replace the faces by an emoji similar to the detected facial expression. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9152464913871798,
        0.945186932278139,
        0.8160998693256659,
        0.8698664795451599,
        0.8720048892283936
      ],
      "excerpt": "The deep learning part of the project has been developed using the PyTorch Library. The emotion detection has been preformed using a Resnet18 model (see https://arxiv.org/pdf/1512.03385.pdf for further information about Residual networks). The model used was pretrained on the ImageNet dataset containing over 1000000 images. \nIn order to adapt this model to our particular task, transfer learning has been applied. \nOn the one hand, the convolutional part of the network, which performs feature extraction, has been kept. \nOn the other hand,  the fully-connected part, which performs classification, has been modified. The last fully-connected layer was replaced for a new layer tailormade for our purpose. \nLabeled images that map facial expressions or facial features to emoji labels are necessary to retrain the network during the transfer learning procedure. Therefore, we have created ourselves a mini-dataset containing (approximately) 200 images for each class.    \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.88699078871991,
        0.9207687935655369,
        0.90054700978836,
        0.9182269558356646
      ],
      "excerpt": "For privacy issues, we do not make our dataset public, but the model state dictionaries of our trained models can be found under the directory /experiments/-/models. \nThe networks was retrained for 10 epochs using the ADAM optimizer and an initial learning rate of 0.001. \nIn just ten epochs, the loss (Binary Cross Entropy) decreases from 0.3 to just 0.05, while the accuracy increases to above 95%. \nOn static images, the network performs inference with a quite high accuracy. A comprehensible evaluatiion has not been carried out, but hopefully the following image is convincing: we can see how for the 6 images the network predicts the correct label. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047,
        0.9600777588843384,
        0.8400118948556984
      ],
      "excerpt": "Crops the faces \nUses the Deep Learning Model to predict an emoji \nReplaces the face by the emoji \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This project uses image processing techniques to perform face detection and deep learning in order to replace the faces by an emoji similar to the detected facial expression.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/angelvillar96/FaceEmoji/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The fact that the dataset used was created by us and not so large makes the network perform suboptimally. Furthermore, some emojies such as the monkey or crying are not easy to be detected as the face needs to be partially covered, thus making the performance of the face detector worse.\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 00:36:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/angelvillar96/FaceEmoji/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "angelvillar96/FaceEmoji",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8487881169677056
      ],
      "excerpt": "The images were taken using the python scrip testTakeImages.py included in the directory /Lib/utils. After being run, this script takes an image every 500ms and saves it into a directory specified by the user as command line argument. For example, the following commands start taking images and saving them into a directory named /happy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.9246227682586091
      ],
      "excerpt": "$ python testTakeImages.py --emoji happy \n$ python testTakeImages.py -e happy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8463830725986965
      ],
      "excerpt": "  <img src=\"/readme_images/inference.png\" width=\"450\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057,
        0.8890818307099057,
        0.8890818307099057,
        0.8890818307099057
      ],
      "excerpt": "  <img src=\"/readme_images/cow.png\" width=\"180\"> \n  <img src=\"/readme_images/happy.png\" width=\"180\"> \n  <img src=\"/readme_images/kiss.png\" width=\"180\"> \n  <img src=\"/readme_images/tongue.png\" width=\"180\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/angelvillar96/FaceEmoji/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Angel Villar Corrales\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Face Emoji",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "FaceEmoji",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "angelvillar96",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/angelvillar96/FaceEmoji/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To get the repository running, you will need the following packages: numpy, matplotlib, openCV and pyTorch\n\nYou can obtain them easily by installing the conda environment file included in the repository. To do so, run the following command from the Conda Command Window:\n\n```shell\n$ conda env create -f environment.yml\n$ activate FaceEmoji\n```\n\n*__Note__:* This step might take a few minutes\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Mon, 27 Dec 2021 00:36:30 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To get the code, fork this repository or clone it using the following command:\n\n>git clone https://github.com/angelvillar96/FaceEmoji.git\n\n",
      "technique": "Header extraction"
    }
  ]
}