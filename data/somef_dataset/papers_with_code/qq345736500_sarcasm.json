{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1609.08359",
      "https://arxiv.org/abs/1301.3781.\n\n[[5]](http://nlp.stanford.edu/pubs/glove.pdf) Jeffrey Pennington, Richard Socher, and Christopher D. Manning. \"GloVe: Global Vectors for Word Representation,\" in Proceedings of the 2014 Conference on Empirical Methods In Natural Language Processing (EMNLP 2014), October 2014.\n\n[[6]](https://arxiv.org/abs/1609.08359) Ben Eisner, Tim Rockt\u00e4schel, Isabelle Augenstein, Matko Bo\u0161njak, and Sebastian Riedel. \u201cemoji2vec: Learning Emoji Representations from their Description,\u201d in Proceedings of the 4th International Workshop on Natural Language Processing for Social Media at EMNLP 2016 (SocialNLP at EMNLP 2016), November 2016.\n\n[[7]](http://www.bioinf.jku.at/publications/older/2604.pdf) Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. 1997. In Neural Computation 9(8):1735-80.\n\n[[8]](https://arxiv.org/pdf/1409.0473.pdf) Dzmitry Bahdanau, KyungHyun Cho and Yoshua Bengio. 2016. Neural Machine Translation by Jointly Learning to Align and Translate. https://arxiv.org/abs/1409.0473v7",
      "https://arxiv.org/abs/1409.0473v7"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[[1]](http://www.cs.utah.edu/~riloff/pdfs/official-emnlp13-sarcasm.pdf) Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, and Ruihong Huang. 2013. Sarcasm as contrast between a positive sentiment and negative situation. In EMNLP, volume 13, pages 704\u2013714.\n\n[[2]](http://www.aclweb.org/anthology/W16-0425) Aniruddha Ghosh and Tony Veale. 2016. Fracking Sarcasm using Neural Network. 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2016). NAACL-HLT.\n\n[[3]](https://pdfs.semanticscholar.org/0c27/64756299a82659605b132aef9159f61a4171.pdf) Tomas Ptacek, Ivan Habernal, and Jun Hong. 2014. Sarcasm detection on Czech and English Twitter. In COLING, pages 213\u2013223.\n\n[[4]](https://arxiv.org/pdf/1301.3781.pdf) Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.\n\n[[5]](http://nlp.stanford.edu/pubs/glove.pdf) Jeffrey Pennington, Richard Socher, and Christopher D. Manning. \"GloVe: Global Vectors for Word Representation,\" in Proceedings of the 2014 Conference on Empirical Methods In Natural Language Processing (EMNLP 2014), October 2014.\n\n[[6]](https://arxiv.org/abs/1609.08359) Ben Eisner, Tim Rockt\u00e4schel, Isabelle Augenstein, Matko Bo\u0161njak, and Sebastian Riedel. \u201cemoji2vec: Learning Emoji Representations from their Description,\u201d in Proceedings of the 4th International Workshop on Natural Language Processing for Social Media at EMNLP 2016 (SocialNLP at EMNLP 2016), November 2016.\n\n[[7]](http://www.bioinf.jku.at/publications/older/2604.pdf) Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. 1997. In Neural Computation 9(8):1735-80.\n\n[[8]](https://arxiv.org/pdf/1409.0473.pdf) Dzmitry Bahdanau, KyungHyun Cho and Yoshua Bengio. 2016. Neural Machine Translation by Jointly Learning to Align and Translate. arXiv:1409.0473v7\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/qq345736500/sarcasm",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-03T21:54:45Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-17T12:11:07Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9986951422014725,
        0.9868420767341521,
        0.9896437286332553
      ],
      "excerpt": "Sarcasm is a form of verbal irony that is intended to express contempt or ridicule. Relying on the shared knowledge between the speaker and his audience, sarcasm requires wit to understand and wit to produce. In our daily interactions, we use gestures and mimics, intonation and prosody to hint the sarcastic intent. Since we do not have access to such paralinguistic cues, detecting sarcasm in written text is a much harder task. \nI investigated various methods to detect sarcasm in tweets, using both traditional machine learning (SVMs and Logistic Regressors on discrete features) and deep learning models (CNNs, LSTMs, GRUs, Bi-directional LSTMs and attention-based LSTMs), evaluating them on 4 different Twitter datasets (details in res/). \nThis research project was completed in partial fulfilment of the requirements for the degree of Bachelor of Science in Computer Science at the University of Manchester and under the careful supervision of Mr John McNaught, my tutor and mentor. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9686072350987922,
        0.9390476904495824,
        0.969595065605856,
        0.8886173015300257,
        0.8241685685579159,
        0.9770971606371841,
        0.9242012850491084
      ],
      "excerpt": "src/ contains all the source code used to process, analyse, train and evaluate the datasets (as described in res/) in order to investigate sarcasm detection on Twitter data \nres/ contains both the raw and processed datasets as well as some useful vocabularies, lists or selections of words/emojis that proved very useful in pre-processing the data \nmodels/ contains all the pre-trained models contributing to the achievement of the claimed results as well as all the trained models, saved after training under the described parameters and DL architectures \nplots/ contains a collection of interesting plots that should be useful in analysing and sustaining the results obtained \nstats/ contains some comparisons between preprocessing phases as well as some raw statistical results collected while training/evaluating \nimages/ contains the visualizations obtained and some pictures of the architectures or models used in the report or screencast \nHere are the results obtained on the considered datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968067112593775
      ],
      "excerpt": "In the sample visualization given below, doctor, late and even lame have heavier weights and therefore are contributing more to sarcasm recognition (since they receive more attention). Historically, we know that going to the doctor is regarded as an undesirable activity (so it is subject to strong sarcastic remarks) while late and lame are sentiment-bearing expressions, confirming previous results about sarcastic cues in written and spoken language.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9526850602816956
      ],
      "excerpt": "Visualize the attention words over the whole (or a selection of the) test set after you have trained the model. The network is paying attention to some specific words (supposedly, those who contribute more towards a sarcasm decision being made). A reddish colour is used to emphasize attention weights while colour gradients are used to distinguish the heavy from the weak weights. Run: \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/qq345736500/sarcasm/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 11:17:40 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/qq345736500/sarcasm/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "qq345736500/sarcasm",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Clone the repository and make sure that all the dependencies listed above are installed.\n2. Download all the resources from [here](https://drive.google.com/open?id=1AcGulyTXcrsn6hStefD3M0MNrzkxV_1n) and place them in the *res/* directory\n3. Download the pre-trained models from [here](https://drive.google.com/open?id=1ss9-4LEzuKC-p1s0lLa0XVu2_ERM-ynL) and place them in the *models/* directory\n4. Go to the *src/* directory\n5. For a thorough feature analysis, run:\n```bash\npython feature_analysis.py\n```\n6. For training and evaluating a traditional machine learning model, run:\n```bash\npython ml_models.py\n```\n7. For training and evaluating the embeddings (word and/or emojis/deepmojis), run:\n```bash\npython embeddings_model.py\n```\n8. For training and evaluating various deep learning models, quickly implemented in Keras, run:\n```bash\npython dl_models.py\n```\n9. For training and evaluating the attention-based LSTM model implemented in TensorFlow, run:\n```bash\npython tf_attention.py\n```\n\nBy default, the dataset collected by [Ghosh and Veale (2016)](http://www.aclweb.org/anthology/W16-0425) is used, but this can be easily replaced by changing the *dataset* parameter in the code (as for all other parameters).\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9655104381551366
      ],
      "excerpt": "python src/visualize_hidden_units.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9655104381551366
      ],
      "excerpt": "python src/visualize_tf_attention.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/qq345736500/sarcasm/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 UCL Machine Reading\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Sarcasm-Detection    it's fault",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sarcasm",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "qq345736500",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/qq345736500/sarcasm/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code included in this repository has been tested to work with Python 3.5 on an Ubuntu 16.04 machine, using Keras 2.0.8 with Tensorflow as the backend.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "* [Python](https://www.python.org/downloads/) 3.5\n* [Keras](https://github.com/fchollet/keras) 2.0\n* [Tensorflow](https://www.tensorflow.org/install/) 1.3\n* [gensim](https://github.com/RaRe-Technologies/gensim) 3.0 \n* [numpy](https://github.com/numpy/numpy) 1.13\n* [scikit-learn](https://github.com/scikit-learn/scikit-learn)\n* [h5py](https://github.com/h5py/h5py)\n* [emoji](https://github.com/carpedm20/emoji)\n* [tqdm](https://github.com/tqdm/tqdm)\n* [pandas](https://github.com/pandas-dev/pandas)\n* [itertools](https://pypi.python.org/pypi/more-itertools) \n* [matplotlib](https://github.com/matplotlib/matplotlib)\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Clone the repository and make sure that all the dependencies listed above are installed.\n2. Download all the resources from [here](https://drive.google.com/open?id=1AcGulyTXcrsn6hStefD3M0MNrzkxV_1n) and place them in the *res/* directory\n3. Download the pre-trained models from [here](https://drive.google.com/open?id=1ss9-4LEzuKC-p1s0lLa0XVu2_ERM-ynL) and place them in the *models/* directory\n4. Go to the *src/* directory\n5. For a thorough feature analysis, run:\n```bash\npython feature_analysis.py\n```\n6. For training and evaluating a traditional machine learning model, run:\n```bash\npython ml_models.py\n```\n7. For training and evaluating the embeddings (word and/or emojis/deepmojis), run:\n```bash\npython embeddings_model.py\n```\n8. For training and evaluating various deep learning models, quickly implemented in Keras, run:\n```bash\npython dl_models.py\n```\n9. For training and evaluating the attention-based LSTM model implemented in TensorFlow, run:\n```bash\npython tf_attention.py\n```\n\nBy default, the dataset collected by [Ghosh and Veale (2016)](http://www.aclweb.org/anthology/W16-0425) is used, but this can be easily replaced by changing the *dataset* parameter in the code (as for all other parameters).\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Fri, 24 Dec 2021 11:17:40 GMT"
    },
    "technique": "GitHub API"
  }
}