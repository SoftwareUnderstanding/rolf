{
  "citation": [
    {
      "confidence": [
        0.8187756947909643,
        0.8187756947909643
      ],
      "excerpt": "'residual': hp.choice( \n    'residual', [None, hp.quniform( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9805378847049522,
        0.9795187663973746
      ],
      "excerpt": "    'all_conv',  #: All-convolutionnal: https://arxiv.org/pdf/1412.6806.pdf \n    'inception'  #: Inspired from: https://arxiv.org/pdf/1602.07261.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8089227735508986
      ],
      "excerpt": "#: The kernel_size for residual convolutions: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8187756947909643
      ],
      "excerpt": "'residual': 4, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    \"history\": {...}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8187756947909643
      ],
      "excerpt": "        \"residual\": 3.0, \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-05-27T18:15:37Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-20T08:17:39Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9852086263605604
      ],
      "excerpt": "This project acts as both a tutorial and a demo to using Hyperopt with Keras, TensorFlow and TensorBoard. Not only we try to find the best hyperparameters for the given hyperspace, but also we represent the neural network architecture as hyperparameters that can be tuned. This automates the process of searching for the best neural architecture configuration and hyperparameters. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.865501696703861
      ],
      "excerpt": "First off, to learn how hyperopt works and what it is for, read the hyperopt tutorial. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9448660768053343
      ],
      "excerpt": "Also, the results are pickled to results.pkl to be able to resume the TPE meta-optimization process later simply by running the program again with python3 hyperopt_optimize.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9374455227091065,
        0.997324171043784
      ],
      "excerpt": "It is possible that you get better results than there are already here. Pull requests / contributions are welcome. Suggestion: trying many different initializers for the layers would be an interesting thing to try. Adding SELU activations would be interesting too. To restart the training with new or removed hyperparameters, it is recommended to delete existing results with ./delete_results.sh. \nHere is a basic overview of the model. I implemented it in such a way that Hyperopt will try to change the shape of the layers and remove or replace some of them according to some pre-parametrized ideas that I have got. Therefore, not only the learning rate is changed with hyperopt, but a lot more parameters. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254751311447194
      ],
      "excerpt": "space = { \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8145507995419725
      ],
      "excerpt": "    #: it vary exponentially, in a multiplicative fashion rather than in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "    #: Choice of optimizer: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9684915575131307
      ],
      "excerpt": "#: The kernel_size for convolutions: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184876049031471
      ],
      "excerpt": "#: The kernel_size for residual convolutions: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9898142133637998
      ],
      "excerpt": "Here is an analysis of the results regarding the effect of every hyperparameters. Here is an excerpt: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9721315147531567,
        0.828364990916022,
        0.9811113601644403,
        0.9875703155706412
      ],
      "excerpt": "This could help to redefine the hyperparameters and to narrow them down successively, relaunching the meta-optimization on refined spaces. \nThe best model is this one: results/model_0.676100010872_6066e.txt.json. \nThe final accuracy is of 67.61% in average on the 100 fine labels, and is of 77.31% in average on the 20 coarse labels. \nMy results are comparable to the ones in the middle of that list, under the CIFAR-100 section. The only image preprocessing that I do is a random flip left-right. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254751311447194
      ],
      "excerpt": "    \"space\": { \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8865247627096642
      ],
      "excerpt": "It is possible to run python3 retrain_best_with_tensorboard.py to retrain the model and save TensorBoard logs, as well as saving the weights at their best state during training for a potential reuse. The instructions to run TensorBoard will be printed in the console at the end of the retraining. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8061581674692981
      ],
      "excerpt": "Here is the command to run TensorBoard once located in the root directory of the project: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8352409088018279,
        0.9899958186441227,
        0.9780654324581962
      ],
      "excerpt": "It suggests that better weights and biases initialization schemes could be used. \nIt is also possible to see in TensorBoard more statistics and things, such as the distribution tab, the graphs tab, and the the scalars tab. See printscreens of all the statistics available under the TensorBoard/previews/ folder of this project. \nWe use the method of gradient ascent in the input space. This consists of generating images that activate certain filters in layers. This consists of using a loss on the filters' activation to then derive and apply gradients in the input space to gradually form input images that activate the given filters maximally. This is done for each filter separately. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Auto-optimizing a neural net (and its architecture) on the CIFAR-100 dataset. Could be easily transferred to another dataset or another classification task.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 75,
      "date": "Thu, 23 Dec 2021 20:45:25 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100/Vooban/AnalyzeTrainHyperoptResults.ipynb",
      "https://raw.githubusercontent.com/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100/Vooban/AnalyzeTestHyperoptResults.ipynb",
      "https://raw.githubusercontent.com/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100/Vooban/IntroductionToHyperopt.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100/Vooban/delete_results_and_reset_project.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8860917406406693
      ],
      "excerpt": "To run the hyperparameter search vy yourself, do: python3 hyperopt_optimize.py. You might want to look at requirements.py and install some of them manually to acquire GPU acceleration (e.g.: installing TensorFlow and Keras especially by yourself). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8437033485426174
      ],
      "excerpt": "#: Use one more FC layer at output \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8768801849289117
      ],
      "excerpt": "    'use_BN': hp.choice('use_BN', [False, True]), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    'use_BN': True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "  <img src=\"model_demo.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "  <img src=\"hyperparameters_scatter_matrix.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "        \"use_BN\": true \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "  <img src=\"model_best.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8451814827105916
      ],
      "excerpt": "Every training's TensorBoard log will be in a new folder under the \"TensorBoard/\" directory with an unique name (the model ID). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8206273742484786
      ],
      "excerpt": "Logs for the best model can be downloaded manually (approximately 7 GB). Refer to the text file under the folder TensorBoard for directions on how to download the logs from Google Drive before running the TensorBoard client with the tensorboard --logdir=TensorBoard/ command.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "  <img src=\"tensorboard_histogram_example.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8069629827411853
      ],
      "excerpt": "To run the visualization, one must edit conv_filters_visualization.py to make it load the good weights (in case a retraining was done) and then run python3 conv_filters_visualization.py. The images for layers will be seen under the folder layers/ of this project. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8729134779148817
      ],
      "excerpt": "  <img src=\"layers/add_1_best_filters_49_(7x7)_out_of_63.png\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100/Vooban/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The MIT License (MIT)\\n\\nCopyright (c) 2017 Vooban Inc.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\nNOTICE: This MIT License is a sublicense on top of the previous work and the\\nprevious MIT License of Guillaume Chevalier, which can be found here: \\nhttps://github.com/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100/blob/master/LICENSE\\nand here:\\nhttps://github.com/guillaume-chevalier/python-caffe-custom-cifar-100-conv-net\\n\\nNOTICE: Code from Fran\\xc3\\xa7ois Chollet is also used (and edited) in the file\\nconv_filter_visualization.py to generate images in the folder \"layers/\". \\nMore information about this sublicense can be seen in that python \".py\" file. \\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Hyperopt for solving CIFAR-100 with a convolutional neural network (CNN) built with Keras and TensorFlow, GPU backend",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Hyperopt-Keras-CNN-CIFAR-100",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "guillaume-chevalier",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100/blob/Vooban/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 101,
      "date": "Thu, 23 Dec 2021 20:45:25 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "hyperopt",
      "hyperparameter-optimization",
      "hyperparameter-tuning",
      "hyperparameters-optimization",
      "hyperparameter-search",
      "keras",
      "cnn",
      "cnn-keras",
      "tensorflow"
    ],
    "technique": "GitHub API"
  }
}