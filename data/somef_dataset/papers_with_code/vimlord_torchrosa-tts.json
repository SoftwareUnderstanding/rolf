{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1312.6114\n\nInspiration for Mel spectrogram use: https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "VAE: https://arxiv.org/abs/1312.6114\n\nInspiration for Mel spectrogram use: https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vimlord/torchrosa-tts",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-03T05:04:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-03T05:06:40Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9512437406383163,
        0.9479072705428064,
        0.934276569152543
      ],
      "excerpt": "This project contains files I put together to try out creating a text-to-speech \nsystem from scratch. The system uses a PyTorch model to learn a VAE that is \nused to learn an embedding that plugs into the decoder. The result is something \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.927580182290449,
        0.8092194609662005,
        0.9625662383767085,
        0.958704036802942
      ],
      "excerpt": "In the future, I might want to mess with using GAN to improve the results. \nI have a couple of ideas here: \nImproving the quality of the phoneme sound bites by training a discriminator to learn realness of phoneme sounds \nLearning a model that converts the generated spectrogram to a more realistic sounding audio \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.842696213172414
      ],
      "excerpt": "it is quite slow on a laptop. One investigation that could be done would be to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A text-to-speech program using VAE on Mel spectrograms of phonemes.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vimlord/torchrosa-tts/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 05:01:20 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vimlord/torchrosa-tts/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "vimlord/torchrosa-tts",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I will try to get around to writing a requirements.txt, but I will summarize the\nrequirements as best as possible. For this project, I installed:\n\n```\npython3\npytorch\nlibrosa\npyaudio\ng2p_en\nscipy\nnumpy\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8718121970824857,
        0.856213546651025
      ],
      "excerpt": "python generate-metadata.py \nThis will generate the CSV file in the current directory. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vimlord/torchrosa-tts/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "TorchRosa TTS",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "torchrosa-tts",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "vimlord",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vimlord/torchrosa-tts/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 05:01:20 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tts",
      "librosa",
      "vae",
      "pytorch",
      "mel-spectrogram"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I will provide the PyTorch model I generated for my own voice for free. To use\nthis model, the interface is provided through generate.py. I designed it so\nadditional models could be setup for the class if desired. The simplest way to\nuse it is to run\n\n```\npython3 generate.py --text \"Hello world\"\n```\n\nThat's it! It should cobble together a speech sample in `output.wav`\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "So this is the fun part. So in order to do this, you will need to record\nyourself speaking the majority of the phonemes. Some of the phonemes were\nfiltered out because they didn't provide continuous sounds, and are replaced\nby ones that do provide continuous sounds that are placed in sequence.\nThese can be seen in `util/preprocess.py`. For a full list of phonemes, run\n\n```\npython phoneme_handler.py list-phonemes\n```\n\nWhen I recorded my voice, I practiced making the phoneme sounds. To actually\nrecord, I provide a convenient utility for this.\n\n```\npython phoneme_handler.py record-phoneme AA\n```\n\nThis will record the phoneme AA. When you run it, it will prompt you to press\nenter when you are ready. I recommend beginning to make the phoneme sound\nimmediately after the button press, and continuing until the program terminates\nfor best results. I do not recommend this if you have pre-existing health\nissues that impact your breathing or lung capacity. It is possible to reduce\nthe recording length in the program, as there is a constant where I se the\nutterance length to five seconds.\n\nIf you have issues with missing directories, creating them should resolve most\nissues.\n\nOnce the phonemes are recorded, you need to generate the spectrograms. This is\ndone by running ```python generate_spectrograms.py```. This will create Mel\nspectrograms of each audio file recorded and save them as numpy files within\nthe package.\n\nFinally, you need to train the model. Running `python train.py` will train the\nVAE model by default. If you wish to do hyperparameter tuning, you can edit\n`vae/hyperparameters.py`. Note that I was lazy with my train-test split setup,\nso there will be contamination. But I found that the hyperparameters there\nworked okay for my files.\n\nOnce `train.py` has been run, it will save `model.pt` in your directory. If\nthis file already exists, it will be overwritten. Once it exists, you can\nuse the model.\n\n",
      "technique": "Header extraction"
    }
  ]
}