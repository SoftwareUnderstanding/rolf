{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1703.03939\n*Ramachandran, Govardana Sachithanandam, and Ajay Sohmshetty. \"Ask Me Even More: Dynamic Memory Tensor Networks (Extended Model",
      "https://arxiv.org/abs/1703.03939 (2017).* \n\nOrginally published as http://cs224d.stanford.edu/reports/SohmshettyRamachandran.pdf\n*Sohmshetty, Ajay, and Govardana Sachithanandam Ramachandran. \"Ask Me Even More: Dynamic Memory Tensor Networks (Extended Model).\" http://cs224d.stanford.edu/reports_2016.html (June 2016)*\n\n**Abstract**:\nWe examine Memory Networks for the task of question answering (QA), under common real world scenario where training examples are scarce and under weakly supervised scenario, that is only extrinsic labels are available for training. We propose extensions for the Dynamic Memory Network (DMN), specifically within the attention mechanism, we call the resulting Neural Architecture as Dynamic Memory Tensor Network (DMTN). Ultimately, we see that our proposed extensions results in over 80% improvement in the number of task passed against the baselined standard DMN and 20% more task passed compared to state-of-the-art End-to-End Memory Network for Facebook's single task weakly trained 1K bAbi dataset.\n\n![dmtncomparison](https://cloud.githubusercontent.com/assets/19319509/25372789/ac4bd34a-294b-11e7-8455-3ebd26d53c42.jpg)\n\n_[Table:1]Accuracies across all tasks for MemN2N, DMN, and DMTN. Here DMN baselines\nserves as the baseline for DTMN to measure the lift with the proposed changes. DMN best* is the\nbest document performance of DMN with optimal hyperparameter tuning on bAbi weakly trained\ndataset- http://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks_\n\nThe above results are obtained by using following Hyper-parameter was used between DMN baseline and DMTN. Please note that due to lack of time & resource Hyper-parameter tunning was not done, Hence we recommend you to play with Hyper-parameter for even better results\n\n![hyper_parameter](https://cloud.githubusercontent.com/assets/19319509/25373388/54ef9f02-294e-11e7-8ed4-9acbdbde9c40.jpg)\n\n_[Table:2]Hyperparameters used for DMN baseline and DMTN._\n\nDMN as described in the [paper by Kumar et al.](http://arxiv.org/abs/1506.07285)\nand to experiment with its various extensions.\n\n\n**Pretrained models on bAbI tasks can be tested [online](http://yerevann.com/dmn-ui/).**\n\nWe will cover the process in a series of blog posts.\n* [The first post](http://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks/) describes the details of the basic architecture and presents our first results on [bAbI tasks](http://fb.ai/babi) v1.2.\n* [The second post](http://yerevann.github.io/2016/02/23/playground-for-babi-tasks/) describes our second model called `dmn_smooth` and introduces our [playground for bAbI tasks](http://yerevann.com/dmn-ui/).\n\n## Repository contents\n\n| file | description |\n| --- | --- |\n| `main.py` | the main entry point to train and test available network architectures on bAbI-like tasks |\n| `dmn_basic.py` | our baseline implementation. It is as close to the original as we could understand the paper, except the number of steps in the main memory GRU is fixed. Attention module uses `T.abs_` function as a distance between two vectors which causes gradients to become `NaN` randomly.  The results reported in [this blog post](http://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks/) are based on this network |\n| `dmn_smooth.py` | uses the square of the Euclidean distance instead of `abs` in the attention module. Training is very stable. Performance on bAbI is slightly better |\n| `dmtn.py` | DMTN implementaion |\n| `dmn_batch.py` | `dmn_smooth` with minibatch training support. The batch size cannot be set to `1` because of the [Theano bug](https://github.com/Theano/Theano/issues/1772) | \n| `dmn_qa_draft.py` | draft version of a DMN designed for answering multiple choice questions | \n| `utils.py` | tools for working with bAbI tasks and GloVe vectors |\n| `nn_utils.py` | helper functions on top of Theano and Lasagne |\n| `fetch_babi_data.sh` | shell script to fetch bAbI tasks (adapted from [MemN2N](https://github.com/npow/MemN2N)) |\n| `fetch_glove_data.sh` | shell script to fetch GloVe vectors (by [5vision](https://github.com/5vision/kaggle_allen)) |\n| `server/` | contains Flask-based restful api server |\n\n\n## Usage\n\nThis implementation is based on Theano and Lasagne. One way to install them is:\n\n    pip install -r https://raw.githubusercontent.com/Lasagne/Lasagne/master/requirements.txt\n    pip install https://github.com/Lasagne/Lasagne/archive/master.zip\n\nThe following bash scripts will download bAbI tasks and GloVe vectors.\n\n    ./fetch_babi_data.sh\n    ./fetch_glove_data.sh\n\nUse `main.py` to train a network:\n\n    python main.py --network dmtn --babi_id 1\n\nThe states of the network will be saved in `states/` folder. \nThere is one pretrained state on the 1st bAbI task. It should give 100% accuracy on the test set:\n\n    python main.py --network dmtn --mode test --babi_id 1 --load_state states/dmn_basic.mh5.n40.babi1.epoch4.test0.00033.state\n\n### Server\n\nIf you want to start a server which will return the predication for bAbi tasks, you should do the following:\n\n1. Generate UI files as described in [YerevaNN/dmn-ui](YerevaNN/dmn-ui)\n2. Copy the UI files to `server/ui`\n3. Run the server \n\n```bash\ncd server && python api.py\n```\n\nIf have Docker installed, you can pull our Docker image with ready DMN server.\n\n```bash\ndocker pull yerevann/docker\ndocker run --name dmn_1 -it --rm -p 5000:5000 yerevann/dmn\n```\n\n## Roadmap\n\n* Mini-batch training ([done](https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano/blob/master/dmn_batch.py), 08/02/2016)\n* Web interface ([done](https://github.com/YerevaNN/dmn-ui), 08/23/2016)\n* Visualization of episodic memory module ([done](https://github.com/YerevaNN/dmn-ui), 08/23/2016)\n* Regularization (work in progress, L2 doesn't help at all, dropout and batch normalization help a little)\n* Support for multiple-choice questions ([work in progress](https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano/blob/master/dmn_qa_draft.py))\n* Evaluation on more complex datasets\n* Import some ideas from [Neural Reasoner](http://arxiv.org/abs/1508.05508)\n\n## License\n[The MIT License (MIT)](./LICENSE)\nCopyright (c) 2016 YerevaNN"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9668253207263927,
        0.9992994182582684
      ],
      "excerpt": "DMTN as described in \u00a0https://arxiv.org/abs/1703.03939 \nRamachandran, Govardana Sachithanandam, and Ajay Sohmshetty. \"Ask Me Even More: Dynamic Memory Tensor Networks (Extended Model).\" arXiv preprint arXiv:1703.03939 (2017).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9158421667818202
      ],
      "excerpt": "dataset- http://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8968592401678916,
        0.9287913210266059
      ],
      "excerpt": "Mini-batch training (done, 08/02/2016) \nWeb interface (done, 08/23/2016) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rgsachin/DMTN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-03-11T06:46:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-16T20:15:45Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9398048401951561
      ],
      "excerpt": "The aim of this repository is to implement Dynamic Memory Tensor Networks, besides the Dynamic memory networks covered in the parent. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9869852074078772,
        0.8564481130183941,
        0.9623186833152617,
        0.8122135556222528
      ],
      "excerpt": "We examine Memory Networks for the task of question answering (QA), under common real world scenario where training examples are scarce and under weakly supervised scenario, that is only extrinsic labels are available for training. We propose extensions for the Dynamic Memory Network (DMN), specifically within the attention mechanism, we call the resulting Neural Architecture as Dynamic Memory Tensor Network (DMTN). Ultimately, we see that our proposed extensions results in over 80% improvement in the number of task passed against the baselined standard DMN and 20% more task passed compared to state-of-the-art End-to-End Memory Network for Facebook's single task weakly trained 1K bAbi dataset. \n[Table:1]Accuracies across all tasks for MemN2N, DMN, and DMTN. Here DMN baselines \nserves as the baseline for DTMN to measure the lift with the proposed changes. DMN best* is the \nbest document performance of DMN with optimal hyperparameter tuning on bAbi weakly trained \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9436451298998528
      ],
      "excerpt": "The above results are obtained by using following Hyper-parameter was used between DMN baseline and DMTN. Please note that due to lack of time & resource Hyper-parameter tunning was not done, Hence we recommend you to play with Hyper-parameter for even better results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8483069764175343
      ],
      "excerpt": "DMN as described in the paper by Kumar et al. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9121178770492642,
        0.8368584650670012,
        0.9565527454922316
      ],
      "excerpt": "We will cover the process in a series of blog posts. \n* The first post describes the details of the basic architecture and presents our first results on bAbI tasks v1.2. \n* The second post describes our second model called dmn_smooth and introduces our playground for bAbI tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9920718900541213,
        0.98481997687341
      ],
      "excerpt": "| dmn_basic.py | our baseline implementation. It is as close to the original as we could understand the paper, except the number of steps in the main memory GRU is fixed. Attention module uses T.abs_ function as a distance between two vectors which causes gradients to become NaN randomly.  The results reported in this blog post are based on this network | \n| dmn_smooth.py | uses the square of the Euclidean distance instead of abs in the attention module. Training is very stable. Performance on bAbI is slightly better | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8968808373629559
      ],
      "excerpt": "| dmn_batch.py | dmn_smooth with minibatch training support. The batch size cannot be set to 1 because of the Theano bug |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8061879376200394,
        0.8230684034748303
      ],
      "excerpt": "| utils.py | tools for working with bAbI tasks and GloVe vectors | \n| nn_utils.py | helper functions on top of Theano and Lasagne | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8455237129665273
      ],
      "excerpt": "| server/ | contains Flask-based restful api server | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8322760022993029
      ],
      "excerpt": "Regularization (work in progress, L2 doesn't help at all, dropout and batch normalization help a little) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8093363215588492
      ],
      "excerpt": "Evaluation on more complex datasets \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rgsachin/DMTN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Thu, 23 Dec 2021 06:01:36 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rgsachin/DMTN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rgsachin/DMTN",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rgsachin/DMTN/master/fetch_glove_data.sh",
      "https://raw.githubusercontent.com/rgsachin/DMTN/master/fetch_babi_data.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8155112086424948
      ],
      "excerpt": "If you want to start a server which will return the predication for bAbi tasks, you should do the following: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8206940304646906
      ],
      "excerpt": "| main.py | the main entry point to train and test available network architectures on bAbI-like tasks | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "| dmtn.py | DMTN implementaion | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8559988378977348
      ],
      "excerpt": "docker run --name dmn_1 -it --rm -p 5000:5000 yerevann/dmn \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rgsachin/DMTN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The MIT License (MIT)\\n\\nCopyright (c) 2016 YerevaNN\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Dynamic Memory Tensor Networks in Theano",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DMTN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rgsachin",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rgsachin/DMTN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 12,
      "date": "Thu, 23 Dec 2021 06:01:36 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This implementation is based on Theano and Lasagne. One way to install them is:\n\n    pip install -r https://raw.githubusercontent.com/Lasagne/Lasagne/master/requirements.txt\n    pip install https://github.com/Lasagne/Lasagne/archive/master.zip\n\nThe following bash scripts will download bAbI tasks and GloVe vectors.\n\n    ./fetch_babi_data.sh\n    ./fetch_glove_data.sh\n\nUse `main.py` to train a network:\n\n    python main.py --network dmtn --babi_id 1\n\nThe states of the network will be saved in `states/` folder. \nThere is one pretrained state on the 1st bAbI task. It should give 100% accuracy on the test set:\n\n    python main.py --network dmtn --mode test --babi_id 1 --load_state states/dmn_basic.mh5.n40.babi1.epoch4.test0.00033.state\n\n",
      "technique": "Header extraction"
    }
  ]
}