{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2103.00020",
      "https://arxiv.org/abs/2108.08688",
      "https://arxiv.org/abs/2103.00020",
      "https://arxiv.org/abs/2101.05783",
      "https://arxiv.org/abs/2101.05783",
      "https://arxiv.org/abs/2103.00020",
      "https://arxiv.org/abs/2108.08688",
      "https://arxiv.org/abs/2101.05783.\n\nGwet, K. L. (2008). [Computing inter\u2010rater reliability and its variance in the presence of high agreement.](https://bpspsychub.onlinelibrary.wiley.com/doi/full/10.1348/000711006X126600) British Journal of Mathematical and Statistical Psychology, 61(1), 29-48.\n\nNozza, D., Bianchi, F., & Hovy, D. (2021, June). [HONEST: Measuring hurtful sentence completion in language models.](https://www.aclweb.org/anthology/2021.naacl-main.191.pdf) In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 2398-2406).\n\nRadford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). [Learning Transferable Visual Models From Natural Language Supervision.](https://arxiv.org/abs/2103.00020) ICML.\n\nReimers, N., & Gurevych, I. (2020, November). [Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation.](https://aclanthology.org/2020.emnlp-main.365/) In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 4512-4525).\n\nScaiella, A., Croce, D., & Basili, R. (2019). [Large scale datasets for Image and Video Captioning in Italian.](http://www.ai-lc.it/IJCoL/v5n2/IJCOL_5_2_3___scaiella_et_al.pdf) IJCoL. Italian Journal of Computational Linguistics, 5(5-2), 49-60.\n\nSharma, P., Ding, N., Goodman, S., & Soricut, R. (2018, July). [Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.](https://aclanthology.org/P18-1238.pdf) In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 2556-2565).\n\nSrinivasan, K., Raman, K., Chen, J., Bendersky, M., & Najork, M. (2021). [WIT: Wikipedia-based image text dataset for multimodal multilingual machine learning](https://arxiv.org/pdf/2103.01913.pdf). arXiv preprint https://arxiv.org/abs/2103.01913.\n\n# Other Notes\nThis readme has been designed using resources from Flaticon.com",
      "https://arxiv.org/abs/2103.01913.\n\n# Other Notes\nThis readme has been designed using resources from Flaticon.com"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Abid, A., Farooqi, M., & Zou, J. (2021). [Persistent anti-muslim bias in large language models.](https://arxiv.org/abs/2101.05783) arXiv preprint arXiv:2101.05783.\n\nGwet, K. L. (2008). [Computing inter\u2010rater reliability and its variance in the presence of high agreement.](https://bpspsychub.onlinelibrary.wiley.com/doi/full/10.1348/000711006X126600) British Journal of Mathematical and Statistical Psychology, 61(1), 29-48.\n\nNozza, D., Bianchi, F., & Hovy, D. (2021, June). [HONEST: Measuring hurtful sentence completion in language models.](https://www.aclweb.org/anthology/2021.naacl-main.191.pdf) In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 2398-2406).\n\nRadford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). [Learning Transferable Visual Models From Natural Language Supervision.](https://arxiv.org/abs/2103.00020) ICML.\n\nReimers, N., & Gurevych, I. (2020, November). [Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation.](https://aclanthology.org/2020.emnlp-main.365/) In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 4512-4525).\n\nScaiella, A., Croce, D., & Basili, R. (2019). [Large scale datasets for Image and Video Captioning in Italian.](http://www.ai-lc.it/IJCoL/v5n2/IJCOL_5_2_3___scaiella_et_al.pdf) IJCoL. Italian Journal of Computational Linguistics, 5(5-2), 49-60.\n\nSharma, P., Ding, N., Goodman, S., & Soricut, R. (2018, July). [Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.](https://aclanthology.org/P18-1238.pdf) In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 2556-2565).\n\nSrinivasan, K., Raman, K., Chen, J., Bendersky, M., & Najork, M. (2021). [WIT: Wikipedia-based image text dataset for multimodal multilingual machine learning](https://arxiv.org/pdf/2103.01913.pdf). arXiv preprint arXiv:2103.01913.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{bianchi2021contrastive,\n  title={Contrastive Language-Image Pre-training for the Italian Language},\n  author={Bianchi, Federico and Attanasio, Giuseppe and Pisoni, Raphael and Terragni, Silvia and Sarti, Gabriele and Lakshmi, Sri},\n  journal={arXiv preprint arXiv:2108.08688},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9056674988540252
      ],
      "excerpt": "vision transformer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.980242875019047,
        0.9985403584435437,
        0.9999537354568557,
        0.9664456561658856
      ],
      "excerpt": "  title={Contrastive Language-Image Pre-training for the Italian Language}, \n  author={Bianchi, Federico and Attanasio, Giuseppe and Pisoni, Raphael and Terragni, Silvia and Sarti, Gabriele and Lakshmi, Sri}, \n  journal={arXiv preprint arXiv:2108.08688}, \n  year={2021} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8365978949010984
      ],
      "excerpt": "| person walking down the aisle                                                     | persona che cammina lungo la navata                                                                     |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "| MRR@10          | 0.5204   | 0.4129| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266,
        0.8955886365383559
      ],
      "excerpt": "| Accuracy@10     |  52.55   | 42.91 | \n| Accuracy@100    |  81.08   | 67.11 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8437515675687063
      ],
      "excerpt": "Look at the following - slightly cherry picked - examples: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008247398284739
      ],
      "excerpt": "GitHub Repository \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/clip-italian/clip-italian",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-07T09:38:02Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T07:04:17Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9640972679320234,
        0.919261538906646,
        0.9743008472911587,
        0.8298127996639446
      ],
      "excerpt": "CLIP (Radford et al., 2021) is a multimodal model that can learn to represent images and text jointly in the same space. \nIn this project, we aim to propose the first CLIP model trained on Italian data, that in this context can be considered a \nlow resource language. Using a few techniques, we have been able to fine-tune a SOTA Italian CLIP model with only 1.4 million training samples. Our Italian CLIP model \nis built upon the pre-trained Italian BERT model provided by dbmdz and the OpenAI \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9189352545239078,
        0.9884210353714975,
        0.9950375615132911,
        0.9673777687201884,
        0.9953150134640676,
        0.9357500471837641
      ],
      "excerpt": "In building this project we kept in mind the following principles: \nNovel Contributions: We created an impressive dataset of ~1.4 million Italian image-text pairs (that we will share with the community) and, to the best of our knowledge, we trained the best Italian CLIP model currently in existence; \nScientific Validity: Claim are easy, facts are hard. That's why validation is important to assess the real impact of a model. We thoroughly evaluated our models on two tasks and made the validation reproducible for everybody. \nBroader Outlook: We always kept in mind which are the possible usages and limitations of this model. \nWe put our hearts and souls into the project during this week! Not only did we work on a cool project, but we were \nable to make new friends and learn a lot from each other to work towards a common goal!  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9952284461251124,
        0.981765257503464,
        0.9362387707150729
      ],
      "excerpt": "Text to Image: This task is essentially an image retrieval task. The user is asked to input a string of text and CLIP is going to \ncompute the similarity between this string of text with respect to a set of images. The webapp is going to display the images that \nhave the highest similarity with the text query. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9865546287755351,
        0.9733045794335389
      ],
      "excerpt": "Image to Text: This task is essentially a zero-shot image classification task. The user is asked for an image and for a set of captions/labels and CLIP \nis going to compute the similarity between the image and each label. The webapp is going to display a probability distribution over the captions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9808167096168022,
        0.9873036526342363
      ],
      "excerpt": "Localization: This is a very cool feature :sunglasses: and at the best of our knowledge, it is a novel contribution. We can use CLIP \nto find where \"something\" (like a \"cat\") is in an image. The location of the object is computed by masking different areas of the image and looking at how the similarity to the image description changes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9859750529371277,
        0.9767997648683069,
        0.9171700312628975,
        0.9682344007673597
      ],
      "excerpt": "The original CLIP model was trained on 400 million image-text pairs; this amount of data is currently not available for Italian.  \nWe indeed worked in a low-resource setting. The only datasets for Italian captioning in the literature are MSCOCO-IT (a translated version of MSCOCO) and WIT.  \nTo get competitive results, we followed three strategies: \nmore and better data; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8306950493044324,
        0.8651453674701054,
        0.9338721322115056,
        0.8782818138422661,
        0.9330934342603037,
        0.818638496967927,
        0.8591107009734517,
        0.8208791376346695,
        0.8077770173335506,
        0.9829336061384873,
        0.9257272654135363,
        0.9895860111681519,
        0.9940151097926382
      ],
      "excerpt": "For those interested, we have a :comet: Comet report  \nthat shows a subset of the experiments we ran. Different hyper-parameters played a role in reducing the validation  \nloss. The optimizer we used gave us great performance and fast convergence, more data and augmentations helped a lot in generalizing, \nworking on the training and on the loss gave us the final increase that you can see in the results. \nWe eventually had to deal with the fact that we do not have the same data that OpenAI had during the training of CLIP. \nThus, we tried to add as much data as possible while keeping the data-quality as high as possible. \nWe considered four main sources of data: \nWIT is an image-caption dataset collected from Wikipedia (see,  \nSrinivasan et al., 2021). We focused on the Reference Description captions  \ndescribed in the paper as they are the ones of highest quality. Nonetheless, many of these captions describe ontological knowledge and encyclopedic facts (e.g., Roberto Baggio in 1994).  \nHowever, this kind of text, without more information, is not useful to learn a good mapping between images and captions.  \n To prevent polluting the data with captions that are not meaningful, we used POS tagging  \n  on the text and removed all the captions that were composed for the 80% or more by PROPN (around ~10% of the data). This is a simple solution that allowed us to retain much \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8547897075821062
      ],
      "excerpt": "MSCOCO dataset and have been translated with Microsoft Translator. The 2017 version of the MSCOCO training set contains more than \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8974839299616466
      ],
      "excerpt": "the work by Sharma et al., 2018. There are more than 3mln image-caption pairs in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9368012067423236
      ],
      "excerpt": "could not retrieve them all. Eventually, we had to translate the captions to Italian. We have been able to collect \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9077055467681723,
        0.9869482362741702,
        0.8536952799373952,
        0.9767295443329095,
        0.9582653630889241,
        0.8526749036554687,
        0.9470004715778676,
        0.8965747136761377
      ],
      "excerpt": "Instead of relying on open-source translators, we decided to use DeepL. Translation quality of the data was the main  \nreason of this choice. With the few images (wrt OpenAI) that we have, we cannot risk polluting our own data. CC is a great resource, \nbut the captions have to be handled accordingly. We translated 700K captions and we evaluated their quality. \nThree of us looked at a sample of 100 of the translations and rated them with scores from 1 to 4.  \nThe meaning of the value is as follows: 1, the sentence has lost is meaning, or it's not possible to understand it; 2, it is possible to get the idea \nbut there is something wrong; 3, good, however a native speaker might complain about some translations; 4, good translation. \nThe average score was of 3.78, and the three annotators had an inter-rater agreement - computed with Gwet's AC1 using ordinal  \nweighting - of 0.858 (great agreement!).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230215811517776
      ],
      "excerpt": "| an endless cargo of tanks on a train pulled down tracks in an empty dry landscape | un carico infinito di carri armati su un treno trascinato lungo i binari in un paesaggio secco e vuoto  |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9902400629355451,
        0.905632683668341,
        0.9586033796546691,
        0.9472077771485479,
        0.979789637587361,
        0.9390430228020659,
        0.9625186986601486,
        0.8284492668447614,
        0.9667128701813414,
        0.8929006612705735,
        0.9436111295195044,
        0.9531927438944688,
        0.8541763769463792,
        0.9783661391488283,
        0.9874359491474182,
        0.9206509747443166,
        0.8545827552886065,
        0.9660923668399151
      ],
      "excerpt": "We know that we annotated our own data; in the spirit of fairness we also share the annotations and the captions so \nthat those interested can check the quality. The Google Sheet is here. \nWe knew that without a good augmentation strategy we could never get competitive results to a model trained on 400 million images. Therefore, we implemented heavy augmentations to make the training more data efficient. \nThey include random affine transformations and perspective changes, as well as occasional equalization and random changes to brightness, contrast, saturation and hue. We made sure to keep hue augmentations limited however, to still give the model the ability to learn color definitions. \nWhile we would have liked to have augmentations for the captions as well, after some experimentation we settled with random sampling from the five captions available in MSCOCO and leaving the rest of the captions unmodified. \nAfter different trials, we realized that the usual way of training this model was \nnot good enough to get good results. We thus modified three different parts of the \ntraining pipeline: the optimizer, the training with frozen components, and the fixed logit_scale parameter. \nWhile the initial code used AdamW as an optimizer, we soon noticed that it introduced some bad properties into the training. The model strated to overfit relatively quickly and the weight decay made this effect worse.  \nWe eventually decided to use an optimization strategy that had worked well for us in similar cases and used AdaBelief with Adaptive Gradient Clipping (AGC) and a Cosine Annealing Schedule. \nTogether with slightly tuning the learning rate this helped us to reduce the validation loss by more than 25%. \nOur implementation is available online here. \nThe ViT used by OpenAI was already trained on 400 million images, and it is the element in our architecture that probably requires the least amount of training. \nThe same is true for the BERT model we use. To allow the randomly initialized re-projection layers to warm up without messing with the tuned weights of the backbones, we decided to do a first training with the backbones of our architecture completely frozen.  \nOnly after these layers converged we unfroze the rest of the model to fine-tune all the components. This technique allowed us to reach a much better validation loss. \nWe tried to improve the loss function in different ways: for example, we tried something similar to a margin based loss but that experiments \ndid not yield the results we hoped for. Eventually, the thing that worked out the best was fixing the logit_scale value to 20. This value \nis used after the computation of the similarity between the images and the texts in CLIP (see the code here). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8357226757934652,
        0.9393426758869885,
        0.9789071421732586
      ],
      "excerpt": "The following picture showcases the effect that these edits have had on our evaluation loss: \nThe purple line is the original training without any of our improvements: you can see that we needed a lot of training steps to get the loss down.  \nYellow line is the loss with the new optimizer, it is striking to see the time we save from this addition! Not only the loss improves, it  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9684656072251198,
        0.9492181404812307
      ],
      "excerpt": "fixed scaling is used in addition to the new optimizer. Finally, we added the backbone freezing strategy, and you can see the \nresults in the light blue loss. Nonetheless, as common in deep learning, having more data played a big role and was another key element \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8873086893502701,
        0.9594446701644259,
        0.9473756506599055,
        0.9904484528327914,
        0.8903885039086384
      ],
      "excerpt": "We split this section in two: we first provide a quantitative evaluation to ensure that what we are learning is in fact good. \nWe then show some qualitative examples of images found by the model. All the code we have written to run our validation experiments (in combination with \ncode made available by Nils Reimers and by the authors of the original CLIP) is available. \nWe tried different combinations of splits sizes for training and validation. Eventually, we focused on a 95% training split with 5% of data \ngoing into the validation, each dataset is split in training and validation data and then we concatenate the files.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372934901708509,
        0.9529217092147185,
        0.8872506195013334,
        0.9241052323491646,
        0.8389031424255233
      ],
      "excerpt": "The hyper-parameters can be found in the repository.  \nWe have a maximum sequence length of 95 tokens. To compute this we look at the distribution of the captions in the various  \ndatasets and we eventually realized that 95 was an excellent compromise between training speed and data coverage. \nWe use a batch size of 128 and a learning rate of 0.00001. \nWe usually train until we see the loss going up and we then pick the model with the best validation loss. We adjusted the number of training epochs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9279429177722764,
        0.9307937508204093,
        0.9695981176650876,
        0.9153720362537573
      ],
      "excerpt": "Showing great images is definitely cool and interesting, but a model is nothing without validation. \nSince this is the first clip-based model in Italian, we decided to use the multilingual CLIP model as a comparison baseline. \nThe multilingual CLIP (henceforth, mCLIP), is a model introduced by Nils Reimers in his \nsentence-transformer library. mCLIP is based on a multilingual encoder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8964811840592966
      ],
      "excerpt": "great capabilities in representing multilingual text in the same space of the images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8552712391280417,
        0.9620743999746117,
        0.8618361357456913,
        0.8814492665834329
      ],
      "excerpt": "+ image-retrieval, in which given a caption the model finds the most semantically similar image \n+ zero-shot classification, in which given an image and a set of captions (or labels), the model finds  \nthe best matching caption for the image \nIn order to make both experiments very easy to replicate, we share the colab notebooks we used to compute the results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9129161890904518
      ],
      "excerpt": "we search for the most similar image in the MSCOCO-IT validation set and check if this is the one that was \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8986911743622028
      ],
      "excerpt": "It is true that we used the training set of MSCOCO-IT in training, and this might give us an advantage. However, the original CLIP model was trained \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9131858999661329
      ],
      "excerpt": "To do this, we used DeepL to automatically translate the image labels in ImageNet. No manual engineering of the labels or prompts was done.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9903599496781126,
        0.8985553629138956,
        0.8313049943550784,
        0.9028979089960677
      ],
      "excerpt": "Our results confirm that CLIP-Italian is very competitive and beats mCLIP on the two different task \nwe have been testing. Note, however, that our results are lower than those shown in the original OpenAI \npaper (see, Radford et al., 2021) that was trained and evaluated on English data.  \nHowever, considering that our results are in line with those obtained by mCLIP we think that the translated image  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9601857654849555
      ],
      "excerpt": "We hereby show some interesting properties of the model. One is its ability to detect colors,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9102212610343678,
        0.9911884275888574
      ],
      "excerpt": "more examples in the \"Gallery\" section of the demo. \nTo our own surprise, many of the answers the model gives make a lot of sense! Note that the model, in this case, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8154070794316465
      ],
      "excerpt": "And finally, here's a very nice \"cat on a chair\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9704387164003391
      ],
      "excerpt": "to clustering, a model like our Italian CLIP can be used to support researchers and practitioners in many different tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9928386937766153
      ],
      "excerpt": "these website often deal with a main source of text that is the query engine and with lots of images of the products. CLIP Italian \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9501431878034701,
        0.987160680688994,
        0.9376889539361537,
        0.9738985129217991,
        0.9840212160210541,
        0.9985132267752643,
        0.9560558833239414
      ],
      "excerpt": "of photos in digital format that are difficult to categorize efficiently.  \nFor example, the Istituto Luce Cinecitt\u00e0 is an Italian governative entity that collects photos of Italy since the \nearly 1900 and is part of the largest movie studios in Europe (Cinecitt\u00e0). A semantic way of finding images in their catalog could be an amazing use case. \nCurrently, the model is not without limits. To mention one, its counting capabilities seem very cool, but from our experiments the model  \nfinds difficult to count after three; this is a general limitation that is common to many models of this type.   \nThere are even more evident issues that we found in our model. Due to the unfiltered nature of our training data, the model is exposed to many biases such as sexism, racism, stereotypes,  \nslurs, and gore that it might replicate without the awareness of their hurtful and harmful nature. Indeed, different BERT models - Italian ones included - are prone to create stereotyped  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9775337930368692,
        0.9554609551991959,
        0.8486671605112609,
        0.8716692804739435,
        0.9343158744828716
      ],
      "excerpt": "While this is not something we intended, it certainly is something that we share the blame for since we were not able to avoid it. \nUnfortunately, these kinds of issues are common to many machine learning algorithms (check Abit et al., 2021 for bias in GPT-3 as an example). \nThis suggests we need to find better approaches to counteract this problem that affects our society. \nGitHub Repository \nModel on HuggingFace \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "CLIP (Contrastive Language\u2013Image Pre-training) for Italian ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/clip-italian/clip-italian/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Thu, 23 Dec 2021 03:06:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/clip-italian/clip-italian/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "clip-italian/clip-italian",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/clip-italian/clip-italian/master/evaluation/CLIP_italian_ImageNet_Zero_Shot_Evaluation_.ipynb",
      "https://raw.githubusercontent.com/clip-italian/clip-italian/master/evaluation/CLIP_Image_Retrieval_and_MRR.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/clip-italian/clip-italian/master/hybrid_clip/run_training.sh",
      "https://raw.githubusercontent.com/clip-italian/clip-italian/master/tests/regex_tester.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8751335956035698
      ],
      "excerpt": "Pre-print available here \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8103024291551426
      ],
      "excerpt": "HuggingFace Spaces demo available here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8735448770123392
      ],
      "excerpt": "<!-- <img src=\"https://huggingface.co/spaces/clip-italian/clip-italian-demo/raw/main/static/img/text_to_image.png\" alt=\"drawing\" width=\"95%\"/> --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8735448770123392
      ],
      "excerpt": "<!-- <img src=\"https://huggingface.co/spaces/clip-italian/clip-italian-demo/raw/main/static/img/image_to_text.png\" alt=\"drawing\" width=\"95%\"/> --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8735448770123392
      ],
      "excerpt": "<!-- <img src=\"https://huggingface.co/spaces/clip-italian/clip-italian-demo/raw/main/static/img/gatto_cane.png\" alt=\"drawing\" width=\"95%\"/> --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8370236109979792
      ],
      "excerpt": "more examples in the \"Gallery\" section of the demo. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8735448770123392
      ],
      "excerpt": "<!-- <img src=\"https://huggingface.co/spaces/clip-italian/clip-italian-demo/raw/main/static/img/fiore_giallo.png\" alt=\"drawing\" width=\"500\"/> --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8735448770123392
      ],
      "excerpt": "<!-- <img src=\"https://huggingface.co/spaces/clip-italian/clip-italian-demo/raw/main/static/img/fiore_blu.png\" alt=\"drawing\" width=\"500\"/> --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8735448770123392
      ],
      "excerpt": "<!-- <img src=\"https://huggingface.co/spaces/clip-italian/clip-italian-demo/raw/main/static/img/gatto.png\" alt=\"drawing\" width=\"500\"/> --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8735448770123392
      ],
      "excerpt": "<!-- <img src=\"https://huggingface.co/spaces/clip-italian/clip-italian-demo/raw/main/static/img/due_gatti.png\" alt=\"drawing\" width=\"500\"/> --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8735448770123392
      ],
      "excerpt": "<!-- <img src=\"https://huggingface.co/spaces/clip-italian/clip-italian-demo/raw/main/static/img/due_cavalli_marroni.png\" alt=\"drawing\" width=\"500\"/> --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8735448770123392
      ],
      "excerpt": "<!-- <img src=\"https://huggingface.co/spaces/clip-italian/clip-italian-demo/raw/main/static/img/gatto_su_sedia.png\" alt=\"drawing\" width=\"500\"/> --> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/clip-italian/clip-italian/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Italian CLIP",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "clip-italian",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "clip-italian",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/clip-italian/clip-italian/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 62,
      "date": "Thu, 23 Dec 2021 03:06:58 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "clip",
      "transformers",
      "transformers-models",
      "contrastive-loss",
      "huggingface",
      "dataset"
    ],
    "technique": "GitHub API"
  }
}