{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2106.12672\">Charformer paper</a>, in Pytorch. The paper proposes a module that automatically learns subword representations, obviating the need for tokenizers in the encoder setting.\n\n<a href=\"https://www.youtube.com/watch?v=debgj24BAZE\">AI Coffee Break with Letitia video</a>\n\n## Install\n\n```bash\n$ pip install charformer-pytorch\n```\n\n## Usage\n\n```python\nimport torch\nfrom charformer_pytorch import GBST\n\ntokenizer = GBST(\n    num_tokens = 257,             # number of tokens, should be 256 for byte encoding (+ 1 special token for padding in this example"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@misc{tay2021charformer,\n    title   = {Charformer: Fast Character Transformers via Gradient-based Subword Tokenization}, \n    author  = {Yi Tay and Vinh Q. Tran and Sebastian Ruder and Jai Gupta and Hyung Won Chung and Dara Bahri and Zhen Qin and Simon Baumgartner and Cong Yu and Donald Metzler},\n    year    = {2021},\n    eprint  = {2106.12672},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{tay2021charformer,\n    title   = {Charformer: Fast Character Transformers via Gradient-based Subword Tokenization}, \n    author  = {Yi Tay and Vinh Q. Tran and Sebastian Ruder and Jai Gupta and Hyung Won Chung and Dara Bahri and Zhen Qin and Simon Baumgartner and Cong Yu and Donald Metzler},\n    year    = {2021},\n    eprint  = {2106.12672},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lucidrains/charformer-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-30T16:32:13Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T12:39:34Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8676339455784633
      ],
      "excerpt": "Implementation of the GBST (gradient-based subword tokenization) module from the <a href=\"https://arxiv.org/abs/2106.12672\">Charformer paper</a>, in Pytorch. The paper proposes a module that automatically learns subword representations, obviating the need for tokenizers in the encoder setting. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of the GBST block from the Charformer paper, in Pytorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lucidrains/charformer-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Sat, 25 Dec 2021 11:26:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lucidrains/charformer-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lucidrains/charformer-pytorch",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ pip install charformer-pytorch\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lucidrains/charformer-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Phil Wang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Charformer - Pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "charformer-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lucidrains",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lucidrains/charformer-pytorch/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-07-15T00:40:09Z",
        "datePublished": "2021-07-15T00:40:24Z",
        "html_url": "https://github.com/lucidrains/charformer-pytorch/releases/tag/0.0.4",
        "name": "0.0.4",
        "tag_name": "0.0.4",
        "tarball_url": "https://api.github.com/repos/lucidrains/charformer-pytorch/tarball/0.0.4",
        "url": "https://api.github.com/repos/lucidrains/charformer-pytorch/releases/46222114",
        "zipball_url": "https://api.github.com/repos/lucidrains/charformer-pytorch/zipball/0.0.4"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-07-08T03:20:07Z",
        "datePublished": "2021-07-08T03:20:43Z",
        "html_url": "https://github.com/lucidrains/charformer-pytorch/releases/tag/0.0.3",
        "name": "0.0.3",
        "tag_name": "0.0.3",
        "tarball_url": "https://api.github.com/repos/lucidrains/charformer-pytorch/tarball/0.0.3",
        "url": "https://api.github.com/repos/lucidrains/charformer-pytorch/releases/45877974",
        "zipball_url": "https://api.github.com/repos/lucidrains/charformer-pytorch/zipball/0.0.3"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-06-30T19:36:28Z",
        "datePublished": "2021-06-30T19:36:47Z",
        "html_url": "https://github.com/lucidrains/charformer-pytorch/releases/tag/0.0.2",
        "name": "0.0.2",
        "tag_name": "0.0.2",
        "tarball_url": "https://api.github.com/repos/lucidrains/charformer-pytorch/tarball/0.0.2",
        "url": "https://api.github.com/repos/lucidrains/charformer-pytorch/releases/45514745",
        "zipball_url": "https://api.github.com/repos/lucidrains/charformer-pytorch/zipball/0.0.2"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-06-30T19:11:38Z",
        "datePublished": "2021-06-30T19:11:49Z",
        "html_url": "https://github.com/lucidrains/charformer-pytorch/releases/tag/0.0.1",
        "name": "0.0.1",
        "tag_name": "0.0.1",
        "tarball_url": "https://api.github.com/repos/lucidrains/charformer-pytorch/tarball/0.0.1",
        "url": "https://api.github.com/repos/lucidrains/charformer-pytorch/releases/45513493",
        "zipball_url": "https://api.github.com/repos/lucidrains/charformer-pytorch/zipball/0.0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 70,
      "date": "Sat, 25 Dec 2021 11:26:49 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "artificial-intelligence",
      "deep-learning",
      "tokenization",
      "transformer"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nimport torch\nfrom charformer_pytorch import GBST\n\ntokenizer = GBST(\n    num_tokens = 257,             #: number of tokens, should be 256 for byte encoding (+ 1 special token for padding in this example)\n    dim = 512,                    #: dimension of token and intra-block positional embedding\n    max_block_size = 4,           #: maximum block size\n    downsample_factor = 4,        #: the final downsample factor by which the sequence length will decrease by\n    score_consensus_attn = True   #: whether to do the cheap score consensus (aka attention) as in eq. 5 in the paper\n)\n\ntokens = torch.randint(0, 257, (1, 1023)) #: uneven number of tokens (1023)\nmask   = torch.ones(1, 1023).bool()\n\n#: both tokens and mask will be appropriately downsampled\n\ntokens, mask = tokenizer(tokens, mask = mask) #: (1, 256, 512), (1, 256)\n\n#: now pass this on to your transformer\n```\n\nDeviating from the paper, you can also specify block size(s) with different offsets. This is to cover a potential use-case for genomics pre-training, where the tokenizer should be able to learn the correct frame. Simply omit the `max_block_size`, and pass in `blocks` as a list of tuples of tuples, each tuple with the format `(block size, offset)`. Offsets must be less than the block size\n\n```python\nimport torch\nfrom charformer_pytorch import GBST\n\ntokenizer = GBST(\n    num_tokens = 4 + 1,\n    dim = 512,\n    blocks = ((3, 0), (3, 1), (3, 2)),  #: block size of 3, with offsets of 0, 1, 2\n    downsample_factor = 3,\n    score_consensus_attn = True\n).cuda()\n\nbasepairs = torch.randint(0, 4, (1, 1023)).cuda()\nmask      = torch.ones(1, 1023).bool().cuda()\n\n#: both basepairs and mask will be appropriately downsampled\n\nbasepairs, mask = tokenizer(basepairs, mask = mask)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}