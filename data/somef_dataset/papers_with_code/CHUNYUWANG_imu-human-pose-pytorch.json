{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{zhe2020fusingimu,\n  title={Fusing Wearable IMUs with Multi-View Images for Human Pose Estimation: A Geometric Approach},\n  author={Zhang, Zhe and Wang, Chunyu and Qin, Wenhu and Zeng, Wenjun},\n  booktitle = {CVPR},\n  year={2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{zhe2020fusingimu,\n  title={Fusing Wearable IMUs with Multi-View Images for Human Pose Estimation: A Geometric Approach},\n  author={Zhang, Zhe and Wang, Chunyu and Qin, Wenhu and Zeng, Wenjun},\n  booktitle = {CVPR},\n  year={2020}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CHUNYUWANG/imu-human-pose-pytorch",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\nThis project has adopted the Microsoft Open Source Code of Conduct.\nFor more information see the Code of Conduct FAQ or\ncontact opencode@microsoft.com with any additional questions or comments.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-26T13:23:05Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-10T09:49:20Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9416107603930863
      ],
      "excerpt": "Since our ORN and ORPSM has no learnable parameters, it can be conveniently appended to any 2D pose estimator. Thus training the SN backbone is sufficient. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8199837720617734
      ],
      "excerpt": "a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8073737513954784,
        0.9783267603835809
      ],
      "excerpt": "This project has adopted the Microsoft Open Source Code of Conduct. \nFor more information see the Code of Conduct FAQ or \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This is an official Pytorch implementation of \"Fusing Wearable IMUs with Multi-View Images for Human Pose Estimation: A Geometric Approach, CVPR 2020\". ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CHUNYUWANG/imu-human-pose-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 13,
      "date": "Mon, 27 Dec 2021 02:18:56 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/CHUNYUWANG/imu-human-pose-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "CHUNYUWANG/imu-human-pose-pytorch",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For **TotalCapture** dataset, please download from [official site](https://cvssp.org/projects/totalcapture/TotalCapture/) and follow [zhezh/TotalCapture-Toolbox](https://github.com/zhezh/TotalCapture-Toolbox) to process data.\n>  We have no permission to redistribute this dataset. Please do not ask us for a copy.\n\nFor **precalculated pictorial model pairwise** term, please download from [HERE](https://dllabml-my.sharepoint.com/:f:/g/personal/research_dllabml_onmicrosoft_com/EtF51b86YvdEvcwErjkluGsBVbQXeXfMTUNEfc04BsNNDA?e=pMK1s9), and save in `data/pict`.\n\nTo reproduce our results in the paper, please download the trained models from [HERE](https://dllabml-my.sharepoint.com/:f:/g/personal/research_dllabml_onmicrosoft_com/EjpV84hHu0RGmiLl_3BjpWMBK1S15OzygM0pNxnf7dLevw?e=bYTlCV).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Clone this repo, and we'll call the directory that you cloned as ${POSE_ROOT}\n2. Install dependencies.\n3. Download pytorch imagenet pretrained models. Please download them under ${POSE_ROOT}/models, and make them look like this:\n\n   ```\n   ${POSE_ROOT}/models\n   \u2514\u2500\u2500 pytorch\n       \u2514\u2500\u2500 imagenet\n           \u251c\u2500\u2500 resnet152-b121ed2d.pth\n           \u251c\u2500\u2500 resnet50-19c8e357.pth\n           \u2514\u2500\u2500 mobilenet_v2.pth.tar\n   ```\n   They can be downloaded from the following link: [Pretrained Model Download](https://1drv.ms/f/s!AjX41AtnTHeThyJfayggVZSd0M6P)\n   \n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8003352366805131
      ],
      "excerpt": "a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8825655122405716
      ],
      "excerpt": "python run/pose2d/valid.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.819573227039251,
        0.9372565577971909
      ],
      "excerpt": "--dataDir . --logDir log --modelDir output \npython run/pose3d/estimate.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.819573227039251,
        0.8825655122405716
      ],
      "excerpt": "--dataDir . --logDir log --modelDir output \npython run/pose2d/valid.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.819573227039251
      ],
      "excerpt": "--dataDir . --logDir log --modelDir output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372565577971909
      ],
      "excerpt": "python run/pose3d/estimate.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.819573227039251
      ],
      "excerpt": "--dataDir . --logDir log --modelDir output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.958402584740488
      ],
      "excerpt": "python run/pose2d/train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.819573227039251
      ],
      "excerpt": "--dataDir . --logDir log --modelDir output \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/CHUNYUWANG/imu-human-pose-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) Microsoft Corporation.\\n\\nMIT License\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED AS IS, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Fusing Wearable IMUs with Multi-View Images for Human Pose Estimation: A Geometric Approach",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "imu-human-pose-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "CHUNYUWANG",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CHUNYUWANG/imu-human-pose-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 65,
      "date": "Mon, 27 Dec 2021 02:18:56 GMT"
    },
    "technique": "GitHub API"
  }
}