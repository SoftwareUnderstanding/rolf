{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1805.08318",
      "https://arxiv.org/abs/1710.10196",
      "https://arxiv.org/abs/1706.08500"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.973971314312959
      ],
      "excerpt": "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"center\">  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alfagao/DeOldify",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-27T06:34:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-09T20:57:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9790851534554414,
        0.9095192854759148
      ],
      "excerpt": "Simply put, the mission of this project is to colorize and restore old images.  I'll get into the details in a bit, but first let's get to the pictures!  BTW \u2013 most of these source images originally came from the TheWayWeWere subreddit, so credit to them for finding such great photos. \nMaria Anderson as the Fairy Fleur de farine and Lyubov Rabtsova as her page in the ballet \u201cSleeping Beauty\u201d at the Imperial Theater, St. Petersburg, Russia, 1890. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977245508412412,
        0.911478160601604,
        0.8167087243980514
      ],
      "excerpt": "Interior of Miller and Shoemaker Soda Fountain, 1899 \nParis in the 1880s \nEdinburgh from the sky in the 1920s \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9831192346713913,
        0.9700778910769211,
        0.9556045562791071
      ],
      "excerpt": "This is a deep learning based model.  More specifically, what I've done is combined the following approaches: \n* Self-Attention Generative Adversarial Network (https://arxiv.org/abs/1805.08318).  Except the generator is a pretrained U-Net, and I've just modified it to have the spectral normalization and self-attention.  It's a pretty straightforward translation. I'll tell you what though \u2013 it made all the difference when I switched to this after trying desperately to get a Wasserstein GAN version to work.  I liked the theory of Wasserstein GANs but it just didn't pan out in practice.  But I'm in love with Self-Attention GANs. \n* Training structure inspired by (but not the same as) Progressive Growing of GANs (https://arxiv.org/abs/1710.10196).  The difference here is the number of layers remains constant \u2013 I just changed the size of the input progressively and adjusted learning rates to make sure that the transitions between sizes happened successfully.  It seems to have the same basic end result \u2013 training is faster, more stable, and generalizes better. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9936856621137384,
        0.9904177327077459,
        0.9941470719758249,
        0.9770187220497442,
        0.9112450195224988
      ],
      "excerpt": "* Generator Loss* is two parts:  One is a basic Perceptual Loss (or Feature Loss) based on VGG16 \u2013 this just biases the generator model to replicate the input image.  The second is the loss score from the critic.  For the curious \u2013 Perceptual Loss isn't sufficient by itself to produce good results.  It tends to just encourage a bunch of brown/green/blue \u2013 you know, cheating to the test, basically, which neural networks are really good at doing!  Key thing to realize here is that GANs essentially are learning the loss function for you \u2013 which is really one big step closer to toward the ideal that we're shooting for in machine learning.  And of course you generally get much better results when you get the machine to learn something you were previously hand coding.  That's certainly the case here. \nThe beauty of this model is that it should be generally useful for all sorts of image modification, and it should do it quite well.  What you're seeing above are the results of the colorization model, but that's just one component in a pipeline that I'm looking to develop here with the exact same model.  \nWhat I develop next with this model will be based on trying to solve the problem of making these old images look great, so the next item on the agenda for me is the \"defade\" model.  I've committed initial efforts on that and it's in the early stages of training as I write this.  Basically it's just training the same model to reconstruct images that augmented with ridiculous contrast/brightness adjustments, as a simulation of fading photos and photos taken with old/bad equipment. I've already seen some promising results on that as well: \nThis project is built around the wonderful Fast.AI library.  Unfortunately, it's the -old- version and I have yet to upgrade it to the new version.  (That's definitely [update 11/18/2018: maybe] on the agenda.)  So prereqs, in summary: \n* Old Fast.AI library (version 0.7) [UPDATE 11/18/2018] A forked version is now bundled with the project, for ease of deployment and independence from whatever happens to the old version from here on out. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9362534590188647
      ],
      "excerpt": "There's a render_factor variable that basically determines the quality of the rendered colors (but not the resolution of the output image).  The higher it is, the better, but you'll also need more GPU memory to accomodate this.  The max I've been able to have my GeForce 1080TI use is 42.  Lower the number if you get a CUDA_OUT_OF_MEMORY error.  You can customize this render_factor per image like this, overriding the default: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8693481550821248,
        0.8059964262540558
      ],
      "excerpt": "For older and low quality images in particular, this seems to improve the colorization pretty reliably.  In contrast, more detailed and higher quality images tend to do better with a higher render_factor. \nModel weight saves are also done automatically during the training runs by the GANTrainer \u2013 defaulting to saving every 1000 iterations (it's an expensive operation).  They're stored in the root training data folder you provide, and the name goes by the save_base_name you provide to the training schedule.  Weights are saved for each training size separately. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.953308434165043,
        0.8895019487847993
      ],
      "excerpt": "The \"GAN Schedules\" you'll see in the notebooks are probably the ugliest looking thing I've put in the code, but they're just my version of implementing progressive GAN training, suited to a Unet generator.  That's all that's going on there really. \nPretrained weights for the colorizer generator again are here (right click and download from this link). The DeFade stuff is still a work in progress so I'll try to get good weights for those up in a few days. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.904853392234943,
        0.9603287581781779
      ],
      "excerpt": "Getting the best images really boils down to the art of selection.  You'll mostly get good results the first go, but playing around with the render_factor a bit may make a difference.  Thus, I'd consider this tool at this point fit for the \"AI artist\" but not something I'd deploy as a general purpose tool for all consumers.  It's just not there yet.  \nThe model loves blue clothing.  Not quite sure what the answer is yet, but I'll be on the lookout for a solution! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8556468627864586,
        0.9106361350485954,
        0.9486251071337889
      ],
      "excerpt": "So first, this image should really help visualize what is going on under the hood. Notice the smallified square image in the center. \nThat small square center image is what the deep learning generator actually generates now.  Before I was just shrinking the images keeping the same aspect ratio.  It turns out, the model does better with squares- even if they're distorted in the process! \nNote that I tried other things like keeping the core image's aspect ratio the same and doing various types of padding to make a square (reflect, symmetric, 0, etc).  None of this worked as well.  Two reasons why I think this works.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9747998639473882,
        0.8963866909654904,
        0.8488529672271025,
        0.9617538155401434
      ],
      "excerpt": "Two- at smaller resolutions I think this is particularly significant- you're giving the model more real image to work with if you just stretch it as opposed to padding.  And padding wasn't something the model trained on anyway. \nIt turns out that the human eye doesn't perceive color (chrominance) with nearly as much sensitivity as it does intensity (luminance).  Hence, we can render the color part at much lower resolution compared to the desired target res. \nBefore, I was having the model render the image at the same size as the end result image that you saw. So you maxed out around 550px (maybe) because the GPU couldn't handle anymore.  Now?  Colors can be rendered at say a tiny 272x272 (as the image above), then the color part of the model output is simply resized and stretched to map over the much higher resolution original images's luminance portion (we already have that!). So the end result looks fantastic, because your eyes can't tell the difference with the color anyway! \nWith the above, we're now able to generate much more consistently good looking images, even at different color gpu rendering sizes.  Basically, you do generally get a better image if you have the model take up more memory with a bigger render.  BUT if you reduce that memory footprint even in half with having the model render a smaller image, the difference in image quality of the end result is often pretty negligible.  This effectively means the colorization is usable on a wide variety of machines now!  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9882891201730739,
        0.9723943570794862
      ],
      "excerpt": "Finally- With the above, I was finally able to narrow down a scheme to make it so that the hunt to find the best version of what the model can render is a lot less tedious.  Basically, it amounts to providing a render_factor (int) by the user and multiplying it by a base size multiplier of 16.  This, combined with the square rendering, plays well together.  It means that you get predictable behavior of rendering as you increase and decrease render_factor, without too many surprise glitches. \nIncrease render_factor: Get more details right.  Decrease:  Still looks good but might miss some details.  Simple!  So you're no longer going to deal with a clumsy sz factor.  Bonus:  The memory usage is consistent and predictable so you just have to figure out the render_factor that works for your gpu once and forget about it.  I'll probably try to make that render_factor determination automatic eventually but this should be a big improvement in the meantime. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alfagao/DeOldify/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Wed, 29 Dec 2021 07:38:08 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alfagao/DeOldify/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "alfagao/DeOldify",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/alfagao/DeOldify/master/DeOldify_colab.ipynb",
      "https://raw.githubusercontent.com/alfagao/DeOldify/master/DeFadeVisualization.ipynb",
      "https://raw.githubusercontent.com/alfagao/DeOldify/master/ColorizeTraining.ipynb",
      "https://raw.githubusercontent.com/alfagao/DeOldify/master/DeFadeTraining.ipynb",
      "https://raw.githubusercontent.com/alfagao/DeOldify/master/ColorizeVisualization.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/alfagao/DeOldify/master/fastai/models/cifar10/main.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You should now be able to do a simple install with Anaconda. Here are the steps:\n\nOpen the command line and navigate to the root folder you wish to install.  Then type the following commands \n```console\ngit clone https://github.com/jantic/DeOldify.git DeOldify\ncd DeOldify\nconda env create -f environment.yml\n```\nThen start running with these commands:\n```console\nsource activate deoldify\njupyter lab\n```\n\nFrom there you can start running the notebooks in Jupyter Lab, via the url they provide you in the console.  \n\n**Disclaimer**: This conda install process is new- I did test it locally but the classic developer's excuse is \"well it works on my machine!\" I'm keeping that in mind- there's a good chance it doesn't necessarily work on others's machines!  I probably, most definitely did something wrong here.  Definitely, in fact.  Please let me know via opening an issue. Pobody's nerfect.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8805254996503489
      ],
      "excerpt": "Get more updates on Twitter <img src=\"result_images/Twitter_Social_Icon_Rounded_Square_Color.svg\" width=\"16\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365,
        0.9255456894846156,
        0.9955367594158723
      ],
      "excerpt": "* Python 3.6 \n* Pytorch 0.4.1 (needs spectral_norm, so  latest stable release is needed). https://pytorch.org/get-started/locally/ \n* Jupyter Lab conda install -c conda-forge jupyterlab \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222541766871409
      ],
      "excerpt": "i.e. You don't need a GeForce 1080TI to do it anymore.  You can get by with much less. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8271114044126654
      ],
      "excerpt": "Get more updates on Twitter <img src=\"result_images/Twitter_Social_Icon_Rounded_Square_Color.svg\" width=\"16\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579
      ],
      "excerpt": "vis.plot_transformed_image(\"test_images/Chief.jpg\", render_factor=17) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alfagao/DeOldify/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Apache License, Version 2.0 Apache License Version 2.0, January 2004 http://www.apache.org/licenses/\\n\\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n1. Definitions.\\n\\n\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.\\n\\n\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.\\n\\n\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.\\n\\n\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\\n\\n\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\\n\\n\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).\\n\\n\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.\\n\\n\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.\\n\\n2. Grant of Copyright License.\\n\\nSubject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\\n\\n3. Grant of Patent License.\\n\\nSubject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.\\n\\n4. Redistribution.\\n\\nYou may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\\n\\nYou must give any other recipients of the Work or Derivative Works a copy of this License; and You must cause any modified files to carry prominent notices stating that You changed the files; and You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.\\n\\n5. Submission of Contributions.\\n\\nUnless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.\\n\\n6. Trademarks.\\n\\nThis License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.\\n\\n7. Disclaimer of Warranty.\\n\\nUnless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.\\n\\n8. Limitation of Liability.\\n\\nIn no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\\n\\n9. Accepting Warranty or Additional Liability.\\n\\nWhile redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.\\n\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeOldify",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeOldify",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "alfagao",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alfagao/DeOldify/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* **(Training Only) BEEFY Graphics card**.  I'd really like to have more memory than the 11 GB in my GeForce 1080TI (11GB).  You'll have a tough time with less.  The Unet and Critic are ridiculously large but honestly I just kept getting better results the bigger I made them.  \n* **(Colorization Alone) A decent graphics card**. You'll benefit from having more memory in a graphics card in terms of the quality of the output achievable by.  Now what the term \"decent\" means exactly...I'm going to say 6GB +.  I haven't tried it but in my head the math works....  \n* **Linux (or maybe Windows 10)**  I'm using Ubuntu 16.04, but nothing about this precludes Windows 10 support as far as I know.  I just haven't tested it and am not going to make it a priority for now.  \n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Wed, 29 Dec 2021 07:38:08 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "So that's the gist of this project \u2013 I'm looking to make old photos look reeeeaaally good with GANs, and more importantly, make the project *useful*.  And yes, I'm definitely interested in doing video, but first I need to sort out how to get this model under control with memory (it's a beast).  It'd be nice if the models didn't take two to three days to train on a 1080TI as well (typical of GANs, unfortunately). In the meantime though this is going to be my baby and I'll be actively updating and improving the code over the foreseeable future.  I'll try to make this as user-friendly as possible, but I'm sure there's going to be hiccups along the way.  \n\nOh and I swear I'll document the code properly...eventually.  Admittedly I'm *one of those* people who believes in \"self documenting code\" (LOL).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The easiest way to get started is to simply try out colorization here on Colab: https://colab.research.google.com/github/jantic/DeOldify/blob/master/DeOldify_colab.ipynb.  This was contributed by Matt Robinson, and it's simply awesome.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}