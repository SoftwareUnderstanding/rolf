{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1312.5602  \n2. https://www.nervanasys.com/demystifying-deep-reinforcement-learning  \n3. http://edersantana.github.io/articles/keras_rl  \n\n## Problem Statement\nThis project\u2019s goal is to use a reinforcement learning method called Q-learning to arrive at a policy that an autonomous driving agent can use to cross a simulated busy intersection without incident and in the minimum time possible. That is to say that our agent should always wait until a large enough gap in traffic appears before driving through it, and it should never miss such an opportunity when presented. Our agent should be neither too timid nor too brave.  \n\nThe environment world is 21 pixels high by 21 pixels wide. All cars, including the agent, are of length 3 pixels. At each time step, each car moves forward one pixel; the agent upwards, and the traffic leftwards.\n\nIn the environment, the agent is not required to stop if there happens to exist a gap in the traffic large enough, otherwise it must come to a stop until such a gap presents itself (that\u2019s this agent\u2019s *raison d\u2019\u00eatre*"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.996858854512695
      ],
      "excerpt": "Playing Atari with Deep Reinforcement Learning, Volodymyr Mnih, Koray Kavukcuoglu, David Silver, et al., https://arxiv.org/abs/1312.5602   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8125579118425675,
        0.9113092432499703
      ],
      "excerpt": "        check if agent at intersection \n        if at intersection: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8125579118425675,
        0.9113092432499703
      ],
      "excerpt": "        check if agent at intersection \n        if at intersection: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/drforester/Q-learning-Intersection-Crossing",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-09T02:10:43Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-09T02:11:58Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9614282004282145
      ],
      "excerpt": "Using a Deep Q Network to learn a policy to cross a busy intersection \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.983804238811254,
        0.9992732528857265
      ],
      "excerpt": "Reinforcement learning (RL) methods have been successfully employed to learn rules, called policies, to solve tasks such as navigate a maze, play poker, auto-pilot an RC helicopter, and even play video games better than humans<sup>(1)</sup>. This last feat was accomplished in 2013 by DeepMind who were quickly bought by Google. Their paper \u201cPlaying Atari with Deep Reinforcement Learning\u201d, describes how a computer can learn to play video games by taking the screen pixels and associated user actions (such as \u201cmove left\u201d, \u201cmove right\u201d, \u201cfire laser\u201d, etc.) as input and and receiving a reward when the game score increased. \nDeep reinforcement learning combines a Markovian decision process with a deep neural network to learn a policy. To better understand Deep RL, and particularly the theory of Q-learning and its uses with deep neural networks, I read Tambet Matiisen\u2019s brilliant Guest Post on the Nervana Systems website<sup>(2)</sup>. I say brilliant because even I could begin to understand the beauty and the elegance of the method. In looking for a Python implementation of this method that I could study, I found Eder Santana\u2019s post \u201cKeras plays catch\u201d<sup>(3)</sup>, which describes his solution to the task of learning a policy to catch a falling piece of fruit (represented by a single pixel) with a basket (represented by three adjacent pixels) that can be moved either left or right to get underneath. In his post, he provides a link to the code. It was that post and code which most inspired this project.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567734429542823,
        0.9968029537584643
      ],
      "excerpt": "To evaluate the learned model and the Q-value implementation, a python program was written which instantiates an environment and loads the trained model. This testing program presents the agent with thousands of randomly generated environments and records the following two evaluation metrics: \n1. Percentage of successful crossings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8667429616931109,
        0.9560187895509076
      ],
      "excerpt": "The project consists of three Python programs, one for training, one for testing, and a plotting utility. Pseudocode for the training file is as follows: \nfor e in epochs:     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9302287581602335
      ],
      "excerpt": "                chose the policy model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9549184055046787,
        0.8230949051708899
      ],
      "excerpt": "        update the model based on experience replay \nPseudocode for the training file is: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for t in trials: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Learn a policy to safely cross a busy intersection with Q-learning.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/drforester/Q-learning-Intersection-Crossing/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project\u2019s goal is to use a reinforcement learning method called Q-learning to arrive at a policy that an autonomous driving agent can use to cross a simulated busy intersection without incident and in the minimum time possible. That is to say that our agent should always wait until a large enough gap in traffic appears before driving through it, and it should never miss such an opportunity when presented. Our agent should be neither too timid nor too brave.  \n\nThe environment world is 21 pixels high by 21 pixels wide. All cars, including the agent, are of length 3 pixels. At each time step, each car moves forward one pixel; the agent upwards, and the traffic leftwards.\n\nIn the environment, the agent is not required to stop if there happens to exist a gap in the traffic large enough, otherwise it must come to a stop until such a gap presents itself (that\u2019s this agent\u2019s *raison d\u2019\u00eatre*). Since vehicles advance by one pixel per time step, we can say that, in the case of a single lane, a gap in traffic must be at least 3 pixels wide in order for the agent to \u201csqueeze through\u201d.  \n\nThe environment is instantiated in the following manner:  \nenv = Drive(grid_dims)  \nwhere grid_dims is a tuple such as (21,21) representing the pixel height and width of the simulated world. Next the environment is set to initial conditions with:\nenv.reset()  \nTraffic is placed on the road with randomly chosen gap lengths. Gaps must be at least one pixel wide. In practice they are usually between one and four, though they can sometimes be several pixels wider. The following call moves the traffic leftward one pixel:\nenv.propagate_horz()  \nThe agent then determines if it is at the intersection with the call:\nenv.at_intersection()\nIf it is not at the intersection, then the only valid action is \u201cforward\u201d. Otherwise, the agent must decide whether to go forward or remain. When the action is determined, the agent action is implemented by:\nenv.propagate_vert(action)\nThe action input to this method is what we must learn.\n\nThe Experience Replay functionality, and the reasons for using it is explained in reference 2 in this way:\n\u201cWhen training the network, random minibatches from the replay memory are used instead of the most recent transition. This breaks the similarity of subsequent training samples, which otherwise might drive the network into a local minimum. Also experience replay makes the training task more similar to usual supervised learning, which simplifies debugging and testing the algorithm.\u201d\nThe experience replay implementation proposed here is almost identical to that Python class used in reference 3. Only one minor change is necessary to make it work with the output of the network described above.  \nThe following software have been used:  \n* Python 3.6  \n* Numpy  \n* Scipy (namely: scipy.ndimage.interpolation.shift)  \n* Matplotlib  \n* Keras  \n* TensorFlow  \n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 15:46:44 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/drforester/Q-learning-Intersection-Crossing/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "drforester/Q-learning-Intersection-Crossing",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8976338241993806
      ],
      "excerpt": "    instantiate environment as env \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342
      ],
      "excerpt": "    instantiate environment \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8117542813833672
      ],
      "excerpt": "Pseudocode for the training file is: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/drforester/Q-learning-Intersection-Crossing/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Q-learning Collision Avoidance",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Q-learning-Intersection-Crossing",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "drforester",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/drforester/Q-learning-Intersection-Crossing/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Training:  \n`$ python3 qlearn_crossing.py`    \nuse the --help or -h flag to see available options.\n\nTesting the trained model:  \n`$ python3 test_drive.py`  \nuse the --help or -h flag to see available options.\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 15:46:44 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "q-learning",
      "python",
      "deep-learning",
      "reinforcement-learning"
    ],
    "technique": "GitHub API"
  }
}