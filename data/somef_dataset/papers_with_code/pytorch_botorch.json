{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1312.6114",
      "https://arxiv.org/abs/1910.06403"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use BoTorch, please cite the following paper:\n> [M. Balandat, B. Karrer, D. R. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy. BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization. Advances in Neural Information Processing Systems 33, 2020.](https://arxiv.org/abs/1910.06403)\n\n```\n@inproceedings{balandat2020botorch,\n  title={{BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization}},\n  author={Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},\n  booktitle = {Advances in Neural Information Processing Systems 33},\n  year={2020},\n  url = {http://arxiv.org/abs/1910.06403}\n}\n```\n\nSee [here](https://botorch.org/docs/papers) for an incomplete selection of peer-reviewed papers that build off of BoTorch.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{balandat2020botorch,\n  title={{BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization}},\n  author={Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},\n  booktitle = {Advances in Neural Information Processing Systems 33},\n  year={2020},\n  url = {http://arxiv.org/abs/1910.06403}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/pytorch/botorch/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pytorch/botorch",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to BoTorch\nWe want to make contributing to BoTorch is as easy and transparent as possible.\nDevelopment installation\nTo get the development installation with all the necessary dependencies for\nlinting, testing, and building the documentation, run the following:\nbash\ngit clone https://github.com/pytorch/botorch.git\ncd botorch\npip install -e .[dev]\nOur Development Process\nCode Style\nBoTorch uses the black code formatter to\nenforce a common code style across the code base. black is installed easily via\npip using pip install black, and run locally by calling\nbash\nblack .\nfrom the repository root. No additional configuration should be needed (see the\nblack documentation\nfor advanced usage).\nImport Sorting\nBoTorch uses [ufmt]https://github.com/omnilib/ufmt library for consistent\nsorting of imports across the codebase. Install via pip install ufmt, and\nauto-sort with\nbash\nufmt format .\nfrom the repository root.\nWe feel strongly that having a consistent code style and imports is important,\nso CI will fail on your PR if it does not pass ufmt muster (note: under the\nhood ufmt also checks black code style).\nType Hints\nBoTorch is fully typed using python 3.7+\ntype hints.\nWe expect any contributions to also use proper type annotations. While we\ncurrently do not enforce full consistency of these in our continuous integration\ntest, you should strive to type check your code locally. For this we recommend\nusing pyre.\nUnit Tests\nTo run the unit tests, you can either use pytest (if installed):\nbash\npytest -ra\nor python's unittest:\nbash\npython -m unittest\nTo get coverage reports we recommend using the pytest-cov plugin:\nbash\npytest -ra --cov=. --cov-report term-missing\nDocumentation\nBoTorch's website is also open source, and is part of this very repository (the\ncode can be found in the website folder).\nIt is built using Docusaurus, and consists of three\nmain elements:\n\nThe documentation in Docusaurus itself (if you know Markdown, you can\n   already contribute!). This lives in the docs.\nThe API reference, auto-generated from the docstrings using\n   Sphinx, and embedded into the Docusaurus website.\n   The sphinx .rst source files for this live in sphinx/source.\nThe Jupyter notebook tutorials, parsed by nbconvert, and embedded into the\n   Docusaurus website. These live in tutorials.\n\nTo build the documentation you will need Node >= 8.x\nand Yarn >= 1.5.\nThe following command will both build the docs and serve the site locally:\nbash\n./scripts/build_docs.sh\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from main.\nIf you have added code that should be tested, add unit tests.\n   In other words, add unit tests.\nIf you have changed APIs, update the documentation. Make sure the\n   documentation builds.\nEnsure the test suite passes.\nMake sure your code passes both black and flake8 formatting checks.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects. You can\ncomplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nFacebook has a bounty program for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\nLicense\nBy contributing to BoTorch, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-30T23:59:57Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T13:59:43Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.908925214220865,
        0.927080656332795
      ],
      "excerpt": "  and optimizers. \n* Harnesses the power of PyTorch, including auto-differentiation, native support \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8217114417017425,
        0.8007814419832356,
        0.9157724916126384,
        0.8241156942213204,
        0.9496247606146396
      ],
      "excerpt": "* Supports Monte Carlo-based acquisition functions via the \n  reparameterization trick, which makes it \n  straightforward to implement new ideas without having to impose restrictive \n  assumptions about the underlying model. \n* Enables seamless integration with deep and/or convolutional architectures in PyTorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151560375203842,
        0.9785394479072214,
        0.8002846852996502,
        0.8857276069110936
      ],
      "excerpt": "  Processes (GPs) deep kernel learning, deep GPs, and approximate inference. \nThe primary audience for hands-on use of BoTorch are researchers and \nsophisticated practitioners in Bayesian Optimization and AI. \nWe recommend using BoTorch as a low-level API for implementing new algorithms \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9610318650292968,
        0.9831266363468735
      ],
      "excerpt": "for end-users, which at the same time is flexible enough for Bayesian \nOptimization researchers to plug into for handling of feature transformations, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8626136958835042
      ],
      "excerpt": "We recommend that end-users who are not actively doing research on Bayesian \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Bayesian optimization in PyTorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pytorch/botorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 235,
      "date": "Thu, 30 Dec 2021 01:21:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pytorch/botorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch/botorch",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/pytorch/botorch/tree/main/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/multi_fidelity_bo.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/composite_bo_with_hogp.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/risk_averse_bo_with_input_perturbations.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/constraint_active_search.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/multi_objective_bo.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/batch_mode_cross_validation.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/composite_mtbo.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/compare_mc_analytic_acquisition.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/bo_with_warped_gp.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/vae_mnist.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/turbo_1.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/preference_bo.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/custom_acquisition.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/fit_model_with_torch_optimizer.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/meta_learning_with_rgpe.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/optimize_with_cmaes.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/one_shot_kg.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/thompson_sampling.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/max_value_entropy.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/discrete_multi_fidelity_bo.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/custom_botorch_model_in_ax.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/constrained_multi_objective_bo.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/optimize_stochastic.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/risk_averse_bo_with_environmental_variables.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/GIBBON_for_efficient_batch_entropy_search.ipynb",
      "https://raw.githubusercontent.com/pytorch/botorch/main/tutorials/closed_loop_botorch_only.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pytorch/botorch/main/scripts/build_and_verify_py_packages.sh",
      "https://raw.githubusercontent.com/pytorch/botorch/main/scripts/test_packaging.sh",
      "https://raw.githubusercontent.com/pytorch/botorch/main/scripts/build_docs.sh",
      "https://raw.githubusercontent.com/pytorch/botorch/main/scripts/build_and_verify_conda_package.sh",
      "https://raw.githubusercontent.com/pytorch/botorch/main/scripts/publish_site.sh",
      "https://raw.githubusercontent.com/pytorch/botorch/main/.conda/build_conda.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you would like to try our bleeding edge features (and don't mind potentially\nrunning into the occasional bug here or there), you can install the latest\ndevelopment version directly from GitHub (this will also require installing\nthe current GPyTorch development version):\n```bash\npip install --upgrade git+https://github.com/cornellius-gp/gpytorch.git\npip install --upgrade git+https://github.com/pytorch/botorch.git\n```\n\n**Manual / Dev install**\n\nAlternatively, you can do a manual install. For a basic install, run:\n```bash\ngit clone https://github.com/pytorch/botorch.git\ncd botorch\npip install -e .\n```\n\nTo customize the installation, you can also run the following variants of the\nabove:\n* `pip install -e .[dev]`: Also installs all tools necessary for development\n  (testing, linting, docs building; see [Contributing](#contributing) below).\n* `pip install -e .[tutorials]`: Also installs all packages necessary for running the tutorial notebooks.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The latest release of BoTorch is easily installed either via\n[Anaconda](https://www.anaconda.com/distribution/#download-section) (recommended):\n```bash\nconda install botorch -c pytorch -c gpytorch\n```\nor via `pip`:\n```bash\npip install botorch\n```\n\nYou can customize your PyTorch installation (i.e. CUDA version, CPU only option)\nby following the [PyTorch installation instructions](https://pytorch.org/get-started/locally/).\n\n***Important note for MacOS users:***\n* Make sure your PyTorch build is linked against MKL (the non-optimized version\n  of BoTorch can be up to an order of magnitude slower in some settings).\n  Setting this up manually on MacOS can be tricky - to ensure this works properly,\n  please follow the [PyTorch installation instructions](https://pytorch.org/get-started/locally/).\n* If you need CUDA on MacOS, you will need to build PyTorch from source. Please\n  consult the PyTorch installation instructions above.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "**Installation Requirements**\n- Python >= 3.7\n- PyTorch >= 1.9\n- gpytorch >= 1.6\n- scipy\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pytorch/botorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "CSS",
      "JavaScript",
      "Shell",
      "Batchfile",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018-present, Facebook, Inc.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of\\nthis software and associated documentation files (the \"Software\"), to deal in\\nthe Software without restriction, including without limitation the rights to\\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\\nof the Software, and to permit persons to whom the Software is furnished to do\\nso, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "### Why BoTorch ?",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "botorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pytorch/botorch/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "#### Compatibility\r\n* Require PyTorch >=1.9 (#1011).\r\n* Require GPyTorch >=1.6 (#1011).\r\n\r\n#### New Features\r\n* New `ApproximateGPyTorchModel` wrapper for various (variational) approximate GP models (#1012).\r\n* New `SingleTaskVariationalGP` stochastic variational Gaussian Process model (#1012).\r\n* Support for Multi-Output Risk Measures (#906, #965).\r\n* Introduce `ModelList` and `PosteriorList` (#829).\r\n* New Constraint Active Search tutorial (#1010).\r\n* Add additional multi-objective optimization test problems (#958).\r\n\r\n#### Other Changes\r\n* Add `covar_module` as an optional input of `MultiTaskGP` models (#941).\r\n* Add `min_range` argument to `Normalize` transform to prevent division by zero (#931).\r\n* Add initialization heuristic for acquisition function optimization that samples around best points (#987).\r\n* Update initialization heuristic to perturb a subset of the dimensions of the best points if the dimension is > 20 (#988).\r\n* Modify `apply_constraints` utility to work with multi-output objectives (#994).\r\n* Short-cut `t_batch_mode_transform` decorator on non-tensor inputs (#991).\r\n\r\n#### Performance Improvements\r\n* Use lazy covariance matrix in `BatchedMultiOutputGPyTorchModel.posterior` (#976).\r\n* Fast low-rank Cholesky updates for `qNoisyExpectedHypervolumeImprovement` (#747, #995, #996).\r\n\r\n#### Bug Fixes\r\n* Update error handling to new PyTorch linear algebra messages (#940).\r\n* Avoid test failures on Ampere devices (#944).\r\n* Fixes to the `Griewank` test function (#972).\r\n* Handle empty base_sample_shape in `Posterior.rsample` (#986).\r\n* Handle `NotPSDError` and hitting `maxiter` in `fit_gpytorch_model` (#1007).\r\n* Use TransformedPosterior for subclasses of GPyTorchPosterior (#983).\r\n* Propagate `best_f` argument to `qProbabilityOfImprovement` in input constructors (f5a5f8b6dc20413e67c6234e31783ac340797a8d)",
        "dateCreated": "2021-12-08T23:59:14Z",
        "datePublished": "2021-12-09T00:16:14Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.6.0",
        "name": "Approximate GP model, Multi-Output Risk Measures, Bug Fixes and Performance Improvements",
        "tag_name": "v0.6.0",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.6.0",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/54914535",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.6.0"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "#### Compatibility\r\n* Require GPyTorch >=1.5.1 (#928).\r\n\r\n#### New Features\r\n* Add `HigherOrderGP` composite Bayesian Optimization tutorial notebook (#864).\r\n* Add Multi-Task Bayesian Optimization tutorial (#867).\r\n* New multi-objective test problems from (#876).\r\n* Add `PenalizedMCObjective` and `L1PenaltyObjective` (#913).\r\n* Add a `ProximalAcquisitionFunction` for regularizing new candidates towards previously generated ones (#919, #924).\r\n* Add a `Power` outcome transform (#925).\r\n\r\n#### Bug Fixes\r\n* Batch mode fix for `HigherOrderGP` initialization (#856).\r\n* Improve `CategoricalKernel` precision (#857).\r\n* Fix an issue with `qMultiFidelityKnowledgeGradient.evaluate` (#858).\r\n* Fix an issue with transforms with `HigherOrderGP`. (#889)\r\n* Fix initial candidate generation when parameter constraints are on different device (#897).\r\n* Fix bad in-place op in `_generate_unfixed_lin_constraints` (#901).\r\n* Fix an input transform bug in `fantasize` call (#902).\r\n* Fix outcome transform bug in `batched_to_model_list` (#917).\r\n\r\n#### Other Changes\r\n* Make variance optional for `TransformedPosterior.mean` (#855).\r\n* Support transforms in `DeterministicModel` (#869).\r\n* Support `batch_shape` in `RandomFourierFeatures` (#877).\r\n* Add a `maximize` flag to `PosteriorMean` (#881).\r\n* Ignore categorical dimensions when validating training inputs in `MixedSingleTaskGP` (#882).\r\n* Refactor `HigherOrderGPPosterior` for memory efficiency (#883).\r\n* Support negative weights for minimization objectives in `get_chebyshev_scalarization` (#884).\r\n* Move `train_inputs` transforms to `model.train/eval` calls (#894).",
        "dateCreated": "2021-09-02T20:02:58Z",
        "datePublished": "2021-09-02T20:44:09Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.5.1",
        "name": "Maintenance Release + New Tutorials",
        "tag_name": "v0.5.1",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.5.1",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/48922559",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.5.1"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "#### Compatibility\r\n* Require PyTorch >=1.8.1 (#832).\r\n* Require GPyTorch >=1.5 (#848).\r\n* Changes to how input transforms are applied: `transform_inputs` is applied in `model.forward` if the model is in `train` mode, otherwise it is applied in the `posterior` call (#819, #835).\r\n\r\n#### New Features\r\n* Improved multi-objective optimization capabilities:\r\n  * `qNoisyExpectedHypervolumeImprovement` acquisition function that improves on `qExpectedHypervolumeImprovement` in terms of tolerating observation noise and speeding up computation for large `q`-batches (#797, #822).\r\n  * `qMultiObjectiveMaxValueEntropy` acqusition function (913aa0e510dde10568c2b4b911124cdd626f6905, #760).\r\n  * Heuristic for reference point selection (#830).\r\n  * `FastNondominatedPartitioning` for Hypervolume computations (#699).\r\n  * `DominatedPartitioning` for partitioning the dominated space (#726).\r\n  * `BoxDecompositionList` for handling box decompositions of varying sizes (#712).\r\n  * Direct, batched dominated partitioning for the two-outcome case (#739).\r\n  * `get_default_partitioning_alpha` utility providing heuristic for selecting approximation level for partitioning algorithms (#793).\r\n  * New method for computing Pareto Frontiers with less memory overhead (#842, #846).\r\n* New `qLowerBoundMaxValueEntropy` acquisition function (a.k.a. GIBBON), a lightweight variant of Multi-fidelity Max-Value Entropy Search using a Determinantal Point Process approximation (#724, #737, #749).\r\n* Support for discrete and mixed input domains:\r\n  * `CategoricalKernel` for categorical inputs (#771).\r\n  * `MixedSingleTaskGP` for mixed search spaces (containing both categorical and ordinal parameters) (#772, #847).\r\n  * `optimize_acqf_discrete` for optimizing acquisition functions over fully discrete domains (#777).\r\n  * Extend `optimize_acqf_mixed` to allow batch optimization (#804).\r\n* Support for robust / risk-aware optimization:\r\n  * Risk measures for robust / risk-averse optimization (#821).\r\n  * `AppendFeatures` transform (#820).\r\n  * `InputPerturbation` input transform for for risk averse BO with implementation errors (#827).\r\n  * Tutorial notebook for Bayesian Optimization of risk measures (#823).\r\n  * Tutorial notebook for risk-averse Bayesian Optimization under input perturbations (#828).\r\n* More scalable multi-task modeling and sampling:\r\n  * `KroneckerMultiTaskGP` model for efficient multi-task modeling for block-design settings (all tasks observed at all inputs) (#637).\r\n  * Support for transforms in Multi-Task GP models (#681).\r\n  * Posterior sampling based on Matheron's rule for Multi-Task GP models (#841).\r\n* Various changes to simplify and streamline integration with Ax:\r\n  * Handle non-block designs in `TrainingData` (#794).\r\n  * Acquisition function input constructor registry (#788, #802, #845).\r\n* Random Fourier Feature (RFF) utilties for fast (approximate) GP function sampling (#750).\r\n* `DelaunayPolytopeSampler` for fast uniform sampling from (simple) polytopes (#741).\r\n* Add `evaluate` method to `ScalarizedObjective` (#795).\r\n\r\n#### Bug Fixes\r\n* Handle the case when all features are fixed in `optimize_acqf` (#770).\r\n* Pass `fixed_features` to initial candidate generation functions (#806).\r\n* Handle batch empty pareto frontier in `FastPartitioning` (#740).\r\n* Handle empty pareto set in `is_non_dominated` (#743).\r\n* Handle edge case of no or a single observation in `get_chebyshev_scalarization` (#762).\r\n* Fix an issue in `gen_candidates_torch` that caused problems with acqusition functions using fantasy models (#766).\r\n* Fix `HigherOrderGP` `dtype` bug (#728).\r\n* Normalize before clamping in `Warp` input warping transform (#722).\r\n* Fix bug in GP sampling (#764).\r\n\r\n#### Other Changes\r\n* Modify input transforms to support one-to-many transforms (#819, #835).\r\n* Make initial conditions for acquisition function optimization honor parameter constraints (#752).\r\n* Perform optimization only over unfixed features if `fixed_features` is passed (#839).\r\n* Refactor Max Value Entropy Search Methods (#734).\r\n* Use Linear Algebra functions from the `torch.linalg` module (#735).\r\n* Use PyTorch's `Kumaraswamy` distribution (#746).\r\n* Improved capabilities and some bugfixes for batched models (#723, #767).\r\n* Pass `callback` argument to `scipy.optim.minimize` in `gen_candidates_scipy` (#744).\r\n* Modify behavior of `X_pending` in in multi-objective acqusiition functions (#747).\r\n* Allow multi-dimensional batch shapes in test functions (#757).\r\n* Utility for converting batched multi-output models into batched single-output models (#759).\r\n* Explicitly raise `NotPSDError` in `_scipy_objective_and_grad` (#787).\r\n* Make `raw_samples` optional if `batch_initial_conditions` is passed (#801).\r\n* Use powers of 2 in qMC docstrings & examples (#812).",
        "dateCreated": "2021-06-29T14:12:03Z",
        "datePublished": "2021-06-29T19:31:00Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.5.0",
        "name": "Improved Multi-Objective Optimization, Support for categorical/mixed domains, robust/risk-aware optimization, efficient MTGP sampling",
        "tag_name": "v0.5.0",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.5.0",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/45447083",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.5.0"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "#### Compatibility\r\n* Require PyTorch >=1.7.1 (#714).\r\n* Require GPyTorch >=1.4 (#714).\r\n\r\n#### New Features\r\n* `HigherOrderGP` - High-Order Gaussian Process (HOGP) model for\r\n  high-dimensional output regression (#631, #646, #648, #680).\r\n* `qMultiStepLookahead` acquisition function for general look-ahead\r\n  optimization approaches (#611, #659).\r\n* `ScalarizedPosteriorMean` and `project_to_sample_points` for more\r\n  advanced MFKG functionality (#645).\r\n* Large-scale Thompson sampling tutorial (#654, #713).\r\n* Tutorial for optimizing mixed continuous/discrete domains (application\r\n  to multi-fidelity KG with discrete fidelities) (#716).\r\n* `GPDraw` utility for sampling from (exact) GP priors (#655).\r\n* Add `X` as optional arg to call signature of `MCAcqusitionObjective` (#487).\r\n* `OSY` synthetic test problem (#679).\r\n\r\n#### Bug Fixes\r\n* Fix matrix multiplication in `scalarize_posterior` (#638).\r\n* Set `X_pending` in `get_acquisition_function` in `qEHVI` (#662).\r\n* Make contextual kernel device-aware (#666).\r\n* Do not use an `MCSampler` in `MaxPosteriorSampling` (#701).\r\n* Add ability to subset outcome transforms (#711).\r\n\r\n#### Performance Improvements\r\n* Batchify box decomposition for 2d case (#642).\r\n\r\n#### Other Changes\r\n* Use scipy distribution in MES quantile bisect (#633).\r\n* Use new closure definition for GPyTorch priors (#634).\r\n* Allow enabling of approximate root decomposition in `posterior` calls (#652).\r\n* Support for upcoming 21201-dimensional PyTorch `SobolEngine` (#672, #674).\r\n* Refactored various MOO utilities to allow future additions (#656, #657, #658, #661).\r\n* Support input_transform in PairwiseGP (#632).\r\n* Output shape checks for t_batch_mode_transform (#577).\r\n* Check for NaN in `gen_candidates_scipy` (#688).\r\n* Introduce `base_sample_shape` property to `Posterior` objects (#718).",
        "dateCreated": "2021-02-23T21:04:39Z",
        "datePublished": "2021-02-23T21:33:59Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.4.0",
        "name": "High Order GP model, multi-step look-ahead acquisition function",
        "tag_name": "v0.4.0",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.4.0",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/38513082",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.4.0"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "#### Compatibility\r\n* Require PyTorch >=1.7 (#614).\r\n* Require GPyTorch >=1.3 (#614).\r\n\r\n#### New Features\r\n* Models (LCE-A, LCE-M and SAC ) for Contextual Bayesian Optimziation (#581).\r\n   * Implements core models from:\r\n     [High-Dimensional Contextual Policy Search with Unknown Context Rewards using Bayesian Optimization](https://proceedings.neurips.cc/paper/2020/hash/faff959d885ec0ecf70741a846c34d1d-Abstract.html).\r\n      Q. Feng, B. Letham, H. Mao, E. Bakshy. NeurIPS 2020.\r\n    * See Ax for usage of these models.\r\n* Hit and run sampler for uniform sampling from a polytope (#592).\r\n* Input warping:\r\n  * Core functionality (#607).\r\n  * Kumaraswamy Distribution (#606).\r\n  * Tutorial (8f34871652042219c57b799669a679aab5eed7e3).\r\n* TuRBO-1 tutorial (#598).\r\n  * Implements the method from:\r\n     [Scalable Global Optimization via Local Bayesian Optimization](https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf).\r\n    D. Eriksson, M. Pearce, J. Gardner, R. D. Turner, M. Poloczek. NeurIPS 2019.\r\n\r\n#### Bug fixes\r\n* Fix bounds of `HolderTable` synthetic function (#596).\r\n* Fix `device` issue in MOO tutorial (#621).\r\n\r\n#### Other changes\r\n* Add `train_inputs` option to `qMaxValueEntropy` (#593).\r\n* Enable gpytorch settings to override BoTorch defaults for `fast_pred_var` and `debug` (#595).\r\n* Rename `set_train_data_transform` -> `preprocess_transform` (#575).\r\n* Modify `_expand_bounds()` shape checks to work with >2-dim bounds (#604).\r\n* Add `batch_shape` property to models (#588).\r\n* Modify `qMultiFidelityKnowledgeGradient.evaluate()` to work with `project`, `expand` and `cost_aware_utility` (#594).\r\n* Add list of papers using BoTorch to website docs (#617).",
        "dateCreated": "2020-12-08T06:07:30Z",
        "datePublished": "2020-12-08T06:42:21Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.3.3",
        "name": "Contextual Bayesian Optimization, Input Warping, TuRBO, sampling from polytopes.",
        "tag_name": "v0.3.3",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.3.3",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/34966006",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.3.3"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "#### New Features\r\n* Add `PenalizedAcquisitionFunction` wrapper (#585)\r\n* Input transforms\r\n  * Reversible input transform (#550)\r\n  * Rounding input transform (#562)\r\n  * Log input transform (#563)\r\n* Differentiable approximate rounding for integers (#561)\r\n\r\n#### Bug fixes\r\n* Fix sign error in UCB when `maximize=False` (a4bfacbfb2109d3b89107d171d2101e1995822bb)\r\n* Fix batch_range sample shape logic (#574)\r\n\r\n#### Other changes\r\n* Better support for two stage sampling in preference learning\r\n  (0cd13d0cb49b1ac8d0971e42f1f0e9dd6126fd9a)\r\n* Remove noise term in `PairwiseGP` and add `ScaleKernel` by default (#571)\r\n* Rename `prior` to `task_covar_prior` in `MultiTaskGP` and `FixedNoiseMultiTaskGP`\r\n  (16573fea066d8bb682dc68526f42b6ec7c22a555)\r\n* Support only transforming inputs on training or evaluation (#551)\r\n* Add `equals` method for `InputTransform` (#552)",
        "dateCreated": "2020-10-24T21:14:04Z",
        "datePublished": "2020-10-26T04:28:04Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.3.2",
        "name": "Maintenance Release",
        "tag_name": "v0.3.2",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.3.2",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/33040511",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.3.2"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "#### New Features\r\n* Constrained Multi-Objective tutorial (#493)\r\n* Multi-fidelity Knowledge Gradient tutorial (#509)\r\n* Support for batch qMC sampling (#510)\r\n* New `evaluate` method for `qKnowledgeGradient` (#515)\r\n\r\n#### Compatibility\r\n* Require PyTorch >=1.6 (#535)\r\n* Require GPyTorch >=1.2 (#535)\r\n* Remove deprecated `botorch.gen module` (#532)\r\n\r\n#### Bug fixes\r\n* Fix bad backward-indexing of task_feature in `MultiTaskGP` (#485)\r\n* Fix bounds in constrained Branin-Currin test function (#491)\r\n* Fix max_hv for C2DTLZ2 and make Hypervolume always return a float (#494)\r\n* Fix bug in `draw_sobol_samples` that did not use the proper effective dimension (#505)\r\n* Fix constraints for `q>1` in `qExpectedHypervolumeImprovement` (c80c4fdb0f83f0e4f12e4ec4090d0478b1a8b532)\r\n* Only use feasible observations in partitioning for `qExpectedHypervolumeImprovement`\r\n  in `get_acquisition_function` (#523)\r\n* Improved GPU compatibility for `PairwiseGP` (#537)\r\n\r\n#### Performance Improvements\r\n* Reduce memory footprint in `qExpectedHypervolumeImprovement` (#522)\r\n* Add `(q)ExpectedHypervolumeImprovement` to nonnegative functions\r\n  [for better initialization] (#496)\r\n\r\n#### Other changes\r\n* Support batched `best_f` in `qExpectedImprovement` (#487)\r\n* Allow to return full tree of solutions in `OneShotAcquisitionFunction` (#488)\r\n* Added `construct_inputs` class method to models to programmatically construct the\r\n  inputs to the constructor from a standardized `TrainingData` representation\r\n  (#477, #482, 3621198d02195b723195b043e86738cd5c3b8e40)\r\n* Acquisition function constructors now accept catch-all `**kwargs` options\r\n  (#478, e5b69352954bb10df19a59efe9221a72932bfe6c)\r\n* Use `psd_safe_cholesky` in `qMaxValueEntropy` for better numerical stabilty (#518)\r\n* Added `WeightedMCMultiOutputObjective` (81d91fd2e115774e561c8282b724457233b6d49f)\r\n* Add ability to specify `outcomes` to all multi-output objectives (#524)\r\n* Return optimization output in `info_dict` for `fit_gpytorch_scipy` (#534)\r\n* Use `setuptools_scm` for versioning (#539)",
        "dateCreated": "2020-09-16T00:56:28Z",
        "datePublished": "2020-09-16T01:58:56Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.3.1",
        "name": "Maintenance Release",
        "tag_name": "v0.3.1",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.3.1",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/31367316",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.3.1"
      },
      {
        "authorType": "User",
        "author_name": "sdaulton",
        "body": "#### New Features\r\n* Multi-Objective Acquisition Functions (#466)\r\n  * q-Expected Hypervolume Improvement\r\n  * q-ParEGO\r\n  * Analytic Expected Hypervolume Improvement with auto-differentiation\r\n* Multi-Objective Utilities (#466)\r\n  * Pareto Computation\r\n  * Hypervolume Calculation\r\n  * Box Decomposition algorithm\r\n* Multi-Objective Test Functions (#466)\r\n  * Suite of synthetic test functions for multi-objective, constrained\r\n  optimzation\r\n* Multi-Objective Tutorial (#468)\r\n* Abstract ConstrainedBaseTestProblem (#454)\r\n* Add optimize_acqf_list method for sequentially, greedily optimizing 1 candidate\r\nfrom each provided acquisition function (d10aec911b241b208c59c192beb9e4d572a092cd)\r\n\r\n\r\n#### Bug fixes\r\n* Fixed re-arranging mean in MultiTask multi-output models (#450).\r\n\r\n#### Other changes\r\n* Move gpt_posterior_settings into models.utils (#449)\r\n* Allow specifications of batch dims to collapse in samplers (#457)\r\n* Remove outcome transform before model-fitting for sequential model fitting\r\nin multi-output models (#458)",
        "dateCreated": "2020-07-06T20:40:41Z",
        "datePublished": "2020-07-06T21:31:51Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.3.0",
        "name": "Multi-Objective Bayesian Optimization",
        "tag_name": "v0.3.0",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.3.0",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/28275293",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "#### Bug fixes\r\n* Fixed issue with broken wheel build (#444).\r\n\r\n#### Other changes\r\n* Changed code style to use absolute imports throughout (#443).",
        "dateCreated": "2020-05-14T16:55:22Z",
        "datePublished": "2020-05-14T18:12:05Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.2.5",
        "name": "Bugfix Release",
        "tag_name": "v0.2.5",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.2.5",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/26523185",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.2.5"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "#### Bug fixes\r\n* There was a mysterious issue with the 0.2.3 wheel on pypi, where part of the `botorch/optim/utils.py` file was not included, which resulted in an `ImportError` for   many central components of the code. Interestingly, the source dist (built with the same command) did not have this issue.\r\n* Preserve order in ChainedOutcomeTransform (#440).\r\n\r\n#### New Features\r\n* Utilities for estimating the feasible volume under outcome constraints (#437).",
        "dateCreated": "2020-05-13T06:20:44Z",
        "datePublished": "2020-05-13T17:08:27Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.2.4",
        "name": "Bugfix Release",
        "tag_name": "v0.2.4",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.2.4",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/26472504",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.2.4"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "Introduces a new Pairwise GP model for Preference Learning with pair-wise preferential feedback, as well as a Sampling Strategies abstraction for generating candidates from a discrete candidate set.\r\n\r\n\r\n#### Compatibility\r\n* Require PyTorch >=1.5 (#423).\r\n* Require GPyTorch >=1.1.1 (#425).\r\n\r\n#### New Features\r\n* Add `PairwiseGP` for preference learning with pair-wise comparison data (#388).\r\n* Add `SamplingStrategy` abstraction for sampling-based generation strategies, including\r\n  `MaxPosteriorSampling` (i.e. Thompson Sampling) and `BoltzmannSampling` (#218, #407).\r\n\r\n#### Deprecations\r\n* The existing `botorch.gen` module is moved to `botorch.generation.gen` and imports\r\n  from `botorch.gen` will raise a warning (an error in the next release) (#218).\r\n\r\n#### Bug fixes\r\n* Fix & update a number of tutorials (#394, #398, #393, #399, #403).\r\n* Fix CUDA tests (#404).\r\n* Fix sobol maxdim limitation in `prune_baseline` (#419).\r\n\r\n#### Other changes\r\n* Better stopping criteria for stochastic optimization (#392).\r\n* Improve numerical stability of `LinearTruncatedFidelityKernel` (#409).\r\n* Allow batched `best_f` in `qExpectedImprovement` and `qProbabilityOfImprovement`\r\n  (#411).\r\n* Introduce new logger framework (#412).\r\n* Faster indexing in some situations (#414).\r\n* More generic `BaseTestProblem` (9e604fe2188ac85294c143d249872415c4d95823).\r\n",
        "dateCreated": "2020-04-26T17:36:30Z",
        "datePublished": "2020-04-26T18:34:00Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.2.3",
        "name": "Pairwise GP for Preference Learning, Sampling Strategies",
        "tag_name": "v0.2.3",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.2.3",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/25898660",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.2.3"
      },
      {
        "authorType": "User",
        "author_name": "danielrjiang",
        "body": "Require Python 3.7 and adds new features for active learning and multi-fidelity optimization, along with a number of bug fixes.\r\n\r\n#### Compatibility\r\n* Require PyTorch >=1.4 (#379).\r\n* Require Python >=3.7 (#378).\r\n\r\n#### New Features\r\n* Add `qNegIntegratedPosteriorVariance` for Bayesian active learning (#377).\r\n* Add `FixedNoiseMultiFidelityGP`, analogous to `SingleTaskMultiFidelityGP` (#386).\r\n* Support `scalarize_posterior` for m>1 and q>1 posteriors (#374).\r\n* Support `subset_output` method on multi-fidelity models (#372).\r\n* Add utilities for sampling from simplex and hypersphere (#369).\r\n\r\n#### Bug fixes\r\n* Fix `TestLoader` local test discovery (#376).\r\n* Fix batch-list conversion of `SingleTaskMultiFidelityGP` (#370).\r\n* Validate tensor args before checking input scaling for more\r\n  informative error messaages (#368).\r\n* Fix flaky `qNoisyExpectedImprovement` test (#362).\r\n* Fix test function in closed-loop tutorial (#360).\r\n* Fix num_output attribute in BoTorch/Ax tutorial (#355).\r\n\r\n#### Other changes\r\n* Require output dimension in `MultiTaskGP` (#383).\r\n* Update code of conduct (#380).\r\n* Remove deprecated `joint_optimize` and `sequential_optimize` (#363).",
        "dateCreated": "2020-03-08T09:09:08Z",
        "datePublished": "2020-03-09T16:58:51Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.2.2",
        "name": "Require Python 3.7 and new features",
        "tag_name": "v0.2.2",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.2.2",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/24356726",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.2.2"
      },
      {
        "authorType": "User",
        "author_name": "sdaulton",
        "body": "Minor bug fix release.\r\n\r\n#### New Features\r\n* Add a static method for getting batch shapes for batched MO models (#346).\r\n\r\n#### Bug fixes\r\n* Revamp qKG constructor to avoid issue with missing objective (#351).\r\n* Make sure MVES can support sampled costs like KG (#352).\r\n\r\n#### Other changes\r\n* Allow custom module-to-array handling in fit_gpytorch_scipy (#341).",
        "dateCreated": "2020-01-16T04:32:41Z",
        "datePublished": "2020-01-16T19:37:05Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.2.1",
        "name": "Compatibility Release",
        "tag_name": "v0.2.1",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.2.1",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/22905482",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.2.1"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "This release adds the popular Max-value Entropy Search (MES) acquisition function, as well as support for multi-fidelity Bayesian optimization via both the Knowledge Gradient (KG) and MES.\r\n\r\n#### Compatibility\r\n* Require PyTorch >=1.3.1 (#313).\r\n* Require GPyTorch >=1.0 (#342).\r\n\r\n#### New Features\r\n* Add cost-aware KnowledgeGradient (`qMultiFidelityKnowledgeGradient`) for multi-fidelity optimization (#292).\r\n* Add `qMaxValueEntropy` and `qMultiFidelityMaxValueEntropy` max-value entropy search acquisition functions (#298).\r\n* Add `subset_output` functionality to (most) models (#324).\r\n* Add outcome transforms and input transforms (#321).\r\n* Add `outcome_transform` kwarg to model constructors for automatic outcome transformation and un-transformation (#327).\r\n* Add cost-aware utilities for cost-sensitive acquisiiton functions (#289).\r\n* Add `DeterminsticModel` and `DetermisticPosterior` abstractions (#288).\r\n* Add `AffineFidelityCostModel` (f838eacb4258f570c3086d7cbd9aa3cf9ce67904).\r\n* Add `project_to_target_fidelity` and `expand_trace_observations` utilities for use in multi-fidelity optimization (1ca12ac0736e39939fff650cae617680c1a16933).\r\n\r\n#### Performance Improvements\r\n* New `prune_baseline` option for pruning `X_baseline` in  `qNoisyExpectedImprovement` (#287).\r\n* Do not use approximate MLL computation for deterministic fitting (#314).\r\n* Avoid re-evaluating the acquisition function in `gen_candidates_torch` (#319).\r\n* Use CPU where possible in `gen_batch_initial_conditions` to avoid memory issues on the GPU (#323).\r\n\r\n#### Bug fixes\r\n* Properly register `NoiseModelAddedLossTerm` in `HeteroskedasticSingleTaskGP` (671c93a203b03ef03592ce322209fc5e71f23a74).\r\n* Fix batch mode for `MultiTaskGPyTorchModel` (#316).\r\n* Honor `propagate_grads` argument in `fantasize` of `FixedNoiseGP` (#303).\r\n* Properly handle `diag` arg in `LinearTruncatedFidelityKernel` (#320).\r\n\r\n#### Other changes\r\n* Consolidate and simplify multi-fidelity models (#308).\r\n* New license header style (#309).\r\n* Validate shape of `best_f` in `qExpectedImprovement` (#299).\r\n* Support specifying observation noise explicitly for all models (#256).\r\n* Add `num_outputs` property to the `Model` API (#330).\r\n* Validate output shape of models upon instantiating acquisition functions (#331).\r\n\r\n#### Tests\r\n* Silence warnings outside of explicit tests (#290).\r\n* Enforce full sphinx docs coverage in CI (#294).",
        "dateCreated": "2019-12-20T21:08:33Z",
        "datePublished": "2019-12-20T21:57:05Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.2.0",
        "name": "Max-value entropy search, multi-fidelity (cost-aware) optimization",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.2.0",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/22401182",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "#### Breaking Changes\r\n* Require explicit output dimensions in BoTorch models (#238)\r\n* Make `joint_optimize` / `sequential_optimize` return acquisition function\r\n  values (#149) [note deprecation notice below]\r\n* `standardize` now works on the second to last dimension (#263)\r\n* Refactor synthetic test functions (#273)\r\n\r\n#### New Features\r\n* Add `qKnowledgeGradient` acquisition function (#272, #276)\r\n* Add input scaling check to standard models (#267)\r\n* Add `cyclic_optimize`, convergence criterion class (#269)\r\n* Add `settings.debug` context manager (#242)\r\n\r\n#### Deprecations\r\n* Consolidate `sequential_optimize` and `joint_optimize` into `optimize_acqf`\r\n  (#150)\r\n\r\n#### Bug fixes\r\n* Properly pass noise levels to GPs using a `FixedNoiseGaussianLikelihood` (#241)\r\n  [requires gpytorch > 0.3.5]\r\n* Fix q-batch dimension issue in `ConstrainedExpectedImprovement`\r\n  (6c067185f56d3a244c4093393b8a97388fb1c0b3)\r\n* Fix parameter constraint issues on GPU (#260)\r\n\r\n#### Minor changes\r\n* Add decorator for concatenating pending points (#240)\r\n* Draw independent sample from prior for each hyperparameter (#244)\r\n* Allow `dim > 1111` for `gen_batch_initial_conditions` (#249)\r\n* Allow `optimize_acqf` to use `q>1` for `AnalyticAcquisitionFunction` (#257)\r\n* Allow excluding parameters in fit functions (#259)\r\n* Track the final iteration objective value in `fit_gpytorch_scipy` (#258)\r\n* Error out on unexpected dims in parameter constraint generation (#270)\r\n* Compute acquisition values in gen_ functions w/o grad (#274)\r\n\r\n#### Tests\r\n* Introduce BotorchTestCase to simplify test code (#243)\r\n* Refactor tests to have monolithic cuda tests (#261)",
        "dateCreated": "2019-10-02T00:53:40Z",
        "datePublished": "2019-10-02T00:59:55Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.1.4",
        "name": "Knowledge Gradient (one-shot), various maintenance",
        "tag_name": "v0.1.4",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.1.4",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/20401161",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.1.4"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "\r\n#### Compatibility\r\n* Updates to support breaking changes in PyTorch to boolean masks and tensor\r\n  comparisons (#224).\r\n* Require PyTorch >=1.2 (#225).\r\n* Require GPyTorch >=0.3.5 (itself a compatibility release).\r\n#### New Features\r\n* Add `FixedFeatureAcquisitionFunction` wrapper that simplifies optimizing\r\n  acquisition functions over a subset of input features (#219).\r\n* Add `ScalarizedObjective` for scalarizing posteriors (#210).\r\n* Change default optimization behavior to use L-BFGS-B by for box constraints\r\n  (#207).\r\n\r\n#### Bug fixes\r\n* Add validation to candidate generation (#213), making sure constraints are\r\n  strictly satisfied (rater than just up to numerical accuracy of the optimizer).\r\n\r\n#### Minor changes\r\n* Introduce `AcquisitionObjective` base class (#220).\r\n* Add propagate_grads context manager, replacing the `propagate_grads` kwarg in\r\n  model `posterior()` calls (#221)\r\n* Add `batch_initial_conditions` argument to `joint_optimize()` for\r\n  warm-starting the optimization (ec3365a37ed02319e0d2bb9bea03aee89b7d9caa).\r\n* Add `return_best_only` argument to `joint_optimize()` (#216). Useful for\r\n  implementing advanced warm-starting procedures.",
        "dateCreated": "2019-08-10T23:19:31Z",
        "datePublished": "2019-08-10T23:27:33Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.1.3",
        "name": "Compatibility and Maintenance release",
        "tag_name": "v0.1.3",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.1.3",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/19211580",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.1.3"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "#### Bug fixes\r\n* Avoid [PyTorch bug](https://github.com/pytorch/pytorch/issues/22353) resulting in bad gradients on GPU by requiring GPyTorch >= 0.3.4\r\n* Fixes to resampling behavior in MCSamplers (#204)\r\n\r\n#### Experimental Features\r\n* Linear truncated kernel for multi-fidelity bayesian optimization (#192)\r\n* SingleTaskMultiFidelityGP for GP models that have fidelity parameters (#181)",
        "dateCreated": "2019-07-10T03:11:16Z",
        "datePublished": "2019-07-10T03:17:01Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.1.2",
        "name": "Maintenance release",
        "tag_name": "v0.1.2",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.1.2",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/18512699",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.1.2"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "\r\n#### Breaking changes\r\n* rename `botorch.qmc` to `botorch.sampling`, move MC samplers from\r\n  `acquisition.sampler` to `botorch.sampling.samplers` (#172)\r\n\r\n#### New Features\r\n* Add `condition_on_observations` and `fantasize` to the Model level API (#173)\r\n* Support pending observations generically for all `MCAcqusitionFunctions` (#176)\r\n* Add fidelity kernel for training iterations/training data points (#178)\r\n* Support for optimization constraints across `q`-batches (to support things like\r\n  sample budget constraints) (2a95a6c3f80e751d5cf8bc7240ca9f5b1529ec5b)\r\n* Add ModelList <-> Batched Model converter (#187)\r\n* New test functions\r\n    * basic: `neg_ackley`, `cosine8`, `neg_levy`, `neg_rosenbrock`, `neg_shekel`\r\n      (e26dc7576c7bf5fa2ba4cb8fbcf45849b95d324b)\r\n    * for multi-fidelity BO: `neg_aug_branin`, `neg_aug_hartmann6`,\r\n      `neg_aug_rosenbrock` (ec4aca744f65ca19847dc368f9fee4cc297533da)\r\n\r\n#### Improved functionality:\r\n* More robust model fitting\r\n    * Catch gpytorch numerical issues and return `NaN` to the optimizer (#184)\r\n    * Restart optimization upon failure by sampling hyperparameters from their prior (#188)\r\n    * Sequentially fit batched and `ModelListGP` models by default (#189)\r\n    * Change minimum inferred noise level (e2c64fef1e76d526a33951c5eb75ac38d5581257)\r\n* Introduce optional batch limit in `joint_optimize` to increases scalability of\r\n  parallel optimization (baab5786e8eaec02d37a511df04442471c632f8a)\r\n* Change constructor of `ModelListGP` to comply with GPyTorch\u2019s `IndependentModelList`\r\n  constructor (a6cf739e769c75319a67c7525a023ece8806b15d)\r\n* Use `torch.random` to set default seed for samplers (rather than `random`) to\r\n  making sampling reproducible when setting `torch.manual_seed`\r\n  (ae507ad97255d35f02c878f50ba68a2e27017815)\r\n\r\n####  Performance Improvements\r\n* Use `einsum` in `LinearMCObjective` (22ca29535717cda0fcf7493a43bdf3dda324c22d)\r\n* Change default Sobol sample size for `MCAquisitionFunctions` to be base-2 for\r\n  better MC integration performance (5d8e81866a23d6bfe4158f8c9b30ea14dd82e032)\r\n* Add ability to fit models in `SumMarginalLogLikelihood` sequentially (and make\r\n  that the default setting) (#183)\r\n* Do not construct the full covariance matrix when computing posterior of\r\n  single-output BatchedMultiOutputGPyTorchModel (#185)\r\n\r\n#### Bug fixes\r\n* Properly handle observation_noise kwarg for BatchedMultiOutputGPyTorchModels (#182)\r\n* Fix a issue where `f_best` was always max for NoisyExpectedImprovement\r\n  (410de585f07de0c66427d5066947e22227d11537)\r\n* Fix bug and numerical issues in `initialize_q_batch`\r\n  (844dcd1dc8f418ae42639e211c6bb8e31a75d8bf)\r\n* Fix numerical issues with `inv_transform` for qMC sampling (#162)\r\n\r\n#### Other\r\n* Bump GPyTorch minimum requirement to 0.3.3",
        "dateCreated": "2019-06-27T23:14:03Z",
        "datePublished": "2019-06-28T00:35:33Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.1.1",
        "name": "API updates, more robust model fitting",
        "tag_name": "v0.1.1",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.1.1",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/18281779",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.1.1"
      },
      {
        "authorType": "User",
        "author_name": "Balandat",
        "body": "First public release.",
        "dateCreated": "2019-05-01T01:00:36Z",
        "datePublished": "2019-05-01T01:31:53Z",
        "html_url": "https://github.com/pytorch/botorch/releases/tag/v0.1.0",
        "name": "Initial beta release",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/pytorch/botorch/tarball/v0.1.0",
        "url": "https://api.github.com/repos/pytorch/botorch/releases/17081545",
        "zipball_url": "https://api.github.com/repos/pytorch/botorch/zipball/v0.1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2154,
      "date": "Thu, 30 Dec 2021 01:21:04 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Here's a quick run down of the main components of a Bayesian optimization loop.\nFor more details see our [Documentation](https://botorch.org/docs/introduction) and the\n[Tutorials](https://botorch.org/tutorials).\n\n1. Fit a Gaussian Process model to data\n  ```python\n  import torch\n  from botorch.models import SingleTaskGP\n  from botorch.fit import fit_gpytorch_model\n  from gpytorch.mlls import ExactMarginalLogLikelihood\n\n  train_X = torch.rand(10, 2)\n  Y = 1 - (train_X - 0.5).norm(dim=-1, keepdim=True)  #: explicit output dimension\n  Y += 0.1 * torch.rand_like(Y)\n  train_Y = (Y - Y.mean()) / Y.std()\n\n  gp = SingleTaskGP(train_X, train_Y)\n  mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n  fit_gpytorch_model(mll)\n  ```\n\n2. Construct an acquisition function\n  ```python\n  from botorch.acquisition import UpperConfidenceBound\n\n  UCB = UpperConfidenceBound(gp, beta=0.1)\n  ```\n\n3. Optimize the acquisition function\n  ```python\n  from botorch.optim import optimize_acqf\n\n  bounds = torch.stack([torch.zeros(2), torch.ones(2)])\n  candidate, acq_value = optimize_acqf(\n      UCB, bounds=bounds, q=1, num_restarts=5, raw_samples=20,\n  )\n  ```\n\n\n",
      "technique": "Header extraction"
    }
  ]
}