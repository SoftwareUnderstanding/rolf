{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2006.04768"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@misc{wang2020linformer,\n    title={Linformer: Self-Attention with Linear Complexity},\n    author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},\n    year={2020},\n    eprint={2006.04768},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{wang2020linformer,\n    title={Linformer: Self-Attention with Linear Complexity},\n    author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},\n    year={2020},\n    eprint={2006.04768},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kuixu/Linear-Multihead-Attention",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-22T17:09:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-17T11:05:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9325242243517775,
        0.8621446403407702
      ],
      "excerpt": "This is an efficient implementation followed with the PyTorch official torch.nn.MultiheadAttention class and F.multi_head_attention_forward function.  \nThree additional argments defined in LinearMultiheadAttention: sequence length, the projected dimention k and the parameter sharing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Reproducing the Linear Multihead Attention introduced in Linformer paper  (Linformer: Self-Attention with Linear Complexity)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kuixu/Linear-Multihead-Attention/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10,
      "date": "Thu, 30 Dec 2021 03:07:22 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kuixu/Linear-Multihead-Attention/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kuixu/Linear-Multihead-Attention",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kuixu/Linear-Multihead-Attention/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Implementation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Linear-Multihead-Attention",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kuixu",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kuixu/Linear-Multihead-Attention/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 44,
      "date": "Thu, 30 Dec 2021 03:07:22 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "linformer",
      "linear-multihead-attention",
      "detr",
      "transformer",
      "attention-mechanism"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Examples of using `torch.nn.MultiheadAttention`:\n```\n>>> import torch\n>>> multihead_attn = torch.nn.MultiheadAttention(embed_dim, num_heads)\n>>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n```\n\nExamples of using `LinearMultiheadAttention`:\n```\n>>> from linear_multihead_attention import LinearMultiheadAttention\n>>> multihead_attn = LinearMultiheadAttention(embed_dim, num_heads) \n>>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n```\n\nExamples of using `LinearMultiheadAttention` with the sequence length of 512 and :\n```\n>>> from linear_multihead_attention import LinearMultiheadAttention\n>>> multihead_attn = LinearMultiheadAttention(embed_dim, num_heads, seq_len=512, proj_k=256, param_sharing='layerwise') \n>>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n```\n\nLinear-DETR: Replace `torch.nn.MultiheadAttention` in [DETR](https://github.com/facebookresearch/detr) with `LinearMultiheadAttention` in three lines in `models/transformer.py`, it saved much more memory and space, hope to have a comparable performance:\n\n```\nfrom linear_multihead_attention import LinearMultiheadAttention\n\n#: TransformerEncoderLayer\n#: self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\nself.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, seq_len=w*h, proj_k=64) #: where w, h are from `bs, c, h, w = src.shape`\n\n\n#: TransformerDecoderLayer\n#: self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n#: self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\nself.self_attn = LinearMultiheadAttention(d_model, nhead, dropout=dropout, seq_len=num_queries, proj_k=64) #: where num_queries = args.num_queries\nself.multihead_attn = LinearMultiheadAttention(d_model, nhead, dropout=dropout, seq_len=w*h, proj_k=64) #: where w, h are from `bs, c, h, w = src.shape`\n\n```\n\n\n",
      "technique": "Header extraction"
    }
  ]
}