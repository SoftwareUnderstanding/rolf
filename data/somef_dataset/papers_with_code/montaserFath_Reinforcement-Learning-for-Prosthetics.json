{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1901.04772v1",
      "https://arxiv.org/abs/1509.02971",
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1509.02971",
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1509.02971",
      "https://arxiv.org/abs/1509.02971",
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1509.02971",
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1509.02971",
      "https://arxiv.org/abs/1707.06347"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- T. Garikayi, D. van den Heever and S. Matope, (2016), Robotic prosthetic challenges for clinical applications, IEEE International Conference on Control and Robotics Engineering (ICCRE), Singapore, 2016, pp. 1-5. doi: 10.1109/ICCRE.2016.7476146\n\n- Joshi, Girish \\& Chowdhary, Girish. (2018). Cross-Domain Transfer in Reinforcement Learning using Target Apprentice.\n\n- Lillicrap, Timothy \\& J. Hunt, Jonathan \\& Pritzel, Alexander \\& Heess, Nicolas \\& Erez, Tom \\& Tassa, Yuval \\& Silver, David \\& Wierstra, Daan. (2015). Continuous control with deep reinforcement learning. CoRR.\n\n- Attia, Alexandre \\& Dayan, Sharone. (2018). Global overview of Imitation Learning.\n\n- Cheng, Qiao \\& Wang, Xiangke \\& Shen, Lincheng. (2017). An Autonomous Inter-task Mapping Learning Method via Artificial Neural Network for Transfer Learning. 10.1109/ROBIO.2017.8324510.\n- J.J. Zhu, DAgger algorithm implementation, (2017), GitHub repository, https://github.com/jj-zhu/jadagger.\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9615125853857841,
        0.9972153637433058,
        0.9423888532349464,
        0.9070406720435389
      ],
      "excerpt": "Best Graduation project at University of khartoum. 2018. Khartoum, Sudan \nPoster paper at NeurIPS 2018 Black in AI workshop, Montreal, Canada \nPoster at Deep Learning Indaba 2018, South Africa \nBenchmarking RL algorithms: Deterministic Policy Gradient DDPG, Trust Region Policy Optimization TRPO and Proximal Policy Optimization PPO algorithms. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/montaserFath/Reinforcement-Learning-for-Prosthetics",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-18T09:04:21Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-01T06:14:53Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9675118150030717
      ],
      "excerpt": "In this repo, we are trying to apply Reinforcement Learning (RL) to enable prosthetics to calibrate with differences between humans and differences between walking environments. using OpenSim to simulate prosthetic. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8832592966436976
      ],
      "excerpt": "Best Graduation project at University of khartoum. 2018. Khartoum, Sudan \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8502278917260802
      ],
      "excerpt": "Modificat DAgger algorithm to balance between exploration and exploiting. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9580312961713875,
        0.908925214220865
      ],
      "excerpt": "Center of mass: the agent observes the position, velocity, and acceleration. \nMuscles activation, lenght and velocity \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9798893453728177,
        0.9842859841631114,
        0.9858838796777528,
        0.9408508766096731,
        0.9897021449513516
      ],
      "excerpt": "Where the <img src=\"https://latex.codecogs.com/gif.latex?V_{t}\"/> is the horizontal velocity vector of the pelvi which is function of all state variables. \nThe termination condition for the episode is filling 300 steps or the height of the pelvis falling below 0.6 meters \nDDPG is a model-free, off-policy actor-critic algorithm using deep function approximators that can learn policies in high-dimensional, continuous action spaces.DDPG is based on the deterministic policy gradient (DPG) algorithm. it combines the actor-critic approach with insights from the recent success of Deep Q Network (DQN). \nPPO is a policy optimization method that use multiple epochs of stochastic gradient ascent to perform each policy update. \nTRPO is a model free, on-policy optimization method that effective for optimizing large nonlinear policies such as neural networks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.929207205163826,
        0.8084121200120246,
        0.9538232554961016,
        0.8613845908404685,
        0.9241711883918061,
        0.9008145956318928,
        0.8481993166426711,
        0.8684886019646468,
        0.8345316386959539,
        0.829650072447378,
        0.9601540494129367
      ],
      "excerpt": "OpenSim ProstheticsEnv is a very complex environment, it contains more than 158 continuous state variables and 19 continuous action variables. \nRL algorithms take a long time to build a complex policy which has the ability to compute all state variables and select action variables which will maximize the reward. \nDDPG algorithm achieves good reward because it designed for high dimensions continuous space environments and it uses the replay buffer. \nPPO the least training time comparing to DDPG and TRPO because PPO uses gradient algorithm approximation instance of the conjugate gradient algorithm. \nTRPO algorithm achieved the maximum Reward because it takes time to reach the \u201ctrusted\u201d region so it slower than DDPG and PPO . \nThe prosthetic model can not walk for large distances. \nEach experiment runs for one time, So we are planing to Repeat each experiment number of times with different random seeds and take the average and variance. \nWe used same hyperparameters for all algorithm to make benchmarking between algorithms, we need to select the best hyperparameters for each algorithm and environment. \nWe benchmarcked three RL algorithms only and from one library(ChainerRL). So we are planing to use different implementations. \nWe transfer learning between similar agents. \nChainerRL is a deep reinforcement learning library that implements various state-of-the-art deep reinforcement algorithms in Python using Chainer, a flexible deep learning framework. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Apply Reinforcement Learning (RL) to enable prosthetics to calibrate with differences between humans and differences between walking environments",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/montaserFath/Reinforcement-Learning-for-Prosthetics/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Fri, 24 Dec 2021 18:06:12 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/montaserFath/Reinforcement-Learning-for-Prosthetics/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "montaserFath/Reinforcement-Learning-for-Prosthetics",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/montaserFath/Reinforcement-Learning-for-Prosthetics/master/PPO/PPO_Prosthetic.ipynb",
      "https://raw.githubusercontent.com/montaserFath/Reinforcement-Learning-for-Prosthetics/master/Imitation%20Learning/DDPG/DDPG_ValueFunction_Dagger.ipynb",
      "https://raw.githubusercontent.com/montaserFath/Reinforcement-Learning-for-Prosthetics/master/Imitation%20Learning/DDPG/DDPG_Dagger.ipynb",
      "https://raw.githubusercontent.com/montaserFath/Reinforcement-Learning-for-Prosthetics/master/Imitation%20Learning/DDPG/DDPG_ImmediateReward_Dagger.ipynb",
      "https://raw.githubusercontent.com/montaserFath/Reinforcement-Learning-for-Prosthetics/master/Imitation%20Learning/DDPG/DDPG_Epsilon_Dagger-.ipynb",
      "https://raw.githubusercontent.com/montaserFath/Reinforcement-Learning-for-Prosthetics/master/TRPO/TRPO_prosthetics.ipynb",
      "https://raw.githubusercontent.com/montaserFath/Reinforcement-Learning-for-Prosthetics/master/RL/DDPG/DDPG_BiPedalWalker.ipynb",
      "https://raw.githubusercontent.com/montaserFath/Reinforcement-Learning-for-Prosthetics/master/RL/DDPG/DDPG_Prosthetic.ipynb",
      "https://raw.githubusercontent.com/montaserFath/Reinforcement-Learning-for-Prosthetics/master/RL/DDPG/DDPG_Pendulum.ipynb",
      "https://raw.githubusercontent.com/montaserFath/Reinforcement-Learning-for-Prosthetics/master/RL/DDPG/DDPG_CartPole.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Install OpenSim Envirnment \n```\nconda create -n opensim-rl -c kidzik opensim python=3.6.1\nsource activate opensim-rl\n```\nInstall ChainerRL libary\n```\npip install chainerrl\n```\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/montaserFath/Reinforcement-Learning-for-Prosthetics/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Montaser Mohammedalamen\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "NeurIPS2018-Challenge-RL-for-Prosthetics",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Reinforcement-Learning-for-Prosthetics",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "montaserFath",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/montaserFath/Reinforcement-Learning-for-Prosthetics/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 18:06:12 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning",
      "transfer-learning",
      "opensim",
      "chainerrl",
      "chainerrl-libary",
      "imitation-learning",
      "prosthetics",
      "ppo",
      "trpo",
      "ddpg-algorithm",
      "neurips-2018",
      "jupyter-notebook"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- **Random Actions**\n\n![Random](https://github.com/montaserFath/NeurIPS2018-Challenge-RL-for-Prosthetics/blob/master/Demos/Random_prothetics.gif)\n\n- **[TRPO](http://proceedings.mlr.press/v37/schulman15.pdf)**\n\n![TRPO](https://github.com/montaserFath/NeurIPS2018-Challenge-RL-for-Prosthetics/blob/master/Demos/TRPO_prothetics.gif)\n\n- **[PPO](https://arxiv.org/abs/1707.06347)**\n\n![PPO](https://github.com/montaserFath/NeurIPS2018-Challenge-RL-for-Prosthetics/blob/master/Demos/PPO_prothetics.gif)\n\n- **[DDPG](https://arxiv.org/abs/1509.02971)**\n\n![DDPG](https://github.com/montaserFath/NeurIPS2018-Challenge-RL-for-Prosthetics/blob/master/Demos/DDPG_prothetics.gif)\n",
      "technique": "Header extraction"
    }
  ]
}