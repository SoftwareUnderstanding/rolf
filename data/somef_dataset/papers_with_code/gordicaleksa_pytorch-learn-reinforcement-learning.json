{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I found these resources useful while developing this project, sorted (approximately) by usefulness:\n\n* [Stable Baselines 3 DQN](https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/dqn/dqn.py)\n* [PyTorch reimplementation of Berkley's DQN](https://github.com/transedward/pytorch-dqn) and [Berkley's DQN](https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3)\n* [pytorch-dqn](https://github.com/jacobaustin123/pytorch-dqn/blob/master/dqn.py)\n* [RL adventures DQN](https://github.com/higgsfield/RL-Adventure/blob/master/1.dqn.ipynb) and [minimal DQN](https://github.com/econti/minimal_dqn/blob/master/main.py)\n* [Pytorch tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this code useful, please cite the following:\n\n```\n@misc{Gordi\u01072021PyTorchLearnReinforcementLearning,\n  author = {Gordi\u0107, Aleksa},\n  title = {pytorch-learn-reinforcement-learning},\n  year = {2021},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/gordicaleksa/pytorch-learn-reinforcement-learning}},\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{Gordi\u01072021PyTorchLearnReinforcementLearning,\n  author = {Gordi\u0107, Aleksa},\n  title = {pytorch-learn-reinforcement-learning},\n  year = {2021},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/gordicaleksa/pytorch-learn-reinforcement-learning}},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8502644800268612
      ],
      "excerpt": "and pasting the http://localhost:6006/ URL into your browser. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629545450213182
      ],
      "excerpt": "1) Debug DQN and achieve the published results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8302556419090275
      ],
      "excerpt": "<a href=\"https://www.youtube.com/watch?v=H1NRNGiS8YU\" target=\"_blank\"><img src=\"https://img.youtube.com/vi/H1NRNGiS8YU/0.jpg\"  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gordicaleksa/pytorch-learn-reinforcement-learning",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-06T09:16:13Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T03:09:56Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9181846987525246,
        0.8600817520135626
      ],
      "excerpt": "It's aimed at making it easy to start playing and learning about RL. <br/> \nThe problem I came across investigating other DQN projects is that they either: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9621440572647062
      ],
      "excerpt": "Visualization and debugging tools \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9645239312013374
      ],
      "excerpt": "This was the project that started the revolution in the RL world - deep Q-network (:link: Mnih et al.), <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625428745377981,
        0.9848847733119127
      ],
      "excerpt": "DQN model learned to play 29 Atari games (out of 49 they it tested on) on a super-human/comparable-to-humans level. \nHere is the schematic of it's CNN architecture: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9383563987168299,
        0.9708402529888407,
        0.8061451147260812
      ],
      "excerpt": "The same architecture was used for all of the 49 games - although the model has to be retrained, from scratch, every single time. \nSince it takes lots of compute and time to train all of the 49 models I'll consider the DQN project completed once \nI succeed in achieving the published results on: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8007135469303818
      ],
      "excerpt": "* I'm also experiencing slowdowns - so any PRs that would improve/explain the perf are welcome! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8052169922503828
      ],
      "excerpt": "Important note: please follow the coding guidelines of this repo before you submit a PR so that we can minimize \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "<img src=\"data/readme_gifs/BreakoutNoFrameskip-v4_1.gif\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8420872506814755,
        0.9270346424323012,
        0.8105244364577425,
        0.9892213950683741
      ],
      "excerpt": "* --learning_rate - DQN originally used RMSProp, I saw that Adam with 1e-4 worked for stable baselines 3 \n* --grad_clipping_value - there was a lot of noise in the gradients so I used this to control it \n* Try using RMSProp (I haven't yet). Adam was an improvement over RMSProp so I doubt it's causing the issues \nLess important settings for getting DQN to work: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8467404579499141,
        0.9259389056766604
      ],
      "excerpt": "and pasting the http://localhost:6006/ URL into your browser. \nI'm currently visualizing the Huber loss (and you can see there is something weird going on): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9070513136004444
      ],
      "excerpt": "Rewards and steps taken per episode (there is a fair bit of correlation between these 2): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9863729308461474
      ],
      "excerpt": "And gradient L2 norms of weights and biases of every CNN/FC layer as well as the complete grad vector: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8736455227111498
      ],
      "excerpt": "As well as epsilon (from the epsilon-greedy algorithm) but that plot is not that informative so I'll omit it here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8739618056541844,
        0.8621711242291025,
        0.88819724897159
      ],
      "excerpt": "and that's what I'm trying to debug atm. \nTo enter the debug mode add the --debug flag to your console or IDE's list of script arguments. \nIt'll visualize the current state that's being fed into the RL agent.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9395481308863488
      ],
      "excerpt": "But mostly all of the 4 frames will be in there: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151554204224076
      ],
      "excerpt": "Here are some videos I made on RL which may help you to better understand how DQN and other RL algorithms work: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8167598149337844
      ],
      "excerpt": "And some other ones: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "* DeepMind: AlphaGo Zero and AlphaZero \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A collection of various RL algorithms like policy gradients, DQN and PPO. The goal of this repo will be to make it a go-to resource for learning about RL. How to visualize, debug and solve RL problems. I've additionally included playground.py for learning more about OpenAI gym, etc.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gordicaleksa/pytorch-learn-reinforcement-learning/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14,
      "date": "Mon, 27 Dec 2021 06:15:09 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gordicaleksa/pytorch-learn-reinforcement-learning/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "gordicaleksa/pytorch-learn-reinforcement-learning",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Let's get this thing running! Follow the next steps:\n\n1. `git clone https://github.com/gordicaleksa/pytorch-learn-reinforcement-learning`\n2. Open Anaconda console and navigate into project directory `cd path_to_repo`\n3. Run `conda env create` from project directory (this will create a brand new conda environment).\n4. Run `activate pytorch-rl-env` (for running scripts from your console or setup the interpreter in your IDE)\n\nIf you're on Windows you'll additionally need to install this:\n`pip install https://github.com/Kojoley/atari-py/releases atary_py` to install gym's Atari dependencies.\n\nOtherwise this should do it `pip install 'gym[atari]'`, if it's not working check out [this](https://stackoverflow.com/questions/49947555/openai-gym-trouble-installing-atari-dependency-mac-os-x) and [this](https://github.com/openai/gym/issues/1170).\n\nThat's it! It should work out-of-the-box executing environment.yml file which deals with dependencies. <br/>\n\n-----\n\nPyTorch pip package will come bundled with some version of CUDA/cuDNN with it,\nbut it is highly recommended that you install a system-wide CUDA beforehand, mostly because of the GPU drivers. \nI also recommend using Miniconda installer as a way to get conda on your system.\nFollow through points 1 and 2 of [this setup](https://github.com/Petlja/PSIML/blob/master/docs/MachineSetup.md)\nand use the most up-to-date versions of Miniconda and CUDA/cuDNN for your system.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8864574716181884
      ],
      "excerpt": "Hardware requirements \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8127610626047506
      ],
      "excerpt": "Important note: please follow the coding guidelines of this repo before you submit a PR so that we can minimize \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9170798757121159
      ],
      "excerpt": "* --env_id - depending on which game you want to train on (I'd focus on the easiest one for now - Breakout) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "Training DQN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9330184127168011
      ],
      "excerpt": "<img src=\"data/readme_pics/dqn.jpg\" width=\"800\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8925925717074311
      ],
      "excerpt": "<img src=\"data/readme_gifs/BreakoutNoFrameskip-v4_1.gif\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9127553531955228
      ],
      "excerpt": "To run with default settings just run python train_DQN_script.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.82449856633056,
        0.8210266215504783
      ],
      "excerpt": "The training script will: \n* Dump checkpoint .pth models into models/checkpoints/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8160731142741525
      ],
      "excerpt": "* Periodically write some training metadata to the console \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151343308272494
      ],
      "excerpt": "<img src=\"data/readme_visualizations/huber_loss.PNG\" width=\"400\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151343308272494,
        0.9151343308272494
      ],
      "excerpt": "<img src=\"data/readme_visualizations/rewards_per_episode.PNG\" width=\"400\"/> \n<img src=\"data/readme_visualizations/steps_per_episode.PNG\" width=\"400\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078564040380478
      ],
      "excerpt": "<img src=\"data/readme_visualizations/grads.PNG\" width=\"850\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9052924254802468
      ],
      "excerpt": "<img src=\"data/readme_visualizations/state_initial.PNG\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9052924254802468
      ],
      "excerpt": "<img src=\"data/readme_visualizations/state_all_frames.PNG\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9330184127168011,
        0.9330184127168011
      ],
      "excerpt": "<img src=\"data/readme_visualizations/pong.jpg\" width=\"240\"/> \n<img src=\"data/readme_visualizations/breakout.jpg\" width=\"240\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gordicaleksa/pytorch-learn-reinforcement-learning/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Reinforcement Learning (PyTorch) :robot: + :cake: = :heart:",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-learn-reinforcement-learning",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "gordicaleksa",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gordicaleksa/pytorch-learn-reinforcement-learning/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You'll need some decent hardware to train the DQN in reasonable time so that you can iterate fast:\n1) **16+ GB of RAM** (Replay Buffer takes around ~7 GBs of RAM).\n2) The faster your GPU is - the better! :sweat_smile: Having said that VRAM is not the bottleneck you'll need **2+ GB VRAM**.\n\nWith 16 GB RAM and RTX 2080 it takes ~5 days to train DQN on my machine - I'm **experiencing some slowdowns** which I\nhaven't debugged yet. Here is the FPS (frames-per-second) metric I'm logging:\n\n<p align=\"center\">\n<img src=\"data/readme_visualizations/fps_metric.PNG\"/>\n</p>\n\nThe shorter, green one is the current experiment I'm running, the red one took over 5 days to train.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 99,
      "date": "Mon, 27 Dec 2021 06:15:09 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "python",
      "deep-learning",
      "jupyter",
      "pytorch",
      "rl",
      "reinforcement-learning",
      "reinforcement-learning-algorithms",
      "ppo",
      "policy-gradient",
      "dqn",
      "pytorch-implementation",
      "deep-q-network",
      "pytorch-dqn",
      "pytorch-policy-gradient",
      "pytorch-ppo"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You just need to link the Python environment you created in the [setup](#setup) section.\n\n",
      "technique": "Header extraction"
    }
  ]
}