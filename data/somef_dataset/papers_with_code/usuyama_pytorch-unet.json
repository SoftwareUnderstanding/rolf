{
  "citation": [
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    print('-' * 10) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.90097832490285
      ],
      "excerpt": "        if phase == 'val' and epoch_loss &lt; best_loss: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=30, gamma=0.1) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/usuyama/pytorch-unet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-27T01:24:14Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T11:27:16Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = ResNetUNet(n_class=6)\nmodel = model.to(device)\n\n#: check keras-like model summary using torchsummary\nfrom torchsummary import summary\nsummary(model, input_size=(3, 224, 224))\n```\n\n    ----------------------------------------------------------------\n            Layer (type)               Output Shape         Param #\n    ================================================================\n                Conv2d-1         [-1, 64, 224, 224]           1,792\n                  ReLU-2         [-1, 64, 224, 224]               0\n                Conv2d-3         [-1, 64, 224, 224]          36,928\n                  ReLU-4         [-1, 64, 224, 224]               0\n                Conv2d-5         [-1, 64, 112, 112]           9,408\n           BatchNorm2d-6         [-1, 64, 112, 112]             128\n                  ReLU-7         [-1, 64, 112, 112]               0\n             MaxPool2d-8           [-1, 64, 56, 56]               0\n                Conv2d-9           [-1, 64, 56, 56]           4,096\n          BatchNorm2d-10           [-1, 64, 56, 56]             128\n                 ReLU-11           [-1, 64, 56, 56]               0\n               Conv2d-12           [-1, 64, 56, 56]          36,864\n          BatchNorm2d-13           [-1, 64, 56, 56]             128\n                 ReLU-14           [-1, 64, 56, 56]               0\n               Conv2d-15          [-1, 256, 56, 56]          16,384\n          BatchNorm2d-16          [-1, 256, 56, 56]             512\n               Conv2d-17          [-1, 256, 56, 56]          16,384\n          BatchNorm2d-18          [-1, 256, 56, 56]             512\n                 ReLU-19          [-1, 256, 56, 56]               0\n           Bottleneck-20          [-1, 256, 56, 56]               0\n               Conv2d-21           [-1, 64, 56, 56]          16,384\n          BatchNorm2d-22           [-1, 64, 56, 56]             128\n                 ReLU-23           [-1, 64, 56, 56]               0\n               Conv2d-24           [-1, 64, 56, 56]          36,864\n          BatchNorm2d-25           [-1, 64, 56, 56]             128\n                 ReLU-26           [-1, 64, 56, 56]               0\n               Conv2d-27          [-1, 256, 56, 56]          16,384\n          BatchNorm2d-28          [-1, 256, 56, 56]             512\n                 ReLU-29          [-1, 256, 56, 56]               0\n           Bottleneck-30          [-1, 256, 56, 56]               0\n               Conv2d-31           [-1, 64, 56, 56]          16,384\n          BatchNorm2d-32           [-1, 64, 56, 56]             128\n                 ReLU-33           [-1, 64, 56, 56]               0\n               Conv2d-34           [-1, 64, 56, 56]          36,864\n          BatchNorm2d-35           [-1, 64, 56, 56]             128\n                 ReLU-36           [-1, 64, 56, 56]               0\n               Conv2d-37          [-1, 256, 56, 56]          16,384\n          BatchNorm2d-38          [-1, 256, 56, 56]             512\n                 ReLU-39          [-1, 256, 56, 56]               0\n           Bottleneck-40          [-1, 256, 56, 56]               0\n               Conv2d-41          [-1, 128, 56, 56]          32,768\n          BatchNorm2d-42          [-1, 128, 56, 56]             256\n                 ReLU-43          [-1, 128, 56, 56]               0\n               Conv2d-44          [-1, 128, 28, 28]         147,456\n          BatchNorm2d-45          [-1, 128, 28, 28]             256\n                 ReLU-46          [-1, 128, 28, 28]               0\n               Conv2d-47          [-1, 512, 28, 28]          65,536\n          BatchNorm2d-48          [-1, 512, 28, 28]           1,024\n               Conv2d-49          [-1, 512, 28, 28]         131,072\n          BatchNorm2d-50          [-1, 512, 28, 28]           1,024\n                 ReLU-51          [-1, 512, 28, 28]               0\n           Bottleneck-52          [-1, 512, 28, 28]               0\n               Conv2d-53          [-1, 128, 28, 28]          65,536\n          BatchNorm2d-54          [-1, 128, 28, 28]             256\n                 ReLU-55          [-1, 128, 28, 28]               0\n               Conv2d-56          [-1, 128, 28, 28]         147,456\n          BatchNorm2d-57          [-1, 128, 28, 28]             256\n                 ReLU-58          [-1, 128, 28, 28]               0\n               Conv2d-59          [-1, 512, 28, 28]          65,536\n          BatchNorm2d-60          [-1, 512, 28, 28]           1,024\n                 ReLU-61          [-1, 512, 28, 28]               0\n           Bottleneck-62          [-1, 512, 28, 28]               0\n               Conv2d-63          [-1, 128, 28, 28]          65,536\n          BatchNorm2d-64          [-1, 128, 28, 28]             256\n                 ReLU-65          [-1, 128, 28, 28]               0\n               Conv2d-66          [-1, 128, 28, 28]         147,456\n          BatchNorm2d-67          [-1, 128, 28, 28]             256\n                 ReLU-68          [-1, 128, 28, 28]               0\n               Conv2d-69          [-1, 512, 28, 28]          65,536\n          BatchNorm2d-70          [-1, 512, 28, 28]           1,024\n                 ReLU-71          [-1, 512, 28, 28]               0\n           Bottleneck-72          [-1, 512, 28, 28]               0\n               Conv2d-73          [-1, 128, 28, 28]          65,536\n          BatchNorm2d-74          [-1, 128, 28, 28]             256\n                 ReLU-75          [-1, 128, 28, 28]               0\n               Conv2d-76          [-1, 128, 28, 28]         147,456\n          BatchNorm2d-77          [-1, 128, 28, 28]             256\n                 ReLU-78          [-1, 128, 28, 28]               0\n               Conv2d-79          [-1, 512, 28, 28]          65,536\n          BatchNorm2d-80          [-1, 512, 28, 28]           1,024\n                 ReLU-81          [-1, 512, 28, 28]               0\n           Bottleneck-82          [-1, 512, 28, 28]               0\n               Conv2d-83          [-1, 256, 28, 28]         131,072\n          BatchNorm2d-84          [-1, 256, 28, 28]             512\n                 ReLU-85          [-1, 256, 28, 28]               0\n               Conv2d-86          [-1, 256, 14, 14]         589,824\n          BatchNorm2d-87          [-1, 256, 14, 14]             512\n                 ReLU-88          [-1, 256, 14, 14]               0\n               Conv2d-89         [-1, 1024, 14, 14]         262,144\n          BatchNorm2d-90         [-1, 1024, 14, 14]           2,048\n               Conv2d-91         [-1, 1024, 14, 14]         524,288\n          BatchNorm2d-92         [-1, 1024, 14, 14]           2,048\n                 ReLU-93         [-1, 1024, 14, 14]               0\n           Bottleneck-94         [-1, 1024, 14, 14]               0\n               Conv2d-95          [-1, 256, 14, 14]         262,144\n          BatchNorm2d-96          [-1, 256, 14, 14]             512\n                 ReLU-97          [-1, 256, 14, 14]               0\n               Conv2d-98          [-1, 256, 14, 14]         589,824\n          BatchNorm2d-99          [-1, 256, 14, 14]             512\n                ReLU-100          [-1, 256, 14, 14]               0\n              Conv2d-101         [-1, 1024, 14, 14]         262,144\n         BatchNorm2d-102         [-1, 1024, 14, 14]           2,048\n                ReLU-103         [-1, 1024, 14, 14]               0\n          Bottleneck-104         [-1, 1024, 14, 14]               0\n              Conv2d-105          [-1, 256, 14, 14]         262,144\n         BatchNorm2d-106          [-1, 256, 14, 14]             512\n                ReLU-107          [-1, 256, 14, 14]               0\n              Conv2d-108          [-1, 256, 14, 14]         589,824\n         BatchNorm2d-109          [-1, 256, 14, 14]             512\n                ReLU-110          [-1, 256, 14, 14]               0\n              Conv2d-111         [-1, 1024, 14, 14]         262,144\n         BatchNorm2d-112         [-1, 1024, 14, 14]           2,048\n                ReLU-113         [-1, 1024, 14, 14]               0\n          Bottleneck-114         [-1, 1024, 14, 14]               0\n              Conv2d-115          [-1, 256, 14, 14]         262,144\n         BatchNorm2d-116          [-1, 256, 14, 14]             512\n                ReLU-117          [-1, 256, 14, 14]               0\n              Conv2d-118          [-1, 256, 14, 14]         589,824\n         BatchNorm2d-119          [-1, 256, 14, 14]             512\n                ReLU-120          [-1, 256, 14, 14]               0\n              Conv2d-121         [-1, 1024, 14, 14]         262,144\n         BatchNorm2d-122         [-1, 1024, 14, 14]           2,048\n                ReLU-123         [-1, 1024, 14, 14]               0\n          Bottleneck-124         [-1, 1024, 14, 14]               0\n              Conv2d-125          [-1, 256, 14, 14]         262,144\n         BatchNorm2d-126          [-1, 256, 14, 14]             512\n                ReLU-127          [-1, 256, 14, 14]               0\n              Conv2d-128          [-1, 256, 14, 14]         589,824\n         BatchNorm2d-129          [-1, 256, 14, 14]             512\n                ReLU-130          [-1, 256, 14, 14]               0\n              Conv2d-131         [-1, 1024, 14, 14]         262,144\n         BatchNorm2d-132         [-1, 1024, 14, 14]           2,048\n                ReLU-133         [-1, 1024, 14, 14]               0\n          Bottleneck-134         [-1, 1024, 14, 14]               0\n              Conv2d-135          [-1, 256, 14, 14]         262,144\n         BatchNorm2d-136          [-1, 256, 14, 14]             512\n                ReLU-137          [-1, 256, 14, 14]               0\n              Conv2d-138          [-1, 256, 14, 14]         589,824\n         BatchNorm2d-139          [-1, 256, 14, 14]             512\n                ReLU-140          [-1, 256, 14, 14]               0\n              Conv2d-141         [-1, 1024, 14, 14]         262,144\n         BatchNorm2d-142         [-1, 1024, 14, 14]           2,048\n                ReLU-143         [-1, 1024, 14, 14]               0\n          Bottleneck-144         [-1, 1024, 14, 14]               0\n              Conv2d-145          [-1, 512, 14, 14]         524,288\n         BatchNorm2d-146          [-1, 512, 14, 14]           1,024\n                ReLU-147          [-1, 512, 14, 14]               0\n              Conv2d-148            [-1, 512, 7, 7]       2,359,296\n         BatchNorm2d-149            [-1, 512, 7, 7]           1,024\n                ReLU-150            [-1, 512, 7, 7]               0\n              Conv2d-151           [-1, 2048, 7, 7]       1,048,576\n         BatchNorm2d-152           [-1, 2048, 7, 7]           4,096\n              Conv2d-153           [-1, 2048, 7, 7]       2,097,152\n         BatchNorm2d-154           [-1, 2048, 7, 7]           4,096\n                ReLU-155           [-1, 2048, 7, 7]               0\n          Bottleneck-156           [-1, 2048, 7, 7]               0\n              Conv2d-157            [-1, 512, 7, 7]       1,048,576\n         BatchNorm2d-158            [-1, 512, 7, 7]           1,024\n                ReLU-159            [-1, 512, 7, 7]               0\n              Conv2d-160            [-1, 512, 7, 7]       2,359,296\n         BatchNorm2d-161            [-1, 512, 7, 7]           1,024\n                ReLU-162            [-1, 512, 7, 7]               0\n              Conv2d-163           [-1, 2048, 7, 7]       1,048,576\n         BatchNorm2d-164           [-1, 2048, 7, 7]           4,096\n                ReLU-165           [-1, 2048, 7, 7]               0\n          Bottleneck-166           [-1, 2048, 7, 7]               0\n              Conv2d-167            [-1, 512, 7, 7]       1,048,576\n         BatchNorm2d-168            [-1, 512, 7, 7]           1,024\n                ReLU-169            [-1, 512, 7, 7]               0\n              Conv2d-170            [-1, 512, 7, 7]       2,359,296\n         BatchNorm2d-171            [-1, 512, 7, 7]           1,024\n                ReLU-172            [-1, 512, 7, 7]               0\n              Conv2d-173           [-1, 2048, 7, 7]       1,048,576\n         BatchNorm2d-174           [-1, 2048, 7, 7]           4,096\n                ReLU-175           [-1, 2048, 7, 7]               0\n          Bottleneck-176           [-1, 2048, 7, 7]               0\n              Conv2d-177           [-1, 1024, 7, 7]       2,098,176\n                ReLU-178           [-1, 1024, 7, 7]               0\n            Upsample-179         [-1, 1024, 14, 14]               0\n              Conv2d-180          [-1, 512, 14, 14]         524,800\n                ReLU-181          [-1, 512, 14, 14]               0\n              Conv2d-182          [-1, 512, 14, 14]       7,078,400\n                ReLU-183          [-1, 512, 14, 14]               0\n            Upsample-184          [-1, 512, 28, 28]               0\n              Conv2d-185          [-1, 512, 28, 28]         262,656\n                ReLU-186          [-1, 512, 28, 28]               0\n              Conv2d-187          [-1, 512, 28, 28]       4,719,104\n                ReLU-188          [-1, 512, 28, 28]               0\n            Upsample-189          [-1, 512, 56, 56]               0\n              Conv2d-190          [-1, 256, 56, 56]          65,792\n                ReLU-191          [-1, 256, 56, 56]               0\n              Conv2d-192          [-1, 256, 56, 56]       1,769,728\n                ReLU-193          [-1, 256, 56, 56]               0\n            Upsample-194        [-1, 256, 112, 112]               0\n              Conv2d-195         [-1, 64, 112, 112]           4,160\n                ReLU-196         [-1, 64, 112, 112]               0\n              Conv2d-197        [-1, 128, 112, 112]         368,768\n                ReLU-198        [-1, 128, 112, 112]               0\n            Upsample-199        [-1, 128, 224, 224]               0\n              Conv2d-200         [-1, 64, 224, 224]         110,656\n                ReLU-201         [-1, 64, 224, 224]               0\n              Conv2d-202          [-1, 6, 224, 224]             390\n    ================================================================\n    Total params: 40,549,382\n    Trainable params: 40,549,382\n    Non-trainable params: 0\n    ----------------------------------------------------------------\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9810939666202909
      ],
      "excerpt": "This repository contains simple PyTorch implementations of U-Net and FCN, which are deep learning segmentation methods proposed by Ronneberger et al. and Long et al. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for x in [input_images, target_masks]: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920033801669215
      ],
      "excerpt": ": Get a batch of training data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8814103556086321
      ],
      "excerpt": "    for k in metrics.keys(): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "def train_model(model, optimizer, scheduler, num_epochs=25): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "            for param_group in optimizer.param_groups: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8852179968684676
      ],
      "excerpt": "        #: deep copy the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "return model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9707693691410405
      ],
      "excerpt": "model = ResNetUNet(num_class).to(device) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9736528848850865,
        0.8611099533139691
      ],
      "excerpt": ":for l in model.base_layers: \n:    for param in l.parameters(): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=60) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Simple PyTorch implementations of U-Net/FullyConvNet (FCN) for image segmentation",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/usuyama/pytorch-unet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 192,
      "date": "Sat, 25 Dec 2021 06:06:44 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/usuyama/pytorch-unet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "usuyama/pytorch-unet",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/usuyama/pytorch-unet/master/pytorch_fcn.ipynb",
      "https://raw.githubusercontent.com/usuyama/pytorch-unet/master/pytorch_unet_resnet18_colab.ipynb",
      "https://raw.githubusercontent.com/usuyama/pytorch-unet/master/pytorch_resnet18_unet.ipynb",
      "https://raw.githubusercontent.com/usuyama/pytorch-unet/master/pytorch_unet.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, datasets, models\n\nclass SimDataset(Dataset):\n    def __init__(self, count, transform=None):\n        self.input_images, self.target_masks = simulation.generate_random_data(192, 192, count=count)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.input_images)\n\n    def __getitem__(self, idx):\n        image = self.input_images[idx]\n        mask = self.target_masks[idx]\n        if self.transform:\n            image = self.transform(image)\n\n        return [image, mask]\n\n#: use the same transformations for train/val in this example\ntrans = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #: imagenet\n])\n\ntrain_set = SimDataset(2000, transform = trans)\nval_set = SimDataset(200, transform = trans)\n\nimage_datasets = {\n    'train': train_set, 'val': val_set\n}\n\nbatch_size = 25\n\ndataloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8032098204076749
      ],
      "excerpt": "First clone the repository and cd into the project directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8233588558014837
      ],
      "excerpt": "    inp = inp.numpy().transpose((1, 2, 0)) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9068127677393759,
        0.9457175861910134
      ],
      "excerpt": "import matplotlib.pyplot as plt \nimport numpy as np \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9312650322969277,
        0.8830433511399133
      ],
      "excerpt": "    print(x.shape) \n    print(x.min(), x.max()) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.846908509220064
      ],
      "excerpt": "input_images_rgb = [x.astype(np.uint8) for x in input_images] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9535879515266492
      ],
      "excerpt": "import torchvision.utils \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9160335223768148,
        0.9241022411340848
      ],
      "excerpt": "    mean = np.array([0.485, 0.456, 0.406]) \n    std = np.array([0.229, 0.224, 0.225]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468,
        0.8712817808361308
      ],
      "excerpt": "    inp = np.clip(inp, 0, 1) \n    inp = (inp * 255).astype(np.uint8) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8028294379516652
      ],
      "excerpt": ": Get a batch of training data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.803149884743526
      ],
      "excerpt": "print(inputs.shape, masks.shape) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8900486270063179
      ],
      "excerpt": "from torchvision import models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "        nn.ReLU(inplace=True), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665800468796203
      ],
      "excerpt": "    self.base_model = models.resnet18(pretrained=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806757247630729
      ],
      "excerpt": "    self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8791175621392118
      ],
      "excerpt": "from collections import defaultdict \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024966767585595
      ],
      "excerpt": "from loss import dice_loss \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8368522740531952
      ],
      "excerpt": "print(\"{}: {}\".format(phase, \", \".join(outputs))) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944978459558092,
        0.9346081970836996
      ],
      "excerpt": "    print('Epoch {}/{}'.format(epoch, num_epochs - 1)) \n    print('-' * 10) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8312527460906908,
        0.8866550200552868
      ],
      "excerpt": "                print(\"LR\", param_group['lr']) \n            model.train()  #: Set model to training mode \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8107953358989355
      ],
      "excerpt": "            model.eval()   #: Set model to evaluate mode \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8461705771429826
      ],
      "excerpt": "            print(\"saving best model\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9335757557148643,
        0.8287282820038653
      ],
      "excerpt": "    print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) \nprint('Best val loss: {:4f}'.format(best_loss)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8396948529258378,
        0.8869999123707137
      ],
      "excerpt": "import time \nimport copy \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/usuyama/pytorch-unet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Naoto Usuyama\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "UNet/FCN PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-unet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "usuyama",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/usuyama/pytorch-unet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 594,
      "date": "Sat, 25 Dec 2021 06:06:44 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "image-segmentation",
      "unet",
      "fully-convolutional-networks",
      "semantic-segmentation"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nimport math\n\nmodel.eval()   #: Set model to the evaluation mode\n\n#: Create another simulation dataset for test\ntest_dataset = SimDataset(3, transform = trans)\ntest_loader = DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=0)\n\n#: Get the first batch\ninputs, labels = next(iter(test_loader))\ninputs = inputs.to(device)\nlabels = labels.to(device)\n\n#: Predict\npred = model(inputs)\n#: The loss functions include the sigmoid function.\npred = F.sigmoid(pred)\npred = pred.data.cpu().numpy()\nprint(pred.shape)\n\n#: Change channel-order and make 3 channels for matplot\ninput_images_rgb = [reverse_transform(x) for x in inputs.cpu()]\n\n#: Map each channel (i.e. class) to each color\ntarget_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\npred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n\nhelper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])\n```\n\n    (3, 6, 192, 192)\n\n",
      "technique": "Header extraction"
    }
  ]
}