{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.06581\n* Double DQNs: Refer https://arxiv.org/abs/1509.06461\n* Prioritised experience replay (**PER**",
      "https://arxiv.org/abs/1509.06461\n* Prioritised experience replay (**PER**",
      "https://arxiv.org/abs/1511.05952\n\n### Efficient Implementation for Prioritised Experience Replay\nThe concept of using prioritised experience replay is to sample experiences with higher TD errors with a higher probability.\n\nHowever, doing so comes at the cost of higher sampling and update times to the experience buffer ***{D}***. \n\nThe following shows the time complexity for key operations required for PER:\n\n* Compute max priority from the experience replay: **O(n",
      "https://arxiv.org/abs/1511.06581 (2015).</cite>\n2. <cite> Van Hasselt, Hado, Arthur Guez, and David Silver. \"Deep reinforcement learning with double q-learning.\" Thirtieth AAAI Conference on Artificial Intelligence. 2016.</cite>\n3. <cite> Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature 518.7540 (2015): 529.  </cite>\n4. <cite> Lillicrap, Timothy P., et al. \"Continuous control with deep reinforcement learning.\" arXiv preprint https://arxiv.org/abs/1509.02971 (2015).</cite>\n5. <cite>Schaul, Tom, et al. \"Prioritized experience replay.\" arXiv preprint https://arxiv.org/abs/1511.05952 (2015).</cite>",
      "https://arxiv.org/abs/1509.02971 (2015).</cite>\n5. <cite>Schaul, Tom, et al. \"Prioritized experience replay.\" arXiv preprint https://arxiv.org/abs/1511.05952 (2015).</cite>",
      "https://arxiv.org/abs/1511.05952 (2015).</cite>"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "Benchmark Mean Reward: 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "CUDA 10.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9990715523422201,
        0.9985319060770389,
        0.9976821089163788
      ],
      "excerpt": "Dueling Q Networks (DDQN): Refer https://arxiv.org/abs/1511.06581 \nDouble DQNs: Refer https://arxiv.org/abs/1509.06461 \nPrioritised experience replay (PER): Refer https://arxiv.org/abs/1511.05952 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/1jsingh/rl_navigation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-09-06T22:01:52Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-16T01:16:39Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.978993860100775
      ],
      "excerpt": "The project uses Dueling Double DQN with prioritized experience replay for training an agent to navigate in an artificial banana world, while trying the maximize the cummulitive reward. The project uses Unity environment for training the reinforcement learning agent. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8774900582893563,
        0.853134726109167
      ],
      "excerpt": "Instructions for getting started \nProject Structure \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8198477516283035
      ],
      "excerpt": "Goal: The agents must learn to move to as many yellow bananas as possible \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9148143511538203
      ],
      "excerpt": "Agents: The environment contains 5 agents linked to a single Brain. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.91589987837751,
        0.91589987837751,
        0.8245791090205943,
        0.9611435476243231,
        0.986894479224407
      ],
      "excerpt": "+1 for interaction with yellow banana \n-1 for interaction with blue banana. \nBrains: One Brain with the following observation/action space. \nVector Observation space: 53 corresponding to velocity of agent (2), whether \n    agent is frozen and/or shot its laser (2), plus ray-based perception of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9744533477331667
      ],
      "excerpt": "The project was built with the following configuration: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8684803241145291
      ],
      "excerpt": "Though not tested, the project can still be expected to work out of the box for most reasonably deviant configurations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8901464892103309
      ],
      "excerpt": "model.py: model definitions for the DQN agent \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8785587425621867
      ],
      "excerpt": "The algorithm uses a Dueling Double DQN along with prioritised experience replay for learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9917120273630528,
        0.901803380350025,
        0.8185852882243421
      ],
      "excerpt": "The concept of using prioritised experience replay is to sample experiences with higher TD errors with a higher probability. \nHowever, doing so comes at the cost of higher sampling and update times to the experience buffer {D}.  \nThe following shows the time complexity for key operations required for PER: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9577619289859243,
        0.991036817027248,
        0.8314062061959274,
        0.9888127112767583,
        0.9589851913832015
      ],
      "excerpt": "Compute sum of priorities for all samples from the experience replay: O(n) \nInsertion of new samples in the experience replay: O(1) \nThus time complexity for a naive implementation for PER : O(n) \nIn order to work around this problem, I designed a fixed size binary search tree for computing the maximum priority with a buffer for storing the sum of these priorities. \nTime complexity for fixed size binary search tree based optimized implementation of PER: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9577619289859243,
        0.991036817027248,
        0.8869170881689385
      ],
      "excerpt": "Compute sum of priorities for all samples from the experience replay: O(1) \nInsertion of new samples in the experience replay: O(1) \nThus the overall time complexity for optimized implementation of PER: O(log(n)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Navigating a banana world using Dueling Double DQN network",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/1jsingh/rl_navigation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 20 Dec 2021 17:51:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/1jsingh/rl_navigation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "1jsingh/rl_navigation",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/1jsingh/rl_navigation/master/Navigation.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Create separate virtual environment for the project using the provided `environment.yml` file\n```\nconda env create -f environment.yml\nconda activate navigation\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9625196775343342
      ],
      "excerpt": "Banana Collector Unity Environment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9832426978359994
      ],
      "excerpt": "Environment Setup \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9322609392449874
      ],
      "excerpt": "Pytorch 1.0 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8482642367569435
      ],
      "excerpt": "Trained Agent Demo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8852495800602792,
        0.8692035179824905
      ],
      "excerpt": "dqn_agent.py: DQN agent and Replay Buffer class \nmodel.py: model definitions for the DQN agent \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9001373101481486
      ],
      "excerpt": "<img src='images/reward_curve-ddqn.png' alt='reward_curve-ddqn'> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/1jsingh/rl_navigation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 JASKIRAT SINGH\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Navigation using DQN",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "rl_navigation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "1jsingh",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/1jsingh/rl_navigation/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 20 Dec 2021 17:51:30 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "dqn",
      "deep-reinforcement-learning",
      "prioritized-experience-replay",
      "dueling-dqn",
      "double-dqn",
      "unity-ml-agents",
      "pytorch"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "![trained agent](images/trained_agent.gif)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Clone the repository (if you haven't already!)\n```bash\ngit clone https://github.com/1jsingh/rl_navigation.git\ncd rl_navigation\n```\n\n2. Download the environment from one of the links below.  You need only select the environment that matches your operating system:\n\n    \n    - Linux: [click here](https://drive.google.com/open?id=1hbezVc5oOthoQ2VF9c4RPWxsf5M8mxEh)\n    - Mac OSX: [click here](https://drive.google.com/open?id=1HTvJxRA24bJKsyzzfy3-J7eOo8XJYpF1N)\n\n    (_For AWS_) If you'd like to train the agent on AWS (and have not [enabled a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md)), then please use [this link](https://drive.google.com/open?id=1BpLCYfGcp7y5WPAPPmxxe0mVcYM1LG9N) to obtain the \"headless\" version of the environment.  You will **not** be able to watch the agent without enabling a virtual screen, but you will be able to train the agent.  (_To watch the agent, you should follow the instructions to [enable a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md), and then download the environment for the **Linux** operating system above._)\n       \n3. Place the downloaded file in the `unity_envs` directory and unzip it.\n  ```\n  mkdir unity_envs && cd unity_envs\n  unzip Banana_Collector_Linux.zip\n  ```\n\n4. Follow along with `Navigation.ipynb` to train your own RL agent.\n\n",
      "technique": "Header extraction"
    }
  ]
}