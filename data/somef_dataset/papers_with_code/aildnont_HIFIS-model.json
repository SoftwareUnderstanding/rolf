{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1602.04938"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8654671031158477
      ],
      "excerpt": "   Prediction horizon search experiment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9195926162616405
      ],
      "excerpt": "Azure Machine Learning Pipelines \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633523472228958
      ],
      "excerpt": "   hyperparameters you wish to study and the number of times you would \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9253259874307139
      ],
      "excerpt": "Depending on your organization's circumstances, you may wish to exclude \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9522273077770335
      ],
      "excerpt": "- EXPERIMENT: The type of LIME interpretability experiment you would \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8999738930005365
      ],
      "excerpt": "EXPERIMENT: The type of prediction experiment you would like to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654671031158477
      ],
      "excerpt": "  Analysis experiment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654671031158477
      ],
      "excerpt": "  Analysis experiment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9071681380397938
      ],
      "excerpt": "  would like to perform if executing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9019226542848389
      ],
      "excerpt": "src/ folder. If you plan on using the Azure machine learning pipelines \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8383350619300797
      ],
      "excerpt": "    \"workspace_name\": \"name-of-your-machine-learning-workspace\" \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aildnont/HIFIS-model",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**Matt Ross**  \nManager, Artificial Intelligence  \nInformation Technology Services, City Manager\u2019s Office  \nCity of London  \nSuite 300 - 201 Queens Ave, London, ON. N6A 1J1  \n\n**Blake VanBerlo**  \nData Scientist  \nCity of London Municipal Artificial Intelligence Applications Lab  \nC: blake@vanberloconsulting.com  \n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-07T13:29:00Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-25T08:20:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9923926307775798,
        0.9701372006304015,
        0.9961225593186362,
        0.9321042121336883,
        0.989425474254284
      ],
      "excerpt": "The purpose of this project is deliver a machine learning solution to \nassist in identifying individuals at risk of chronic homelessness. A \nmodel was built for the Homeless Prevention division of the City of \nLondon, Ontario, Canada. This work was led by the Municipal Artificial Intelligence Applications Lab out of the Information \nTechnology Services division. For more information on results of the London project, review our pre-print article.This repository contains the code used to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8994321274201729
      ],
      "excerpt": "(HIFIS) database as either at risk or not at risk of chronic \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9065166167617044,
        0.8906778746869327,
        0.9705570004057509,
        0.8011917070827809,
        0.9544836086201969,
        0.9925639055162538,
        0.9563892249891057,
        0.9563936333877426
      ],
      "excerpt": "regulation of automated decision-making systems, this repository applies \ninterpretability and bias-reducing methods to explain the model's \npredictions. The model also employs functionality to enable ease of \nclient record removal, entire feature removal and audit trails to \nfacilitate appeals and other data governance processes. This repository \nis intended to serve as a turnkey template for other municipalities \nusing the HIFIS application and HIFIS database schema who wish to \nexplore the application of this model in their own locales. This model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9908502588685393
      ],
      "excerpt": "   Exclusion of sensitive features \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.853134726109167
      ],
      "excerpt": "Project Structure \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8191875960878647
      ],
      "excerpt": "   Getting Started for help obtaining \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8006990177960035
      ],
      "excerpt": "Ensure data has been preprocessed properly. That is, verify that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9426422718842838,
        0.852745908201928
      ],
      "excerpt": "   HIFIS_Processed_OHE.csv. The latter is identical to the former with \n   the exception being that its single-valued categorical features have \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.916934143823561
      ],
      "excerpt": "   is the current time. The model's logs will be located in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8435914691858043
      ],
      "excerpt": "   time in the same format. These logs contain information about the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9343124734203861
      ],
      "excerpt": "   (such as the example below) are available in the SCALARS tab of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9343010454680492,
        0.9324654637716586
      ],
      "excerpt": "   set. See below for an example of the ROC Curve and Confusion Matrix \n   based on test set predictions. In our implementation, these plots are \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8214396606136685,
        0.9604426526421755,
        0.9870317977205382
      ],
      "excerpt": "The diagram below depicts an overview the model's architecture. We call \nthis model \"HIFIS MLP\", as the model is an example of a multilayer \nperceptron. NODES0 and NODES1 correspond to hyperparameters \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8055697265137426
      ],
      "excerpt": "3. Decide which metrics you would like to optimize and in what order. In \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8913646018224853
      ],
      "excerpt": "   yyyymmdd-hhmmss is the current time. The model's logs will be located \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8875036722024155,
        0.9727977859319193,
        0.8435363925673574,
        0.9442467382662506,
        0.9772297931323096,
        0.9800369673634658,
        0.9104978110373639,
        0.9363226105204592,
        0.941082109641463,
        0.82606501542761,
        0.860059181823877,
        0.986081203887623,
        0.8345947157417949,
        0.9596882296656882,
        0.9653025858723123,
        0.9416778278912775,
        0.8940797197523032,
        0.8663137834621295,
        0.8544833846037938,
        0.9835081963054922
      ],
      "excerpt": "Since the predictions made by this model are to be used by a government \ninstitution to benefit vulnerable members of society, it is imperative \nthat the model's predictions may be explained so as to facilitate \nensuring the model is making responsible predictions, as well as \nassuring transparency and accountability of government decision-making \nprocesses. Since this model is a neural network, it is difficult to \ndecipher which rules or heuristics it is employing to make its \npredictions. Interpretability in machine learning is a growing concern, \nespecially with applications in the healthcare and social services \ndomains. We used Local Interpretable \nModel-Agnostic Explanations (i.e. \nLIME) to explain the predictions of the neural network classifier that \nwe trained. We used the implementation available in the authors' GitHub \nrepository. LIME perturbs the \nfeatures in an example and fits a linear model to approximate the neural \nnetwork at the local region in the feature space surrounding the \nexample. It then uses the linear model to determine which features were \nmost contributory to the model's prediction for that example. By \napplying LIME to our trained model, we can conduct informed feature \nengineering based on any obviously inconsequential features we see (e.g. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9689294420531083,
        0.916689167233136
      ],
      "excerpt": "model is learning any unintended bias and eliminate that bias through \nadditional feature engineering. See the steps below to apply LIME to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9544148318627323
      ],
      "excerpt": "      results, and produce a visualization of the average explainable \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.892662738066311,
        0.9537968194923492
      ],
      "excerpt": "      submodular pick algorithm (as described in the \n      LIME paper) to pick and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8921590776045816
      ],
      "excerpt": "      attempt to explain the model's functionality as a whole. Global \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9334404055643901
      ],
      "excerpt": "      be generated that depicts the top explainable features that the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8468735149729586
      ],
      "excerpt": "      Client_client_id_exp_yyyymmdd-hhmmss.png. See below for an \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9025198291620795,
        0.9590223394061265
      ],
      "excerpt": "4. Interpret the output of the LIME explainer. LIME partitions features \n   into classes or ranges and reports the features most contributory to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9428317724269287,
        0.8161898756685079,
        0.8523798217961692,
        0.9902374691896364,
        0.8924075773350275
      ],
      "excerpt": "   range of values) of a feature and its associated weight in the \n   prediction. In the example portrayed by the bar graph below, the fact \n   that TotalStays was greater than 4 but less than or equal to 23 \n   contributed negatively with a magnitude of about 0.22 to a positive \n   prediction (meaning it contributed at a magnitude of 0.22 toward a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9261042532279565,
        0.9239622808697366,
        0.9582287711277194
      ],
      "excerpt": "   indicates that a Boolean feature is present, and =0 indicates that \n   a Boolean feature is not present) and that this explanation \n   contributed with a weight of about 0.02 toward a positive prediction. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9522163015610853
      ],
      "excerpt": "   value is \"Yes - Tribe Not Known\", which contributed with a weight of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9077042238856291,
        0.9377735292870111,
        0.9674908550865982,
        0.9202739503673958,
        0.9210190671473504
      ],
      "excerpt": "- Missing records for numerical features are given a value of -1 \n- Missing records for categorical features are given a value of \"Unknown\" \nHyperparameter tuning is an important part of the standard machine \nlearning workflow. We chose to conduct a series of random hyperparameter \nsearches. The results of one search informed the next, leading us to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9940835400450485,
        0.8490037945672047
      ],
      "excerpt": "visualization to aid the random hyperparameter search. With the help of \nthe HParam \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.957125040548471
      ],
      "excerpt": "one can see the effect of different combinations of hyperparameters on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9858739097809865,
        0.9156634076141417,
        0.9696839085429355
      ],
      "excerpt": "In our random hyperparameter search, we study the effects of x random \ncombinations of hyperparameters by training the model y times for each \nof the x combinations and recording the results. See the steps below \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8378149749369099
      ],
      "excerpt": "skip steps 2-4, as a default set of hyperparameter ranges is already \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9565842585963646
      ],
      "excerpt": "1. In the in the HP subsection of the TRAIN section of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8610065013356322,
        0.8653040184761134
      ],
      "excerpt": "   like to train the model for each combination (see \n   Project Config for help). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9052829580873318
      ],
      "excerpt": "2. Set the ranges of hyperparameters you wish to study in the HP \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8383961727729009
      ],
      "excerpt": "   (i.e. real) or discrete and whether any need to be investigated on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9598908555174542
      ],
      "excerpt": "    HParam objects to the list of hyperparameters being considered. The \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9696832777075512
      ],
      "excerpt": "   based on the random combination in either \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8635745920787571
      ],
      "excerpt": "   different combinations of hyperparameters. The logs can be visualized \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8193239889680336
      ],
      "excerpt": "   locally. See below for an example of a view offered by the HParams \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8161753752018266,
        0.8599741418600249,
        0.99007528928622
      ],
      "excerpt": "   graph compares values of hyperparameters to test set metrics. \nOnce a trained model is produced, the user may wish to obtain \npredictions and explanations for all clients currently in the HIFIS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8459702577796318
      ],
      "excerpt": "the HIFIS database change as well. Thus, it is useful to rerun \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874929902494114
      ],
      "excerpt": "for all clients, given raw data from HIFIS and a trained model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8176698590868708
      ],
      "excerpt": "   training your model, as it will have generated and saved a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401428284797218
      ],
      "excerpt": "   data folder (data/raw/). See Getting Started \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8667064650002715
      ],
      "excerpt": "4. By changing the value of the EXPERIMENT field of the PREDICTION \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8110161992357651
      ],
      "excerpt": "   can opt to either (i) save predictions to a new file or (ii) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.88114291338489
      ],
      "excerpt": "   1. Setting the EXPERIMENT field of the PREDICTION section of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.88114291338489
      ],
      "excerpt": "   2. Setting the EXPERIMENT field of the PREDICTION section of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9374807299429565
      ],
      "excerpt": "      predictions and explanations in the same method as described in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9700166101238226
      ],
      "excerpt": "      the user to compare the change in predictions and explanations for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8285258982373842
      ],
      "excerpt": "Cross validation helps us select a model that is as unbiased as possible \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9939174265474332
      ],
      "excerpt": "increasingly confident in the external validity of our results. This \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9636647823458647,
        0.922188815550403,
        0.9688089884835964,
        0.95355119392691,
        0.9863736928898783
      ],
      "excerpt": "model and the HIFIS-RNN-MLP. K-fold cross validation is used in the case \nof HIFIS-MLP, whereas nested cross-validation with day-forward chaining \nis used for HIFIS-RNN-MLP. The difference in our cross validation \nalgorithms is a result of the different types of data used for both \nscenarios. In HIFIS-MLP, data is randomly partitioned by ClientID into \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8331317603671488
      ],
      "excerpt": "series data, the validation and test sets are taken to be the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8498130880155698
      ],
      "excerpt": "strategy for time series data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8101674447427318
      ],
      "excerpt": "   standard deviation of metrics for all folds. The file will be located \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9223965468157215,
        0.9695726215580922,
        0.9892315181316329
      ],
      "excerpt": "The prediction horizon (N) is defined as the amount of time from now \nthat the model makes its predictions for. In our case, the prediction \nhorizon is how far in the future (in weeks) the model is predicting risk \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9066445313102094,
        0.9344832606708362,
        0.9795764686857678
      ],
      "excerpt": "model is predicting whether or not a client will be at risk of chronic \nhomelessness in 26 weeks. While developing this model, we noticed that \nthe model's performance is inversely correlated with the prediction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.898866998512261
      ],
      "excerpt": "validation at multiple values of N. For each value of N, the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9863645704333321,
        0.9934111754899448,
        0.8839933136495411
      ],
      "excerpt": "weeks of records. The relationships of N and several model metrics are \ngraphed for the user to deliver insight on the impact of N and make a \nbusiness decision as to which value yields optimal results. See below \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9435015659966263,
        0.9105829930156175
      ],
      "excerpt": "   N_MIN, N_MAX, N_INTERVAL and RUNS_PER_N according to your \n   organization's needs (see Project Config for help). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8556958877169957
      ],
      "excerpt": "   where yyyymmdd-hhmmss is the current time. A graphical representation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8916615162592548
      ],
      "excerpt": "   horizon_experiment_yyyymmdd-hhmmss.png. See below for an example of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9948296369997291,
        0.997473244754322,
        0.8143380999053514,
        0.9798670401443291
      ],
      "excerpt": "not cause bias in the model is to avoid including it as a feature of the \nmodel. This project supports the exclusion of specific features by \ndropping them at the start of data preprocessing. See the below steps \nfor details on how to accomplish this: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9323564628781548
      ],
      "excerpt": "2. Features that the model will be trained on correspond to the column \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9805286331689178,
        0.9900154159526805,
        0.8235955215477265
      ],
      "excerpt": "   you decided on during step 2 to the list of features detailed in the \n   FEATURES_TO_DROP_FIRST field of the DATA section of \n   config.yml (for more info see Project Config). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9400792552813068,
        0.9560451249169168
      ],
      "excerpt": "clustered. Since HIFIS consists of numerical and categorical data, and \nin the spirit of minimizing time complexity, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8454269656135521
      ],
      "excerpt": "was selected as the clustering algorithm. We wish to acknowledge Nico de \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9619273432711279
      ],
      "excerpt": "implementation of k-prototypes. This \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9801993519642257
      ],
      "excerpt": "assigned clusters' centroids). Our intention is that by examining \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9820483147550684
      ],
      "excerpt": "insight into patterns in the data and how the model behaves in different \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9730460797300226
      ],
      "excerpt": "LIME explanations of these centroids. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8176698590868708
      ],
      "excerpt": "   training your model, as it will have generated and saved a LIME \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8750660609237434,
        0.8926823428535553
      ],
      "excerpt": "   Getting Started for help. \n3. Set the EXPERIMENT field of the K-PROTOTYPES section of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184804664950295
      ],
      "excerpt": "      appended to the end by default. The spreadsheet will be located at \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801038231378024
      ],
      "excerpt": "   3. A graphic depicting the LIME explanations of all centroids will be \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9980157367464884
      ],
      "excerpt": "A tradeoff for the efficiency of k-prototypes is the fact that the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8040597098290228
      ],
      "excerpt": "determine the optimal number of clusters, the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.985115974222865
      ],
      "excerpt": "computed for a range of values of k. A graph is produced that plots \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9497293209160328
      ],
      "excerpt": "is, the more optimal k is. To run this experiment, see the below \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8210756883231021
      ],
      "excerpt": "1. Follow steps 1 and 2 as outlined above in the clustering \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8926823428535553
      ],
      "excerpt": "2. Set the EXPERIMENT field of the K-PROTOTYPES section of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8362264544047048,
        0.8632753159330564
      ],
      "excerpt": "   you may wish to change the K_MIN and K_MAX field of the \n   K-PROTOTYPES section from their defaults. Values of k in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090482159044082
      ],
      "excerpt": "service usage as time series features. The motivation behind the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9396091035879679
      ],
      "excerpt": "of homelessness, thereby improving model predictions for clients at \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.820647885446162
      ],
      "excerpt": "give further context to a client's story that could be formalized as an \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.933548358991922,
        0.9805894365888305
      ],
      "excerpt": "management, food bank visits) are quantified over time. In the original \nHIFIS MLP model, these features were totalled up to the date of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8267243349018474,
        0.8827327511852925,
        0.8967953915183172
      ],
      "excerpt": "length (i.e. T_X) defines how many of the most recent timesteps to \ninclude in a single client record. For instance, suppose that the \ntimestep is 30 days and T_X is 6. A single client record will contain \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8645070343817138
      ],
      "excerpt": "An additional benefit of formulating time series client records was that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8042845734945103
      ],
      "excerpt": "data preprocessing, records are calculated for each client at different \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9302287581602335
      ],
      "excerpt": "the MLP model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8209679701962586
      ],
      "excerpt": "instead of randomly by ClientID. The test and validation sets comprise \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8718738870443548,
        0.8586545719136922,
        0.9352710692529362,
        0.8766586842444776,
        0.8863613479429381,
        0.8610732814872879,
        0.9659421328099079,
        0.9633523412625715,
        0.8872710996563091
      ],
      "excerpt": "training set is taken to be all records with dates earlier than those in \nthe test and validation sets. This way, the model is tested on the most \nrecent client data and is likely to perform well on new client data. Due \nto the non-random partitions of training, validation and test sets, the \ncross validation experiment used in this repository for time series data \nis nested cross validation with day-forward chaining (see Cross \nValidation for more info). \nThe time series forecasting model is different than that of the first \nmodel described. The first iteration of the HIFIS model was a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8082293773827857,
        0.9344177475660356,
        0.8873510319142203,
        0.9697531475234152,
        0.9184392100360909
      ],
      "excerpt": "architecture. This model, dubbed \"RNN-MLP model\", captures the state \nof time series features by incorporating an LSTM layer through which the \ntime series features are fed. The static features (i.e. non-time-series \nfeatures) are concatenated with the output of the LSTM layer, and fed \ninto an MLP, whose output is the model's decision. Examples of static \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801624933055319,
        0.9619749887626418
      ],
      "excerpt": "included in this group, as they capture a client's service usage since \nthe beginning of their inclusion in HIFIS. See below for a diagram \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9061403376891369,
        0.924431999712162
      ],
      "excerpt": "Explanations are computed for the RNN-MLP model in the same way as they \nwere for the original MLP model, except that they are computed for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359796032955099,
        0.8969763792281082,
        0.9113912005548862,
        0.9400481842726057
      ],
      "excerpt": "time series features (especially those of total stays during different \ntimesteps) appeared more often as being important in explanations. The \ntime series features are named for their position in the input sequence \nand the duration of the timestep. For example, a feature called \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9415810294430973
      ],
      "excerpt": "apply this repository to your municipality's HIFIS database, along with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.850756177020258,
        0.8100067910335585,
        0.8200226883889041,
        0.8624881409478627
      ],
      "excerpt": "  the DATA seection of config.yml that does not exist as \n  a column in the CSV of raw data extracted from the HIFIS database. \n  This could occur if one of the default features in those lists does \n  not exist in your HIFIS database. To fix this, remove the feature (in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.83902512800931,
        0.960470988604244
      ],
      "excerpt": "- A feature in your database is missing in the preprocessed data CSV \n  All features are either classified as noncategorical or categorical. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9750325322349946
      ],
      "excerpt": "  NONCATEGORICAL_FEATURES and DATA > CATEGORICAL_FEATURES (in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9484084887102262,
        0.9202467078053661
      ],
      "excerpt": "  that you wish to use as features for the model. \n- Incorrect designation of feature as noncategorical vs. categorical \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9202173223639137
      ],
      "excerpt": "  correctly classify features as noncategorical or categorical. Strange \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8956899362119053
      ],
      "excerpt": "  incorrectly. Remember that categorical features can take on one of a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8672617382205682
      ],
      "excerpt": "  'CurrentWeightKG' is a noncategorical feature. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9082317553620031,
        0.9613026883362862,
        0.9111189830458379,
        0.8045740983896029
      ],
      "excerpt": "  time as needed to produce preprocessed data, given the parameters that \n  you set. It is important to ensure that the sum of the prediction \n  horizon and the length of time covered by each time series example is \n  less than the total time over which raw data was collected. For \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8095347334096898
      ],
      "excerpt": "  (found at DATA > TIME_SERIES > TIME_STEP), and input sequence length \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9128402382173685,
        0.8565566098512598
      ],
      "excerpt": "  least 26\u00d77 + 30\u00d76 = 362 days of raw HIFIS data. Note that this is \n  the very minimum - you should have significantly more days of HIFIS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "  data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8596670764521972,
        0.8568602691908475
      ],
      "excerpt": "  results/models/model.h5/{saved_model.pbtxt|saved_model.pb} \n  This common error, experienced when attempting to load model weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9307863776072047
      ],
      "excerpt": "  is set at the PATHS > MODEL_TO_LOAD field of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9252663589696839
      ],
      "excerpt": "  'model.h5' to the filename of a model weights file that exists in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944385157680936
      ],
      "excerpt": "  where yyyymmdd-hhmmss is a datetime. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8413002987999805
      ],
      "excerpt": "  Submodular pick involves generating LIME explanations for a large \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9790266287210199,
        0.8056694070844628
      ],
      "excerpt": "  repository tends to use larger amounts of memory than is required to \n  temporarily store the list of explanations accumulated during a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9655851666249254
      ],
      "excerpt": "  depending on available RAM. The only known fix is to decrease the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8167496467328377,
        0.9598713343362235
      ],
      "excerpt": "  value to 0.2 when running submodular pick on a training set of \n  approximately 90000 records on a virtual machine with 56 GiB of RAM. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9233560562248747
      ],
      "excerpt": "any .gitkeep files, as their only purpose is to force Git to track \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "\u251c\u2500\u2500 data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9814432466297809,
        0.9366362736336832
      ],
      "excerpt": "Many of the components of this project are ready for use on your HIFIS \ndata. However, this project contains several configurable variables that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8447758004870781,
        0.8296280426956326
      ],
      "excerpt": "For user convenience, the config file is organized into major steps in \nour model development pipeline. Many fields need not be modified by the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9519877968745308
      ],
      "excerpt": "goals. A summary of the major configurable elements in this file is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8573281093884906,
        0.9494106979051721
      ],
      "excerpt": "N_WEEKS: The number of weeks in the future the model will be \n  predicting the probability of chronic homelessness (i.e. predictive \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9629988715571126
      ],
      "excerpt": "  state of chronic homelessness) for clients. Set to either 'today' or \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8802858193350928,
        0.9040109885300848
      ],
      "excerpt": "CLIENT_EXCLUSIONS: A list of Client IDs (integers) that specifies \n  clients who did not provide consent to be included in this project. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8704856784774967
      ],
      "excerpt": "  entirely from the model. For us, this list evolved through trial and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9424345957644998
      ],
      "excerpt": "  explanations, we realized that features in the database that should \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8930129419330403
      ],
      "excerpt": "  in some explanations; thus, they were added to this list so that these \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8055872231401632,
        0.9949457065051883,
        0.9741785246305401
      ],
      "excerpt": "  model. Incidentally, this iterative feature engineering using LIME \n  (explainable AI) to identify bad correlations is the foundation of \n  ensuring a machine learning model is free of bias and that its \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9771869615898838,
        0.9502648975571144
      ],
      "excerpt": "IDENTIFYING_FEATURES_TO_DROP_LAST: A list of features that are \n  used to preprocess data but are eventually excluded from the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8555991213358021
      ],
      "excerpt": "  likely have to edit this unless you have additional data features \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9378776339607952,
        0.8859523545529203,
        0.8613487086675287
      ],
      "excerpt": "TIMED_FEATURES_TO_DROP_LAST: A list of features containing dates \n  that are used in preprocessing but are eventually excluded from the \n  preprocessed data. Add any features describing a start or end date to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9419274138016789,
        0.9770051557542023,
        0.9777794583061428
      ],
      "excerpt": "  feature (e.g. 'LifeEventStartDate') and every value is a list of \n  features that are associated with the timestamp. For example, the \n  'LifeEvent' feature is associated with the 'LifeEventStartDate' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.933440357622284
      ],
      "excerpt": "  only one of 'LifeEventStartDate' and 'LifeEventEndDate' as a key, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9514285880692783
      ],
      "excerpt": "TIMED_EVENTS: A list of columns that correspond to significant \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9635429172939546,
        0.9306486217657758
      ],
      "excerpt": "  period of time to include as features. The feature value is calculated \n  by summing the days over which the service is received. Note that the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8258012650637256
      ],
      "excerpt": "  modification of this list. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.921654777919703
      ],
      "excerpt": "  particular time to include as features. The feature value is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8642135565146749
      ],
      "excerpt": "  different, necessitating modification of this list. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8965519892336503,
        0.919972656061905,
        0.8227315309194174,
        0.93280741272823
      ],
      "excerpt": "TIME_SERIES: Parameters associated with time series data \nTIME_STEP: Length of time step in days \nT_X: Length of input sequence length to LSTM layer. Also \n    described as the number of past time steps to include with each \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.910771446251877
      ],
      "excerpt": "FOLDS: Number of folds for nested cross validation with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9495314055707125
      ],
      "excerpt": "SPDAT: Parameters associated with addition of client SPDAT data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "    SPDAT data during preprocessing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.832368967142213,
        0.9757337736563794,
        0.903917070238729,
        0.8827708270090459
      ],
      "excerpt": "    only answers to SPDAT questions in the preprocessed dataset \nHIFIS_MLP: Contains definitions of configurable hyperparameters \n  associated with the HIFIS MLP model architecture. The values currently \n  in this section were the optimal values for our dataset informed by a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9757337736563794,
        0.8534510156023627,
        0.9636729485291621,
        0.9233103436070303
      ],
      "excerpt": "HIFIS_RNN_MLP: Contains definitions of configurable \n  hyperparameters associated with the HIFIS RNN-MLP model architecture. \n  This model is to be trained with time series data. The values \n  currently in this section were the optimal values for our dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8271982307081956
      ],
      "excerpt": "  HIFIS RNN-MLP hybrid model. Also dictates how the raw HIFIS data will \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9969763385670622
      ],
      "excerpt": "TRAIN_SPLIT, VAL_SPLIT, TEST_SPLIT: Fraction of the data allocated \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808172971347457
      ],
      "excerpt": "EPOCHS: Number of epochs to train the model for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865,
        0.87426485254358,
        0.919159111440973,
        0.9042245514686252
      ],
      "excerpt": "  increase recall and decrease precision. \nIMB_STRATEGY: Class imbalancing strategy to employ. In our \n  dataset, the ratio of positive to negative ground truth was very low, \n  prompting the use of these strategies. Set either to 'class_weight', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9481140776086794,
        0.9359589104753755
      ],
      "excerpt": "METRIC_PREFERENCE: A list of metrics in order of importance (from \n  left to right) to guide selection of the best model after training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8432540367405359
      ],
      "excerpt": "NUM_RUNS: The number of times to train a model in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9018574358698532
      ],
      "excerpt": "THRESHOLDS: A single float or list of floats in range [0, 1] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8354549392469336,
        0.8595695243951312,
        0.81796789931563
      ],
      "excerpt": "HP: Parameters associated with random hyperparameter search \nMETRICS: List of metrics on validation set to monitor in \n    hyperparameter search. Can be any combination of {'accuracy', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9342620354083542,
        0.8219906286477904,
        0.8599959353262461
      ],
      "excerpt": "COMBINATIONS: Number of random combinations of hyperparameters \n    to try in hyperparameter search \nREPEATS: Number of times to repeat training per combination of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877832984761874,
        0.9184967628330425
      ],
      "excerpt": "    your ranges are defined correctly as real or discrete intervals (see \n    Random Hyperparameter Search for an \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.875995030307904,
        0.908925214220865
      ],
      "excerpt": "HIFIS_MLP and HIFIS_RNN_MLP architectures: KERNEL_WIDTH, \nFEATURE_SELECTION, NUM_FEATURES, NUM_SAMPLES, and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8795691642995499
      ],
      "excerpt": "- KERNEL_WIDTH: Affects size of neighbourhood around which LIME \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9406025997054608
      ],
      "excerpt": "  within the continuous range of [1.0, 2.0] is large enough to produce \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8949154326290241
      ],
      "excerpt": "  that approach a global surrogate model. This field is numeric; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9736039340095809
      ],
      "excerpt": "- FEATURE_SELECTION: The strategy to select features for LIME \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828868086925022,
        0.9753829684788538
      ],
      "excerpt": "  for more information. \n- NUM_FEATURES: The number of features to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8154104550603786,
        0.9530962064066446,
        0.8864953682043856
      ],
      "excerpt": "- NUM_SAMPLES: The number of samples \n  used to fit a linear model when explaining a prediction using LIME \n- MAX_DISPLAYED_RULES: The maximum number of explanations to be \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8354549392469336,
        0.826371125858538,
        0.9317337610577043,
        0.899496403127125,
        0.8282178042594603,
        0.8888596109159043
      ],
      "excerpt": "- SP: Parameters associated with submodular pick \n  - SAMPLE_FRACTION: A float in the range [0.0, 1.0] that \n    specifies the fraction of samples from the training and validation \n    sets to generate candidate explanations for. Alternatively, set to \n    'all' to sample the entire training and validation sets. \n  - NUM_EXPLANATIONS: The desired number of explanations that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9581934888440857
      ],
      "excerpt": "N_INTERVAL: Size of increment to increase the prediction horizon \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9013931729440021
      ],
      "excerpt": "CLASS_NAMES: Identifiers for the classes predicted by the neural \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8496828035201064,
        0.8698245060809225
      ],
      "excerpt": "N_JOBS: Number of parallel compute jobs to create for \n  k-prototypes. This is useful when N_RUNS is high and you want to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9165605419385903
      ],
      "excerpt": "K_MIN: Minimum value of k to investigate in the Silhouette \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9628089934463608
      ],
      "excerpt": "K_MAX: Maximum value of k to investigate in the Silhouette \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.976203769765285
      ],
      "excerpt": "We deployed our model retraining and batch predictions functionality to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8114227860476569
      ],
      "excerpt": "case they may benefit any parties hoping to use this project. Note that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8813260903587183,
        0.9577252184629269
      ],
      "excerpt": "necessary to get started are in the src/ folder. \nWe deployed our model training and batch predictions functionality to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8758933467333959,
        0.9140940749053917
      ],
      "excerpt": "hoping to use this project. Note that Azure is not required to run \nHIFIS-v2 code, as all Python files necessary to get started are in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057681085526861,
        0.9009614607129621
      ],
      "excerpt": "3. In the Azure portal, create a resource \n   group. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8275636248607501
      ],
      "excerpt": "   created in step 2. When you open your workspace in the portal, there \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8584904157133588
      ],
      "excerpt": "   workspace, which contains confidential information about your \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9939337040840127
      ],
      "excerpt": "   in the HIFIS-v2 repository. For reference, the contents of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Machine learning models for prediction of chronic homelessness using the HIFIS Application.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aildnont/HIFIS-model/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Sun, 26 Dec 2021 23:39:09 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/aildnont/HIFIS-model/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "aildnont/HIFIS-model",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/aildnont/HIFIS-model/master/azure/inference_pipeline.ipynb",
      "https://raw.githubusercontent.com/aildnont/HIFIS-model/master/azure/train_pipeline.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8027042300937105
      ],
      "excerpt": "      config.yml to 'lime_experiment', you will run LIME \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8758539286163938
      ],
      "excerpt": "      config.yml to 'submodular_pick', you will run the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8027042300937105
      ],
      "excerpt": "      config.yml to 'explain_client', you will run LIME \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9249132962004118
      ],
      "excerpt": "      resemble the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8719011934091327
      ],
      "excerpt": "1. Ensure that you have already run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8503047558000603
      ],
      "excerpt": "To run cross validation, see the steps below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9611818486674287
      ],
      "excerpt": "scenarios. By following the steps below, you can cluster clients into a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8719011934091327
      ],
      "excerpt": "1. Ensure that you have already run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8867231269720702
      ],
      "excerpt": "   instructions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8504588748675384
      ],
      "excerpt": "Below are some common error scenarios that you may experience if you \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8536549080531642
      ],
      "excerpt": "  with a filename following the convention 'modelyyyymmdd-hhmmss.h5', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8168552111802723
      ],
      "excerpt": "  accessed. Note that the services offered in your locale may be \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8422574376073271
      ],
      "excerpt": "Note that the following fields have separate values for the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8311091761489756
      ],
      "excerpt": "Azure is not required to run HIFIS-v2 code, as all Python files \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060617423477031
      ],
      "excerpt": "hoping to use this project. Note that Azure is not required to run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9007701977359944
      ],
      "excerpt": "2. Ensure you have installed the azureml-sdk and azureml_widgets pip packages. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8004456081981836
      ],
      "excerpt": "   ws_config.json resemble the following: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8341381590100594,
        0.8274547789566351
      ],
      "excerpt": "Once you have HIFIS_Clients.csv sitting in the raw data folder \n   (data/raw/), execute preprocess.py. See \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9045517034772826,
        0.8092692659191917
      ],
      "excerpt": "Execute train.py. The trained model's weights will be \n   located in results/models/, and its filename will resemble the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8900913293004291
      ],
      "excerpt": "   results/logs/training/, and its directory name will be the current \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8878099947494594
      ],
      "excerpt": "   below for an example of a plot from a TensorBoard log file depicting \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8019596923826664
      ],
      "excerpt": "this model \"HIFIS MLP\", as the model is an example of a multilayer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8091886423076307
      ],
      "excerpt": "   chosen number of training sessions. For example, if you wish to train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8084159981578425
      ],
      "excerpt": "5. Execute train.py. The weights of the model that had \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8610411858311756
      ],
      "excerpt": "   will be located in results/models/training/, and its filename will \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8981930630670489
      ],
      "excerpt": "   in results/logs/training/, and its directory name will be the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8683279118796938
      ],
      "excerpt": "explain the model's predictions on examples in the test set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8651644936868789
      ],
      "excerpt": "   pick, or (iii) run LIME on 1 test set example. Once the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "   lime_explain.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9027903983911619
      ],
      "excerpt": "      on all examples in the test set, create a .csv file of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8164123528085507
      ],
      "excerpt": "      lime_submodular_pick.csv. Subsequent submodular picks will be \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801687884477328,
        0.861264011146863
      ],
      "excerpt": "      client who is in the test set in the main function of \n      lime_explain.py (it \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8261024098898821
      ],
      "excerpt": "   NODES0: [80, 100]               #: Discrete range \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8100891984449521
      ],
      "excerpt": "      BATCH_SIZE: [128, 256]          #: Discrete range \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9496475044008872
      ],
      "excerpt": "   model.py or train.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8283894706268619,
        0.9004399117266336,
        0.8050042032851596
      ],
      "excerpt": "    TRAIN section to 'hparam_search'. \n6. Execute train.py. The experiment's logs will be \n   located in results/logs/hparam_search/, and the directory name will \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8521331658730049
      ],
      "excerpt": "   dashboard of TensorBoard. Each point represents 1 training run. The \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8254548465174459
      ],
      "excerpt": "   lime_explain.py after \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8237388780376618
      ],
      "excerpt": "      explain these predictions. Results will be saved in a .csv file, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8968848908329866
      ],
      "excerpt": "5.  Execute predict.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9111552194302585
      ],
      "excerpt": "3. Execute train.py. A model will be trained for each \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8404585216904911
      ],
      "excerpt": "   in results/experiments/, and its filename will resemble the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8551387596256005
      ],
      "excerpt": "   training the HIFIS-RNN-MLP model, the file will be called \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8732138498764884
      ],
      "excerpt": "2. Run src/horizon_search.py. This may take several minutes to hours, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8254548465174459
      ],
      "excerpt": "   lime_explain.py after \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9364541555520881
      ],
      "excerpt": "4. Run cluster.py. Consult \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8514722639678816
      ],
      "excerpt": "3. Run cluster.py. An image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8233325758396305
      ],
      "excerpt": "total service usage over a timestep as a feature. The input sequence \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289617759281073
      ],
      "excerpt": "into an MLP, whose output is the model's decision. Examples of static \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9528269473629188
      ],
      "excerpt": "- File \"preprocess.py\", line 443, in assemble_time_sequences \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.810485342763105
      ],
      "excerpt": "  results/models/model.h5/{saved_model.pbtxt|saved_model.pb} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.9078689516253233,
        0.8409523427491659
      ],
      "excerpt": "  lime_explain.py, \n  predict.py, or \n  cluster.py. The model weights' path \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8227927808852032
      ],
      "excerpt": "  config.yml. You must change its default value from \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.859964629983037
      ],
      "excerpt": "  value to 0.2 when running submodular pick on a training set of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151270764930737
      ],
      "excerpt": "\u251c\u2500\u2500 src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8417222405266663,
        0.8828951051356521
      ],
      "excerpt": "|   \u251c\u2500\u2500 predict.py                &lt;- Script for prediction on raw data using trained models \n|   \u2514\u2500\u2500 train.py                  &lt;- Script for training model on preprocessed data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8201918259791889
      ],
      "excerpt": "RAW_DATA: Path to .csv file generated by running \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8657396724768318
      ],
      "excerpt": "RAW_SPDAT_DATA: Path to .json file containing client SPDAT data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.855599066113979
      ],
      "excerpt": "  along with ['LifeEvent'] as the associated value. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778487586960795
      ],
      "excerpt": "    example \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.814031721618772
      ],
      "excerpt": "  to train the HIFIS MLP model, or set to 'hifis_rnn_mlp' to train the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8182653584315152
      ],
      "excerpt": "BATCH_SIZE: Mini-batch size during training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8960280287473383
      ],
      "excerpt": "  experiment in train.py) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778487586960795
      ],
      "excerpt": "    example). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8644049654702843
      ],
      "excerpt": "  lime_explain.py. Choices are \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8644049654702843
      ],
      "excerpt": "  cluster.py. Choices are \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327152366707452
      ],
      "excerpt": "   rename it to ws_config.json. Move this file to the azure/ folder \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/aildnont/HIFIS-model/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "PowerShell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 The Corporation of the City of London\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Chronic Homelessness AI (CHAI) Model",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "HIFIS-model",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "aildnont",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aildnont/HIFIS-model/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Sun, 26 Dec 2021 23:39:09 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "machine-learning",
      "machine-learning-algorithms",
      "homelessness",
      "artificial-intelligence",
      "rnn"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Clone this repository (for help see this\n   [tutorial](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository)).\n2. Install the necessary dependencies (listed in\n   [requirements.txt](requirements.txt)). To do this, open a terminal in\n   the root directory of the project and run the following:\n   ```\n   $ pip install -r requirements.txt\n   ```\n3. Open [_retrieve_raw_data.ps1_](retrieve_raw_data.ps1) for\n   editing. Replace \"_[Instance Name goes here]_\" with your HIFIS\n   database instance name. Execute\n   [_retrieve_raw_data.ps1_](retrieve_raw_data.ps1). A file\n   named _\"HIFIS_Clients.csv\"_ should now be within the _data/raw/_\n   folder. See\n   [HIFIS_Clients_example.csv](data/raw/HIFIS_Clients_example.csv) for\n   an example of the column names in our _\"HIFIS_Clients.csv\"_ (note\n   that the data is fabricated; this file is included for illustrative\n   purposes).\n4. Check that your features in _HIFIS_Clients.csv_ match in\n   [config.yml](config.yml). If necessary, update feature\n   classifications in this file (for help see\n   [Project Config](#project-config)).\n5. Execute [_preprocess.py_](src/data/preprocess.py) to transform the\n   data into the format required by the machine learning model.\n   Preprocessed data will be saved within _data/preprocessed/_.\n6. Execute [_train.py_](src/train.py) to train the neural network model\n   on your preprocessed data. The trained model weights will be saved\n   within _results/models/_, and its filename will resemble the\n   following structure: modelyyyymmdd-hhmmss.h5, where yyyymmdd-hhmmss\n   is the current time. The\n   [TensorBoard](https://www.tensorflow.org/tensorboard) log files will\n   be saved within _results/logs/training/_.\n7. In [config.yml](config.yml), set _MODEL_TO_LOAD_ within _PATHS_ to\n   the path of the model weights file that was generated in step 6 (for help see\n   [Project Config](#project-config)).\n   Execute [_lime_explain.py_](src/interpretability/lime_explain.py) to\n   generate interpretable explanations for the model's predictions on\n   the test set. A spreadsheet of predictions and explanations will be\n   saved within _results/experiments/_.\n\n![alt text](documents/readme_images/workflow_summary.png \"Flow Chart of\nHIFIS Model Workflow Summary\")\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. In [config.yml](config.yml), set _MODEL_DEF_ within _TRAIN_ to\n   _'hifis_rnn_mlp'_.\n2. Follow steps 1-4 in\n   _[Train a model and visualize results](#train-a-model-and-visualize-results)_\n   to preprocess the raw data and train a model.\n3. See the steps in _[LIME Explanations](#lime-explanations)_ for\n   instructions on how to run different explainability experiments. If\n   you wish to explain a single client, be sure that you pass a value\n   for the _date_ parameter to _explain_single_client()_ in the\n   _yyyy-mm-dd_ format.\n\n",
      "technique": "Header extraction"
    }
  ]
}