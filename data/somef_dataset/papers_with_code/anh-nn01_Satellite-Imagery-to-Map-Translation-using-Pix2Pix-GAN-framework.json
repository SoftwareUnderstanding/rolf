{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1611.07004<br>\n**Sample:**<br><br>\n<img src=\"Visualization/8.png\"><br><br>\n\n# Trained Generator and Discriminator:<br>\n* Click this link to download the trained weights for the Sat2Map Generator and Discriminator: [Download Weights](https://drive.google.com/file/d/1vvv2dXL98_M4SrjUgGps2vt1FzGRKH7B/view?usp=sharing",
      "https://arxiv.org/abs/1505.04597\n* Instead of mapping a random noise to an image, the Generator maps an image to another representation of the same image. This is image translation task, and that's why the framework was named \"Pix2Pix\".\n* The neural architecture that the Generator uses is the U-Net, which is basically Encoder-Decoder with skip connections between layers in the Encoder and corresponding layers in the Decoder.\n* Architecture of the U-Net:<br>\n<img src=\"Visualization/2.png\"><br>\n* The architecture above is the U-Net 572. What I used in this project is U-Net 256. The only difference is the input image size and the output image size.\n* Function: \n  - The Encoder is the convolutional layers to the left of the network. The role of those layers is to extract core features of the image and map those features to the bottlekneck latent space at the middle of the network (an 1024-D array"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9965271881743462
      ],
      "excerpt": "<b>Reference</b>: https://arxiv.org/abs/1611.07004<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9912556242864674
      ],
      "excerpt": "Reference: https://arxiv.org/abs/1505.04597 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/anh-nn01/Satellite-Imagery-to-Map-Translation-using-Pix2Pix-GAN-framework",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-12T14:58:24Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-03T05:17:32Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9268125260919812
      ],
      "excerpt": "PyTorch Implementation of Pix2Pix framework from scratch to train a U-Net with Generative Adversarial Network which translates Satellite Imagery to an equivalent Map.<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9466701564906066
      ],
      "excerpt": "As suggested by the paper in the reference, here are the values of the hyper-parameters to train the Sat2Map model:</br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9094287869869263,
        0.9968677257915004,
        0.9749907145583022
      ],
      "excerpt": "Original GAN trains the Generator to map a random noise z to the output image G(z) which should look as realistic as possible and trains the Discriminator to recognize the input image x as real/fake image based solely on the input image x. \nSince GAN model learns to generate image G(z) based only on a random noise z, and since we cannot easily control the distribution of z, it is often difficult to control the output according to our desire. GAN model only ensures that G(z) is realistic, but not necessarily matches our expectation. For instance, GAN Generator can be trained to map a random noise to a very realistic human face, but we cannot control the Generator to generate a white human face which look like Emma Watson. \ncGAN solves this problem by taking both the random noise z and the input image x to produce an output image G(z|x) which looks realistic and corresponds to x. Since we can control which input image x is fed into the network, we can control what the output image G(z|x) will be like. The Discriminator is trained so that D(y|x)=1 if y is BOTH realistic and correspond to x; on the other hand, D(y|x)=0 if y is either unrealistic or unrelated to input x or neither. This forces the Generator to learn to generate output G(z|x) that is both realistic and capture the overall distribution of input x and makes the image translation task possible. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9811362770892932,
        0.9298577351553188,
        0.8178027976700971
      ],
      "excerpt": "Instead of mapping a random noise to an image, the Generator maps an image to another representation of the same image. This is image translation task, and that's why the framework was named \"Pix2Pix\". \nThe neural architecture that the Generator uses is the U-Net, which is basically Encoder-Decoder with skip connections between layers in the Encoder and corresponding layers in the Decoder. \nArchitecture of the U-Net:<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9704087126995409
      ],
      "excerpt": "The architecture above is the U-Net 572. What I used in this project is U-Net 256. The only difference is the input image size and the output image size. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9976298605502845,
        0.981679729154503,
        0.9090073637203663
      ],
      "excerpt": "The Encoder is the convolutional layers to the left of the network. The role of those layers is to extract core features of the image and map those features to the bottlekneck latent space at the middle of the network (an 1024-D array). This latent space is the encoded form which contains the most important information of the input image. \nThe Decoder is the transpose convolutional layers to the right of the network. Those layers map encoded latent space of the image back to a full-size image. Those layers do not simply output the same input image, but they are trained to map the encoded features to an image with another representation. For instance, the Encoder encodes the information of a Dog photo, and then the Decoder maps the encoded information of the dog to a drawing of the same dog. Both input and output have the same information: a dog, but they have different representation: a photo and a drawing.  \nTo make training more stable, extracted information from the Encoder network was concantenated to corresponding layers in the Decoder network. This ensures that the Decoder has sufficient information to map the latent space to a realistic image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9139932050727129,
        0.9936195058706394
      ],
      "excerpt": "In Pix2Pix and in cGAN, Discriminator is still a binary Convolutional Neural Network. \nThe difference between Discriminator in Pix2Pix with that in the original GAN is that the Discriminator in Pix2Pix not only takes the examined image y but also the conditional image x as the inputs. In other words, x was concatenated to y as an input before feeding into the network, and the input now have 6 channels (3 for the examined image and 3 for the conditional image) instead of 3 channels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9460475647476398
      ],
      "excerpt": "In Image translation task, the GAN training scheme is almost the same as the original GAN, except now we have conditional input and an additional L1 loss to ensure the generated image is not too different from the expected output. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9654494543947579
      ],
      "excerpt": "Just like original GAN, optimizing this Loss will forces the Generator to produce results with overall distribution close to that of the image representation in the dataset and thus improve the structural quality of the Generator's output.<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356067640524845
      ],
      "excerpt": "By using pixel-wise loss between 2 images, this loss forces the output image to be as close to the expected output as possible. In other words, it improves the minor details of the output.<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8227191645759983,
        0.882857139766553
      ],
      "excerpt": "We simply combine GAN loss and L1 Loss to have the final Loss for the entire algorithm. \nThe image patch with 24 samples below shows the results of the Sat2Map Generator.<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch Implementation of Pix2Pix framework to train a U-Net with Generative Adversarial Network to map Satellite Imagery to an equivalent Map.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/anh-nn01/Satellite-Imagery-to-Map-Translation-using-Pix2Pix-GAN-framework/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Fri, 24 Dec 2021 06:32:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/anh-nn01/Satellite-Imagery-to-Map-Translation-using-Pix2Pix-GAN-framework/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "anh-nn01/Satellite-Imagery-to-Map-Translation-using-Pix2Pix-GAN-framework",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/anh-nn01/Satellite-Imagery-to-Map-Translation-using-Pix2Pix-GAN-framework/main/Pix2Pix.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8233517200089654
      ],
      "excerpt": "Dataset: Download Sat2Map Dataset \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/anh-nn01/Satellite-Imagery-to-Map-Translation-using-Pix2Pix-GAN-framework/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Satellite-Imagery-to-Map-Translation-using-Pix2Pix-GAN-framework",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Satellite-Imagery-to-Map-Translation-using-Pix2Pix-GAN-framework",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "anh-nn01",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/anh-nn01/Satellite-Imagery-to-Map-Translation-using-Pix2Pix-GAN-framework/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10,
      "date": "Fri, 24 Dec 2021 06:32:14 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pix2pix-gan-framework",
      "satellite-imagery",
      "gan-model",
      "gan",
      "deep-learning",
      "pytorch",
      "pix2pix",
      "generator",
      "image-translation"
    ],
    "technique": "GitHub API"
  }
}