{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1909.05858",
      "https://arxiv.org/abs/1909.05858"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this work helpful, please cite our publication [Aspect-Controlled Neural Argument Generation](https://aclanthology.org/2021.naacl-main.34/):\n\n    @inproceedings{schiller-etal-2021-aspect,\n    title = \"Aspect-Controlled Neural Argument Generation\",\n    author = \"Schiller, Benjamin  and\n      Daxenberger, Johannes  and\n      Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.34\",\n    doi = \"10.18653/v1/2021.naacl-main.34\",\n    pages = \"380--396\",\n    abstract = \"We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we present the Arg-CTRL - a language model for argument generation that can be controlled to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that the Arg-CTRL is able to generate high-quality, aspect-specific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL.\",\n    }\n\nContact person: [Benjamin Schiller](mailto:schiller@ukp.informatik.tu-darmstadt.de)\n\nhttps://www.ukp.tu-darmstadt.de/\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{schiller-etal-2021-aspect,\ntitle = \"Aspect-Controlled Neural Argument Generation\",\nauthor = \"Schiller, Benjamin  and\n  Daxenberger, Johannes  and\n  Gurevych, Iryna\",\nbooktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\nmonth = jun,\nyear = \"2021\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2021.naacl-main.34\",\ndoi = \"10.18653/v1/2021.naacl-main.34\",\npages = \"380--396\",\nabstract = \"We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we present the Arg-CTRL - a language model for argument generation that can be controlled to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that the Arg-CTRL is able to generate high-quality, aspect-specific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL.\",\n}",
      "technique": "Regular expression"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/UKPLab/controlled-argument-generation/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/UKPLab/controlled-argument-generation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-28T08:09:52Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-05T08:48:41Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "_Accompanying repository for the paper [Aspect-Controlled Neural Argument Generation](https://aclanthology.org/2021.naacl-main.34/)._\n\nWe rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing \nin turn. However, finding and formulating arguments can be challenging. \nTo tackle this challenge, we trained a language model (based on the CTRL by \n[Keskar et al. (2019)](https://arxiv.org/abs/1909.05858)) for argument generation that can be controlled on a fine-grained \nlevel to generate sentence-level arguments for a given topic, stance, and aspect. \nWe define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset \nwith 5,032 arguments annotated with aspects. We release this dataset, as well as the training data for the argument generation\nmodel, its weights, and the arguments generated with the model.\n\nThe following figure shows how the argument generation model was trained:\n\n![Image description](arg_gen_pipeline.png)\n\n(1) We gather several million documents for eight different topics from two large data sources. \nAll sentences are classified into pro-, con-, and non-arguments. We detect aspects of all arguments with a model trained\non a novel dataset and concatenate arguments with the same topic, stance, and aspect into training documents. \n(2) We use the collected classified data to condition the CTRL model\non the topics, stances, and aspects of all gathered arguments. (3) At inference, passing the control code \n_[Topic]_ _[Stance]_ _[Aspect]_ will generate an argument that follows these commands.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9045686032854446,
        0.9060694674125904,
        0.9463118523736049
      ],
      "excerpt": "We have added the code for the aspect-controlled neural argument generation model and detailled descriptions on how \nto use it. The model and code modifies the work by Keskar et al. (2019).  \nThe link to the fine-tuned model weights and training data can be found in the Downloads section. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164926999270088,
        0.8211464027062695
      ],
      "excerpt": "to reproduce the fine-tuning of the argument generation model (reddit_training_data.7z, cc_training_data.7z). \nNote: Due to license reasons, these files cannot be distributed freely. Clicking on any of the files will redirect you to a form, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9022639030497076
      ],
      "excerpt": "As a first step, training documents for a topic of interest need to be gathered. (Note: This step is not part of the code \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906436683940062,
        0.8887629566754438,
        0.900189025201717,
        0.9647219021621171,
        0.961116417542481,
        0.8966230765994376
      ],
      "excerpt": "Common-Crawl and Reddit-comments and indexing  \nthem with ElasticSearch. The outcome needs to be documents  \nthat are stored at training_data/[INDEX_NAME]/[TOPIC_NAME]/unprocessed/, where [INDEX_NAME] is the name of the  \ndata source (e.g. common-crawl-en) and [TOPIC_NAME] is the search topic for which documents were gathered  \n(replace whitespaces in the [INDEX_NAME] and [TOPIC_NAME] with underscores). \nEach document is a separate JSON-File with at least the key \"sents\" which holds a list of sentences from this document: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8796341986814645
      ],
      "excerpt": "The argument_classification.py takes all documents gathered for a given topic and classifies their sentences into \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9092508499167093
      ],
      "excerpt": "Non-arguments are discarded and the final classified arguments are stored into files with a maximum of 200,000 arguments each at  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8162664993376209
      ],
      "excerpt": "All arguments with their aspects are then stored into a single file merged.jsonl at \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8711377985138441
      ],
      "excerpt": "and the models described in the corresponding publication by Stab et al. (2018). For better \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9286038054614453
      ],
      "excerpt": "(due to license reasons, it is necessary to fill the form with your name and email). As a model, we suggest  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Controlling Argument Generation via topic, stance, and aspect",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Download datasets from [here](https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2329). You can download the following files:\n    \n    - _argument_aspect_detection_v1.0.7z_: The argument aspect detection dataset\n    - _generated_arguments.7z_: All arguments generated with our fine-tuned models\n    - _reddit_training_data.7z_: Holds classified samples used to fine-tune our model based on [Reddit-comments](https://files.pushshift.io/reddit/comments/) data\n    - _cc_training_data.7z_: Holds classified samples used to fine-tune our model based on [Common-Crawl](https://commoncrawl.org/) data\n\n    _Note_: Due to license reasons, these files cannot be distributed freely. Clicking on any of the files will redirect you to a form,\n    where you have to leave you name and email. After submitting the form, you will receive a download link shortly.\n\n- Use _scripts/download_weights.sh_ to download the model weights. The script will download the weights for the model\nfine-tuned on [Reddit-comments](https://files.pushshift.io/reddit/comments/) and [Common-Crawl](https://commoncrawl.org/) data\nand unzips them into the main project folder. \n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/UKPLab/controlled-argument-generation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 16:47:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/UKPLab/controlled-argument-generation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "UKPLab/controlled-argument-generation",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/UKPLab/controlled-argument-generation/master/scripts/download_weights.sh",
      "https://raw.githubusercontent.com/UKPLab/controlled-argument-generation/master/training_utils/pipeline/prepare_documents_all.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In order to prepare training documents and fine-tune the model, you can use the _prepare_documents.py_ as described in \n[I. Use our pipeline (with ArgumenText API), step d.](#d-prepare-training-documents)\n if you keep your classified data in the following format:\n\n- The file should be named _merged.jsonl_ and located in the directory _training_data/[INDEX_NAME]/[TOPIC_NAME]/processed/_, \nwhere [INDEX_NAME] is the data source from where the samples were gathered and [TOPIC_NAME] the name of the respective\nsearch query for this data.\n- Each line represents a training sample in the following format:\n    \n        {\"id\": id of the sample, starting with 0 (int), \"stance\": \"Argument_against\" or \"Argument_for\", depending on the stance (string), \"sent\": The argument sentence (string), \"aspect_string\": A list of aspects for this argument (list of string)}   \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The _prepare_documents.py_ appends all arguments that have the same topic, stance, and (stemmed) aspect to a training document:\n\n     python prepare_documents.py --max_sents [MAX_SENTS] --topic [TOPIC_NAME] --index [INDEX_NAME] --max_aspect_cluster_size [MAX_ASPECT_CLUSTER_SIZE] --min_aspect_cluster_size [MIN_ASPECT_CLUSTER_SIZE]\n\n[MAX_SENTS] sets the maximum number of arguments to use (evenly devided between pro and con arguments if possible)\nand [MIN_ASPECT_CLUSTER_SIZE]/[MAX_ASPECT_CLUSTER_SIZE] sets the min/max number of allowed arguments to append for a \nsingle training document. The final documents are stored in folder _training_data/[INDEX_NAME]/[TOPIC_NAME]/final/_. \nThe script _prepare_all_documents.sh_ can be used to automate the process.\n\nFinally, to create training sequences from the documents and start fine-tuning the model, please download our fine-tuned \nweights (see [Download section](#downloads)) and follow \n[B. Use given training data to reproduce/fine-tune the model](#b-use-given-training-data-to-reproducefine-tune-the-model),\n Steps 4-5.\n\n_IMPORTANT_: In addition to the training documents, a file with all control codes based on the training \ndocuments is created at _training_data/[INDEX_NAME]/[TOPIC_NAME]/generation_data/control_codes.jsonl_. This file holds\nall control codes to generate arguments from after fine-tuning has finished. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The code was tested with `Python3.6`. Install all requirements with \n    \n    pip install -r requirements.txt \n    \nand follow the instructions in the [original Readme at _Usage_](README_original.md#usage), _Step 1 and 2_.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8012039891755844
      ],
      "excerpt": "The Argument Aspect Detection dataset can be downloaded from here \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811643849807129
      ],
      "excerpt": "pro-/con-/non-Arguments. The following command starts the classification: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8459663302331403
      ],
      "excerpt": "The following command starts the aspect detection: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8663601744308138
      ],
      "excerpt": "python argument_classification.py --topic [TOPIC_NAME] --index [TOPIC_NAME] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8663601744308138
      ],
      "excerpt": "python aspect_detection.py --topic [TOPIC_NAME] --index [TOPIC_NAME] \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/UKPLab/controlled-argument-generation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Aspect-Controlled Neural Argument Generation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "controlled-argument-generation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "UKPLab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/UKPLab/controlled-argument-generation/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 15,
      "date": "Fri, 24 Dec 2021 16:47:33 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "argument-generation",
      "argument-mining",
      "language-model",
      "aspects",
      "aspect-detection"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In the following, we describe three approaches to use the aspect-controlled neural argument generation model:\n\n[A. Use model for generation only](#a-use-model-for-generation-only)\n\n[B. Use available training data to reproduce/fine-tune the model](#b-use-given-training-data-to-reproducefine-tune-the-model)\n\n[C. Use your own data to fine-tune a new aspect-controlled neural argument generation model](#c-use-your-own-data-to-fine-tune-a-new-aspect-controlled-neural-argument-generation-model)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "In order to generate arguments, please first download the weights for the models (download script at _scripts/download_weights.sh_). \nRun the model via `python generation.py --model_dir reddit_seqlen256_v1` for the model trained\non Reddit-comments data or `python generation.py --model_dir cc_seqlen256_v1` for the model trained on Common-Crawl data. \nAfter loading is complete, type in a control code, for example `nuclear energy CON waste`, to generate arguments \nthat follow this control code. To get better results for the first generated argument, you can end the control code with \na period or colon (\".\" or \":\"). For more details, please refer to the paper. \n\n_Note_: Allowed control codes for each topic and data source can be found in the _training_data_ folder.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "In order to fine-tune the model as we have done in our work, please follow these steps:\n\n1. Download the pre-trained weights from the original paper \n([original Readme at _Usage_](README_original.md#usage), _Step 3_). \n2. Download the training data (see [Downloads section](#downloads). You need the file _reddit_training_data.7z_ or _cc_training_data.7z_. \nDepending on the source (cc or reddit), put the archives either\ninto folder _training_data/common-crawl-en/_ or _training_data/redditcomments-en/_  and unzip via:\n\n        7za x [FILENAME].7z\n\n3. To reproduce the same training documents from the training data as we used for fine-tuning, please use the script at\n_training_utils/pipeline/prepare_documents_all.sh_ and adapt the _INDEX_ parameter.\nDepending on your hardware, the training document generation can take an hour or more to compute.\n\n4. Lastly, TFRecords need to be generated from all training documents. To do so, please run:\n\n        python make_tf_records_multitag.py --files_folder [FOLDER] --sequence_len 256\n        \n     [FOLDER] needs to point to the folder of the training documents, \n     e.g. _training_data/common-crawl-en/abortion/final/_. After generating, the number of\n     training sequences generated for this specific topic is printed. Use this to determine the\n     number of steps the model should be trained on. The TFRecords are stored in folder _training_utils_.\n\n5. Train the model:\n\n        python training.py --model_dir [WEIGHTS FOLDER] --iterations [NUMBER OF TRAINING STEPS]\n        \n    The model takes the generated TFRecords automatically from the _training_utils_ folder.\n    Please note that the weights in [WEIGHTS FOLDER] will be overwritten. For generation with\n    the newly fine-tuned model, follow the instructions in \"_A. Use model for generation only_\".\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To ease the process of gathering your own training data, we add our implementation of the pipeline described\nin our publication (see [I. Use our pipeline (with ArgumenText API)](#i-use-our-pipeline-with-argumentext-api)). To label sentences as arguments and to identify their stances and aspects, we\nuse the [ArgumenText-API](https://api.argumentsearch.com). Alternatively, you can also train your own models \n(see [II. Create your own pipeline (without ArgumenText API)](#ii-create-your-own-pipeline-without-argumentext-api)).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Please [request](https://api.argumentsearch.com/en/api_registration) a userID and apiKey for the \n[ArgumenText-API](https://api.argumentsearch.com). Write both id and key in the respective\nconstants at _training_utils/pipeline/credentials.py_.\n\n",
      "technique": "Header extraction"
    }
  ]
}