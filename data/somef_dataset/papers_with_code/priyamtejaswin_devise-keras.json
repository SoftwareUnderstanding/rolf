{
  "citation": [
    {
      "confidence": [
        0.9946078828789969,
        0.9946078828789969
      ],
      "excerpt": "<a href=\"https://imgur.com/bLsYxMr\"><img src=\"https://i.imgur.com/bLsYxMr.gif\" title=\"source: imgur.com\" width=\"60%\" /></a> \n<a href=\"https://imgur.com/kBBHc8X\"><img src=\"https://i.imgur.com/kBBHc8X.gif\" title=\"source: imgur.com\" width=\"60%\" /></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "\u2502   \u251c\u2500\u2500 jquery-1.12.4.js \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/priyamtejaswin/devise-keras",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-04-01T12:31:56Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-06T20:29:18Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project extends the original [Google DeViSE](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf) paper to create a functioning image search engine with a focus on interpreting search results. We have extended the original paper in the following ways. First, we added an RNN to process variable length queries as opposed to single words. Next, to understand how the network responds to different parts of the query(like noun phrases) and the image, we leverage [Ribeiro et.al's LIME](https://arxiv.org/pdf/1602.04938v1.pdf) for model-agnostic interpretability. It has been tested on subsets of the [UIUC-PASCAL dataset](http://vision.cs.uiuc.edu/pascal-sentences/) and the final network has been trained on the [MSCOCO 2014 dataset](http://cocodataset.org/#home).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8192505441185383,
        0.8112076498761489
      ],
      "excerpt": "- by Priyam Tejaswin and Akshay Chawla \nLIVE WEB DEMO AT: http://35.227.39.159:5050/  . This will forward to a GCP instance endpoint. If you're having issues accessing it from your internal work or office network, please raise an issue or contact Priyam, Akshay. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9866875150047507
      ],
      "excerpt": "LIME is the main deployment branch for this project. The code is organised as follows. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9938561102280534,
        0.9622704743030742,
        0.8979100867634053,
        0.9841905503322492,
        0.832675799821227,
        0.9754258853995467,
        0.979309670980051,
        0.9456981963781951,
        0.9714875136794932,
        0.9894286850029101,
        0.976146545844139,
        0.9641163482375983,
        0.9393388390825714,
        0.9808114424448505
      ],
      "excerpt": "master: This repository houses code to replicate the base DeViSE paper. Since the project - and it\u2019s scope - has grown organically as we advanced, we decided to branch out and leave the vanilla DeViSE code base intact. The master branch contains code to setup the experiment, download and pre-process data for implementing the paper. model.py contains the code for the model. Due to computational constraints, the experiments are run on the UIUC-PASCAL sentences dataset as opposed to ImageNet. This dataset contains 50 images per category for 20 categories along with 5 captions per image. These captions are used for the extension of the project in the devise-rnn branch. \ndevise-rnn: This branch is an extension of the master codebase. rnn_model.py contains the code for the extended model. Due to computational constraints, the experiments are run on the UIUC PASCAL sentences dataset. This dataset contains 50 images per category along with 5 captions per image. These captions are used in the same pairwise loss function to learn an image search model entirely from annotated images. \nmscoco-search: This branch extends devise-rnn. It contains updates and changes for training the model on the MSCOCO 2014 dataset. \nui: This branch contains code for building the user-interface of the search engine and the backend for running the model. \nLIME: This branch extends mscoco-search and includes a frontend from ui. Additionally, it adds interpretability modules. \nIn this paper, the authors present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. They accomplish this by minimizing a combination of the cosine similarity and hinge rank loss between the embedding vectors learned by the language model and the vectors from the core visual model as shown below.  \nIn the interest of time, we did not train a skip-gram model ourselves but chose to use the GloVe (Global Vectors for Word Representation) model from the stanford NLP group as our initilization. \nIn order to encode variable length captions in the language model, we used an RNN network consisting of an Embedding input layer and 2 LSTM cells with 300 hidden units. This gave us an output vector that can be used for computing the similarity metric mentioned before.  \nThis allows us to map images and their captions to a common semantic embedding space. We use this feature to search for images in the embedding space that are close to the query entered by a user. \nIn order to explain the relevance of our results, we modified Ribeiro et al's LIME such that it highlights salient regions relevant to the user's query. This gives visual cues about the regions in an image which maximally contributed to its retrieval. \nWe deployed our work as an image search engine by building html, css and js components. Concretely, we run a server in the background that communicates with a frontend ui that displays the search results and lime saliency regions.  \nThe user enters a search query which is communicated to the server. The server runs the query string through the trained RNN model to find its final state vector. We search for the top 10 images closest to that query in the embedding space and return the links to those images.  \nOnce the retrieved images have been displayed on the webpage, we request the server to extract appropriate noun and verb phrases using a dependancy parser. These phrases are displayed as button on the webpage. We also request the server to fetch salient regions for each phrase and each returned image. Selecting a phrase button will highlight its approprate region in all images.  \nNOTE: Calculating LIME results for each (query, images) tuple requires ~3 hours as each phrase has to be run against every image retrieved. Hence, in the interest of time (and the limitations of having 1/0 GPUs) we pre-cache the LIME results for some sample queries. These sample queries can be accessed via clicking on the drop-down menu which appears when the user clicks on the search box. While LIME results are available only for a limited set of queries, the search and retrieval sans lime works for all queries, provided the input tokes are present in our dictionary.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Interpretable Image Search by Priyam Tejaswin and Akshay Chawla",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/priyamtejaswin/devise-keras/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Mon, 27 Dec 2021 00:49:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/priyamtejaswin/devise-keras/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "priyamtejaswin/devise-keras",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/priyamtejaswin/devise-keras/LIME/gpu1.sh",
      "https://raw.githubusercontent.com/priyamtejaswin/devise-keras/LIME/gpu0.sh",
      "https://raw.githubusercontent.com/priyamtejaswin/devise-keras/LIME/gpu2.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9023697225149864,
        0.9023697225149864,
        0.9023697225149864
      ],
      "excerpt": "\u251c\u2500\u2500 gpu0.sh \n\u251c\u2500\u2500 gpu1.sh \n\u251c\u2500\u2500 gpu2.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9033987252512259
      ],
      "excerpt": "\u251c\u2500\u2500 requirements.txt \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991,
        0.9336801098518991,
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "\u251c\u2500\u2500 build_cache.py \n\u251c\u2500\u2500 cache_lime.py \n\u251c\u2500\u2500 complete_model.py \n\u251c\u2500\u2500 contour_utils.py \n\u251c\u2500\u2500 extract_features_and_dump.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "\u251c\u2500\u2500 nlp_stuff.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "\u251c\u2500\u2500 rnn_model.py \n\u251c\u2500\u2500 server_lime_contours.py \n\u251c\u2500\u2500 server_nolime.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "\u251c\u2500\u2500 simplified_complete_model.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "\u251c\u2500\u2500 tensorboard_logging.py \n\u2514\u2500\u2500 validation_script.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/priyamtejaswin/devise-keras/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "JavaScript",
      "HTML",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Interpretable Image Search",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "devise-keras",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "priyamtejaswin",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/priyamtejaswin/devise-keras/blob/LIME/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. First download these required files and place them in a folder called devise_cache. \n\t1. vgg16 pre-trained weights: https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels.h5\n\t2. word index mappings: https://www.dropbox.com/s/h9m7ju42sckehy5/DICT_word_index.VAL.pkl?dl=0\n\t3. Pre-trained DeVISE weights: https://www.dropbox.com/s/7lsubnf9fna7kun/epoch_13.hdf5?dl=0\n\t4. MS COCO captions: http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n\t5. cache.h5: https://www.dropbox.com/s/xoza70y5zyh5d99/cache.h5?dl=0\n\n2. Clone this repository to your local system. \n3. Extract epoch_9_cache. You will get all LIME images for epoch_9 in all_images_epoch_9.tar.gz.\n4. Copy the tar to /devise-keras/static/overlays_cache/ and untar.\n5. Run the server using: \n\n```\npython server_lime_contours.py \\\n--word_index=/path/to/devise_cache/DICT_word_index.VAL.pkl \\\n--cache=/path/to/devise_cache/cache.h5 \\\n--model=/path/to/devise_cache/epoch_9.hdf5 \\\n--threaded=0 \\\n--host=127.0.0.1 \\\n--port=5000 \\\n--dummy=0 \\\n--captions_train=/path/to/devise_cache/annotations/captions_train2014.json \\\n--captions_valid=/path/to/devise_cache/annotations/captions_val2014.json \\\n--vgg16=/path/to/devise_cache/vgg16_weights_th_dim_ordering_th_kernels.h5\n```\n\n\n4. Be careful to replace /path/to/devise_cache/ to the correct path to your devise_cache folder.\n5.  Open a modern web browser (we tested this on firefox quantum 57) and navigate to localhost:5000 to view the webpage.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 23,
      "date": "Mon, 27 Dec 2021 00:49:23 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "machine-learning",
      "interpretable-deep-learning",
      "image-search"
    ],
    "technique": "GitHub API"
  }
}