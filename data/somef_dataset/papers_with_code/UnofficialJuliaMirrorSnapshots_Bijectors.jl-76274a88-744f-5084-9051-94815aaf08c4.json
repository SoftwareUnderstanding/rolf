{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1505.05770v6",
      "https://arxiv.org/abs/1603.00788v1",
      "https://arxiv.org/abs/1505.05770](https://arxiv.org/abs/1505.05770v6).\n2. Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., & Blei, D. M. (2016). Automatic Differentiation Variational Inference. [https://arxiv.org/abs/1603.00788](https://arxiv.org/abs/1603.00788v1).",
      "https://arxiv.org/abs/1603.00788](https://arxiv.org/abs/1603.00788v1)."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Most of the methods and types mention below will have docstrings with more elaborate explanation and examples, e.g.\n```julia\nhelp?> Bijectors.Composed\n  Composed(ts::A)\n  \n  \u2218(b1::Bijector{N}, b2::Bijector{N})::Composed{<:Tuple}\n  composel(ts::Bijector{N}...)::Composed{<:Tuple}\n  composer(ts::Bijector{N}...)::Composed{<:Tuple}\n\n  where A refers to either\n\n    \u2022    Tuple{Vararg{<:Bijector{N}}}: a tuple of bijectors of dimensionality N\n\n    \u2022    AbstractArray{<:Bijector{N}}: an array of bijectors of dimensionality N\n\n  A Bijector representing composition of bijectors. composel and composer results in a Composed for which application occurs from left-to-right and right-to-left, respectively.\n\n  Note that all the alternative ways of constructing a Composed returns a Tuple of bijectors. This ensures type-stability of implementations of all relating methdos, e.g. inv.\n\n  If you want to use an Array as the container instead you can do\n\n  Composed([b1, b2, ...])\n\n  In general this is not advised since you lose type-stability, but there might be cases where this is desired, e.g. if you have a insanely large number of bijectors to compose.\n\n  Examples\n  \u2261\u2261\u2261\u2261\u2261\u2261\u2261\u2261\u2261\u2261\n\n  It's important to note that \u2218 does what is expected mathematically, which means that the bijectors are applied to the input right-to-left, e.g. first applying b2 and then b1:\n\n  (b1 \u2218 b2)(x) == b1(b2(x))     #: => true\n\n  But in the Composed struct itself, we store the bijectors left-to-right, so that\n\n  cb1 = b1 \u2218 b2                  #: => Composed.ts == (b2, b1)\n  cb2 = composel(b2, b1)         #: => Composed.ts == (b2, b1)\n  cb1(x) == cb2(x) == b1(b2(x))  #: => true\n```\nIf anything is lacking or not clear in docstrings, feel free to open an issue or PR.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "struct Identity{N} <: Bijector{N} end \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "       end \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8273283539786315
      ],
      "excerpt": "  -PlanarLayer: \u00a74.1 Eq. (10) in [1] \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/UnofficialJuliaMirrorSnapshots/Bijectors.jl-76274a88-744f-5084-9051-94815aaf08c4",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-07T19:16:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-20T10:17:54Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.971950429939275
      ],
      "excerpt": "This package implements a set of functions for transforming constrained random variables (e.g. simplexes, intervals) to Euclidean space. The 3 main functions implemented in this package are the link, invlink and logpdf_with_trans for a number of distributions. The distributions supported are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "5. PDMatDistribution: Union{InverseWishart, Wishart}, and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9431474177729995,
        0.9961976224581671,
        0.9787281885785365,
        0.9917061938089939
      ],
      "excerpt": "A Bijector is a differentiable bijection with a differentiable inverse. That's basically it. \nThe primary application of Bijectors is the (very profitable) business of transforming (usually continuous) probability densities. If we transfrom a random variable x ~ p(x) to y = b(x) where b is a Bijector, we also get a canonical density q(y) = p(b\u207b\u00b9(y)) |det J(b\u207b\u00b9, y)| for y. Here J(b\u207b\u00b9, y) is the jacobian of the inverse transform evaluated at y. q is also known as the push-forward of p by b in measure theory. \nThere's plenty of different reasons why one would want to do something like this. It can be because your p has non-zero probability (support) on a closed interval [a, b] and you want to use AD without having to worry about reaching the boundary. E.g. Beta has support [0, 1] so if we could transform p = Beta into a density q with support on \u211d, we could instead compute the derivative of logpdf(q, y) wrt. y, and then transform back x = b\u207b\u00b9(y). This is very useful for certain inference methods, e.g. Hamiltonian Monte-Carlo, where we need to take the derivative of the logpdf-computation wrt. input. \nAnother use-case is constructing a parameterized Bijector and consider transforming a \"simple\" density, e.g. MvNormal, to match a more complex density. One class of such bijectors is Normalizing Flows (NFs) which are compositions of differentiable and invertible neural networks, i.e. composition of a particular family of parameterized bijectors.[1] We'll see an example of this later on. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9667767644707661,
        0.9694699878118811
      ],
      "excerpt": "Pretty neat, huh? Inversed{Logit} is also a Bijector where we've defined (ib::Inversed{&lt;:Logit})(y) as the inverse transformation of (b::Logit)(x). Note that it's not always the case that inv(b) isa Inversed, e.g. the inverse of Exp is simply Log so inv(Exp()) isa Log is true. \nOne more thing. See the 0 in Inversed{Logit{Float64}, 0}? It represents the dimensionality of the bijector, in the same sense as for an AbstractArray with the exception of 0 which means it expects 0-dim input and output, i.e. &lt;:Real. This can also be accessed through dimension(b): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9910777916031343
      ],
      "excerpt": "In most cases specification of the dimensionality is unnecessary as a Bijector{N} is usually only defined for a particular value of N, e.g. Logit isa Bijector{0} since it only makes sense to apply Logit to a real number (or a vector of reals if you're doing batch-computation). As a user, you'll rarely have to deal with this dimensionality specification. Unfortunately there are exceptions, e.g. Exp which can be applied to both real numbers and a vector of real numbers, in both cases treating it as a single input. This means that when Exp receives a vector input x as input, it's ambiguous whether or not to treat x as a batch of 0-dim inputs or as a single 1-dim input. As a result, to support batch-computation it is necessary to know the expected dimensionality of the input and output. Notice that we assume the dimensionality of the input and output to be the same. This is a reasonable assumption considering we're working with bijections. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9653621888043994
      ],
      "excerpt": "When computing logpdf(td, y) where td is the transformed distribution corresponding to Beta(2, 2), it makes more semantic sense to compute the pdf of the transformed variable y rather than using the \"un-transformed\" variable x to do so, as we do in logpdf_with_trans. With that being said, we can also do \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9016425988399971
      ],
      "excerpt": "In the computation of both logpdf and logpdf_forward we need to compute log(abs(det(jacobian(inv(b), y)))) and log(abs(det(jacobian(b, x)))), respectively. This computation is available using the logabsdetjac method \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9732531883194967
      ],
      "excerpt": "which is always the case for a differentiable bijection with differentiable inverse. Therefore if you want to compute logabsdetjac(b\u207b\u00b9, y) and we know that logabsdetjac(b, b\u207b\u00b9(y)) is actually more efficient, we'll return -logabsdetjac(b, b\u207b\u00b9(y)) instead. For some bijectors it might be easy to compute, say, the forward pass b(x), but expensive to compute b\u207b\u00b9(y). Because of this you might want to avoid doing anything \"backwards\", i.e. using b\u207b\u00b9. This is where forward comes to good use: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9649267713037445,
        0.8781744841113411
      ],
      "excerpt": "In fact, the purpose of forward is to just do the right thing, not necessarily \"forward\". In this function we'll have access to both the original value x and the transformed value y, so we can compute logabsdetjac(b, x) in either direction. Furthermore, in a lot of cases we can re-use a lot of the computation from b(x) in the computation of logabsdetjac(b, x), or vice-versa. forward(b, x) will take advantage of such opportunities (if implemented). \nAt this point we've only shown that we can replicate the existing functionality. But we said TransformedDistribution isa Distribution, so we also have rand: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.985905801804679
      ],
      "excerpt": "A very interesting application is that of normalizing flows.[1] Usually this is done by sampling from a multivariate normal distribution, and then transforming this to a target distribution using invertible neural networks. Currently there are two such transforms available in Bijectors.jl: PlanarFlow and RadialFlow. Let's create a flow with a single PlanarLayer: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216742196580028
      ],
      "excerpt": "That's it. Now we can sample from it using rand and compute the logpdf, like any other Distribution. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8428920402420913
      ],
      "excerpt": "julia> logpdf(flow, y)         #: uses inverse of b; not very efficient for PlanarFlow and not 100% accurate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8731981490571852
      ],
      "excerpt": "Another useful function is the forward(d::Distribution) method. It is similar to forward(b::Bijector) in the sense that it does a forward pass of the entire process \"sample then transform\" and returns all the most useful quantities in process using the most efficent computation path. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9870551946645798
      ],
      "excerpt": "This method is for example useful when computing quantities such as the expected lower bound (ELBO) between this transformed distribution and some other joint density. If no analytical expression is available, we have to approximate the ELBO by a Monte Carlo estimate. But one term in the ELBO is the entropy of the base density, which we do know analytically in this case. Using the analytical expression for the entropy and then using a monte carlo estimate for the rest of the terms in the ELBO gives an estimate with lower variance than if we used the monte carlo estimate for the entire expectation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9370387010679837
      ],
      "excerpt": ": see the proper implementation for logabsdetjac in general \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8338784837827069
      ],
      "excerpt": "A slightly more complex example is Logit: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8525267501400249
      ],
      "excerpt": "(ib::Inversed{<:Logit})(y) = @. (ib.orig.b - ib.orig.a) * logistic(y) + ib.orig.a  #: orig contains the Bijector which was inverted \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.996073636369532
      ],
      "excerpt": "(Batch computation is not fully supported by all bijectors yet (see issue #35), but is actively worked on. In the particular case of Logit there's only one thing that makes sense, which is elementwise application. Therefore we've added @. to the implementation above, thus this works for any AbstractArray{&lt;:Real}.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "julia> logabsdetjac(inv(b), y) #: defaults to - logabsdetjac(b, inv(b)(x)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9185033315443899
      ],
      "excerpt": "We could also have implemented Logit as an ADBijector: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278145711182455
      ],
      "excerpt": "No implementation of logabsdetjac, but: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8782215033009202
      ],
      "excerpt": "Neat! And just to verify that everything works: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8795917359700014,
        0.8864876105166647
      ],
      "excerpt": "  - Bijector: super-type of all bijectors.  \n  - ADBijector{AD} &lt;: Bijector: subtypes of this only require the user to implement (b::UserBijector)(x) and (ib::Inversed{&lt;:UserBijector})(y). Automatic differentation will be used to compute the jacobian(b, x) and thus logabsdetjac(b, x). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8468597420982513
      ],
      "excerpt": "  -SimplexBijector: mostly used as the constrained-to-unconstrained bijector forSimplexDistribution, e.g.Dirichlet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9792098516137469,
        0.9460346416039218,
        0.8535612003918885
      ],
      "excerpt": "The following methods are implemented by all subtypes of Bijector, this also includes bijectors such as Composed. \n- (b::Bijector)(x): implements the transform of the Bijector \n- inv(b::Bijector): returns the inverse of b, i.e. ib::Bijector s.t. (ib \u2218 b)(x) \u2248 x. In most cases this is Inversed{&lt;:Bijector}. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9120990351103198,
        0.8785002968103128,
        0.8691383472260388
      ],
      "excerpt": "- \u2218, composel, composer: convenient and type-safe constructors for Composed. composel(bs...) composes s.t. the resulting composition is evaluated left-to-right, while composer(bs...) is evaluated right-to-left. \u2218 is right-to-left, as excepted from standard mathematical notation. \n- jacobian(b::Bijector, x) [OPTIONAL]: returns the jacobian of the transformation. In some cases the analytical jacobian has been implemented for efficiency. \n- dimension(b::Bijector): returns the dimensionality of b. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156055220933724,
        0.8305682693097215
      ],
      "excerpt": "For TransformedDistribution, together with default implementations for Distribution, we have the following methods: \n- bijector(d::Distribution): returns the default constrained-to-unconstrained bijector for d \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9233778883813964,
        0.8509668451915501
      ],
      "excerpt": "- logpdf_forward(d::Distribution, x), logpdf_forward(d::Distribution, x, logjac): computes the logpdf(td, td.transform(x)) using the forward pass, which is potentially faster depending on the transform at hand. \n- forward(d::Distribution): returns (x = rand(dist), y = b(x), logabsdetjac = logabsdetjac(b, x), logpdf = logpdf_forward(td, x)) where b = td.transform. This combines sampling from base distribution and transforming into one function. The intention is that this entire process should be performed in the most efficient manner, e.g. the logabsdetjac(b, x) call might instead be implemented as - logabsdetjac(inv(b), b(x)) depending on which is most efficient. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Last snapshots taken from https://github.com/UnofficialJuliaMirror/Bijectors.jl-76274a88-744f-5084-9051-94815aaf08c4 on 2019-11-20T05:17:53.322-05:00 by @UnofficialJuliaMirrorBot via Travis job 153.9 , triggered by Travis cron job on branch \"master\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/UnofficialJuliaMirrorSnapshots/Bijectors.jl-76274a88-744f-5084-9051-94815aaf08c4/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 23:26:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/UnofficialJuliaMirrorSnapshots/Bijectors.jl-76274a88-744f-5084-9051-94815aaf08c4/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "UnofficialJuliaMirrorSnapshots/Bijectors.jl-76274a88-744f-5084-9051-94815aaf08c4",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8358087677440971
      ],
      "excerpt": "All exported names from the Distributions.jl package are reexported from Bijectors. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902204990677808
      ],
      "excerpt": "julia> using Bijectors \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> x == z \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902204990677808
      ],
      "excerpt": "julia> using Bijectors \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> b\u207b\u00b9 = inv(b) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> b\u207b\u00b9(y) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> Bijectors.dimension(b) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> id_y = (b \u2218 b\u207b\u00b9) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> id_y(y) \u2248 y \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> id_x = inv(id_y) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> id_x(x) \u2248 x \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902204990677808
      ],
      "excerpt": "julia> using Bijectors: TransformedDistribution \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> td isa UnivariateDistribution \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> logpdf(td, y) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> logabsdetjac(b\u207b\u00b9, y) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> logabsdetjac(b, x) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> y = rand(td)              #: \u2208 \u211d \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> b = PlanarLayer(2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8238850187734125
      ],
      "excerpt": "That's it. Now we can sample from it using rand and compute the logpdf, like any other Distribution. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> b = sb \u2218 PlanarLayer(2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> y = rand(td) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> 0 < y[1] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> 0 \u2264 y[2] \u2264 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902204990677808
      ],
      "excerpt": "julia> using Tracker \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> Tracker.grad(b.w) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902204990677808,
        0.8391088477907983,
        0.8391088477907983,
        0.8391088477907983
      ],
      "excerpt": "julia> using Flux \njulia> @Flux.treelike Composed \njulia> @Flux.treelike TransformedDistribution \njulia> @Flux.treelike PlanarLayer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877657484961249
      ],
      "excerpt": "There's mainly two ways you can implement your own Bijector, and which way you choose mainly depends on the following question: are you bothered enough to manually implement logabsdetjac? If the answer is \"Yup!\", then you subtype from Bijector, if \"Naaaah\" then you subtype ADBijector. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> b = Logit(0.0, 1.0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> b(0.6) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> inv(b)(y) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> logabsdetjac(b, 0.6) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079918167389044
      ],
      "excerpt": "As you can see it's a very contrived example, but you get the idea. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> b_ad = ADLogit(0.0, 1.0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> logabsdetjac(b_ad, 0.6) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> y = b_ad(0.6) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> inv(b_ad)(y) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> logabsdetjac(inv(b_ad), y) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> b = Logit(0.0, 1.0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> logabsdetjac(b, 0.6) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> logabsdetjac(b_ad, 0.6) \u2248 logabsdetjac(b, 0.6) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> Bijectors.setadbackend(:reverse_diff) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> b_ad = ADLogit(0.0, 1.0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391088477907983
      ],
      "excerpt": "julia> logabsdetjac(b_ad, 0.6) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8667439673264737
      ],
      "excerpt": "The following are the bijectors available: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8184394336364812
      ],
      "excerpt": "2-element Array{Float64,1}: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184394336364812
      ],
      "excerpt": "PlanarLayer{Array{Float64,2},Array{Float64,1}}([1.77786; -1.1449], [-0.468606; 0.156143], [-2.64199]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184394336364812
      ],
      "excerpt": "2-element Array{Float64,1}: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184394336364812
      ],
      "excerpt": "2-element Array{Float64,1}: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184394336364812
      ],
      "excerpt": "2-element Array{Float64,1}: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184394336364812
      ],
      "excerpt": "PlanarLayer{TrackedArray{\u2026,Array{Float64,2}},TrackedArray{\u2026,Array{Float64,1}}}([-1.05099; 0.502079] (tracked), [-0.216248; -0.706424] (tracked), [-4.33747] (tracked)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184394336364812
      ],
      "excerpt": "Tracked 2-element Array{Float64,1}: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184394336364812
      ],
      "excerpt": "2-element Array{Float64,1}: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184394336364812
      ],
      "excerpt": "2\u00d71 Array{Float64,2}: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060372108626078
      ],
      "excerpt": "Params([[-1.05099; 0.502079] (tracked), [-0.216248; -0.706424] (tracked), [-4.33747] (tracked)]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674
      ],
      "excerpt": "import Bijectors: logabsdetjac \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184394336364812
      ],
      "excerpt": "Tracked 2-element Array{Float64,1}: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184394336364812
      ],
      "excerpt": "Tracked 2-element Array{Float64,1}: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/UnofficialJuliaMirrorSnapshots/Bijectors.jl-76274a88-744f-5084-9051-94815aaf08c4/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Julia"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 The Turing Language\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Bijectors.jl",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Bijectors.jl-76274a88-744f-5084-9051-94815aaf08c4",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "UnofficialJuliaMirrorSnapshots",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/UnofficialJuliaMirrorSnapshots/Bijectors.jl-76274a88-744f-5084-9051-94815aaf08c4/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 23:26:45 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Other than the `logpdf_with_trans` methods, the package also provides a more composable interface through the `Bijector` types. Consider for example the one from above with `Beta(2, 2)`.\n\n```julia\njulia> using Random; Random.seed!(42);\n\njulia> using Bijectors; using Bijectors: Logit\n\njulia> dist = Beta(2, 2)\nBeta{Float64}(\u03b1=2.0, \u03b2=2.0)\n\njulia> x = rand(dist)\n0.36888689965963756\n\njulia> b = bijector(dist) #: bijection (0, 1) \u2192 \u211d\nLogit{Float64}(0.0, 1.0)\n\njulia> y = b(x)\n-0.5369949942509267\n```\n\nIn this case we see that `bijector(d::Distribution)` returns the corresponding constrained-to-unconstrained bijection for `Beta`, which indeed is a `Logit` with `a = 0.0` and `b = 1.0`. The resulting `Logit <: Bijector` has a method `(b::Logit)(x)` defined, allowing us to call it just like any other function. Comparing with the above example, `b(x) == link(dist, x)`. Just to convince ourselves:\n\n```julia\njulia> b(x) == link(dist, x)\ntrue\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "But the real utility of `TransformedDistribution` becomes more apparent when using `transformed(dist, b)` for any bijector `b`. To get the transformed distribution corresponding to the `Beta(2, 2)`, we called `transformed(dist)` before. This is simply an alias for `transformed(dist, bijector(dist))`. Remember `bijector(dist)` returns the constrained-to-constrained bijector for that particular `Distribution`. But we can of course construct a `TransformedDistribution` using different bijectors with the same `dist`. This is particularly useful in something called _Automatic Differentiation Variational Inference (ADVI)_.[2] An important part of ADVI is to approximate a constrained distribution, e.g. `Beta`, as follows:\n1. Sample `x` from a `Normal` with parameters `\u03bc` and `\u03c3`, i.e. `x ~ Normal(\u03bc, \u03c3)`.\n2. Transform `x` to `y` s.t. `y \u2208 support(Beta)`, with the transform being a differentiable bijection with a differentiable inverse (a \"bijector\")\n\nThis then defines a probability density with same _support_ as `Beta`! Of course, it's unlikely that it will be the same density, but it's an _approximation_. Creating such a distribution becomes trivial with `Bijector` and `TransformedDistribution`:\n\n```julia\njulia> dist = Beta(2, 2)\nBeta{Float64}(\u03b1=2.0, \u03b2=2.0)\n\njulia> b = bijector(dist)              #: (0, 1) \u2192 \u211d\nLogit{Float64}(0.0, 1.0)\n\njulia> b\u207b\u00b9 = inv(b)                    #: \u211d \u2192 (0, 1)\nInversed{Logit{Float64},0}(Logit{Float64}(0.0, 1.0))\n\njulia> td = transformed(Normal(), b\u207b\u00b9) #: x \u223c \ud835\udcdd(0, 1) then b(x) \u2208 (0, 1)\nTransformedDistribution{Normal{Float64},Inversed{Logit{Float64},0},Univariate}(\ndist: Normal{Float64}(\u03bc=0.0, \u03c3=1.0)\ntransform: Inversed{Logit{Float64},0}(Logit{Float64}(0.0, 1.0))\n)\n\n\njulia> x = rand(td)                    #: \u2208 (0, 1)\n0.538956748141868\n```\n\nIt's worth noting that `support(Beta)` is the _closed_ interval `[0, 1]`, while the constrained-to-unconstrained bijection, `Logit` in this case, is only well-defined as a map `(0, 1) \u2192 \u211d` for the _open_ interval `(0, 1)`. This is of course not an implementation detail. `\u211d` is itself open, thus no continuous bijection exists from a _closed_ interval to `\u211d`. But since the boundaries of a closed interval has what's known as measure zero, this doesn't end up affecting the resulting density with support on the entire real line. In practice, this means that\n\n```julia\ntd = transformed(Beta())\n\ninv(td.transform)(rand(td))\n```\n\nwill never result in `0` or `1` though any sample arbitrarily close to either `0` or `1` is possible. _Disclaimer: numerical accuracy is limited, so you might still see `0` and `1` if you're lucky._\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We can also do _multivariate_ ADVI using the `Stacked` bijector. `Stacked` gives us a way to combine univariate and/or multivariate bijectors into a singe multivariate bijector. Say you have a vector `x` of length 2 and you want to transform the first entry using `Exp` and the second entry using `Log`. `Stacked` gives you an easy and efficient way of representing such a bijector.\n\n```julia\njulia> Random.seed!(42);\n\njulia> using Bijectors: Exp, Log, SimplexBijector\n\njulia> #: Original distributions\n       dists = (\n           Beta(),\n           InverseGamma(),\n           Dirichlet(2, 3)\n       );\n\njulia> #: Construct the corresponding ranges\n       ranges = [];\n\njulia> idx = 1;\n\njulia> for i = 1:length(dists)\n           d = dists[i]\n           push!(ranges, idx:idx + length(d) - 1)\n\n           global idx\n           idx += length(d)\n       end;\n\njulia> ranges\n3-element Array{Any,1}:\n 1:1\n 2:2\n 3:4\n\njulia> #: Base distribution; mean-field normal\n       num_params = ranges[end][end]\n4\n\njulia> d = MvNormal(zeros(num_params), ones(num_params))\nDiagNormal(\ndim: 4\n\u03bc: [0.0, 0.0, 0.0, 0.0]\n\u03a3: [1.0 0.0 0.0 0.0; 0.0 1.0 0.0 0.0; 0.0 0.0 1.0 0.0; 0.0 0.0 0.0 1.0]\n)\n\n\njulia> #: Construct the transform\n       bs = bijector.(dists)     #: constrained-to-unconstrained bijectors for dists\n(Logit{Float64}(0.0, 1.0), Log{0}(), SimplexBijector{Val{true}}())\n\njulia> ibs = inv.(bs)            #: invert, so we get unconstrained-to-constrained\n(Inversed{Logit{Float64},0}(Logit{Float64}(0.0, 1.0)), Exp{0}(), Inversed{SimplexBijector{Val{true}},1}(SimplexBijector{Val{true}}()))\n\njulia> sb = Stacked(ibs, ranges) #: => Stacked <: Bijector\nStacked{Tuple{Inversed{Logit{Float64},0},Exp{0},Inversed{SimplexBijector{Val{true}},1}},3}((Inversed{Logit{Float64},0}(Logit{Float64}(0.0, 1.0)), Exp{0}(), Inversed{SimplexBijector{Val{true}},1}(SimplexBijector{Val{true}}())), (1:1, 2:2, 3:4))\n\njulia> #: Mean-field normal with unconstrained-to-constrained stacked bijector\n       td = transformed(d, sb);\n\njulia> y = rand(td)\n4-element Array{Float64,1}:\n 0.36446726136766217\n 0.6412195576273355 \n 0.5067884173521743 \n 0.4932115826478257 \n\njulia> 0.0 \u2264 y[1] \u2264 1.0   #: => true\ntrue\n\njulia> 0.0 < y[2]         #: => true\ntrue\n\njulia> sum(y[3:4]) \u2248 1.0  #: => true\ntrue\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}