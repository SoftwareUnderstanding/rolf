{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/2001.08210",
      "https://arxiv.org/abs/1911.12246?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529.pdf"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{taktasheva-etal-2021-shaking,\n    title = \"Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations\",\n    author = \"Taktasheva, Ekaterina  and\n      Mikhailov, Vladislav  and\n      Artemova, Ekaterina\",\n    booktitle = \"Proceedings of the 1st Workshop on Multilingual Representation Learning\",\n    month = nov,\n    year = \"2021\",\n    address = \"Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.mrl-1.17\",\n    pages = \"191--210\",\n    abstract = \"Recent research has adopted a new experimental field centered around the concept of text perturbations which has revealed that shuffled word order has little to no impact on the downstream performance of Transformer-based language models across many NLP tasks. These findings contradict the common understanding of how the models encode hierarchical and structural information and even question if the word order is modeled with position embeddings. To this end, this paper proposes nine probing datasets organized by the type of controllable text perturbation for three Indo-European languages with a varying degree of word order flexibility: English, Swedish and Russian. Based on the probing analysis of the M-BERT and M-BART models, we report that the syntactic sensitivity depends on the language and model pre-training objectives. We also find that the sensitivity grows across layers together with the increase of the perturbation granularity. Last but not least, we show that the models barely use the positional information to induce syntactic trees from their intermediate self-attention and contextualized representations.\",\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{taktasheva-etal-2021-shaking,\n    title = \"Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations\",\n    author = \"Taktasheva, Ekaterina  and\n      Mikhailov, Vladislav  and\n      Artemova, Ekaterina\",\n    booktitle = \"Proceedings of the 1st Workshop on Multilingual Representation Learning\",\n    month = nov,\n    year = \"2021\",\n    address = \"Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.mrl-1.17\",\n    pages = \"191--210\",\n    abstract = \"Recent research has adopted a new experimental field centered around the concept of text perturbations which has revealed that shuffled word order has little to no impact on the downstream performance of Transformer-based language models across many NLP tasks. These findings contradict the common understanding of how the models encode hierarchical and structural information and even question if the word order is modeled with position embeddings. To this end, this paper proposes nine probing datasets organized by the type of controllable text perturbation for three Indo-European languages with a varying degree of word order flexibility: English, Swedish and Russian. Based on the probing analysis of the M-BERT and M-BART models, we report that the syntactic sensitivity depends on the language and model pre-training objectives. We also find that the sensitivity grows across layers together with the increase of the perturbation granularity. Last but not least, we show that the models barely use the positional information to induce syntactic trees from their intermediate self-attention and contextualized representations.\",\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/evtaktasheva/dependency_extraction",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-05T14:45:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-19T08:48:34Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.980591441128444,
        0.9546713505104439,
        0.9329786530993655,
        0.9730664048248956,
        0.8599552212382059
      ],
      "excerpt": "The paper is accepted to the 1st Workshop on Multilingual Representation Learning (MRL) at EMNLP 2021. \nThe paper  proposes nine  probing  datasets  organized  by  the  type of controllable text perturbation for three Indo-European languages with a varying degree of word order flexibility:  nglish (West Germanic, analytic), Swedish (North Germanic, analytic), and Russian (Balto-Slavic, fusional). \nThe (NShift) task tests the LM sensitivity to local perturbations taking into account the syntactic structure. \nThe (ClauseShift) task probes the LM sensitivity to distant perturbations at the level of syntactic clauses.  \nThe (RandomShift) task tests the LM sensitivity to global perturbations obtained by shuffling the word order. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327617943809398,
        0.9738793746416479
      ],
      "excerpt": "M-BERT (Devlin et al. 2019), a transformer model of the encoder architecture, trained on multilingual Wikipedia data using the Masked LM (MLM) and Next Sentence Prediction pre-training objectives. \nM-BART (Liu et al. 2020), a sequence-to-sequence model that comprises a BERT encoder and an autoregressive GPT-2 decoder \\cite{radford2019language}. The model is pre-trained on the CC25 corpus in 25 languages using text infilling and sentence shuffling objectives, where it learns to predict masked word spans and reconstruct the permuted input. We use only the encoder in our experiments. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/evtaktasheva/dependency_extraction/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 12:29:56 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/evtaktasheva/dependency_extraction/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "evtaktasheva/dependency_extraction",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/evtaktasheva/dependency_extraction/main/install_tools.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "TBA\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/evtaktasheva/dependency_extraction/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "dependency_extraction",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "evtaktasheva",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/evtaktasheva/dependency_extraction/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sat, 25 Dec 2021 12:29:56 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "TBA\n\n",
      "technique": "Header extraction"
    }
  ]
}