{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite as\n```\n@inproceedings{wang2019vizseq,\n  title = {VizSeq: A Visual Analysis Toolkit for Text Generation Tasks},\n  author = {Changhan Wang, Anirudh Jain, Danlu Chen, Jiatao Gu},\n  booktitle = {In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},\n  year = {2019},\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{wang2019vizseq,\n  title = {VizSeq: A Visual Analysis Toolkit for Text Generation Tasks},\n  author = {Changhan Wang, Anirudh Jain, Danlu Chen, Jiatao Gu},\n  booktitle = {In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},\n  year = {2019},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9240732891640214,
        0.851697698412276
      ],
      "excerpt": "VizSeq is a Python toolkit for visual analysis on text generation tasks like machine translation, summarization, \nimage captioning, speech translation and video description. It takes multi-modal sources, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9932230983117876,
        0.9588939560799149,
        0.959546310417776,
        0.9816881292350949,
        0.8714162992508173,
        0.8988136409836113
      ],
      "excerpt": "| Text | Machine translation, text summarization, dialog generation, grammatical error correction, open-domain question answering | \n| Image | Image captioning, image question answering, optical character recognition                                                | \n| Audio | Speech recognition, speech translation                                                                                   | \n| Video | Video description                                                                                                        | \n| Multimodal | Multimodal machine translation \nAccelerated with multi-processing/multi-threading. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999999999781437,
        0.9999057755001175
      ],
      "excerpt": "| N-gram-based | BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee et al., 2005), TER (Snover et al., 2006), RIBES (Isozaki et al., 2010), chrF (Popovi\u0107 et al., 2015), GLEU (Wu et al., 2016), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015), WER | \n| Embedding-based | LASER (Artetxe and Schwenk, 2018), BERTScore (Zhang et al., 2019) | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8730726134075774
      ],
      "excerpt": "$ git clone https://github.com/facebookresearch/vizseq \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/facebookresearch/vizseq/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/facebookresearch/vizseq",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Changhan Wang ([changhan@fb.com](mailto:changhan@fb.com)), Jiatao Gu ([jgu@fb.com](mailto:jgu@fb.com))\n",
      "technique": "Header extraction"
    }
  ],
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing\nWe want to make contributing to this project as easy and transparent as\npossible.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nLicense\nBy contributing to this project, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-26T13:19:38Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T03:08:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8029469003851454,
        0.8401273061627635
      ],
      "excerpt": "in Jupyter Notebook or a \nbuilt-in Web App \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9024442320689046
      ],
      "excerpt": "VizSeq also provides a collection of multi-process scorers as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.810173815853767
      ],
      "excerpt": "And then, navigate to the following URL in your web browser: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "An Analysis Toolkit for Natural Language Generation (Translation, Captioning, Summarization, etc.)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/facebookresearch/vizseq/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 45,
      "date": "Tue, 28 Dec 2021 05:28:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/facebookresearch/vizseq/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "facebookresearch/vizseq",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/facebookresearch/vizseq/tree/main/website/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/facebookresearch/vizseq/main/examples/multimodal_machine_translation.ipynb",
      "https://raw.githubusercontent.com/facebookresearch/vizseq/main/examples/jupyter_notebook.ipynb",
      "https://raw.githubusercontent.com/facebookresearch/vizseq/main/examples/speech_translation.ipynb",
      "https://raw.githubusercontent.com/facebookresearch/vizseq/main/examples/multilingual_machine_translation.ipynb",
      "https://raw.githubusercontent.com/facebookresearch/vizseq/main/examples/machine_translation.ipynb",
      "https://raw.githubusercontent.com/facebookresearch/vizseq/main/examples/fairseq_integration.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/facebookresearch/vizseq/main/get_example_data.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "VizSeq requires **Python 3.6+** and currently runs on **Unix/Linux** and **macOS/OS X**. It will support **Windows** as well in the future.\n\nYou can install VizSeq from PyPI repository:\n```bash\n$ pip install vizseq\n```\n\nOr install it from source:\n```bash\n$ git clone https://github.com/facebookresearch/vizseq\n$ cd vizseq\n$ pip install -e .\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8024121214511752
      ],
      "excerpt": "in Jupyter Notebook or a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9067662967425743
      ],
      "excerpt": "a normal Python package. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9745978183701172,
        0.9906248903846466,
        0.9465718491881494
      ],
      "excerpt": "$ git clone https://github.com/facebookresearch/vizseq \n$ cd vizseq \n$ bash get_example_data.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8705050761500154
      ],
      "excerpt": "<img src=\"teaser.gif\" alt=\"VizSeq Teaser\" width=\"480\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9276528860243863
      ],
      "excerpt": "Download example data: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/facebookresearch/vizseq/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "HTML",
      "JavaScript",
      "CSS",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2015, Xinlei Chen, Hao Fang, Tsung-Yi Lin, and Ramakrishna Vedantam\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\nThe views and conclusions contained in the software and documentation are those\\nof the authors and should not be interpreted as representing official policies,\\neither expressed or implied, of the FreeBSD Project.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "<img src=\"logo.png\" alt=\"VizSeq\" width=\"160\">",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "vizseq",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "facebookresearch",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/facebookresearch/vizseq/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 373,
      "date": "Tue, 28 Dec 2021 05:28:23 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [Basic example](https://facebookresearch.github.io/vizseq/docs/getting_started/ipynb_example)\n- [Multimodal Machine Translation](examples/multimodal_machine_translation.ipynb)\n- [Multilingual Machine Translation](examples/multilingual_machine_translation.ipynb)\n- [Speech Translation](examples/speech_translation.ipynb)\n\n",
      "technique": "Header extraction"
    }
  ]
}