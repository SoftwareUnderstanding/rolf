{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "https://arxiv.org/pdf/1509.02971.pdf",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alathiya/RL-Quadcoptor-Flying",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-01T20:17:39Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-04T02:47:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9299591983747819
      ],
      "excerpt": "In this project, I have implemented an agent to fly a quadcopter, and then train it using a reinforcement learning algorithm  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9831321882512746,
        0.817275152410526,
        0.8848148488200275
      ],
      "excerpt": "I have designed Actor/Critic deep neural network model using Keras. Task of agent is to take off vertically in z direction and reach from position  \n(0,0,0) to (0,0,10). Agent controls the rotor speed of 4 rotors. By controlling the rotor speed agents learns to take off vertically.  \nQuadcoptor environment is simulation provided by udacity. Both state and action space are continous. That's the reason I have chosen DDPG algo for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9842563459668344,
        0.9943939960988863,
        0.9569836457415911,
        0.8164364266796915,
        0.9578565490645872
      ],
      "excerpt": "of last 100 episode to get sense of how learning curve is changing when reward is averaged over 100 episode. \nGetting started was hardest part of the project as I had to fully understand how DDPG algorithm works and get some intuitive sense of how learning is  \ntaking place with actor, critic and agent. So to get concept clear I had to revisit lesson videos on Policy gradients and Actor/Critic model multiple times.  \nAfter this I had to fully understand Physics simulation model to understand how task can be defined given continous state and action space.  \nUnderstand size/dimensionations of state/action space and how state is getting transformed after stepping with actions. I had to focus much on designing  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9663012123585489,
        0.8283255445245086,
        0.9753260555189064,
        0.9015814370173116
      ],
      "excerpt": "Yes it was interesting to learn that much of how agent learns is driven by how good reward function is design.  \nI concluded that was what mostly driving the agent performance. Now obivously other hyperparameter tunning with network archtitecture do matters and  \nI saw how agent changes learning curves when these parameters are changed. For example I observed large oscillation in reward score is reduced with  \nreduction in learning rate passed to actor and critic model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382156009321375
      ],
      "excerpt": "and batch normalization etc best learning I could get is shown in jupyter notebook plot. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9006658230800787
      ],
      "excerpt": "  taking good step but we have not penalized agent much with negative reward when agent takes bad steps. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8959589545098173
      ],
      "excerpt": "- Many times agent reaches local optima and keeps oscillating without further improving. Momentum can be added to learning process to get  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Udacity project for teaching a Quadcoptor how to fly. ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alathiya/RL-Quadcoptor-Flying/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 20 Dec 2021 22:48:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alathiya/RL-Quadcoptor-Flying/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "alathiya/RL-Quadcoptor-Flying",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/alathiya/RL-Quadcoptor-Flying/master/Quadcopter_Project.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8312813181498871
      ],
      "excerpt": "training agent.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801091406263014
      ],
      "excerpt": "Now with several experimentation with task, reward function, NN archtitecture, hyperparameters tunning, changing activations, regularizer function values  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alathiya/RL-Quadcoptor-Flying/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Teach a Quadcopter How to Fly!",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "RL-Quadcoptor-Flying",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "alathiya",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alathiya/RL-Quadcoptor-Flying/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project requires following libraries to be imported. \n\n\t- Numpy\n\t- Matplotlib\n\t- Pandas\n\t- Sys\n\t- keras\n\t- collections\t\t\n\t- random\n\t- copy\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 20 Dec 2021 22:48:03 GMT"
    },
    "technique": "GitHub API"
  }
}