{
  "citation": [
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if x < recomb_loc: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jliphard/DeepEvolve",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-11-10T17:23:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-18T01:18:21Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.993548553152099,
        0.8612090946284414,
        0.9965967486247165,
        0.9884316569342101
      ],
      "excerpt": "These days, it's relatively easy to train neural networks, but it's still difficult to figure out which network architectures and other hyperparameters to use - e.g. how many neurons, how many layers, and which activation functions? In the long term, of course, neural networks will learn how to architect themselves, without human intervention. Until then, the speed of developing application-optimized neural networks will remain limited by the time and expertise required to chose and refine hyperparameters. DeepEvolve is designed to help solve this problem, by rapidly returning good hyperparameters for particular datasets and classification problems. The code supports hyperparameter discovery for MLPs (ie. fully connected networks) and convolutional neural networks. \nIf you had infinite time and infinite computing resources, you could brute-force the problem, and just compare and contrast all parameter combinations. However, in most real-world applications of neural networks, you will probably have to balance competing demands (time, cost, desire to continuously optimize AI performance in dynamic environments) and you may - for whatever reason - have a strong interest to be able to rapidly generate good networks for diverse datasets. In that case, genetic algorithms will be useful. \nGenetic algorithms can be used to solve complex nonlinear optimization problems. DeepEvolve is a simple Keras framework for rapidly discovering good hyperparameters using cycles of mutation, recombination, training, and selection. The role of point mutations in genomes is readily apparent - create diversity - but the functions of other genome operations, such as recombination, are not as widely appreciated. Briefly, recombination addresses clonal interference, which is a major kinetic bottleneck in discovering optimal genomes in evolving populations.  \nImagine that two (or more) different beneficial mutations in different genes arise independently in different individuals. These individuals will have higher fitness, but the algorithm (aka evolution) can not easily converge on the optimal genome. Evolution solves clonal interference through recombination, which allows two genomes to swap entire regions, increasing the likelihood of generating a single genome with both beneficial genes. If you are curious about clonal interference, a good place to start is The fate of competing beneficial mutations in an asexual population. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9630592007560215
      ],
      "excerpt": "Each AI network architecture is represented as a string of genes. These architectures/genomes recombine with some frequency, at one randomly selected position along the genome. Note that a genome with N genes can recombine at N - 1 nontrivial positions (1, 2, 3, N-1). Specifically, recomb_loc = 0 || len(self.all_possible_genes) does not lead to recombination, but just returns the original parental genomes, and therefore recomb_loc = random.randint(1, len(self.all_possible_genes) - 1).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8128496230936851
      ],
      "excerpt": " CORE RECOMBINATION CODE * \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9892323344660776
      ],
      "excerpt": "To increase the rate of discovering optimal hyperparameters, we also keep track of all genomes in all previous generations; each genome is identified via its MD5 hash and we block recreation of duplicate, previously generated and trained genomes.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.843642241935623
      ],
      "excerpt": "Finally, we also facilitate genome uniqueness during the mutation operation, by limiting random choices to ones that differ from the gene's current value.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Rapid hyperparameter discovery for neural nets using genetic algorithms",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jliphard/DeepEvolve/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 53,
      "date": "Wed, 29 Dec 2021 09:21:00 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jliphard/DeepEvolve/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jliphard/DeepEvolve",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jliphard/DeepEvolve/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/jliphard/DeepEvolve/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'COPYRIGHT\\n\\nAll contributions by Jan Liphardt:\\nCopyright (c) 2017 - 2019, Jan Liphardt.\\nAll rights reserved.\\n\\nAll contributions by Gabriel Downs:\\nCopyright (c) 2018 - 2019,  Gabriel Downs.\\nAll rights reserved.\\n\\nAll contributions by Saumya Tiwari:\\nCopyright (c) 2018 - 2019,  Gabriel Downs.\\nAll rights reserved.\\n\\nEach contributor holds copyright over their respective contributions.\\nThe project versioning (Git) records all such contribution source information.\\n\\nLICENSE\\n\\nMIT License\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepEvolve",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepEvolve",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jliphard",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jliphard/DeepEvolve/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run the brute force algorithm which goes through all possible choices one by one:\n\n```python3 brute.py```\n\nTo run the genetic algorithm:\n\n```python3 main.py```\n\nIn general, you will want to run the code in the cloud - we use [floydhub.com](http:floydhub.com):\n\n```$ floyd run --gpu --env keras \"python main.py\"```\n\nFor a convolutional neural network being trained on `cifar10`, expect run times of about 3.2 hours on a Tesla K80 (30 genomes and 8 generations). Compared to the brute force solution, you should expect to get high performing hyperparameter combinations within about 3.5 generations, about 8x to 12x faster than brute force. Of course, with a genetic algorithm, you are not *guaranteed* to find the best solution, you are merely likely to find something suitable relatively quickly. You can choose the various options in ```main.py```.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 160,
      "date": "Wed, 29 Dec 2021 09:21:00 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Here are the time dynamics of discovering the optimal optimizer (Adam). The x axis is time, and the y axis is gene variant frequency in the population. Adaptive Moment Estimation ([Adam](https://arxiv.org/pdf/1412.6980.pdf)) computes adaptive learning rates based on exponential moving averages of the 1st moment (the mean) and the 2nd raw moment (the uncentered variance) of the gradient. By the 5th generation, genomes with adaptive moment estimation are the dominant subspecies in the population. \n\n![alt text](https://github.com/jliphard/DeepEvolve/blob/726aaf3dfdc8d6d2c6bc64d3a55e3ab3023b29c7/Images/Optimizer.png \"Optimizer kinetics\")\n\nIn this figure, you can see a small selection of genomes being trained for 4 epochs on cifar10. The x axis is time, and the y axis is accuracy. Some genomes fail to learn at all (flat line at 10% accuracy), while about 1/2 of the genomes achieve 35% accuracy within one epoch. Early performance does not perfectly predict learning rates in subsequent epochs, but it comes close. \n\n![alt text](https://github.com/jliphard/DeepEvolve/blob/4f8cf547797b2263659f053e0824bf34b39e337a/Images/Evolve.png \"Evolution kinetics\")\n\nThis is a graphical overview of how the AI hyperparameter sets (aka genomes) evolve, showing the initial (random) population of genomes (left), and the various cycles of recombination and selection. \n\n![alt text](https://github.com/jliphard/DeepEvolve/blob/55473015692e2af75be35fa1baf6536e300032bc/Images/Network.png \"Evolution of AI hyperparameter sets\")\n",
      "technique": "Header extraction"
    }
  ]
}