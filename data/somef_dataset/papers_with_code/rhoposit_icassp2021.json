{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code is based on [mkotha/WaveRNN](https://github.com/mkotha/WaveRNN)\n\nAnd is also based on [nii-yamagishilab/Extended_VQVAE](https://github.com/nii-yamagishilab/Extended_VQVAE)\n\n\n\nThis work was partially supported by the EPSRC Centre for DoctoralTraining in Data Science, funded by the UK Engineering and Physical Sci-ences Research Council (grant EP/L016427/1) and University of Edinburgh;and by a JST CREST Grant (JPMJCR18A6, VoicePersonae project), Japan.The numerical calculations were carried out on the TSUBAME 3.0 super-computer at the Tokyo Institute of Technology.\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2010.10727",
      "https://arxiv.org/abs/1711.00937",
      "https://arxiv.org/abs/2010.10727"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9326826708383243
      ],
      "excerpt": "Please find our samples here. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rhoposit/icassp2021",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-05T17:44:38Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T20:31:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9917515502148364,
        0.9898935277891084
      ],
      "excerpt": "This is a Pytorch implementation of Dual-Encoder VQVAE mentioned in our paper. \nWe introduce a dual learning space that learns a speaker codebook (global features) alongside the original phone codebook (local). The new framework generalizes well to unseen speakers and the learned representations achieve reasonable performance in downstream tasks such as phone recognition and speaker diarization.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8337791774347607
      ],
      "excerpt": "Changed the global condition to learned VQVAE speaker code.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.853378889837193,
        0.883973552586221
      ],
      "excerpt": "Authors of the paper: Jennifer Williams, Yi Zhao, Erica Cooper, Junichi Yamagishi \nFor any question related to the paper or the scripts, please contact j.williams[email mark]ed.ac.uk. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rhoposit/icassp2021/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Wed, 22 Dec 2021 12:29:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rhoposit/icassp2021/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rhoposit/icassp2021",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rhoposit/icassp2021/main/run_slurm.sh",
      "https://raw.githubusercontent.com/rhoposit/icassp2021/main/run.sh"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rhoposit/icassp2021/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Jennifer Williams\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "icassp2021",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "icassp2021",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rhoposit",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rhoposit/icassp2021/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please install the environment in project.yml before using the scripts.\n```\nconda env create -f project.yml\nconda activate project\n```\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14,
      "date": "Wed, 22 Dec 2021 12:29:30 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please see example commands in run.sh for training the vavae model.\n\nOr you can use python train.py -m [model type]. The -m option can be used to tell the the script to train a different model.\n\n[model type] can be:\n- 'sys2': train original VQVAE\n- 'sys3': train a self-supervised VQVAE with dual encoders\n- 'sys4': train a semi-supervised VQVAE with dual encoders\n- 'sys4a': train system 4a but use angular softmax\n- 'sys5': train a semi-supervised VQVAE with dual encoders, and gradient reversal\n- 'sys5a': train system 5a but use angular softmax\n\n\nPlease modify sampling rate and other parameters in [config.py](https://github.com/rhoposit/icassp2021/blob/main/config.py) before training.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}