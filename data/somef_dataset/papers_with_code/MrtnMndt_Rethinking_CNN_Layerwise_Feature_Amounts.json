{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1812.05836](https://arxiv.org/abs/1812.05836",
      "https://arxiv.org/abs/1812.05836",
      "https://arxiv.org/abs/1502.01852",
      "https://arxiv.org/abs/1608.03983"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.988333501392485,
        0.9998244408981862,
        0.9247807915432515,
        0.9950911808181527
      ],
      "excerpt": "If you use or extend the code please cite our work: \nMartin Mundt, Sagnik Majumder, Tobias Weis, Visvanathan Ramesh, \"Rethinking Layer-wise Feature Amounts in Convolutional Neural Network Architectures\", International Conference on Neural Information Processing Systems (NeurIPS) 2018, Critiquing and Correcting Trends in Machine Learning (CRACT) Workshop \nYou can find the complete CRACT workshop here: https://ml-critique-correct.github.io  \nand our paper here: https://arxiv.org/abs/1812.05836 or through the above website. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MrtnMndt/Rethinking_CNN_Layerwise_Feature_Amounts",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-25T16:25:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-13T17:06:09Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9852815383153826
      ],
      "excerpt": "In this repository we provide open-source PyTorch code for our NeurIPS 2018 CRACT workshop paper, where we characterize the classification accuracy of a family of VGG-like models by shifting a constant amount of total features to different convolutional layers and show how large amounts of features in early layers challenge common design assumptions. If you use or extend the code please cite our work: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9630405736112104
      ],
      "excerpt": "We provide interactive visualization to explore our results more easily on the repositories github page: https://mrtnmndt.github.io/Rethinking_CNN_Layerwise_Feature_Amounts/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9553666807578232
      ],
      "excerpt": "We tried our best to use a generic data-loading pipeline by implementing classes named according to the datasets in lib/Datasets/datasets.py. The main file creates an instance of the dataset and dataloaders in the following lines:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9389416625042205
      ],
      "excerpt": "We provide a command line argument vgg-depth to generate VGG-like models with different amount of layers (this is possible because the 3x3 convolutions with padding = 1 do not change spatial dimensionality and can be stacked arbitrarily). We follow the pattern of the original paper, where e.g. depth = 16 corresponds to VGG-D and depth = 19 corresponds to VGG-E etc. As an example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9809711115253219
      ],
      "excerpt": "In theory you can add your own non-VGG like networks by adding the definition to the lib/models/architectures.py file. We provide convolutional and classifier blocks (with BN and batch-norm and multiple layers) for this. An interesting thing to do would be to evaluate whether our findings hold for e.g. residual networks (vgg-like networks with skip connections). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch implementation of our paper \"Rethinking Layer-wise Feature Amounts in Convolutional Neural Network Architectures\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MrtnMndt/Rethinking_CNN_Layerwise_Feature_Amounts/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Sat, 25 Dec 2021 14:47:05 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MrtnMndt/Rethinking_CNN_Layerwise_Feature_Amounts/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "MrtnMndt/Rethinking_CNN_Layerwise_Feature_Amounts",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8760995667515016
      ],
      "excerpt": "You can find the complete CRACT workshop here: https://ml-critique-correct.github.io  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.928886575279414
      ],
      "excerpt": "python main.py --datasets MNIST \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9325508948798644
      ],
      "excerpt": "python main.py --vgg-depth 19 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8381730050701258
      ],
      "excerpt": "All results are getting saved automatically to a runs/ directory with a time stamped folder. We save a csv file per architecture but keep appending previous results. This way, if the experiment crashes at some point, the run can be resumed with resume-model-id. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MrtnMndt/Rethinking_CNN_Layerwise_Feature_Amounts/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2018, Martin Mundt\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rethinking Layer-wise Feature Amounts in Convolutional Neural Network Architectures",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rethinking_CNN_Layerwise_Feature_Amounts",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "MrtnMndt",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MrtnMndt/Rethinking_CNN_Layerwise_Feature_Amounts/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Our code can be used with default arguments by simply executing\n\n`python main.py`\n\nusing python 3.5 (although the code should generally work with other python versions as well).\n\nThis will launch the CIFAR-10 experiment with approx. 200 architectural variants of the VGG-16 (D) network as specified in the paper. The dataset will be downloaded automatically and a folder per experiment created with a time stamp. All necessary parameters, ranging from datasets to hyper-parameters such as learning rate, mini-batch size as well as architectures, are exposed in the command line parser that can be found in *lib/cmdparser.py*. Note that data pre-processing (global contrast normalization) is off by default and can be added with `--preprocessing True`.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Sat, 25 Dec 2021 14:47:05 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "As many of the model variants have many filters in early layers, we have implemented a GPU memory usage estimate when the model gets build initially. Taking into account the batch size and model configuration, we automatically split the mini-batch into smaller chunks if the currently available GPU memory is exceeded. The gradients are then accumulated until the specified mini-batch size (default: 128) is reached before an optimizer update is done. While our code can be used with multiple GPUs (through use of PyTorch's DataParallel) this allows our code to be run on any single GPU. \n\nWe note that this check crashes occasionally and memory still gets exceeded because the theoretically allocated memory and the effective memory used by CUDNN differ. We came up with a rough heuristic for this, but *if anyone using our code has a proper solution to this issue, we will very much appreciate feedback, suggestions or a pull request!*. \n\n",
      "technique": "Header extraction"
    }
  ]
}