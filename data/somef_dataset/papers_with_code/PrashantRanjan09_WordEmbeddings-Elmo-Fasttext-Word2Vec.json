{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PrashantRanjan09/WordEmbeddings-Elmo-Fasttext-Word2Vec",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-14T18:20:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-12T07:53:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9750327656054076,
        0.8685437591730714,
        0.8583016418662396
      ],
      "excerpt": "This implementation gives the flexibility of choosing word embeddings on your corpus. One has the option of choosing word Embeddings from ELMo (https://arxiv.org/pdf/1802.05365.pdf) - recently introduced by Allennlp and these word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. Also fastext embeddings (https://arxiv.org/pdf/1712.09405.pdf) published in LREC from Thomas Mikolov and team is available.  \nELMo embeddings outperformed the Fastext, Glove and Word2Vec on an average by 2~2.5% on a simple Imdb sentiment classification task (Keras Dataset). \nword_embeddings.py \u2013 contains all the functions for embedding and choosing which word embedding model you want to choose. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.854409093321573
      ],
      "excerpt": "You have the option of choosing the word vector model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Using pre trained word embeddings (Fasttext, Word2Vec)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PrashantRanjan09/WordEmbeddings-Elmo-Fasttext-Word2Vec/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 31,
      "date": "Thu, 23 Dec 2021 22:05:24 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PrashantRanjan09/WordEmbeddings-Elmo-Fasttext-Word2Vec/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "PrashantRanjan09/WordEmbeddings-Elmo-Fasttext-Word2Vec",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8477324614674181
      ],
      "excerpt": "config.json \u2013 you can mention all your parameters here (embedding dimension, maxlen for padding, etc) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8035990487091483,
        0.8480647042441976
      ],
      "excerpt": "model_params.json - you can mention all your model parameters here (epochs, batch size etc.) \nmain.py \u2013 This is the main file. Just use this file to run in terminal. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PrashantRanjan09/WordEmbeddings-Elmo-Fasttext-Word2Vec/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "WordEmbeddings-ELMo, Fasttext, FastText (Gensim) and Word2Vec",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "WordEmbeddings-Elmo-Fasttext-Word2Vec",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "PrashantRanjan09",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PrashantRanjan09/WordEmbeddings-Elmo-Fasttext-Word2Vec/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 151,
      "date": "Thu, 23 Dec 2021 22:05:24 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "wordembeddings",
      "fasttext",
      "word2vec",
      "fair",
      "glove-embeddings",
      "glove",
      "fasttext-python",
      "wordembedding",
      "gensim-word2vec",
      "gensim",
      "nlp",
      "elmo-8",
      "allennlp",
      "ai2",
      "classification"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run it on the Imdb dataset, \n\n\trun: python main.py\n\nTo run it on your data: comment out line 32-40 and uncomment 41-53\n\n\n",
      "technique": "Header extraction"
    }
  ]
}