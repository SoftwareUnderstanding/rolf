{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2104.09864\">rotary embeddings</a> to transformers in Tesnorflow, following its success as <a href=\"https://blog.eleuther.ai/rotary-embeddings/\">relative positional encoding</a>. Specifically it will make rotating information into any axis of a tensor easy and efficient, whether they be fixed positional or learned. This library will give you state of the art results for positional embedding, at little costs.\n\nMy gut also tells me there is something <a href=\"https://www.nature.com/articles/s41593-021-00821-9\">more</a> to rotations that can be exploited in artificial neural networks.\n\n## Note\nAn implemented version of Pytorch is available in this <a href=\"https://github.com/lucidrains/rotary-embedding-torch\">repository</a>.\n\nThis version is written by converting to the version of Pytorch. \n\nThe three functions of rearrange, irearrange and repeat have been written due to the incompatibility of the einops library with tensorflow 2.x.\n## Install\n\n```bash\n$ pip install rotary-embedding-tensorflow\n```\n\n## Usage\n\n```python\nimport tensorflow as tf\nfrom rotary_embedding_tensorflow import apply_rotary_emb, RotaryEmbedding\n\n# instantiate the positional embedding in your transformer and pass to all your attention layers\n\npos_emb = RotaryEmbedding(dim = 32"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@misc{su2021roformer,\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding}, \n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n    year    = {2021},\n    eprint  = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n\n@misc{rotary-embedding-torch,\n    title   = {Rotary Embeddings - Pytorch}, \n    author  = {Phil Wang (lucidrains)},\n    year    = {2021},\n    url  = {https://github.com/lucidrains/rotary-embedding-torch},\n    publisher = {Github},\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{rotary-embedding-torch,\n    title   = {Rotary Embeddings - Pytorch}, \n    author  = {Phil Wang (lucidrains)},\n    year    = {2021},\n    url  = {https://github.com/lucidrains/rotary-embedding-torch},\n    publisher = {Github},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{su2021roformer,\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding}, \n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n    year    = {2021},\n    eprint  = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AryaAftab/rotary-embedding-tensorflow",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-28T13:02:39Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-12T19:12:10Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8761672874246101,
        0.8857220038233964,
        0.9180867584091293,
        0.952076767082314,
        0.9846030765136158
      ],
      "excerpt": "A standalone library for adding <a href=\"https://arxiv.org/abs/2104.09864\">rotary embeddings</a> to transformers in Tesnorflow, following its success as <a href=\"https://blog.eleuther.ai/rotary-embeddings/\">relative positional encoding</a>. Specifically it will make rotating information into any axis of a tensor easy and efficient, whether they be fixed positional or learned. This library will give you state of the art results for positional embedding, at little costs. \nMy gut also tells me there is something <a href=\"https://www.nature.com/articles/s41593-021-00821-9\">more</a> to rotations that can be exploited in artificial neural networks. \nAn implemented version of Pytorch is available in this <a href=\"https://github.com/lucidrains/rotary-embedding-torch\">repository</a>. \nThis version is written by converting to the version of Pytorch.  \nThe three functions of rearrange, irearrange and repeat have been written due to the incompatibility of the einops library with tensorflow 2.x. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9215812865940216
      ],
      "excerpt": ": queries and keys for frequencies to be rotated into \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8265780490682185
      ],
      "excerpt": ": broadcat function makes this easy without a bunch of expands \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9163138123498547
      ],
      "excerpt": ": you could also concat the rotations together and pass it in all at once \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of Rotary Embeddings, from the Roformer paper, in Tensorflow ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AryaAftab/rotary-embedding-tensorflow/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 19:18:47 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AryaAftab/rotary-embedding-tensorflow/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "AryaAftab/rotary-embedding-tensorflow",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ pip install rotary-embedding-tensorflow\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8419387638069318
      ],
      "excerpt": "This version is written by converting to the version of Pytorch.  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.925671696398174,
        0.8801854956928516
      ],
      "excerpt": "import tensorflow as tf \nfrom rotary_embedding_tensorflow import apply_rotary_emb, RotaryEmbedding, broadcat \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925671696398174,
        0.8628110089442617,
        0.8801854956928516,
        0.8202561346525306
      ],
      "excerpt": "import tensorflow as tf \nfrom tensorflow.keras import layers \nfrom rotary_embedding_tensorflow import apply_learned_rotations \nx = tf.random.normal((1, 1024, 512)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.84760898352076
      ],
      "excerpt": "rots = tf.concat((rots1, rots2), axis = -1) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AryaAftab/rotary-embedding-tensorflow/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Arya Aftab\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Rotary Embeddings - Tensorflow",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "rotary-embedding-tensorflow",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "AryaAftab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AryaAftab/rotary-embedding-tensorflow/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Thu, 23 Dec 2021 19:18:47 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tensorflow2",
      "deeplearning",
      "positional-encoding"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nimport tensorflow as tf\nfrom rotary_embedding_tensorflow import apply_rotary_emb, RotaryEmbedding\n\n#: instantiate the positional embedding in your transformer and pass to all your attention layers\n\npos_emb = RotaryEmbedding(dim = 32)\n\n#: generate the rotations\n\nfreqs = pos_emb(tf.range(1024), cache_key = 1024) #: cache with a key that is the sequence length, so that it does not need to recompute\n\n#: mock queries and keys\n\nq = tf.random.normal((1, 1024, 64)) #: queries - (batch, seq len, dimension of head)\nk = tf.random.normal((1, 1024, 64)) #: keys\n\n#: apply the rotations to your queries and keys after the heads have been split out, but prior to the dot product and subsequent softmax (attention)\n\nfreqs = freqs[None, ...] #: expand dimension for batch dimension\nq = apply_rotary_emb(freqs, q)\nk = apply_rotary_emb(freqs, k)\n\n#: then do your attention with your queries (q) and keys (k)\n```\n\nIf you do all the steps above correctly, you should see a dramatic improvement during training\n\n",
      "technique": "Header extraction"
    }
  ]
}