{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2005.04078"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9964795866512832
      ],
      "excerpt": "(IEEE Xplore, arXiv)   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9578459081533138
      ],
      "excerpt": "Institute for Automotive Engineering (ika), RWTH Aachen University \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9933537248551054,
        0.9999933077262751
      ],
      "excerpt": "  author={L. {Reiher} and B. {Lampe} and L. {Eckstein}}, \n  booktitle={2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)},  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9664456561658856,
        0.9950371986714769
      ],
      "excerpt": "  year={2020}, \n  doi={10.1109/ITSC45102.2020.9294462}} \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ika-rwth-aachen/Cam2BEV",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-30T13:50:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T12:17:20Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9968770616767347,
        0.9056304233708298
      ],
      "excerpt": "This repository contains the official implementation of our methodology for the computation of a semantically segmented bird's eye view (BEV) image given the images of multiple vehicle-mounted cameras as presented in our paper: \nA Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird\u2019s Eye View \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "Lennart Reiher, Bastian Lampe, and Lutz Eckstein \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9990462267109828,
        0.9206779160228614
      ],
      "excerpt": "Abstract \u2014 Accurate environment perception is essential for automated driving. When using monocular cameras, the distance estimation of elements in the environment poses a major challenge. Distances can be more easily estimated when the camera perspective is transformed to a bird's eye view (BEV). For flat surfaces, Inverse Perspective Mapping (IPM) can accurately transform images to a BEV. Three-dimensional objects such as vehicles and vulnerable road users are distorted by this transformation making it difficult to estimate their position relative to the sensor. This paper describes a methodology to obtain a corrected 360\u00b0 BEV image given images from multiple vehicle-mounted cameras. The corrected BEV image is segmented into semantic classes and includes a prediction of occluded areas. The neural network approach does not rely on manually labeled data, but is trained on a synthetic dataset in such a way that it generalizes well to real-world data. By using semantically segmented images as input, we reduce the reality gap between simulated and real-world data and are able to show that our method can be successfully applied in the real world. Extensive experiments conducted on the synthetic data demonstrate the superiority of our approach compared to IPM. \nWe hope our paper, data and code can help in your research. If this is the case, please cite: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9350232276671755
      ],
      "excerpt": "\u251c\u2500\u2500 data                        #: where our synthetic datasets are downloaded to by default   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8496324297873032
      ],
      "excerpt": "\u2502   \u251c\u2500\u2500 architecture                #: TensorFlow implementations of neural network architectures \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9805663979881232
      ],
      "excerpt": "    \u251c\u2500\u2500 camera_configs              #: files defining the intrinsics/extrinsics of the cameras used in our datasets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9173388494151308
      ],
      "excerpt": "    \u251c\u2500\u2500 ipm                         #: script for generating a classical homography image by means of IPM \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9713579050471381
      ],
      "excerpt": "We provide two synthetic datasets, which can be used to train the neural networks. The datasets are hosted in the Cam2BEV Data Repository. Both datasets were used to produce the results presented in our paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9865499819051644
      ],
      "excerpt": "For more information regarding the data, please refer to the repository's README. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109816489625613
      ],
      "excerpt": "Our paper describes two preprocessing techniques: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9879326911512153
      ],
      "excerpt": "Traffic participants and static obstacles may occlude parts of the environment making predictions for those areas in a BEV image mostly impossible. In order to formulate a well-posed problem, an additional semantic class needs to be introduced to the label images for areas in BEV, which are occluded in the camera perspectives. To this end, preprocessing/occlusion can be used. See below for an example of the occlusion preprocessing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8088262613230242
      ],
      "excerpt": "Run the following command to process the original label images of dataset 1_FRLR and introduce an occluded class. You need to provide camera intrinsics/extrinsics for the drone camera and all vehicle-attached cameras (in the form of the yaml files). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9364301448239745,
        0.9941114986841957
      ],
      "excerpt": "See preprocessing/occlusion/README.md for more information. \nAs part of the incorporation of the Inverse Perspective Mapping (IPM) technique into our methods, the homographies, i.e. the projective transformations between vehicle camera frames and BEV need to be computed. As a preprocessing step to the first variation of our approach (Section III-C), IPM is applied to all images from the vehicle cameras. The transformation is set up to capture the same field of view as the ground truth BEV image. To this end, preprocessing/ipm can be used. See below for an example homography image computed from images of four vehicle-mounted cameras. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8762379005151244
      ],
      "excerpt": "Note: To save time, we also provide already preprocessed data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9364301448239745
      ],
      "excerpt": "See preprocessing/ipm/README.md for more information. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9858838310037233
      ],
      "excerpt": "We provide implementations for the use of the neural network architectures DeepLab and uNetXST in model/architecture. DeepLab comes with two different backbone networks: MobileNetV2 or Xception. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8365963593998759
      ],
      "excerpt": "set input-training and the other input directory parameters to a list of folders containing the images from each camera (e.g. [data/front, data/rear, data/left, data/right]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9073175360141144,
        0.9326845704311415,
        0.8137898663802444
      ],
      "excerpt": "we provide these homographies for our two datasets in preprocessing/homography_converter/uNetXST_homographies/1_FRLR.py and preprocessing/homography_converter/uNetXST_homographies/2_F.py \nin order to compute these homographies for different camera configurations, follow the instructions in preprocessing/homography_converter \nThe image datasets we provide include all 30 CityScapes class colors. How these are reduced to say 10 classes is defined in the one-hot conversion files in model/one_hot_conversion. Use the training parameters --one-hot-palette-input and --one-hot-palette-label to choose one of the files. You can easily create your own one-hot conversion file, they are quite self-explanatory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "TensorFlow Implementation for Computing a Semantically Segmented Bird's Eye View (BEV) Image Given the Images of Multiple Vehicle-Mounted Cameras.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ika-rwth-aachen/Cam2BEV/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 53,
      "date": "Wed, 22 Dec 2021 22:19:36 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ika-rwth-aachen/Cam2BEV/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ika-rwth-aachen/Cam2BEV",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ika-rwth-aachen/Cam2BEV/master/data/download.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Run the training script with `--help`-flag or have a look at one of the provided exemplary config files to see what parameters you can easily set.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We suggest to setup a **Python 3.8** virtual environment (e.g. by using _virtualenv_ or _conda_). Inside the virtual environment, users can then use _pip_ to install all package dependencies. The most important packages are _TensorFlow 2.7_ and _OpenCV 4.5_\n```bash\npip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9081823518352278
      ],
      "excerpt": "cd preprocessing/occlusion \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9081823518352278
      ],
      "excerpt": "cd preprocessing/ipm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.83773631402575
      ],
      "excerpt": "The following commands will guide you through training uNetXST on dataset 1_FRLR. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8717746993898107
      ],
      "excerpt": "If you adjust --one-hot-palette-label, you will also need to modify --loss-weights. Either omit the parameter to weight all output classes evenly, or compute new suitable loss weights. The weights found in the provided config files were computed (from the model directory) with the following Python snippet. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.874766740666588
      ],
      "excerpt": "<img src=\"assets/teaser.gif\" align=\"right\" width=320 height=200> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.876159424168093,
        0.9247043267631385
      ],
      "excerpt": "./occlusion.py \\ \n    --batch ../../data/1_FRLR/train/bev \\ \n    --output ../../data/1_FRLR/train/bev+occlusion \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8820142249990125,
        0.9247043267631385
      ],
      "excerpt": "./ipm.py --batch --cc \\ \n    --output ../../data/1_FRLR/train/homography \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.894036606553709
      ],
      "excerpt": "    ../../data/1_FRLR/train/rear \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.894036606553709
      ],
      "excerpt": "    ../../data/1_FRLR/train/left \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8100695784007182
      ],
      "excerpt": "    ../../data/1_FRLR/train/right \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9210745324031628
      ],
      "excerpt": "Use the scripts model/train.py, model/evaluate.py, and model/predict.py to train a model, evaluate it on validation data, and make predictions on a testing dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.867176140926261
      ],
      "excerpt": "Start training uNetXST by passing the provided config file model/config.1_FRLR.unetxst.yml. Training will automatically stop if the MIoU score on the validation dataset is not rising anymore. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8982450259545369,
        0.8050285457931645,
        0.8237272824901091
      ],
      "excerpt": "./train.py -c config.1_FRLR.unetxst.yml \nYou can visualize training progress by pointing TensorBoard to the output directory (model/output by default). Training metrics will also be printed to stdout. \nBefore evaluating your trained model, set the parameter model-weights to point to the best_weights.hdf5 file in the Checkpoints folder of its model directory. Then run evaluation to compute a confusion matrix and class IoU scores. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8136119130970406
      ],
      "excerpt": "set input-training and the other input directory parameters to the folders containing the homography images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802527598699518,
        0.8414586565135456
      ],
      "excerpt": "set input-training and the other input directory parameters to a list of folders containing the images from each camera (e.g. [data/front, data/rear, data/left, data/right]) \nset unetxst-homographies to a Python file containing the homographies as a list of NumPy arrays stored in a variable H (e.g. ../preprocessing/homography_converter/uNetXST_homographies/1_FRLR.py)   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9457175861910134,
        0.9535879515266492
      ],
      "excerpt": "import numpy as np \nimport utils \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ika-rwth-aachen/Cam2BEV/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright 2020 Institute for Automotive Engineering of RWTH Aachen University.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "<img src=\"assets/logo.png\" width=50> Cam2BEV",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Cam2BEV",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ika-rwth-aachen",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ika-rwth-aachen/Cam2BEV/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 275,
      "date": "Wed, 22 Dec 2021 22:19:36 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "computer-vision",
      "machine-learning",
      "deep-learning",
      "autonomous-vehicles",
      "ipm",
      "segmentation",
      "sim2real",
      "simulation",
      "birds-eye-view"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You will need to run the preprocessing methods on your own data. A rough outline on what you need to consider:\n- specify camera intrinsics/extrinsics similar to the files found in [preprocessing/camera_configs]([preprocessing/camera_configs])\n- run [preprocessing/occlusion/occlusion.py](preprocessing/occlusion/occlusion.py)\n- run [preprocessing/occlusion/ipm.py](preprocessing/occlusion/ipm.py)\n- compute uNetXST-compatible homographies by following the instructions in [preprocessing/homography_converter](preprocessing/homography_converter)\n- adjust or create a new one-hot conversion file ([model/one_hot_conversion](model/one_hot_conversion))\n- set all training parameters in a dedicated config file\n- start training\n",
      "technique": "Header extraction"
    }
  ]
}