{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1603.08511\n\n* **Colorization Using ConvNet and GAN** : *by Quiwen Fu, Wei-Ting Hsu and Mu-Heng Yang from Stanford University* : \n\nhttps://www.semanticscholar.org/paper/Colorization-Using-ConvNet-and-GAN-Fu-Hsu/327f96c410ab390b2778ffb579d89632b210d337   \n\n\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* **Colorful Image Colorization** - *by Richard Zhang, Phillip Isola, and Alexei A. Efros from University of California, Berkeley*  :\n\nhttps://arxiv.org/abs/1603.08511\n\n* **Colorization Using ConvNet and GAN** : *by Quiwen Fu, Wei-Ting Hsu and Mu-Heng Yang from Stanford University* : \n\nhttps://www.semanticscholar.org/paper/Colorization-Using-ConvNet-and-GAN-Fu-Hsu/327f96c410ab390b2778ffb579d89632b210d337   \n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8374695145293155
      ],
      "excerpt": "Kaggle link : https://www.kaggle.com/arnaud58/landscape-pictures \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CodingWitcher/reverse-oldification",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-02T07:53:11Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-07T17:57:27Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9718133329460967,
        0.9975591475893021,
        0.8600854635089019
      ],
      "excerpt": "(A black and white image of the Golden Gate rejuvenated using colorization by deep learning) \nHere we aim to study and implement a deep learning architecture revolving around the application of a neural network in order to rejuvenate black and white images, that is by colorizing them, and hence making them \u2018alive\u2019 again. Image restoration cum reconstruction has always been a topic of interest, with applications such as extracting useful information from the images of ancient historical artifacts(after reverse-oldifying it to increase the color channels and hence, the amount of information encapsulated), or even bringing a black and white snapshot from the 90s to this century(applications in entertainment industry), or colorizing the popular Mangas(Japanese comics), which are drawn without colors(mostly).  The heavy process has expedited with the advent of the modern deep-learning/Big Data era, where GPUs and TPUs are getting more and more powerful as time progresses, along with a massive surge in the amount of data available to learn from. \nThe dataset is a result of seven researches from the website flickr containing real world photos :  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8519801768041678
      ],
      "excerpt": "Fundamental package for scientific computing in Python3, helping us in creating and managing n-dimensional tensors. A vector can be regarded as a 1-D tensor, matrix as 2-D, and so on. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9169514267667588,
        0.9784169553142925
      ],
      "excerpt": "Is an open source Python3 package designed for image processing. \nIs an open source deep learning framework for dataflow and differentiable programming.  It\u2019s created and maintained by Google. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.950980732850426,
        0.9909432598499112,
        0.9992023744584735,
        0.9234244330901668,
        0.9826291726367935
      ],
      "excerpt": "All the images of the data set will be resized to 224 X 224 dimensions. This is in accordance with the input size expected by the neural network architecture(encoder). \nInstead of feeding the whole set of 3000+ images to our deep learning model in one go, images are fed in batch of 16; so that our Jupyter notebook doesn\u2019t try to load data that may exceed the RAM allotted by Google Colab(~ 12 GB). \nImageDataGenerator, a Tensorflow image pre-processing deep learning module is used for this cumbersome task. Using ImageDataGenerator, we rescale each image in our data set, so that the value of each pixel lies in the range of 0-1, instead of 0-255. This rescaling aids our subsequent deep learning model to converge faster as the skewness of the overall data distribution is tremendously reduced, thereby expediting the gradient descent operation. Moreover, an additional parameter known as \u2018validation_split\u2019 is also passed, which segregates a small fraction of images for cross-validation(after the model is trained on the training data set). \nHelper functions such as convert_lab(for converting an RGB image into LAB format using skimage library), convert_rgb(for converting the constructed LAB image, after A and B channels are predicted, into RGB format using skimage image pre-processing library), and plot_image(for displaying an image) are designed. \nLab  is a color space that completely separates the lightness from color. Think of lightness as some sort of grayscale image, it only has luminosity but, no colors at all. channel L is responsible for that lightness (grayscale) and the other two channels ab are responsible for the colors. as you can see in the images below the color information is embedded in the ab channel.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.998507841181094,
        0.9814491099026043
      ],
      "excerpt": "Auto encoders are deep neural networks used to determine a compressed version of the input data with the lowest amount of loss in information. The concept of PCA(Principal Component Analysis) is to find the best and relevant parameters for training of a model where the dataset has a huge number of parameters. An autoencoder works in a similar fashion. The encoder part of the architecture breaks down the input data into a compressed version ensuring that important data is not lost but the overall size of the data is significantly reduced. This concept is called Dimensionality reduction.    \nOne important property of an autoencoder is that they are data-specific, which means that they will only be able to compress data similar to what they have been trained on. This is why our hold-out cross validation set consists only of landscape images, as those are the ones our convolutional autoencoder had trained on.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.972844944860545
      ],
      "excerpt": "Convolution layer : A convolution is a simple application of a filter to an input that results in an activation. Using convolving filters of the size (3,3), we extract useful features from an image to be colored. By adding an \u2018additional\u2019 stride component, the images are downsampled from their original dimensions.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9025657302058843
      ],
      "excerpt": "Upsampling layer : This is applied in the decoder section once we start reconstructing the image, after the encoder has extracted all the useful features. Upsampling simply results in doubling the dimensions of the input.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9852205047997593
      ],
      "excerpt": "VGG net 16 has been used for our encoder to obtain high accuracy in the feature extraction process. The architecture of VGG-16 is such that it takes an input of size 224 X 224 X 3(hence, the image resizing in pre-processing), and outputs a softmax prediction over a thousand classes. As discussed above, this network also has immense stacking of convolving layers, and the last feature extraction layer results in an output of 7 X 7X 512. Hence, we will use these layers for feature extraction for our own encoder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9355393810808563,
        0.9955206164628371,
        0.9176103244450756,
        0.9182924285141356,
        0.807196094899949,
        0.911447747979744
      ],
      "excerpt": "(Note that VGG 16 takes a three dimensional input, whereas we have only our L channel as input, which is one dimensional. Hence, in order to satisfy the need of this architecture, we will make our input 3-D by stacking the \u2018L\u2019 layer behind itself two more times, forming a total of three dimensions ) \nIn short what\u2019s happening with our autoencoder model. The input image has only one channel, that is \u2018L\u2019(which is later stacked behind itself to give an impression of depth 3 in order to satisfy our encoder model), and output comprises two sets of predictions, that is \u2018a\u2019 and \u2018b\u2019 channel; which are then combined with the input \u2018L\u2019 to form a reconstructed Lab image. This Lab image is subsequently converted into an RGB image, by the convert_rgb() function earlier defined and displayed.  \nThe relu activation has been used throughout the network\u2019s architecture, however for the final layer, we have used tanh activation function. The reason is before preprocessing the \u2018a\u2019 and \u2018b\u2019 channel values were divided by 128 as the range of both channels is in (-127, 128). Hence, by dividing the values, each pixel lies between (-1,1).  The mathematical \u201ctanh\u201d activation function outputs in the range of (-1, 1), therefore making it the ideal function of choice. \nThe model was trained on 3,680 images for 1000 epochs. An overall accuracy of 86.12% was achieved by the model in coloring the images of dimension : (224, 224) \nThe training process took roughly four- five hours to complete. The entire model was trained using Google Collaboratory notebooks, which are powerful GPU-enabled Jupyter notebooks supported by Google cloud.  \nThe RAM availability for the whole project was roughly around 12 GB using a free Tesla K-80 GPU.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Using Machine Learning to rejuvenate black and white images ! ~~",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CodingWitcher/reverse-oldification/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 15:55:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/CodingWitcher/reverse-oldification/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "CodingWitcher/reverse-oldification",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/CodingWitcher/reverse-oldification/master/reverse_oldify.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The training and validation numpy array are created. All the images in the training set are loaded one after the other, and are subsequently converted into LAB format. The **x_train list** contains the **\u2018L\u2019 channel of our LAB image**, whereas y_train contains the **\u2018A\u2019 and \u2018B\u2019 channels**. Same goes for x_val and y_val. Finally the list is converted into a numpy array, as they are the input type desired by Tensorflow.\n* The shape of x_train observed is (3680, 224, 224) signifying that this 3-D tensor contains 3680 images, where each image has a dimension of 224 X 224. This makes sense as earlier we rescaled all the images to 224 X 224. Moreover, as the training set contains only the \u2018L\u2019 channel, therefore each image has only dimension(depth wise). \n\n* The shape of x_val observed is (3680, 224, 224, 2) signifying that this 3-D tensor contains 3680 images, where each image has a dimension of 224 X 224 along with depth 2. This is because both \u2018a\u2019 and \u2018b\u2019 channels of the Lab image are incorporated in the x_val tensor. All the images in a and b are 224 X 224, and they are stacked together in x_val, imparting the shape an overall depth of two. \n\n![](https://github.com/CodingWitcher/reverse-oldification/blob/master/images/lab.png)\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8314410601033915
      ],
      "excerpt": "Some sample images from landscape data set :  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9029016578810056
      ],
      "excerpt": "Initial result 01  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8091818271915538,
        0.9129769092340554
      ],
      "excerpt": "Initial result 02 \nFinal result 01 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8102780086823556
      ],
      "excerpt": "Final result 02  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/CodingWitcher/reverse-oldification/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Reverse-Oldification Of Images",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "reverse-oldification",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "CodingWitcher",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CodingWitcher/reverse-oldification/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Mon, 27 Dec 2021 15:55:07 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "image-processing",
      "exploratory-data-analysis",
      "python3",
      "autoencoders",
      "gpu",
      "keras-tensorflow",
      "opencv",
      "skimage"
    ],
    "technique": "GitHub API"
  }
}