{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1812.05905",
      "https://arxiv.org/abs/1708.05144",
      "https://arxiv.org/abs/1606.03476",
      "https://arxiv.org/abs/1801.00690",
      "https://arxiv.org/abs/1709.06560"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{pytorchrl,\n  author = {Kostrikov, Ilya},\n  title = {PyTorch Implementations of Reinforcement Learning Algorithms},\n  year = {2018},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail}},\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-08-22T15:57:25Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T13:55:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9661857468358137
      ],
      "excerpt": "PPO is great, but Soft Actor Critic can be better for many continuous control tasks. Please check out my new RL repository in jax. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356074087953868
      ],
      "excerpt": "PyBullet (including Racecar, Minitaur and Kuka) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8007499712173752,
        0.8914961974447433,
        0.9606493691648552
      ],
      "excerpt": "I highly recommend PyBullet as a free open source alternative to MuJoCo for continuous control tasks. \nAll environments are operated using exactly the same Gym interface. See their documentations for a comprehensive list. \nTo use the DeepMind Control Suite environments, set the flag --env-name dm.&lt;domain_name&gt;.&lt;task_name&gt;, where domain_name and task_name are the name of a domain (e.g. hopper) and a task within that domain (e.g. stand) from the DeepMind Control Suite. Refer to their repo and their tech report for a full list of available domains and tasks. Other than setting the task, the API for interacting with the environment is exactly the same as for all the Gym environments thanks to dm_control2gym. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.931629310598512,
        0.9651840970820931
      ],
      "excerpt": "Also I'm searching for volunteers to run all experiments on Atari and MuJoCo (with multiple random seeds). \nIt's extremely difficult to reproduce results for Reinforcement Learning methods. See \"Deep Reinforcement Learning that Matters\" for more information. I tried to reproduce OpenAI results as closely as possible. However, majors differences in performance can be caused even by minor differences in TensorFlow and PyTorch libraries. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9296311323113919
      ],
      "excerpt": "Improve performance of KFAC, see kfac.py for more information \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9450036871582932
      ],
      "excerpt": "ACKTR requires some modifications to be made specifically for MuJoCo. But at the moment, I want to keep this code as unified as possible. Thus, I'm going for better ways to integrate it into the codebase. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch implementation of Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO), Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (ACKTR) and Generative Adversarial Imitation Learning (GAIL).",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 706,
      "date": "Thu, 30 Dec 2021 07:45:54 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ikostrikov/pytorch-a2c-ppo-acktr-gail/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ikostrikov/pytorch-a2c-ppo-acktr-gail",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/master/visualize.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.9180317706970327
      ],
      "excerpt": "python main.py --env-name \"PongNoFrameskip-v4\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8962043684218663
      ],
      "excerpt": "python main.py --env-name \"PongNoFrameskip-v4\" --algo ppo --use-gae --lr 2.5e-4 --clip-param 0.1 --value-loss-coef 0.5 --num-processes 8 --num-steps 128 --num-mini-batch 4 --log-interval 1 --use-linear-lr-decay --entropy-coef 0.01 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8565828217461355
      ],
      "excerpt": "python main.py --env-name \"PongNoFrameskip-v4\" --algo acktr --num-processes 32 --num-steps 20 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8397449062828484
      ],
      "excerpt": "python main.py --env-name \"Reacher-v2\" --num-env-steps 1000000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8226620014200005
      ],
      "excerpt": "python main.py --env-name \"Reacher-v2\" --algo ppo --use-gae --log-interval 1 --num-steps 2048 --num-processes 1 --lr 3e-4 --entropy-coef 0 --value-loss-coef 0.5 --ppo-epoch 10 --num-mini-batch 32 --gamma 0.99 --gae-lambda 0.95 --num-env-steps 1000000 --use-linear-lr-decay --use-proper-time-limits \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8857366393967361
      ],
      "excerpt": "python enjoy.py --load-dir trained_models/a2c --env-name \"PongNoFrameskip-v4\" \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ikostrikov/pytorch-a2c-ppo-acktr-gail/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 Ilya Kostrikov\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-a2c-ppo-acktr",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-a2c-ppo-acktr-gail",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ikostrikov",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python 3 (it might work with Python 2, but I didn't test it)\n* [PyTorch](http://pytorch.org/)\n* [Stable baselines3](https://github.com/DLR-RM/stable-baselines3)\n\nIn order to install requirements, follow:\n\n```bash\n#: PyTorch\nconda install pytorch torchvision -c soumith\n\n#: Other requirements\npip install -r requirements.txt\n\n#: Gym Atari\nconda install -c conda-forge gym-atari\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2645,
      "date": "Thu, 30 Dec 2021 07:45:54 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "reinforcement-learning",
      "deep-learning",
      "deep-reinforcement-learning",
      "actor-critic",
      "advantage-actor-critic",
      "a2c",
      "ppo",
      "proximal-policy-optimization",
      "acktr",
      "second-order",
      "hessian",
      "natural-gradients",
      "atari",
      "mujoco",
      "roboschool",
      "continuous-control",
      "kfac",
      "kronecker-factored-approximation",
      "ale"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is a PyTorch implementation of\n* Advantage Actor Critic (A2C), a synchronous deterministic version of [A3C](https://arxiv.org/pdf/1602.01783v1.pdf)\n* Proximal Policy Optimization [PPO](https://arxiv.org/pdf/1707.06347.pdf)\n* Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation [ACKTR](https://arxiv.org/abs/1708.05144)\n* Generative Adversarial Imitation Learning [GAIL](https://arxiv.org/abs/1606.03476)\n\nAlso see the OpenAI posts: [A2C/ACKTR](https://blog.openai.com/baselines-acktr-a2c/) and [PPO](https://blog.openai.com/openai-baselines-ppo/) for more information.\n\nThis implementation is inspired by the OpenAI baselines for [A2C](https://github.com/openai/baselines/tree/master/baselines/a2c), [ACKTR](https://github.com/openai/baselines/tree/master/baselines/acktr) and [PPO](https://github.com/openai/baselines/tree/master/baselines/ppo1). It uses the same hyper parameters and the model since they were well tuned for Atari games.\n\nPlease use this bibtex if you want to cite this repository in your publications:\n\n    @misc{pytorchrl,\n      author = {Kostrikov, Ilya},\n      title = {PyTorch Implementations of Reinforcement Learning Algorithms},\n      year = {2018},\n      publisher = {GitHub},\n      journal = {GitHub repository},\n      howpublished = {\\url{https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail}},\n    }\n\n",
      "technique": "Header extraction"
    }
  ]
}