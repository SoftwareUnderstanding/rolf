{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1705.06830"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8650879946950151
      ],
      "excerpt": "Bas\u00e9 sur cet article et cette page TF Hub.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Paul BUCAMP \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Notre \u00e9tude s'est tout d\u2019abord port\u00e9e sur le mod\u00e8le NST de d2l se basant sur une optimisation par entra\u00eenement. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998033479129727
      ],
      "excerpt": "Redimensionnement des images \u00e0 la m\u00eame taille : on garde le ratio de l'image de contenu en diminuant la taille si besoin afin de r\u00e9duire la dur\u00e9e de l'entra\u00eenement. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999998305592755,
        0.9502140581228744,
        0.9999257357631157
      ],
      "excerpt": "De mani\u00e8re g\u00e9n\u00e9rale, le mod\u00e8le tire parti des repr\u00e9sentations en couches d\u2019un r\u00e9seau neuronal convolutif (CNN) afin d\u2019appliquer automatiquement le style d\u2019une image \u00e0 une autre image. Pour cela, deux images sont utilis\u00e9es : l\u2019image de contenu et l\u2019image de style. Un r\u00e9seau neuronal est ensuite utilis\u00e9 pour modifier l\u2019image de contenu pour la rendre proche en style de l\u2019image de style. Il s\u2019agit donc d'optimiser l\u2019image de contenu avec l\u2019image de style. \nLa m\u00e9thode pr\u00e9sent\u00e9e par d2l consid\u00e8re l\u2019image synth\u00e9tis\u00e9e (en sortie) comme les param\u00e8tres d\u2019un mod\u00e8le, initialis\u00e9 avec l\u2019image de contenu.  \nL\u2019algorithme utilise un mod\u00e8le pr\u00e9-entra\u00een\u00e9 d\u2019extraction hi\u00e9rarchique de caract\u00e9ristiques. Compos\u00e9 de plusieurs couches, on peut choisir la sortie de certains d\u2019entre eux comme caract\u00e9ristique de contenu ou de style.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9700494325394076,
        0.8707921115408231,
        0.9847135926867628,
        0.9990438480639905
      ],
      "excerpt": "Les fonctions de perte sont calcul\u00e9es \u00e0 travers une forward propagation, puis les param\u00e8tres du mod\u00e8le (l\u2019image synth\u00e9tis\u00e9e en sortie) sont mis jour \u00e0 travers une back propagation.  \nOn compte 3 fonctions de perte :  \ncontent loss (rend l'image synth\u00e9tis\u00e9e et l'image de contenu proches en terme de contenu) ; \nstyle loss (rend l'image synth\u00e9tis\u00e9e et l'image de style proches en terme de style) ; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9504763884891976
      ],
      "excerpt": "A la fin de l\u2019entra\u00eenement, les param\u00e8tres du mod\u00e8le sont r\u00e9cup\u00e9r\u00e9s pour g\u00e9n\u00e9rer l\u2019image synth\u00e9tis\u00e9e finale. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999998985057747
      ],
      "excerpt": "La mise \u00e0 jour it\u00e9rative d'une image pour synth\u00e9tiser une texture visuelle ou un transfert de style artistique \u00e0 une image est une proc\u00e9dure d'optimisation lente. Elle exclut aussi toute possibilit\u00e9 d'apprentissage d'une repr\u00e9sentation d'un style de peinture. De plus, la modification des (hyper)param\u00e8tres n'apportent pas beaucoup de changements sur l'image synth\u00e9tis\u00e9e \u00e0 la fin de son optimisation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999186073929216,
        0.9864633984068458,
        0.9999999976050447,
        0.9999998541313708,
        0.9999999998889848,
        0.9999999851885947,
        0.9851576054070618,
        0.9962061571389547,
        0.999634466428013
      ],
      "excerpt": "Les images entr\u00e9es par l'utilisateur sont d'abord r\u00e9duites en taille pour r\u00e9duire le temps de calcul (et le temps d'envoi pour la version en ligne). Un URI en base 64 est cr\u00e9e pour chaque image et envoy\u00e9 au serveur avant d'\u00eatre pass\u00e9 \u00e0 la fonction de pr\u00e9diction en elle-m\u00eame. Enfin, unee fois charg\u00e9es sous forme matricielle, la valeur de chaque pixel est normalis\u00e9e. \nLes travaux sur le NST ont tout d\u2019abord port\u00e9 sur une m\u00e9thode d'optimisation par mise \u00e0 jour it\u00e9rative de l\u2019image synth\u00e9tis\u00e9e tel que pour la m\u00e9thode de pr\u00e9sent\u00e9e ci-dessus. \nEnsuite, d\u2019autres travaux ont introduit un second r\u00e9seau apprenant la transformation de l\u2019image de contenu \u00e0 sa version artistique. Ce r\u00e9seau de transfert de style est un r\u00e9seau de neurones convolutifs formul\u00e9 dans la structure d'un codeur/d\u00e9codeur. Le r\u00e9seau r\u00e9sultant peut r\u00e9aliser le transfert de style d\u2019une image beaucoup plus rapidement, mais un r\u00e9seau distinct doit \u00eatre entra\u00een\u00e9 pour chaque style de peinture. Cela repr\u00e9sente du gaspillage dans la mesure o\u00f9 certains styles de peintures partagent des textures, des palettes de couleurs ou s\u00e9mantiques d\u2019identification de sc\u00e8ne communes.  \nL\u2019id\u00e9e suivante a donc \u00e9t\u00e9 de construire un r\u00e9seau de transfert de style avec une architecture typique encoder/decoder mais sp\u00e9cialisant les param\u00e8tres de normalisation pour chaque style de peinture : conditional instance normalization, la normalisation de chaque unit\u00e9 d\u2019activation, de sorte que la transformation lin\u00e9aire de chaque peinture soit unique. Ainsi, un vecteur d\u2019int\u00e9gration (embedding) d\u2019environ 300-d repr\u00e9sente le style artistique d\u2019une peinture. \nEn explorant cette question, un fait tr\u00e8s surprenant a \u00e9t\u00e9 trouv\u00e9 sur le r\u00f4le de la normalisation dans les r\u00e9seaux de transfert de style : pour mod\u00e9liser un style, il suffit de sp\u00e9cialiser les param\u00e8tres de mise \u00e0 l'\u00e9chelle et de d\u00e9calage apr\u00e8s la normalisation \u00e0 chaque style sp\u00e9cifique. En d'autres termes, tous les poids convolutifs d'un r\u00e9seau de transfert de style peuvent \u00eatre partag\u00e9s entre de nombreux styles, et il suffit de r\u00e9gler les param\u00e8tres pour une transformation affine apr\u00e8s normalisation pour chaque style. \nLe mod\u00e8le NST de Google Brain est donc la combinaison de mod\u00e8les dont la m\u00e9thode est d\u2019apprendre la caract\u00e9risation de l\u2019image de style aux param\u00e8tres de style directement. Dans le cas du mod\u00e8le de Google Brain, pour faire le NST d\u2019une image de style vers une image de contenu, deux r\u00e9seaux sont utilis\u00e9s : le style transfer <img src=\"https://render.githubusercontent.com/render/math?math=T(.%2C%5Cvec%20S)\"> et le style prediction <img src=\"https://render.githubusercontent.com/render/math?math=P(.)\">.  \nR\u00e9seau <img src=\"https://render.githubusercontent.com/render/math?math=T\">:  <img src=\"https://render.githubusercontent.com/render/math?math=T(.%2C%5Cvec%20S)\"> peut ainsi faire le transfert de style en une seule propagation avant de n\u2019importe quelle image de style \u00e0 partir du moment qu\u2019on conna\u00eet le vecteur <img src=\"https://render.githubusercontent.com/render/math?math=%5Cvec%20S%20%3D%20(%5Cgamma_s%2C%20%5Cbeta_s)\">, calcul\u00e9 par le r\u00e9seau <img src=\"https://render.githubusercontent.com/render/math?math=P(.)\">. \n<img src=\"https://render.githubusercontent.com/render/math?math=T(.%2C%5Cvec%20S)\"> utilise la normalisation pour transformer une couche d'activation <img src=\"https://render.githubusercontent.com/render/math?math=z\"> en une activation normalis\u00e9e <img src=\"https://render.githubusercontent.com/render/math?math=%5Ctilde%20z\"> sp\u00e9cifique \u00e0 un style <img src=\"https://render.githubusercontent.com/render/math?math=s\"> de sorte que <img src=\"https://render.githubusercontent.com/render/math?math=%5Ctilde%20z%20%3D%20%5Cgamma_s%5Cleft(%5Cfrac%7Bz%20-%20%5Cmu%7D%7B%5Csigma%7D%5Cright)%20%2B%20%5Cbeta_s\">, o\u00f9 <img src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bz%20-%20%5Cmu%7D%7B%5Csigma%7D\"> repr\u00e9sente la couche <img src=\"https://render.githubusercontent.com/render/math?math=z\"> normalis\u00e9e, et <img src=\"https://render.githubusercontent.com/render/math?math=%5Ctilde%20z\"> est la normalisation d'instance conditionnelle. Ils effectuent une mise \u00e0 l'\u00e9chelle et un d\u00e9placement \u00e0 l'aide de vecteurs de param\u00e8tres d\u00e9pendants du style. \nR\u00e9seau <img src=\"https://render.githubusercontent.com/render/math?math=P\">: Le r\u00e9seau <img src=\"https://render.githubusercontent.com/render/math?math=P(.)\"> fait la pr\u00e9diction du vecteur <img src=\"https://render.githubusercontent.com/render/math?math=%5Cvec%20S\"> \u00e0 partir de l\u2019image de style donn\u00e9e en entr\u00e9e. Il est bas\u00e9 sur le mod\u00e8le pr\u00e9-entra\u00een\u00e9 Inception-v3 : selon l\u2019article, il calcule la moyenne \u00e0 travers tous les channels d\u2019activation de la couche Mixed-6e et retourne un feature vector sur lequel il applique 2 couches fully-connected pour pr\u00e9dire <img src=\"https://render.githubusercontent.com/render/math?math=%5Cvec%20S\">. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9865374311292784,
        0.9992485561361633,
        0.9997120744252818
      ],
      "excerpt": "Cette m\u00e9thode est tr\u00e8s avantageuse, que ce soit par la qualit\u00e9 des r\u00e9sultats produits, par la vitesse de rendu, ou par l'\u00e9tendue des styles compatibles.  \nDes am\u00e9liorations d'ordre pratique pourraient \u00eatre faites au niveau de l'interface utilisateur, comme le choix de la taille de l'image de sortie. \nNous avons mis en place une plateforme d'essai du mod\u00e8le de Google Brain en utilisant le code disponible sur TF Hub et en l'int\u00e9grant \u00e0 un serveur Python en utilisant Flask. Le serveur dispose d'une interface web (HTML/CSS/JS) qui permet \u00e0 l'utilisateur de choisir une image de style et une image de contenu et d'obtenir le r\u00e9sultat du transfert de style. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9939383674869147,
        0.8348362997093185
      ],
      "excerpt": "Cliquer sur les boutons Browse pour choisir une image de contenu et de style :  \nUne miniature de pr\u00e9visualisation appara\u00eet pour chaque image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9538405059445785
      ],
      "excerpt": "Actuellement, les m\u00e9thodes et mod\u00e8les pr\u00e9sent\u00e9s r\u00e9alisent le transfert de style d'une image \u00e0 une autre image, typiquement d'une peinture ou texture \u00e0 une photographie. Une autre application du NST serait l'application d'un style calligraphique \u00e0 du texte. Ensuite, une possibilit\u00e9 serait l'application d'un style visuel (peinture/texture) en temps r\u00e9el \u00e0 une vid\u00e9o. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9874896680299456,
        0.9972474703159213
      ],
      "excerpt": "Tutoriel de NST d2l : https://d2l.ai/chapter_computer-vision/neural-style.html  \nInception-v3, Rethinking the Inception Architecture for Computer Vision : https://arxiv.org/pdf/1512.00567.pdf  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/RemiArbache/style-transfer-M2",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-03T15:00:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-05T21:08:50Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Le NST permet, \u00e0 partir d'une image de contenu (ex: une photo) et d'une image de r\u00e9f\u00e9rence de style (ex: une peinture), de cr\u00e9er une image qui maintienne le contenu de la premi\u00e8re tout en reproduisant le style de la seconde. [Source](https://www.tensorflow.org/tutorials/generative/style_transfer )\n\nAutrement dit, l\u2019algorithme NST manipule des images dans le but de leur donner l\u2019apparence ou le style visuel d\u2019une autre image. Ces algorithmes utilisent des r\u00e9seaux neuronaux profonds pour pouvoir r\u00e9aliser la transformation d\u2019images.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Ce projet a pour objectif la d\u00e9couverte du Neural Style Transfer (NST) \u00e0 travers l\u2019utilisation des mod\u00e8les de d2l et de Google Brain. L'objectif est de r\u00e9aliser un transfert d'une image de style \u00e0 une image de contenu, et et de cr\u00e9er un site web d\u2019application du mod\u00e8le de Google Brain. \nCe readMe permet d\u2019expliquer la m\u00e9thode de NST de d2l suivant un principe d\u2019optimisation pour ensuite la comparer avec la m\u00e9thode de Google Brain qui permet l\u2019obtention en une boucle de l\u2019image synth\u00e9tis\u00e9e sans entra\u00eenement pr\u00e9alable sur les images donn\u00e9es en entr\u00e9e.\nCe d\u00e9p\u00f4t contient la mise en place d'un site web / API pour faire la d\u00e9monstration de l\u2019application du mod\u00e8le choisi, cr\u00e9\u00e9 par Google Brain, prenant des images en entr\u00e9e pour donner en sortie l\u2019image synth\u00e9tis\u00e9e.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8070593771974934
      ],
      "excerpt": "Sch\u00e9ma exemple avec un CNN d\u2019extraction de caract\u00e9ristiques \u00e0 3 couches : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Academic project aiming to experiment with Neural Style Transfer",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/RemiArbache/style-transfer-M2/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 23:53:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/RemiArbache/style-transfer-M2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "RemiArbache/style-transfer-M2",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/RemiArbache/style-transfer-M2/main/notebook/neural_style.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.914915317283376,
        0.9717106327039013
      ],
      "excerpt": "python -m flask run \nVersion disponible en ligne heberg\u00e9e par Heroku. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8773344959568669
      ],
      "excerpt": "La version d\u00e9ploy\u00e9e en ligne peut pr\u00e9senter des performances r\u00e9duites car Heroku ne supporte pas tensorflow-gpu et utilise donc tensorflow-cpu. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.834708152648845
      ],
      "excerpt": "Code Inception-v3 GitHub : https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/inception.py#L64  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8295854008755725
      ],
      "excerpt": "python -m flask run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8921498016224351
      ],
      "excerpt": "Deux images (.png, .jpg) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.885229443891788
      ],
      "excerpt": "| <img src=\"images/paysage.jpg\" alt=\"paysage\" style=\"zoom:75%;\" /> |        |  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.885229443891788
      ],
      "excerpt": "| <img src=\"images/paysage.jpg\" alt=\"paysage\" style=\"zoom:75%;\" /> |        |  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8213380337316241
      ],
      "excerpt": "Cliquer sur le bouton Predict pour obtenir un r\u00e9sultat : \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/RemiArbache/style-transfer-M2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "HTML",
      "JavaScript",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 RemiArbache\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "style-transfer-M2",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "style-transfer-M2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "RemiArbache",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/RemiArbache/style-transfer-M2/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 23:53:33 GMT"
    },
    "technique": "GitHub API"
  }
}