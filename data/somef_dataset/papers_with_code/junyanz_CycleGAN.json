{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Code borrows from [pix2pix](https://github.com/phillipi/pix2pix) and [DCGAN](https://github.com/soumith/dcgan.torch). The data loader is modified from [DCGAN](https://github.com/soumith/dcgan.torch) and  [Context-Encoder](https://github.com/pathak22/context-encoder). The generative network is adopted from [neural-style](https://github.com/jcjohnson/neural-style) with [Instance Normalization](https://github.com/DmitryUlyanov/texture_nets/blob/master/InstanceNormalization.lua).\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1605.09782"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code for your research, please cite our [paper](https://junyanz.github.io/CycleGAN/):\n\n```\n@inproceedings{CycleGAN2017,\n  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networkss},\n  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},\n  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},\n  year={2017}\n}\n\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{CycleGAN2017,\n  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networkss},\n  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},\n  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9878673543585983,
        0.9022277929869588,
        0.9361143504680042
      ],
      "excerpt": "Jun-Yan Zhu*,  Taesung Park*, Phillip Isola, Alexei A. Efros \n Berkeley AI Research Lab, UC Berkeley \n In ICCV 2017. (* equal contributions)   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8497496570404576
      ],
      "excerpt": "The code was written by Jun-Yan Zhu and Taesung Park. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8227893355900447
      ],
      "excerpt": "<p><a href=\"https://github.com/leehomyc/cyclegan-1\"> [Tensorflow]</a> (by Harry Yang), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8868558925400613,
        0.8868558925400613,
        0.9893914671403817
      ],
      "excerpt": "- orange2apple (orange -> apple) and apple2orange: trained on ImageNet categories apple and orange. \n- horse2zebra (horse -> zebra) and zebra2horse (zebra -> horse): trained on ImageNet categories horse and zebra. \n- style_monet (landscape photo -> Monet painting style),  style_vangogh (landscape photo  -> Van Gogh painting style), style_ukiyoe (landscape photo  -> Ukiyo-e painting style), style_cezanne (landscape photo  -> Cezanne painting style): trained on paintings and Flickr landscape photos. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9615825383762427
      ],
      "excerpt": "Open this URL in your browser: http://localhost:8000 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/junyanz/CycleGAN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-03-30T15:56:21Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T08:50:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8161748291168422,
        0.9264525518212987
      ],
      "excerpt": "Torch implementation for learning an image-to-image translation (i.e. pix2pix) without input-output pairs, for example: \nNew:  Please check out contrastive-unpaired-translation (CUT), our new unpaired image-to-image translation model that enables fast and memory-efficient training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8099033291246154
      ],
      "excerpt": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9337372482229468
      ],
      "excerpt": "This package includes CycleGAN, pix2pix, as well as other methods like BiGAN/ALI and Apple's paper S+U learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9131876627832857,
        0.8549588799067096
      ],
      "excerpt": "Update: Please check out PyTorch implementation for CycleGAN and pix2pix. \nThe PyTorch version is under active development and can produce results comparable or better than this Torch version. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8971408392605149
      ],
      "excerpt": "Please refer to Model Zoo for more pre-trained models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9691422525939112
      ],
      "excerpt": "(Optionally) start the display server to view results as the model trains. (See Display UI for more details): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9582807447200652
      ],
      "excerpt": "- iphone2dslr_flower (iPhone photos of flowers -> DSLR photos of flowers): trained on Flickr photos. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9709643801597526
      ],
      "excerpt": "There are other options that can be used. For example, you can specify resize_or_crop=crop option to avoid resizing the image to squares. This is indeed how we trained GTA2Cityscapes model in the projet webpage and Cycada model. We prepared the images at 1024px resolution, and used resize_or_crop=crop fineSize=360 to work with the cropped images of size 360x360. We also used lambda_identity=1.0. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833797416181476,
        0.8172075594923899,
        0.9381883625193309
      ],
      "excerpt": "- summer2winter_yosemite: 1273 summer Yosemite images and 854 winter Yosemite images were downloaded using Flickr API. See more details in our paper. \n- monet2photo, vangogh2photo, ukiyoe2photo, cezanne2photo: The art images were downloaded from Wikiart. The real photos are downloaded from Flickr using the combination of the tags landscape and landscapephotography. The training set size of each class is Monet:1074, Cezanne:584, Van Gogh:401, Ukiyo-e:1433, Photographs:6853. \n- iphone2dslr_flower: both classes of images were downloaded from Flickr. The training set size of each class is iPhone:1813, DSLR:3316. See more details in our paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9350665472344665
      ],
      "excerpt": "By default, the server listens on localhost. Pass 0.0.0.0 to allow external connections on any interface: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9982403218747765
      ],
      "excerpt": "Our model does not work well when the test image is rather different from the images on which the model is trained, as is the case in the figure to the left (we trained on horses and zebras without riders, but test here one a horse with a rider).  See additional typical failure cases here. On translation tasks that involve color and texture changes, like many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of dog&lt;-&gt;cat transfiguration, the learned translation degenerates into making minimal changes to the input. We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method. In some cases, this gap may be very hard -- or even impossible,-- to close: for example, our method sometimes permutes the labels for tree and building in the output of the cityscapes photos->labels task. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Software that can generate photos from paintings,  turn horses into zebras,  perform style transfer, and more.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/junyanz/CycleGAN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1852,
      "date": "Fri, 24 Dec 2021 12:39:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/junyanz/CycleGAN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "junyanz/CycleGAN",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/junyanz/CycleGAN/master/datasets/download_dataset.sh",
      "https://raw.githubusercontent.com/junyanz/CycleGAN/master/examples/test_vangogh_style_on_ae_photos.sh",
      "https://raw.githubusercontent.com/junyanz/CycleGAN/master/examples/train_maps.sh",
      "https://raw.githubusercontent.com/junyanz/CycleGAN/master/pretrained_models/download_vgg.sh",
      "https://raw.githubusercontent.com/junyanz/CycleGAN/master/pretrained_models/download_model.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To train CycleGAN model on your own datasets, you need to create a data folder with two subdirectories `trainA` and `trainB` that contain images from domain A and B. You can test your model on your training set by setting ``phase='train'`` in  `test.lua`. You can also create subdirectories `testA` and `testB` if you have test data.\n\nYou should **not** expect our method to work on just any random combination of input and output datasets (e.g. `cats<->keyboards`). From our experiments, we find it works better if two datasets share similar visual content. For example, `landscape painting<->landscape photographs` works much better than `portrait painting <-> landscape photographs`. `zebras<->horses` achieves compelling results while `cats<->dogs` completely fails.  See the following section for more discussion.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Install torch and dependencies from https://github.com/torch/distro\n- Install torch packages `nngraph`, `class`, `display`\n```bash\nluarocks install nngraph\nluarocks install class\nluarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec\n```\n- Clone this repo:\n```bash\ngit clone https://github.com/junyanz/CycleGAN\ncd CycleGAN\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8547506928230144,
        0.9259408091992843
      ],
      "excerpt": "<a href=\"https://github.com/yunjey/mnist-svhn-transfer\">[Minimal PyTorch]</a> (by yunjey), \n<a href=\"https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN\">[Mxnet]</a> (by Ldpe2G), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8917133644880719
      ],
      "excerpt": "bash ./datasets/download_dataset.sh ae_photos \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9465718491881494
      ],
      "excerpt": "bash ./pretrained_models/download_model.sh style_cezanne \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8917133644880719
      ],
      "excerpt": "bash ./datasets/download_dataset.sh horse2zebra \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9530718778138788,
        0.9063585311891094
      ],
      "excerpt": "(CPU only) The same training command without using a GPU or CUDNN. Setting the environment variables gpu=0 cudnn=0 forces CPU only \nDATA_ROOT=./datasets/horse2zebra name=horse2zebra_model gpu=0 cudnn=0 th train.lua \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8576885868213461
      ],
      "excerpt": "bash ./pretrained_models/download_model.sh model_name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9166425167626439,
        0.9100151517165272
      ],
      "excerpt": "CPU models can be downloaded using: \nbash pretrained_models/download_model.sh &lt;name&gt;_cpu \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8917133644880719
      ],
      "excerpt": "bash ./datasets/download_dataset.sh dataset_name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.970409074371173
      ],
      "excerpt": "Install it with: luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8260262293211895
      ],
      "excerpt": "<img src=\"https://junyanz.github.io/CycleGAN/images/teaser_high_res.jpg\" width=\"1000px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8260262293211895,
        0.8260262293211895,
        0.8112254896007854,
        0.8260262293211895,
        0.8260262293211895
      ],
      "excerpt": "<img src=\"https://junyanz.github.io/CycleGAN/images/painting2photo.jpg\" width=\"1000px\"/> \n<img src=\"https://junyanz.github.io/CycleGAN/images/photo2painting.jpg\" width=\"1000px\"/> \n<img src=\"https://junyanz.github.io/CycleGAN/images/objects.jpg\" width=\"1000px\"/> \n<img src=\"https://junyanz.github.io/CycleGAN/images/season.jpg\" width=\"1000px\"/> \n<img src=\"https://junyanz.github.io/CycleGAN/images/photo_enhancement.jpg\" width=\"1000px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8981055980185798
      ],
      "excerpt": "DATA_ROOT=./datasets/ae_photos name=style_cezanne_pretrained model=one_direction_test phase=test loadSize=256 fineSize=256 resize_or_crop=\"scale_width\" th test.lua \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8212503127933796
      ],
      "excerpt": "Download a dataset (e.g. zebra and horse images from ImageNet): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737873112808705
      ],
      "excerpt": "Train a model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9189848219427753
      ],
      "excerpt": "DATA_ROOT=./datasets/horse2zebra name=horse2zebra_model th train.lua \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8788831929973947
      ],
      "excerpt": "DATA_ROOT=./datasets/horse2zebra name=horse2zebra_model gpu=0 cudnn=0 th train.lua \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222206534514169,
        0.8077225519429095
      ],
      "excerpt": "DATA_ROOT=./datasets/horse2zebra name=horse2zebra_model phase=test th test.lua \nThe test results will be saved to an HTML file here: ./results/horse2zebra_model/latest_test/index.html. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8576591754281959
      ],
      "excerpt": "To train a model, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9215401035499162
      ],
      "excerpt": "DATA_ROOT=/path/to/data/ name=expt_name th train.lua \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8467708910668323
      ],
      "excerpt": "To test the model, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254978200225202
      ],
      "excerpt": "DATA_ROOT=/path/to/data/ name=expt_name phase=test th test.lua \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8191760434432828
      ],
      "excerpt": "A webpage with result images will be saved to ./results/expt_name (can be changed by passing results_dir=your_dir in test.lua). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8063780123977431
      ],
      "excerpt": "<img align=\"left\" style=\"padding:10px\" src=\"https://junyanz.github.io/CycleGAN/images/failure_putin.jpg\" width=320> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/junyanz/CycleGAN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Lua",
      "Python",
      "Shell",
      "TeX"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/junyanz/CycleGAN/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2017, Jun-Yan Zhu and Taesung Park\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\n\\n--------------------------- LICENSE FOR pix2pix --------------------------------\\nBSD License\\n\\nFor pix2pix software\\nCopyright (c) 2016, Phillip Isola and Jun-Yan Zhu\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n----------------------------- LICENSE FOR DCGAN --------------------------------\\nBSD License\\n\\nFor dcgan.torch software\\n\\nCopyright (c) 2015, Facebook, Inc. All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\\n\\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n\\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n\\nNeither the name Facebook nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "CycleGAN",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CycleGAN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "junyanz",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/junyanz/CycleGAN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Linux or OSX\n- NVIDIA GPU + CUDA CuDNN (CPU mode and CUDA without CuDNN may work with minimal modification, but untested)\n- For MAC users, you need the Linux/GNU commands `gfind` and `gwc`, which can be installed with `brew install findutils coreutils`.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10942,
      "date": "Fri, 24 Dec 2021 12:39:17 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "gan",
      "generative-adversarial-network",
      "deep-learning",
      "image-generation",
      "image-manipulation",
      "cyclegan",
      "pix2pix",
      "gans",
      "computer-vision",
      "computer-graphics",
      "torch"
    ],
    "technique": "GitHub API"
  }
}