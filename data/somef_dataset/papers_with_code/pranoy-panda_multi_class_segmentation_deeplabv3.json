{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.05587",
      "https://arxiv.org/abs/1706.05587"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "eTRIMS Image Database\n```\n@techreport{ korc-forstner-tr09-etrims,\n             author = \"Kor{\\v c}, F. and F{\\\" o}rstner, W.\",\n             title = \"{eTRIMS} {I}mage {D}atabase for Interpreting Images of Man-Made Scenes\",\n             number = \"TR-IGG-P-2009-01\",\n             month = \"April\",\n             year = \"2009\",\n             institute = \"Dept. of Photogrammetry, University of Bonn\",\n             url = \"http://www.ipb.uni-bonn.de/projects/etrims_db/\" }\n```\nDeepLabv3 semantic segmentation architecture\n```\n@article{chen2017rethinking,\n  title={Rethinking atrous convolution for semantic image segmentation},\n  author={Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},\n  journal={arXiv preprint arXiv:1706.05587},\n  year={2017}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@techreport{ korc-forstner-tr09-etrims,\n             author = \"Kor{\\v c}, F. and F{\\\" o}rstner, W.\",\n             title = \"{eTRIMS} {I}mage {D}atabase for Interpreting Images of Man-Made Scenes\",\n             number = \"TR-IGG-P-2009-01\",\n             month = \"April\",\n             year = \"2009\",\n             institute = \"Dept. of Photogrammetry, University of Bonn\",\n             url = \"http://www.ipb.uni-bonn.de/projects/etrims_db/\" }\nDeepLabv3 semantic segmentation architecture\n@article{chen2017rethinking,\n  title={Rethinking atrous convolution for semantic image segmentation},\n  author={Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},\n  journal={arXiv preprint arXiv:1706.05587},\n  year={2017}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pranoy-panda/multi_class_segmentation_deeplabv3",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-14T14:44:04Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-31T07:40:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9858781991812173,
        0.9809867304188669,
        0.9350495410432167,
        0.8682819676653215,
        0.850955048479527
      ],
      "excerpt": "This repository contains code for Fine Tuning DeepLabV3 ResNet101 in PyTorch. The model is from the torchvision module. \nI've fine tuned the model for the eTRIMS Image Database data-set. I have used the eTRIMS Database beta 2, with 8 classes of objects(Building, Car, Door, Pavement, Road, Sky, Vegetation, Window) \nThe model was fine tuned for 40 epochs and achieves an testing loss value of 0.859484. \nThe code was written using pyTorch and trained on NVIDIA Tesla k80 GPU with 12GB memory via the Google colab \nThe segmentation output of the model on a sample image are shown below. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pranoy-panda/multi_class_segmentation_deeplabv3/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 19:10:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pranoy-panda/multi_class_segmentation_deeplabv3/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pranoy-panda/multi_class_segmentation_deeplabv3",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pranoy-panda/multi_class_segmentation_deeplabv3/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Multi-class semantic segmentation via Transfer Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "multi_class_segmentation_deeplabv3",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pranoy-panda",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pranoy-panda/multi_class_segmentation_deeplabv3/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run the code the dataset use the following command.\n\n```\npython main.py \"data_directory_path\" \"experiment_folder_where weights and log file need to be saved\"\n```\nIt has following two optional arguments:\n```\n--epochs : Specify the number of epochs. Default is 25.\n--batchsize: Specify the batch size. Default is 4.\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Mon, 27 Dec 2021 19:10:29 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "semantic-segmentation",
      "deep-learning",
      "deeplabv3",
      "transfer-learning"
    ],
    "technique": "GitHub API"
  }
}