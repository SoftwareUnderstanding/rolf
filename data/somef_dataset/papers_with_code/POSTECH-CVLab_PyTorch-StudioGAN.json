{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.06434",
      "https://arxiv.org/abs/1611.04076",
      "https://arxiv.org/abs/1705.02894",
      "https://arxiv.org/abs/1701.04862",
      "https://arxiv.org/abs/1704.00028",
      "https://arxiv.org/abs/1705.07215",
      "https://arxiv.org/abs/1802.05637",
      "https://arxiv.org/abs/1802.05957",
      "https://arxiv.org/abs/1805.08318",
      "https://arxiv.org/abs/1907.02690",
      "https://arxiv.org/abs/1902.05687",
      "https://arxiv.org/abs/1809.11096",
      "https://arxiv.org/abs/1809.11096",
      "https://arxiv.org/abs/1912.00953",
      "https://arxiv.org/abs/1912.04958",
      "https://arxiv.org/abs/1910.12027",
      "https://arxiv.org/abs/2006.10738",
      "https://arxiv.org/abs/2006.06676",
      "https://arxiv.org/abs/2006.12681",
      "https://arxiv.org/abs/1912.04216",
      "https://arxiv.org/abs/2002.04724",
      "https://arxiv.org/abs/2107.10060",
      "https://arxiv.org/abs/2111.01118",
      "https://arxiv.org/abs/1610.07629",
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/1610.09585",
      "https://arxiv.org/abs/1802.05637",
      "https://arxiv.org/abs/1907.02690",
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/2006.12681",
      "https://arxiv.org/abs/1912.04216",
      "https://arxiv.org/abs/2107.10060",
      "https://arxiv.org/abs/2111.01118",
      "https://arxiv.org/abs/1910.12027",
      "https://arxiv.org/abs/2002.05709",
      "https://arxiv.org/abs/2006.10738",
      "https://arxiv.org/abs/2006.07733",
      "https://arxiv.org/abs/2006.06676",
      "https://arxiv.org/abs/2002.10964",
      "https://arxiv.org/abs/2002.06224",
      "https://arxiv.org/abs/2007.06600",
      "https://arxiv.org/abs/1606.03498",
      "https://arxiv.org/abs/1706.08500",
      "https://arxiv.org/abs/1904.06991",
      "https://arxiv.org/abs/1905.10887",
      "https://arxiv.org/abs/2002.09797",
      "https://arxiv.org/abs/1710.03740",
      "https://arxiv.org/abs/1809.11096",
      "https://arxiv.org/abs/2003.06060",
      "https://arxiv.org/abs/2002.10964",
      "https://arxiv.org/abs/2006.12681",
      "https://arxiv.org/abs/1806.00035",
      "https://arxiv.org/abs/1910.12027",
      "https://arxiv.org/abs/2002.04724",
      "https://arxiv.org/abs/2006.10738",
      "https://arxiv.org/abs/2006.06676",
      "https://arxiv.org/abs/1912.00953",
      "https://arxiv.org/abs/1809.11096",
      "https://arxiv.org/abs/1910.12027",
      "https://arxiv.org/abs/2002.04724",
      "https://arxiv.org/abs/2006.06676",
      "https://arxiv.org/abs/2006.06676",
      "https://arxiv.org/abs/1809.11096",
      "https://arxiv.org/abs/2006.12681",
      "https://arxiv.org/abs/2006.12681",
      "https://arxiv.org/abs/1610.09585",
      "https://arxiv.org/abs/1809.11096"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "StudioGAN is established for the following research projects. Please cite our work if you use StudioGAN.\n```bib\n@inproceedings{kang2020ContraGAN,\n  title   = {{ContraGAN: Contrastive Learning for Conditional Image Generation}},\n  author  = {Minguk Kang and Jaesik Park},\n  journal = {Conference on Neural Information Processing Systems (NeurIPS)},\n  year    = {2020}\n}\n```\n\n```bib\n@inproceedings{kang2021ReACGAN,\n  title   = {{Rebooting ACGAN: Auxiliary Classifier GANs with Stable Training}},\n  author  = {Minguk Kang, Woohyeon Shim, Minsu Cho, and Jaesik Park},\n  journal = {Conference on Neural Information Processing Systems (NeurIPS)},\n  year    = {2021}\n}\n```\n---------------------------------------\n\n<a name=\"footnote_1\">[1]</a> Experiments on Tiny ImageNet are conducted using the ResNet architecture instead of CNN.\n\n<a name=\"footnote_2\">[2]</a> Our re-implementation of [ACGAN (ICML'17)](https://arxiv.org/abs/1610.09585) with slight modifications, which bring strong performance enhancement for the experiment using CIFAR10.\n\n<a name=\"footnote_3\">[3]</a> Our re-implementation of [BigGAN/BigGAN-Deep (ICLR'18)](https://arxiv.org/abs/1809.11096) with slight modifications, which bring strong performance enhancement for the experiment using CIFAR10.\n\n<a name=\"footnote_4\">[4]</a> IS is computed using Tensorflow official code.\n\n<a name=\"footnote_5\">[5]</a> The difference in FID values between the original StyleGAN2 and StudioGAN implementation is caused by the presence of random flip augmentation.\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{kang2021ReACGAN,\n  title   = {{Rebooting ACGAN: Auxiliary Classifier GANs with Stable Training}},\n  author  = {Minguk Kang, Woohyeon Shim, Minsu Cho, and Jaesik Park},\n  journal = {Conference on Neural Information Processing Systems (NeurIPS)},\n  year    = {2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{kang2020ContraGAN,\n  title   = {{ContraGAN: Contrastive Learning for Conditional Image Generation}},\n  author  = {Minguk Kang and Jaesik Park},\n  journal = {Conference on Neural Information Processing Systems (NeurIPS)},\n  year    = {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8550101043698384,
        0.8944178096468923
      ],
      "excerpt": "| LOGAN | arXiv'19 |  Big ResNet | cBN | PD | Hinge | True | \n| StyleGAN2 | CVPR' 20 | StyleGAN2 | cAdaIN | SPD | Logistic | True | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| StyleGAN2 + ADA | Neurips'20 | StyleGAN2 | cAdaIN | SPD | Logistic | True | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "| ADCGAN | arXiv'21 | Big ResNet | cBN | ADC | Hinge | True | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "MH : Multi-Hinge loss. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.8356013927728488,
        0.8356013927728488
      ],
      "excerpt": "| DiffAugment | Neurips'2020 | - | \n| BYOL | Neurips'2020 | - | \n| ADA | Neurips'2020 | Logistic | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066,
        0.8944178096468923
      ],
      "excerpt": "| Top-K Training | Neurips'2020 | - | \n| SeFa | CVPR'2021 | BigGAN | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.8356013927728488
      ],
      "excerpt": "| Improved Precision & Recall | Neurips'19 | Inception_V3 | \n| Classifier Accuracy Score (CAS) | Neurips'19 | Inception_V3 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8249003161989972
      ],
      "excerpt": "DistributedDataParallel (Please refer to Here) (-DDP) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "| WGAN-DRA | StudioGAN |  6.432 | 41.586 | 0.922 | 0.863 |  Cfg | Log | Link | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "| BigGAN + CR | Paper | - | 11.5 | - | - | - | - | - | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372151656983196,
        0.9372151656983196,
        0.8444342525991423
      ],
      "excerpt": "| BigGAN-Mod + CR | StudioGAN |  10.380 | 7.178 | 0.994 | 0.993 | Cfg | Log | Link | \n| BigGAN-Mod + ICR | StudioGAN | 10.153 | 7.430 | 0.994 | 0.993 | Cfg | Log | Link | \n| BigGAN-Mod + DiffAugment | StudioGAN |  9.775 | 7.157 | 0.996 | 0.993 | Cfg | Log | Link | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "| ContraGAN | StudioGAN |  9.729 | 8.065 | 0.993 | 0.992 | Cfg | Log | Link | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372151656983196
      ],
      "excerpt": "| ContraGAN + ICR | StudioGAN |  10.117 | 7.547 | 0.996 | 0.993 | Cfg | Log | Link | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372151656983196
      ],
      "excerpt": "| ReACGAN + DiffAugment | StudioGAN | 10.181 | 6.717| 0.996 | 0.994 | Cfg | Log | Link | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372151656983196,
        0.9372151656983196,
        0.9372151656983196,
        0.9742177718389462,
        0.9372151656983196
      ],
      "excerpt": "| StyleGAN2 + ADA<sup>5</sup> | Paper | 10.14<sup>4</sup> | 2.42 | - | - | - | - | - | \n| StyleGAN2 | StudioGAN | 10.149 | 3.889 | 0.979 | 0.893 | Cfg | Log | Link | \n| StyleGAN2 + D2D-CE | StudioGAN | 10.320 | 3.385 | 0.974 | 0.899 | Cfg | Log | Link | \n| StyleGAN2 + ADA | StudioGAN | 10.477 | 2.316 | 1.049 | 0.929 | Cfg | Log | Link | \n| StyleGAN2 + ADA + D2D-CE | StudioGAN | 10.548 | 2.325 | 1.052 | 0.929 | Cfg | Log | Link | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "| BigGAN-Mod | StudioGAN | 11.998 | 31.920 | 0.956 | 0.879 | Cfg | Log| Link | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "| SNGAN | StudioGAN | 32.247 | 26.792 | 0.938 | 0.913 | Cfg | Log | Link | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9537267708315463,
        0.8906174419333412
      ],
      "excerpt": "| StyleGAN2 + ADA | StudioGAN | 12.907 | 4.992 | 1.282 | 0.835 | Cfg | Log | Link | \n| StyleGAN2 + ADA + D2D-CE | StudioGAN | 12.792 | 4.950 | - | - | Cfg | Log | Link | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479,
        0.9733164443857801,
        0.9105368110547479
      ],
      "excerpt": "StyleGAN2: https://github.com/NVlabs/stylegan2 \nDiffAugment: https://github.com/mit-han-lab/data-efficient-gans \nAdaptive Discriminator Augmentation: https://github.com/NVlabs/stylegan2 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/POSTECH-CVLab/PyTorch-StudioGAN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-19T04:09:13Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-26T13:36:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8909841706728786
      ],
      "excerpt": "Our new paper \"Rebooting ACGAN: Auxiliary Classifier GANs with Stable Training (ReACGAN)\" is made public on Neurips 2021 Openreview. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8166877985253634
      ],
      "excerpt": "Add five types of differentiable augmentation: CR, DiffAugment, ADA, SimCLR, BYOL. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9980472987668473
      ],
      "excerpt": "Verify the reproducibility of StyleGAN2 and BigGAN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8670513641339441,
        0.8393454288498029,
        0.8221562463113681,
        0.9419813441835941,
        0.8240327573279708
      ],
      "excerpt": "The only repository to train/evaluate BigGAN and StyleGAN2 baselines in a unified training pipeline. \nComprehensive benchmark of GANs using CIFAR10, Tiny ImageNet, CUB200, and ImageNet datasets. \nProvide pre-trained models that are fully compatible with up-to-date PyTorch environment. \nEasy to handle other personal datasets (i.e. AFHQ, anime, and much more!). \nBetter performance and lower memory consumption than original implementations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8253570072676873
      ],
      "excerpt": "| LSGAN | ICCV'17 | CNN/ResNet<sup>1</sup> | N/A | N/A | Least Sqaure | False | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8335770966143471,
        0.8205584423227983
      ],
      "excerpt": "GC/DC indicates the way how we inject label information to the Generator or Discriminator. \nEMA: Exponential Moving Average update to the generator. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "SPD : Modified PD for StyleGAN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9135286234739305
      ],
      "excerpt": "D2D-CE : Data-to-Data Cross-Entropy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9380565105103916
      ],
      "excerpt": "  #:#:#: NODE_0, 4_GPUs, All ports are open to NODE_1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9380565105103916
      ],
      "excerpt": "  #:#:#: NODE_1, 4_GPUs, All ports are open to NODE_0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333166446462158
      ],
      "excerpt": "StudioGAN supports Image visualization, K-nearest neighbor analysis, Linear interpolation, Frequency analysis, TSNE analysis, and Semantic factorization. All results will be saved in SAVE_DIR/figures/RUN_NAME/*.png. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8122345169575081
      ],
      "excerpt": "Linear Interpolation (applicable only to conditional Big ResNet models) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8884074833474234
      ],
      "excerpt": "Frequency Analysis \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8582859489243233,
        0.8766896416732443
      ],
      "excerpt": "StudioGAN supports Inception Score, Frechet Inception Distance, Improved Precision and Recall, Density and Coverage, Intra-Class FID, Classifier Accuracy Score, SwAV backbone FID. Users can get Intra-Class FID, Classifier Accuracy Score, SwAV backbone FID scores using -iFID, -GAN_train, -GAN_test, and --eval_backbone \"SwAV\" options, respectively. \nInception Score (IS) is a metric to measure how much GAN generates high-fidelity and diverse images. Calculating IS requires the pre-trained Inception-V3 network, and recent approaches utilize OpenAI's TensorFlow implementation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9870575643338009
      ],
      "excerpt": "After that, execute TensorFlow official IS implementation. Note that we do not split a dataset into ten folds to calculate IS ten times. We use the entire dataset to compute IS only once, which is the evaluation strategy used in the CompareGAN repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9202996746311991,
        0.9929606682122566,
        0.9810906810207597,
        0.9539208280805779,
        0.9428835803702788,
        0.9296909753469619
      ],
      "excerpt": "Note that StudioGAN logs Pytorch-based IS during the training. \nFID is a widely used metric to evaluate the performance of a GAN model. Calculating FID requires the pre-trained Inception-V3 network, and modern approaches use Tensorflow-based FID. StudioGAN utilizes the PyTorch-based FID to test GAN models in the same PyTorch environment. We show that the PyTorch based FID implementation provides almost the same results with the TensorFlow implementation (See Appendix F of our paper). \nImproved precision and recall are developed to make up for the shortcomings of the precision and recall. Like IS, FID, calculating improved precision and recall requires the pre-trained Inception-V3 model. StudioGAN uses the PyTorch implementation provided by developers of density and coverage scores. \nDensity and coverage metrics can estimate the fidelity and diversity of generated images using the pre-trained Inception-V3 model. The metrics are known to be robust to outliers, and they can detect identical real and fake distributions. StudioGAN uses the authors' official PyTorch implementation, and StudioGAN follows the author's suggestion for hyperparameter selection. \nPrecision measures how accurately the generator can learn the target distribution. Recall measures how completely the generator covers the target distribution. Like IS and FID, calculating Precision and Recall requires the pre-trained Inception-V3 model. StudioGAN uses the same hyperparameter settings with the original Precision and Recall implementation, and StudioGAN calculates the F-beta score suggested by Sajjadi et al. \nWe report the best IS, FID, and F_beta values of various GANs. B. S. means batch size for training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.823421233923837
      ],
      "excerpt": "CR, ICR, DiffAugment, ADA, and LO refer to regularization or optimization techiniques: CR (Consistency Regularization), ICR (Improved Consistency Regularization), DiffAugment (Differentiable Augmentation), ADA (Adaptive Discriminator Augmentation), and LO (Latent Optimization), respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8609975619259712
      ],
      "excerpt": "IS, FID, and F_beta values are computed using 10K validation and 10K generated Images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8609975619259712
      ],
      "excerpt": "IS, FID, and F_beta values are computed using 50K validation and 50K generated Images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "StudioGAN is a Pytorch library providing implementations of representative Generative Adversarial Networks (GANs) for conditional/unconditional image generation.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 202,
      "date": "Sun, 26 Dec 2021 16:11:21 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/POSTECH-CVLab/PyTorch-StudioGAN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "POSTECH-CVLab/PyTorch-StudioGAN",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8227254576670002
      ],
      "excerpt": "Extensive GAN implementations using PyTorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8002458752553352
      ],
      "excerpt": "Provide pre-trained models that are fully compatible with up-to-date PyTorch environment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "  bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "  bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "  bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "  bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "  ```bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "  bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "  bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "  bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.821347293830159
      ],
      "excerpt": "  <img width=\"95%\" src=\"https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/docs/figures/AFHQ_.png\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.821347293830159
      ],
      "excerpt": "  <img width=\"95%\" src=\"https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/docs/figures/Anime_.png\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9609813570409922
      ],
      "excerpt": "Keep in mind that you need to have TensorFlow 1.3 or earlier version installed! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9301018676424139,
        0.9667326416418838,
        0.8137353387448052,
        0.8298615966031382
      ],
      "excerpt": "Exponential Moving Average: https://github.com/ajbrock/BigGAN-PyTorch \nSynchronized BatchNorm: https://github.com/vacancy/Synchronized-BatchNorm-PyTorch \nSelf-Attention module: https://github.com/voletiv/self-attention-GAN-pytorch \nImplementation Details: https://github.com/ajbrock/BigGAN-PyTorch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918974083095406
      ],
      "excerpt": "StyleGAN2: https://github.com/NVlabs/stylegan2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8179861601561125,
        0.879325272055491,
        0.9725002728977397,
        0.8384146609079162
      ],
      "excerpt": "Tensorflow IS: https://github.com/openai/improved-gan \nTensorflow FID: https://github.com/bioinf-jku/TTUR \nPytorch FID: https://github.com/mseitzer/pytorch-fid \nTensorflow Precision and Recall: https://github.com/msmsajjadi/precision-recall-distributions \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8138782399503394
      ],
      "excerpt": "| BigGAN | ICLR'19 |  Big ResNet | cBN | PD | Hinge | True | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8329331596940573
      ],
      "excerpt": "| BigGAN-Mod<sup>3</sup> | - |  Big ResNet | cBN | PD | Hinge | True | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471078796617689
      ],
      "excerpt": "| CRGAN | ICLR'20 |  Big ResNet | cBN | PD | Hinge | True | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8329331596940573
      ],
      "excerpt": "| MHGAN | WACV'21 |  Big ResNet | cBN | MH | MH | True | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9146768359513707
      ],
      "excerpt": "CIFAR10/CIFAR100: StudioGAN will automatically download the dataset once you execute main.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "    \u251c\u2500\u2500 train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9376273115306133
      ],
      "excerpt": "  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -t -hdf5 -l -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9557886031341571
      ],
      "excerpt": "  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -t -mpc -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9557886031341571
      ],
      "excerpt": "  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -t -sync_bn -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9230038792649249
      ],
      "excerpt": "  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -std_stat -std_max STD_MAX -std_step STD_STEP -cfg CONFIG_PATH -ckpt CKPT -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9230038792649249
      ],
      "excerpt": "  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -batch_stat -cfg CONFIG_PATH -ckpt CKPT -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9230038792649249
      ],
      "excerpt": "  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py --truncation_factor TRUNCATION_FACTOR -cfg CONFIG_PATH -ckpt CKPT -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9230038792649249
      ],
      "excerpt": "  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -lgv -lgv_rate LGV_RATE -lgv_std LGV_STD -lgv_decay LGV_DECAY -lgv_decay_steps LGV_DECAY_STEPS -lgv_steps LGV_STEPS -cfg CONFIG_PATH -ckpt CKPT -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9491125459498525
      ],
      "excerpt": "  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -t --freezeD FREEZED -ckpt SOURCE_CKPT -cfg TARGET_CONFIG_PATH -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9124291861258201
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -v -cfg CONFIG_PATH -ckpt CKPT -save SAVE_DIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9230038792649249
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -knn -cfg CONFIG_PATH -ckpt CKPT -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9124291861258201
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -itp -cfg CONFIG_PATH -ckpt CKPT -save SAVE_DIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9230038792649249
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -fa -cfg CONFIG_PATH -ckpt CKPT -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9230038792649249
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -tsne -cfg CONFIG_PATH -ckpt CKPT -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9124291861258201
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -sefa -sefa_axis SEFA_AXIS -sefa_max SEFA_MAX -cfg CONFIG_PATH -ckpt CKPT -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9428658126377512
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -s -c CONFIG_PATH --checkpoint_folder CHECKPOINT_FOLDER --log_output_path LOG_OUTPUT_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9049378897809343
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,...,N python3 src/inception_tf13.py --run_name RUN_NAME --type \"fake\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9275081128138123
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python3 src/main.py -t -hdf5 -l -batch_stat -metrics is fid prdc -ref \"test\" -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8407899818332929
      ],
      "excerpt": "| ACGAN-Mod | StudioGAN | 6.629 | 45.571 | 0.857 | 0.847 | Cfg | Log | Link | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9270769169142963
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,1 python3 src/main.py -t -hdf5 -l -mpc -metrics is fid prdc -ref \"train\" -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944591071459393
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -t -hdf5 -l -batch_stat -metrics is fid prdc -ref \"valid\" -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8104679754142359,
        0.8104679754142359
      ],
      "excerpt": "| ContraGAN + ICR | StudioGAN | 15.830 | 21.940 | 0.980 | 0.944 | Cfg | Log | Link | \n| ContraGAN + DiffAugment | StudioGAN | 17.303 | 15.755 | 0.984 | 0.962 | Cfg | Log | Link | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944591071459393
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -t -hdf5 -l -sync_bn -metrics is fid prdc --eval_type \"valid\" -cfg CONFIG_PATH -std_stat -std_max STD_MAX -std_step STD_STEP -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8084885163832897
      ],
      "excerpt": "| SAGAN | StudioGAN | 29.848 | 34.726 | 0.849 | 0.914 | Cfg | Log | Link | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801509951594895
      ],
      "excerpt": "| ContraGAN | StudioGAN | 25.249 | 25.161 | 0.947 | 0.855 | Cfg | Log | Link | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9270769169142963
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,1,2,3 python3 src/main.py -t -hdf5 -l -mpc -metrics is fid prdc -ref \"train\" -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/POSTECH-CVLab/PyTorch-StudioGAN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/POSTECH-CVLab/PyTorch-StudioGAN/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Jiayuan MAO\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "News",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PyTorch-StudioGAN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "POSTECH-CVLab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "MINGUKKANG",
        "body": "- Add SOTA GANs: LGAN, TACGAN, StyleGAN2, MDGAN, MHGAN, ADCGAN, [ReACGAN (our new paper)](https://arxiv.org/abs/2111.01118).\r\n- Add five types of differentiable augmentation: CR, DiffAugment, ADA, SimCLR, BYOL.\r\n- Implement useful regularizations: Top-K training, Feature Matching, R1-Regularization, MaxGP\r\n- Add Improved Precision & Recall, Density & Coverage, iFID, and CAS for reliable evaluation.\r\n- Support Inception_V3 and SwAV backbones for GAN evaluation.\r\n- Verify the reproducibility of StyleGAN2 and BigGAN.\r\n- Fix bugs in FreezeD, DDP training, Mixed Precision training, and ADA.\r\n- Support Discriminator Driven Latent Sampling, Semantic Factorization for BigGAN evaluation.\r\n- Support Wandb logging instead of Tensorboard.",
        "dateCreated": "2021-11-05T16:01:34Z",
        "datePublished": "2021-11-05T16:08:22Z",
        "html_url": "https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/releases/tag/v.0.3.0",
        "name": "v.0.3.0",
        "tag_name": "v.0.3.0",
        "tarball_url": "https://api.github.com/repos/POSTECH-CVLab/PyTorch-StudioGAN/tarball/v.0.3.0",
        "url": "https://api.github.com/repos/POSTECH-CVLab/PyTorch-StudioGAN/releases/52797844",
        "zipball_url": "https://api.github.com/repos/POSTECH-CVLab/PyTorch-StudioGAN/zipball/v.0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "MINGUKKANG",
        "body": "## Second release of StudioGAN with following features\r\n\r\n- Fix minor bugs (slow convergence of training GAN + ADA models, tracking bn statistics during evaluation, etc.)\r\n- Add multi-node DistributedDataParallel (DDP) training.\r\n- Comprehensive benchmarks on CIFAR10, Tiny_ImageNet, and ImageNet datasets.\r\n- Provide pre-trained models and log files for the future research.\r\n- Add LARS optimizer and TSNE analysis. ",
        "dateCreated": "2021-02-23T14:14:04Z",
        "datePublished": "2021-02-23T14:33:14Z",
        "html_url": "https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/releases/tag/v0.2.0",
        "name": "v0.2.0",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/POSTECH-CVLab/PyTorch-StudioGAN/tarball/v0.2.0",
        "url": "https://api.github.com/repos/POSTECH-CVLab/PyTorch-StudioGAN/releases/38448325",
        "zipball_url": "https://api.github.com/repos/POSTECH-CVLab/PyTorch-StudioGAN/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "MINGUKKANG",
        "body": "## First StudioGAN release with following features\r\n\r\n- Extensive GAN implementations for Pytorch: From DCGAN to ADAGAN\r\n- Comprehensive benchmark of GANs using CIFAR10 dataset\r\n- Better performance and lower memory consumption than original implementations\r\n- Providing pre-trained models that are fully compatible with up-to-date PyTorch environment\r\n- Support Multi-GPU(both DP and DDP), Mixed precision, Synchronized Batch Normalization, and Tensorboard Visualization",
        "dateCreated": "2020-12-07T02:43:53Z",
        "datePublished": "2020-12-07T03:02:04Z",
        "html_url": "https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/releases/tag/v0.1.0",
        "name": "v0.1.0",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/POSTECH-CVLab/PyTorch-StudioGAN/tarball/v0.1.0",
        "url": "https://api.github.com/repos/POSTECH-CVLab/PyTorch-StudioGAN/releases/34907845",
        "zipball_url": "https://api.github.com/repos/POSTECH-CVLab/PyTorch-StudioGAN/zipball/v0.1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "First, install PyTorch meeting your environment (at least 1.7, recommmended 1.10):\n```bash\npip3 install torch==1.10.0+cu111 torchvision==0.11.1+cu111 torchaudio==0.10.0+cu111 -f https://download.pytorch.org/whl/cu111/torch_stable.html\n```\n\nThen, use the following command to install the rest of the libraries:\n```bash\npip3 install tqdm ninja h5py kornia matplotlib pandas sklearn scipy seaborn wandb PyYaml click requests pyspng imageio-ffmpeg prdc\n```\n\nWith docker, you can use:\n```bash\ndocker pull mgkang/studio_gan:latest\n```\n\nThis is my command to make a container named \"StudioGAN\".\n\n```bash\ndocker run -it --gpus all --shm-size 128g --name StudioGAN -v /home/USER:/root/code --workdir /root/code mgkang/studio_gan:latest /bin/bash\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2332,
      "date": "Sun, 26 Dec 2021 16:11:21 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "deep-learning",
      "generative-adversarial-network",
      "biggan",
      "stylegan2",
      "machine-learning"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Before starting, users should login wandb using their personal API key.\n\n```bash\nwandb login PERSONAL_API_KEY\n```\nFrom release 0.3.0, you can now define which evaluation metrics to use through ``-metrics`` option. Not specifying option defaults to calculating FID only. \ni.e. ``-metrics is fid`` calculates only IS and FID and ``-metrics none`` skips evaluation.\n\n\n* Train (``-t``) and evaluate IS, FID, Prc, Rec, Dns, Cvg (``-metrics is fid prdc``) of the model defined in ``CONFIG_PATH`` using GPU ``0``.\n```bash\nCUDA_VISIBLE_DEVICES=0 python3 src/main.py -t -metrics is fid prdc -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH\n```\n\n* Train (``-t``) and evaluate FID of the model defined in ``CONFIG_PATH`` through ``DataParallel`` using GPUs ``(0, 1, 2, 3)``. Evaluation of FID does not require (``-metrics``) argument!\n```bash\nCUDA_VISIBLE_DEVICES=0,1,2,3 python3 src/main.py -t -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH\n```\n\n* Train (``-t``) and skip evaluation (``-metrics none``) of the model defined in ``CONFIG_PATH`` through ``DistributedDataParallel`` using GPUs ``(0, 1, 2, 3)``, ``Synchronized batch norm``, and ``Mixed precision``.\n```bash\nexport MASTER_ADDR=\"localhost\"\nexport MASTER_PORT=2222\nCUDA_VISIBLE_DEVICES=0,1,2,3 python3 src/main.py -t -metrics none -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH -DDP -sync_bn -mpc \n```\n\nTry ``python3 src/main.py`` to see available options.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}