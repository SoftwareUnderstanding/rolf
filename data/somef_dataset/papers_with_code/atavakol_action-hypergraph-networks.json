{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1812.06110\n[dqn]: https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n[prioritized_replay]: https://arxiv.org/abs/1511.05952\n[rainbow]: https://arxiv.org/abs/1710.02298\n[branching]: https://arxiv.org/abs/1711.08946\n[hypergraph]: https://openreview.net/pdf?id=Xv_s64FiXTv\n[time_limits]: http://proceedings.mlr.press/v80/pardo18a.html\n[ale]: https://jair.org/index.php/jair/article/view/10819\n[machado]: https://jair.org/index.php/jair/article/view/11182\n[this_paper]: https://openreview.net/forum?id=Xv_s64FiXTv\n[gym]: https://github.com/openai/gym\n[pybullet]: https://github.com/bulletphysics/bullet3\n[dmc]: https://github.com/deepmind/dm_control",
      "https://arxiv.org/abs/1511.05952\n[rainbow]: https://arxiv.org/abs/1710.02298\n[branching]: https://arxiv.org/abs/1711.08946\n[hypergraph]: https://openreview.net/pdf?id=Xv_s64FiXTv\n[time_limits]: http://proceedings.mlr.press/v80/pardo18a.html\n[ale]: https://jair.org/index.php/jair/article/view/10819\n[machado]: https://jair.org/index.php/jair/article/view/11182\n[this_paper]: https://openreview.net/forum?id=Xv_s64FiXTv\n[gym]: https://github.com/openai/gym\n[pybullet]: https://github.com/bulletphysics/bullet3\n[dmc]: https://github.com/deepmind/dm_control",
      "https://arxiv.org/abs/1710.02298\n[branching]: https://arxiv.org/abs/1711.08946\n[hypergraph]: https://openreview.net/pdf?id=Xv_s64FiXTv\n[time_limits]: http://proceedings.mlr.press/v80/pardo18a.html\n[ale]: https://jair.org/index.php/jair/article/view/10819\n[machado]: https://jair.org/index.php/jair/article/view/11182\n[this_paper]: https://openreview.net/forum?id=Xv_s64FiXTv\n[gym]: https://github.com/openai/gym\n[pybullet]: https://github.com/bulletphysics/bullet3\n[dmc]: https://github.com/deepmind/dm_control",
      "https://arxiv.org/abs/1711.08946\n[hypergraph]: https://openreview.net/pdf?id=Xv_s64FiXTv\n[time_limits]: http://proceedings.mlr.press/v80/pardo18a.html\n[ale]: https://jair.org/index.php/jair/article/view/10819\n[machado]: https://jair.org/index.php/jair/article/view/11182\n[this_paper]: https://openreview.net/forum?id=Xv_s64FiXTv\n[gym]: https://github.com/openai/gym\n[pybullet]: https://github.com/bulletphysics/bullet3\n[dmc]: https://github.com/deepmind/dm_control",
      "https://arxiv.org/abs/1812.06110*.][dopamine]\n\n[Mnih et al. (2015). \nHuman-level control through deep reinforcement learning. \n*Nature* 518(7540), pp. 529-533.][dqn]\n\n[Hessel et al. (2018). \nRainbow: Combining improvements in deep reinforcement learning.\nIn *Proceedings of the 32nd AAAI Conference on Artificial Intelligence*, pp. 3215-3222.][rainbow]\n\n[Tavakoli et al. (2018). \nAction branching architectures for deep reinforcement learning. \nIn *Proceedings of the 32nd AAAI Conference on Artificial Intelligence*, pp. 4131-4138.][branching]\n\n[Pardo et al. (2018).\nTime limits in reinforcement learning.\nIn *Proceedings of the 35th International Conference on Machine Learning*, pp. 4045-4054.][time_limits]\n\n[dopamine]: https://arxiv.org/abs/1812.06110\n[dqn]: https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n[prioritized_replay]: https://arxiv.org/abs/1511.05952\n[rainbow]: https://arxiv.org/abs/1710.02298\n[branching]: https://arxiv.org/abs/1711.08946\n[hypergraph]: https://openreview.net/pdf?id=Xv_s64FiXTv\n[time_limits]: http://proceedings.mlr.press/v80/pardo18a.html\n[ale]: https://jair.org/index.php/jair/article/view/10819\n[machado]: https://jair.org/index.php/jair/article/view/11182\n[this_paper]: https://openreview.net/forum?id=Xv_s64FiXTv\n[gym]: https://github.com/openai/gym\n[pybullet]: https://github.com/bulletphysics/bullet3\n[dmc]: https://github.com/deepmind/dm_control"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[Castro et al. (2018).\nDopamine: A research framework for deep reinforcement learning.\n*arXiv:1812.06110*.][dopamine]\n\n[Mnih et al. (2015). \nHuman-level control through deep reinforcement learning. \n*Nature* 518(7540), pp. 529-533.][dqn]\n\n[Hessel et al. (2018). \nRainbow: Combining improvements in deep reinforcement learning.\nIn *Proceedings of the 32nd AAAI Conference on Artificial Intelligence*, pp. 3215-3222.][rainbow]\n\n[Tavakoli et al. (2018). \nAction branching architectures for deep reinforcement learning. \nIn *Proceedings of the 32nd AAAI Conference on Artificial Intelligence*, pp. 4131-4138.][branching]\n\n[Pardo et al. (2018).\nTime limits in reinforcement learning.\nIn *Proceedings of the 35th International Conference on Machine Learning*, pp. 4045-4054.][time_limits]\n\n[dopamine]: https://arxiv.org/abs/1812.06110\n[dqn]: https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n[prioritized_replay]: https://arxiv.org/abs/1511.05952\n[rainbow]: https://arxiv.org/abs/1710.02298\n[branching]: https://arxiv.org/abs/1711.08946\n[hypergraph]: https://openreview.net/pdf?id=Xv_s64FiXTv\n[time_limits]: http://proceedings.mlr.press/v80/pardo18a.html\n[ale]: https://jair.org/index.php/jair/article/view/10819\n[machado]: https://jair.org/index.php/jair/article/view/11182\n[this_paper]: https://openreview.net/forum?id=Xv_s64FiXTv\n[gym]: https://github.com/openai/gym\n[pybullet]: https://github.com/bulletphysics/bullet3\n[dmc]: https://github.com/deepmind/dm_control\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{tavakoli2021learning,\n   title={Learning to Represent Action Values as a Hypergraph on the Action Vertices},\n   author={Arash Tavakoli and Mehdi Fatemi and Petar Kormushev},\n   booktitle={International Conference on Learning Representations},\n   year={2021},\n   url={https://openreview.net/forum?id=Xv_s64FiXTv}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8453636397710822
      ],
      "excerpt": "Implementation of the hypergraph Q-networks (HGQN) agent in TensorFlow, built on Dopamine ([Castro et al., 2018][dopamine]). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998356279819474,
        0.9851174215621725
      ],
      "excerpt": "If you find this code useful, please cite the [paper][this_paper]: \nTavakoli, A., Fatemi, M., and Kormushev, P. (2021). Learning to represent action values as a hypergraph on the action vertices. In Proceedings of the 9th International Conference on Learning Representations (ICLR). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9358223674441953,
        0.8944178096468923
      ],
      "excerpt": "[Research paper][this_paper] \nVideo demonstrations \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/atavakol/action-hypergraph-networks",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-07T16:23:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-06T19:28:08Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9270526445295603
      ],
      "excerpt": "Implementation of the hypergraph Q-networks (HGQN) agent in TensorFlow, built on Dopamine ([Castro et al., 2018][dopamine]). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8905803682894057
      ],
      "excerpt": "Tavakoli, A., Fatemi, M., and Kormushev, P. (2021). Learning to represent action values as a hypergraph on the action vertices. In Proceedings of the 9th International Conference on Learning Representations (ICLR). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9577501507522039,
        0.9901114441056823
      ],
      "excerpt": "Action-value estimation is a critical component of many reinforcement learning methods whereby sample complexity relies heavily on how fast a good estimator for action value can be learned. By viewing this problem through the lens of representation learning, good representations of both state and action can facilitate action-value estimation. While advances in deep learning have seamlessly driven progress in learning state representations, given the specificity of the notion of agency to reinforcement learning, little attention has been paid to learning action representations. We conjecture that leveraging the combinatorial structure of multi-dimensional action spaces is a key ingredient for learning good representations of action. To test this, we set forth the action hypergraph networks framework - a class of functions for learning action representations in multi-dimensional discrete action spaces with a structural inductive bias. Using this framework we realise an agent class based on a combination with deep Q-networks (DQN by [Mnih et al., 2015][dqn]), which we dub hypergraph Q-networks (HGQN). \nUsing this agent class, we show the effectivess of our approach on a myriad of domains: Atari 2600 games and discretised physical control benchmarks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8788290927086805,
        0.965261838515305
      ],
      "excerpt": "This codebase is compatible with [Arcade Learning Environments][ale] (i.e. Atari 2600), [PyBullet][pybullet], [DeepMind Control Suite][dmc] (DMC), and [OpenAI Gym][gym] (MuJoCo or Box2D) environments. Each agent in the [paper][this_paper] (including all baselines except for DDPG) is configured by a gin file which can be used to reproduce the results in the paper, or to run the same agents in more environments. These configuration files can be modified directly or via the command-line to experiment with other environment or agent parameters. Next we outline how each agent can be run from the command-line by utilising the gin files. \nUse the following to run the agents in our main Atari 2600 experiment (set agent to dqn or hgqn). For HGQN, you can modify hyperedge_orders (default is [1,2,3]; [1], [2], [1,2], [1,3], and [2,3] are also valid choices) and mixer (default is sum; universal is supported too) in the agent's gin configuration file (located hyperdopamine/agents/dqn/configs/hgqn_atari.gin) to create arbitrary action hypergraph networks for the Atari 2600 suite. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9763526599249419
      ],
      "excerpt": "To see the full list of physical control environments that are readily supported (as well as their metadata), please refer to hyperdopamine/interfaces/environment_metadata.py.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.981510462490957,
        0.9825920551357413
      ],
      "excerpt": "As such, in our physical control environments which have relatively short time limits, we apply this technique to all agents. \nUse the following to run HGQN in the popular physical control environments. You can modify hyperedge_orders and mixer (default is sum; universal is supported too) in hyperdopamine/agents/rainbow/configs/hgqn.gin to create arbitrary action hypergraph networks in such tasks. For example, for environment = HopperBulletEnv (with a 3-dimensional action space), setting hyperedge_orders = [1,-1] instantiates a model based on a hypergraph with all the 1-hyperedges and the single highest-order hyperedge (including [-1] captures the highest-order). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9837831120597172
      ],
      "excerpt": "As stated in the [paper][this_paper], the class of rank-1 monotonic-mixer HGQN agents can gracefully scale to very high-dimensional discrete action spaces. To achieve the respective computational benefits, we release a separate implementation of this class of agents. You can modify mixing_network (default is sum_mixer; monotonic_linear_mixer and monotonic_nonlinear_mixer are supported too) to achieve different monotonic mixing strategies. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.962221652629094
      ],
      "excerpt": "We also release an implementation of the Branching-DQN (BDQN) agent ([Tavakoli et al., 2018][branching]), which can also scale to very high-dimensional discrete action spaces just as our rank-1 monotonic-mixer HGQN agent.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8677641535310878
      ],
      "excerpt": "To visualise the interaction process during the evaluation phase (i.e. when using Runner and not TrainRunner) simply set Runner.render = True in the configuration file or via the gin_bindings flag:   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9860646959655679
      ],
      "excerpt": "This repository also contains the performance logs of the agents in the Atari 2600 games and physical control environments of the [paper][this_paper]. In addition, we release unpublished results on the OpenAI-Gym's Humanoid environment and a full evaluation of our implementation of the Branching-DQN (BDQN) agent ([Tavakoli et al., 2018][branching]) in the physical control environments. Run the following command to plot the full physical-control benchmark:    \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "(ICLR 2021) Learning to Represent Action Values as a Hypergraph on the Action Vertices",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/atavakol/action-hypergraph-networks/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Thu, 30 Dec 2021 07:55:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/atavakol/action-hypergraph-networks/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "atavakol/action-hypergraph-networks",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To use the proposed agents and baselines, you need to install `python3` and make sure `pip` is up to date. If you want to experiment with [PyBullet][pybullet], [DeepMind Control Suite][dmc] (DMC), or [OpenAI Gym][gym] (MuJoCo or Box2D), you should install them separately. To do so, please check the following repositories:\n\n- **PyBullet** (free, open-source): [github.com/bulletphysics/bullet3][pybullet]\n- **DeepMind Control Suite**: [github.com/deepmind/dm_control][dmc]\n- **OpenAI-Gym MuJoCo**: [github.com/openai/gym][gym]\n- **OpenAI-Gym Box2D** (free, open-source): [github.com/openai/gym][gym]\n\nTo start using this repository, follow the steps below.\n\n**1. Clone the repository:** \n```sh\ngit clone https://github.com/atavakol/action-hypergraph-networks\ncd action-hypergraph-networks\n```\n\n**2. Create a virtual environment:**\n\nIf you do not wish to run this code in a `virtual environment`, you can skip this step.\n\nFirst install [Anaconda](https://docs.anaconda.com/anaconda/install/) which\nwe will use as the environment manager, e.g. via:\n\n```sh\nwget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh\nbash Anaconda3-2019.10-Linux-x86_64.sh\n```\n\nThen create an environment as below:\n\n```sh\nconda create --name hyperdopamine-env python=3.6\nconda activate hyperdopamine-env\n```\n\nThis will create a directory called `hyperdopamine-env` in which your virtual\nenvironment lives. The last command activates the environment.\n\n**3. Setup the environment and install dependencies:** \n\nFinally setup the virtual environment and install the main dependencies via:\n\n```sh\npip install -U pip\nsudo apt-get update && sudo apt-get install cmake zlib1g-dev\npip install -e .\nconda install tensorflow-gpu==1.15\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9625196775343342
      ],
      "excerpt": "  --gin_bindings=\"atari_lib.create_atari_environment.game_name='$environment'\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9212855821824482
      ],
      "excerpt": "environment=BipedalWalker  #: needs OpenAI-Gym Box2D installed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8178410279692755
      ],
      "excerpt": "  --base_dir=./logs/$environment/hgqn-r2-sum/$seed \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342,
        0.9625196775343342,
        0.9932422768023783
      ],
      "excerpt": "  --gin_bindings=\"RainbowAgent.observation_shape=%environment_metadata.${environment^^}_OBSERVATION_SHAPE\" \\ \n  --gin_bindings=\"create_discretised_environment.environment_name='$environment'\" \\ \n  --gin_bindings=\"create_discretised_environment.version=%environment_metadata.${environment^^}_ENV_VER\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342
      ],
      "excerpt": "  --gin_bindings=\"Runner.max_steps_per_episode=%environment_metadata.${environment^^}_TIMELIMIT\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9212855821824482
      ],
      "excerpt": "environment=AntBulletEnv  #: needs PyBullet installed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8178410279692755
      ],
      "excerpt": "  --base_dir=./logs/$environment/hgqn-r1-sum/$seed \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342,
        0.9625196775343342,
        0.9932422768023783
      ],
      "excerpt": "  --gin_bindings=\"HGQNr1Agent.observation_shape=%environment_metadata.${environment^^}_OBSERVATION_SHAPE\" \\ \n  --gin_bindings=\"create_discretised_environment.environment_name='$environment'\" \\ \n  --gin_bindings=\"create_discretised_environment.version=%environment_metadata.${environment^^}_ENV_VER\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342
      ],
      "excerpt": "  --gin_bindings=\"Runner.max_steps_per_episode=%environment_metadata.${environment^^}_TIMELIMIT\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342
      ],
      "excerpt": "  --gin_bindings=\"WrappedPrioritizedReplayBuffer.action_shape=%environment_metadata.${environment^^}_ACTION_SHAPE\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9212855821824482
      ],
      "excerpt": "environment=Humanoid  #: needs OpenAI-Gym MuJoCo installed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8178410279692755
      ],
      "excerpt": "  --base_dir=./logs/$environment/bdqn/$seed \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342,
        0.9625196775343342,
        0.9932422768023783
      ],
      "excerpt": "  --gin_bindings=\"BDQNAgent.observation_shape=%environment_metadata.${environment^^}_OBSERVATION_SHAPE\" \\ \n  --gin_bindings=\"create_discretised_environment.environment_name='$environment'\" \\ \n  --gin_bindings=\"create_discretised_environment.version=%environment_metadata.${environment^^}_ENV_VER\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342
      ],
      "excerpt": "  --gin_bindings=\"Runner.max_steps_per_episode=%environment_metadata.${environment^^}_TIMELIMIT\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342
      ],
      "excerpt": "  --gin_bindings=\"WrappedPrioritizedReplayBuffer.action_shape=%environment_metadata.${environment^^}_ACTION_SHAPE\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342,
        0.9625196775343342,
        0.9932422768023783
      ],
      "excerpt": "  --gin_bindings=\"RainbowAgent.observation_shape=%environment_metadata.${environment^^}_OBSERVATION_SHAPE\" \\ \n  --gin_bindings=\"create_discretised_environment.environment_name='$environment'\" \\ \n  --gin_bindings=\"create_discretised_environment.version=%environment_metadata.${environment^^}_ENV_VER\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342
      ],
      "excerpt": "  --gin_bindings=\"Runner.max_steps_per_episode=%environment_metadata.${environment^^}_TIMELIMIT\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8178410279692755
      ],
      "excerpt": "  --base_dir=./logs/$environment/hgqn-r1-sum/$seed \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342,
        0.9625196775343342,
        0.9932422768023783
      ],
      "excerpt": "  --gin_bindings=\"HGQNr1Agent.observation_shape=%environment_metadata.${environment^^}_OBSERVATION_SHAPE\" \\ \n  --gin_bindings=\"create_discretised_environment.environment_name='$environment'\" \\ \n  --gin_bindings=\"create_discretised_environment.version=%environment_metadata.${environment^^}_ENV_VER\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342
      ],
      "excerpt": "  --gin_bindings=\"Runner.max_steps_per_episode=%environment_metadata.${environment^^}_TIMELIMIT\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342
      ],
      "excerpt": "  --gin_bindings=\"WrappedPrioritizedReplayBuffer.action_shape=%environment_metadata.${environment^^}_ACTION_SHAPE\" \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8156894893537006
      ],
      "excerpt": "<img width=\"70%\" src=\"./data/images/framework.gif\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9196600214391931
      ],
      "excerpt": "<img width=\"60%\" src=\"./data/images/dqn_vs_hgqn.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944978598192337
      ],
      "excerpt": "<img width=\"70%\" src=\"./data/images/humanoid_CMU_silly.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python plot_physical.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9216507441889071
      ],
      "excerpt": "<img width=\"90%\" src=\"./data/images/physical_results.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8036388029056536
      ],
      "excerpt": "To plot the Atari 2600 benchmark, run the following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python plot_atari.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9216507441889071
      ],
      "excerpt": "<img width=\"90%\" src=\"./data/images/atari_results.png\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/atavakol/action-hypergraph-networks/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Arash Tavakoli\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Learning to Represent Action Values as a Hypergraph on the Action Vertices",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "action-hypergraph-networks",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "atavakol",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/atavakol/action-hypergraph-networks/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10,
      "date": "Thu, 30 Dec 2021 07:55:29 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning",
      "agents",
      "iclr",
      "deep-reinforcement-learning",
      "deepmind-control-suite",
      "openai-gym",
      "pybullet",
      "box2d",
      "mujoco",
      "atari",
      "arcade-learning-environment"
    ],
    "technique": "GitHub API"
  }
}