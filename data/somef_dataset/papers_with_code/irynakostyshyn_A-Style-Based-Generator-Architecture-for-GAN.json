{
  "citation": [
    {
      "confidence": [
        0.9738165058290004
      ],
      "excerpt": "Title: A Style-Based Generator Architecture for Generative Adversarial Networks \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/irynakostyshyn/A-Style-Based-Generator-Architecture-for-GAN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-25T20:14:14Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-25T22:37:34Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* **What:**\n\n  * The authors propose an alternative generator architecture for generative adversarial\n   networks, borrowing from style transfer literature \n  * The new generator improves the state-of-the-art in terms\n   of traditional distribution quality metrics\n  * The authors propose two new, automated methods to quantify interpolation quality \n  and disentanglement, that are applicable to any generator architecture\n  * The authors introduce a new, highly varied and high-quality dataset of\n   human faces The (FFHQ, Flickr-Faces-HQ).\n\n  * The authors proposed the way to control different features of the generated image and made\n  the model more explainable. See theirs official video (https://www.youtube.com/watch?v=kSLJriaOumA&feature=youtu.be)\n* **Method:**\n\nThe idea of creating Mapping Network with the transformed/inner latent space is used, instead of using input \nlatent code. This mapping part consists of 8 MLP blocks(8 gave the best performance), which gives __new__ `W-space`\n latent code, that is passed to the __Generator__ as a style which consists of 18 conv layers(two for each resolution 4 ** 2 - 1024 ** 2).\n See [#Architecture] figure.\n \n Through the experimental process it was found out that starting image at the top of `g-Generator` can be learn \n constant tensor instead of a real image. Following the architecture this image then in each block\n  follows the next process. First conv layer. Then summed with _per channel scaled\n noise_ (scaling is learned as B-parameter). Then again conv. Then AdaIN block.\n \n AdaIN Block can be described the next way:\n ![AdaIN](assets/adain.png)\n where `x_i` is input per channel feature passed vertically from top and `(y_s_i, y_b_i)` are _style_ parameters\n got from transformation of `w-vector` from input `W` latent space. To get these style parameters this vector is \n first passed through learned affine transformation. \n  \n  **To sum it all up**, the main interesting points are: the input `4x4x512` tensor is always the same and we\n  only get styles from our latent variable. Moreover they say that at each scale changing our _style_ inputs\n  we can change different **very** localised parts of generated image (See video).\n  \n  > Copying the styles corresponding to coarse spatial resolutions (4 ** 2\u20138 ** 2) brings high-level aspects such as pose, general hair style, face shape, and eyeglasses from source B, while all colors(eyes, hair, lighting) and finer facial features resemble A. If we instead copy the styles of middle resolutions (16 ** 2\u201332 ** 2) from B, we inheritsmaller scale facial features, hair style, eyes open/closed from B, while the pose, general face shape, and eyeglasses from A are preserved.Finally, copying the fine styles (64 ** 2\u20131024 ** 2) \n\nBy the way most of the architecture/training process is based on https://arxiv.org/pdf/1710.10196.pdf (Baseline configuration is the Progressive GAN). \nThe changes made to architecture:\n- The baseline using bilinear up/downsampling operations, longer training, and tuned hyperparameters\n- Adding the mapping network and AdaIN operations\n- Simplify the architecture by removing the traditional input layer and starting the image synthesis from a learned 4 \u00d7 4 \u00d7 512 constant tensor\n- Adding the noise inputs\n- Adding the mixing regularization(I didn't mention it. The idea is the following: in the learning process we don't \ntake one latent variable, but take two. Then for one part of the network we feed as style first variable and then the other.\nThat way the model separate the influence and doesn't entangle it. At test time see [Mixing] figure below)\n\n![Architecture](assets/photo5197353128075307948.jpg)\n\nFigure _Architecture_\n* **W** - an intermediate latent space\n* **AdaIN** - adaptive *style* instance normalization\n* **A** - learned affine transform\n* **B** - learned per-channel scaling fac-tors to the noise input\n\n![Mixture](assets/mixture.png)\n\n_Figure Mixing_. Two sets of images were generated from their respective latent codes (sources A and B); the rest of the images were generated bycopying a specified subset of styles from source B and taking the rest from source A.\n\n\n* **Proposed metrics:**\n  \n    - Perceptual path length:\n    ![Path_length](assets/path_length.png)\n    where `z1, z2` are latent variables; `t` is random value in `(0,1)` interval;\n    `eps` = `1e-4`; `G` stands for generator; `slerp` - spherical \n    interpolation; `d(x1, x2)` - `l2(vgg(x1), vgg(x2))`;\n    The idea is that for less entangled and informative latent code interpolation of latent-space \n    vectors should not yield surprisingly non-linear changes in the image. Quantative results in the Table 3.\n    - Linear separability. Tries to find out the separability in the input\n    latent code. Given the features of generated images as Male/Female, find out\n    how informative for this info is input latent code.\n    The idea is the next: given good dataset as CELEBA-HQ train good classifier of \n    a given feature; then generate images using our **Generator** and label using trained\n    classifier so we have <latent variable, class of interest> mapping; then train SVM on this\n    data and exponent of cross entropy of this model will be the score of Linear Separability.\n    See table:\n    ![Metrics](assets/metrics.png) \n* **Results:**\n\n  * **Frechet inception distance (FID)**\n\n![FID](assets/photo5195447889172737473.jpg)\n\n \n\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/irynakostyshyn/A-Style-Based-Generator-Architecture-for-GAN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 23:57:39 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/irynakostyshyn/A-Style-Based-Generator-Architecture-for-GAN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "irynakostyshyn/A-Style-Based-Generator-Architecture-for-GAN",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8971516614830636
      ],
      "excerpt": "<img src=\"./assets/photo5197499139783502838.jpg\" width=\"250\" align=\"right\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/irynakostyshyn/A-Style-Based-Generator-Architecture-for-GAN/issues{/number}",
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Paper Review: A Style-Based Generator Architecture for Generative Adversarial Networks",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "A-Style-Based-Generator-Architecture-for-GAN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "irynakostyshyn",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/irynakostyshyn/A-Style-Based-Generator-Architecture-for-GAN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 23:57:39 GMT"
    },
    "technique": "GitHub API"
  }
}