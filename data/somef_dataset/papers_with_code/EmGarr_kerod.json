{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1612.03144",
      "https://arxiv.org/abs/1703.06870"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "policy = mixed_precision.Policy('mixed_float16') \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EmGarr/kerod",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contribution\nTests\nbash\nmake tests\nDeveloppers\nCode coverage\nWhenever you implement a new functionality you should reach a proper code coverage. Every pull requests will be rejected if the code coverage doesn't reach 90%.\nDocstring formatting\nThe doc generation tool used is portray which handle markdown format\n```python\ndef func(a, b):\n  \"\"\"Describe my function\nArguments:\n\na: A param a description\nb: A param b description\n\nReturns:\nThe sum of a + b\nRaises: (If exception are raised)\nExceptions:\n  1\n  \"\"\"\n  return a + b\n```\nThe code formatting used is yapf\nThe config are automatically loaded from .style.yapf\nYAPF tries very hard to get the formatting correct. But for some code, it won't be as good as hand-formatting. In particular, large data literals may become horribly disfigured under YAPF.\nThe reasons for this are manyfold. In short, YAPF is simply a tool to help with development. It will format things to coincide with the style guide, but that may not equate with readability.\nWhat can be done to alleviate this situation is to indicate regions YAPF should ignore when reformatting something:\n```python\nyapf: disable\nFOO = {\n    # ... some very large, complex data literal.\n}\nBAR = [\n    # ... another large data literal.\n]\nyapf: enable\n```\nYou can also disable formatting for a single literal like this:\npython\nBAZ = {\n    (1, 2, 3, 4),\n    (5, 6, 7, 8),\n    (9, 10, 11, 12),\n}  # yapf: disable\nIn addition of this, it's recommended to have an automatic formatter of the\nimports. Imports of the same module should be imported together:\n```python\nfrom libs.fooo.resnet_v1_101 import (create_resnet, support_utils_resnet_foo, upload_foo)\n```\nImports should be structured in 3 parts, each separated by a blank line:\n```python\nimport os  # Base package\nimport keras  # External package from pip\nfrom libs import foo  # import package inner modules\n```\nFinally, it is prefered that the imports are sorted alphabetically.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-21T18:54:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-14T02:23:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9657754407609266
      ],
      "excerpt": "[x] Single scale SMCA (WIP not 100% the exact implementation of the paper): Fast Convergence of DETR with Spatially Modulated Co-Attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9479336397348601
      ],
      "excerpt": "Pytorch: means resnet implementation Pytorch style. In the residual block we have: conv (1x1) stride 1 -> conv (3x3) stride 2 instead of conv (1x1) stride 2 -> conv (3x3) stride 1 (Caffe, Keras implementations) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9733112157445836
      ],
      "excerpt": "Why: In deep learning each parameter is important. You must think thoroughly before a change on how it will impact your model. Here, the code base is super simple just rewrite the blocks that you need and create new layers using the power of Keras. Also, it makes the code easier to read. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = factory.build_model(num_classes) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.8720709625308953
      ],
      "excerpt": "data =  data.padded_batch(batch_size, padded_shape) \ndata = data.prefetch(tf.data.experimental.AUTOTUNE) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127,
        0.860059181823877
      ],
      "excerpt": "with mirrored_strategy.scope():  \n    model = factory.build_model(num_classes) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9518602519746595
      ],
      "excerpt": "model.fit(data, epochs=2, callbacks=[ModelCheckpoint('checkpoints')]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9886745384591392
      ],
      "excerpt": "See the outputs of the predict_step of your. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8619089259402867
      ],
      "excerpt": "bbox: A Tensor of shape [batch_size, max_detections, 4] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9945593714433894
      ],
      "excerpt": "in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "DETR - Faster RCNN implementation in tensorflow 2",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EmGarr/kerod/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Thu, 23 Dec 2021 16:26:56 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/EmGarr/kerod/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "EmGarr/kerod",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/EmGarr/kerod/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/EmGarr/kerod/master/notebooks/detr_coco_training_multi_gpu.ipynb",
      "https://raw.githubusercontent.com/EmGarr/kerod/master/notebooks/overfit-detr.ipynb",
      "https://raw.githubusercontent.com/EmGarr/kerod/master/notebooks/coco_training_multi_gpu.ipynb",
      "https://raw.githubusercontent.com/EmGarr/kerod/master/notebooks/coco_training.ipynb",
      "https://raw.githubusercontent.com/EmGarr/kerod/master/notebooks/mixed_precision_pascal_voc_training_fpn50.ipynb",
      "https://raw.githubusercontent.com/EmGarr/kerod/master/notebooks/pascal_voc_training_fpn50.ipynb",
      "https://raw.githubusercontent.com/EmGarr/kerod/master/notebooks/smca_coco_training_multi_gpu.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repo is tested on Python 3.6, 3.7, 3.8 and TensorFlow 2.4.0\n\nYou may want to install 'kerod' in a [virtual environment](https://docs.python.org/3/library/venv.html) or with [pyenv](https://github.com/pyenv/pyenv). Create a virtual environment with the version of Python you wanna use and activate it.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8470241704914727
      ],
      "excerpt": "Mixed_precision. You can try it with this notebook \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8727487047449518
      ],
      "excerpt": "| FasterRcnnFPNResnet50Pytorch | COCO      |             |                    |                   |                              |   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8142304550446777
      ],
      "excerpt": "| DetrResnet50Pytorch | COCO      | NEED 16 GPUS for 3 days | :heavy_check_mark: |                   |               |   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9984079357284334,
        0.9893272198983933,
        0.9906248903846466,
        0.999746712887969,
        0.9043597798168064,
        0.8504291753296438,
        0.9948772453318471,
        0.9373140675019294,
        0.999746712887969
      ],
      "excerpt": "pip install git+https://github.com/EmGarr/kerod.git \ngit clone https://github.com/EmGarr/kerod.git \ncd kerod \npip install . \nWhen you update the repository, you should upgrade installation and its dependencies as follows: \ngit pull \npip install --upgrade . \nYou can install the package in dev mode as follow and everytime you refresh the package it will be automatically updated: \npip install -e . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9531754173649162,
        0.9961885120585449
      ],
      "excerpt": "In order to run the tests you should install pytest. \npip install pytest \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8069792203632054
      ],
      "excerpt": "make test \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8147774496549085,
        0.8209196318474163,
        0.8934477375241742
      ],
      "excerpt": "from tensorflow.keras.mixed_precision import experimental as mixed_precision \nfrom kerod.dataset.preprocessing import expand_dims_for_single_batch, preprocess \nfrom kerod.model import factory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9457175861910134,
        0.8209196318474163,
        0.8934477375241742
      ],
      "excerpt": "import numpy as np \nfrom kerod.dataset.preprocessing import expand_dims_for_single_batch, preprocess \nfrom kerod.model import factory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.823549636584484,
        0.8211888460937542,
        0.8062108599016755,
        0.8123763140827432
      ],
      "excerpt": "data = tf.data.Dataset.from_tensor_slices(inputs) \ndata =  data.padded_batch(batch_size, padded_shape) \ndata = data.prefetch(tf.data.experimental.AUTOTUNE) \nmirrored_strategy = tf.distribute.MirroredStrategy() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "    optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8434247051783932
      ],
      "excerpt": "image_information = tf.cast(tf.shape(image)[:2], dtype=tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8793144094799785,
        0.8573839104710763
      ],
      "excerpt": "    tf.expand_dims(images, axis=0).numpy().tolist(), \n  tf.expand_dims(image_information, axis=0).numpy().tolist() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8013062111464563
      ],
      "excerpt": "headers = {\"content-type\": \"application/json\"} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8104514181613496
      ],
      "excerpt": "bbox: A Tensor of shape [batch_size, max_detections, 4] \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/EmGarr/kerod/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) [year] [fullname]\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Features",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "kerod",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "EmGarr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EmGarr/kerod/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you don't run the examples on Colab please install `tensorflow_datasets`:\n\n```bash\npip install tensorflow_datasets\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 37,
      "date": "Thu, 23 Dec 2021 16:26:56 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "detection",
      "tensorflow2",
      "object-detection",
      "computer-vision",
      "instance-segmentation",
      "faster-rcnn",
      "feature-pyramid-network",
      "detr",
      "transformer",
      "coco",
      "tensorflow",
      "detections"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run a training you just need to write the following. \n\n```python\nimport numpy as np\nfrom kerod.dataset.preprocessing import expand_dims_for_single_batch, preprocess\nfrom kerod.model import factory, KerodModel\n\nnum_classes = 20\nmodel = factory.build_model(num_classes, name=KerodModel.faster_rcnn_resnet50_pytorch)\n\n#: Same format than COCO and Pascal VOC in tensorflow datasets\ninputs = {\n    'image': np.zeros((2, 100, 50, 3)),\n    'objects': {\n        BoxField.BOXES: np.array([[[0, 0, 1, 1]], [[0, 0, 1, 1]]], dtype=np.float32),\n        BoxField.LABELS: np.array([[1], [1]])\n    }\n}\n\ndata = tf.data.Dataset.from_tensor_slices(inputs)\ndata = data.map(preprocess)\ndata = data.map(expand_dims_for_single_batch)\n\nbase_lr = 0.02\noptimizer = tf.keras.optimizers.SGD(learning_rate=base_lr)\nmodel.compile(optimizer=optimizer, loss=None)\nmodel.fit(data, epochs=2, callbacks=[ModelCheckpoint('checkpoints')])\n\nresults = model.predict(data, batch_size=1)\n\nuse_faster_rcnn = True\nif use_faster_rcnn:\n    model.export_for_serving('saved_model')\nelse:\n    model.save('saved_model')\nreload_model = tf.keras.models.load_model('saved_model')\nfor x, _ in data:\n    if use_faster_rcnn:\n        reload_model.serving_step(x[DatasetField.IMAGES], x[DatasetField.IMAGES_INFO])\n    else:\n        reload_model.predict_step(x)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}