{
  "citation": [
    {
      "confidence": [
        0.8511535834563841
      ],
      "excerpt": "| Source (iPhone) | Target (DSLR) | Results | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511535834563841
      ],
      "excerpt": "| Source (DSLR) | Target (iPhone) | Results | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fuenwang/CVFX-hw1",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-06T17:10:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-07T14:46:27Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8848749653761805,
        0.9844079242381417
      ],
      "excerpt": "The goal of this assignment is to train a color/texture transfer model using CycleGAN. \nIn this assignment, we use the iphone2dslr_flower dataset, which is introduced in the CycleGAN paper. iphone2dslr_flower contains iPhone and DSLR photos of flowers downloaded from Flickr photos. The main difference between these two classes is the depth of field in the images. Depth of field (DOF) is the distance between the nearest and the furthest objects that are in acceptably sharp focus in an image. We summarize the total number of examples and corresponding DOF type of each class under the below table.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8640748741499912,
        0.806286979624536
      ],
      "excerpt": "we translate from one domain to the other and back again we should arrive at where we started. \nMore specifically, the two losses are forward cycle-consistency loss and backward cycle-consistency loss. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8136664393043375
      ],
      "excerpt": "In the original paper, the total cycle loss is defined as: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9494903591174774,
        0.9338936587238725,
        0.9875922431274538
      ],
      "excerpt": "which is the sum of two L1 normalized loss(forward_loss amd backward_loss). \nApart from the cycle loss, the identity loss is also introduced in the paper. The intuition behind the identity loss is to encourage the mapping to preserve color composition between the input and output. \nWe use our personal image (e.g., photoed by our iPhones) as inputs. As the table shown below, our model can learn to generate photos in the style of both DSLR and iPhone. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8258459716512729
      ],
      "excerpt": "We compare our result with Color Transfer between Images [Reinhard et al., 2001]. In the paper, the authors utilize the Lab color space and the mean and std of each L, a, and b channel, respectively, to transfer the color between two images. We use the opencv implementation provided in here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9884059057001969
      ],
      "excerpt": "Our CycleGAN model shows significantly better results comparing to color transfer. Unlike color transfer which learns to transfer the style of a single selected piece of art, our CycleGAN model learns to mimic the style of an entire collection of dataset. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fuenwang/CVFX-hw1/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 23:29:48 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/fuenwang/CVFX-hw1/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "fuenwang/CVFX-hw1",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/fuenwang/CVFX-hw1/master/download_dataset.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8838148168639296,
        0.9083158493364345
      ],
      "excerpt": "examples | 1813   | 3316 | \ninstance | <img src=\"output/imgs/sample_iphone.png\" alt=\"drawing\" width=\"150\"/> | <img src=\"output/imgs/sample_dslr.png\" alt=\"drawing\" width=\"150\"/> | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8912702828996482,
        0.8912702828996482,
        0.8912702828996482,
        0.9008300864076799,
        0.8700298619160237,
        0.8852281616469837
      ],
      "excerpt": "|<img src=\"output/imgs/inference/3.png\" alt=\"drawing\" width=\"300\"/>|<img src=\"output/imgs/inference/6.png\" alt=\"drawing\" width=\"300\"/>| \n|<img src=\"output/imgs/inference/7.png\" alt=\"drawing\" width=\"300\"/>|<img src=\"output/imgs/inference/1.png\" alt=\"drawing\" width=\"300\"/>| \n|<img src=\"output/imgs/inference/8.png\" alt=\"drawing\" width=\"300\"/>|<img src=\"output/imgs/inference/2.png\" alt=\"drawing\" width=\"300\"/>| \n|<img src=\"output/imgs/inference/10.png\" alt=\"drawing\" width=\"300\"/>|<img src=\"output/imgs/inference/4.png\" alt=\"drawing\" width=\"300\"/>| \n|<img src=\"output/imgs/inference/11.png\" alt=\"drawing\" width=\"300\"/>|<img src=\"output/imgs/inference/5.png\" alt=\"drawing\" width=\"300\"/>| \n|<img src=\"output/imgs/inference/12.png\" alt=\"drawing\" width=\"300\"/>|<img src=\"output/imgs/inference/9.png\" alt=\"drawing\" width=\"300\"/>| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9192545052756208
      ],
      "excerpt": "|<img src=\"output/imgs/inference/a.jpg\" alt=\"drawing\" height=\"300\"/>|<img src=\"output/imgs/inference/b.jpg\" alt=\"drawing\" height=\"300\"/>|<img src=\"output/imgs/inference/c.jpg\" alt=\"drawing\" height=\"300\"/>| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9192545052756208
      ],
      "excerpt": "|<img src=\"output/imgs/inference/b.jpg\" alt=\"drawing\" height=\"300\"/>|<img src=\"output/imgs/inference/a.jpg\" alt=\"drawing\" height=\"300\"/>|<img src=\"output/imgs/inference/d.jpg\" alt=\"drawing\" height=\"300\"/>| \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/fuenwang/CVFX-hw1/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Homework1 Report",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CVFX-hw1",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "fuenwang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fuenwang/CVFX-hw1/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 23:29:48 GMT"
    },
    "technique": "GitHub API"
  }
}