{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805.\n\n**Disclaimer**\n\n- Most of the code is taken from google-research github account\n- The bert model is fine-tuned only.\n- The code modified as per necesscity\n- Used the bert base model with 110M parameters\n- All the referance are mentioned in the referances section\n\n**For ipynb notebook , please check the bert folder**\n\n## Blog:\n**I have written a detailed post regarding this on medium. You can read it here https://medium.com/@raman.shinde15/neural-question-and-answering-using-sqad-dataset-and-attention-983d3a1dd42c**\n\n\n## Observations:\n\n* Obtained micro f1_score of 40.33% on test data.\n* Algined question embedding and f_exact match found to be the moset effective as mentioned in paper\n* f1_score can be further improoved by adding Algined question embedding feature to context.\n* Algined question embedding was omitted due to computational power limits\n* To train on 1 epoch it took around hour without Algined question embedding\n* Algined question embedding was omittited because, training on 1 epoch was taking more than 5 hours.\n* Performance can be improoved further by considering:\n    * All data points\n    * Taking 128 units and 3 Layer of Bi_LSTM as mentioned in paper.\n    * Considering Algined question embedding + f_exact together.\n* Fine tuned Bert Uncased state of the art model to get the results.\n* Bert model results are obtained using TPU provided by google\n\n## Summary:\n \n![Summary](/images/summary.PNG"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9999855583970872
      ],
      "excerpt": "Please refer this research paper. https://arxiv.org/abs/1810.04805. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9962869161615556
      ],
      "excerpt": "ReadingWikipedia to Answer Open-Domain Questions https://arxiv.org/pdf/1704.00051.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Raman-Raje/Machine-Reading-Comprehension-Neural-Question-Answer-",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-04T05:23:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-11T23:31:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": " \n![Summary](/images/summary.PNG)\n \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8821199869857212,
        0.8357111580527303,
        0.8691738340205444,
        0.9873589437433746
      ],
      "excerpt": "Data Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. You can download this dataset here https://rajpurkar.github.io/SQuAD-explorer/ \nSQuAD 1.1: The previous version of the SQuAD dataset, contains 100,000+ question-answer pairs on 500+ articles. \nImplemented standford attentive reader model using keras.Please refer this paper. \nBERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.92012901686672,
        0.9809298547684522
      ],
      "excerpt": "Most of the code is taken from google-research github account \nThe bert model is fine-tuned only. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.934068379845864,
        0.9545486555431445
      ],
      "excerpt": "Used the bert base model with 110M parameters \nAll the referance are mentioned in the referances section \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9014435807945483,
        0.8531997114762001
      ],
      "excerpt": "Obtained micro f1_score of 40.33% on test data. \nAlgined question embedding and f_exact match found to be the moset effective as mentioned in paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.938213131926166,
        0.8621306424512055
      ],
      "excerpt": "All data points \nTaking 128 units and 3 Layer of Bi_LSTM as mentioned in paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9864899762457232,
        0.8552816943401367
      ],
      "excerpt": "Fine tuned Bert Uncased state of the art model to get the results. \nBert model results are obtained using TPU provided by google \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This repository belong to Machine Reading Comprehension(Neural Question Answer)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Raman-Raje/Machine-Reading-Comprehension-Neural-Question-Answer-/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Predicting the right answer for the given question and context.\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Wed, 22 Dec 2021 15:24:53 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Raman-Raje/Machine-Reading-Comprehension-Neural-Question-Answer-/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Raman-Raje/Machine-Reading-Comprehension-Neural-Question-Answer-",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Raman-Raje/Machine-Reading-Comprehension-Neural-Question-Answer-/master/standford%20attentive%20reader/raman.shinde15%40gmail.com_29.ipynb",
      "https://raw.githubusercontent.com/Raman-Raje/Machine-Reading-Comprehension-Neural-Question-Answer-/master/bert_on_squad/raman.shinde15%40gmail.com_bert.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Raman-Raje/Machine-Reading-Comprehension-Neural-Question-Answer-/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "HTML",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Machine Reading Comprehension using SQUAD v.1",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Machine-Reading-Comprehension-Neural-Question-Answer-",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Raman-Raje",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Raman-Raje/Machine-Reading-Comprehension-Neural-Question-Answer-/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Wed, 22 Dec 2021 15:24:53 GMT"
    },
    "technique": "GitHub API"
  }
}