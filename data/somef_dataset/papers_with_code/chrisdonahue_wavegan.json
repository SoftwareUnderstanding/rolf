{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1802.04208",
      "https://arxiv.org/abs/1802.04208"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{donahue2019wavegan,\n  title={Adversarial Audio Synthesis},\n  author={Donahue, Chris and McAuley, Julian and Puckette, Miller},\n  booktitle={ICLR},\n  year={2019}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chrisdonahue/wavegan",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-02-09T21:27:42Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T06:46:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9354123229438299,
        0.9565323960049587
      ],
      "excerpt": "Official implementation of WaveGAN, a machine learning algorithm which learns to generate raw audio waveforms. \nUPDATE (2/2/19): We have made substantial improvements to this repository in response to common requests: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452803311729376,
        0.9870399030968572
      ],
      "excerpt": "This is the official TensorFlow implementation of WaveGAN (Donahue et al. 2018) (paper) (demo) (sound examples). WaveGAN is a machine learning algorithm which learns to synthesize raw waveform audio by observing many examples of real audio. WaveGAN is comparable to the popular DCGAN approach (Radford et al. 2016) for learning to generate images. \nIn this repository, we include an implementation of WaveGAN capable of learning to generate up to 4 seconds of audio at 16kHz. For comparison, we also include an implementation of SpecGAN, an approach to audio generation which applies image-generating GANs to image-like audio spectrograms. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9810637568726364
      ],
      "excerpt": "WaveGAN is capable of learning to synthesize audio in many different sound domains. In the above figure, we visualize real and WaveGAN-generated audio of speech, bird vocalizations, drum sound effects, and piano excerpts. These sound examples and more can be heard here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "    --data_dir ./data/dir_with_longer_audio_files \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8361671097027958
      ],
      "excerpt": "Because our codebase buffers audio clips directly from files, it is important to change the data-related command line arguments to be appropriate for your dataset (see #data-considerations). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8453029134563275
      ],
      "excerpt": "If your results are too noisy, try adding a post-processing filter with --wavegan_genr_pp. You may also want to change the amount of or remove phase shuffle using --wavegan_disc_phaseshuffle 0. Increasing either the model size (--wavegan_dim) or filter length (--wavegan_kernel_len) may improve results but will increase training time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051737066599788
      ],
      "excerpt": "To back up checkpoints every hour (GAN training may occasionally collapse so it's good to have backups) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9890224187898062,
        0.841018885291046
      ],
      "excerpt": "The primary focus of this repository is on WaveGAN, our raw audio generation method. For comparison, we also include an implementation of SpecGAN, an approach to generating audio by applying image-generating GANs on image-like audio spectrograms. This implementation only generates spectrograms of one second in length at 16khz. \nBefore training a SpecGAN, we must first compute mean and variance of each spectrogram bin to use for normalization. This may take a while(you can also measure these statistics on a subset of the data) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "    --data_dir ./data/dir_with_mp3s \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "    --data_dir ./data/dir_with_mp3s \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095869796286659
      ],
      "excerpt": "The training scripts for both WaveGAN and SpecGAN create simple TensorFlow MetaGraphs for generating audio waveforms, located in the training directory. An example usage is below; see this Colab notebook for additional features. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "saver.restore(sess, 'model.ckpt') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9564422586521601
      ],
      "excerpt": "Under web, we also include a JavaScript implementation of WaveGAN (generation only). Using this implementation, we created a procedural drum machine powered by a WaveGAN trained on drum sound effects. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "WaveGAN: Learn to synthesize raw audio with generative adversarial networks",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chrisdonahue/wavegan/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 234,
      "date": "Sat, 25 Dec 2021 15:51:22 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/chrisdonahue/wavegan/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "chrisdonahue/wavegan",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/chrisdonahue/wavegan/master/eval/noise/wavegan_response.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/chrisdonahue/wavegan/master/web/ckpts/dumper/dump.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8805017502890526
      ],
      "excerpt": "WaveGAN can now be trained on datasets of arbitrary audio files (previously required preprocessing). You can use any folder containing audio, but here are a few example datasets to help you get started: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.865147164723687
      ],
      "excerpt": "<img src=\"static/wavegan.png\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8736328308621912
      ],
      "excerpt": "<img src=\"static/results.png\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8065124344305686
      ],
      "excerpt": "Here is how you would begin (or resume) training a WaveGAN on random clips from a directory containing longer audio, i.e., more than a few seconds per file: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9363510515943466
      ],
      "excerpt": "python train_wavegan.py train ./train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9363510515943466,
        0.894036606553709
      ],
      "excerpt": "python train_wavegan.py train ./train \\ \n    --data_dir ./data/sc09/train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.82632991695146
      ],
      "excerpt": "The WaveGAN training script is configured out-of-the-box to be appropriate for training on random slices from a directory containing longer audio files (e.g., songs, a common use case). If you want to train WaveGAN on shorter sound effects, you will almost likely want to use the flag --data_first_slice to only extract the first slice from each audio file. If your clips are extremely short (i.e., less than $16384$ samples each as in SC09), you will want to add --data_pad_end so that they get zero padded to fill the slice. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train_wavegan.py preview ./train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8800201964783315
      ],
      "excerpt": "python backup.py ./train 60 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8149623929371536
      ],
      "excerpt": "tensorboard --logdir=./train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train_wavegan.py incept ./train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train_specgan.py moments ./train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9363510515943466
      ],
      "excerpt": "python train_specgan.py train ./train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train_specgan.py preview ./train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8800201964783315
      ],
      "excerpt": "python backup.py ./train 60 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8149623929371536
      ],
      "excerpt": "tensorboard --logdir=./train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train_specgan.py incept ./train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925671696398174
      ],
      "excerpt": "import tensorflow as tf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.91161623999058
      ],
      "excerpt": "saver = tf.train.import_meta_graph('infer.meta') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "sess = tf.InteractiveSession() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8905573287067622
      ],
      "excerpt": "_z = (np.random.rand(50, 100) * 2.) - 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008331685760428
      ],
      "excerpt": "_G_z = sess.run(G_z, {z: _z}) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8609520445175076
      ],
      "excerpt": "python score.py --audio_dir wavs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9128644026793049
      ],
      "excerpt": "python score.py --audio_dir sc09/train  --fix_length --n 18620 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/chrisdonahue/wavegan/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "JavaScript",
      "HTML",
      "CSS",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "WaveGAN (v2)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "wavegan",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "chrisdonahue",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chrisdonahue/wavegan/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\npip install tensorflow-gpu==1.12.0\npip install scipy==1.0.0\npip install matplotlib==3.0.2\npip install librosa==0.6.2\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1071,
      "date": "Sat, 25 Dec 2021 15:51:22 GMT"
    },
    "technique": "GitHub API"
  }
}