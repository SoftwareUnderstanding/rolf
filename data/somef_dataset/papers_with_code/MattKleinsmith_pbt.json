{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repo is inspired by [bkj's pbt repo](https://github.com/bkj/pbt), where they replicated figure 2 of the paper.\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1711.09846",
      "https://arxiv.org/abs/1711.09846"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8360866863053694
      ],
      "excerpt": "Population Based Training of Neural Networks, Jaderberg et al. @ DeepMind \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MattKleinsmith/pbt",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-12-17T06:23:10Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-23T16:49:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.844610800610474,
        0.9598235882618746
      ],
      "excerpt": "Population Based Training of Neural Networks, Jaderberg et al. @ DeepMind \nA PyTorch implementation of PBT. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9843023992769377,
        0.8375628084200069,
        0.9797281634236903,
        0.9699079609130673,
        0.9663535291741238,
        0.9303706457520163,
        0.8924492270608281
      ],
      "excerpt": "These figures are for building intuition for PBT. They aren't the results of a rigorous experiment. In the accuracy plots, the best model is shown in purple. In the hyperparameter scatter plots, the size of the dots grow as the models train. The hyperparameter configurations of the best model from each population are purple stars. \nNotice how the hyperparameter configurations evolve in PBT, but stay the same in random search. \nPBT trains each model partially and assesses them on the validation set. It then transfers the parameters and hyperparameters from the top performing models to the bottom performing models (exploitation). After transferring the hyperparameters, PBT perturbs them (exploration). Each model is then trained some more, and the process repeats. This allows PBT to learn a hyperparameter schedule instead of only a fixed hyperparameter configuration. PBT can be used with different selection methods (e.g. different ways of defining \"top\" and \"bottom\" (e.g. top 5, top 5%, etc.)). \nFor more information, see the paper or blog post. \nTruncation selection: For each model in the bottom 20% of performance, sample a model from the top 20% and transfer its parameters and hyperparameters to the worse model. One can think of the models in the bottom 20% as being truncated during each exploitation step. Leave the top 80% unchanged. This selection method was used in the paper. \nThe figures above were produced with a naive PBT selection method: select the best model each time. The accuracy improves to around 99.35% with the selection method in the paper: truncation selection. Seeds will change results. A simple conv net was used. Dataset: MNIST. \nI produced these figures using an old and very different version of this repo. I haven't yet re-added logging and plotting. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Population Based Training (in PyTorch with sqlite3). Status: Unsupported",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MattKleinsmith/pbt/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 21,
      "date": "Wed, 22 Dec 2021 09:50:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MattKleinsmith/pbt/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "MattKleinsmith/pbt",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/MattKleinsmith/pbt/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MattKleinsmith/pbt/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 Matt Kleinsmith\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## PBT: Population Based Training",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pbt",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "MattKleinsmith",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MattKleinsmith/pbt/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 149,
      "date": "Wed, 22 Dec 2021 09:50:31 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pbt",
      "hyperparameters",
      "population-based-training",
      "deep-learning",
      "hyperparameter-optimization",
      "hyperparameter-tuning",
      "hyperparameter-search",
      "deepmind"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Warning: This implementation isn't user friendly yet. If you have any questions, [create a github issue](https://github.com/MattKleinsmith/pbt/issues/new) and I'll try to help you.\n\nSteps:\n\n1. Wrestle with dependencies.\n2. Edit config.py to set your options.\n3. Store your data as bcolz carrays. See datasets.py for an example.\n4. In a terminal, enter: `python main.py --exploiter`\n5. If you want to use a second GPU, then in a second terminal, enter: `python main.py --gpu 1 --population_id -1`, where \"1\" refers to your GPU's ID in nvidia-smi, and \"-1\" means to work on the most recently created population.\n\nWhen finished, the process will print the path to the weights of the best performing model.\n\n",
      "technique": "Header extraction"
    }
  ]
}