{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "| **UCF101** \t| S-Conv \t|  F-Conv  \t|  F-Conv - Temporal  |\n|------------\t|:------:\t|:--------:\t|:--------:\t|\n| RN-18      \t|  38.6  \t| **40.6** \t|**42.2** |\n| RN-34      \t|  37.0  \t| **46.9** \t| - \t|\n| RN-50      \t|  36.2  \t| **44.1** \t| -\t|\n\n\n| **HMDB51** \t| S-Conv \t|  F-Conv  \t|\n|------------\t|:------:\t|:--------:\t|\n| RN-18      \t|  16.1  \t| **19.3** \t|\n| RN-34      \t|  15.2  \t| **18.3** \t|\n| RN-50      \t|  14.3  \t| **19.0** \t|\n\n\n<img src=\"images/action_training_curves.png\" align=\"center\" width=\"700\" title=\"Training curves\">\n\nThe experiment folder is [here](full_conv/Exp5_small_datasets/action_recognition).\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2003.07064",
      "https://arxiv.org/abs/1907.07174",
      "https://arxiv.org/abs/1512.03385",
      "https://arxiv.org/abs/1705.10872"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8132852241448368,
        0.955282076251007
      ],
      "excerpt": "In CVPR, 2020. \nOsman Semih Kayhan and Jan van Gemert.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8707843925837186
      ],
      "excerpt": "<img src=\"images/chess.png\" align=\"center\" width=\"200\" title=\"Rook\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9342780156679585
      ],
      "excerpt": "<img src=\"images/4QI_results.png\" align=\"center\" width=\"600\" title=\"4QI Results\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9179592177678713
      ],
      "excerpt": "<img src=\"images/redgreen.png\" align=\"center\" width=\"500\" title=\"Red and Green\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| RN34          |  87.62    |  90.12    |    89.21      |  91.53    | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/oskyhn/CNNs-Without-Borders",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-11T15:32:48Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-21T15:12:42Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.908925214220865,
        0.9714132768939688,
        0.9807185048962037,
        0.9968029537584643
      ],
      "excerpt": "Osman Semih Kayhan and Jan van Gemert.  \nYou can find the core of the paper and the detailed explanation of the experiments in our blog. \nThis repository contains the experiments of the paper.<br>  \nTable of contents<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8142622690469968,
        0.834798951531691
      ],
      "excerpt": "2. Border handling variants, with Red and Green experiments<br> \n3. Sensitivity to image shifts for image classification<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9490478838373951
      ],
      "excerpt": "We used pytorch for all the experiments in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693482611997382,
        0.9103776772392753
      ],
      "excerpt": "We have different demostrations of the fact: \nWe use a simple patch which is placed on the top-left and bottom-right of the image. We train 5x5 filter with zero padded same convolution followed by ReLU, global max pooling and softmax classifier by using SGD optimizer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8819116518775234,
        0.8850833189663614
      ],
      "excerpt": "The answer is YES! FCN can predict the location. You can try it by using the notebook. \nWe demonstrate the fact by using Imagenet pretrained weights, training from stratch with randomly initiliazed weights and using randomly initiliazed frozen weights on BagNet-33, Resnet-18 and DenseNet-121 methods. You can see the results below:<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9531657668992268
      ],
      "excerpt": "We evaluate convolution types in terms of border handling. We generate the Red-Green two class classification dataset (image below) for evaluating exploitation of absolute position. The upper row of images is class 1: Red-to-the-left-of-Green. The lower row of images is class 2: Green-to-the-left-of-Red. The Similar Testset is matching the Train-Val set in absolute location: Class 1 at the top and class 2 at the bottom. The Dissimilar testset is an exact copy of the Similar testset where absolute location is swapped between classes: Class 1 at the bottom, Class 2 at the top. If absolute location plays no role then classification on the Similar Testset would perform equal to the Dissimilar Testset. For each convolution type, the network has 4 convolution layers followed by global max pooling and softmax classifier. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9487211665378437
      ],
      "excerpt": "We show below the results of different convolution types on Red and Green dataset:<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9765812325198565
      ],
      "excerpt": "By using Imagenet-A dataset, we compare 4 different methods in terms of diagonal shifting and consistency on 4 ResNet architectures.<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8148237274405065
      ],
      "excerpt": "The experiment is done by using Imagenet-2012 dataset with Resnet-50 architecture. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Official repository of CVPR 2020 paper \"On Translation Invariance in CNNs: Convolutional Layers can Exploit Absolute Spatial Location\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/oskyhn/CNNs-Without-Borders/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Wed, 22 Dec 2021 19:44:46 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/oskyhn/CNNs-Without-Borders/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "oskyhn/CNNs-Without-Borders",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/oskyhn/CNNs-Without-Borders/master/full_conv/Exp1_image_boundary/chess.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/oskyhn/CNNs-Without-Borders/master/full_conv/Exp4_data_efficiency/patch_matching/code/train_hardnet_on_HPatches_per_split.sh",
      "https://raw.githubusercontent.com/oskyhn/CNNs-Without-Borders/master/full_conv/Exp4_data_efficiency/patch_matching/code/train_hardnet_on_6Brown.sh",
      "https://raw.githubusercontent.com/oskyhn/CNNs-Without-Borders/master/full_conv/Exp4_data_efficiency/patch_matching/code/full_run_single.sh",
      "https://raw.githubusercontent.com/oskyhn/CNNs-Without-Borders/master/full_conv/Exp4_data_efficiency/patch_matching/code/run_me.sh",
      "https://raw.githubusercontent.com/oskyhn/CNNs-Without-Borders/master/full_conv/Exp4_data_efficiency/patch_matching/code/run_me_fconv.sh",
      "https://raw.githubusercontent.com/oskyhn/CNNs-Without-Borders/master/full_conv/Exp4_data_efficiency/patch_matching/code/full_run.sh",
      "https://raw.githubusercontent.com/oskyhn/CNNs-Without-Borders/master/full_conv/Exp4_data_efficiency/patch_matching/code/run_ablation_study_training.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9875663681624087
      ],
      "excerpt": "Install PyTorch (pytorch.org) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8747620622364003
      ],
      "excerpt": "| Type      |  Pad  | Similar Test  | Dissimilar Test   | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/oskyhn/CNNs-Without-Borders/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2020, Osman Semih Kayhan\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\n3. Neither the name of the copyright holder nor the names of its\\n   contributors may be used to endorse or promote products derived from\\n   this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "On Translation Invariance in CNNs: Convolutional Layers can Exploit Absolute Spatial Location",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CNNs-Without-Borders",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "oskyhn",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/oskyhn/CNNs-Without-Borders/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 67,
      "date": "Wed, 22 Dec 2021 19:44:46 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For clarity, we put each experiment under a specific folder.<br>\n\n",
      "technique": "Header extraction"
    }
  ]
}