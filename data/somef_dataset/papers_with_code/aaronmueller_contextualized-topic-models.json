{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2104.05064",
      "https://arxiv.org/abs/2004.03974",
      "https://arxiv.org/abs/2004.07737"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use the materials in this repository in a research work, please cite this paper:\n\n```\n    @inproceedings{mueller-dredze-2021-encoders,\n        title = \"Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual Neural Topic Modeling\",\n        author = \"Mueller, Aaron  and\n          Dredze, Mark\",\n        booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n        month = jun,\n        year = \"2021\",\n        address = \"Online\",\n        publisher = \"Association for Computational Linguistics\",\n        url = \"https://www.aclweb.org/anthology/2021.naacl-main.243\",\n        pages = \"3054--3068\"\n    }\n```\n\nIn addition, please cite the following papers on contextualized topic modeling:\n\n```\n    @article{bianchi2020pretraining,\n        title={Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence},\n        author={Federico Bianchi and Silvia Terragni and Dirk Hovy},\n        year={2020},\n       journal={arXiv preprint arXiv:2004.03974},\n    }\n\n\n    @article{bianchi2020crosslingual,\n        title={Cross-lingual Contextualized Topic Models with Zero-shot Learning},\n        author={Federico Bianchi and Silvia Terragni and Dirk Hovy and Debora Nozza and Elisabetta Fersini},\n        year={2020},\n       journal={arXiv preprint arXiv:2004.07737},\n    }\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{mueller-dredze-2021-encoders,\n        title = \"Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual Neural Topic Modeling\",\n        author = \"Mueller, Aaron  and\n          Dredze, Mark\",\n        booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n        month = jun,\n        year = \"2021\",\n        address = \"Online\",\n        publisher = \"Association for Computational Linguistics\",\n        url = \"https://www.aclweb.org/anthology/2021.naacl-main.243\",\n        pages = \"3054--3068\"\n    }\nIn addition, please cite the following papers on contextualized topic modeling:\n```\n    @article{bianchi2020pretraining,\n        title={Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence},\n        author={Federico Bianchi and Silvia Terragni and Dirk Hovy},\n        year={2020},\n       journal={arXiv preprint arXiv:2004.03974},\n    }\n@article{bianchi2020crosslingual,\n    title={Cross-lingual Contextualized Topic Models with Zero-shot Learning},\n    author={Federico Bianchi and Silvia Terragni and Dirk Hovy and Debora Nozza and Elisabetta Fersini},\n    year={2020},\n   journal={arXiv preprint arXiv:2004.07737},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8934497586185194,
        0.9173774835682452
      ],
      "excerpt": "Cross-lingual Contextualized Topic Models with Zero-shot Learning: https://arxiv.org/pdf/2004.07737v1.pdf \nPre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence: https://arxiv.org/pdf/2004.03974.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aaronmueller/contextualized-topic-models",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-09T01:41:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-18T09:55:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9974789051074822
      ],
      "excerpt": "This respository contains code for replicating the experiments of our NAACL 2021 paper, Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual Neural Topic Modeling. Specifically, this repository contains code for preprocessing input data, the article IDs for the Wikipedia dataset we use in the paper, and the code for TCCTM modeling. This repository is very similar to the original contextualized topic modeling repository, but with the addition of our specific TCCTM model and evaluation code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.950546619338169,
        0.9381700694586352,
        0.9130295266345396
      ],
      "excerpt": "For fine-tuning sentence embeddings, use the sentence-transformers repository. We have included our SBERT training scripts in the sentence-transformers folder, which is structured such that you should be able to copy its contents over the contents of the original sentence-transformers repository. You will need to create topic classification datasets (instructions below) to run training_topics.py. \nContextualized Topic Models (CTMs) are a family of topic models that use pre-trained representations of language (e.g., BERT) to \nsupport topic modeling. See the original CTM papers for details: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8231065948657993
      ],
      "excerpt": "We use a subset of the aligned multilingual Wikipedia Comparable Corpora dataset, which may be found here: https://linguatools.org/tools/corpora/wikipedia-comparable-corpora/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8628534989590873,
        0.8118069050419727,
        0.9887162486965759,
        0.9108551031975947
      ],
      "excerpt": "We create aligned testing sets for cross-lingual evaluation. But if we only train on English, why create an aligned training set? This is to generate language-specific vocabularies that have (ideally 100%, but realistically a bit less) overlap in lexical-semantic content cross-linguistically. \nWe include our preprocessing notebook in examples/preprocessing_wiki.ipynb. Note that to use this, you will first need to generate vocabularies for each language. The vocabularies should be text files where each line contains one token.  We simply took the 5000 most frequent tokens per-language, though the original CTM paper used 2000 tokens per-language. There is also a built-in preprocessing script in the original CTM repository. \nTopic classification is a supervised task proposed in this paper. It is functionally equivalent to document classification, except that the document labels are from a topic model rather than human annotators. We use MalletLDA (as implemented in the gensim wrapper) to topic model our training data, searching over the number of topics by NPMI coherence. Then, we use the topic model with the highest coherence to assign each article a topic. \nThe scripts we use to create the sentence-transformers training data for this task may be found in contextualized_topic_models/data/wiki. Specifically, use the following to topic model the training data: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8425714179688054,
        0.9739179481439748,
        0.9811908748207531
      ],
      "excerpt": "To train a regular CTM, use the model_wiki.py script. This script is currently instantiated with the best hyperparameters we found on the dataset used in our paper. Note that you will need to modify the paths \nTo train a TCCTM, use the model_wiki_topicreg.py script. The primary difference between this and model_wiki.py is that this script uses a new CTMDatasetTopReg data processor, rather than the default CTMDataset; this data processor loads the input data as well as topic labels for each article. The document labels are generated from an LDA topic model. When the CTMDatasetTopReg processor is used, the TCCTM model is automatically used without any further changes needed in the main code. This behavior is defined in the CTM class of the CTM model definition script. \nThe difference between a CTM model and TCCTM model is that the TCCTM contains a topic classifier. The model maps from the hidden representation of the input sentences produced by the VAE to a topic label, using a negative log-likelihood loss. This loss is added to the loss of the topic model. If you do not wish to fine-tune your contextualized sentence embeddings before applying them to monolingual topic modeling, TCCTM achieves similar performance to a CTM with well-tuned sentence embeddings for this task. However, note that if you want good zero-shot cross-lingual topic transfer, you will want to fine-tune your embeddings. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A python package to setup topic classification fine-tuning, run contextualized topic modeling, and run TCCTMs",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "As this repository is forked from a repository which uses the MIT License, we also use the MIT License. You may freely reuse code found here in proprietary software, provided you include the MIT License terms and copyright notice.\n\n* Free software: MIT license\n* Further CTM Documentation: https://contextualized-topic-models.readthedocs.io.\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aaronmueller/contextualized-topic-models/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 16:30:54 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/aaronmueller/contextualized-topic-models/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "aaronmueller/contextualized-topic-models",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/aaronmueller/contextualized-topic-models/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/aaronmueller/contextualized-topic-models/master/examples/multilingual-topic-modeling.ipynb",
      "https://raw.githubusercontent.com/aaronmueller/contextualized-topic-models/master/examples/topic-modeling.ipynb",
      "https://raw.githubusercontent.com/aaronmueller/contextualized-topic-models/master/examples/preprocessing_wiki.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/aaronmueller/contextualized-topic-models/master/sentence-transformers/sentence_transformers/train_mldoc.sh",
      "https://raw.githubusercontent.com/aaronmueller/contextualized-topic-models/master/sentence-transformers/sentence_transformers/train_nli.sh",
      "https://raw.githubusercontent.com/aaronmueller/contextualized-topic-models/master/sentence-transformers/sentence_transformers/train_topics.sh",
      "https://raw.githubusercontent.com/aaronmueller/contextualized-topic-models/master/cpt/continue_pretraining.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8568351164025192
      ],
      "excerpt": "This will save a topic model using the specified number of topics to a .pkl file in the directory in which the script is run. Then, run the following (in the same directory) to obtain a .json file with documents classified by topic: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8825439090514388
      ],
      "excerpt": "This will output a file called topic_full.json. We use the first 80,000 lines of this file to create a training .json, the next 10,000 lines to generate a dev .json, and the final 10,000 lines to generate a test .json. You may then use this dataset to train a sentence-transformers model. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/aaronmueller/contextualized-topic-models/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Makefile",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020, Federico Bianchi\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual Neural Topic Modeling",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "contextualized-topic-models",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "aaronmueller",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aaronmueller/contextualized-topic-models/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Thu, 23 Dec 2021 16:30:54 GMT"
    },
    "technique": "GitHub API"
  }
}