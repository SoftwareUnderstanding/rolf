{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1610.02391 <br>\n`Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\nRamprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra`\n\nhttps://arxiv.org/abs/1710.11063 <br>\n`Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks\nAditya Chattopadhyay, Anirban Sarkar, Prantik Howlader, Vineeth N Balasubramanian`\n\nhttps://arxiv.org/abs/1910.01279 <br>\n`Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks\nHaofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, Xia Hu`\n\nhttps://ieeexplore.ieee.org/abstract/document/9093360/ <br>\n`Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization.\nSaurabh Desai and Harish G Ramaswamy. In WACV, pages 972\u2013980, 2020`\n\nhttps://arxiv.org/abs/2008.02312 <br>\n`Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs\nRuigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao, Biao Li`\n\nhttps://arxiv.org/abs/2008.00299 <br>\n`Eigen-CAM: Class Activation Map using Principal Components\nMohammed Bany Muhammad, Mohammed Yeasin`\n\nhttp://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf <br>\n`LayerCAM: Exploring Hierarchical Class Activation Maps for Localization\nPeng-Tao Jiang; Chang-Bin Zhang; Qibin Hou; Ming-Ming Cheng; Yunchao Wei`\n\nhttps://arxiv.org/abs/1905.00780 <br>\n`Full-Gradient Representation for Neural Network Visualization\nSuraj Srinivas, Francois Fleuret`",
      "https://arxiv.org/abs/1710.11063 <br>\n`Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks\nAditya Chattopadhyay, Anirban Sarkar, Prantik Howlader, Vineeth N Balasubramanian`\n\nhttps://arxiv.org/abs/1910.01279 <br>\n`Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks\nHaofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, Xia Hu`\n\nhttps://ieeexplore.ieee.org/abstract/document/9093360/ <br>\n`Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization.\nSaurabh Desai and Harish G Ramaswamy. In WACV, pages 972\u2013980, 2020`\n\nhttps://arxiv.org/abs/2008.02312 <br>\n`Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs\nRuigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao, Biao Li`\n\nhttps://arxiv.org/abs/2008.00299 <br>\n`Eigen-CAM: Class Activation Map using Principal Components\nMohammed Bany Muhammad, Mohammed Yeasin`\n\nhttp://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf <br>\n`LayerCAM: Exploring Hierarchical Class Activation Maps for Localization\nPeng-Tao Jiang; Chang-Bin Zhang; Qibin Hou; Ming-Ming Cheng; Yunchao Wei`\n\nhttps://arxiv.org/abs/1905.00780 <br>\n`Full-Gradient Representation for Neural Network Visualization\nSuraj Srinivas, Francois Fleuret`",
      "https://arxiv.org/abs/1910.01279 <br>\n`Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks\nHaofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, Xia Hu`\n\nhttps://ieeexplore.ieee.org/abstract/document/9093360/ <br>\n`Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization.\nSaurabh Desai and Harish G Ramaswamy. In WACV, pages 972\u2013980, 2020`\n\nhttps://arxiv.org/abs/2008.02312 <br>\n`Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs\nRuigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao, Biao Li`\n\nhttps://arxiv.org/abs/2008.00299 <br>\n`Eigen-CAM: Class Activation Map using Principal Components\nMohammed Bany Muhammad, Mohammed Yeasin`\n\nhttp://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf <br>\n`LayerCAM: Exploring Hierarchical Class Activation Maps for Localization\nPeng-Tao Jiang; Chang-Bin Zhang; Qibin Hou; Ming-Ming Cheng; Yunchao Wei`\n\nhttps://arxiv.org/abs/1905.00780 <br>\n`Full-Gradient Representation for Neural Network Visualization\nSuraj Srinivas, Francois Fleuret`",
      "https://arxiv.org/abs/2008.02312 <br>\n`Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs\nRuigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao, Biao Li`\n\nhttps://arxiv.org/abs/2008.00299 <br>\n`Eigen-CAM: Class Activation Map using Principal Components\nMohammed Bany Muhammad, Mohammed Yeasin`\n\nhttp://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf <br>\n`LayerCAM: Exploring Hierarchical Class Activation Maps for Localization\nPeng-Tao Jiang; Chang-Bin Zhang; Qibin Hou; Ming-Ming Cheng; Yunchao Wei`\n\nhttps://arxiv.org/abs/1905.00780 <br>\n`Full-Gradient Representation for Neural Network Visualization\nSuraj Srinivas, Francois Fleuret`",
      "https://arxiv.org/abs/2008.00299 <br>\n`Eigen-CAM: Class Activation Map using Principal Components\nMohammed Bany Muhammad, Mohammed Yeasin`\n\nhttp://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf <br>\n`LayerCAM: Exploring Hierarchical Class Activation Maps for Localization\nPeng-Tao Jiang; Chang-Bin Zhang; Qibin Hou; Ming-Ming Cheng; Yunchao Wei`\n\nhttps://arxiv.org/abs/1905.00780 <br>\n`Full-Gradient Representation for Neural Network Visualization\nSuraj Srinivas, Francois Fleuret`",
      "https://arxiv.org/abs/1905.00780 <br>\n`Full-Gradient Representation for Neural Network Visualization\nSuraj Srinivas, Francois Fleuret`"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "https://arxiv.org/abs/1610.02391 <br>\n`Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\nRamprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra`\n\nhttps://arxiv.org/abs/1710.11063 <br>\n`Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks\nAditya Chattopadhyay, Anirban Sarkar, Prantik Howlader, Vineeth N Balasubramanian`\n\nhttps://arxiv.org/abs/1910.01279 <br>\n`Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks\nHaofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, Xia Hu`\n\nhttps://ieeexplore.ieee.org/abstract/document/9093360/ <br>\n`Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization.\nSaurabh Desai and Harish G Ramaswamy. In WACV, pages 972\u2013980, 2020`\n\nhttps://arxiv.org/abs/2008.02312 <br>\n`Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs\nRuigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao, Biao Li`\n\nhttps://arxiv.org/abs/2008.00299 <br>\n`Eigen-CAM: Class Activation Map using Principal Components\nMohammed Bany Muhammad, Mohammed Yeasin`\n\nhttp://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf <br>\n`LayerCAM: Exploring Hierarchical Class Activation Maps for Localization\nPeng-Tao Jiang; Chang-Bin Zhang; Qibin Hou; Ming-Ming Cheng; Yunchao Wei`\n\nhttps://arxiv.org/abs/1905.00780 <br>\n`Full-Gradient Representation for Neural Network Visualization\nSuraj Srinivas, Francois Fleuret`\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this for research, please cite. Here is an example BibTeX entry:\n\n```\n@misc{jacobgilpytorchcam,\n  title={PyTorch library for CAM methods},\n  author={Jacob Gildenblat and contributors},\n  year={2021},\n  publisher={GitHub},\n  howpublished={\\url{https://github.com/jacobgil/pytorch-grad-cam}},\n}\n```\n\n----------\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{jacobgilpytorchcam,\n  title={PyTorch library for CAM methods},\n  author={Jacob Gildenblat and contributors},\n  year={2021},\n  publisher={GitHub},\n  howpublished={\\url{https://github.com/jacobgil/pytorch-grad-cam}},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9453492361951639
      ],
      "excerpt": "\u2b50 Comprehensive collection of Pixel Attribution methods for Computer Vision. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jacobgil/pytorch-grad-cam",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-05-31T19:55:15Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T21:04:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8473574671411658
      ],
      "excerpt": "\u2b50 Tested on many Common CNN Networks and Vision Transformers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9488944324254838
      ],
      "excerpt": "\u2b50 Full support for batches of images in all methods. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8537882709825169
      ],
      "excerpt": "| EigenCAM  | Takes the first principle component of the 2D Activations (no class discrimination, but seems to give great results)| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.892838121524236
      ],
      "excerpt": "| FullGrad  | Computes the gradients of the biases from all over the network, and then sums them | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9391568420276115,
        0.8079223578773941
      ],
      "excerpt": "It seems that GradCAM++ is almost the same as GradCAM, in \nmost networks except VGG where the advantage is larger. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924212690395703
      ],
      "excerpt": "| VGG16    |  | |  |  |   |  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8261417906607024,
        0.9220708893717232,
        0.9763330171521227
      ],
      "excerpt": "Some common choices are: \n- Resnet18 and 50: model.layer4[-1] \n- VGG and densenet161: model.features[-1] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.9013519106584089
      ],
      "excerpt": "target_layers = [model.layer4[-1]] \ninput_tensor = #: Create an input tensor image for your model.. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560546876546713
      ],
      "excerpt": ": with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8101832217064551,
        0.8289822393890457,
        0.9349260263432599
      ],
      "excerpt": ": will be used for every image in the batch. \n: target_category can also be an integer, or a list of different integers \n: for every image in the batch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131396809881262
      ],
      "excerpt": "In ViT the output of the layers are typically BATCH x 197 x 192. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9321077635651654,
        0.9368299278589819
      ],
      "excerpt": "We can treat the last 196 elements as a 14x14 spatial image, with 192 channels. \nTo reshape the activations and gradients to 2D spatial images, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.828171665097927
      ],
      "excerpt": "This can also be a starting point for other architectures that will come in the future. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "GradCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.879647673951545
      ],
      "excerpt": "Since the final classification is done on the class token computed in the last attention block, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8440081648744667
      ],
      "excerpt": "The gradient of the output with respect to them, will be 0! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.898863945890699,
        0.9368299278589819
      ],
      "excerpt": "We can treat the last 49 elements as a 7x7 spatial image, with 1024 channels. \nTo reshape the activations and gradients to 2D spatial images, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.828171665097927
      ],
      "excerpt": "This can also be a starting point for other architectures that will come in the future. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "GradCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9431886435104959
      ],
      "excerpt": "Since the swin transformer is different from ViT, it does not contains cls_token as present in ViT, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Many Class Activation Map methods implemented in Pytorch for CNNs and Vision Transformers. Including Grad-CAM, Grad-CAM++, Score-CAM, Ablation-CAM and XGrad-CAM",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jacobgil/pytorch-grad-cam/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 762,
      "date": "Tue, 21 Dec 2021 21:43:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jacobgil/pytorch-grad-cam/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jacobgil/pytorch-grad-cam",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "![Combined](https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/cam_gb_dog.jpg?raw=true)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.999746712887969
      ],
      "excerpt": "pip install grad-cam \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8092646829622805
      ],
      "excerpt": ": You can also use it within a with statement, to make sure it is freed, \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8801854956928516,
        0.9070429883360203,
        0.8900486270063179,
        0.9024287239548907
      ],
      "excerpt": "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM \nfrom pytorch_grad_cam.utils.image import show_cam_on_image \nfrom torchvision.models import resnet50 \nmodel = resnet50(pretrained=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8167344035225302,
        0.8137291148914628
      ],
      "excerpt": "    result = tensor[:, 1 :  , :].reshape(tensor.size(0), \n        height, width, tensor.size(2)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8102780086823556
      ],
      "excerpt": "result = result.transpose(2, 3).transpose(1, 2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8167344035225302,
        0.8137291148914628
      ],
      "excerpt": "    result = tensor.reshape(tensor.size(0), \n        height, width, tensor.size(2)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8102780086823556
      ],
      "excerpt": "result = result.transpose(2, 3).transpose(1, 2) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jacobgil/pytorch-grad-cam/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Jacob Gildenblat\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Class Activation Map methods implemented in Pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-grad-cam",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jacobgil",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jacobgil/pytorch-grad-cam/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Usage: `python cam.py --image-path <path_to_image> --method <method>`\n\nTo use with CUDA:\n`python cam.py --image-path <path_to_image> --use-cuda`\n\n----------\n\nYou can choose between:\n\n`GradCAM` , `ScoreCAM`, `GradCAMPlusPlus`, `AblationCAM`, `XGradCAM` , `LayerCAM` and `EigenCAM`.\n\nSome methods like ScoreCAM and AblationCAM require a large number of forward passes,\nand have a batched implementation.\n\nYou can control the batch size with\n`cam.batch_size = `\n\n----------\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3835,
      "date": "Tue, 21 Dec 2021 21:43:38 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "pytorch",
      "grad-cam",
      "visualizations",
      "interpretability",
      "interpretable-ai",
      "interpretable-deep-learning",
      "score-cam",
      "class-activation-maps",
      "vision-transformers"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To reduce noise in the CAMs, and make it fit better on the objects,\ntwo smoothing methods are supported:\n\n- `aug_smooth=True`\n\n  Test time augmentation: increases the run time by x6.\n\n  Applies a combination of horizontal flips, and mutiplying the image\n  by [1.0, 1.1, 0.9].\n\n  This has the effect of better centering the CAM around the objects.\n\n\n- `eigen_smooth=True`\n\n  First principle component of `activations*weights`\n\n  This has the effect of removing a lot of noise.\n\n\n|AblationCAM | aug smooth | eigen smooth | aug+eigen smooth|\n|------------|------------|--------------|--------------------|\n![](./examples/nosmooth.jpg) | ![](./examples/augsmooth.jpg) | ![](./examples/eigensmooth.jpg) | ![](./examples/eigenaug.jpg) | \n\n----------\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Usage: `python cam.py --image-path <path_to_image> --method <method>`\n\nTo use with CUDA:\n`python cam.py --image-path <path_to_image> --use-cuda`\n\n----------\n\nYou can choose between:\n\n`GradCAM` , `ScoreCAM`, `GradCAMPlusPlus`, `AblationCAM`, `XGradCAM` , `LayerCAM` and `EigenCAM`.\n\nSome methods like ScoreCAM and AblationCAM require a large number of forward passes,\nand have a batched implementation.\n\nYou can control the batch size with\n`cam.batch_size = `\n\n----------\n\n",
      "technique": "Header extraction"
    }
  ]
}