{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1708.02002",
      "https://arxiv.org/abs/1706.02677",
      "https://arxiv.org/abs/1612.03144"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002).\n  Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Doll\u00e1r.\n  ICCV, 2017.\n- [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677).\n  Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, Kaiming He.\n  June 2017.\n- [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144).\n  Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie.\n  CVPR, 2017.\n- [Deep Residual Learning for Image Recognition](http://arxiv.org/abs/1512.03385).\n  Kaiming He, Xiangyu Zhang, Shaoqing Renm Jian Sun.\n  CVPR, 2016.\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8486140851448399
      ],
      "excerpt": "Fast and accurate single stage object detection with end-to-end GPU optimization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8501823123991106,
        0.8501823123991106
      ],
      "excerpt": "ResNet101FPN | 0.376 | 10 hrs | 22 ms;</br>46 FPS | 27 ms;</br>37 FPS | 13 ms;</br>78 FPS | 9 ms;</br>117 FPS \nResNet152FPN | 0.393 | 12 hrs | 26 ms;</br>38 FPS | 33 ms;</br>31 FPS | 15 ms;</br>66 FPS | 10 ms;</br>103 FPS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778748068942859
      ],
      "excerpt": "Evaluate your detection model on COCO 2017: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NVIDIA/retinanet-examples",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Reporting problems, asking questions\nWe appreciate feedback, questions or bug reports. When you need help with the code, try to follow the process outlined in the Stack Overflow (https://stackoverflow.com/help/mcve) document. \nAt a minimum, your issues should describe the following:\n\nWhat command you ran \nThe hardware and container that you are using\nThe version of ODTK you are using\nWhat was the result you observed\nWhat was the result you expected",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-14T21:26:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-25T13:05:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "ODTK is a single shot object detector with various backbones and detection heads. This allows performance/accuracy trade-offs.\n\nIt is optimized for end-to-end GPU processing using:\n* The [PyTorch](https://pytorch.org) deep learning framework with [ONNX](https://onnx.ai) support\n* NVIDIA [Apex](https://github.com/NVIDIA/apex) for mixed precision and distributed training\n* NVIDIA [DALI](https://github.com/NVIDIA/DALI) for optimized data pre-processing\n* NVIDIA [TensorRT](https://developer.nvidia.com/tensorrt) for high-performance inference\n* NVIDIA [DeepStream](https://developer.nvidia.com/deepstream-sdk) for optimized real-time video streams support\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8277607962581505,
        0.804180432459249
      ],
      "excerpt": "Fast and accurate single stage object detection with end-to-end GPU optimization. \nThis repo now supports rotated bounding box detections. See rotated detections training and rotated detections inference documents for more information on how to use the --rotated-bbox command.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9149239174659963,
        0.9254786456346326,
        0.9295015359504221
      ],
      "excerpt": "The detection pipeline allows the user to select a specific backbone depending on the latency-accuracy trade-off preferred. \nODTK RetinaNet model accuracy and inference latency & FPS (frames per seconds) for COCO 2017 (train/val) after full training schedule. Inference results include bounding boxes post-processing for a batch size of 1. Inference measured at --resize 800 using --with-dali on a FP16 TensorRT engine. \nBackbone |  mAP @[IoU=0.50:0.95] | Training Time on DGX1v | Inference latency FP16 on V100 | Inference latency INT8 on T4 | Inference latency FP16 on A100 | Inference latency INT8 on A100 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8186330619067844
      ],
      "excerpt": "Fine-tune a pre-trained model on your dataset. In the example below we use Pascal VOC with JSON annotations: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8802178430417705
      ],
      "excerpt": "For faster inference, export the detection model to an optimized FP16 TensorRT engine: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8610284908872664
      ],
      "excerpt": "Evaluate the model with TensorRT backend on COCO 2017: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8563042971391165
      ],
      "excerpt": "For even faster inference, do INT8 calibration to create an optimized INT8 TensorRT engine: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8763122799935859
      ],
      "excerpt": "This will create an INT8CalibrationTable file that can be used to create INT8 TensorRT engines for the same model later on without needing to do calibration. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8703661319230357
      ],
      "excerpt": "RetinaNet supports annotations in the COCO JSON format. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8009512079859675
      ],
      "excerpt": "        \"bbox\" : [x, y, w, h]   #: all floats \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8398914599988863,
        0.9557100568005175
      ],
      "excerpt": "If using the --rotated-bbox flag for rotated detections, add an additional float theta to the annotations. To get validation scores you also need to fill the segmentation section. \n\"bbox\" : [x, y, w, h, theta]    #: all floats, where theta is measured in radians anti-clockwise from the x-axis. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9270004155733109
      ],
      "excerpt": "This is a research project, not an official NVIDIA product. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Fast and accurate object detection with end-to-end GPU optimization",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NVIDIA/retinanet-examples/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 245,
      "date": "Sat, 25 Dec 2021 19:45:48 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/NVIDIA/retinanet-examples/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "NVIDIA/retinanet-examples",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/NVIDIA/retinanet-examples/main/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/NVIDIA/retinanet-examples/main/extras/test.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For best performance, use the latest [PyTorch NGC docker container](https://ngc.nvidia.com/catalog/containers/nvidia:pytorch). Clone this repository, build and run your own image:\n\n```bash\ngit clone https://github.com/nvidia/retinanet-examples\ndocker build -t odtk:latest retinanet-examples/\ndocker run --gpus all --rm --ipc=host -it odtk:latest\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8992468410715686
      ],
      "excerpt": "When converting the annotations from your own dataset into JSON, the following entries are required: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9217839524503798
      ],
      "excerpt": "        \"iscrowd\": 0            #: Required for validation scores \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9217839524503798
      ],
      "excerpt": "                                        #: Required for validation scores. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8116931946925107
      ],
      "excerpt": "ResNet18FPN | 0.318 | 5 hrs  | 14 ms;</br>71 FPS | 18 ms;</br>56 FPS | 9 ms;</br>110 FPS | 7 ms;</br>141 FPS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.867690944385,
        0.8265895612026185,
        0.8122982156598066,
        0.8554064085506395
      ],
      "excerpt": "ResNet34FPN | 0.343 | 6 hrs  | 16 ms;</br>64 FPS | 20 ms;</br>50 FPS | 10 ms;</br>103 FPS | 7 ms;</br>142 FPS \nResNet50FPN | 0.358 | 7 hrs  | 18 ms;</br>56 FPS | 22 ms;</br>45 FPS | 11 ms;</br>93 FPS | 8 ms;</br>129 FPS \nResNet101FPN | 0.376 | 10 hrs | 22 ms;</br>46 FPS | 27 ms;</br>37 FPS | 13 ms;</br>78 FPS | 9 ms;</br>117 FPS \nResNet152FPN | 0.393 | 12 hrs | 26 ms;</br>38 FPS | 33 ms;</br>31 FPS | 15 ms;</br>66 FPS | 10 ms;</br>103 FPS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8218418111077602
      ],
      "excerpt": "Fine-tune a pre-trained model on your dataset. In the example below we use Pascal VOC with JSON annotations: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9196864414779545
      ],
      "excerpt": "odtk infer retinanet_rn50fpn.pth --images /dataset/val --output detections.json \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/NVIDIA/retinanet-examples/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "Cuda",
      "CMake",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b\"Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions\\nare met:\\n * Redistributions of source code must retain the above copyright\\n   notice, this list of conditions and the following disclaimer.\\n * Redistributions in binary form must reproduce the above copyright\\n   notice, this list of conditions and the following disclaimer in the\\n   documentation and/or other materials provided with the distribution.\\n * Neither the name of NVIDIA CORPORATION nor the names of its\\n   contributors may be used to endorse or promote products derived\\n   from this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\\nEXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\\nPURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\\nOF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\"",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "NVIDIA Object Detection Toolkit (ODTK)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "retinanet-examples",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "NVIDIA",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NVIDIA/retinanet-examples/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "yashnv",
        "body": "### Added\r\n* `--dynamic-batch-opts` option to `odtk export`.\r\n  * This parameter allows you to provide TensorRT Optimiation Profile batch sizes for engine export (min, opt, max).\r\n\r\n### Changed\r\n* Updated TensorRT plugins to allow for dynamic batch sizes (see https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work_dynamic_shapes and https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_plugin_v2_dynamic_ext.html).",
        "dateCreated": "2020-06-28T03:19:11Z",
        "datePublished": "2020-06-28T03:23:11Z",
        "html_url": "https://github.com/NVIDIA/retinanet-examples/releases/tag/v0.2.5",
        "name": "Adds dynamic batch support for TensorRT 7.1",
        "tag_name": "v0.2.5",
        "tarball_url": "https://api.github.com/repos/NVIDIA/retinanet-examples/tarball/v0.2.5",
        "url": "https://api.github.com/repos/NVIDIA/retinanet-examples/releases/27994750",
        "zipball_url": "https://api.github.com/repos/NVIDIA/retinanet-examples/zipball/v0.2.5"
      },
      {
        "authorType": "User",
        "author_name": "james-nvidia",
        "body": "This release adds:\r\n\r\n* `MobileNetV2FPN` backbone\r\n* Rotated bounding box detections models can now be exported to ONNX and TensorRT using `odtk export model.pth model.plan --rotated-bbox`\r\n* The `--rotated-bbox` flag is automatically applied when running odtk infer or odtk export on a model trained with ODTK version 0.2.2 or later.",
        "dateCreated": "2020-06-28T03:19:11Z",
        "datePublished": "2020-06-28T03:23:33Z",
        "html_url": "https://github.com/NVIDIA/retinanet-examples/releases/tag/v0.2.3",
        "name": "Adds MobileNetV2FPN and TensorRT support for rotated detections",
        "tag_name": "v0.2.3",
        "tarball_url": "https://api.github.com/repos/NVIDIA/retinanet-examples/tarball/v0.2.3",
        "url": "https://api.github.com/repos/NVIDIA/retinanet-examples/releases/25689793",
        "zipball_url": "https://api.github.com/repos/NVIDIA/retinanet-examples/zipball/v0.2.3"
      },
      {
        "authorType": "User",
        "author_name": "james-nvidia",
        "body": "Version 0.2.0 introduces rotated detections.\r\n\r\n### Added\r\n* `train arguments`:\r\n  * `--rotated-bbox`: Trains a model is predict rotated bounding boxes `[x, y, w, h, theta]` instead of axis aligned boxes `[x, y, w, h]`.\r\n* `infer arguments`:\r\n  * `--rotated-bbox`: Infer a rotated model.\r\n\r\n### Changed\r\nThe project has reverted to the name **Object Detection Toolkit** (ODTK), to better reflect the multi-network nature of the repo.\r\n* `retinanet` has been replaced with `odtk`. All subcommands remain the same. \r\n\r\n### Limitations\r\n* Models trained using the `--rotated-bbox` flag cannot be exported to ONNX or a TensorRT Engine.\r\n* PyTorch raises two warnings which can be ignored:\r\n\r\nWarning 1: NCCL watchdog\r\n```\r\n[E ProcessGroupNCCL.cpp:284] NCCL watchdog thread terminated\r\n```\r\n\r\nWarning 2: Save state warning\r\n```\r\n/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:201: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\r\n  warnings.warn(SAVE_STATE_WARNING, UserWarning)\r\n```",
        "dateCreated": "2020-03-12T23:17:21Z",
        "datePublished": "2020-03-12T23:25:08Z",
        "html_url": "https://github.com/NVIDIA/retinanet-examples/releases/tag/v0.2.0",
        "name": "Rotated detections",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/NVIDIA/retinanet-examples/tarball/v0.2.0",
        "url": "https://api.github.com/repos/NVIDIA/retinanet-examples/releases/24484146",
        "zipball_url": "https://api.github.com/repos/NVIDIA/retinanet-examples/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "james-nvidia",
        "body": "This release adds image augmentation (brightness, contrast, hue, saturation) and four degree rotational augmentation.\r\n\r\nAdded parameters:\r\n* `--augment-rotate`: Randomly rotates the training images by 0\u00b0, 90\u00b0, 180\u00b0 or 270\u00b0.\r\n* `--augment-brightness` (float): Randomly adjusts brightness of image. The value sets the standard deviation of a Gaussian distribution. The degree of augmentation is selected from this distribution. Default: 0.05\r\n* `--augment-contrast` (float): Randomly adjusts contrast of image. The value sets the standard deviation of a Gaussian distribution. The degree of augmentation is selected from this distribution. Default: 0.05\r\n* `--augment-hue` (float): Randomly adjusts hue of image. The value sets the standard deviation of a Gaussian distribution. The degree of augmentation is selected from this distribution. Default: 0.01\r\n* `--augment-saturation` (float): Randomly adjusts saturation of image. The value sets the standard deviation of a Gaussian distribution. The degree of augmentation is selected from this distribution. Default: 0.05\r\n* `--regularization-l2` (float): Sets the L2 regularization of the optimizer. Default: 0.0001",
        "dateCreated": "2020-03-12T23:17:21Z",
        "datePublished": "2020-03-12T23:24:59Z",
        "html_url": "https://github.com/NVIDIA/retinanet-examples/releases/tag/v0.1.1",
        "name": "Augmentation",
        "tag_name": "v0.1.1",
        "tarball_url": "https://api.github.com/repos/NVIDIA/retinanet-examples/tarball/v0.1.1",
        "url": "https://api.github.com/repos/NVIDIA/retinanet-examples/releases/24364853",
        "zipball_url": "https://api.github.com/repos/NVIDIA/retinanet-examples/zipball/v0.1.1"
      },
      {
        "authorType": "User",
        "author_name": "pkashinkunti",
        "body": "This pre-release, corresponding with the NVIDIA GPU Cloud (NGC) PyTorch 19.04 container version, includes the first iteration of pretrained RetinaNet models created with this project:\r\n* ResNet18FPN backbone\r\n* ResNet34FPN backbone\r\n* ResNet50FPN backbone\r\n* ResNet101FPN backbone\r\n* ResNet152FPN backbone\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "dateCreated": "2019-05-23T17:31:26Z",
        "datePublished": "2019-05-31T22:56:55Z",
        "html_url": "https://github.com/NVIDIA/retinanet-examples/releases/tag/19.04",
        "name": "retinanet-examples 19.04",
        "tag_name": "19.04",
        "tarball_url": "https://api.github.com/repos/NVIDIA/retinanet-examples/tarball/19.04",
        "url": "https://api.github.com/repos/NVIDIA/retinanet-examples/releases/17717366",
        "zipball_url": "https://api.github.com/repos/NVIDIA/retinanet-examples/zipball/19.04"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 774,
      "date": "Sat, 25 Dec 2021 19:45:48 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "object-detection",
      "python",
      "neural-network",
      "retinanet",
      "pytorch",
      "tensorrt"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Training, inference, evaluation and model export can be done through the `odtk` utility. \nFor more details, including a list of parameters, please refer to the [TRAINING](TRAINING.md) and [INFERENCE](INFERENCE.md) documentation.\n\n",
      "technique": "Header extraction"
    }
  ]
}