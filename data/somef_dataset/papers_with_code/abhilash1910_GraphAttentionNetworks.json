{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1710.10903"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Epoch 10/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Epoch 12/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Epoch 30/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Epoch 37/200 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/abhilash1910/GraphAttentionNetworks",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-21T20:28:18Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-11T03:35:29Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9793184566646204,
        0.9728927845890524
      ],
      "excerpt": "This package is used for extracting Graph Attention Embeddings and provides a framework for a Tensorflow Graph Attention Layer which can be used for knowledge graph /node base semantic tasks. It determines the pair wise embedding matrix for a higher order node representation and concatenates them with an attention weight. It then passes it through a leakyrelu activation for importance sampling and damps out negative effect of a node.It then applies a softmax layer for normalization of the attention results and determines the final output scores.The GraphAttentionBase.py script implements a Tensorflow/Keras Layer for the GAT which can be used and the GraphMultiheadAttention.py is used to extract GAT embeddings. \nThis is a TensorFlow 2 implementation of Graph Attention Networks for generating node embeddings for Knowledge Graphs as well as for implementing a keras layer for Multihead Graph Attention from the paper, Graph Attention Networks (Veli\u010dkovi\u0107 et al., ICLR 2018). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.965544612104956
      ],
      "excerpt": "This library is built with Tensorflow: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9653336665072386
      ],
      "excerpt": "Create a function to read the input csv file. The input should contain atleast 2 columns - source and target(labels). And both should be in text format. These can include textual extracts and their corresponding labels. The graph is then created as a MultiDigraph from [networkx] with the target and source columns from the input csv file. While generating the embeddings, the extracts from the labels are also considered and can be used to determine which label is the closest to the provided source(input text). In the example below, the 'test_gat_embeddings' method shows this. The dataset chosen for this demonstration is Google Quest QnA and as such any dataset having a source and a label column(textual contents) can be used to generate the embeddings. The  method requires the get_gat_embeddings method.This method takes as parameters: hidden_units (denotes the hidden embedding dimension of the neural network), num_heads(number of attention heads), epochs (number of training iterations),num_layers(number of layers for the network),mode(defaults to averaging mode attention, for concatenation see GraphAttentionBase.py), the dataframe along with the source and target labels. The model outputs a embedding matrix (no of entries, no of hidden dims) and the corresponding graph.The dimensions are internally reduced to suit the output of the GAT embeddings. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9919417398455209,
        0.9756284908864592,
        0.9680472148625069,
        0.929930384785836,
        0.9671101197034129
      ],
      "excerpt": "Neural GCN Multiplication: In order to obtain sufficient expressive power to transform the input features into higher level features, atleast one learnable linear transformation is required. To that end, as an initial step, a shared linear transformation, parametrized by a weight matrix, W\u2208RF'\u00d7F , is applied to every node. \nSelf-Attention Pointwise: We then compute a pair-wise un-normalized attention score between two neighbors. Here, it first concatenates the z embeddings of the two nodes, where || denotes concatenation, then takes a dot product of it with a learnable weight vector  and applies a LeakyReLU in the end. This form of attention is usually called additive attention, in contrast with the dot-product attention used for the Transformer model. We then perform self-attention on the nodes, a shared attentional mechanism a : RF'\u00d7RF'\u2192R to compute attention coefficients  \nSoftmax Aggregation: In this case we are applying a softmax kernel on the attention scores (normalized) and then multiplying it with the feature map. The aggregation map can be concatenation or avergaing.This is the case for multihead attention. If we perform multi-head attention on the final (prediction) layer of the network, concatenation is no longer sensible and instead, averaging is employed, and delay applying the final nonlinearity (usually a softmax or logistic sigmoid for classification problems).  \nThe GraphAttenionBase.py implements the core GAT Multihead algorithm with both concatenation and aggregation variation. The returned output is of dimensions -> [batch size, number of nodes, labels] \nFor GCN embeddings please refer to the repository:GCN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8390953026424354
      ],
      "excerpt": "Input parameters are hidden units , number of layers,subset (values of entries to be considered for embeddings),epochs  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8086752552501011,
        0.860059181823877
      ],
      "excerpt": "shape of target (39, 5) \nModel: \"model_50\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8067430640478228
      ],
      "excerpt": "Fitting model with {hidden_units} units \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806279299360857,
        0.806279299360857,
        0.806279299360857,
        0.806279299360857
      ],
      "excerpt": "WARNING:tensorflow:Gradients do not exist for variables ['multihead_attention_31/graph_attention_269/kernel_W:0', 'multihead_attention_31/graph_attention_269/bias:0', 'multihead_attention_31/graph_attention_270/kernel_W:0', 'multihead_attention_31/graph_attention_270/bias:0', 'multihead_attention_31/graph_attention_271/kernel_W:0', 'multihead_attention_31/graph_attention_271/bias:0', 'multihead_attention_31/graph_attention_272/kernel_W:0', 'multihead_attention_31/graph_attention_272/bias:0', 'multihead_attention_31/graph_attention_273/kernel_W:0', 'multihead_attention_31/graph_attention_273/bias:0', 'multihead_attention_31/graph_attention_274/kernel_W:0', 'multihead_attention_31/graph_attention_274/bias:0', 'multihead_attention_31/graph_attention_275/kernel_W:0', 'multihead_attention_31/graph_attention_275/bias:0', 'multihead_attention_31/graph_attention_276/kernel_W:0', 'multihead_attention_31/graph_attention_276/bias:0'] when minimizing the loss. \nWARNING:tensorflow:Gradients do not exist for variables ['multihead_attention_31/graph_attention_269/kernel_W:0', 'multihead_attention_31/graph_attention_269/bias:0', 'multihead_attention_31/graph_attention_270/kernel_W:0', 'multihead_attention_31/graph_attention_270/bias:0', 'multihead_attention_31/graph_attention_271/kernel_W:0', 'multihead_attention_31/graph_attention_271/bias:0', 'multihead_attention_31/graph_attention_272/kernel_W:0', 'multihead_attention_31/graph_attention_272/bias:0', 'multihead_attention_31/graph_attention_273/kernel_W:0', 'multihead_attention_31/graph_attention_273/bias:0', 'multihead_attention_31/graph_attention_274/kernel_W:0', 'multihead_attention_31/graph_attention_274/bias:0', 'multihead_attention_31/graph_attention_275/kernel_W:0', 'multihead_attention_31/graph_attention_275/bias:0', 'multihead_attention_31/graph_attention_276/kernel_W:0', 'multihead_attention_31/graph_attention_276/bias:0'] when minimizing the loss. \nWARNING:tensorflow:Gradients do not exist for variables ['multihead_attention_31/graph_attention_269/kernel_W:0', 'multihead_attention_31/graph_attention_269/bias:0', 'multihead_attention_31/graph_attention_270/kernel_W:0', 'multihead_attention_31/graph_attention_270/bias:0', 'multihead_attention_31/graph_attention_271/kernel_W:0', 'multihead_attention_31/graph_attention_271/bias:0', 'multihead_attention_31/graph_attention_272/kernel_W:0', 'multihead_attention_31/graph_attention_272/bias:0', 'multihead_attention_31/graph_attention_273/kernel_W:0', 'multihead_attention_31/graph_attention_273/bias:0', 'multihead_attention_31/graph_attention_274/kernel_W:0', 'multihead_attention_31/graph_attention_274/bias:0', 'multihead_attention_31/graph_attention_275/kernel_W:0', 'multihead_attention_31/graph_attention_275/bias:0', 'multihead_attention_31/graph_attention_276/kernel_W:0', 'multihead_attention_31/graph_attention_276/bias:0'] when minimizing the loss. \nWARNING:tensorflow:Gradients do not exist for variables ['multihead_attention_31/graph_attention_269/kernel_W:0', 'multihead_attention_31/graph_attention_269/bias:0', 'multihead_attention_31/graph_attention_270/kernel_W:0', 'multihead_attention_31/graph_attention_270/bias:0', 'multihead_attention_31/graph_attention_271/kernel_W:0', 'multihead_attention_31/graph_attention_271/bias:0', 'multihead_attention_31/graph_attention_272/kernel_W:0', 'multihead_attention_31/graph_attention_272/bias:0', 'multihead_attention_31/graph_attention_273/kernel_W:0', 'multihead_attention_31/graph_attention_273/bias:0', 'multihead_attention_31/graph_attention_274/kernel_W:0', 'multihead_attention_31/graph_attention_274/bias:0', 'multihead_attention_31/graph_attention_275/kernel_W:0', 'multihead_attention_31/graph_attention_275/bias:0', 'multihead_attention_31/graph_attention_276/kernel_W:0', 'multihead_attention_31/graph_attention_276/bias:0'] when minimizing the loss. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8394137182808135
      ],
      "excerpt": "Dimensions of embeddings (39, 5) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This package is a Tensorflow2/Keras implementation for Graph Attention Network embeddings and also provides a Trainable layer for Multihead Graph Attention.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/abhilash1910/GraphAttentionNetworks/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 15:15:43 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/abhilash1910/GraphAttentionNetworks/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "abhilash1910/GraphAttentionNetworks",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9192548315505698,
        0.999746712887969
      ],
      "excerpt": "Installation is carried out using the pip command as follows: \npip install GraphAttentionNetworks==0.1 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8123744059238021
      ],
      "excerpt": "The steps for generating Graph Attention Embeddings requires import of GraphMultiheadAttention.py script. An example is shown in the test_Script.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8897107595245954
      ],
      "excerpt": "    train_df=pd.read_csv(\"E:\\train_graph\\train.csv\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8074576745807105
      ],
      "excerpt": "    print(\"Input parameters are hidden units , number of layers,subset (values of entries to be considered for embeddings),epochs \") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9312650322969277
      ],
      "excerpt": "    print(gat_emb.shape) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.856989540636594
      ],
      "excerpt": "Layer (type)                    Output Shape         Param #:     Connected to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8264799765521145
      ],
      "excerpt": "first (InputLayer)              [(None, 39)]         0                                             \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8473008171669779
      ],
      "excerpt": "                                                                 first[0][0]                     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8774600314586265
      ],
      "excerpt": "Total params: 6,241 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8246920233215528
      ],
      "excerpt": "Epoch 10/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8503319398749835
      ],
      "excerpt": "Epoch 15/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8045519302510645
      ],
      "excerpt": "Epoch 20/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8347145294736125
      ],
      "excerpt": "Epoch 23/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8173362059188815
      ],
      "excerpt": "Epoch 25/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.832953711710991
      ],
      "excerpt": "Epoch 34/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8774993820378052
      ],
      "excerpt": "Epoch 45/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8404825610888312
      ],
      "excerpt": "Epoch 67/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8015320654164421
      ],
      "excerpt": "Epoch 75/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8151306458292187
      ],
      "excerpt": "Epoch 80/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8031973169795594
      ],
      "excerpt": "Epoch 90/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8331418080712036
      ],
      "excerpt": "Epoch 100/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8183691710873832
      ],
      "excerpt": "Epoch 105/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8183691710873832
      ],
      "excerpt": "Epoch 110/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8631021378956488
      ],
      "excerpt": "Epoch 123/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8049033782878816
      ],
      "excerpt": "Epoch 127/200 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8304343258856914
      ],
      "excerpt": "Epoch 128/200 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/abhilash1910/GraphAttentionNetworks/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# GraphAttentionNetworks\n\n\n## A framework implementation of Graph Attention Networks :robot:\n\n![img1](imgs/GAT.jpg)\n\n[This package](https://pypi.org/project/GraphAttentionNetworks/0.1/) is used for extracting Graph Attention Embeddings and provides a framework for a Tensorflow Graph Attention Layer which can be used for knowledge graph /node base semantic tasks. It determines the pair wise embedding matrix for a higher order node representation and concatenates them with an attention weight. It then passes it through a leakyrelu activation for importance sampling and damps out negative effect of a node.It then applies a softmax layer for normalization of the attention results and determines the final output scores.The GraphAttentionBase.py script implements a Tensorflow/Keras Layer for the GAT which can be used and the GraphMultiheadAttention.py is used to extract GAT embeddings.\n\nThis is a TensorFlow 2 implementation of Graph Attention Networks for generating node embeddings for Knowledge Graphs as well as for implementing a keras layer for Multihead Graph Attention from the paper, [Graph Attention Networks (Veli\u010dkovi\u0107 et al., ICLR 2018)](https://arxiv.org/abs/1710.10903).\n\n\n## Dependencies\n\n<a href=\"https://www.tensorflow.org/\">Tensorflow</a>\n\n\n<a href=\"https://networkx.org/\">Networkx</a>\n\n\n<a href=\"https://scipy.org/\">scipy</a>\n\n\n<a href=\"https://scikit-learn.org/stable/\">sklearn</a>\n\n\n\n## Usability\n\nInstallation is carried out using the pip command as follows:\n\n```python\npip install GraphAttentionNetworks==0.1\n```\n\nThis library is built with Tensorflow:\n\n<img src=\"https://media.wired.com/photos/5955aeeead90646d424bb349/master/pass/google-tensor-flow-logo-black-S.jpg\">\n\nThe steps for generating Graph Attention Embeddings requires import of [GraphMultiheadAttention.py](https://github.com/abhilash1910/GraphAttentionNetworks/blob/master/GraphAttentionNetworks/GraphMultiheadAttention.py) script. An example is shown in the [test_Script.py](https://github.com/abhilash1910/GraphAttentionNetworks/blob/master/test_script.py)\n\nCreate a function to read the input csv file. The input should contain atleast 2 columns - source and target(labels). And both should be in text format. These can include textual extracts and their corresponding labels. The graph is then created as a MultiDigraph from [networkx] with the target and source columns from the input csv file. While generating the embeddings, the extracts from the labels are also considered and can be used to determine which label is the closest to the provided source(input text). In the example below, the 'test_gat_embeddings' method shows this. The dataset chosen for this demonstration is [Google Quest QnA](https://www.kaggle.com/c/google-quest-challenge) and as such any dataset having a source and a label column(textual contents) can be used to generate the embeddings. The  method requires the ```get_gat_embeddings``` method.This method takes as parameters: hidden_units (denotes the hidden embedding dimension of the neural network), num_heads(number of attention heads), epochs (number of training iterations),num_layers(number of layers for the network),mode(defaults to averaging mode attention, for concatenation see ```GraphAttentionBase.py```), the dataframe along with the source and target labels. The model outputs a embedding matrix (no of entries, no of hidden dims) and the corresponding graph.The dimensions are internally reduced to suit the output of the GAT embeddings.\n\n\n```python\ndef test_gat_embeddings():\n    print(\"Testing for VanillaGCN embeddings having a source and target label\")\n    train_df=pd.read_csv(\"E:\\\\train_graph\\\\train.csv\")\n    source_label='question_body'\n    target_label='category'\n    print(\"Input parameters are hidden units , number of layers,subset (values of entries to be considered for embeddings),epochs \")\n    hidden_units=32\n    num_layers=4\n    subset=34\n    epochs=40\n    num_heads=8\n    mode='concat'\n    gat_emb,gat_graph=gat.get_gat_embeddings(hidden_units,train_df,source_label,target_label,epochs,num_layers,num_heads,mode,subset)\n    print(gat_emb.shape)\n    return gat_emb,gat_graph\n\n```\n\n### Theory\n\n<img src=\"https://dsgiitr.com/images/blogs/GAT/GCN_vs_GAT.jpg\">\n\n\n- Neural GCN Multiplication: In order to obtain sufficient expressive power to transform the input features into higher level features, atleast one learnable linear transformation is required. To that end, as an initial step, a shared linear transformation, parametrized by a weight matrix, W\u2208RF'\u00d7F , is applied to every node.\n\n- Self-Attention Pointwise: We then compute a pair-wise un-normalized attention score between two neighbors. Here, it first concatenates the z embeddings of the two nodes, where || denotes concatenation, then takes a dot product of it with a learnable weight vector  and applies a LeakyReLU in the end. This form of attention is usually called additive attention, in contrast with the dot-product attention used for the Transformer model. We then perform self-attention on the nodes, a shared attentional mechanism a : RF'\u00d7RF'\u2192R to compute attention coefficients \n\n- Softmax Aggregation: In this case we are applying a softmax kernel on the attention scores (normalized) and then multiplying it with the feature map. The aggregation map can be concatenation or avergaing.This is the case for multihead attention. If we perform multi-head attention on the final (prediction) layer of the network, concatenation is no longer sensible and instead, averaging is employed, and delay applying the final nonlinearity (usually a softmax or logistic sigmoid for classification problems). \n\nThe [GraphAttenionBase.py](https://github.com/abhilash1910/GraphAttentionNetworks/blob/master/GraphAttentionNetworks/GraphAttentionBase.py) implements the core GAT Multihead algorithm with both concatenation and aggregation variation. The returned output is of dimensions -> [batch size, number of nodes, labels]\n\nFor GCN embeddings please refer to the repository:[GCN](https://github.com/abhilash1910/SpectralEmbeddings)\n\n### Test logs:\n\n```Testing for GAT embeddings having a source and target label\nInput parameters are hidden units , number of layers,subset (values of entries to be considered for embeddings),epochs \nadj (39, 39)\nshape of target (39, 5)\nModel: \"model_50\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "GraphAttentionNetworks",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "abhilash1910",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/abhilash1910/GraphAttentionNetworks/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "abhilash1910",
        "body": "Highlights:\r\n\r\n- Provides a Tensorflow 2/Keras Layer implementation of Graph Attention Layer (Multihead) \r\n- Provides an implementation for extraction Graph Attention Network Embeddings\r\n- First release of GAT backend with TF2",
        "dateCreated": "2021-09-23T09:16:03Z",
        "datePublished": "2021-09-23T11:38:23Z",
        "html_url": "https://github.com/abhilash1910/GraphAttentionNetworks/releases/tag/v_01",
        "name": "First release of GAN",
        "tag_name": "v_01",
        "tarball_url": "https://api.github.com/repos/abhilash1910/GraphAttentionNetworks/tarball/v_01",
        "url": "https://api.github.com/repos/abhilash1910/GraphAttentionNetworks/releases/50125999",
        "zipball_url": "https://api.github.com/repos/abhilash1910/GraphAttentionNetworks/zipball/v_01"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<a href=\"https://www.tensorflow.org/\">Tensorflow</a>\n\n\n<a href=\"https://networkx.org/\">Networkx</a>\n\n\n<a href=\"https://scipy.org/\">scipy</a>\n\n\n<a href=\"https://scikit-learn.org/stable/\">sklearn</a>\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Thu, 23 Dec 2021 15:15:43 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tf2",
      "graph-attention-networks",
      "multihead-attention",
      "self-attention",
      "leaky-relu",
      "keras-tensorflow"
    ],
    "technique": "GitHub API"
  }
}