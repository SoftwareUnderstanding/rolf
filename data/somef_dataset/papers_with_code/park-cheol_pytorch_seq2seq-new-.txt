# Seq2Seq(without / with attention)
## Model
![Screenshot from 2021-07-02 21-21-23](https://user-images.githubusercontent.com/76771847/124273844-7ed0b700-db7b-11eb-9e73-cf54870fb488.png)

## Paper
https://arxiv.org/pdf/1409.3215.pdf: seq2seq learning with nn 

https://arxiv.org/pdf/1506.05869.pdf: a neural conversational mdoel

https://arxiv.org/pdf/1406.1078.pdf: Learning Phrase Representations using RNN Encoder–Decoder
for Statistical Machine Translation

## Reference
https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html

*제일많이참고https://github.com/astorfi/sequence-to-sequence-from-scratch/tree/3e4660a6a2e26293657e1cdd528516887cf9e0d2

https://github.com/littleflow3r/seq2seq-learning-for-machine-translation/blob/master/main.py

https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Sequence_to_Sequence_with_LSTM_Tutorial.ipynb

## dataset
http://www.manythings.org/anki/

AIHUB

