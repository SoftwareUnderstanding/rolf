{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nIf you use the techniques introduced in the paper or the code in this repository for your research, please cite the paper:\r\n```\r\n@InProceedings{Brouwer_2020_NeurIPS_Workshops},\r\n    author = {Brouwer, Hans},\r\n    title = {Audio-reactive Latent Interpolations with StyleGAN},\r\n    booktitle = {Proceedings of the 4th Workshop on Machine Learning for Creativity and Design at NeurIPS 2020},\r\n    month = {December},\r\n    year = {2020},\r\n    url={https://jcbrouwer.github.io/assets/audio-reactive-stylegan/paper.pdf}\r\n}\r\n```\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{Brouwer_2020_NeurIPS_Workshops},\n    author = {Brouwer, Hans},\n    title = {Audio-reactive Latent Interpolations with StyleGAN},\n    booktitle = {Proceedings of the 4th Workshop on Machine Learning for Creativity and Design at NeurIPS 2020},\n    month = {December},\n    year = {2020},\n    url={https://jcbrouwer.github.io/assets/audio-reactive-stylegan/paper.pdf}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8170993371344485,
        0.842790493796475
      ],
      "excerpt": "  --out_size OUT_SIZE                      #: ouput video size: [512, 1024, or 1920] \n  --fps FPS                                #: output video framerate \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JCBrouwer/maua-stylegan2",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-18T15:08:45Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T12:44:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.980075826790533,
        0.9657370156740632,
        0.9565240148213036,
        0.8559636663193722
      ],
      "excerpt": "This is the repo for my experiments with StyleGAN2. There are many like it, but this one is mine. \nIt contains the code for Audio-reactive Latent Interpolations with StyleGAN for the NeurIPS 2020 Workshop on Machine Learning for Creativity and Design. \nThe original base is Kim Seonghyeon's excellent implementation, but I've gathered code from multiple different repositories or other places online and hacked/grafted it all together. License information for the code should all be in the LICENSE folder, but if you find anything missing or incorrect please let me know and I'll fix it immediately. Tread carefully when trying to distribute any code from this repo, it's meant for research and demonstration. \nThe files/folders of interest and their purpose are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9103976036719195
      ],
      "excerpt": "| select_latents.py | GUI for selecting latents, left click to add to top set, right click to add to bottom \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9883984920243942,
        0.9805376515099179
      ],
      "excerpt": "The rest of the code is experimental, probably broken, and unsupported. \nThe simplest way to get started is to try either (in shell): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8029469003851454
      ],
      "excerpt": "or (in e.g. a jupyter notebook): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9245705187162206
      ],
      "excerpt": "To customize the generated interpolation, more functions can be defined to generate latents, noise, network bends, model rewrites, and truncation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.94456026577551,
        0.9827685920290873
      ],
      "excerpt": "One important thing to note is that the outputs of the functions must adhere strictly to the expected formats.  \nEach of the functions is called with all of the arguments from the command line (or generate()) in the args variable. On top of the arguments, args also contains: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.810385849632316
      ],
      "excerpt": "    #: intialize values used in multiple of the following functions here \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8998148105925168
      ],
      "excerpt": "    #: this is useful to prevent duplicate computations (get_noise is called for each noise size) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9120555515168769
      ],
      "excerpt": "    #: height and width are the spatial dimensions of the current noise layer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944093796478039,
        0.8390490819117825
      ],
      "excerpt": "    #:     \"modulation\": time dependent modulation of the transformation, shape=(n_frames, ...),  \n    #:     \"transform\": function that takes a batch of modulation and returns a torch.nn.Module \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9397242858400464
      ],
      "excerpt": "    #: (The second one is technical debt in a nutshell. It's a workaround to get kornia transforms \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9112046075252406
      ],
      "excerpt": "    #:  has its modulation as an attribute and keeps count of which frame it's rendering internally). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.885161441575637
      ],
      "excerpt": "    #: generate a sequence of truncation values of shape (n_frames,) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9628736971936036
      ],
      "excerpt": "  --duration DURATION                      #: duration of interpolation to generate in seconds (leave empty for length of audiofile) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8474375320832573
      ],
      "excerpt": "  --shuffle_latents                        #: whether to shuffle the supplied latents or not \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8010251728507111,
        0.8523084801062801
      ],
      "excerpt": "  --batch BATCH                            #: batch size to render with \n  --truncation TRUNCATION                  #: truncation to render with (leave empty if get_truncations() is in --audioreactive_file) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8449053462072824,
        0.8761224423664857,
        0.8435726461288271
      ],
      "excerpt": "  --stylegan1                              #: if the model checkpoint is StyleGAN1 \n  --G_res G_RES                            #: training resolution of the generator \n  --base_res_factor BASE_RES_FACTOR        #: factor to increase generator noise maps by (useful when e.g. doubling 512px net to 1024px) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9248170278387646
      ],
      "excerpt": "  --latent_dim LATENT_DIM                  #: latent vector size of the generator \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This is the repo for my experiments with StyleGAN2. There are many like it, but this one is mine. Contains code for the paper Audio-reactive Latent Interpolations with StyleGAN.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JCBrouwer/maua-stylegan2/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 21,
      "date": "Fri, 24 Dec 2021 16:48:53 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JCBrouwer/maua-stylegan2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "JCBrouwer/maua-stylegan2",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n```bash\r\ngit clone https://github.com/JCBrouwer/maua-stylegan2\r\ncd maua-stylegan2\r\npip install -r requirements.txt\r\n```\r\n\r\nAlternatively, check out this [Colab Notebook](https://colab.research.google.com/drive/1Ig1EXfmBC01qik11Q32P0ZffFtNipiBR)\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8024121214511752
      ],
      "excerpt": "or (in e.g. a jupyter notebook): \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8006233563220954
      ],
      "excerpt": "| File/Folder | Description \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8393357341834662
      ],
      "excerpt": "| generate_audiovisual.py | used to generate audio-reactive interpolations \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.850426586608251,
        0.9136204677015993
      ],
      "excerpt": "| output/ | default generated output folder \n| train.py | code for training models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8360847926529431
      ],
      "excerpt": "python generate_audiovisual.py --ckpt \"/path/to/model.pt\" --audio_file \"/path/to/audio.wav\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.888329083696728
      ],
      "excerpt": "from generate_audiovisual import generate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8016302886814469,
        0.888329083696728
      ],
      "excerpt": "import audioreactive as ar \nfrom generate_audiovisual import generate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8095524723380809
      ],
      "excerpt": "generate(ckpt=\"/path/to/model.pt\", audio_file=\"/path/to/audio.wav\", initialize=initialize, get_latents=get_latents, get_noise=get_noise) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8500839802118044
      ],
      "excerpt": "python generate_audiovisual.py --ckpt \"/path/to/model.pt\" --audio_file \"/path/to/audio.wav\" --audioreactive_file \"/path/to/the/code_above.py\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8077003926696855
      ],
      "excerpt": "    #: generate an audioreactive latent tensor of shape [n_frames, layers, latent_dim] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8240041177334463
      ],
      "excerpt": "    #: generate an audioreactive noise tensor of shape [n_frames, 1, height, width] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8071900997917977
      ],
      "excerpt": "    #: generate a sequence of truncation values of shape (n_frames,) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8077377692846955,
        0.8165938069650776
      ],
      "excerpt": "  --output_dir OUTPUT_DIR                  #: path to output dir \n  --offset OFFSET                          #: starting time in audio in seconds (defaults to 0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8084748747375617
      ],
      "excerpt": "Alternatively, generate() can be called directly from python. It takes the same arguments as generate_audiovisual.py except instead of supplying an audioreactive_file, the functions should be supplied directly (i.e. initialize, get_latents, get_noise, get_bends, get_rewrites, and get_truncation as arguments). \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JCBrouwer/maua-stylegan2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "maua-stylegan2",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "maua-stylegan2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "JCBrouwer",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JCBrouwer/maua-stylegan2/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 125,
      "date": "Fri, 24 Dec 2021 16:48:53 GMT"
    },
    "technique": "GitHub API"
  }
}