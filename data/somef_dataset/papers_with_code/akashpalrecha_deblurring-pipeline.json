{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.02921",
      "https://arxiv.org/abs/1701.07875",
      "https://arxiv.org/abs/1704.00028",
      "https://arxiv.org/abs/1803.02735"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{Lim_2017_CVPR_Workshops,\n  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},\n  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},\n  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},\n  month = {July},\n  year = {2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9999765775085891,
        0.9999999995534665
      ],
      "excerpt": "If you find our work useful in your research or publication, please cite our work: \n[1] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee, \"Enhanced Deep Residual Networks for Single Image Super-Resolution,\" <i>2nd NTIRE: New Trends in Image Restoration and Enhancement workshop and challenge on image super-resolution in conjunction with CVPR 2017. </i> [PDF] [arXiv] [Slide] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9234453050875043
      ],
      "excerpt": "* Jan 04, 2018 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9662067084056507
      ],
      "excerpt": "Jan 09, 2018 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9234453050875043
      ],
      "excerpt": "Jan 16, 2018 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186665153711271
      ],
      "excerpt": "Feb 21, 2018 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186665153711271
      ],
      "excerpt": "Feb 23, 2018 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186665153711271
      ],
      "excerpt": "Mar 5, 2018 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9662067084056507
      ],
      "excerpt": "Mar 11, 2018 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186665153711271
      ],
      "excerpt": "Mar 20, 2018 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9662067084056507
      ],
      "excerpt": "Mar 29, 2018 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186665153711271
      ],
      "excerpt": "Apr 9, 2018 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186665153711271
      ],
      "excerpt": "Apr 26, 2018 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186665153711271
      ],
      "excerpt": "July 22, 2018 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9662067084056507
      ],
      "excerpt": "Oct 18, 2018 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/akashpalrecha/deblurring-pipeline",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-12T10:59:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-28T08:22:38Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9926145669250742
      ],
      "excerpt": "This repository is an official PyTorch implementation of the paper \"Enhanced Deep Residual Networks for Single Image Super-Resolution\" from CVPRW 2017, 2nd NTIRE. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9134573782067649
      ],
      "excerpt": "[1] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee, \"Enhanced Deep Residual Networks for Single Image Super-Resolution,\" <i>2nd NTIRE: New Trends in Image Restoration and Enhancement workshop and challenge on image super-resolution in conjunction with CVPR 2017. </i> [PDF] [arXiv] [Slide] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9037479608139666
      ],
      "excerpt": "* Codes are much more compact. (Removed all unnecessary parts.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.812613566765595
      ],
      "excerpt": "Add --chop_forward argument to your script to enable it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8608952792122652,
        0.8555660567823643
      ],
      "excerpt": "This code now only saves the best-performing model by default. For MDSR, 'the best' can be ambiguous. Use --save_models argument to keep all the intermediate models. \nPyTorch 0.3.1 changed their implementation of DataLoader function. Therefore, I also changed my implementation of MSDataLoader. You can find it on feature/dataloader branch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8720425819893306
      ],
      "excerpt": "With a new src/data/DIV2K.py code, one can easily create new data class for super-resolution. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8604817451123941
      ],
      "excerpt": "With --ext bin, this code will automatically generate and saves the binary data pack that corresponds to previous DIV2K_decoded. (This requires huge RAM (~45GB, Swap can be used.), so please be careful.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8651616931815402
      ],
      "excerpt": "Added performance comparison between Torch7 model and PyTorch models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8040951333142154
      ],
      "excerpt": "Fixed some typos in the code and script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9390239481463074
      ],
      "excerpt": "Skip_batch operation is implemented. Use --skip_threshold argument to skip the batch that you want to ignore. Although this function is not exactly the same with that of Torch7 version, it will work as you expected. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9147436775022373
      ],
      "excerpt": "Changed the behavior of skip_batch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8843851374344245,
        0.8114706005937831,
        0.8855197138547592,
        0.8775715145012851
      ],
      "excerpt": "We now provide all models from our paper. \nWe also provide MDSR_baseline_jpeg model that suppresses JPEG artifacts in the original low-resolution image. Please use it if you have any trouble. \nMyImage dataset is changed to Demo dataset. Also, it works more efficient than before. \nSome codes and script are re-written. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9440872436951873
      ],
      "excerpt": "VGG and Adversarial loss is implemented based on SRGAN. WGAN and gradient penalty are also implemented, but they are not tested yet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.934598563227368
      ],
      "excerpt": "D-DBPN is implemented. The default setting is D-DBPN-L. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9327474827669753
      ],
      "excerpt": "Compatible with PyTorch 0.4.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Contains code to run experiments on image deblurring using deep learning.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/akashpalrecha/deblurring-pipeline/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 04:02:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/akashpalrecha/deblurring-pipeline/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "akashpalrecha/deblurring-pipeline",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/akashpalrecha/deblurring-pipeline/master/scripts/slurm_run_baseline_2x.sh",
      "https://raw.githubusercontent.com/akashpalrecha/deblurring-pipeline/master/scripts/run_benchmark.sh",
      "https://raw.githubusercontent.com/akashpalrecha/deblurring-pipeline/master/scripts/matlab_evaluation/benchmark_eval.sh",
      "https://raw.githubusercontent.com/akashpalrecha/deblurring-pipeline/master/src/demo.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8716477416254461
      ],
      "excerpt": "  * There have been minor changes with the 1.1.0 update. Now we support PyTorch 1.1.0 by default, and please use the legacy branch if you prefer older version. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8168242151229482
      ],
      "excerpt": "Differences between Torch version \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.981239730950735,
        0.9516924449267574
      ],
      "excerpt": "git clone https://github.com/thstkdgus35/EDSR-PyTorch \ncd EDSR-PyTorch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8020412881934886
      ],
      "excerpt": "We recommend you to pre-process the images before training. This step will decode all png files and save them as binaries. Use --ext sep_reset argument on your first run. You can skip the decoding part and use saved binaries with --ext sep argument. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9113306577364304
      ],
      "excerpt": "cd src       #: You are now in */EDSR-PyTorch/src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8269554792375329
      ],
      "excerpt": "Add --chop_forward argument to your script to enable it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9305992811110446
      ],
      "excerpt": "Now PyTorch 0.3.1 is a default. Use legacy/0.3.0 branch if you use the old version. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9067277237264989
      ],
      "excerpt": "If you cannot make the binary pack, use the default setting (--ext img). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8214496797402424
      ],
      "excerpt": "Please use the legacy/0.3.1 branch if you are using the old version of PyTorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8804558529242338
      ],
      "excerpt": "with --pre_train download, pretrained models will be automatically downloaded from the server. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8764785317518224
      ],
      "excerpt": "We support PyTorch 1.0.0. If you prefer the previous versions of PyTorch, use legacy branches. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.871979857569022
      ],
      "excerpt": "Unpack the tar file to any place you want. Then, change the dir_data argument in src/option.py to the place where DIV2K images are located. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806335260163684
      ],
      "excerpt": "cd src       #: You are now in */EDSR-PyTorch/src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8973823538356233
      ],
      "excerpt": "Missing files are included (src/data/MyImage.py). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.800332261972376,
        0.8548200128062389
      ],
      "excerpt": "Use --ext sep-reset to pre-decode large png files. Those decoded files will be saved to the same directory with DIV2K png files. After the first run, you can use --ext sep to save time. \nNow supports various benchmark datasets. For example, try --data_test Set5 to test your model on the Set5 images. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/akashpalrecha/deblurring-pipeline/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "MATLAB",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Sanghyun Son\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "EDSR-PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "deblurring-pipeline",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "akashpalrecha",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/akashpalrecha/deblurring-pipeline/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python 3.6\n* PyTorch >= 1.0.0\n* numpy\n* skimage\n* **imageio**\n* matplotlib\n* tqdm\n* cv2 >= 3.xx (Only if you want to use video input/output)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 04:02:13 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can test our super-resolution algorithm with your images. Place your images in ``test`` folder. (like ``test/<your_image>``) We support **png** and **jpeg** files.\n\nRun the script in ``src`` folder. Before you run the demo, please uncomment the appropriate line in ```demo.sh``` that you want to execute.\n```bash\ncd src       #: You are now in */EDSR-PyTorch/src\nsh demo.sh\n```\n\nYou can find the result images from ```experiment/test/results``` folder.\n\n| Model | Scale | File name (.pt) | Parameters | ****PSNR** |\n|  ---  |  ---  | ---       | ---        | ---  |\n| **EDSR** | 2 | EDSR_baseline_x2 | 1.37 M | 34.61 dB |\n| | | *EDSR_x2 | 40.7 M | 35.03 dB |\n| | 3 | EDSR_baseline_x3 | 1.55 M | 30.92 dB |\n| | | *EDSR_x3 | 43.7 M | 31.26 dB |\n| | 4 | EDSR_baseline_x4 | 1.52 M | 28.95 dB |\n| | | *EDSR_x4 | 43.1 M | 29.25 dB |\n| **MDSR** | 2 | MDSR_baseline | 3.23 M | 34.63 dB |\n| | | *MDSR | 7.95 M| 34.92 dB |\n| | 3 | MDSR_baseline | | 30.94 dB |\n| | | *MDSR | | 31.22 dB |\n| | 4 | MDSR_baseline | | 28.97 dB |\n| | | *MDSR | | 29.24 dB |\n\n*Baseline models are in ``experiment/model``. Please download our final models from [here](https://cv.snu.ac.kr/research/EDSR/model_pytorch.tar) (542MB)\n**We measured PSNR using DIV2K 0801 ~ 0900, RGB channels, without self-ensemble. (scale + 2) pixels from the image boundary are ignored.\n\nYou can evaluate your models with widely-used benchmark datasets:\n\n[Set5 - Bevilacqua et al. BMVC 2012](http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html),\n\n[Set14 - Zeyde et al. LNCS 2010](https://sites.google.com/site/romanzeyde/research-interests),\n\n[B100 - Martin et al. ICCV 2001](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/),\n\n[Urban100 - Huang et al. CVPR 2015](https://sites.google.com/site/jbhuang0604/publications/struct_sr).\n\nFor these datasets, we first convert the result images to YCbCr color space and evaluate PSNR on the Y channel only. You can download [benchmark datasets](https://cv.snu.ac.kr/research/EDSR/benchmark.tar) (250MB). Set ``--dir_data <where_benchmark_folder_located>`` to evaluate the EDSR and MDSR with the benchmarks.\n\nYou can download some results from [here](https://cv.snu.ac.kr/research/EDSR/result_image/edsr-results.tar).\nThe link contains **EDSR+_baseline_x4** and **EDSR+_x4**.\nOtherwise, you can easily generate result images with ``demo.sh`` scripts.\n\n",
      "technique": "Header extraction"
    }
  ]
}