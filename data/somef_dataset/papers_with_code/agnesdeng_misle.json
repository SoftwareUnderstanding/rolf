{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1506.02142\n\nGal, Y., Hron, J., & Kendall, A. (2017",
      "https://arxiv.org/abs/1312.6114\n\nAlex Stenlake & Ranjit Lall. Python Package MIDAS: Multiple Imputation with Denoising Autoencoders https://github.com/Oracen/MIDAS"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "JJ Allaire and Yuan Tang (2019). tensorflow: R Interface to 'TensorFlow'. R package version 2.0.0. https://github.com/rstudio/tensorflow\n\nTianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen,Rory Mitchell, Ignacio Cano, Tianyi Zhou, Mu Li,Junyuan Xie, Min Lin, Yifeng Geng and Yutian Li (2019). xgboost: Extreme Gradient Boosting. R package version 0.90.0.2. https://CRAN.R-project.org/package=xgboost\n\nJJ Allaire and Fran\u00e7ois Chollet (2019). keras: R Interface to 'Keras'. R package version 2.2.4.1.9001. https://keras.rstudio.com\n\nRubin, D. B. (1987). Multiple imputation for nonresponse in surveys (1. print. ed.). New York [u.a.]: Wiley.\n\nVincent, P., Larochelle, H., Bengio, Y., \\& Manzagol, P. (Jul 5, 2008). Extracting and composing robust features with denoising autoencoders. Paper presented at the 1096-1103. doi:10.1145/1390156.1390294 Retrieved from http://dl.acm.org/citation.cfm?id=1390294\n\nGal, Y., \\& Ghahramani, Z. (2015). Dropout as a bayesian approximation: Representing model uncertainty in deep learning. Retrieved from https://arxiv.org/abs/1506.02142\n\nGal, Y., Hron, J., & Kendall, A. (2017). Concrete Dropout. NIPS.\n\nKendall, A., & Gal, Y. (2017). What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? NIPS.\n \nKingma, D. P., \\& Welling, M. (2013). Auto-encoding variational bayes. Retrieved from https://arxiv.org/abs/1312.6114\n\nAlex Stenlake & Ranjit Lall. Python Package MIDAS: Multiple Imputation with Denoising Autoencoders https://github.com/Oracen/MIDAS\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9404034918786912
      ],
      "excerpt": "Visual diagnostic for imputation results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944178096468923
      ],
      "excerpt": "Visual diagnostics \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/agnesdeng/misle",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-23T03:38:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-30T11:19:09Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9635529323757382,
        0.984842885273187
      ],
      "excerpt": "The R package misle is built using TensorFlow\u2122, which enables fast numerical computation and thus provides a solution for large-scale multiple imputation. \nmisle is still at the early stage of development so lots of work have to be done before it is officially released. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.803370734157694
      ],
      "excerpt": "multiple imputation by denoising autoencoders(with dropout) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8790639676187806,
        0.8528197297318668,
        0.9628855744780781,
        0.8370494396058642,
        0.9161002791575005
      ],
      "excerpt": "Imputation models can be saved to impute new unseen data. \nFixed bugs related to impute.new( ) function for mixgb imputer. \nA data cleaning function for users to roughly check their data before feeding in an imputer. \nAdded several error and warning messages related to user errors for mixgb. \nFixing bugs and add warning messages user errors for midae and mivae. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "Pretune hyperparamters for imputers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "imputed.data=MIDAE$impute(m = 5) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "imputed.data=MIVAE$impute(m = 5) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.982013266502453
      ],
      "excerpt": "It is highly recommended to clean and check your data before feeding in the imputer. Here are some common issues: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550326213680377
      ],
      "excerpt": "Since the original data doesn't have any missing value, we create some. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9237942670157109
      ],
      "excerpt": "We can use the training data (with missing values) to obtain m imputed datasets. Imputed datasets, the models used in training processes and some parameters are saved in the object mixgb.obj. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.984585930857119,
        0.949809288958563
      ],
      "excerpt": "By default, an ensemble of imputation models for all variables in the training dataset will be saved in the object  mixgb.obj. This is convenient when we do not know which variables of the future unseen data have missing values. However, this process would take longer time and space. \nIf users are confident that only certain variables of future data will have missing values, they can choose to specify these variables to speed up the process. Users can either use the indices or the names of the variables in the argument save.vars. Models for variables with missing values in the training data and those specified in save.vars will be saved. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9213331888349223
      ],
      "excerpt": "We can now use this object to impute new unseen data by using the function impute.new( ).  If PMM is applied, predicted values of missing entries in the new dataset are matched with training data by default.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8541187343611466
      ],
      "excerpt": "to show whether multiple imputation using statistical learning (machine learning) techniques will lead to statistical valid inference.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Multiple imputation through statistical learning",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/agnesdeng/misle/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 06:32:39 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/agnesdeng/misle/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "agnesdeng/misle",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If users only want to use multiple imputation through XGBoost, please install this simplified R package `mixgb` instead.\n\n\n```r\ndevtools::install_github(\"agnesdeng/mixgb\")\nlibrary(mixgb)\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Our package  `misle`  is built using `tensorflow`.  If P package  `tensorflow`  has been properly installed, users can directly install the newest version of  `misle`  from GitHub.\n\n``` r\ndevtools::install_github(\"agnesdeng/misle\")\nlibrary(misle)\n```\n\nIf  `tensorflow` has not been installed, we recommend to use virtual environment to install it.\n\n``` r\nlibrary(reticulate)\n\n#:By default, python package tensorflow would be installed in the virtual environment named 'r-reticulate' \nvirtualenv_install(packages = c(\"tensorflow==1.14.0\"))\n\n#:Install tensorflow R package\ninstall.packages(\"tensorflow\")\nlibrary(tensorflow)\ninstall_tensorflow(method=\"virtualenv\",version=\"1.14.0\",envname = \"r-reticulate\")\n\n#:Install misle \ndevtools::install_github(\"agnesdeng/misle\")\nlibrary(misle)\n\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9391425397979111
      ],
      "excerpt": "Mixgb imputer with GPU support. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8132430770352027,
        0.8747471251300003
      ],
      "excerpt": "idx=sample(1:n, size = round(0.7*n), replace=FALSE) \n:use training data to train the models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8132430770352027,
        0.8747471251300003
      ],
      "excerpt": "idx=sample(1:n, size = round(0.7*n), replace=FALSE) \n:use training data to train the models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017436260062111
      ],
      "excerpt": "Empty cells should be coded as NA or sensible values \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9090940564556204
      ],
      "excerpt": "First we can split a dataset as training data and test data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8132430770352027
      ],
      "excerpt": "idx=sample(1:n, size = round(0.7*n), replace=FALSE) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/agnesdeng/misle/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "R"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "misle",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "misle",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "agnesdeng",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/agnesdeng/misle/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Fri, 24 Dec 2021 06:32:39 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```r\n#:a data frame (consists of numeric/binary/multicalss variables) with NAs\nwithNA.df=createNA(adult,p=0.3)\n\n#:create a variational autoencoder imputer with your choice of settings or leave it as default\nMIDAE=Midae$new(withNA.df,iteration=20,input_drop=0.2,hidden_drop=0.3,n_h=4L)\n\n#:training\nMIDAE$train()\n\n#:impute m datasets\nimputed.data=MIDAE$impute(m = 5)\n\n#:the 2nd imputed dataset\nimputed.data[[2]]\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```r\n#:a data frame (consists of  numeric/binary/multicalss variables) with NAs\nwithNA.df=createNA(adult,p=0.3)\n\n#:create a variational autoencoder imputer with your choice of settings or leave it as default\nMIVAE=Mivae$new(withNA.df,iteration=20,input_drop=0.2,hidden_drop=0.3,n_h=4L)\n\n#:training\nMIVAE$train()\n\n#:impute m datasets\nimputed.data=MIVAE$impute(m = 5)\n\n#:the 2nd imputed dataset\nimputed.data[[2]]\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We first load the NHANES dataset from the R package \"hexbin\".\n``` r\nlibrary(hexbin)\ndata(\"NHANES\")\n```\n\nCreate 30% MCAR missing data.\n``` r\n#:a dataframe (consists of numeric/binary/multicalss variables) with NAs\nwithNA.df<-createNA(NHANES,p=0.3)\n```\n\nCreate an Mixgb imputer with your choice of settings or leave it as default.\n\nNote that users do not need to convert the data frame into dgCMatrix or one-hot coding themselves. Ths imputer will convert it automatically for you. The type of variables should be one of the following: numeric, integer, or factor (binary/multiclass).\n``` r\nMIXGB<-Mixgb$new(withNA.df,pmm.type=\"auto\",pmm.k = 5)\n```\n\nUse this imputer to obtain m imputed datasets.\n``` r\nmixgb.data<-MIXGB$impute(m=5)\n``` \n\nUsers can change the values for hyperparameters in an imputer. The default values are as follows.\n\n``` r\nMIXGB<-Mixgb$new(data=.., nrounds=50,max_depth=6,gamma=0.1,eta=0.3,nthread=4,early_stopping_rounds=10,colsample_bytree=1,min_child_weight=1,subsample=1,pmm.k=5,pmm.type=\"auto\",pmm.link=\"logit\",scale_pos_weight=1,initial.imp=\"random\",tree_method=\"auto\",gpu_id=0,predictor=\"auto\",print_every_n = 10L,verbose=0)\n```\n\n\n",
      "technique": "Header extraction"
    }
  ]
}