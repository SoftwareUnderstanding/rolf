{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.10934\n- Source code Yolo v4:https://github.com/AlexeyAB/darknet\n- More details: http://pjreddie.com/darknet/yolo/\n\n```\n@article{yolov4,\n  title={YOLOv4: YOLOv4: Optimal Speed and Accuracy of Object Detection},\n  author={Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao},\n  journal = {arXiv},\n  year={2020}\n}\n```\n\nContact:  \nIssues should be raised directly in the repository.  \nIf you are very interested in this project, please feel free to contact me (george_chen@owlting.com"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{yolov4,\n  title={YOLOv4: YOLOv4: Optimal Speed and Accuracy of Object Detection},\n  author={Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao},\n  journal = {arXiv},\n  year={2020}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/OwlTing/AI_basketball_games_video_editor",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-28T08:35:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T05:05:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "AI Basketball Games Video Editor is a program to get basketball highlight video by PyTorch YOLOv4 object detection",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/OwlTing/AI_basketball_games_video_editor/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Thu, 23 Dec 2021 12:18:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/OwlTing/AI_basketball_games_video_editor/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "OwlTing/AI_basketball_games_video_editor",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- `mkdir result`\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- download your basketball video in the directory `dataset/`\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.943250422025502,
        0.8394334062845149
      ],
      "excerpt": "\u251c\u2500\u2500 pytorch_YOLOv4                    pytorch-YOLOv4 source code \n\u2502\u00a0  \u251c\u2500\u2500 weights                       need to download weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8110305571767358
      ],
      "excerpt": "  <img src=\"https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/pic/gif_highlight.gif\" width=\"267\" height=\"225\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9772227696564274,
        0.9770335174395833,
        0.9906248903846466
      ],
      "excerpt": "conda create --name py36_env python=3.6 \nconda activate py36_env \ncd AI_basketball_games_video_editor \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902627162932362
      ],
      "excerpt": "mkdir pytorch_YOLOv4/weights/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902627162932362
      ],
      "excerpt": "mkdir pytorch_YOLOv4/weights/ \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9586232994076559
      ],
      "excerpt": "\u2502\u00a0\u00a0 \u2514\u2500\u2500 utils.py                   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8102780086823556
      ],
      "excerpt": "\u251c\u2500\u2500 result \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/OwlTing/AI_basketball_games_video_editor/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "Cuda",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "AI Basketball Games Video Editor",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "AI_basketball_games_video_editor",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "OwlTing",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Debian 10  \npython 3.6  \nnumpy  \npandas  \ntqdm  \ncv2  \npytorch 1.3.0  \nPlease refer to the official documentation for installing pytorch https://pytorch.org/get-started/locally/  \nMore details for different cuda version https://pytorch.org/get-started/previous-versions/  \nExample:  \nconda install pytorch==1.3.0 torchvision==0.4.1 cudatoolkit=10.0 -c pytorch  \n\nOptional (For tensorrt yolov4 object detector engine):  \ntensorrt 7.0.0  \nPlease refer to the official documentation for installing tensorrt with different cuda version  \nhttps://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html  \nExample: (For Debian 10 cuda 10.0)  \n1. mkdir tensorrt  \n2. From https://developer.nvidia.com/tensorrt, to download \n   `TensorRT-7.0.0.11.Ubuntu-18.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz`  \n   (select TensorRT 7.0) in the directory `tensorrt/`  \n3. tar xzvf `TensorRT-7.0.0.11.Ubuntu-18.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz`  \n4. export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/<path_your_tensorrt>/TensorRT-7.0.0.11/lib  \n5. cd TensorRT-7.0.0.11/python/  \n6. pip install `tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl`   \n7. \n```\nsudo cp /<path_your_tensorrt>/TensorRT-7.0.0.11/lib/libnvinfer.so.7 /usr/lib/ ;  \nsudo cp /<path_your_tensorrt>/TensorRT-7.0.0.11/lib/libnvonnxparser.so.7 /usr/lib/ ;  \nsudo cp /<path_your_tensorrt>/TensorRT-7.0.0.11/lib/libnvparsers.so.7 /usr/lib/ ;  \nsudo cp /<path_your_tensorrt>/TensorRT-7.0.0.11/lib/libnvinfer_plugin.so.7 /usr/lib/ ;  \nsudo cp /<path_your_tensorrt>/TensorRT-7.0.0.11/lib/libmyelin.so.1 /usr/lib/  \n```    \n8. pip install pycuda  \n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```sh\npython video_editor.py --video_path VIDEO_PATH --output_path OUTPUT_PATH --output_video_name OUTPUT_VIDEO_NAME [OPTIONS]\n\n#: example\npython video_editor.py --video_path dataset/basketball_demo.mp4 --output_path result/demo --output_video_name out_demo.mp4\n```\n\n- It will generate `your_output_video_name.mp4 obj_log_name.data` in the directory `result/`\n\n- If you had finished extracting features. You can use `--read_flag 1` to read log for different output video mode. \n\n- If you use pytorch yolov4 object detector engine `--inference_detector pytorch`.  \n  For image input size, you can select any inference_size = (height, width) in   \n  height = 320 + 96 * n, n in {0, 1, 2, 3, ...}  \n  width = 320 + 96 * m, m in {0, 1, 2, 3, ...}  \n  Exmaple `--inference_size (1184, 1184)` or `--inference_size (704, 704)`  \n  Default inference_size is (1184, 1184)\n  \n- If you use tensorrt yolov4 object detector engine `--inference_detector tensorrt`.  \n  For image input size, you only can select `--inference_size (1184, 1184)`.  \n  Tensorrt engine 3x faster than pytorch engine fps.\n\n- You can use `--output_mode shot` to select different output video mode.\n  ```\n  output video mode  \n  full            show person basketball basketball_hoop frame_information  \n  basketball      show basketball basketball_hoop frame_information  \n  shot            show basketball shot frame_information  \n  standard        show frame_information  \n  clean           only cutting video\n  ```\n![image](https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/pic/output_mode_clean.jpg)\n![image](https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/pic/output_mode_full.jpg)\n![image](https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/pic/output_mode_basketball.jpg)\n![image](https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/pic/output_mode_shot.jpg)\n\n- You can refer the command-line options.\n  ```\n  optional arguments:\n  -h, --help                                       show this help message and exit\n  \n  --video_path VIDEO_PATH                          input video path (default: None)\n                                                   \n  --output_path OUTPUT_PATH                        output folder path (default: None)\n                                                   \n  --output_video_name OUTPUT_VIDEO_NAME            output video name (default: None)\n                                                   \n  --highlight_flag HIGHLIGHT_FLAG                  select 1 with auto-generated highlight or \n                                                   0 without auto-generated highlight (default: 1)\n                                                   \n  --output_mode OUTPUT_MODE                        output video mode \n                                                   full       show person basketball basketball_hoop frame_information \n                                                   basketball show basketball basketball_hoop frame_information \n                                                   shot       show basketball shot frame_information \n                                                   standard   show frame_information \n                                                   clean      only cutting video (default: shot)\n                                                   \n  --process_frame_init PROCESS_FRAME_INIT          start processing frame (default: 0)\n                                                   \n  --process_frame_final PROCESS_FRAME_FINAL        end processing frame. If process_frame_final < 0, \n                                                   use video final frame (default: -1)\n                                                   \n  --obj_log_name OBJ_LOG_NAME                      save frame information and obj detect result \n                                                   (default: obj_log_name.data)\n                                                   \n  --save_step SAVE_STEP                            save obj log for each frame step (default: 2000)\n                                                   \n  --weight_path WEIGHT_PATH                        Yolov4 weight path (default: pytorch_YOLOv4/weights/yolov4-basketball.weights)\n                                                   \n  --cfg_path CFG_PATH                              Yolov4 cfg path (default: pytorch_YOLOv4/cfg/yolov4-basketball.cfg)\n  \n  --num_classes NUM_CLASSES                        num classes = 3 (person/basketball/basketball_hoop) (default: 3)\n                                                   \n  --namesfile_path NAMESFILE_PATH                  Yolov4 class names path (default: pytorch_YOLOv4/data/basketball_obj.names)\n                                                   \n  --inference_detector INFERENCE_DETECTOR          object detector engine. You can select pytorch or tensorrt (default: pytorch)\n                                                   \n  --inference_size INFERENCE_SIZE                  Image input size for inference \n                                                   If you use pytorch yolov4 object detector engine \n                                                   height = 320 + 96 * n, n in {0, 1, 2, 3, ...} \n                                                   width = 320 + 96 * m, m in {0, 1, 2, 3, ...} \n                                                   inference_size= (height, width) \n                                                   \n                                                   If you use tensorrt yolov4 object detector engine Image input size for\n                                                   inference only with inference_size = (1184, 1184) (default: (1184, 1184))\n                                                   \n  --read_flag READ_FLAG                            read log mode flag If you had finished extracting features. You can use \n                                                   select 1 to read log for different output video mode. (default: 0)\n                                                                                                    \n  --cut_frame CUT_FRAME                            cut frame range around shot frame index for highlight video (default: 50)  \n  ```\n\nReference:\n- https://github.com/Tianxiaomo/pytorch-YOLOv4\n- https://github.com/eriklindernoren/PyTorch-YOLOv3\n- https://github.com/marvis/pytorch-caffe-darknet-convert\n- https://github.com/marvis/pytorch-yolo3\n\n- Paper Yolo v4: https://arxiv.org/abs/2004.10934\n- Source code Yolo v4:https://github.com/AlexeyAB/darknet\n- More details: http://pjreddie.com/darknet/yolo/\n\n```\n@article{yolov4,\n  title={YOLOv4: YOLOv4: Optimal Speed and Accuracy of Object Detection},\n  author={Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao},\n  journal = {arXiv},\n  year={2020}\n}\n```\n\nContact:  \nIssues should be raised directly in the repository.  \nIf you are very interested in this project, please feel free to contact me (george_chen@owlting.com).  \n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 41,
      "date": "Thu, 23 Dec 2021 12:18:45 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "basketball",
      "machine-learning",
      "ai",
      "computer-vision",
      "artificial-intelligence",
      "deep-learning",
      "yolov4",
      "pytorch",
      "pytorch-yolov4",
      "tensorrt",
      "objection-detection"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```sh\ngit clone https://github.com/OwlTing/AI_basketball_games_video_editor.git\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```sh\npython video_editor.py --video_path VIDEO_PATH --output_path OUTPUT_PATH --output_video_name OUTPUT_VIDEO_NAME [OPTIONS]\n\n#: example\npython video_editor.py --video_path dataset/basketball_demo.mp4 --output_path result/demo --output_video_name out_demo.mp4\n```\n\n- It will generate `your_output_video_name.mp4 obj_log_name.data` in the directory `result/`\n\n- If you had finished extracting features. You can use `--read_flag 1` to read log for different output video mode. \n\n- If you use pytorch yolov4 object detector engine `--inference_detector pytorch`.  \n  For image input size, you can select any inference_size = (height, width) in   \n  height = 320 + 96 * n, n in {0, 1, 2, 3, ...}  \n  width = 320 + 96 * m, m in {0, 1, 2, 3, ...}  \n  Exmaple `--inference_size (1184, 1184)` or `--inference_size (704, 704)`  \n  Default inference_size is (1184, 1184)\n  \n- If you use tensorrt yolov4 object detector engine `--inference_detector tensorrt`.  \n  For image input size, you only can select `--inference_size (1184, 1184)`.  \n  Tensorrt engine 3x faster than pytorch engine fps.\n\n- You can use `--output_mode shot` to select different output video mode.\n  ```\n  output video mode  \n  full            show person basketball basketball_hoop frame_information  \n  basketball      show basketball basketball_hoop frame_information  \n  shot            show basketball shot frame_information  \n  standard        show frame_information  \n  clean           only cutting video\n  ```\n![image](https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/pic/output_mode_clean.jpg)\n![image](https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/pic/output_mode_full.jpg)\n![image](https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/pic/output_mode_basketball.jpg)\n![image](https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/pic/output_mode_shot.jpg)\n\n- You can refer the command-line options.\n  ```\n  optional arguments:\n  -h, --help                                       show this help message and exit\n  \n  --video_path VIDEO_PATH                          input video path (default: None)\n                                                   \n  --output_path OUTPUT_PATH                        output folder path (default: None)\n                                                   \n  --output_video_name OUTPUT_VIDEO_NAME            output video name (default: None)\n                                                   \n  --highlight_flag HIGHLIGHT_FLAG                  select 1 with auto-generated highlight or \n                                                   0 without auto-generated highlight (default: 1)\n                                                   \n  --output_mode OUTPUT_MODE                        output video mode \n                                                   full       show person basketball basketball_hoop frame_information \n                                                   basketball show basketball basketball_hoop frame_information \n                                                   shot       show basketball shot frame_information \n                                                   standard   show frame_information \n                                                   clean      only cutting video (default: shot)\n                                                   \n  --process_frame_init PROCESS_FRAME_INIT          start processing frame (default: 0)\n                                                   \n  --process_frame_final PROCESS_FRAME_FINAL        end processing frame. If process_frame_final < 0, \n                                                   use video final frame (default: -1)\n                                                   \n  --obj_log_name OBJ_LOG_NAME                      save frame information and obj detect result \n                                                   (default: obj_log_name.data)\n                                                   \n  --save_step SAVE_STEP                            save obj log for each frame step (default: 2000)\n                                                   \n  --weight_path WEIGHT_PATH                        Yolov4 weight path (default: pytorch_YOLOv4/weights/yolov4-basketball.weights)\n                                                   \n  --cfg_path CFG_PATH                              Yolov4 cfg path (default: pytorch_YOLOv4/cfg/yolov4-basketball.cfg)\n  \n  --num_classes NUM_CLASSES                        num classes = 3 (person/basketball/basketball_hoop) (default: 3)\n                                                   \n  --namesfile_path NAMESFILE_PATH                  Yolov4 class names path (default: pytorch_YOLOv4/data/basketball_obj.names)\n                                                   \n  --inference_detector INFERENCE_DETECTOR          object detector engine. You can select pytorch or tensorrt (default: pytorch)\n                                                   \n  --inference_size INFERENCE_SIZE                  Image input size for inference \n                                                   If you use pytorch yolov4 object detector engine \n                                                   height = 320 + 96 * n, n in {0, 1, 2, 3, ...} \n                                                   width = 320 + 96 * m, m in {0, 1, 2, 3, ...} \n                                                   inference_size= (height, width) \n                                                   \n                                                   If you use tensorrt yolov4 object detector engine Image input size for\n                                                   inference only with inference_size = (1184, 1184) (default: (1184, 1184))\n                                                   \n  --read_flag READ_FLAG                            read log mode flag If you had finished extracting features. You can use \n                                                   select 1 to read log for different output video mode. (default: 0)\n                                                                                                    \n  --cut_frame CUT_FRAME                            cut frame range around shot frame index for highlight video (default: 50)  \n  ```\n\nReference:\n- https://github.com/Tianxiaomo/pytorch-YOLOv4\n- https://github.com/eriklindernoren/PyTorch-YOLOv3\n- https://github.com/marvis/pytorch-caffe-darknet-convert\n- https://github.com/marvis/pytorch-yolo3\n\n- Paper Yolo v4: https://arxiv.org/abs/2004.10934\n- Source code Yolo v4:https://github.com/AlexeyAB/darknet\n- More details: http://pjreddie.com/darknet/yolo/\n\n```\n@article{yolov4,\n  title={YOLOv4: YOLOv4: Optimal Speed and Accuracy of Object Detection},\n  author={Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao},\n  journal = {arXiv},\n  year={2020}\n}\n```\n\nContact:  \nIssues should be raised directly in the repository.  \nIf you are very interested in this project, please feel free to contact me (george_chen@owlting.com).  \n",
      "technique": "Header extraction"
    }
  ]
}