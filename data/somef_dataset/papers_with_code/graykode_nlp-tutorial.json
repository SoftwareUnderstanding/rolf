{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.0473",
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1810.04805"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8829046255316265
      ],
      "excerpt": "<p align=\"center\"><img width=\"100\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/225px-TensorFlowLogo.svg.png\" />  <img width=\"100\" src=\"https://media-thumbs.golden.com/OLqzmrmwAzY1P7Sl29k2T9WjJdM=/200x200/smart/golden-storage-production.s3.amazonaws.com/topic_images/e08914afa10a4179893eeb07cb5e4713.png\" /></p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9686000830583983
      ],
      "excerpt": "Paper - Bag of Tricks for Efficient Text Classification(2016) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8950846984587078
      ],
      "excerpt": "Paper - Convolutional Neural Networks for Sentence Classification(2014) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9680477010791311
      ],
      "excerpt": "    for Statistical Machine Translation(2014) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9284491028834256
      ],
      "excerpt": "Paper - Neural Machine Translation by Jointly Learning to Align and Translate(2014) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.90848751255209
      ],
      "excerpt": "Paper - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9809396775432889
      ],
      "excerpt": "Author Email : nlkey2022@gmail.com \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/graykode/nlp-tutorial",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contribution Guidelines\nThank you to everyone who contributes. Here are some rules to follow before contributing.\n1. Contributions are open to the smallest details such as typos, comments and code refactors.\n2. Do not commit the jupyter notebook file(*.ipynb). When the modified python code is merged into the master branch, the github action automatically generates an ipynb.\n3. Please attach a commit message appropriate to the modified code.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-09T11:44:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T13:56:41Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9723520987113622,
        0.9286510634182558,
        0.822180783024899,
        0.872614216094921
      ],
      "excerpt": "nlp-tutorial is a tutorial for who is studying NLP(Natural Language Processing) using Pytorch. Most of the models in NLP were implemented with less than 100 lines of code.(except comments or blank lines) \n[08-14-2020] Old TensorFlow v1 code is archived in the archive folder. For beginner readability, only pytorch version 1.0 or higher is supported. \n1-1. NNLM(Neural Network Language Model) - Predict Next Word \nPaper -  A Neural Probabilistic Language Model(2003) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8464826149552599,
        0.9394836466715629
      ],
      "excerpt": "1-2. Word2Vec(Skip-gram) - Embedding Words and Show Graph \nPaper - Distributed Representations of Words and Phrases \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8031935954000641
      ],
      "excerpt": "Paper - Bag of Tricks for Efficient Text Classification(2016) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "4-2. Seq2Seq with Attention - Translate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8847633527855355
      ],
      "excerpt": "4-3. Bi-LSTM with Attention - Binary Sentiment Classification \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "5-1.  The Transformer - Translate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8650761365249389
      ],
      "excerpt": "Paper - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Natural Language Processing Tutorial for Deep Learning Researchers",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/graykode/nlp-tutorial/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3013,
      "date": "Wed, 22 Dec 2021 15:25:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/graykode/nlp-tutorial/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "graykode/nlp-tutorial",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/graykode/nlp-tutorial/master/2-1.TextCNN/TextCNN.ipynb",
      "https://raw.githubusercontent.com/graykode/nlp-tutorial/master/3-2.TextLSTM/TextLSTM.ipynb",
      "https://raw.githubusercontent.com/graykode/nlp-tutorial/master/5-1.Transformer/Transformer.ipynb",
      "https://raw.githubusercontent.com/graykode/nlp-tutorial/master/5-1.Transformer/Transformer%28Greedy_decoder%29.ipynb",
      "https://raw.githubusercontent.com/graykode/nlp-tutorial/master/1-1.NNLM/NNLM.ipynb",
      "https://raw.githubusercontent.com/graykode/nlp-tutorial/master/1-2.Word2Vec/Word2Vec-Skipgram%28Softmax%29.ipynb",
      "https://raw.githubusercontent.com/graykode/nlp-tutorial/master/3-1.TextRNN/TextRNN.ipynb",
      "https://raw.githubusercontent.com/graykode/nlp-tutorial/master/4-3.Bi-LSTM%28Attention%29/Bi-LSTM%28Attention%29.ipynb",
      "https://raw.githubusercontent.com/graykode/nlp-tutorial/master/4-2.Seq2Seq%28Attention%29/Seq2Seq%28Attention%29.ipynb",
      "https://raw.githubusercontent.com/graykode/nlp-tutorial/master/3-3.Bi-LSTM/Bi-LSTM.ipynb",
      "https://raw.githubusercontent.com/graykode/nlp-tutorial/master/1-3.FastText/FastText.ipynb",
      "https://raw.githubusercontent.com/graykode/nlp-tutorial/master/4-1.Seq2Seq/Seq2Seq.ipynb",
      "https://raw.githubusercontent.com/graykode/nlp-tutorial/master/5-2.BERT/BERT.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/graykode/nlp-tutorial/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 TaeHwan Jung\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# nlp-tutorial",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "nlp-tutorial",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "graykode",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/graykode/nlp-tutorial/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3.5+\n- Pytorch 1.0.0+\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9907,
      "date": "Wed, 22 Dec 2021 15:25:29 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "nlp",
      "natural-language-processing",
      "tutorial",
      "pytorch",
      "tensorflow",
      "transformer",
      "attention",
      "paper",
      "bert"
    ],
    "technique": "GitHub API"
  }
}