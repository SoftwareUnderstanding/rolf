{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2010.07468 <https://arxiv.org/abs/2010.07468>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| AdaBound     | *Adaptive Gradient Methods with Dynamic Bound of Learning Rate*                        | `github <https://github.com/Luolc/AdaBound/blob/master/adabound/adabound.py>`__   | `https://openreview.net/forum?id=Bkg3g2R9FX <https://openreview.net/forum?id=Bkg3g2R9FX>`__   |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| AdaHessian   | *An Adaptive Second Order Optimizer for Machine Learning*                              | `github <https://github.com/amirgholami/adahessian>`__                            | `https://arxiv.org/abs/2006.00719 <https://arxiv.org/abs/2006.00719>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| AdamD        | *Improved bias-correction in Adam*                                                     |                                                                                   | `https://arxiv.org/abs/2110.10828 <https://arxiv.org/abs/2110.10828>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| AdamP        | *Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights*         | `github <https://github.com/clovaai/AdamP>`__                                     | `https://arxiv.org/abs/2006.08217 <https://arxiv.org/abs/2006.08217>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| diffGrad     | *An Optimization Method for Convolutional Neural Networks*                             | `github <https://github.com/shivram1987/diffGrad>`__                              | `https://arxiv.org/abs/1909.11015v3 <https://arxiv.org/abs/1909.11015v3>`__                   |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| MADGRAD      | *A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic*               | `github <https://github.com/facebookresearch/madgrad>`__                          | `https://arxiv.org/abs/2101.11075 <https://arxiv.org/abs/2101.11075>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| RAdam        | *On the Variance of the Adaptive Learning Rate and Beyond*                             | `github <https://github.com/LiyuanLucasLiu/RAdam>`__                              | `https://arxiv.org/abs/1908.03265 <https://arxiv.org/abs/1908.03265>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*   | `github <https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer>`__          | `https://bit.ly/3zyspC3 <https://bit.ly/3zyspC3>`__                                           |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger21     | *a synergistic deep learning optimizer*                                                | `github <https://github.com/lessw2020/Ranger21>`__                                | `https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/2010.07468>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| AdaBound     | *Adaptive Gradient Methods with Dynamic Bound of Learning Rate*                        | `github <https://github.com/Luolc/AdaBound/blob/master/adabound/adabound.py>`__   | `https://openreview.net/forum?id=Bkg3g2R9FX <https://openreview.net/forum?id=Bkg3g2R9FX>`__   |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| AdaHessian   | *An Adaptive Second Order Optimizer for Machine Learning*                              | `github <https://github.com/amirgholami/adahessian>`__                            | `https://arxiv.org/abs/2006.00719 <https://arxiv.org/abs/2006.00719>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| AdamD        | *Improved bias-correction in Adam*                                                     |                                                                                   | `https://arxiv.org/abs/2110.10828 <https://arxiv.org/abs/2110.10828>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| AdamP        | *Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights*         | `github <https://github.com/clovaai/AdamP>`__                                     | `https://arxiv.org/abs/2006.08217 <https://arxiv.org/abs/2006.08217>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| diffGrad     | *An Optimization Method for Convolutional Neural Networks*                             | `github <https://github.com/shivram1987/diffGrad>`__                              | `https://arxiv.org/abs/1909.11015v3 <https://arxiv.org/abs/1909.11015v3>`__                   |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| MADGRAD      | *A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic*               | `github <https://github.com/facebookresearch/madgrad>`__                          | `https://arxiv.org/abs/2101.11075 <https://arxiv.org/abs/2101.11075>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| RAdam        | *On the Variance of the Adaptive Learning Rate and Beyond*                             | `github <https://github.com/LiyuanLucasLiu/RAdam>`__                              | `https://arxiv.org/abs/1908.03265 <https://arxiv.org/abs/1908.03265>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*   | `github <https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer>`__          | `https://bit.ly/3zyspC3 <https://bit.ly/3zyspC3>`__                                           |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger21     | *a synergistic deep learning optimizer*                                                | `github <https://github.com/lessw2020/Ranger21>`__                                | `https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/2006.00719 <https://arxiv.org/abs/2006.00719>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| AdamD        | *Improved bias-correction in Adam*                                                     |                                                                                   | `https://arxiv.org/abs/2110.10828 <https://arxiv.org/abs/2110.10828>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| AdamP        | *Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights*         | `github <https://github.com/clovaai/AdamP>`__                                     | `https://arxiv.org/abs/2006.08217 <https://arxiv.org/abs/2006.08217>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| diffGrad     | *An Optimization Method for Convolutional Neural Networks*                             | `github <https://github.com/shivram1987/diffGrad>`__                              | `https://arxiv.org/abs/1909.11015v3 <https://arxiv.org/abs/1909.11015v3>`__                   |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| MADGRAD      | *A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic*               | `github <https://github.com/facebookresearch/madgrad>`__                          | `https://arxiv.org/abs/2101.11075 <https://arxiv.org/abs/2101.11075>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| RAdam        | *On the Variance of the Adaptive Learning Rate and Beyond*                             | `github <https://github.com/LiyuanLucasLiu/RAdam>`__                              | `https://arxiv.org/abs/1908.03265 <https://arxiv.org/abs/1908.03265>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*   | `github <https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer>`__          | `https://bit.ly/3zyspC3 <https://bit.ly/3zyspC3>`__                                           |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger21     | *a synergistic deep learning optimizer*                                                | `github <https://github.com/lessw2020/Ranger21>`__                                | `https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/2006.00719>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| AdamD        | *Improved bias-correction in Adam*                                                     |                                                                                   | `https://arxiv.org/abs/2110.10828 <https://arxiv.org/abs/2110.10828>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| AdamP        | *Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights*         | `github <https://github.com/clovaai/AdamP>`__                                     | `https://arxiv.org/abs/2006.08217 <https://arxiv.org/abs/2006.08217>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| diffGrad     | *An Optimization Method for Convolutional Neural Networks*                             | `github <https://github.com/shivram1987/diffGrad>`__                              | `https://arxiv.org/abs/1909.11015v3 <https://arxiv.org/abs/1909.11015v3>`__                   |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| MADGRAD      | *A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic*               | `github <https://github.com/facebookresearch/madgrad>`__                          | `https://arxiv.org/abs/2101.11075 <https://arxiv.org/abs/2101.11075>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| RAdam        | *On the Variance of the Adaptive Learning Rate and Beyond*                             | `github <https://github.com/LiyuanLucasLiu/RAdam>`__                              | `https://arxiv.org/abs/1908.03265 <https://arxiv.org/abs/1908.03265>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*   | `github <https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer>`__          | `https://bit.ly/3zyspC3 <https://bit.ly/3zyspC3>`__                                           |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger21     | *a synergistic deep learning optimizer*                                                | `github <https://github.com/lessw2020/Ranger21>`__                                | `https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/2110.10828 <https://arxiv.org/abs/2110.10828>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| AdamP        | *Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights*         | `github <https://github.com/clovaai/AdamP>`__                                     | `https://arxiv.org/abs/2006.08217 <https://arxiv.org/abs/2006.08217>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| diffGrad     | *An Optimization Method for Convolutional Neural Networks*                             | `github <https://github.com/shivram1987/diffGrad>`__                              | `https://arxiv.org/abs/1909.11015v3 <https://arxiv.org/abs/1909.11015v3>`__                   |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| MADGRAD      | *A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic*               | `github <https://github.com/facebookresearch/madgrad>`__                          | `https://arxiv.org/abs/2101.11075 <https://arxiv.org/abs/2101.11075>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| RAdam        | *On the Variance of the Adaptive Learning Rate and Beyond*                             | `github <https://github.com/LiyuanLucasLiu/RAdam>`__                              | `https://arxiv.org/abs/1908.03265 <https://arxiv.org/abs/1908.03265>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*   | `github <https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer>`__          | `https://bit.ly/3zyspC3 <https://bit.ly/3zyspC3>`__                                           |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger21     | *a synergistic deep learning optimizer*                                                | `github <https://github.com/lessw2020/Ranger21>`__                                | `https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/2110.10828>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| AdamP        | *Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights*         | `github <https://github.com/clovaai/AdamP>`__                                     | `https://arxiv.org/abs/2006.08217 <https://arxiv.org/abs/2006.08217>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| diffGrad     | *An Optimization Method for Convolutional Neural Networks*                             | `github <https://github.com/shivram1987/diffGrad>`__                              | `https://arxiv.org/abs/1909.11015v3 <https://arxiv.org/abs/1909.11015v3>`__                   |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| MADGRAD      | *A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic*               | `github <https://github.com/facebookresearch/madgrad>`__                          | `https://arxiv.org/abs/2101.11075 <https://arxiv.org/abs/2101.11075>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| RAdam        | *On the Variance of the Adaptive Learning Rate and Beyond*                             | `github <https://github.com/LiyuanLucasLiu/RAdam>`__                              | `https://arxiv.org/abs/1908.03265 <https://arxiv.org/abs/1908.03265>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*   | `github <https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer>`__          | `https://bit.ly/3zyspC3 <https://bit.ly/3zyspC3>`__                                           |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger21     | *a synergistic deep learning optimizer*                                                | `github <https://github.com/lessw2020/Ranger21>`__                                | `https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/2006.08217 <https://arxiv.org/abs/2006.08217>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| diffGrad     | *An Optimization Method for Convolutional Neural Networks*                             | `github <https://github.com/shivram1987/diffGrad>`__                              | `https://arxiv.org/abs/1909.11015v3 <https://arxiv.org/abs/1909.11015v3>`__                   |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| MADGRAD      | *A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic*               | `github <https://github.com/facebookresearch/madgrad>`__                          | `https://arxiv.org/abs/2101.11075 <https://arxiv.org/abs/2101.11075>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| RAdam        | *On the Variance of the Adaptive Learning Rate and Beyond*                             | `github <https://github.com/LiyuanLucasLiu/RAdam>`__                              | `https://arxiv.org/abs/1908.03265 <https://arxiv.org/abs/1908.03265>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*   | `github <https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer>`__          | `https://bit.ly/3zyspC3 <https://bit.ly/3zyspC3>`__                                           |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger21     | *a synergistic deep learning optimizer*                                                | `github <https://github.com/lessw2020/Ranger21>`__                                | `https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/2006.08217>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| diffGrad     | *An Optimization Method for Convolutional Neural Networks*                             | `github <https://github.com/shivram1987/diffGrad>`__                              | `https://arxiv.org/abs/1909.11015v3 <https://arxiv.org/abs/1909.11015v3>`__                   |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| MADGRAD      | *A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic*               | `github <https://github.com/facebookresearch/madgrad>`__                          | `https://arxiv.org/abs/2101.11075 <https://arxiv.org/abs/2101.11075>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| RAdam        | *On the Variance of the Adaptive Learning Rate and Beyond*                             | `github <https://github.com/LiyuanLucasLiu/RAdam>`__                              | `https://arxiv.org/abs/1908.03265 <https://arxiv.org/abs/1908.03265>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*   | `github <https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer>`__          | `https://bit.ly/3zyspC3 <https://bit.ly/3zyspC3>`__                                           |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger21     | *a synergistic deep learning optimizer*                                                | `github <https://github.com/lessw2020/Ranger21>`__                                | `https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/1909.11015v3 <https://arxiv.org/abs/1909.11015v3>`__                   |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| MADGRAD      | *A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic*               | `github <https://github.com/facebookresearch/madgrad>`__                          | `https://arxiv.org/abs/2101.11075 <https://arxiv.org/abs/2101.11075>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| RAdam        | *On the Variance of the Adaptive Learning Rate and Beyond*                             | `github <https://github.com/LiyuanLucasLiu/RAdam>`__                              | `https://arxiv.org/abs/1908.03265 <https://arxiv.org/abs/1908.03265>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*   | `github <https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer>`__          | `https://bit.ly/3zyspC3 <https://bit.ly/3zyspC3>`__                                           |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger21     | *a synergistic deep learning optimizer*                                                | `github <https://github.com/lessw2020/Ranger21>`__                                | `https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/1909.11015v3>`__                   |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| MADGRAD      | *A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic*               | `github <https://github.com/facebookresearch/madgrad>`__                          | `https://arxiv.org/abs/2101.11075 <https://arxiv.org/abs/2101.11075>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| RAdam        | *On the Variance of the Adaptive Learning Rate and Beyond*                             | `github <https://github.com/LiyuanLucasLiu/RAdam>`__                              | `https://arxiv.org/abs/1908.03265 <https://arxiv.org/abs/1908.03265>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*   | `github <https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer>`__          | `https://bit.ly/3zyspC3 <https://bit.ly/3zyspC3>`__                                           |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger21     | *a synergistic deep learning optimizer*                                                | `github <https://github.com/lessw2020/Ranger21>`__                                | `https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/2101.11075 <https://arxiv.org/abs/2101.11075>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| RAdam        | *On the Variance of the Adaptive Learning Rate and Beyond*                             | `github <https://github.com/LiyuanLucasLiu/RAdam>`__                              | `https://arxiv.org/abs/1908.03265 <https://arxiv.org/abs/1908.03265>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*   | `github <https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer>`__          | `https://bit.ly/3zyspC3 <https://bit.ly/3zyspC3>`__                                           |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger21     | *a synergistic deep learning optimizer*                                                | `github <https://github.com/lessw2020/Ranger21>`__                                | `https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/2101.11075>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| RAdam        | *On the Variance of the Adaptive Learning Rate and Beyond*                             | `github <https://github.com/LiyuanLucasLiu/RAdam>`__                              | `https://arxiv.org/abs/1908.03265 <https://arxiv.org/abs/1908.03265>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*   | `github <https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer>`__          | `https://bit.ly/3zyspC3 <https://bit.ly/3zyspC3>`__                                           |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger21     | *a synergistic deep learning optimizer*                                                | `github <https://github.com/lessw2020/Ranger21>`__                                | `https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/1908.03265 <https://arxiv.org/abs/1908.03265>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*   | `github <https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer>`__          | `https://bit.ly/3zyspC3 <https://bit.ly/3zyspC3>`__                                           |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger21     | *a synergistic deep learning optimizer*                                                | `github <https://github.com/lessw2020/Ranger21>`__                                | `https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/1908.03265>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*   | `github <https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer>`__          | `https://bit.ly/3zyspC3 <https://bit.ly/3zyspC3>`__                                           |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n| Ranger21     | *a synergistic deep learning optimizer*                                                | `github <https://github.com/lessw2020/Ranger21>`__                                | `https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/2106.13731 <https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/2106.13731>`__                       |\n+--------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n\nUseful Resources\n----------------\n\nSeveral optimization ideas to regularize & stabilize the training. Most\nof the ideas are applied in ``Ranger21`` optimizer.\n\nAlso, most of the captures are taken from ``Ranger21`` paper.\n\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |\n+------------------------------------------+---------------------------------------------+--------------------------------------------+\n| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive",
      "https://arxiv.org/abs/2102.06171>`__\n\nGradient Centralization\n-----------------------\n\n+-----------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/gradient_centralization.png  |\n+-----------------------------------------------------------------------------------------------------------------+\n\n``Gradient Centralization (GC",
      "https://arxiv.org/abs/2004.01461>`__\n\nSoftplus Transformation\n-----------------------\n\nBy running the final variance denom through the softplus function, it lifts extremely tiny values to keep them viable.\n\n-  paper : `arXiv <https://arxiv.org/abs/1908.00700>`__\n\nGradient Normalization\n----------------------\n\nNorm Loss\n---------\n\n+---------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/norm_loss.png  |\n+---------------------------------------------------------------------------------------------------+\n\n-  paper : `arXiv <https://arxiv.org/abs/2103.06583>`__\n\nPositive-Negative Momentum\n--------------------------\n\n+--------------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/positive_negative_momentum.png  |\n+--------------------------------------------------------------------------------------------------------------------+\n\n-  code : `github <https://github.com/zeke-xie/Positive-Negative-Momentum>`__\n-  paper : `arXiv <https://arxiv.org/abs/2103.17182>`__\n\nLinear learning rate warmup\n---------------------------\n\n+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/linear_lr_warmup.png  |\n+----------------------------------------------------------------------------------------------------------+\n\n-  paper : `arXiv <https://arxiv.org/abs/1910.04209>`__\n\nStable weight decay\n-------------------\n\n+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/stable_weight_decay.png  |\n+-------------------------------------------------------------------------------------------------------------+\n\n-  code : `github <https://github.com/zeke-xie/stable-weight-decay-regularization>`__\n-  paper : `arXiv <https://arxiv.org/abs/2011.11152>`__\n\nExplore-exploit learning rate schedule\n--------------------------------------\n\n+---------------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/explore_exploit_lr_schedule.png  |\n+---------------------------------------------------------------------------------------------------------------------+\n\n-  code : `github <https://github.com/nikhil-iyer-97/wide-minima-density-hypothesis>`__\n-  paper : `arXiv <https://arxiv.org/abs/2003.03977>`__\n\nLookahead\n---------\n\n| ``k`` steps forward, 1 step back. ``Lookahead`` consisting of keeping an exponential moving average of the weights that is\n| updated and substituted to the current weights every ``k_{lookahead}`` steps (5 by default",
      "https://arxiv.org/abs/1908.00700>`__\n\nGradient Normalization\n----------------------\n\nNorm Loss\n---------\n\n+---------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/norm_loss.png  |\n+---------------------------------------------------------------------------------------------------+\n\n-  paper : `arXiv <https://arxiv.org/abs/2103.06583>`__\n\nPositive-Negative Momentum\n--------------------------\n\n+--------------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/positive_negative_momentum.png  |\n+--------------------------------------------------------------------------------------------------------------------+\n\n-  code : `github <https://github.com/zeke-xie/Positive-Negative-Momentum>`__\n-  paper : `arXiv <https://arxiv.org/abs/2103.17182>`__\n\nLinear learning rate warmup\n---------------------------\n\n+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/linear_lr_warmup.png  |\n+----------------------------------------------------------------------------------------------------------+\n\n-  paper : `arXiv <https://arxiv.org/abs/1910.04209>`__\n\nStable weight decay\n-------------------\n\n+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/stable_weight_decay.png  |\n+-------------------------------------------------------------------------------------------------------------+\n\n-  code : `github <https://github.com/zeke-xie/stable-weight-decay-regularization>`__\n-  paper : `arXiv <https://arxiv.org/abs/2011.11152>`__\n\nExplore-exploit learning rate schedule\n--------------------------------------\n\n+---------------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/explore_exploit_lr_schedule.png  |\n+---------------------------------------------------------------------------------------------------------------------+\n\n-  code : `github <https://github.com/nikhil-iyer-97/wide-minima-density-hypothesis>`__\n-  paper : `arXiv <https://arxiv.org/abs/2003.03977>`__\n\nLookahead\n---------\n\n| ``k`` steps forward, 1 step back. ``Lookahead`` consisting of keeping an exponential moving average of the weights that is\n| updated and substituted to the current weights every ``k_{lookahead}`` steps (5 by default",
      "https://arxiv.org/abs/2103.06583>`__\n\nPositive-Negative Momentum\n--------------------------\n\n+--------------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/positive_negative_momentum.png  |\n+--------------------------------------------------------------------------------------------------------------------+\n\n-  code : `github <https://github.com/zeke-xie/Positive-Negative-Momentum>`__\n-  paper : `arXiv <https://arxiv.org/abs/2103.17182>`__\n\nLinear learning rate warmup\n---------------------------\n\n+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/linear_lr_warmup.png  |\n+----------------------------------------------------------------------------------------------------------+\n\n-  paper : `arXiv <https://arxiv.org/abs/1910.04209>`__\n\nStable weight decay\n-------------------\n\n+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/stable_weight_decay.png  |\n+-------------------------------------------------------------------------------------------------------------+\n\n-  code : `github <https://github.com/zeke-xie/stable-weight-decay-regularization>`__\n-  paper : `arXiv <https://arxiv.org/abs/2011.11152>`__\n\nExplore-exploit learning rate schedule\n--------------------------------------\n\n+---------------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/explore_exploit_lr_schedule.png  |\n+---------------------------------------------------------------------------------------------------------------------+\n\n-  code : `github <https://github.com/nikhil-iyer-97/wide-minima-density-hypothesis>`__\n-  paper : `arXiv <https://arxiv.org/abs/2003.03977>`__\n\nLookahead\n---------\n\n| ``k`` steps forward, 1 step back. ``Lookahead`` consisting of keeping an exponential moving average of the weights that is\n| updated and substituted to the current weights every ``k_{lookahead}`` steps (5 by default",
      "https://arxiv.org/abs/2103.17182>`__\n\nLinear learning rate warmup\n---------------------------\n\n+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/linear_lr_warmup.png  |\n+----------------------------------------------------------------------------------------------------------+\n\n-  paper : `arXiv <https://arxiv.org/abs/1910.04209>`__\n\nStable weight decay\n-------------------\n\n+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/stable_weight_decay.png  |\n+-------------------------------------------------------------------------------------------------------------+\n\n-  code : `github <https://github.com/zeke-xie/stable-weight-decay-regularization>`__\n-  paper : `arXiv <https://arxiv.org/abs/2011.11152>`__\n\nExplore-exploit learning rate schedule\n--------------------------------------\n\n+---------------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/explore_exploit_lr_schedule.png  |\n+---------------------------------------------------------------------------------------------------------------------+\n\n-  code : `github <https://github.com/nikhil-iyer-97/wide-minima-density-hypothesis>`__\n-  paper : `arXiv <https://arxiv.org/abs/2003.03977>`__\n\nLookahead\n---------\n\n| ``k`` steps forward, 1 step back. ``Lookahead`` consisting of keeping an exponential moving average of the weights that is\n| updated and substituted to the current weights every ``k_{lookahead}`` steps (5 by default",
      "https://arxiv.org/abs/1910.04209>`__\n\nStable weight decay\n-------------------\n\n+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/stable_weight_decay.png  |\n+-------------------------------------------------------------------------------------------------------------+\n\n-  code : `github <https://github.com/zeke-xie/stable-weight-decay-regularization>`__\n-  paper : `arXiv <https://arxiv.org/abs/2011.11152>`__\n\nExplore-exploit learning rate schedule\n--------------------------------------\n\n+---------------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/explore_exploit_lr_schedule.png  |\n+---------------------------------------------------------------------------------------------------------------------+\n\n-  code : `github <https://github.com/nikhil-iyer-97/wide-minima-density-hypothesis>`__\n-  paper : `arXiv <https://arxiv.org/abs/2003.03977>`__\n\nLookahead\n---------\n\n| ``k`` steps forward, 1 step back. ``Lookahead`` consisting of keeping an exponential moving average of the weights that is\n| updated and substituted to the current weights every ``k_{lookahead}`` steps (5 by default",
      "https://arxiv.org/abs/2011.11152>`__\n\nExplore-exploit learning rate schedule\n--------------------------------------\n\n+---------------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/explore_exploit_lr_schedule.png  |\n+---------------------------------------------------------------------------------------------------------------------+\n\n-  code : `github <https://github.com/nikhil-iyer-97/wide-minima-density-hypothesis>`__\n-  paper : `arXiv <https://arxiv.org/abs/2003.03977>`__\n\nLookahead\n---------\n\n| ``k`` steps forward, 1 step back. ``Lookahead`` consisting of keeping an exponential moving average of the weights that is\n| updated and substituted to the current weights every ``k_{lookahead}`` steps (5 by default",
      "https://arxiv.org/abs/2003.03977>`__\n\nLookahead\n---------\n\n| ``k`` steps forward, 1 step back. ``Lookahead`` consisting of keeping an exponential moving average of the weights that is\n| updated and substituted to the current weights every ``k_{lookahead}`` steps (5 by default",
      "https://arxiv.org/abs/1907.08610v2>`__\n\nChebyshev learning rate schedule\n--------------------------------\n\nAcceleration via Fractal Learning Rate Schedules\n\n-  paper : `arXiv <https://arxiv.org/abs/2103.01338v1>`__\n\n(Adaptive",
      "https://arxiv.org/abs/2103.01338v1>`__\n\n(Adaptive",
      "https://arxiv.org/abs/2010.01412>`__\n-  ASAM paper : `paper <https://arxiv.org/abs/2102.11600>`__\n-  A/SAM code : `github <https://github.com/davda54/sam>`__\n\nOn the Convergence of Adam and Beyond\n-------------------------------------\n\n- paper : `paper <https://openreview.net/forum?id=ryQu7f-RZ>`__\n\nGradient Surgery for Multi-Task Learning\n----------------------------------------\n\n- paper : `paper <https://arxiv.org/abs/2001.06782>`__\n\nCitations\n---------\n\nAdamP\n\n::\n\n    @inproceedings{heo2021adamp,\n        title={AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights},\n        author={Heo, Byeongho and Chun, Sanghyuk and Oh, Seong Joon and Han, Dongyoon and Yun, Sangdoo and Kim, Gyuwan and Uh, Youngjung and Ha, Jung-Woo},\n        year={2021},\n        booktitle={International Conference on Learning Representations (ICLR",
      "https://arxiv.org/abs/2102.11600>`__\n-  A/SAM code : `github <https://github.com/davda54/sam>`__\n\nOn the Convergence of Adam and Beyond\n-------------------------------------\n\n- paper : `paper <https://openreview.net/forum?id=ryQu7f-RZ>`__\n\nGradient Surgery for Multi-Task Learning\n----------------------------------------\n\n- paper : `paper <https://arxiv.org/abs/2001.06782>`__\n\nCitations\n---------\n\nAdamP\n\n::\n\n    @inproceedings{heo2021adamp,\n        title={AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights},\n        author={Heo, Byeongho and Chun, Sanghyuk and Oh, Seong Joon and Han, Dongyoon and Yun, Sangdoo and Kim, Gyuwan and Uh, Youngjung and Ha, Jung-Woo},\n        year={2021},\n        booktitle={International Conference on Learning Representations (ICLR",
      "https://arxiv.org/abs/2001.06782>`__\n\nCitations\n---------\n\nAdamP\n\n::\n\n    @inproceedings{heo2021adamp,\n        title={AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights},\n        author={Heo, Byeongho and Chun, Sanghyuk and Oh, Seong Joon and Han, Dongyoon and Yun, Sangdoo and Kim, Gyuwan and Uh, Youngjung and Ha, Jung-Woo},\n        year={2021},\n        booktitle={International Conference on Learning Representations (ICLR",
      "https://arxiv.org/abs/2102.06171",
      "https://arxiv.org/abs/2103.01338",
      "https://arxiv.org/abs/1907.08610",
      "https://arxiv.org/abs/2103.17182",
      "https://arxiv.org/abs/2003.03977",
      "https://arxiv.org/abs/1910.04209",
      "https://arxiv.org/abs/2011.11152",
      "https://arxiv.org/abs/1908.00700",
      "https://arxiv.org/abs/2101.11075",
      "https://arxiv.org/abs/2006.00719",
      "https://arxiv.org/abs/2010.07468",
      "https://arxiv.org/abs/2010.01412",
      "https://arxiv.org/abs/2102.11600",
      "https://arxiv.org/abs/1904.09237",
      "https://arxiv.org/abs/2001.06782",
      "https://arxiv.org/abs/2110.10828"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "AdamP\n\n::\n\n    @inproceedings{heo2021adamp,\n        title={AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights},\n        author={Heo, Byeongho and Chun, Sanghyuk and Oh, Seong Joon and Han, Dongyoon and Yun, Sangdoo and Kim, Gyuwan and Uh, Youngjung and Ha, Jung-Woo},\n        year={2021},\n        booktitle={International Conference on Learning Representations (ICLR)},\n    }\n\nAdaptive Gradient Clipping (AGC)\n\n::\n\n    @article{brock2021high,\n      author={Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan},\n      title={High-Performance Large-Scale Image Recognition Without Normalization},\n      journal={arXiv preprint arXiv:2102.06171},\n      year={2021}\n    }\n\nChebyshev LR Schedules\n\n::\n\n    @article{agarwal2021acceleration,\n      title={Acceleration via Fractal Learning Rate Schedules},\n      author={Agarwal, Naman and Goel, Surbhi and Zhang, Cyril},\n      journal={arXiv preprint arXiv:2103.01338},\n      year={2021}\n    }\n\nGradient Centralization (GC)\n\n::\n\n    @inproceedings{yong2020gradient,\n      title={Gradient centralization: A new optimization technique for deep neural networks},\n      author={Yong, Hongwei and Huang, Jianqiang and Hua, Xiansheng and Zhang, Lei},\n      booktitle={European Conference on Computer Vision},\n      pages={635--652},\n      year={2020},\n      organization={Springer}\n    }\n\nLookahead\n\n::\n\n    @article{zhang2019lookahead,\n      title={Lookahead optimizer: k steps forward, 1 step back},\n      author={Zhang, Michael R and Lucas, James and Hinton, Geoffrey and Ba, Jimmy},\n      journal={arXiv preprint arXiv:1907.08610},\n      year={2019}\n    }\n\nRAdam\n\n::\n\n    @inproceedings{liu2019radam,\n     author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},\n     booktitle = {Proceedings of the Eighth International Conference on Learning Representations (ICLR 2020)},\n     month = {April},\n     title = {On the Variance of the Adaptive Learning Rate and Beyond},\n     year = {2020}\n    }\n\nNorm Loss\n\n::\n\n    @inproceedings{georgiou2021norm,\n      title={Norm Loss: An efficient yet effective regularization method for deep neural networks},\n      author={Georgiou, Theodoros and Schmitt, Sebastian and B{\\\"a}ck, Thomas and Chen, Wei and Lew, Michael},\n      booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},\n      pages={8812--8818},\n      year={2021},\n      organization={IEEE}\n    }\n\nPositive-Negative Momentum\n\n::\n\n    @article{xie2021positive,\n      title={Positive-Negative Momentum: Manipulating Stochastic Gradient Noise to Improve Generalization},\n      author={Xie, Zeke and Yuan, Li and Zhu, Zhanxing and Sugiyama, Masashi},\n      journal={arXiv preprint arXiv:2103.17182},\n      year={2021}\n    }\n\nExplore-Exploit learning rate schedule\n\n::\n\n    @article{iyer2020wide,\n      title={Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule},\n      author={Iyer, Nikhil and Thejas, V and Kwatra, Nipun and Ramjee, Ramachandran and Sivathanu, Muthian},\n      journal={arXiv preprint arXiv:2003.03977},\n      year={2020}\n    }\n\nLinear learning-rate warm-up\n\n::\n\n    @article{ma2019adequacy,\n      title={On the adequacy of untuned warmup for adaptive optimization},\n      author={Ma, Jerry and Yarats, Denis},\n      journal={arXiv preprint arXiv:1910.04209},\n      volume={7},\n      year={2019}\n    }\n\nStable weight decay\n\n::\n\n    @article{xie2020stable,\n      title={Stable weight decay regularization},\n      author={Xie, Zeke and Sato, Issei and Sugiyama, Masashi},\n      journal={arXiv preprint arXiv:2011.11152},\n      year={2020}\n    }\n\nSoftplus transformation\n\n::\n\n    @article{tong2019calibrating,\n      title={Calibrating the adaptive learning rate to improve convergence of adam},\n      author={Tong, Qianqian and Liang, Guannan and Bi, Jinbo},\n      journal={arXiv preprint arXiv:1908.00700},\n      year={2019}\n    }\n\nMADGRAD\n\n::\n\n    @article{defazio2021adaptivity,\n      title={Adaptivity without compromise: a momentumized, adaptive, dual averaged gradient method for stochastic optimization},\n      author={Defazio, Aaron and Jelassi, Samy},\n      journal={arXiv preprint arXiv:2101.11075},\n      year={2021}\n    }\n\nAdaHessian\n\n::\n\n    @article{yao2020adahessian,\n      title={ADAHESSIAN: An adaptive second order optimizer for machine learning},\n      author={Yao, Zhewei and Gholami, Amir and Shen, Sheng and Mustafa, Mustafa and Keutzer, Kurt and Mahoney, Michael W},\n      journal={arXiv preprint arXiv:2006.00719},\n      year={2020}\n    }\n\nAdaBound\n\n::\n\n    @inproceedings{Luo2019AdaBound,\n      author = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},\n      title = {Adaptive Gradient Methods with Dynamic Bound of Learning Rate},\n      booktitle = {Proceedings of the 7th International Conference on Learning Representations},\n      month = {May},\n      year = {2019},\n      address = {New Orleans, Louisiana}\n    }\n\nAdaBelief\n\n::\n\n    @article{zhuang2020adabelief,\n      title={Adabelief optimizer: Adapting stepsizes by the belief in observed gradients},\n      author={Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James S},\n      journal={arXiv preprint arXiv:2010.07468},\n      year={2020}\n    }\n\nSharpness-Aware Minimization\n\n::\n\n    @article{foret2020sharpness,\n      title={Sharpness-aware minimization for efficiently improving generalization},\n      author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},\n      journal={arXiv preprint arXiv:2010.01412},\n      year={2020}\n    }\n\nAdaptive Sharpness-Aware Minimization\n\n::\n\n    @article{kwon2021asam,\n      title={ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks},\n      author={Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},\n      journal={arXiv preprint arXiv:2102.11600},\n      year={2021}\n    }\n\ndiffGrad\n\n::\n\n    @article{dubey2019diffgrad,\n      title={diffgrad: An optimization method for convolutional neural networks},\n      author={Dubey, Shiv Ram and Chakraborty, Soumendu and Roy, Swalpa Kumar and Mukherjee, Snehasis and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},\n      journal={IEEE transactions on neural networks and learning systems},\n      volume={31},\n      number={11},\n      pages={4500--4511},\n      year={2019},\n      publisher={IEEE}\n    }\n\nOn the Convergence of Adam and Beyond\n\n::\n\n    @article{reddi2019convergence,\n      title={On the convergence of adam and beyond},\n      author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},\n      journal={arXiv preprint arXiv:1904.09237},\n      year={2019}\n    }\n\nGradient Surgery for Multi-Task Learning\n\n::\n\n    @article{yu2020gradient,\n      title={Gradient surgery for multi-task learning},\n      author={Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},\n      journal={arXiv preprint arXiv:2001.06782},\n      year={2020}\n    }\n\nAdamD: Improved bias-correction in Adam\n\n::\n\n    @article{john2021adamd,\n      title={AdamD: Improved bias-correction in Adam},\n      author={John, John St},\n      journal={arXiv preprint arXiv:2110.10828},\n      year={2021}\n    }\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{john2021adamd,\n  title={AdamD: Improved bias-correction in Adam},\n  author={John, John St},\n  journal={arXiv preprint arXiv:2110.10828},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{yu2020gradient,\n  title={Gradient surgery for multi-task learning},\n  author={Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},\n  journal={arXiv preprint arXiv:2001.06782},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{reddi2019convergence,\n  title={On the convergence of adam and beyond},\n  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},\n  journal={arXiv preprint arXiv:1904.09237},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{dubey2019diffgrad,\n  title={diffgrad: An optimization method for convolutional neural networks},\n  author={Dubey, Shiv Ram and Chakraborty, Soumendu and Roy, Swalpa Kumar and Mukherjee, Snehasis and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},\n  journal={IEEE transactions on neural networks and learning systems},\n  volume={31},\n  number={11},\n  pages={4500--4511},\n  year={2019},\n  publisher={IEEE}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{kwon2021asam,\n  title={ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks},\n  author={Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},\n  journal={arXiv preprint arXiv:2102.11600},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{foret2020sharpness,\n  title={Sharpness-aware minimization for efficiently improving generalization},\n  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},\n  journal={arXiv preprint arXiv:2010.01412},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhuang2020adabelief,\n  title={Adabelief optimizer: Adapting stepsizes by the belief in observed gradients},\n  author={Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James S},\n  journal={arXiv preprint arXiv:2010.07468},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{Luo2019AdaBound,\n  author = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},\n  title = {Adaptive Gradient Methods with Dynamic Bound of Learning Rate},\n  booktitle = {Proceedings of the 7th International Conference on Learning Representations},\n  month = {May},\n  year = {2019},\n  address = {New Orleans, Louisiana}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{yao2020adahessian,\n  title={ADAHESSIAN: An adaptive second order optimizer for machine learning},\n  author={Yao, Zhewei and Gholami, Amir and Shen, Sheng and Mustafa, Mustafa and Keutzer, Kurt and Mahoney, Michael W},\n  journal={arXiv preprint arXiv:2006.00719},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{defazio2021adaptivity,\n  title={Adaptivity without compromise: a momentumized, adaptive, dual averaged gradient method for stochastic optimization},\n  author={Defazio, Aaron and Jelassi, Samy},\n  journal={arXiv preprint arXiv:2101.11075},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{tong2019calibrating,\n  title={Calibrating the adaptive learning rate to improve convergence of adam},\n  author={Tong, Qianqian and Liang, Guannan and Bi, Jinbo},\n  journal={arXiv preprint arXiv:1908.00700},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{xie2020stable,\n  title={Stable weight decay regularization},\n  author={Xie, Zeke and Sato, Issei and Sugiyama, Masashi},\n  journal={arXiv preprint arXiv:2011.11152},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{ma2019adequacy,\n  title={On the adequacy of untuned warmup for adaptive optimization},\n  author={Ma, Jerry and Yarats, Denis},\n  journal={arXiv preprint arXiv:1910.04209},\n  volume={7},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{iyer2020wide,\n  title={Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule},\n  author={Iyer, Nikhil and Thejas, V and Kwatra, Nipun and Ramjee, Ramachandran and Sivathanu, Muthian},\n  journal={arXiv preprint arXiv:2003.03977},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{xie2021positive,\n  title={Positive-Negative Momentum: Manipulating Stochastic Gradient Noise to Improve Generalization},\n  author={Xie, Zeke and Yuan, Li and Zhu, Zhanxing and Sugiyama, Masashi},\n  journal={arXiv preprint arXiv:2103.17182},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{georgiou2021norm,\n  title={Norm Loss: An efficient yet effective regularization method for deep neural networks},\n  author={Georgiou, Theodoros and Schmitt, Sebastian and B{\\\"a}ck, Thomas and Chen, Wei and Lew, Michael},\n  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},\n  pages={8812--8818},\n  year={2021},\n  organization={IEEE}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{liu2019radam,\n author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},\n booktitle = {Proceedings of the Eighth International Conference on Learning Representations (ICLR 2020)},\n month = {April},\n title = {On the Variance of the Adaptive Learning Rate and Beyond},\n year = {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhang2019lookahead,\n  title={Lookahead optimizer: k steps forward, 1 step back},\n  author={Zhang, Michael R and Lucas, James and Hinton, Geoffrey and Ba, Jimmy},\n  journal={arXiv preprint arXiv:1907.08610},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{yong2020gradient,\n  title={Gradient centralization: A new optimization technique for deep neural networks},\n  author={Yong, Hongwei and Huang, Jianqiang and Hua, Xiansheng and Zhang, Lei},\n  booktitle={European Conference on Computer Vision},\n  pages={635--652},\n  year={2020},\n  organization={Springer}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{agarwal2021acceleration,\n  title={Acceleration via Fractal Learning Rate Schedules},\n  author={Agarwal, Naman and Goel, Surbhi and Zhang, Cyril},\n  journal={arXiv preprint arXiv:2103.01338},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{brock2021high,\n  author={Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan},\n  title={High-Performance Large-Scale Image Recognition Without Normalization},\n  journal={arXiv preprint arXiv:2102.06171},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{heo2021adamp,\n    title={AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights},\n    author={Heo, Byeongho and Chun, Sanghyuk and Oh, Seong Joon and Han, Dongyoon and Yun, Sangdoo and Kim, Gyuwan and Uh, Youngjung and Ha, Jung-Woo},\n    year={2021},\n    booktitle={International Conference on Learning Representations (ICLR)},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9999896920242366
      ],
      "excerpt": "| AdaBelief    | Adapting Stepsizes by the Belief in Observed Gradients                               | github &lt;https://github.com/juntang-zhuang/Adabelief-Optimizer&gt;                | https://arxiv.org/abs/2010.07468 &lt;https://arxiv.org/abs/2010.07468&gt;                       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8326434735942855
      ],
      "excerpt": "| AdaBound     | Adaptive Gradient Methods with Dynamic Bound of Learning Rate                        | github &lt;https://github.com/Luolc/AdaBound/blob/master/adabound/adabound.py&gt;   | https://openreview.net/forum?id=Bkg3g2R9FX &lt;https://openreview.net/forum?id=Bkg3g2R9FX&gt;   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999986554135434
      ],
      "excerpt": "| AdaHessian   | An Adaptive Second Order Optimizer for Machine Learning                              | github &lt;https://github.com/amirgholami/adahessian&gt;                            | https://arxiv.org/abs/2006.00719 &lt;https://arxiv.org/abs/2006.00719&gt;                       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998860528097746
      ],
      "excerpt": "| AdamD        | Improved bias-correction in Adam                                                     |                                                                                   | https://arxiv.org/abs/2110.10828 &lt;https://arxiv.org/abs/2110.10828&gt;                       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999587494603137
      ],
      "excerpt": "| AdamP        | Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights         | github &lt;https://github.com/clovaai/AdamP&gt;                                     | https://arxiv.org/abs/2006.08217 &lt;https://arxiv.org/abs/2006.08217&gt;                       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999865865883125
      ],
      "excerpt": "| diffGrad     | An Optimization Method for Convolutional Neural Networks                             | github &lt;https://github.com/shivram1987/diffGrad&gt;                              | https://arxiv.org/abs/1909.11015v3 &lt;https://arxiv.org/abs/1909.11015v3&gt;                   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999999150755789
      ],
      "excerpt": "| MADGRAD      | A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic               | github &lt;https://github.com/facebookresearch/madgrad&gt;                          | https://arxiv.org/abs/2101.11075 &lt;https://arxiv.org/abs/2101.11075&gt;                       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999939008818165
      ],
      "excerpt": "| RAdam        | On the Variance of the Adaptive Learning Rate and Beyond                             | github &lt;https://github.com/LiyuanLucasLiu/RAdam&gt;                              | https://arxiv.org/abs/1908.03265 &lt;https://arxiv.org/abs/1908.03265&gt;                       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.958213997427973
      ],
      "excerpt": "| Ranger       | a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer   | github &lt;https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer&gt;          | https://bit.ly/3zyspC3 &lt;https://bit.ly/3zyspC3&gt;                                           | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999996262796009
      ],
      "excerpt": "| Ranger21     | a synergistic deep learning optimizer                                                | github &lt;https://github.com/lessw2020/Ranger21&gt;                                | https://arxiv.org/abs/2106.13731 &lt;https://arxiv.org/abs/2106.13731&gt;__                       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "| Lookahead                             | Chebyshev learning rate schedule         | (Adaptive) Sharpness-Aware Minimization | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8889756546478684
      ],
      "excerpt": "| On the Convergence of Adam and Beyond | Gradient Surgery for Multi-Task Learning |                                            | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9761779635198798,
        0.9997775598278186
      ],
      "excerpt": "code : github &lt;https://github.com/deepmind/deepmind-research/tree/master/nfnets&gt;__ \npaper : arXiv &lt;https://arxiv.org/abs/2102.06171&gt;__ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9575749443040075,
        0.9997775598278186
      ],
      "excerpt": "code : github &lt;https://github.com/Yonghongwei/Gradient-Centralization&gt;__ \npaper : arXiv &lt;https://arxiv.org/abs/2004.01461&gt;__ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997775598278186
      ],
      "excerpt": "paper : arXiv &lt;https://arxiv.org/abs/1908.00700&gt;__ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997775598278186
      ],
      "excerpt": "paper : arXiv &lt;https://arxiv.org/abs/2103.06583&gt;__ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9827986258409598,
        0.9997775598278186
      ],
      "excerpt": "code : github &lt;https://github.com/zeke-xie/Positive-Negative-Momentum&gt;__ \npaper : arXiv &lt;https://arxiv.org/abs/2103.17182&gt;__ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997775598278186
      ],
      "excerpt": "paper : arXiv &lt;https://arxiv.org/abs/1910.04209&gt;__ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9786155246510595,
        0.9997775598278186
      ],
      "excerpt": "code : github &lt;https://github.com/zeke-xie/stable-weight-decay-regularization&gt;__ \npaper : arXiv &lt;https://arxiv.org/abs/2011.11152&gt;__ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9345709139347155,
        0.9997775598278186
      ],
      "excerpt": "code : github &lt;https://github.com/nikhil-iyer-97/wide-minima-density-hypothesis&gt;__ \npaper : arXiv &lt;https://arxiv.org/abs/2003.03977&gt;__ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8771779615911968,
        0.9997775598278186
      ],
      "excerpt": "code : github &lt;https://github.com/alphadl/lookahead.pytorch&gt;__ \npaper : arXiv &lt;https://arxiv.org/abs/1907.08610v2&gt;__ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997775598278186,
        0.8422862053358879
      ],
      "excerpt": "paper : arXiv &lt;https://arxiv.org/abs/2103.01338v1&gt;__ \n| Sharpness-Aware Minimization (SAM) simultaneously minimizes loss value and loss sharpness. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9995168170647625,
        0.9987778024492702,
        0.9575749443040075,
        0.9832486572795036,
        0.9987778024492702
      ],
      "excerpt": "SAM paper : paper &lt;https://arxiv.org/abs/2010.01412&gt;__ \nASAM paper : paper &lt;https://arxiv.org/abs/2102.11600&gt;__ \nA/SAM code : github &lt;https://github.com/davda54/sam&gt;__ \npaper : paper &lt;https://openreview.net/forum?id=ryQu7f-RZ&gt;__ \npaper : paper &lt;https://arxiv.org/abs/2001.06782&gt;__ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9762144469590832
      ],
      "excerpt": ".. |workflow| image:: https://github.com/kozistr/pytorch_optimizer/actions/workflows/ci.yml/badge.svg?branch=main \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kozistr/pytorch_optimizer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-21T05:52:24Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-08T04:08:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9528725173938954,
        0.9683237718879083
      ],
      "excerpt": "| Bunch of optimizer implementations in PyTorch with clean-code, strict types. Also, including useful optimization ideas. \n| Most of the implementations are based on the original paper, but I added some tweaks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8099670728533422
      ],
      "excerpt": "| Optimizer    | Description                                                                            | Official Code                                                                     | Paper                                                                                         | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9621093226672568,
        0.9092075660111981
      ],
      "excerpt": "of the ideas are applied in Ranger21 optimizer. \nAlso, most of the captures are taken from Ranger21 paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.963124161227541
      ],
      "excerpt": "| On the Convergence of Adam and Beyond | Gradient Surgery for Multi-Task Learning |                                            | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9668224270832776,
        0.8505429400322503
      ],
      "excerpt": "| k steps forward, 1 step back. Lookahead consisting of keeping an exponential moving average of the weights that is \n| updated and substituted to the current weights every k_{lookahead} steps (5 by default). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Bunch of optimizer implementations in PyTorch",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "https://pytorch-optimizers.readthedocs.io/en/latest/\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kozistr/pytorch_optimizer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Sat, 25 Dec 2021 13:32:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kozistr/pytorch_optimizer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kozistr/pytorch_optimizer",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/kozistr/pytorch_optimizer/tree/main/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9226155860684281
      ],
      "excerpt": "| |workflow| |Documentation Status| |PyPI version| |PyPi download| |black| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924198124826511
      ],
      "excerpt": "| Highly inspired by pytorch-optimizer &lt;https://github.com/jettify/pytorch-optimizer&gt;__. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8664346695112637
      ],
      "excerpt": "code : github &lt;https://github.com/alphadl/lookahead.pytorch&gt;__ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9054686736669485,
        0.828212074001604,
        0.9471790056154378
      ],
      "excerpt": ".. |Documentation Status| image:: https://readthedocs.org/projects/pytorch-optimizers/badge/?version=latest \n   :target: https://pytorch-optimizers.readthedocs.io/en/latest/?badge=latest \n.. |PyPI version| image:: https://badge.fury.io/py/pytorch-optimizer.svg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9579370850914374
      ],
      "excerpt": ".. |PyPi download| image:: https://img.shields.io/pypi/dm/pytorch-optimizer?style=plastic \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kozistr/pytorch_optimizer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-optimizer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch_optimizer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kozistr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kozistr/pytorch_optimizer/blob/main/README.rst",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "",
        "dateCreated": "2021-11-29T14:14:52Z",
        "datePublished": "2021-11-29T14:15:14Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.2.2",
        "name": "pytorch-optimizer v0.2.2",
        "tag_name": "v0.2.2",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.2.2",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/54243638",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.2.2"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "",
        "dateCreated": "2021-11-22T01:49:54Z",
        "datePublished": "2021-11-22T01:50:16Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.2.1",
        "name": "pytorch-optimizer v0.2.1",
        "tag_name": "v0.2.1",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.2.1",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/53807385",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.2.1"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "",
        "dateCreated": "2021-11-15T07:39:04Z",
        "datePublished": "2021-11-15T07:40:28Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.2.0",
        "name": "pytorch-optimizer v0.2.0",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.2.0",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/53365129",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "",
        "dateCreated": "2021-10-09T11:59:09Z",
        "datePublished": "2021-10-09T11:59:29Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.1.1",
        "name": "pytorch-optimizer v0.1.1",
        "tag_name": "v0.1.1",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.1.1",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/51077130",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.1.1"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "",
        "dateCreated": "2021-10-06T13:34:43Z",
        "datePublished": "2021-10-06T13:37:16Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.1.0",
        "name": "pytorch-optimizer v0.1.0",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.1.0",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/50891133",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.1.0"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "",
        "dateCreated": "2021-10-06T13:22:33Z",
        "datePublished": "2021-10-06T13:27:34Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.0.11",
        "name": "pytorch-optimizer v0.0.11",
        "tag_name": "v0.0.11",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.0.11",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/50890293",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.0.11"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "* Support GC on `AdamP` optimizer : [code](https://github.com/kozistr/pytorch_optimizer/blob/main/pytorch_optimizer/adamp.py#L153)",
        "dateCreated": "2021-09-25T13:25:08Z",
        "datePublished": "2021-09-25T13:25:30Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.0.10",
        "name": "pytorch-optimizer v0.0.10",
        "tag_name": "v0.0.10",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.0.10",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/50251066",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.0.10"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "",
        "dateCreated": "2021-09-23T10:32:41Z",
        "datePublished": "2021-09-23T10:32:58Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.0.9",
        "name": "pytorch-optimizer v0.0.9",
        "tag_name": "v0.0.9",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.0.9",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/50122066",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.0.9"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "",
        "dateCreated": "2021-09-23T06:40:21Z",
        "datePublished": "2021-09-23T06:40:43Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.0.8",
        "name": "pytorch-optimizer v0.0.8",
        "tag_name": "v0.0.8",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.0.8",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/50107441",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.0.8"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "",
        "dateCreated": "2021-09-22T13:51:50Z",
        "datePublished": "2021-09-22T13:52:09Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.0.7",
        "name": "pytorch-optimizer v0.0.7",
        "tag_name": "v0.0.7",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.0.7",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/50060497",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.0.7"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "* Implement Sharpness-Aware Minimization (SAM) optimizer\r\n    * Support Adaptive SAM",
        "dateCreated": "2021-09-22T08:27:50Z",
        "datePublished": "2021-09-22T08:28:10Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.0.6",
        "name": "pytorch-optimizer v0.0.6",
        "tag_name": "v0.0.6",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.0.6",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/50038939",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.0.6"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "* Combine `AdaBoundW` into `AdaBound` optimizer with `weight_decouple` parameter.\r\n* Implement `AdaBelief` optimizer\r\n    * Support fp16\r\n    * Support `weight_decouple` with `AdamW` scheme\r\n    * Support rectified update similar to `RAdam` \r\n",
        "dateCreated": "2021-09-22T07:41:47Z",
        "datePublished": "2021-09-22T07:42:06Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.0.5",
        "name": "pytorch-optimizer v0.0.5",
        "tag_name": "v0.0.5",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.0.5",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/50036354",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.0.5"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "",
        "dateCreated": "2021-09-22T06:59:01Z",
        "datePublished": "2021-09-22T06:59:19Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.0.4",
        "name": "pytorch-optimizer v0.0.4",
        "tag_name": "v0.0.4",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.0.4",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/50034125",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.0.4"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "",
        "dateCreated": "2021-09-22T05:58:06Z",
        "datePublished": "2021-09-22T05:58:22Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.0.3",
        "name": "pytorch-optimizer v0.0.3",
        "tag_name": "v0.0.3",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.0.3",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/50031893",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.0.3"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "",
        "dateCreated": "2021-09-21T16:09:24Z",
        "datePublished": "2021-09-21T16:09:40Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.0.2",
        "name": "Release v0.0.2",
        "tag_name": "v0.0.2",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.0.2",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/49997985",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.0.2"
      },
      {
        "authorType": "User",
        "author_name": "kozistr",
        "body": "",
        "dateCreated": "2021-09-21T13:52:01Z",
        "datePublished": "2021-09-21T13:52:26Z",
        "html_url": "https://github.com/kozistr/pytorch_optimizer/releases/tag/v0.0.1",
        "name": "Release v0.0.1",
        "tag_name": "v0.0.1",
        "tarball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/tarball/v0.0.1",
        "url": "https://api.github.com/repos/kozistr/pytorch_optimizer/releases/49986526",
        "zipball_url": "https://api.github.com/repos/kozistr/pytorch_optimizer/zipball/v0.0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 19,
      "date": "Sat, 25 Dec 2021 13:32:49 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "optimizer",
      "pytorch",
      "ranger",
      "agc",
      "gc",
      "chebyshev",
      "adamp",
      "radam",
      "madgrad",
      "adahessian",
      "adabound",
      "adaboundw",
      "adabelief",
      "sam",
      "asam",
      "lookahead",
      "deep-learning",
      "diffgrad",
      "diffrgrad",
      "adamd"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Install\n~~~~~~~\n\n::\n\n    $ pip3 install pytorch-optimizer\n\nSimple Usage\n~~~~~~~~~~~~\n\n::\n\n    from pytorch_optimizer import AdamP\n\n    ...\n    model = YourModel()\n    optimizer = AdamP(model.parameters())\n    ...\n\nor you can use optimizer loader, simply passing a name of the optimizer.\n\n::\n\n    from pytorch_optimizer import load_optimizers\n\n    ...\n    model = YourModel()\n    opt = load_optimizers(optimizer='adamp', use_fp16=True)\n    optimizer = opt(model.parameters())\n    ...\n\n",
      "technique": "Header extraction"
    }
  ]
}