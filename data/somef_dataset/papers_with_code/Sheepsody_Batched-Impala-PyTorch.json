{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1611.06256",
      "https://arxiv.org/abs/1802.01561",
      "https://arxiv.org/abs/1312.5602",
      "https://arxiv.org/abs/1602.01783",
      "https://arxiv.org/abs/1611.06256",
      "https://arxiv.org/abs/1802.01561",
      "https://arxiv.org/abs/1611.06256](https://arxiv.org/abs/1611.06256)) ;\n  * This method takes advantage of the GPU's parallelisation to improve the results of A3C. Namely, all the predictions and the loss computations are batched together to increase the throughput of the model. This could also be extended to the multi-GPU case.\n* **IMPALA**\n  * This framework was proposed in *IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures* ([https://arxiv.org/abs/1802.01561](https://arxiv.org/abs/1802.01561)) ;\n  * *V-Trace* (a novel actor-critic algorithm), was also introduced in this paper. When using asynchronous updates based on trajectories recorded by agents, it provides an off-policy correction that reduces the bias and the variance, while maintaining a high thoughput.\n* **PyTorch and Tensorboard**\n  * The implementation I proposed is based on the popular framework [PyTorch](https://pytorch.org/), develloped by Facebook. I used **Torchscripts** to improve the overall training speed, with significant improvements\n  * A process is dedicated to visualisation thanks to tensorboard, and intergrates various metrics (duration, cumulated reward, loss, batches rates, etc...);\n* And among the rest...\n  * **Gym Wrappers** for the [retro](https://github.com/openai/retro) environnement of OpenAI ;\n  * **Ram Locations** of the main variables when emulating Mario Kart (using [Bizhawk](https://github.com/TASVideos/BizHawk)) ;\n\n\n## Running the code\n\n### Installation\n\nTo run this project, you need to install **nvidia-docker**. Just follow the installation steps on the [official repository from nvidia](https://github.com/NVIDIA/nvidia-docker). You can also run the code directly on CPU, but I wouldn't recommand it, since it's coded with a GPU perspective.\n\n### Building the docker image\n\nThe project can be runned into a Docker Container, which contains all the dependencies for the project.\n\n```bash\n# Docker build the image\ndocker build -t KartRL:latest . \n\n# Docker run the container\n# You can also use volume to work on the code inside a container [-v [folder]:/App]\ndocker run -it --rm -p 6006:6006 KartRL:latest\n```\n\n### Running the code\n\n```bash\n# Running the code\npython run.py\n\n# Outputing the result (replace state and checkpoint if necessary)\npython record.py -s MarioCircuit.Act3 -p checkpoint.pt\n\n# Launch the tensorboard\ntensorboard --bind_all --port 6006 --logdir=./logs/\n```\n\n**Enjoy!** If you have any nice results, don't hesitate to message me!\n\n## Possible improvements\n\n- [ ] Replay Memory\n- [ ] Fix Callback process\n- [ ] Automatic tunning\n- [ ] Multi-GPU case\n- [ ] Comparaison with other algos\n- [ ] Tuning of the parameters\n\n\n## References :\n\n* Courses  \n  * [Reinforcement learning by David Silver (UCL)](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n* Papers\n  * Playing Atari with Deep Reinforcement Learning [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)\n  * Asynchronous Methods for Deep Reinforcement Learning [https://arxiv.org/abs/1602.01783](https://arxiv.org/abs/1602.01783)\n  * Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU [https://arxiv.org/abs/1611.06256](https://arxiv.org/abs/1611.06256)\n  * IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures [https://arxiv.org/abs/1802.01561](https://arxiv.org/abs/1802.01561)\n  * Great source for RL papers by OpenAI: [Spinning-up OpenAI](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)\n* Others \n  * [Bizhawk](https://github.com/TASVideos/BizHawk) : SNES emulator with ram watch\n  * [Maps](http://www.mariouniverse.com/maps-snes-smk/), [RAM map](https://datacrystal.romhacking.net/wiki/Super_Mario_Kart:RAM_map",
      "https://arxiv.org/abs/1802.01561](https://arxiv.org/abs/1802.01561)) ;\n  * *V-Trace* (a novel actor-critic algorithm), was also introduced in this paper. When using asynchronous updates based on trajectories recorded by agents, it provides an off-policy correction that reduces the bias and the variance, while maintaining a high thoughput.\n* **PyTorch and Tensorboard**\n  * The implementation I proposed is based on the popular framework [PyTorch](https://pytorch.org/), develloped by Facebook. I used **Torchscripts** to improve the overall training speed, with significant improvements\n  * A process is dedicated to visualisation thanks to tensorboard, and intergrates various metrics (duration, cumulated reward, loss, batches rates, etc...);\n* And among the rest...\n  * **Gym Wrappers** for the [retro](https://github.com/openai/retro) environnement of OpenAI ;\n  * **Ram Locations** of the main variables when emulating Mario Kart (using [Bizhawk](https://github.com/TASVideos/BizHawk)) ;\n\n\n## Running the code\n\n### Installation\n\nTo run this project, you need to install **nvidia-docker**. Just follow the installation steps on the [official repository from nvidia](https://github.com/NVIDIA/nvidia-docker). You can also run the code directly on CPU, but I wouldn't recommand it, since it's coded with a GPU perspective.\n\n### Building the docker image\n\nThe project can be runned into a Docker Container, which contains all the dependencies for the project.\n\n```bash\n# Docker build the image\ndocker build -t KartRL:latest . \n\n# Docker run the container\n# You can also use volume to work on the code inside a container [-v [folder]:/App]\ndocker run -it --rm -p 6006:6006 KartRL:latest\n```\n\n### Running the code\n\n```bash\n# Running the code\npython run.py\n\n# Outputing the result (replace state and checkpoint if necessary)\npython record.py -s MarioCircuit.Act3 -p checkpoint.pt\n\n# Launch the tensorboard\ntensorboard --bind_all --port 6006 --logdir=./logs/\n```\n\n**Enjoy!** If you have any nice results, don't hesitate to message me!\n\n## Possible improvements\n\n- [ ] Replay Memory\n- [ ] Fix Callback process\n- [ ] Automatic tunning\n- [ ] Multi-GPU case\n- [ ] Comparaison with other algos\n- [ ] Tuning of the parameters\n\n\n## References :\n\n* Courses  \n  * [Reinforcement learning by David Silver (UCL)](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n* Papers\n  * Playing Atari with Deep Reinforcement Learning [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)\n  * Asynchronous Methods for Deep Reinforcement Learning [https://arxiv.org/abs/1602.01783](https://arxiv.org/abs/1602.01783)\n  * Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU [https://arxiv.org/abs/1611.06256](https://arxiv.org/abs/1611.06256)\n  * IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures [https://arxiv.org/abs/1802.01561](https://arxiv.org/abs/1802.01561)\n  * Great source for RL papers by OpenAI: [Spinning-up OpenAI](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)\n* Others \n  * [Bizhawk](https://github.com/TASVideos/BizHawk) : SNES emulator with ram watch\n  * [Maps](http://www.mariouniverse.com/maps-snes-smk/), [RAM map](https://datacrystal.romhacking.net/wiki/Super_Mario_Kart:RAM_map",
      "https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)\n  * Asynchronous Methods for Deep Reinforcement Learning [https://arxiv.org/abs/1602.01783](https://arxiv.org/abs/1602.01783)\n  * Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU [https://arxiv.org/abs/1611.06256](https://arxiv.org/abs/1611.06256)\n  * IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures [https://arxiv.org/abs/1802.01561](https://arxiv.org/abs/1802.01561)\n  * Great source for RL papers by OpenAI: [Spinning-up OpenAI](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)\n* Others \n  * [Bizhawk](https://github.com/TASVideos/BizHawk) : SNES emulator with ram watch\n  * [Maps](http://www.mariouniverse.com/maps-snes-smk/), [RAM map](https://datacrystal.romhacking.net/wiki/Super_Mario_Kart:RAM_map",
      "https://arxiv.org/abs/1602.01783](https://arxiv.org/abs/1602.01783)\n  * Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU [https://arxiv.org/abs/1611.06256](https://arxiv.org/abs/1611.06256)\n  * IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures [https://arxiv.org/abs/1802.01561](https://arxiv.org/abs/1802.01561)\n  * Great source for RL papers by OpenAI: [Spinning-up OpenAI](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)\n* Others \n  * [Bizhawk](https://github.com/TASVideos/BizHawk) : SNES emulator with ram watch\n  * [Maps](http://www.mariouniverse.com/maps-snes-smk/), [RAM map](https://datacrystal.romhacking.net/wiki/Super_Mario_Kart:RAM_map",
      "https://arxiv.org/abs/1611.06256](https://arxiv.org/abs/1611.06256)\n  * IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures [https://arxiv.org/abs/1802.01561](https://arxiv.org/abs/1802.01561)\n  * Great source for RL papers by OpenAI: [Spinning-up OpenAI](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)\n* Others \n  * [Bizhawk](https://github.com/TASVideos/BizHawk) : SNES emulator with ram watch\n  * [Maps](http://www.mariouniverse.com/maps-snes-smk/), [RAM map](https://datacrystal.romhacking.net/wiki/Super_Mario_Kart:RAM_map",
      "https://arxiv.org/abs/1802.01561](https://arxiv.org/abs/1802.01561)\n  * Great source for RL papers by OpenAI: [Spinning-up OpenAI](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)\n* Others \n  * [Bizhawk](https://github.com/TASVideos/BizHawk) : SNES emulator with ram watch\n  * [Maps](http://www.mariouniverse.com/maps-snes-smk/), [RAM map](https://datacrystal.romhacking.net/wiki/Super_Mario_Kart:RAM_map"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Courses  \n  * [Reinforcement learning by David Silver (UCL)](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n* Papers\n  * Playing Atari with Deep Reinforcement Learning [arXiv:1312.5602](https://arxiv.org/abs/1312.5602)\n  * Asynchronous Methods for Deep Reinforcement Learning [arXiv:1602.01783](https://arxiv.org/abs/1602.01783)\n  * Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU [arXiv:1611.06256](https://arxiv.org/abs/1611.06256)\n  * IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures [arXiv:1802.01561](https://arxiv.org/abs/1802.01561)\n  * Great source for RL papers by OpenAI: [Spinning-up OpenAI](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)\n* Others \n  * [Bizhawk](https://github.com/TASVideos/BizHawk) : SNES emulator with ram watch\n  * [Maps](http://www.mariouniverse.com/maps-snes-smk/), [RAM map](https://datacrystal.romhacking.net/wiki/Super_Mario_Kart:RAM_map)",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Sheepsody/Batched-Impala-PyTorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-29T09:10:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T07:28:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9955725830066068,
        0.9280638773227196,
        0.9679155728258334
      ],
      "excerpt": "This is an implementation of IMPALA (with batching for single GPU case) in PyTorch. It also incorporates the Gym Wrappers to test the framework on SuperMarioKart-Snes. \nI trained an AI with this framework for 12 hours on my personnal computer, that unfortunalely suffers from a poor configuration (1060 and 8gb RAM), so only 2 async agents could be runned at a time. However, this still produces pretty nice results. \nThe configuration for this traning can be found in default.cfg. In this situation, only the MarioCircuit was used, with the circuits divided between a \"traning\" dataset (1~3) and a \"testing\" (unseen) dataset. The results are displayed bellow. You can also see the maps of the circuits that I used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9868591366812524
      ],
      "excerpt": "The main features of this project are : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9918322292353338
      ],
      "excerpt": "  * This method takes advantage of the GPU's parallelisation to improve the results of A3C. Namely, all the predictions and the loss computations are batched together to increase the throughput of the model. This could also be extended to the multi-GPU case. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8512935703448639,
        0.9887778261697255
      ],
      "excerpt": "  * This framework was proposed in IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures (arXiv:1802.01561) ; \n  * V-Trace (a novel actor-critic algorithm), was also introduced in this paper. When using asynchronous updates based on trajectories recorded by agents, it provides an off-policy correction that reduces the bias and the variance, while maintaining a high thoughput. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9796986696255232
      ],
      "excerpt": "  * The implementation I proposed is based on the popular framework PyTorch, develloped by Facebook. I used Torchscripts to improve the overall training speed, with significant improvements \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554025773396336,
        0.9983367987839078,
        0.9082962679454561,
        0.9780604325449144
      ],
      "excerpt": "* And among the rest... \n  * Gym Wrappers for the retro environnement of OpenAI ; \n  * Ram Locations of the main variables when emulating Mario Kart (using Bizhawk) ; \nThe project can be runned into a Docker Container, which contains all the dependencies for the project. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Reinforcement learning - Batched Impala - PyTorch - Mario Kart  ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Sheepsody/Batched-Impala-PyTorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Thu, 30 Dec 2021 02:54:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Sheepsody/Batched-Impala-PyTorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Sheepsody/Batched-Impala-PyTorch",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Sheepsody/Batched-Impala-PyTorch/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/Sheepsody/Batched-Impala-PyTorch/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run this project, you need to install **nvidia-docker**. Just follow the installation steps on the [official repository from nvidia](https://github.com/NVIDIA/nvidia-docker). You can also run the code directly on CPU, but I wouldn't recommand it, since it's coded with a GPU perspective.\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8571734051716672
      ],
      "excerpt": ": Docker run the container \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Sheepsody/Batched-Impala-PyTorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Reinforced learning on \u30de\u30ea\u30aa\u30ab\u30fc\u30c8 using Batched-IMPALA",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Batched-Impala-PyTorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Sheepsody",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Sheepsody/Batched-Impala-PyTorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n#: Running the code\npython run.py\n\n#: Outputing the result (replace state and checkpoint if necessary)\npython record.py -s MarioCircuit.Act3 -p checkpoint.pt\n\n#: Launch the tensorboard\ntensorboard --bind_all --port 6006 --logdir=./logs/\n```\n\n**Enjoy!** If you have any nice results, don't hesitate to message me!\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Thu, 30 Dec 2021 02:54:31 GMT"
    },
    "technique": "GitHub API"
  }
}