{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JakobHavtorn/nn",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-05-14T13:04:12Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-13T10:04:40Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.925801283965845,
        0.9649925941597369,
        0.9934757070771361,
        0.9064191722390998,
        0.9468610494343194
      ],
      "excerpt": "This is an implementation of some common neural network architectural modules using the Numerical Python (Numpy) library. \nThe overall modular structure is inspired by that of PyTorch. All network modules are children of a parent Module. Both layers, such as linear, convolutional, and recurrent, and nonlinear activation functions are implemented as subclasses of the Module class. Network models are also instantiated as subclasses of the Module class and hold their layers and activation functions as attributes, effectively forming a graph. \nConversely to PyTorch, the backwards and forward pass of any module is explicitly coded in Python using NumPy rather than in a C backend. This is obviously not competitive on performance, but served as nice personal exercises in deriving and implementing backpropagation for a range of different network layers. All code executes exclusively on the CPU. \nexamples: This folder contains examples of code used to train different networks on data. The models are defined in models.py, the data from torchvision``torchvision is part of PyTorch and contains among other things datasets for computer vision and data loader classes. is loaded using the method from loaders.py. Current examples are mnist\\_fnn.py and mnist\\_cnn.py. \nnn: This folder holds the main network modules. The names of the contained files are self-descriptive; for instance, the affine (linear) transformation of the FNN is defined as a class in linear.py. All modules are subclasses of the Module base class which defines common behaviour. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8312142454937986
      ],
      "excerpt": "Currently implemented modules are \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8919953112632298,
        0.8972577739308879
      ],
      "excerpt": "Implementation of weight decay according to http://arxiv.org/abs/1711.05101 and not as L2 regularization term added to gradient \nLayers on the roadmap for implementation are \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8695433958003913,
        0.9753528758108579,
        0.8837228403596576,
        0.9707667271210699
      ],
      "excerpt": "In the __init__ method, the network should have its layers and activation functions etc. added either as either named attributes or using the add_module method. The latter option is well suited in cases where many similar layers are added sequentially.  \nThe class should have the required forward and backward methods that define the forward and backward propagations using the forward and backward methods of the models modules. When the models are sequential, a simple for loop is easy to use. \nA nice blog post on neural networks in 100 lines of Python. \nSome inspiration has been found at the DTU PhD Deep Learning Summer School 2015, see website and github repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of neural network modules in numpy",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JakobHavtorn/nn/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 22 Dec 2021 03:03:55 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JakobHavtorn/nn/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "JakobHavtorn/nn",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\npip install --upgrade --editable .\n```\n\nor\n\n```bash\nconda env create -f environment.yml\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\nconda env create -f environment.yml\n```\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JakobHavtorn/nn/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Jakob Drachmann Havtorn\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "nn",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "nn",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "JakobHavtorn",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JakobHavtorn/nn/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 03:03:55 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Below is an example of how to construct an FNN classifier. The classifier has\n\n- variable input and output dimensions\n- variable number of hidden layers and dimensions\n- specifiable activation function\n- potential batchnorm and dropout layers\n\n```python\nclass FNNClassifier(nn.Module):\n    def __init__(self, in_features, out_classes, hidden_dims=[256, 128, 64], activation=nn.ReLU, batchnorm=False, dropout=False):\n        super(FNNClassifier, self).__init__()\n        dims = [in_features, *hidden_dims, out_classes]\n        for i in range(len(dims) - 1):\n            is_output_layer = i == len(dims) - 2\n            self.add_module(\"Linear\" + str(i), nn.Linear(dims[i], dims[i+1]))\n            if batchnorm and not is_output_layer:\n                self.add_module(\"BatchNorm\" + str(i), nn.BatchNorm1D(dims[i+1]))\n            if dropout and not is_output_layer:\n                self.add_module(\"Dropout\" + str(i), nn.Dropout(p=dropout))\n            if not is_output_layer:\n                self.add_module(\"Activation\" + str(i), activation())\n            else:\n                self.add_module(\"Activation\" + str(i), nn.Softmax())\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], -1)\n        for module in self._modules.values():\n            x = module.forward(x)\n        return x\n\n    def backward(self, delta):\n        for module in reversed(self._modules.values()):\n            delta = module.backward(delta)\n``` \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Below is an example of how to construct an CNN classifier. The classifier has\n\n- variable input and output dimensions\n- variable number of hidden layers and dimensions\n- specifiable activation function\n- potential batchnorm and dropout layers\n\nFor this classifier however, changing the convolutional layers require a corresponding change to the fully connected classifier layers. Alternatively, a completely convolutional model could be created.\n\n```python\nclass CNNClassifier(nn.Module):\n    def __init__(self, in_features, out_classes, feature_maps=[16, 32], hidden_dims=[512], activation=nn.ReLU, batchnorm=False, dropout=False):\n        super(CNNClassifier, self).__init__()\n        #: Convolutional layers\n        self.add_module(\"Convolutional0\", nn.Conv2D(1, feature_maps[0], kernel_size=(5, 5)))\n        self.add_module(\"Maxpool0\", nn.MaxPool2D(kernel_size=(2, 2), stride=2, padding=0))\n        self.add_module(\"Activation0\", activation())\n        self.add_module(\"Convolutional1\", nn.Conv2D(feature_maps[0], feature_maps[1], kernel_size=(5, 5)))\n        self.add_module(\"Maxpool1\", nn.MaxPool2D(kernel_size=(2, 2), stride=2, padding=0))\n        self.add_module(\"Activation1\", activation())\n        self.add_module(\"Flatten\", nn.Flatten())\n        #: Feedforward classifier\n        dims = [*hidden_dims, out_classes]\n        for i in range(len(dims) - 1):\n            is_output_layer = i == len(dims) - 2\n            if batchnorm:\n                self.add_module(\"BatchNorm\" + str(i), nn.BatchNorm1D(dims[i]))\n            self.add_module(\"Linear\" + str(i), nn.Linear(dims[i], dims[i+1]))\n            if dropout and not is_output_layer:\n                self.add_module(\"Dropout\" + str(i), nn.Dropout(p=dropout))\n            if not is_output_layer:\n                self.add_module(\"Activation\" + str(i+2), activation())\n            else:\n                self.add_module(\"Activation\" + str(i+2), nn.Softmax())\n\n    def forward(self, x):\n        for module in self._modules.values():\n            x = module.forward(x)\n        return x\n\n    def backward(self, delta):\n        for module in reversed(self._modules.values()):\n            delta = module.backward(delta)\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "In /examples, two MNIST examples has been created for testing purposes. \n\nThe above FNN classifier without batchnorm and dropout as well as the CNN classifier were trained. The FNN was overfitted on the training set while the convolutional architecture is much less prone to overfitting. Final performance was\n\n|               |  FNN |          | CNN  |          |\n| ------------- | ---- | -------- | ---- | -------- |\n| Data set      | Loss | Accuracy | Loss | Accuracy |\n| Training      | 0.01 | 99.97%   | 0.02 | 99.31%   |\n| Validation    | 0.07 | 97.95%   | 0.04 | 99.63%   |\n\nThe learning curves are seen below.\n\n",
      "technique": "Header extraction"
    }
  ]
}