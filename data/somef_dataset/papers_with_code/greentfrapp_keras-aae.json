{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.05644",
      "https://arxiv.org/abs/1312.6114",
      "https://arxiv.org/abs/1511.05644 (2015)](https://arxiv.org/abs/1511.05644) with Keras.\n\n## Summary\n\nThe Adversarial Autoencoder behaves similarly to [Variational Autoencoders](https://arxiv.org/abs/1312.6114), forcing the latent space of an autoencoder to follow a predefined prior. In the case of the Adversarial Autoencoder, this latent space can be defined arbitrarily and easily sampled and fed into the Discriminator in the network.\n\n<div>\n<img src=\"https://raw.githubusercontent.com/greentfrapp/keras-aae/master/images/aae_latent.png\" alt=\"Latent space from Adversarial Autoencoder\" width=\"whatever\" height=\"300px\" style=\"display: inline-block;\">\n<img src=\"https://raw.githubusercontent.com/greentfrapp/keras-aae/master/images/regular_latent.png\" alt=\"Latent space from regular Autoencoder\" width=\"whatever\" height=\"300px\" style=\"display: inline-block;\">\n</div>\n\n*The left image shows the latent space of an unseen MNIST test set after training with an Adversarial Autoencoder for 50 epochs, which follows a 2D Gaussian prior. Contrast this with the latent space of the regular Autoencoder trained under the same conditions, with a far more irregular latent distribution.*\n\n## Instructions\n\nTo train a model just run\n\n```\n$ python keras-aae.py --train\n```\n\nFor more parameters, run with `--help` flag.\n\nFor comparison with a regular autoencoder, run\n\n```\n$ python regular-ae.py --train --noadversarial\n``"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9985289520865075
      ],
      "excerpt": "Reproduces Adversarial Autoencoder architecture from Makhzani, Alireza, et al. \"Adversarial autoencoders.\" arXiv preprint arXiv:1511.05644 (2015) with Keras. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/greentfrapp/keras-aae",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-04-16T08:51:37Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-18T20:14:51Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The Adversarial Autoencoder behaves similarly to [Variational Autoencoders](https://arxiv.org/abs/1312.6114), forcing the latent space of an autoencoder to follow a predefined prior. In the case of the Adversarial Autoencoder, this latent space can be defined arbitrarily and easily sampled and fed into the Discriminator in the network.\n\n<div>\n<img src=\"https://raw.githubusercontent.com/greentfrapp/keras-aae/master/images/aae_latent.png\" alt=\"Latent space from Adversarial Autoencoder\" width=\"whatever\" height=\"300px\" style=\"display: inline-block;\">\n<img src=\"https://raw.githubusercontent.com/greentfrapp/keras-aae/master/images/regular_latent.png\" alt=\"Latent space from regular Autoencoder\" width=\"whatever\" height=\"300px\" style=\"display: inline-block;\">\n</div>\n\n*The left image shows the latent space of an unseen MNIST test set after training with an Adversarial Autoencoder for 50 epochs, which follows a 2D Gaussian prior. Contrast this with the latent space of the regular Autoencoder trained under the same conditions, with a far more irregular latent distribution.*\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8372314958117771
      ],
      "excerpt": "For more parameters, run with --help flag. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of Adversarial Autoencoder in Keras",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/greentfrapp/keras-aae/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 15,
      "date": "Sat, 25 Dec 2021 03:24:35 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/greentfrapp/keras-aae/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "greentfrapp/keras-aae",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8807024220639211,
        0.9503189345333785
      ],
      "excerpt": "To train a model just run \n$ python keras-aae.py --train \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/greentfrapp/keras-aae/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "keras-aae",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "keras-aae",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "greentfrapp",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/greentfrapp/keras-aae/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 32,
      "date": "Sat, 25 Dec 2021 03:24:35 GMT"
    },
    "technique": "GitHub API"
  }
}