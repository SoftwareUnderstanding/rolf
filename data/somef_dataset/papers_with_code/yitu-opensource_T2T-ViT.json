{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this repo useful, please consider citing:\n```\n@InProceedings{Yuan_2021_ICCV,\n    author    = {Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis E.H. and Feng, Jiashi and Yan, Shuicheng},\n    title     = {Tokens-to-Token ViT: Training Vision Transformers From Scratch on ImageNet},\n    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\n    month     = {October},\n    year      = {2021},\n    pages     = {558-567}\n}\n```\n\nOur codes are based on the [official imagenet example](https://github.com/pytorch/examples/tree/master/imagenet) by [PyTorch](https://pytorch.org/) and [pytorch-image-models](https://github.com/rwightman/pytorch-image-models) by [Ross Wightman](https://github.com/rwightman)\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{Yuan_2021_ICCV,\n    author    = {Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis E.H. and Feng, Jiashi and Yan, Shuicheng},\n    title     = {Tokens-to-Token ViT: Training Vision Transformers From Scratch on ImageNet},\n    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\n    month     = {October},\n    year      = {2021},\n    pages     = {558-567}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8025038807982551
      ],
      "excerpt": "The results look like: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yitu-opensource/T2T-ViT",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-01-23T02:59:09Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T17:15:52Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9732192340575616
      ],
      "excerpt": "2021/03/11: update our new results. Now our T2T-ViT-14 with 21.5M parameters can reach 81.5% top1-acc with 224x224 image resolution, and 83.3\\% top1-acc with 384x384 resolution.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9881947426484001
      ],
      "excerpt": "2021/01/28: release codes and upload most of the pretrained models of T2T-ViT. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9920529930459461
      ],
      "excerpt": "The three lite variants of T2T-ViT (Comparing with MobileNets): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9888152922201394
      ],
      "excerpt": "Top-1 accuracy of the model is: 81.5% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8669531772562081
      ],
      "excerpt": "We resize CIFAR10/100 to 224x224 and finetune our pretrained T2T-ViT-14/19 to CIFAR10/100 by running: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8351829970101771
      ],
      "excerpt": "Visualize the image features of ResNet50, you can open and run the visualization_resnet.ipynb file in jupyter notebook or jupyter lab; some results are given as following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8351829970101771
      ],
      "excerpt": "Visualize the image features of ViT, you can open and run the visualization_vit.ipynb file in jupyter notebook or jupyter lab; some results are given as following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8122199204195946
      ],
      "excerpt": "Visualize attention map, you can refer to this file. A simple example by visualizing the attention map in attention block 4 and 5 is: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "ICCV2021, Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yitu-opensource/T2T-ViT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 136,
      "date": "Wed, 22 Dec 2021 21:05:44 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yitu-opensource/T2T-ViT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "yitu-opensource/T2T-ViT",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/yitu-opensource/T2T-ViT/main/visualization_vit.ipynb",
      "https://raw.githubusercontent.com/yitu-opensource/T2T-ViT/main/visualization_resnet.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/yitu-opensource/T2T-ViT/main/distributed_train.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8886563876056374
      ],
      "excerpt": "| Model    | T2T Transformer | Top1 Acc | #params | MACs |  Download| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801608852907022
      ],
      "excerpt": "| T2T-ViT_t-24 | Transformer |   82.6   |  64.1M  | 15.0G| here |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8886563876056374
      ],
      "excerpt": "| Model    | T2T Transformer | Top1 Acc | #params | MACs |  Download| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.827797439604211
      ],
      "excerpt": "| T2T-ViT-10   |  Performer  |   75.2   |  5.9M   | 1.5G  | here|  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9210188757447015
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python main.py path/to/data --model t2t_vit_14 -b 100 --eval_checkpoint path/to/checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.849436359944601,
        0.9210188757447015
      ],
      "excerpt": "Download the T2T-ViT-7, T2T-ViT-10 or T2T-ViT-12, then test it by running: \nCUDA_VISIBLE_DEVICES=0 python main.py path/to/data --model t2t_vit_7 -b 100 --eval_checkpoint path/to/checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.890453276818876
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python main.py path/to/data --model t2t_vit_14 --img-size 384 -b 100 --eval_checkpoint path/to/T2T-ViT-14-384 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8214858730342897
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,1,2,3 ./distributed_train.sh 4 path/to/data --model t2t_vit_7 -b 128 --lr 1e-3 --weight-decay .03 --amp --img-size 224 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8491159867857213,
        0.8380762698219618
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,1,2,3 ./distributed_train.sh 4 path/to/data --model t2t_vit_14 -b 128 --lr 1e-3 --weight-decay .05 --amp --img-size 224 \nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 ./distributed_train.sh 8 path/to/data --model t2t_vit_14 -b 64 --lr 5e-4 --weight-decay .05 --amp --img-size 224 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8416281603013129,
        0.8517328404924447
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 ./distributed_train.sh 8 path/to/data --model t2t_vit_19 -b 64 --lr 5e-4 --weight-decay .065 --amp --img-size 224 \n| Model        |  ImageNet | CIFAR10 |  CIFAR100| #params|  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8925019399326494
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,1 transfer_learning.py --lr 0.05 --b 64 --num-classes 10 --img-size 224 --transfer-learning True --transfer-model /path/to/pretrained/T2T-ViT-19 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8221155366440788
      ],
      "excerpt": "<img src=\"https://github.com/yitu-opensource/T2T-ViT/blob/main/images/attention_visualization.png\" width=\"600\" height=\"400\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yitu-opensource/T2T-ViT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/yitu-opensource/T2T-ViT/main/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The Clear BSD License\\n\\nCopyright (c) [2012]-[2021] Shanghai Yitu Technology Co., Ltd.\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification, are permitted (subject to the limitations in the disclaimer below) provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n* Neither the name of Shanghai Yitu Technology Co., Ltd. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\n\\nNO EXPRESS OR IMPLIED LICENSES TO ANY PARTY\\'S PATENT RIGHTS ARE GRANTED BY THIS LICENSE. THIS SOFTWARE IS PROVIDED BY SHANGHAI YITU TECHNOLOGY CO., LTD. AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL SHANGHAI YITU TECHNOLOGY CO., LTD. OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet, [ICCV 2021](https://openaccess.thecvf.com/content/ICCV2021/html/Yuan_Tokens-to-Token_ViT_Training_Vision_Transformers_From_Scratch_on_ImageNet_ICCV_2021_paper.html)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "T2T-ViT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "yitu-opensource",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yitu-opensource/T2T-ViT/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "yuanli2333",
        "body": "T2T-ViT pretrained models",
        "dateCreated": "2021-03-06T05:21:08Z",
        "datePublished": "2021-03-07T11:19:24Z",
        "html_url": "https://github.com/yitu-opensource/T2T-ViT/releases/tag/main",
        "name": "T2T-ViT pretrained models",
        "tag_name": "main",
        "tarball_url": "https://api.github.com/repos/yitu-opensource/T2T-ViT/tarball/main",
        "url": "https://api.github.com/repos/yitu-opensource/T2T-ViT/releases/39393846",
        "zipball_url": "https://api.github.com/repos/yitu-opensource/T2T-ViT/zipball/main"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[timm](https://github.com/rwightman/pytorch-image-models), pip install timm==0.3.4\n\ntorch>=1.4.0\n\ntorchvision>=0.5.0\n\npyyaml\n\ndata prepare: ImageNet with the following folder structure, you can extract imagenet by this [script](https://gist.github.com/BIGBALLON/8a71d225eff18d88e469e6ea9b39cef4).\n\n```\n\u2502imagenet/\n\u251c\u2500\u2500train/\n\u2502  \u251c\u2500\u2500 n01440764\n\u2502  \u2502   \u251c\u2500\u2500 n01440764_10026.JPEG\n\u2502  \u2502   \u251c\u2500\u2500 n01440764_10027.JPEG\n\u2502  \u2502   \u251c\u2500\u2500 ......\n\u2502  \u251c\u2500\u2500 ......\n\u251c\u2500\u2500val/\n\u2502  \u251c\u2500\u2500 n01440764\n\u2502  \u2502   \u251c\u2500\u2500 ILSVRC2012_val_00000293.JPEG\n\u2502  \u2502   \u251c\u2500\u2500 ILSVRC2012_val_00002138.JPEG\n\u2502  \u2502   \u251c\u2500\u2500 ......\n\u2502  \u251c\u2500\u2500 ......\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 849,
      "date": "Wed, 22 Dec 2021 21:05:44 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "vision-transformer",
      "t2t-transformer",
      "vit"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The way to use our pretrained T2T-ViT:\n```\nfrom models.t2t_vit import *\nfrom utils import load_for_transfer_learning \n\n#: create model\nmodel = t2t_vit_14()\n\n#: load the pretrained weights\nload_for_transfer_learning(model, /path/to/pretrained/weights, use_ema=True, strict=False, num_classes=1000)  #: change num_classes based on dataset, can work for different image size as we interpolate the position embeding for different image size.\n```\n\n\n",
      "technique": "Header extraction"
    }
  ]
}