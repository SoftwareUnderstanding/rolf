{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2005.14165>"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Bibtex citation will be available soon.\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nlx-group/overlapy",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-14T19:17:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-23T11:07:21Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9950912283075144,
        0.9977903211204913,
        0.8482702128640349,
        0.9557549297424768,
        0.9709138357536475
      ],
      "excerpt": "Overlapy is a Python package developed to evaluate textual overlap (N-Grams) between two volumes of text. In fact, it comes from the necessity of evaluating \"data contamination\" between pre-training datasets for Language Models and testsets of NLP tasks. This problem is starting to become relevant: as models become ever larger, rapidly entering the trillions of parameters mark, they can fit larger pre-training language modelling datasets, which have started to inch closer to the terabytes mark. \nThe web is a source of nearly unlimited natural language text, making it one of the favourite sources to obtain unlabelled text. Websites like Reddit (https://reddit.com/) aggregate content and outbound links in inconcievable amounts. However, these resources are not exclusive to the language modelling task, and other tasks use them to construct even labelled datasets. As web crawlers extend their scrapped nodes, the probability of obtaining text that has been used in other tasks grows larger. With the capability of these models to memorize spans of text, it can just so happen that specific spans from examples of a tasks' testset could have been found in the pre-training dataset. The language model could have memorized it, making it previously seen data less than ideal as we want to test our models with unseen (o.o.d) data. This constitutes a problem for the present and future. \nThe methodology followed for this implementation is described in GPT-3's paper appendix (https://arxiv.org/abs/2005.14165). It can be decomposed into three main parts: tokenize, choosing N-Gram size, calculate N-Gram collisions between pre-training datasets and testsets. \nA token is considered an alphanumeric character, delimited by whitespace, and lowercased. In overlapy, the tokenization function is arbitrary (user-defined), and does not need to follow this definition. \nN-Gram size is determined to be the 5th percentile of the distribution of testset examples lengths. The authors set a minimum size of 8 and maximum size of 13. We follow this definition, however, allow the user to redefine the percentile, minimum and maximum size. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Python package developed to evaluate textual overlap (N-Grams) between two volumes of text.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nlx-group/overlapy/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 22 Dec 2021 14:03:36 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nlx-group/overlapy/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "nlx-group/overlapy",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Packaged developed to work with Python 3+. Some examples require Python 3.6+ and nltk (<http://www.nltk.org/>) installed.\n\ntqdm (<https://github.com/tqdm/tqdm>) not mandatory to have installed but is recommended to track the progress, especially for jobs with several hundreds of gigabytes of text.\n\n```bash\npip install overlapy\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nlx-group/overlapy/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Ruben Branco\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "about\">About</a> \u26ad",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "overlapy",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "nlx-group",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nlx-group/overlapy/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 22 Dec 2021 14:03:36 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "nlp",
      "textual-analysis",
      "data-contamination"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "It follows the contents of an usage example from one of our examples found [here](examples/).\n\n```python\nfrom overlapy import OverlapyTestSet, Overlapy\n\npretraining_dataset = [\n    \"A B A C D E F G\",\n    \"A C F J K H E\",\n    \"V L N M Q\",\n    \"A B A C \u00c7 T Z V E\",\n    \"L M N O P\",\n]\n\ntestset_examples = [\n    \"B A B A C O Q W R\",  #: Match A B A C with #:1, #:4 from pretraining_dataset\n    \"O P Q F J K H\",  #: Match F J K H with #:2 from pretraining_dataset\n    \"W E R E\",  #: No match\n    \"I E T Z V E L\",  #: Match T Z V E with #:4 from pretraining_dataset\n    \"K E K W\",  #: No match\n]\n#: Total examples matched: 3\n\n\ndef tokenizer(s):\n    #: Simple tokenization by whitespace.\n    return s.split()\n\n\n#: We'll override the parameter min_n and set it to 1 as we want the ngram value to be allowed\n#: to be less than 8. The testset examples were constructed for it to be 4, actually.\ntestset = OverlapyTestSet(\n    \"test\", min_n=1, examples=[tokenizer(s) for s in testset_examples]\n)\nprint(f\"N value: {testset.compute_n()}\")\nprint(f\"#: NGrams: {len(set(map(tuple, list(testset.ngrams()))))}\")\n\n#: We create an Overlapy object, handing three arguments:\n#:   * Testsets: A list of OverlapyTestSet objects that we want to study.\n#:   * Dataset: Dataset we want to calculate collisions with\n#:   * n_workers: Number of worker processes to use\noverlapy = Overlapy(\n    testsets=[testset],\n    dataset=[tokenizer(s) for s in pretraining_dataset],\n    n_workers=2,\n)\n#: Let's run and get the matches\nmatches = overlapy.run()\n\n#: We should be getting 3 testset examples that have been flagged for matches.\n#:    #:0 matches on A B A C\n#:    #:1 matches on F J K H\n#:    #:3 matches on T V Z E\n#: As we had noted above\nprint(f\"Matches: {list(testset.get_matches(matches))}\")\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}