{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1808.00948",
      "https://arxiv.org/abs/1802.05957"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{DRIT,\n  author = {Lee, Hsin-Ying and Tseng, Hung-Yu and Huang, Jia-Bin and Singh, Maneesh Kumar and Yang, Ming-Hsuan},\n  booktitle = {European Conference on Computer Vision},\n  title = {Diverse Image-to-Image Translation via Disentangled Representations},\n  year = {2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9948955163972709
      ],
      "excerpt": "Contact: Hsin-Ying Lee (hlee246@ucmerced.edu) and Hung-Yu Tseng (htseng6@ucmerced.edu) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999994049539177,
        0.9999982320254116,
        0.9999428227140414
      ],
      "excerpt": "Hsin-Ying Lee*, Hung-Yu Tseng*, Jia-Bin Huang, Maneesh Kumar Singh, and Ming-Hsuan Yang<br> \nEuropean Conference on Computer Vision (ECCV), 2018 (oral) (* equal contribution) \nPlease cite our paper if you find the code or dataset useful for your research. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/guy-oren/DIRT-OST",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-14T14:39:50Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-14T15:40:20Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9144180176039384,
        0.9822046432511119
      ],
      "excerpt": "[Project Page][Paper] \nPytorch implementation for our image-to-image translation method. With the proposed disentangled representation framework, we are able to learn diverse image-to-image translation from unpaired training data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9249275215573023
      ],
      "excerpt": "Due to the usage of adaptive pooling for attribute encoders, our model supports various input size. For example, here's the result of Grayscale -> RGB using 340x340 images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9880108959173285,
        0.9276463627473887,
        0.9401349653259836,
        0.9462688425141021
      ],
      "excerpt": "We provide two different methods for combining content representation and attribute vector. One is simple concatenation, the other is feature-wise transformation (learn to scale and bias features). In our experience, if the translation involves less shape variation (e.g. winter <-> summer), simple concatenation produces better results. On the other hand, for the translation with shape variation (e.g. cat <-> dog, photography <-> painting), feature-wise transformation should be used (i.e. set --concat 0) in order to generate diverse results. \nIn our experience, using the multiscale discriminator often gets better results. You can set the number of scales manually with --dis_scale. \nThere is a hyper-parameter \"d_iter\" which controls the training schedule of the content discriminator. The default value is d_iter = 3, yet the model can still generate diverse results with d_iter = 1. Set --d_iter 1 if you would like to save some training time.  \nWe also provide option --dis_spectral_norm for using spectral normalization (https://arxiv.org/abs/1802.05957). We use the code from the master branch of pytorch since pytorch 0.5.0 is not stable yet. However, despite using spectral normalization significantly stabilizes the training, we fail to observe consistent quality improvement. We encourage everyone to play around with various settings and explore better configurations. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/guy-oren/DIRT-OST/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 04:40:46 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/guy-oren/DIRT-OST/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "guy-oren/DIRT-OST",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/guy-oren/DIRT-OST/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/guy-oren/DIRT-OST/master/datasets/download_dataset.sh",
      "https://raw.githubusercontent.com/guy-oren/DIRT-OST/master/models/download_model.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Clone this repo:\n```\ngit clone https://github.com/HsinYingLee/DRIT.git\ncd DRIT/src\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.937413416679031,
        0.8917133644880719
      ],
      "excerpt": "Download the dataset using the following script. \nbash ../datasets/download_dataset.sh dataset_name \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8882747530828131
      ],
      "excerpt": "<img src='imgs/flower.png' width=\"900px\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/guy-oren/DIRT-OST/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Dockerfile",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Diverse Image-to-Image Translation via Disentangled Representations",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DIRT-OST",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "guy-oren",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/guy-oren/DIRT-OST/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3.5 or Python 3.6\n- Pytorch 0.4.0 and torchvision (https://pytorch.org/)\n- [TensorboardX](https://github.com/lanpa/tensorboard-pytorch)\n- [Tensorflow](https://www.tensorflow.org/) (for tensorboard usage)\n- We provide a Docker file for building the environment based on CUDA 9.0, CuDNN 7.1, and Ubuntu 16.04.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 04:40:46 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src='imgs/teaser.png' width=\"1000px\"/>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Yosemite (summer <-> winter)\n```\npython3 train.py --dataroot ../datasets/yosemite -name yosemite\ntensorboard --logdir ../logs/yosemite\n```\nResults and saved models can be found at `../results/yosemite`.\n\n- Portrait (photograpy <-> painting)\n```\npython3 train.py --dataroot ../datasets/portrait --name portrait --concat 0\ntensorboard --logdir ../logs/portrait\n```\nResults and saved models can be found at `../results/portrait`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Download a pre-trained model (We will upload the latest models in a few days)\n```\nbash ../models/download_model.sh\n```\n- Generate results with randomly sampled attributes\n  - Require folder `testA` (for a2b) or `testB` (for b2a) under dataroot\n```\npython3 test.py --dataroot ../datasets/yosemite --name yosemite_random --resume ../models/example.pth\n```\nDiverse generated winter images can be found at `../outputs/yosemite_random`\n\n- Generate results with attributes encoded from given images\n  - Require both folders `testA` and `testB` under dataroot\n```\npython3 test_transfer.py --dataroot ../datasets/yosemite --name yosemite_encoded --resume ../models/example.pth\n```\nDiverse generated winter images can be found at `../outputs/yosemite_encoded`\n\n",
      "technique": "Header extraction"
    }
  ]
}