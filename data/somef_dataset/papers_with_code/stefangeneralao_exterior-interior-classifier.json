{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I thank Vic Ciesielski, Prince Hemrajani, Kendall Taylor and fellow students for providing valuable material and comments for various parts of this project. I also thank graphic designer student Jenna Knuuti for automating the process of creating occlusion based on the class activation maps.\n\n<div style=\"page-break-after: always;\"></div>\n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] Arzhaeva, Y., Wang, D., Devnath, L., Amirgholipour, S., McBean, R., Hillhouse, J., Luo, S., Meredith, D., Newbigin, K. and Yates, D. (2019). *Development of Automated Diagnostic Tools for Pneumoconiosis Detection from Chest X-Ray Radiographs*. Available at: https://www.coalservices.com.au/wp-content/uploads/2017/11/Project-No.-20647-Final-Report.pdf [Accessed 18 Oct. 2019].\n\n[2] Clevert, D., Unterthiner, T. and Hochreiter, S. (2016). *FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)*. Johannes Kepler University, Linz, Austria. Available at: https://arxiv.org/pdf/1511.07289.pdf [Accessed 18 Oct. 2019].\n\n[3] Keras.io. (n.d.). *Home - Keras Documentation*. [online] Available at: https://keras.io/ [Accessed 18 Oct. 2019].\n\n[4] Docs.python.org. (n.d.). *The Python Language Reference \u2014 Python 3.6.9 documentation*. [online] Available at: https://docs.python.org/3.6/reference/ [Accessed 18 Oct. 2019].\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "    <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "    <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "  <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0 30px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "max_pooling2d_2 (MaxPooling2D)      (None, 29, 29, 32)              0            \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "<p style=\"font-size:90%; font-style:italic; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; text-align:center; margin-top:20px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066,
        0.9559715772848645
      ],
      "excerpt": "      <img src=\"report_images/loss_history.jpg\" alt=\"training loss history\"/> \n      <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066,
        0.9559715772848645
      ],
      "excerpt": "      <img src=\"report_images/accuracy_history.jpg\" alt=\"training accuracy history\"/> \n      <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "  <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0 30px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "  <p style=\"font-size:90%; font-style:italic; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "  <p style=\"font-size:90%; font-style:italic; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Exterior     | 1841           10             709                                \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Interior     | 10             1782           768                                \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645,
        0.8919274046014751
      ],
      "excerpt": "  <p style=\"font-size:90%; font-style:italic; text-align:center;\"> \n    Figure 10: Results of ToC for LT=0.01 and UT=0.99 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "  <p style=\"font-size:90%; font-style:italic; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "  <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0 30px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "  <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0 30px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "  <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0 30px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "        Exterior: 10.1%<br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "  <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0 30px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "    <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "  <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0 30px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "  <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0 30px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "      <p style=\"font-size:90%; font-style:italic; margin:0; margin-left:15px; text-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "  <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0 30px;\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/stefangeneralao/exterior-interior-classifier",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-26T03:27:15Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-07T23:57:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Classifying unlabeled data is generally the final purpose of supervised learning problems. Fitting a classifier for accurate predictions of unlabeled data can not be achieved with supervised learning, unsupervised learning may be considered instead. The typical purpose of unsupervised learning is to find relations between datapoints but compared to supervised learning they do not output any target predictions.\n\nSemi-supervised learning includes the challange to classify a large unlabeled dataset using a small labeled dataset for training the classifier. Reasons of pursuing semi-supervised learning rather than supervised may be to reduce the manual labour of having a domain expert label every datapoint.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9650485323851279,
        0.9903805048372276,
        0.839458424591888
      ],
      "excerpt": "Upon this project a large unlabeled dataset of images was presented which had been generated by webscraping of various Australian real estate agents. The images consisted of exterior, interior, floor plan and aerial images of presumably Australian homes. All of the observed images had good levels of exposure and none were significally noisy. The depth of field was large on most of the observed images and high dynamic range seems to be an apparent choice by the majority of real estate photographers. Some images included watermarks of the real estate agent. \nBoth me and my coordinator agreed on analysing the dataset using some kind of semi-supervised learning to classify images to either exterior or interior. Besides building an image classifier some additional work was required for the project. Our discussions included numerous approaches such as: \n- Estimating the accuracy of the unlabeled dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9933448206352916,
        0.9931951750252473
      ],
      "excerpt": "GANs proves to be an effective way of increasing the volume of the training set. Using a variant of GAN, Yulia Arzhaeva et. al. managed to increase the accuracy of their classifier for classifying a certain medical condition: \"In this work, we train a CycleGAN using our 56 normal and 56 pneumoconiosis images to generate 1,000 normal and 1,000 pneumoconiosis images, respectively. Experiments show that overall good accuracy is achieved when using the synthetic images generated by CycleGAN trained for 30 epochs.\" [1]. Although somewhat relevant for this study, it was decided to not be implemented due to speculative advanced implementations of GAN. \nToC is a classification strategy that lets predictions between a set lower and upper threshold be classified to unknown. The unknown label is a way for the classifier to indicate that further domain assistance is required. The ToC strategy also infers that the accuracy of the classifier would be increased as predictions with low certainty will be excluded. The idea is that when the range of unknown increases so will the accuracy and also the number of unknown classifications. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9174672388919249
      ],
      "excerpt": "    Figure 3: Respresentations of ToC for two different variants \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9785314055327998,
        0.9524819640496305,
        0.8228872407647376,
        0.9769139078813179
      ],
      "excerpt": "The strategy can be further developed to incoperate images with high certainty to the training set, instead of letting a domain expert label every unlabeled image. Letting a domain expert label the unknown classified images will further provide more training data. The importance of either high accuracy or low rate of unknown classifications depends on the upper and lower thresholds. The decision can be considered based on the business goals and limits of the project. \nStudies about approaches similar to ToC could not be found being used as a semi-supervised learning fashion. Therefore it was decided to investigate the possibilities with ToC. I would also appreciate any input or findings about prior research. \nGenerating class activation maps is a way of interpreting and sometimes debugging a convolutional neural network. It was decided that class activation maps would be tested on the trained classifier. \nThe architecture of the classifier is a convolutional neural network, also known as ConvNet. The model consists of five trainable layers, two of which are convolutional and the rest are fully connected. The rather unconventional activation function called <i>Exponential Linear Unit</i> (ELU) was used for every layer except the output. For many image datasets, ELUs shows to be superior in training time and accuracy compared to the more common <i>Rectified Linear Units</i> (ReLUs) and <i>Leaky Rectified Linear Units</i> (LeakyReLUs) [2]. For the output layer, the activation function Softmax was applied instead of ELU. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577302739126875
      ],
      "excerpt": "dense_1 (Dense)                     (None, 32)                      861216       \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577302739126875
      ],
      "excerpt": "dense_3 (Dense)                     (None, 2)                       34 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9048893383220121
      ],
      "excerpt": "    Figure 5: Architecure of the classifier \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9941324567703868,
        0.9916941529005494
      ],
      "excerpt": "A clever way of extracting more data from small datasets is to generate multiple images from one. For this project multiple parameters for image augmentation was used including horizontal flip, rotation and zooming. Every training batch is randomly generated based on the set parameters of the image augmentation. \nInitially, image transformation was applied for all RGB-channels but in later stages of the project grayscale manipulation was applied as well as it proved to yield more reliable results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9877859891405516
      ],
      "excerpt": "Categorical crossentropy loss function was used for fitting the classifier as it is suitable for determining the likelihood of a certain category using ConvNets. The classifier trained for 512 epochs but the validation loss reached convergence after about 360 epochs. The optimisation was achieved by stochastic gradient descent with learning rate of 0.01. The training session ended with a validation accuracy of 94.5%. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9139375991304484,
        0.9898088925637859
      ],
      "excerpt": "In this section the results of the ToC-strategy is revealed. The lower threshold (LT) and upper threshold (UT) was configured at various settings. The results also includes a confusion matrix providing further insights. During the evaluation, 512 validation images were each augmented 10 times creating a total of 5120 data points. \nAs expected, for LT and UT both set at 0.5, the rate of unknowns is 0% while the accuracy is 94.5% as seen in section 7. These thresholds means that unknowns are completely excluded from the range resulting in classifications of interior and exterior only. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8978425274031282
      ],
      "excerpt": "    Figure 8: Results of ToC for LT=0.5 and UT=0.5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9849892778393157
      ],
      "excerpt": "As discussed in section 3.3, the accuracy is expected to increase if the range of unknown classifications is increased as well. Setting the LT at 0.1 and UT at 0.9 increases the accuracy to 98.3% but also increases the rate of unknowns to 10.9%. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8978425274031282
      ],
      "excerpt": "    Figure 9: Results of ToC for LT=0.1 and UT=0.9 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9470136828873104
      ],
      "excerpt": "Increasing the thresholds to LT at 0.01 and UT at 0.99 further increases the accuracy to 99.4% and rate of unknowns to 28.3% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9242655208036565
      ],
      "excerpt": "Extreme thresholds yields 100% accuracy for this classifier and 78.6% rate of unknowns. Whether extreme thresholds such as LT at 0.0001 and UT at 0.9999 actually provides any value for semi-supervised learning is questionable. Notice how exterior images seem to be easier to classify at this point. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8978425274031282
      ],
      "excerpt": "    Figure 11: Results of ToC for LT=0.0001 and UT=0.9999 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.858923974600136,
        0.9678117728909399
      ],
      "excerpt": "Class activations maps was successfully applied to the trained classifier as seen in figure 12 and 13. As discussed in section 11.2, the heat maps may or may not be arbitrary and/or prone to confirmation bias. \nMasking the parts of the image with high focus shows to slightly affect predictions. The process simply masks the parts of the images with the heat map generated from the class activation map. Class activation maps created from the occluded images shows that the focus shifts to other parts of the image. In many cases the classifier manages to do correct classifications even though occlusion is applied. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9828272539071959,
        0.9664368643336759
      ],
      "excerpt": "As occlusion on the interior images affects the prediction more than the exterior images, it seems that the prediction is barely affected by the output of the exterior class. This assumption is based on these samples and multiple others. It means that the classifier predicts interior if features of interior are detected, in all other cases it predicts exterior. The classifier was tested on noise images to strengthen the assumption. All noise images were predicted as exterior with very high certainty reinforcing the assumption. \nAlthough relatively uncommon, incorrect classifications does occur. Many of which may be difficult to classify due to the nature of the image augmentation. In figure 17, cases of highly certain incorrect classifications are presented. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8648211165141049
      ],
      "excerpt": "    Figure 17: Samples of extremely bad predictions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079208811671938
      ],
      "excerpt": "Early versions of the classifier was horribly overfitted on blue color and classified many images with blue color as exterior. Confirmation of the assumption was made by taking an interior image, which was also classified as interior, and then coloring the ceiling blue in an editing software. The modified image was then classified as exterior. At this part it was decided that the classifier would be trained on grayscale images instead of colored. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "      <img src=\"report_images/blue_ceiling.jpg\" alt=\"Blue ceiling edited with blue ceiling\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9199127921369119
      ],
      "excerpt": "        (b) Sample image of interior edited with blue ceiling \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9838949388380823,
        0.8006347307601901,
        0.9818532159543403
      ],
      "excerpt": "Many interior environments are worse lit than exterior environments. Low light environments forces the photographer to compensate the conditions in order to take an image with good exposure. Camera settings to compensate these conditions often creates noise, artefacts and other side effects. Changes are that the classifier might pick up on specific, or a combination of, side effects due to low light environments and therefore classify those images as interior or exterior based on the side effects. \nIncreased ISO-value increases the exposure of an image but also increases noise and artefacts. Whether interior images in the dataset had higher noise compared to exterior environments was not investigated. \nLow light environments may also force the photographer to increase the size of the aperture. A side effect of a bigger aperture is a shallower depth of field creating blurry areas. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9872659614928037,
        0.8791321846965374
      ],
      "excerpt": "The generated heat maps does not entirely reveal the insides of the classifier. Interestingly though, occlusion seem to be effective, although partially. Whether the partial effectiveness of occlusion is due to a solid classifier or other factors needs more research to determine. To interpret the classifiers decision objectively and avoiding human confirmation bias, other techniques should be implemented. \nVarious interesting classifications are presented in this section. The classifications are consciously left uncommented inviting conversations and opinions from readers of this report. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372396012058242
      ],
      "excerpt": "      <img src=\"report_images/painting_int.jpg\" alt=\"Painting on wall\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372396012058242
      ],
      "excerpt": "      <img src=\"report_images/painting_int_2.jpg\" alt=\"Painting on wall\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051614661132027
      ],
      "excerpt": "    Figure 19: Samples of classifications \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Using convolutional neural network to classify images as exterior or interior",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/stefangeneralao/exterior-interior-classifier/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This has been attempted by uncredible sources and I decided not to pursue it at this time. It is suspected to be a very advanced field of mathematics and data analysis. Assessing my current experience in the field, I am afraid I would not be able nor approariate to produce any credible results.\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 00:32:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/stefangeneralao/exterior-interior-classifier/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "stefangeneralao/exterior-interior-classifier",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/stefangeneralao/exterior-interior-classifier/master/client/Dockerfile",
      "https://raw.githubusercontent.com/stefangeneralao/exterior-interior-classifier/master/classifier_server/Dockerfile",
      "https://raw.githubusercontent.com/stefangeneralao/exterior-interior-classifier/master/api_server/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/stefangeneralao/exterior-interior-classifier/master/client/update_docker_image.sh",
      "https://raw.githubusercontent.com/stefangeneralao/exterior-interior-classifier/master/classifier_server/update_docker_image.sh",
      "https://raw.githubusercontent.com/stefangeneralao/exterior-interior-classifier/master/api_server/update_docker_image.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<div style=\"float:right; width:175px; margin-left:30px; page-break-inside:avoid;\">\n  <div style=\"margin-bottom:30px; margin-top:7px;\">\n    <img src=\"report_images/labeling-app.jpg\" alt=\"Manual labeling application\"/>\n    <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\">\n      Figure 4: Screenshot of manual labeling web application for Android\n    </p>\n  </div>\n</div>\n\nThe size of the initial training set was decided to 512 interior and 512 exterior together with a validation set of 256 interior and 256 exterior images. In other words, 768 interior and 768 exterior images had to be manually labeled at minimum, or 1536 in total. All images being neither an interior nor exterior image was simply classified as <i>skipped</i> and would not be used in the training sessions. Although 1024 datapoints is relatively small it was thought to be enough for semi-supervised learning.\n\nA small sample was manually labeled by placing images in folders of the corresponding label. It was then discovered that the unlabeled dataset was unbalanced by around 1:7 ratio of interior to exterior and the skipped images consisted of around 6%. Using this information and solving the equation for `(interior>768)AND(exterior>768)` meant that around 6500 images had to be manually labeled in order to achieve a 1:1 ratio between interior and exterior.\n\nThe duration of opening an image, analysing and placing it to the corresponding folder (interior/exterior/skip) was estimated to take around five seconds. Labelling 6500 images would therefore require around nine hours. To alleviate the tedious manual labour of classifying the initial dataset it was decided to develop a web application as a tool. Being an experienced full-stack web developer, the process of building a complete application took around two hours. Using the web application a sufficient dataset for training was extracted after 6383 images and around three hours. Having an application for manual labelling at disposal also means that additional labelling will be effortless and also increasing scalability of future progress.\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9583924952147198
      ],
      "excerpt": "    <img src=\"report_images/sample-interior.jpg\" alt=\"interior example\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9583924952147198
      ],
      "excerpt": "    <img src=\"report_images/sample-exterior.jpg\" alt=\"exterior example\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9101045780906348
      ],
      "excerpt": "  <div style=\"width:100%; display:grid; grid-template-columns: 1fr 1fr; grid-gap:25px; margin-bottom:10px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9307412236125174
      ],
      "excerpt": "      <img src=\"report_images/toc_1.jpg\" alt=\"Threshold of certainty example 1\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9307412236125174
      ],
      "excerpt": "      <img src=\"report_images/toc_2.jpg\" alt=\"Threshold of certainty example 2\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.906614234580968
      ],
      "excerpt": "Layer (type)                        Output Shape                    Param \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934745846565956
      ],
      "excerpt": "Total params: 914,674 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9064846179678109
      ],
      "excerpt": "      <img src=\"report_images/image_augmentation_samples.jpg\" alt=\"Image augmentation samples\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9101045780906348
      ],
      "excerpt": "  <div style=\"width:100%; display:grid; grid-template-columns: 1fr 1fr; grid-gap:25px; margin-bottom:10px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8768182054938249
      ],
      "excerpt": "      <img src=\"report_images/loss_history.jpg\" alt=\"training loss history\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8540136318574084
      ],
      "excerpt": "      <img src=\"report_images/accuracy_history.jpg\" alt=\"training accuracy history\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9101045780906348
      ],
      "excerpt": "  <div style=\"width:100%; display:grid; grid-template-columns: 1fr 1fr 1fr; grid-gap:25px; margin-bottom:10px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8361675780394401,
        0.8375849652775302
      ],
      "excerpt": "      <img src=\"report_images/cam_original_1.jpg\" alt=\"class activation map original 1\"/> \n      <img src=\"report_images/cam_heatmap_1.jpg\" alt=\"class activation map 1\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8361675780394401,
        0.8375849652775302
      ],
      "excerpt": "      <img src=\"report_images/cam_original_4.jpg\" alt=\"class activation map original 4\"/> \n      <img src=\"report_images/cam_heatmap_4.jpg\" alt=\"class activation map 4\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8361675780394401,
        0.8375849652775302
      ],
      "excerpt": "      <img src=\"report_images/cam_original_5.jpg\" alt=\"class activation map original 5\"/> \n      <img src=\"report_images/cam_heatmap_5.jpg\" alt=\"class activation map 5\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9101045780906348
      ],
      "excerpt": "  <div style=\"width:100%; display:grid; grid-template-columns: 1fr 1fr 1fr; grid-gap:25px; margin-bottom:10px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9374237393983283,
        0.9374237393983283
      ],
      "excerpt": "      <img src=\"report_images/occ_ext_1.jpg\" alt=\"Occlusion sample 1\"/> \n      <img src=\"report_images/occ_heatmap_1.jpg\" alt=\"Occlusion sample 1\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9374237393983283,
        0.9374237393983283
      ],
      "excerpt": "      <img src=\"report_images/occ_ext_4.jpg\" alt=\"Occlusion sample 4\"/> \n      <img src=\"report_images/occ_heatmap_4.jpg\" alt=\"Occlusion sample 4\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9374237393983283,
        0.9374237393983283
      ],
      "excerpt": "      <img src=\"report_images/occ_ext_5.jpg\" alt=\"Occlusion sample 5\"/> \n      <img src=\"report_images/occ_heatmap_5.jpg\" alt=\"Occlusion sample 5\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9101045780906348
      ],
      "excerpt": "  <div style=\"width:100%; display:grid; grid-template-columns: 1fr 1fr 1fr; grid-gap:25px; margin-bottom:10px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8361675780394401,
        0.8375849652775302
      ],
      "excerpt": "      <img src=\"report_images/cam_original_2.jpg\" alt=\"class activation map original 2\"/> \n      <img src=\"report_images/cam_heatmap_2.jpg\" alt=\"class activation map 2\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8361675780394401,
        0.8375849652775302
      ],
      "excerpt": "      <img src=\"report_images/cam_original_3.jpg\" alt=\"class activation map original 3\"/> \n      <img src=\"report_images/cam_heatmap_3.jpg\" alt=\"class activation map 3\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8361675780394401,
        0.8375849652775302
      ],
      "excerpt": "      <img src=\"report_images/cam_original_6.jpg\" alt=\"class activation map original 6\"/> \n      <img src=\"report_images/cam_heatmap_6.jpg\" alt=\"class activation map 6\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9101045780906348
      ],
      "excerpt": "    <div style=\"width:100%; display:grid; grid-template-columns: 1fr 1fr 1fr; grid-gap:25px; margin-bottom:10px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9374237393983283,
        0.9374237393983283
      ],
      "excerpt": "      <img src=\"report_images/occ_int_2.jpg\" alt=\"Occlusion sample 2\"/> \n      <img src=\"report_images/occ_heatmap_2.jpg\" alt=\"Occlusion sample 2\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9374237393983283,
        0.9374237393983283
      ],
      "excerpt": "      <img src=\"report_images/occ_int_3.jpg\" alt=\"Occlusion sample 3\"/> \n      <img src=\"report_images/occ_heatmap_3.jpg\" alt=\"Occlusion sample 3\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9374237393983283,
        0.9374237393983283
      ],
      "excerpt": "      <img src=\"report_images/occ_int_6.jpg\" alt=\"Occlusion sample 6\"/> \n      <img src=\"report_images/occ_heatmap_6.jpg\" alt=\"Occlusion sample 6\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.805620047655719
      ],
      "excerpt": "    <img src=\"report_images/noise.jpg\" alt=\"Noise\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9101045780906348
      ],
      "excerpt": "  <div style=\"width:100%; display:grid; grid-template-columns: 1fr 1fr 1fr 1fr; grid-gap:5px; margin-bottom:10px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241184305967362
      ],
      "excerpt": "      <img src=\"report_images/fp_1.jpg\" alt=\"Incorrect classification example 1\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241184305967362
      ],
      "excerpt": "      <img src=\"report_images/fp_4.jpg\" alt=\"Incorrect classification example 4\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241184305967362
      ],
      "excerpt": "      <img src=\"report_images/fp_2.jpg\" alt=\"Incorrect classification example 2\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241184305967362
      ],
      "excerpt": "      <img src=\"report_images/fp_3.jpg\" alt=\"Incorrect classification example 3\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9101045780906348
      ],
      "excerpt": "  <div style=\"width:100%; display:grid; grid-template-columns: 1fr 1fr; grid-gap:25px; margin-bottom:10px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8248588610879851
      ],
      "excerpt": "      <img src=\"report_images/blue_ceiling_original.jpg\" alt=\"Blue ceiling before editing \"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8832005762322652
      ],
      "excerpt": "      <img src=\"report_images/blue_ceiling.jpg\" alt=\"Blue ceiling edited with blue ceiling\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9069724717576688
      ],
      "excerpt": "  <div style=\"width:100%; display:grid; grid-template-columns: 1fr 1fr 1fr 1fr; grid-template-rows: 1fr 1fr; grid-gap:5px; margin-bottom:10px;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9202509175650415
      ],
      "excerpt": "      <img src=\"report_images/southpark_ext.jpg\" alt=\"Southpark exterior\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9202509175650415
      ],
      "excerpt": "      <img src=\"report_images/southpark_int.jpg\" alt=\"Southpark interior\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9202509175650415
      ],
      "excerpt": "      <img src=\"report_images/xmastree_int.jpg\" alt=\"Christmas tree interior\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8994015263814481
      ],
      "excerpt": "      <img src=\"report_images/window_int.jpg\" alt=\"Window from inside\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9005378357095909
      ],
      "excerpt": "      <img src=\"report_images/painting_int.jpg\" alt=\"Painting on wall\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9005378357095909
      ],
      "excerpt": "      <img src=\"report_images/painting_int_2.jpg\" alt=\"Painting on wall\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9202509175650415
      ],
      "excerpt": "      <img src=\"report_images/rmit.jpg\" alt=\"Rmit\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8049802724398785
      ],
      "excerpt": "      <img src=\"report_images/victoria_state_library.jpg\" alt=\"Victoria state library\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/stefangeneralao/exterior-interior-classifier/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "JavaScript",
      "Python",
      "CSS",
      "Shell",
      "HTML",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Deep Learning Final Project Report\n**Stefan Generalao**<br/>stefan.generalao@gmail.com<br/>RMIT University, Melbourne<br/>Data Mining, Semester 2, 2019<br/>\n\n**Course Coordinator**: Assoc. Prof. Vic Ciesielski\n\n## 1 Introduction\nClassifying unlabeled data is generally the final purpose of supervised learning problems. Fitting a classifier for accurate predictions of unlabeled data can not be achieved with supervised learning, unsupervised learning may be considered instead. The typical purpose of unsupervised learning is to find relations between datapoints but compared to supervised learning they do not output any target predictions.\n\nSemi-supervised learning includes the challange to classify a large unlabeled dataset using a small labeled dataset for training the classifier. Reasons of pursuing semi-supervised learning rather than supervised may be to reduce the manual labour of having a domain expert label every datapoint.\n\n## 2 Dataset\n<div style=\"float:right; width:175px; margin-left:30px; page-break-inside:avoid;\">\n  <div style=\"margin-bottom:30px; margin-top:7px;\">\n    <img src=\"report_images/sample-interior.jpg\" alt=\"interior example\"/>\n    <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\">\n      Figure 1: Interior image\n    </p>      \n  </div>\n  <div style=\"margin-bottom:30px; margin-top:7px;\">\n    <img src=\"report_images/sample-exterior.jpg\" alt=\"exterior example\"/>\n    <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\">\n      Figure 2: Exterior image\n    </p>\n  </div>\n</div>\n\nUpon this project a large unlabeled dataset of images was presented which had been generated by webscraping of various Australian real estate agents. The images consisted of exterior, interior, floor plan and aerial images of presumably Australian homes. All of the observed images had good levels of exposure and none were significally noisy. The depth of field was large on most of the observed images and high dynamic range seems to be an apparent choice by the majority of real estate photographers. Some images included watermarks of the real estate agent.\n\n## 3 Initial Planning\nBoth me and my coordinator agreed on analysing the dataset using some kind of semi-supervised learning to classify images to either exterior or interior. Besides building an image classifier some additional work was required for the project. Our discussions included numerous approaches such as:\n- Estimating the accuracy of the unlabeled dataset\n- Generative Adversarial Networks to augment a bigger training set\n- Introducing a third class for images hard to classify or neither interior nor exterior\n- Threshold of uncertainty for incorporating classified images into training\n- Class Activation Maps and occlusion\n- Transfer Learning\n\n### 3.1 Estimating the Error Rate of the Unlabeled Dataset\nThis has been attempted by uncredible sources and I decided not to pursue it at this time. It is suspected to be a very advanced field of mathematics and data analysis. Assessing my current experience in the field, I am afraid I would not be able nor approariate to produce any credible results.\n\n### 3.2 Generative Adversarial Networks (GAN) for Image Augmentation\nGANs proves to be an effective way of increasing the volume of the training set. Using a variant of GAN, Yulia Arzhaeva et. al. managed to increase the accuracy of their classifier for classifying a certain medical condition: \"In this work, we train a CycleGAN using our 56 normal and 56 pneumoconiosis images to generate 1,000 normal and 1,000 pneumoconiosis images, respectively. Experiments show that overall good accuracy is achieved when using the synthetic images generated by CycleGAN trained for 30 epochs.\" [1]. Although somewhat relevant for this study, it was decided to not be implemented due to speculative advanced implementations of GAN.\n\n### 3.3 Threshold of Certainty (ToC)\nToC is a classification strategy that lets predictions between a set lower and upper threshold be classified to unknown. The unknown label is a way for the classifier to indicate that further domain assistance is required. The ToC strategy also infers that the accuracy of the classifier would be increased as predictions with low certainty will be excluded. The idea is that when the range of unknown increases so will the accuracy and also the number of unknown classifications.\n<div style=\"background-color:#f8f8f8; padding:30px; page-break-inside:avoid;\">\n  <div style=\"width:100%; display:grid; grid-template-columns: 1fr 1fr; grid-gap:25px; margin-bottom:10px;\">\n    <div>\n      <img src=\"report_images/toc_1.jpg\" alt=\"Threshold of certainty example 1\"/>\n      <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\">\n        (a) Lower threshold=0.1, upper threshold=0.9\n      </p>\n    </div>\n    <div>\n      <img src=\"report_images/toc_2.jpg\" alt=\"Threshold of certainty example 2\"/>\n      <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\">\n        (b) Lower threshold=0.4, upper threshold=0.6\n      </p>\n    </div>\n  </div>\n  <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0 30px;\">\n    Figure 3: Respresentations of ToC for two different variants\n  </p>\n</div>\n\nThe strategy can be further developed to incoperate images with high certainty to the training set, instead of letting a domain expert label every unlabeled image. Letting a domain expert label the unknown classified images will further provide more training data. The importance of either high accuracy or low rate of unknown classifications depends on the upper and lower thresholds. The decision can be considered based on the business goals and limits of the project.\n\nStudies about approaches similar to ToC could not be found being used as a semi-supervised learning fashion. Therefore it was decided to investigate the possibilities with ToC. I would also appreciate any input or findings about prior research.\n\n### 3.4 Class Activation Maps\nGenerating class activation maps is a way of interpreting and sometimes debugging a convolutional neural network. It was decided that class activation maps would be tested on the trained classifier.\n\n## 4 Manual Labeling\n<div style=\"float:right; width:175px; margin-left:30px; page-break-inside:avoid;\">\n  <div style=\"margin-bottom:30px; margin-top:7px;\">\n    <img src=\"report_images/labeling-app.jpg\" alt=\"Manual labeling application\"/>\n    <p style=\"font-size:90%; font-style:italic; text-align:center; margin:0;\">\n      Figure 4: Screenshot of manual labeling web application for Android\n    </p>\n  </div>\n</div>\n\nThe size of the initial training set was decided to 512 interior and 512 exterior together with a validation set of 256 interior and 256 exterior images. In other words, 768 interior and 768 exterior images had to be manually labeled at minimum, or 1536 in total. All images being neither an interior nor exterior image was simply classified as <i>skipped</i> and would not be used in the training sessions. Although 1024 datapoints is relatively small it was thought to be enough for semi-supervised learning.\n\nA small sample was manually labeled by placing images in folders of the corresponding label. It was then discovered that the unlabeled dataset was unbalanced by around 1:7 ratio of interior to exterior and the skipped images consisted of around 6%. Using this information and solving the equation for `(interior>768)AND(exterior>768)` meant that around 6500 images had to be manually labeled in order to achieve a 1:1 ratio between interior and exterior.\n\nThe duration of opening an image, analysing and placing it to the corresponding folder (interior/exterior/skip) was estimated to take around five seconds. Labelling 6500 images would therefore require around nine hours. To alleviate the tedious manual labour of classifying the initial dataset it was decided to develop a web application as a tool. Being an experienced full-stack web developer, the process of building a complete application took around two hours. Using the web application a sufficient dataset for training was extracted after 6383 images and around three hours. Having an application for manual labelling at disposal also means that additional labelling will be effortless and also increasing scalability of future progress.\n\n## 5 Classifier\nThe architecture of the classifier is a convolutional neural network, also known as ConvNet. The model consists of five trainable layers, two of which are convolutional and the rest are fully connected. The rather unconventional activation function called <i>Exponential Linear Unit</i> (ELU) was used for every layer except the output. For many image datasets, ELUs shows to be superior in training time and accuracy compared to the more common <i>Rectified Linear Units</i> (ReLUs) and <i>Leaky Rectified Linear Units</i> (LeakyReLUs) [2]. For the output layer, the activation function Softmax was applied instead of ELU.\n<div style=\"padding:10px; background-color:#f8f8f8; text-align:center; page-break-inside: avoid;\">\n\t<pre style=\"font-size:90%\">\nLayer (type)                        Output Shape                    Param #",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "exterior-interior-classifier",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "stefangeneralao",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/stefangeneralao/exterior-interior-classifier/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Mon, 27 Dec 2021 00:32:02 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**Stefan Generalao**<br/>stefan.generalao@gmail.com<br/>RMIT University, Melbourne<br/>Data Mining, Semester 2, 2019<br/>\n\n**Course Coordinator**: Assoc. Prof. Vic Ciesielski\n\n",
      "technique": "Header extraction"
    }
  ]
}