{
  "citation": [
    {
      "confidence": [
        0.9633194698530168
      ],
      "excerpt": "Ask questions in natural language and find granular answers in your documents. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9841034378552055
      ],
      "excerpt": "<a href=\"https://github.com/deepset-ai/haystack/graphs/contributors\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deepset-ai/haystack",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to Haystack\nWe are very open to community contributions and appreciate anything that improves haystack! This includes fixings typos, adding missing documentation, fixing bugs or adding new features.\nTo avoid unnecessary work on either side, please stick to the following process:\n\nCheck if there is already a related issue.\nOpen a new issue to start a quick discussion. Some features might be a nice idea, but don't fit in the scope of Haystack and we hate to close finished PRs!\nCreate a pull request in an early draft version and ask for feedback. If this is your first pull request and you wonder how to actually create a pull request, checkout this manual.\nVerify that all tests in the CI pass (and add new ones if you implement anything new)\n\nFormatting of Pull Requests\nPlease give a concise description in the first comment in the PR that includes: \n- What is changing?\n- Why? \n- What are limitations?\n- Breaking changes (Example of before vs. after)\n- Link the issue that this relates to\nRunning tests\nCI\nTests will automatically run in our CI for every commit you push to your PR. This is the most convenient way for you and we encourage you to create early \"WIP Pull requests\".\nLocal\nHowever, you can also run the tests locally by executing pytest in your terminal from the /test folder.\nRunning all tests\nImportant: If you want to run all tests locally, you'll need all document stores running in the background before you run the tests.\nMany of the tests will then be executed multiple times with different document stores.\nYou can launch them like this:\ndocker run -d -p 9200:9200 -e \"discovery.type=single-node\" -e \"ES_JAVA_OPTS=-Xms128m -Xmx128m\" elasticsearch:7.9.2\ndocker run -d -p 19530:19530 -p 19121:19121 milvusdb/milvus:1.1.0-cpu-d050721-5e559c\ndocker run -d -p 8080:8080 --name haystack_test_weaviate --env AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED='true' --env PERSISTENCE_DATA_PATH='/var/lib/weaviate' semitechnologies/weaviate:1.7.2\ndocker run -d -p 7200:7200 --name haystack_test_graphdb deepset/graphdb-free:9.4.1-adoptopenjdk11\ndocker run -d -p 9998:9998 -e \"TIKA_CHILD_JAVA_OPTS=-JXms128m\" -e \"TIKA_CHILD_JAVA_OPTS=-JXmx128m\" apache/tika:1.24.1\nThen run all tests:\ncd test\npytest\nRecommendation: Running a subset of tests\nIn most cases you rather want to run a subset of tests locally that are related to your dev:\nThe most important option to reduce the number of tests in a meaningful way, is to shrink the \"test grid\" of document stores.\nThis is possible by adding the --document_store_type arg to your pytest command. Possible values are: \"elasticsearch, faiss, memory, milvus, weaviate\".\nFor example, calling pytest . --document_store_type=\"memory\" will run all tests that can be run with the InMemoryDocumentStore, i.e.: \n- all the tests that we typically run on the whole \"document store grid\" will only be run for InMemoryDocumentStore\n- any test that is specific to other document stores (e.g. elasticsearch) and is not supported by the chosen document store will be skipped (and marked in the logs accordingly)\nRun tests that are possible for a selected document store. The InMemoryDocument store is a very good starting point as it doesn't require any of the external docker containers from above: \npytest . --document_store_type=\"memory\"\nRun tests using a combination of document stores:\npytest . --document_store_type=\"memory,elasticsearch\"\nNote: You will need to launch the elasticsearch container here as described above'\nJust run one individual test:\npytest -v test_retriever.py::test_dpr_embedding\nSelect a logical subset of tests via markers and the optional \"not\" keyword:\npytest -m not elasticsearch\npytest -m elasticsearch\npytest -m generator\npytest -m tika\npytest -m not slow\n...\nWriting tests\nIf you are writing a test that depend on a document store, there are a few conventions to define on which document store type this test should/can run:\nOption 1: The test should run on all document stores / those supplied in the CLI arg --document_store_type:\nUse one of the fixtures document_store or document_store_with_docs or document_store_type.\nDo not parameterize it yourself. \nExample:\n```\ndef test_write_with_duplicate_doc_ids(document_store):\n        ...\n        document_store.write(docs)\n        ....\n```\nOption 2: The test is only compatible with certain document stores:\nSome tests you don't want to run on all possible document stores. Either because the test is specific to one/few doc store(s) or the test is not really document store related and it's enough to test it on one document store and speed up the execution time.\nExample:\n```\nCurrently update_document_meta() is not implemented for InMemoryDocStore so it's not listed here as an option\n@pytest.mark.parametrize(\"document_store\", [\"elasticsearch\", \"faiss\"], indirect=True)\ndef test_update_meta(document_store):\n    ....\n``` \nOption 3: The test is not using a document_store/ fixture, but still has a hard requirement for a certain document store:\nExample:\n@pytest.mark.elasticsearch\ndef test_elasticsearch_custom_fields(elasticsearch_fixture):\n    client = Elasticsearch()\n    client.indices.delete(index='haystack_test_custom', ignore=[404])\n    document_store = ElasticsearchDocumentStore(index=\"haystack_test_custom\", text_field=\"custom_text_field\",\n                                                embedding_field=\"custom_embedding_field\")",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-14T09:05:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T21:51:43Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8076321280026626
      ],
      "excerpt": "Perform semantic search and retrieve documents according to meaning, not keywords \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826229569765047,
        0.9611728882946069
      ],
      "excerpt": "Leverage existing knowledge bases and better handle the long tail of queries that chatbots receive. \nAutomate processes by automatically applying a list of questions to new documents and using the extracted answers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9863281793813375,
        0.9437049650274693,
        0.8617934830971493
      ],
      "excerpt": "Pipelines: The Node and Pipeline design of Haystack allows for custom routing of queries to only the relevant components. \nOpen: 100% compatible with HuggingFace's model hub. Tight interfaces to other frameworks (e.g., Transformers, FARM, sentence-transformers) \nScalable: Scale to millions of docs via retrievers, production-ready backends like Elasticsearch / FAISS, and a fastAPI REST API \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9218907404188454,
        0.8001421413404353,
        0.8875097459428726
      ],
      "excerpt": "Developer friendly: Easy to debug, extend and modify. \nCustomizable: Fine-tune models to your domain or implement your custom DocumentStore. \nContinuous Learning: Collect new training data via user feedback in production & improve your models continuously \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9061064776322387,
        0.8775496673337577,
        0.8661285760501358,
        0.8005842660935314,
        0.9521911978613373
      ],
      "excerpt": "| :bar_chart: Benchmarks | Speed & Accuracy of Retriever, Readers and DocumentStores | \n| :telescope: Roadmap | Public roadmap of Haystack | \n| :newspaper: Blog | Read our articles on Medium | \n| :phone: Jobs | We're hiring! Have a look at our open positions | \nThere is a very vibrant and active community around Haystack which we are regularly interacting with! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9715505505787095
      ],
      "excerpt": "If you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9074719791671008,
        0.9281394759914637
      ],
      "excerpt": "We also check Twitter and Stack Overflow. \nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature!  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9523471334185014
      ],
      "excerpt": "To learn how to get started, check out our Contributor Guidelines first. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9835817551548829
      ],
      "excerpt": "Thanks so much to all those who have contributed to our project! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": ":mag: Haystack is an open source NLP framework that leverages Transformer models. It enables developers to implement production-ready neural search, question answering, semantic document search and summarization for a wide range of applications.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deepset-ai/haystack/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 592,
      "date": "Wed, 22 Dec 2021 23:31:24 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/deepset-ai/haystack/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "deepset-ai/haystack",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/Dockerfile",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/ui/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/deepset-ai/haystack/tree/master/docs",
      "https://github.com/deepset-ai/haystack/tree/master/test/samples/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial9_DPR_training.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial7_RAG_Generator.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial16_Document_Classifier_at_Index_Time.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial15_TableQA.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial4_FAQ_style_QA.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial8_Preprocessing.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial12_LFQA.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial6_Better_Retrieval_via_DPR.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial11_Pipelines.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial5_Evaluation.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial10_Knowledge_Graph.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial14_Query_Classifier.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/tutorials/Tutorial13_Question_generation.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/run_docker_gpu.sh",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/.github/workflows/shell_check.sh",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/release_docs.sh",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/v0.10.0/_src/api/api/generate_docstrings.sh",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/v0.9.0/_src/api/api/generate_docstrings.sh",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/_src/api/api/generate_docstrings.sh",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/v1.0.0/_src/api/api/generate_docstrings.sh",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/v0.7.0/_src/api/api/generate_docstrings.sh",
      "https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/v0.8.0/_src/api/api/generate_docstrings.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you're interested in learning more about Haystack and using it as part of your application, we offer several options.\n\n**1. Installing from a package**\n\nYou can install Haystack by using [pip](https://github.com/pypa/pip).\n\n```\n    pip3 install farm-haystack\n```\n\nPlease check our page [on PyPi](https://pypi.org/project/farm-haystack/) for more information.\n\n**2. Installing from GitHub**\n\nYou can also clone it from GitHub \u2014 in case you'd like to work with the master branch and check the latest features:\n\n```\n    git clone https://github.com/deepset-ai/haystack.git\n    cd haystack\n    pip install --editable .\n```\n\nTo update your installation, do a ``git pull``. The ``--editable`` flag will update changes immediately.\n\n**3. Installing on Windows**\n\nOn Windows, you might need:\n\n```\n    pip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.98932330057785
      ],
      "excerpt": "| :floppy_disk: Installation | How to install Haystack | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078780248002013,
        0.8204318494116448
      ],
      "excerpt": "If you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project,  \nyou can start a thread in Github Discussions or our Slack channel. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/deepset-ai/haystack/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Dockerfile",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'\\n                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright 2021 deepset GmbH\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# What to build with Haystack",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "haystack",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "deepset-ai",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deepset-ai/haystack/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "tholor",
        "body": "# :gift: Haystack 1.0 \r\nWe worked hard to bring you an early Christmas present: 1.0 is out! In the last months, we re-designed many essential parts of Haystack, introduced new features, and simplified many user-facing methods. We believe Haystack is now much easier to use and a solid base for many exciting upcoming features that we plan. This release is a major milestone on our journey with you, the community, and we want to thank you again for all the great contributions, discussions, questions, and bug reports that helped us to build a better Haystack. This journey has just started :rocket: \r\n\r\n# :star: Highlights\r\n## Improved Evaluation of Pipelines\r\n\r\nEvaluation helps you find out how well your system is doing on your data. This includes Pipeline level evaluation to ensure that the system's output is really what you're after, but also Node level evaluation so that you can figure out whether it's your Reader or Retriever that is holding back the performance. \r\n\r\nIn this release, evaluation is much simpler and cleaner to perform. All the functionality is now baked into the `Pipeline` class and you can kick off the process by providing `Label` or `MultiLabel` objects to the `Pipeline.eval()` method. \r\n\r\n```python\r\neval_result = pipeline.eval(\r\n    labels=labels,\r\n    params={\"Retriever\": {\"top_k\": 5}},\r\n)\r\n```\r\n\r\nThe output is an `EvaluationResult` object which stores each Node's prediction for each sample in a Pandas `DataFrame` - so you can easily inspect granular predictions and potential mistakes without re-running the whole thing. There is a `EvaluationResult.calculate_metrics()` method which will return the relevant metrics for your evaluation and you can print a convenient summary report via the new .\r\n\r\n\r\n```python\r\nmetrics = eval_result.calculate_metrics()\r\n\r\npipeline.print_eval_report(eval_result)\r\n```\r\n\r\nIf you'd like to start evaluating your own systems on your own data, check out our [Evaluation Tutorial](https://haystack.deepset.ai/tutorials/evaluation)!\r\n\r\n## Table QA\r\n\r\nA lot of valuable information is stored in tables - we've heard this again and again from the community. While they are an efficient structured data format, it hasn't been possible to search for table contents using traditional NLP techniques. But now, with the new `TableTextRetriever` and `TableReader` our users have all the tools they need to query for relevant tables and perform Question Answering.\r\n\r\nThe `TableTextRetriever` is the result of our team's research into table retrieval methods which you can read about in [this paper](https://arxiv.org/abs/2108.04049) that was presented at EMNLP 2021. Behind the scenes, it uses three transformer-based encoders - one for text passages, one for tables, and one for the query. However, in Haystack, you can swap it in for any other dense retrieval model and start working with tables. The `TableReader` is built upon the [TAPAS model](https://github.com/google-research/tapas) and when handed table containing Documents, it can return a single cell as an answer or perform an aggregation operation on a set of cells to form a final answer. \r\n\r\n```python\r\nretriever = TableTextRetriever(\r\n    document_store=document_store,\r\n    query_embedding_model=\"deepset/bert-small-mm_retrieval-question_encoder\",\r\n    passage_embedding_model=\"deepset/bert-small-mm_retrieval-passage_encoder\",\r\n    table_embedding_model=\"deepset/bert-small-mm_retrieval-table_encoder\",\r\n    embed_meta_fields=[\"title\", \"section_title\"]\r\n)\r\n```\r\n\r\n```python\r\nreader = TableReader(\r\n\t\tmodel_name_or_path=\"google/tapas-base-finetuned-wtq\",\r\n\t\tmax_seq_len=512\r\n)\r\n```\r\n\r\nHave a look at the [Table QA documentation](https://haystack.deepset.ai/guides/table-qa) if you'd like to learn more or dive into the [Table QA tutorial](https://haystack.deepset.ai/tutorials/table-qa) to start unlocking the information in your table data.\r\n\r\n## Improved Debugging of Pipelines & Nodes\r\n\r\nWe've made debugging much simpler and also more informative! As long as your node receives a boolean `debug` argument, it can propagate its input, output or even some custom information to the output of the pipeline. It is now a built-in feature of all existing nodes and can also easily be inherited by your custom nodes.\r\n\r\n```python\r\nresult = pipeline.run(\r\n        query=\"Who is the father of Arya Stark?\",\r\n        params={\r\n            \"debug\": True\r\n        }\r\n    )\r\n```\r\n\r\n```python\r\n{'ESRetriever': {'input': {'debug': True,\r\n                           'query': 'Who is the father of Arya Stark?',\r\n                           'root_node': 'Query',\r\n                           'top_k': 1},\r\n                 'output': {'documents': [<Document: {'content': \"\\n===In the Riverlands===\\nThe Stark army reaches the Twins, a bridge strong\", ...}>]\r\n                            ...}\r\n```\r\n\r\nTo find out more about this feature, check out [debugging](https://haystack.deepset.ai/components/pipelines#returning-debugging-information). To learn how to define custom debug information, have a look at [custom debugging](https://haystack.deepset.ai/components/pipelines#custom-debugging-functionality).\r\n\r\n## FARM Migration\r\n\r\nThose of you following Haystack from its first days will know that Haystack first evolved out of the [FARM framework](https://github.com/deepset-ai/FARM). While FARM is designed to handle diverse NLP models and tasks, Haystack gives full end-to-end support to search and question answering use cases with a focus on coordinating all components that take a proof-of-concept into production. \r\n\r\nHaystack has always relied on FARM for much lower-level processing and modeling. To reduce the implementation overhead and simplify debugging, we have migrated the relevant parts of FARM into the new `haystack/modeling` package. \r\n\r\n# :warning: Breaking Changes & Migration Guide\r\n## Migration to v1.0\r\n\r\nWith the release of v1.0, we decided to make some bold changes.\r\nWe believe this has brought a significant improvement in usability and makes the project more future-proof.\r\nWhile this does come with a few breaking changes, and we do our best to guide you on how to go from v0.x to v1.0.\r\nFor more details see the [Migration Guide](https://haystack.deepset.ai/overview/migration) and if you need more guidance, just reach out via [Slack](https://haystack.deepset.ai/community/join). \r\n\r\n## New Package Structure & Changed Imports \r\n\r\nDue to the ever-increasing number of Nodes and Document Stores being integrated into Haystack,\r\nwe felt the need to implement a repository structure that makes it easier to navigate to what you're looking for. We've also shortened the length of the imports.\r\n\r\n`haystack.document_stores`\r\n\r\n- All Document Stores can now be directly accessed from here\r\n- Note the pluralization of `document_store` to `document_stores`\r\n\r\n`haystack.nodes`\r\n\r\n- This directory directly contains any class that can be used as a node\r\n- This includes File Converters and PreProcessors\r\n\r\n`haystack.pipelines`\r\n\r\n- This contains all the base, custom and pre-made pipeline classes\r\n- Note the pluralization of `pipeline` to `pipelines`\r\n\r\n`haystack.utils`\r\n\r\n- Any utility functions\r\n\r\n:arrow_right:  For the large majority of imports, the old style still works but this will be deprecated in future releases!\r\n\r\n## Primitive Objects\r\n\r\nInstead of relying on dictionaries, Haystack now standardizes more of the inputs and outputs of Nodes using the following primitive classes:\r\n\r\n- [Document](https://haystack.deepset.ai/components/primitives#document)\r\n- [Answer](https://haystack.deepset.ai/components/primitives#answer)\r\n- [Label](https://haystack.deepset.ai/components/primitives#label)\r\n- [MultiLabel](https://haystack.deepset.ai/components/primitives#multilabel)\r\n- [Span](https://haystack.deepset.ai/components/primitives#span)\r\n\r\nWith these, there is now support for data structures beyond text and the REST API schema is built around their structure.\r\nUsing these classes also allows for the autocompletion of fields in your IDE.\r\n\r\n**Tip:** To see examples of these primitive classes being returned, have a look at [Ready-Made Pipelines](https://haystack.deepset.ai/components/ready-made-pipelines).\r\n\r\n\r\nMany of the fields in these classes have also been renamed or removed.\r\nYou can see a more comprehensive list of them in this [Github issue](https://github.com/deepset-ai/haystack/issues/1590).\r\nBelow, we will go through a few cases that are likely to impact established workflows.\r\n\r\n## Input Document Format\r\n\r\nThis dictionary schema used to be the recommended way to prepare your data to be indexed.\r\nNow we strongly recommend using our dedicated [`Document`](https://haystack.deepset.ai/components/primitives) class as a replacement.\r\nThe `text` field has been renamed `content` to accommodate for cases where it is used for another data format,\r\nfor example in [Table QA](https://haystack.deepset.ai/guides/table-qa).\r\n\r\n<details>\r\n  <summary>Click here to see code example</summary>\r\n\r\n**v0.x:**\r\n```\r\ndoc = {\r\n\t'text': 'DOCUMENT_TEXT_HERE',\r\n\t'meta': {'name': DOCUMENT_NAME, ...}\r\n}\r\n```\r\n\r\n**v1.0:**\r\n```\r\ndoc = Document(\r\n    content='DOCUMENT_TEXT_HERE',\r\n    meta={'name': DOCUMENT_NAME, ...}\r\n)\r\n```\r\nFrom here, you can take the same steps to write Documents into your Document Store.\r\n```\r\ndocument_store.write_documents([doc])\r\n```\r\n\r\n</details>\r\n\r\n## Response format of Reader\r\n\r\nAll Reader Nodes now return [Answer](https://haystack.deepset.ai/components/primitives#label) objects instead of dictionaries.\r\n<details>\r\n  <summary>Click here to see code example</summary>\r\n\r\n**v0.x:**\r\n```\r\n[\r\n    {\r\n        'answer': 'Fang',\r\n        'score': 13.26807975769043,\r\n        'probability': 0.9657130837440491,\r\n        'context': \"\"\"\u041a\u0440\u0438\u0432\u043e\u043b\u0430\u043f\u0438\u043a (Kryvolapyk, kryvi lapy \"crooked paws\")\r\n            ===Fang (Hagrid's dog)===\r\n            *Chinese (PRC): \u7259\u7259 (ya2 ya) (from \u7259 \"tooth\", \u7259,\"\"\"\r\n    }\r\n]\r\n```\r\n\r\n**v1.0:**\r\n```\r\n[\r\n    <Answer {'answer': 'Eddard', 'type': 'extractive', 'score': 0.9946763813495636, 'context': \"s Nymeria after a legendary warrior queen. She travels...\", 'offsets_in_document': [{'start': 147, 'end': 153}], 'offsets_in_context': [{'start': 72, 'end': 78}], 'document_id': 'ba2a8e87ddd95e380bec55983ee7d55f', 'meta': {'name': '43_Arya_Stark.txt'}}>,\r\n    <Answer {'answer': 'King Robert', 'type': 'extractive', 'score': 0.9251320660114288, 'context': 'ordered by the Lord of Light. Melisandre later reveals to Gendry that...', 'offsets_in_document': [{'start': 1808, 'end': 1819}], 'offsets_in_context': [{'start': 70, 'end': 81}], 'document_id': '7b67b0e27571c2b2025a34b4db18ad49', 'meta': {'name': '349_List_of_Game_of_Thrones_characters.txt'}}>,\r\n    <Answer {'answer': 'Ned', 'type': 'extractive', 'score': 0.8103329539299011, 'context': \" girl disguised as a boy all along and is surprised to learn she is Arya...\", 'offsets_in_document': [{'start': 920, 'end': 923}], 'offsets_in_context': [{'start': 74, 'end': 77}], 'document_id': '7b67b0e27571c2b2025a34b4db18ad49', 'meta': {'name': '349_List_of_Game_of_Thrones_characters.txt'}}>,\r\n    ...\r\n]\r\n```\r\n\r\n</details>\r\n\r\n## Label Structure\r\n\r\nThe attributes of the Label object have gone through some changes.\r\nTo see their current structure see [Label](https://haystack.deepset.ai/components/primitives#label).\r\n\r\n<details>\r\n  <summary>Click here to see code example</summary>\r\n\r\n**v0.x:**\r\n```\r\nlabel = Label(\r\n    question=QUESTION_TEXT_HERE,\r\n    answer=ANSWER_STRING_HERE,\r\n    ...\r\n)\r\n```\r\n\r\n**v1.0:**\r\n```\r\nlabel = Label(\r\n    query=QUERY_TEXT_HERE,\r\n    answer=Answer(...),\r\n    ...\r\n)\r\n```\r\n\r\n</details>\r\n\r\n## REST API Format\r\nThe response format for the `/query` matches that of the primitive objects, only in JSON form.\r\nThis means, there are similar breaking changes as described above for the `Answer` format of a Reader. \r\nParticularly, the names of the offset fields have changed and need to be aligned to the new format when coming from Haystack v0.x.\r\nFor detailed examples and guidance see the [Migration Guide](https://haystack.deepset.ai/overview/migration).\r\n\r\n## Other breaking changes\r\n* Save/load of FAISSDocumentstore @ZanSara in https://github.com/deepset-ai/haystack/pull/1459\r\n* Add AzureConverter & change response format of FileConverter.convert() by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1813\r\n\r\n\r\n# :nerd_face: Detailed Changes\r\n### Pipeline\r\n* Return intermediate nodes output in pipelines by @ZanSara in https://github.com/deepset-ai/haystack/pull/1558\r\n* Add `debug` and `debug_logs` params to standard pipelines by @tholor in https://github.com/deepset-ai/haystack/pull/1586\r\n* Pipeline node names validation by @ZanSara in https://github.com/deepset-ai/haystack/pull/1601\r\n* Multi query eval by @tstadel in https://github.com/deepset-ai/haystack/pull/1746\r\n* Pipelines now tolerate custom _debug content by @ZanSara in https://github.com/deepset-ai/haystack/pull/1756\r\n* Adding yaml functionality to standard pipelines (save/load...) by @MichelBartels in https://github.com/deepset-ai/haystack/pull/1735\r\n* Calculation of metrics and presentation of eval results by @tstadel in https://github.com/deepset-ai/haystack/pull/1760\r\n* Fix loading and saving of EvaluationReszult by @tstadel in https://github.com/deepset-ai/haystack/pull/1831\r\n* remove queries param from pipeline.eval() by @tstadel in https://github.com/deepset-ai/haystack/pull/1836\r\n* Deprecate old pipeline eval nodes: EvalDocuments and EvalAnswers by @tstadel in https://github.com/deepset-ai/haystack/pull/1778\r\n### Models\r\n* Farm merging base by @Timoeller in https://github.com/deepset-ai/haystack/pull/1422\r\n* Add inferencer for QA only by @julian-risch in https://github.com/deepset-ai/haystack/pull/1484\r\n* Remove mentions of FARM from Ranker comments by @julian-risch in https://github.com/deepset-ai/haystack/pull/1535\r\n* Remove NER and text classification from model conversion by @julian-risch in https://github.com/deepset-ai/haystack/pull/1536\r\n* TransformersDocumentClassifier replacing FARMClassifier by @julian-risch in https://github.com/deepset-ai/haystack/pull/1540\r\n* LFQA: Remove InferenceProcessor dependency by @vblagoje in https://github.com/deepset-ai/haystack/pull/1559\r\n* Add BatchEncoding flatten by @vblagoje in https://github.com/deepset-ai/haystack/pull/1562\r\n* Enable GPU usage for question generator by @tholor in https://github.com/deepset-ai/haystack/pull/1571\r\n* Create EntityExtractor by @ZanSara in https://github.com/deepset-ai/haystack/pull/1573\r\n* Add more flexible options for model downloads (Proxies, resume_download, local_files_only...) by @tholor in https://github.com/deepset-ai/haystack/pull/1256\r\n* Add checkpointing for reader.train() to allow stopping + resuming training by @gak97 in https://github.com/deepset-ai/haystack/pull/1554\r\n* DPR training: Rename `TransformersAdamW` to `AdamW` by @ZanSara in https://github.com/deepset-ai/haystack/pull/1613\r\n* Add TableTextRetriever by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1529\r\n* Truncate too large tables for TableReader by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1662\r\n* ensure tf-idf matrix calculation before retrieval by @julian-risch in https://github.com/deepset-ai/haystack/pull/1665\r\n* Add TableTextRetriever to nodes' __init__.py by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1678\r\n* Fix TableReader when model does not select any cells by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1703\r\n* Standardize initialisation of device settings by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1683\r\n* fix issue #1687 - DPR training fails on multiple GPU's by @AlonEirew in https://github.com/deepset-ai/haystack/pull/1688\r\n* Allow TableReader models without aggregation classifier by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1772\r\n* Huggingface private model support via API tokens (FARMReader) by @ArzelaAscoIi in https://github.com/deepset-ai/haystack/pull/1775\r\n* private hugging face models for retrievers by @ArzelaAscoIi in https://github.com/deepset-ai/haystack/pull/1785\r\n* Model Distillation by @MichelBartels in https://github.com/deepset-ai/haystack/pull/1758\r\n* Added max_seq_length and batch_size params to embeddingretriever by @AhmedIdr in https://github.com/deepset-ai/haystack/pull/1817\r\n* Fix bug ranker: wrong lambda function by @gabinguo in https://github.com/deepset-ai/haystack/pull/1824 \r\n### DocumentStores\r\n* Fix bug when loading FAISS from supplied config file path by @ZanSara in https://github.com/deepset-ai/haystack/pull/1506\r\n* Standardize `delete_documents(filter=...)` across all document stores by @ZanSara in https://github.com/deepset-ai/haystack/pull/1509\r\n* Update sql.py to ignore multi thread issues. by @adithyaur99 in https://github.com/deepset-ai/haystack/pull/1442\r\n* [fix] MySQL connection 'check_same_thread' error by @CandiceYu8 in https://github.com/deepset-ai/haystack/pull/1585\r\n* Delete documents by ID in all document stores by @ZanSara in https://github.com/deepset-ai/haystack/pull/1606\r\n* Fix Opensearch field type (flattened -> nested) by @tholor in https://github.com/deepset-ai/haystack/pull/1609\r\n* Add delete_labels() except for weaviate doc store by @julian-risch in https://github.com/deepset-ai/haystack/pull/1604\r\n* Experimental changes to support Milvus 2.x by @lalitpagaria in https://github.com/deepset-ai/haystack/pull/1473\r\n* Fix import in Milvus2DocumentStore by @ZanSara in https://github.com/deepset-ai/haystack/pull/1646\r\n* Allow setting of `scroll` param in ElasticsearchDocumentStore by @Timoeller in https://github.com/deepset-ai/haystack/pull/1645\r\n* Rename every occurrence of 'embed_passages' with 'embed_documents' by @ZanSara in https://github.com/deepset-ai/haystack/pull/1667\r\n* Cosine similarity for the rest of DocStores. by @fingoldo in https://github.com/deepset-ai/haystack/pull/1569\r\n* Make weaviate more compliant to other doc stores (UUIDs and dummy embedddings) by @julian-risch in https://github.com/deepset-ai/haystack/pull/1656\r\n* Make FAISSDocumentStore work with yaml by @tstadel in https://github.com/deepset-ai/haystack/pull/1727\r\n* Capitalize starting letter in params by @nishanthcgit in https://github.com/deepset-ai/haystack/pull/1750\r\n* Support Tables in all DocumentStores by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1744\r\n* Facilitate concurrent query / indexing in Elasticsearch with dense retrievers (new `skip_missing_embeddings` param) by @cvgoudar in https://github.com/deepset-ai/haystack/pull/1762\r\n* Introduced an arg to add synonyms - Elasticsearch by @SjSnowball in https://github.com/deepset-ai/haystack/pull/1625\r\n* Allow SQLDocumentStore to filter by many filters by @ZanSara in https://github.com/deepset-ai/haystack/pull/1776\r\n### REST API\r\n* Add rest api endpoint to delete documents by filter by @ZanSara in https://github.com/deepset-ai/haystack/pull/1546\r\n* Fix circular import in the REST API by @ZanSara in https://github.com/deepset-ai/haystack/pull/1556\r\n* Add `/documents/get_by_filters` endpoint by @ZanSara in https://github.com/deepset-ai/haystack/pull/1580\r\n* Add a restart policy `on-failure` to all containers by @ZanSara in https://github.com/deepset-ai/haystack/pull/1664\r\n* Add execute permissions to file upload folder by @Timoeller in https://github.com/deepset-ai/haystack/pull/1666\r\n* disable file upload for InMemoryDocStore by @julian-risch in https://github.com/deepset-ai/haystack/pull/1677\r\n* Improve open api spec by @tholor in https://github.com/deepset-ai/haystack/pull/1700\r\n* Fix usage of filters in `/query` endpoint in REST API by @tholor in https://github.com/deepset-ai/haystack/pull/1774\r\n* ignore empty filters parameter by @julian-risch in https://github.com/deepset-ai/haystack/pull/1783\r\n* Fix the REST API tests by @ZanSara in https://github.com/deepset-ai/haystack/pull/1791\r\n### UI / Demo\r\n* Add \"API is loading\" message in the UI by @ZanSara in https://github.com/deepset-ai/haystack/pull/1493\r\n* Fix answer format in ui by @tholor in https://github.com/deepset-ai/haystack/pull/1591\r\n* Change 'ESRetriever' with 'Retriever' in the Streamlit app by @ZanSara in https://github.com/deepset-ai/haystack/pull/1620\r\n* Public demo by @ZanSara in https://github.com/deepset-ai/haystack/pull/1747\r\n* Small fixes to the public demo by @ZanSara in https://github.com/deepset-ai/haystack/pull/1781\r\n* Add missing dependency to the Streamlit container by @ZanSara in https://github.com/deepset-ai/haystack/pull/1798\r\n* Improve the Random Question functionality by @ZanSara in https://github.com/deepset-ai/haystack/pull/1808\r\n* Add description to the demo by @ZanSara in https://github.com/deepset-ai/haystack/pull/1809\r\n* Fix UI demo feedback by @ZanSara in https://github.com/deepset-ai/haystack/pull/1816\r\n* Remove feedback from no-answers by @ZanSara in https://github.com/deepset-ai/haystack/pull/1827\r\n* Demo UI add env vars & other small fixes by @ZanSara in https://github.com/deepset-ai/haystack/pull/1828\r\n* More demo bugfixes by @ZanSara in https://github.com/deepset-ai/haystack/pull/1832\r\n* Add backlink below the context, if available in the doc's meta by @ZanSara in https://github.com/deepset-ai/haystack/pull/1834\r\n### Documentation\r\n* changed delete_all_documents to delete_documents in Tutorial5 by @ju-gu in https://github.com/deepset-ai/haystack/pull/1477\r\n* Regenerate API and Tutorial md files by @brandenchan in https://github.com/deepset-ai/haystack/pull/1480\r\n* Define SAS model in notebook by @brandenchan in https://github.com/deepset-ai/haystack/pull/1485\r\n* Update Tutorial1_Basic_QA_Pipeline.ipynb by @julian-risch in https://github.com/deepset-ai/haystack/pull/1489\r\n* Clarify PDF conversion, languages and encodings by @MarkusSagen in https://github.com/deepset-ai/haystack/pull/1570\r\n* Fix Tutorials by @tholor in https://github.com/deepset-ai/haystack/pull/1594\r\n* Update Crawler documentation by @ju-gu in https://github.com/deepset-ai/haystack/pull/1588\r\n* add note on gpu runtime to tutorial 13 by @julian-risch in https://github.com/deepset-ai/haystack/pull/1614\r\n* Update jobs link to personio by @julian-risch in https://github.com/deepset-ai/haystack/pull/1611\r\n* Update jobs link in readme by @julian-risch in https://github.com/deepset-ai/haystack/pull/1629\r\n* Bugfix Tutorial 5 parameters, adjust default split length by @Timoeller in https://github.com/deepset-ai/haystack/pull/1635\r\n* Fix parameter names in tutorial 5 and 12 by @julian-risch in https://github.com/deepset-ai/haystack/pull/1639\r\n* Link the logo to the website by @aantti in https://github.com/deepset-ai/haystack/pull/1649\r\n* Replace Haystack banner for readme by @brandenchan in https://github.com/deepset-ai/haystack/pull/1654\r\n* Update README.md by @brandenchan in https://github.com/deepset-ai/haystack/pull/1653\r\n* fix typo in docstring of crawler  by @ju-gu in https://github.com/deepset-ai/haystack/pull/1673\r\n* Add TableQA tutorial by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1670\r\n* Add collapsing sections to readme by @brandenchan in https://github.com/deepset-ai/haystack/pull/1663\r\n* Fix links in readme.md by @brandenchan in https://github.com/deepset-ai/haystack/pull/1682\r\n* fixed typo by @julian-risch in https://github.com/deepset-ai/haystack/pull/1680\r\n* Standardize similarity argument description by @brandenchan in https://github.com/deepset-ai/haystack/pull/1684\r\n* Fix Typo in TableQA Tutorial by @Timoeller in https://github.com/deepset-ai/haystack/pull/1690\r\n* Improve tutorials' output by @ZanSara in https://github.com/deepset-ai/haystack/pull/1694\r\n* Tutorial for DocumentClassifier at Index Time by @tstadel in https://github.com/deepset-ai/haystack/pull/1697\r\n* Update API Reference Pages for v1.0 by @brandenchan in https://github.com/deepset-ai/haystack/pull/1729\r\n* Add debugging example to tutorial by @brandenchan in https://github.com/deepset-ai/haystack/pull/1731\r\n* Fix a few details of some tutorials by @ZanSara in https://github.com/deepset-ai/haystack/pull/1733\r\n* initialize doc store with doc and label index in tutorial 5 by @julian-risch in https://github.com/deepset-ai/haystack/pull/1730\r\n* Fix Tutorial 11 on Google Colab by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1795\r\n* Fix link to colab notebook in tutorial 16 by @julian-risch in https://github.com/deepset-ai/haystack/pull/1802\r\n### Other Changes\r\n* Redesign primitives by @tholor in https://github.com/deepset-ai/haystack/pull/1398\r\n* Adding prediction head, trainer, evaluator from FARM by @julian-risch in https://github.com/deepset-ai/haystack/pull/1419\r\n* Farm merging base bogdan by @Timoeller in https://github.com/deepset-ai/haystack/pull/1424\r\n* Add data, add tests for qa processor, add dpr tests (some failing) by @Timoeller in https://github.com/deepset-ai/haystack/pull/1425\r\n* Fix DPR tests + add Tokenizer tests by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1429\r\n* Farm merging base fix test by @Timoeller in https://github.com/deepset-ai/haystack/pull/1444\r\n* Automate updates docstrings tutorials by @PiffPaffM in https://github.com/deepset-ai/haystack/pull/1461\r\n* fixed workflow conflict with intorducting new one by @PiffPaffM in https://github.com/deepset-ai/haystack/pull/1472\r\n* feat: normalize embeddings for faiss cosine similarity by @mathislucka in https://github.com/deepset-ai/haystack/pull/1352\r\n* Remove 'restart=always' from 'haystack-api' in both docker-compose files by @ZanSara in https://github.com/deepset-ai/haystack/pull/1498\r\n* Add comment to tutorial notebooks about restarting runtime in colab by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1486\r\n* Feat: Download archive from url without temp file by @lalitpagaria in https://github.com/deepset-ai/haystack/pull/1470\r\n* Add newline between paragraphs in DocxToTextConverter by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1500\r\n* Release Docs 0.10.0 by @PiffPaffM in https://github.com/deepset-ai/haystack/pull/1460\r\n* Simplify tests & allow running on individual doc stores by @tholor in https://github.com/deepset-ai/haystack/pull/1487\r\n* Replace FARM import statements; add dependencies by @julian-risch in https://github.com/deepset-ai/haystack/pull/1492\r\n* Fix document_store_type flag for tests with multiple fixtures by @tholor in https://github.com/deepset-ai/haystack/pull/1526\r\n* Remove double mentions from requirements by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1545\r\n* Format doc classifier usage example by @julian-risch in https://github.com/deepset-ai/haystack/pull/1550\r\n* Adding TfidfRetriever to __init__.py of the retriever package by @mhamdan91 in https://github.com/deepset-ai/haystack/pull/1575\r\n* Limit generator tests to memory doc store; split pipeline tests by @julian-risch in https://github.com/deepset-ai/haystack/pull/1602\r\n* Add Table Reader by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1446\r\n* Switch from dataclass to pydantic dataclass & Fix Swagger API Docs by @tholor in https://github.com/deepset-ai/haystack/pull/1598\r\n* Use smaller model for one generator test case by @julian-risch in https://github.com/deepset-ai/haystack/pull/1622\r\n* Make EntityExtractor work when loaded from YAML by @ZanSara in https://github.com/deepset-ai/haystack/pull/1636\r\n* Improve docker images: Add nltk download, add folder for file upload by @Timoeller in https://github.com/deepset-ai/haystack/pull/1633\r\n* Refactoring of the `haystack` package by @ZanSara in https://github.com/deepset-ai/haystack/pull/1624\r\n* Remove trailing comma in import statement by @julian-risch in https://github.com/deepset-ai/haystack/pull/1655\r\n* Raise a warning if the 'query' param of the 'query' method of 'ElasticsearchDocumentStore' is not a string by @ZanSara in https://github.com/deepset-ai/haystack/pull/1674\r\n* Add CI for windows runner by @lalitpagaria in https://github.com/deepset-ai/haystack/pull/1458\r\n* rename text variable of document to content by @julian-risch in https://github.com/deepset-ai/haystack/pull/1704\r\n* Simplify logs management by @ZanSara in https://github.com/deepset-ai/haystack/pull/1696\r\n* Change answer aggregation key to (doc_id, query) instead of (label_id, query) by @julian-risch in https://github.com/deepset-ai/haystack/pull/1726\r\n* Fix another self.device/s typo by @ZanSara in https://github.com/deepset-ai/haystack/pull/1734\r\n* Fix `print_answers` by @ZanSara in https://github.com/deepset-ai/haystack/pull/1743\r\n* Split pipeline tests into three suites by @ZanSara in https://github.com/deepset-ai/haystack/pull/1755\r\n* Split summarizer tests in order to make windows CI work again by @tstadel in https://github.com/deepset-ai/haystack/pull/1757\r\n* Update test_pipeline_extractive_qa.py by @julian-risch in https://github.com/deepset-ai/haystack/pull/1763\r\n* Exclude test_summarizer_translation.py for windows_ci by @tstadel in https://github.com/deepset-ai/haystack/pull/1759\r\n* Upgrade torch to v1.10.0 by @bogdankostic in https://github.com/deepset-ai/haystack/pull/1789\r\n* Adapt docker-compose-gpu.yml to use DPR by default by @ZanSara in https://github.com/deepset-ai/haystack/pull/1810\r\n* bugfix metadata extraction in form recognizer & split of surrounding content length by @ju-gu in https://github.com/deepset-ai/haystack/pull/1829\r\n* Fix OOM in test_eval.py Windows CI by @tstadel in https://github.com/deepset-ai/haystack/pull/1830\r\n* Update evaluation tutorial to cover the new `pipeline.eval()` by @julian-risch in https://github.com/deepset-ai/haystack/pull/1765\r\n* Add config for github release notes by @tholor in https://github.com/deepset-ai/haystack/pull/1840\r\n* Extend categories for release notes by @tholor in https://github.com/deepset-ai/haystack/pull/1841\r\n\r\n## New Contributors\r\n* @mathislucka made their first contribution in https://github.com/deepset-ai/haystack/pull/1352\r\n* @ZanSara made their first contribution in https://github.com/deepset-ai/haystack/pull/1459\r\n* @ju-gu made their first contribution in https://github.com/deepset-ai/haystack/pull/1477\r\n* @adithyaur99 made their first contribution in https://github.com/deepset-ai/haystack/pull/1442\r\n* @mhamdan91 made their first contribution in https://github.com/deepset-ai/haystack/pull/1575\r\n* @CandiceYu8 made their first contribution in https://github.com/deepset-ai/haystack/pull/1585\r\n* @gak97 made their first contribution in https://github.com/deepset-ai/haystack/pull/1554\r\n* @fingoldo made their first contribution in https://github.com/deepset-ai/haystack/pull/1569\r\n* @AlonEirew made their first contribution in https://github.com/deepset-ai/haystack/pull/1688\r\n* @tstadel made their first contribution in https://github.com/deepset-ai/haystack/pull/1697\r\n* @nishanthcgit made their first contribution in https://github.com/deepset-ai/haystack/pull/1750\r\n* @ArzelaAscoIi made their first contribution in https://github.com/deepset-ai/haystack/pull/1775\r\n* @SjSnowball made their first contribution in https://github.com/deepset-ai/haystack/pull/1625\r\n* @AhmedIdr made their first contribution in https://github.com/deepset-ai/haystack/pull/1817\r\n* @gabinguo made their first contribution in https://github.com/deepset-ai/haystack/pull/1824 \r\n\r\n:heart: Thanks to all contributors and the whole community!",
        "dateCreated": "2021-12-08T07:46:38Z",
        "datePublished": "2021-12-08T08:05:22Z",
        "html_url": "https://github.com/deepset-ai/haystack/releases/tag/v1.0.0",
        "name": "1.0.0",
        "tag_name": "v1.0.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/haystack/tarball/v1.0.0",
        "url": "https://api.github.com/repos/deepset-ai/haystack/releases/54851132",
        "zipball_url": "https://api.github.com/repos/deepset-ai/haystack/zipball/v1.0.0"
      },
      {
        "authorType": "User",
        "author_name": "tholor",
        "body": "# :star: Highlights\r\n## :rocket: Making Pipelines more scalable \r\nYou can now easily scale and distribute Haystack Pipelines thanks to the new integration of the Ray framework (https://ray.io/). \r\nRay allows distributing a Pipeline's components across a cluster of machines. The individual components of a Pipeline can be independently scaled. For instance, an extractive QA Pipeline deployment can have three replicas of the Reader and a single replica for the Retriever. It enables efficient resource utilization by horizontally scaling Components. You can use Ray via the new `RayPipeline` class ([#1255](https://github.com/deepset-ai/haystack/pull/1255))\r\n\r\nTo set the number of replicas, add replicas in the YAML config for the node in a pipeline:\r\n\r\n``` \r\ncomponents:\r\n    ...\r\n\r\npipelines:\r\n  - name: ray_query_pipeline\r\n    type: RayPipeline\r\n    nodes:\r\n      - name: ESRetriever\r\n        replicas: 2  # number of replicas to create on the Ray cluster\r\n        inputs: [ Query ]\r\n```\r\n\r\nA RayPipeline currently can only be created with a YAML Pipeline config:\r\n\r\n```\r\nfrom haystack.pipeline import RayPipeline\r\npipeline = RayPipeline.load_from_yaml(path=\"my_pipelines.yaml\", pipeline_name=\"my_query_pipeline\")\r\npipeline.run(query=\"What is the capital of Germany?\")\r\n```\r\n\r\nSee [docs](https://haystack.deepset.ai/components/pipelines#distributed-pipelines-with-ray) for more details \r\n\r\n## :heart_eyes: Making Pipelines more user-friendly \r\nThe old `Pipeline` design came with a couple of flaws: \r\n- Impossible to route certain parameters (e.g. `top_k`) to dedicated nodes\r\n- Incorrect parameters in pipeline.run() are silently swallowed \r\n- Hard to understand what is in **kwargs when working with node.run() methods\r\n- Hard to debug\r\n\r\nWe tackled those with a big refactoring of the `Pipeline` class and changed how data is passed between nodes [#1321](https://github.com/deepset-ai/haystack/pull/1321).\r\nThis comes now with a few breaking changes: \r\n\r\nComponent params like top_k, no_ans_boost for Pipeline.run() must be passed in a params dict\r\n\r\n```\r\npipeline.run(query=\"Why?\", params={\"top_k\":10, \"no_ans_boost\":0.5})\r\n\r\n```\r\nComponent specific top_ks like top_k_reader, top_k_retriever are now replaced with top_k. To disambiguate, the params can be \"targeted\" to a specific node. \r\n\r\n```\r\npipeline.run(query=\"Why?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5})\r\n```\r\n\r\nSee *breaking changes* section and the [docs](https://haystack.deepset.ai/components/pipelines) for details\r\n\r\n\r\n## :chart_with_upwards_trend: Better evaluation metric for QA: Semantic Answer Similarity (SAS) \r\nThe evaluation of question answering models compares ground-truth annotations with model predictions. However, as of today, this comparison is mostly lexical-based and therefore misses out on answers that have no lexical overlap but are still semantically similar, thus treating correct answers as false. This underestimation of the true performance of models hinders user acceptance in applications and complicates a fair comparison of different models. Therefore, there is a need for an evaluation metric that is based on semantics instead of pure string similarity. In our recent EMNLP paper, we proposed \"SAS\", a cross-encoder-based metric for the estimation of semantic answer similarity. We compared it to seven existing metrics and found that it correlates better with human judgement.  [See our paper](https://arxiv.org/abs/2108.06130) [#1338](https://github.com/deepset-ai/haystack/pull/1338)\r\n\r\nYou can use it in Haystack like this: \r\n\r\n```\r\n...\r\n# initialize the node with a SAS model\r\neval_reader = EvalAnswers(sas_model=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\r\n\r\n# define a pipeline \r\np = Pipeline()\r\np.add_node(component=retriever, name=\"ESRetriever\", inputs=[\"Query\"])\r\np.add_node(component=eval_retriever, name=\"EvalDocuments\", inputs=[\"ESRetriever\"])\r\np.add_node(component=reader, name=\"QAReader\", inputs=[\"EvalDocuments\"])\r\np.add_node(component=eval_reader, name=\"EvalAnswers\", inputs=[\"QAReader\"])\r\n...\r\n```\r\nSee our updated [Tutorial 5](https://haystack.deepset.ai/tutorials/evaluation) for a full example.\r\n\r\n## :exploding_head: New nodes: Doc Classifier, Re-Ranker, QuestionGenerator & more \r\nMore nodes, more use cases:\r\n- `FARMClassifier` node for Document Classification: tag a document at indexing time or add a class downstream in your inference pipeline [#1265](https://github.com/deepset-ai/haystack/pull/1265)\r\n- `SentenceTransformersRanker`: Re-Rank your documents after retrieval to maximize the relevance of your results. This implementation uses the popular sentence-transformer models [#1209](https://github.com/deepset-ai/haystack/pull/1209)\r\n- `QuestionGenerator`: Question Answering systems are trained to find an answer given a question and a document; but with the recent advances in generative NLP, there are now models that can read a document and suggest questions that can be answered by that document. All this power is available to you now via the `QuestionGenerator` class.\r\n`QuestionGenerator` models can be trained using Question Answering datasets. Instead of predicting answers, the `QuestionGenerator` takes the document as input and is trained to output the questions. This can be useful when you want to add \"autosuggest\" questions in your search bar or accelerate labeling processes [See docs](https://haystack.deepset.ai/components/question-generator) ([#1267](https://github.com/deepset-ai/haystack/pull/1267))\r\n\r\n##  :telescope: Better support for OpenSearch\r\nWe now support Approximate nearest neighbour (ANN) search in OpenSearch ([#1225](https://github.com/deepset-ai/haystack/pull/1225)) and fixed some initialization issues.\r\n\r\n\r\n## :bookmark_tabs: New Tutorials \r\n-   Tutorial 13 - Question Generation:[Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial13_Question_generation.ipynb)|[Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial13_Question_generation.ipynb)|[Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial13_Question_generation.py)\r\n-   Tutorial 14 - Query Classifier:[Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial14_Query_Classifier.ipynb)|[Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial14_Query_Classifier.ipynb)|[Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial14_Query_Classifier.py)\r\n\r\n\r\n# \u26a0\ufe0f Breaking Changes\r\n## `probability` field removed from results [#1340](https://github.com/deepset-ai/haystack/pull/1340)\r\nHaving two fields `probability` and `score` in answers / documents returned from nodes caused often confusion.\r\nFrom now on we'll only have **one** field called `score` that is in range [0,1]. In QA results, this field is populated with the old `probability` value, so you can simply switch to this one. These fields have changed in Python and REST API.\r\n\r\n**Old:** \r\n```\r\n{\r\n  \"query\": \"Who is the father of Arya Stark?\",\r\n  \"answers\": [\r\n    {\r\n      \"answer\": \"Lord Eddard Stark\",\r\n      \"score\": 14.684528350830078,\r\n      \"probability\": 0.9044522047042847,\r\n      \"context\": ...,\r\n      ...\r\n    },\r\n   ...\r\n   ]\r\n}\r\n\r\n```\r\n**New:** \r\n```\r\n{\r\n  \"query\": \"Who is the father of Arya Stark?\",\r\n  \"answers\": [\r\n    {\r\n      \"answer\": \"Lord Eddard Stark\",\r\n      \"score\": 0.9044522047042847,\r\n      \"context\": ...,\r\n      ...\r\n    },\r\n   ...\r\n   ]\r\n}\r\n\r\n```\r\n\r\n## Removed`Finder` [#1326](https://github.com/deepset-ai/haystack/pull/1326)\r\nAfter being deprecated a few months ago, `Finder` is now gone - R.I.P \r\n\r\n## Params in `Pipeline.run()` [#1321](https://github.com/deepset-ai/haystack/pull/1321) \r\nComponent params like top_k, no_ans_boost for Pipeline.run() must be passed in a params dict\r\n\r\n**Old:** \r\n```\r\npipeline.run(query=\"Why?\", top_k_retriever=10, no_ans_boost=0.5)\r\n```\r\n\r\n**New:** \r\n```\r\npipeline.run(query=\"Why?\", params={\"top_k\":10, \"no_ans_boost\":0.5})\r\n\r\n```\r\nComponent specific top_ks like top_k_reader, top_k_retriever are now replaced with top_k. To disambiguate, the params can be \"targeted\" to a specific node. \r\n**Old:** \r\n``` \r\npipeline.run(query=\"Why?\", top_k_retriever=10, top_k_reader=5)\r\n```\r\n**New:** \r\n\r\n```\r\npipeline.run(query=\"Why?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5})\r\n```\r\n\r\nAlso, custom nodes must not have **kwargs in their run methods anymore and should only return the data (e.g. answers) they produce themselves. \r\n\r\n\r\n# :nerd_face: Detailed Changes\r\n\r\n## Crawler\r\n- Serialize crawler output to JSON [#1284](https://github.com/deepset-ai/haystack/pull/1284)\r\n- Add Crawler support for indexing pipeline [#1360](https://github.com/deepset-ai/haystack/pull/1360)\r\n\r\n## Converter\r\n- Add ImageToTextConverter and PDFToTextOCRConverter that utilize OCR [#1349](https://github.com/deepset-ai/haystack/pull/1349)\r\n\r\n## Preprocessor\r\n- Add PreProcessor optional language parameter. [#1160](https://github.com/deepset-ai/haystack/pull/1160)\r\n- Improve preprocessing logging [#1263](https://github.com/deepset-ai/haystack/pull/1263)\r\n- Make PreProcessor.process() work on lists of documents [#1163](https://github.com/deepset-ai/haystack/pull/1163)\r\n\r\n## Pipeline\r\n- Add Ray integration for Pipelines [#1255](https://github.com/deepset-ai/haystack/pull/1255)\r\n- MostSimilarDocumentsPipeline introduced [#1413](https://github.com/deepset-ai/haystack/pull/1413)\r\n- QoL function: access certain nodes in pipeline [#1441](https://github.com/deepset-ai/haystack/pull/1441)\r\n- Refactor `replicas` config for Ray Pipelines [#1378](https://github.com/deepset-ai/haystack/pull/1378)\r\n- Add simple docs2answer node to allow FAQ style QA / Doc search in API [#1361](https://github.com/deepset-ai/haystack/pull/1361)\r\n- Allow for batch indexing when using Pipelines fix #1168 [#1231](https://github.com/deepset-ai/haystack/pull/1231)\r\n\r\n## Document Stores\r\n- Implement OpenSearch ANN [#1225](https://github.com/deepset-ai/haystack/pull/1225)\r\n- Bump Weaviate version to 1.7.0 [#1412](https://github.com/deepset-ai/haystack/pull/1412)\r\n- Catch Elastic's search_phase_execution and raise with descriptive message. [#1371](https://github.com/deepset-ai/haystack/pull/1371)\r\n- Fix behavior of delete_documents() with filters for Milvus [#1354](https://github.com/deepset-ai/haystack/pull/1354)\r\n- delete_all_documents() replaced by delete_documents() [#1377](https://github.com/deepset-ai/haystack/pull/1377)\r\n- Support OpenDistro init [#1334](https://github.com/deepset-ai/haystack/pull/1334)\r\n- Integrate filters with knn queries in OpenDistroElasticsearchDocumentStore [#1301](https://github.com/deepset-ai/haystack/pull/1301)\r\n- feat: add support for elastic search to connect without any authentication [#1294](https://github.com/deepset-ai/haystack/pull/1294)\r\n- Raise warning when labels are overwritten [#1257](https://github.com/deepset-ai/haystack/pull/1257)\r\n- Fix SQLAlchemy relationship warnings [#1289](https://github.com/deepset-ai/haystack/pull/1289)\r\n- Added explicit refresh call during refresh_type is false in update em\u2026 [#1259](https://github.com/deepset-ai/haystack/pull/1259)\r\n- Add `id` in `write_labels()` for `SQLDocumentStore` [#1253](https://github.com/deepset-ai/haystack/pull/1253)\r\n- ElasticsearchDocumentStore get_label_count() bug fixed. [#1252](https://github.com/deepset-ai/haystack/pull/1252)\r\n- SQLDocumentStore get_label_count() index bug fixed. [#1251](https://github.com/deepset-ai/haystack/pull/1251)\r\n\r\n## Retriever\r\n- Adding multi gpu support for DPR inference [#1414](https://github.com/deepset-ai/haystack/pull/1414)\r\n- Ensure num_hard_negatives is 0 when embedding passages [#1402](https://github.com/deepset-ai/haystack/pull/1402)\r\n- global_loss_buffer_size to the DensePassageRetriever, fix exceeds max_size [#1245](https://github.com/deepset-ai/haystack/pull/1245)\r\n\r\n## Summarizer\r\n- Transformer summarizer truncation bug fixed  [#1309](https://github.com/deepset-ai/haystack/pull/1309)\r\n\r\n## Document Classifier\r\n- Add FARMClassifier node for Document Classification [#1265](https://github.com/deepset-ai/haystack/pull/1265)\r\n\r\n## Re-Ranker\r\n- Add SentenceTransformersRanker with pre-trained Cross-Encoder [#1209](https://github.com/deepset-ai/haystack/pull/1209)\r\n\r\n## Reader\r\n- Use Reader's device by default [#1208](https://github.com/deepset-ai/haystack/pull/1208)\r\n\r\n## Generator\r\n- Add QuestionGenerator [#1267](https://github.com/deepset-ai/haystack/pull/1267)\r\n\r\n## Evaluation\r\n- Add new QA eval metric: Semantic Answer Similarity (SAS)  [#1338](https://github.com/deepset-ai/haystack/pull/1338)\r\n\r\n\r\n## REST API\r\n- Fix handling of filters in Search REST API [#1431](https://github.com/deepset-ai/haystack/pull/1431)\r\n- Add support for Dense Retrievers in REST API Indexing Pipeline [#1430](https://github.com/deepset-ai/haystack/pull/1430)\r\n- Add Header in sample REST API Search Request [#1293](https://github.com/deepset-ai/haystack/pull/1293)\r\n- Fix convert integer CONCURRENT_REQUEST_PER_WORKER [#1247](https://github.com/deepset-ai/haystack/pull/1247)\r\n- Env var CONCURRENT_REQUEST_PER_WORKER [#1235](https://github.com/deepset-ai/haystack/pull/1235)\r\n- Small UI and REST API fixes [#1223](https://github.com/deepset-ai/haystack/pull/1223)\r\n- Add scaffold for defining custom components for Pipelines [#1205](https://github.com/deepset-ai/haystack/pull/1205)\r\n\r\n## Docker\r\n- Update DocumentStore env in docker-compose [#1450](https://github.com/deepset-ai/haystack/pull/1450)\r\n- Enable docker-compose for GPUs & Add public UI image [#1406](https://github.com/deepset-ai/haystack/pull/1406)\r\n- Fix tesseract installation in Dockerfile [#1405](https://github.com/deepset-ai/haystack/pull/1405)\r\n\r\n\r\n## User Interface\r\n- Allow multiple files to upload for Haystack UI [#1323](https://github.com/deepset-ai/haystack/pull/1323)\r\n- Add faq annotation [#1333](https://github.com/deepset-ai/haystack/pull/1333)\r\n- Upgrade streamlit [#1279](https://github.com/deepset-ai/haystack/pull/1279)\r\n\r\n## Documentation and Tutorials\r\n- new docs version for 0.9.0 [#1217](https://github.com/deepset-ai/haystack/pull/1217)\r\n- Added functionality for Google Colab usecase in Crawler Module [#1436](https://github.com/deepset-ai/haystack/pull/1436)\r\n- Update sentence transformer model in FAQ tutorial [#1401](https://github.com/deepset-ai/haystack/pull/1401)\r\n- crawler api docs updated. [#1388](https://github.com/deepset-ai/haystack/pull/1388)\r\n- Add support for no Docker envs in Tutorial 13 [#1365](https://github.com/deepset-ai/haystack/pull/1365)\r\n- Rag tutorial fixes [#1375](https://github.com/deepset-ai/haystack/pull/1375)\r\n- Editing docs read.me for new docs website workflow [#1372](https://github.com/deepset-ai/haystack/pull/1372)\r\n- Add query classifier usage docs [#1348](https://github.com/deepset-ai/haystack/pull/1348)\r\n- Adding tutorial 13 and 14 [#1364](https://github.com/deepset-ai/haystack/pull/1364)\r\n- Remove Finder from tutorials [#1329](https://github.com/deepset-ai/haystack/pull/1329)\r\n- Tutorial1 remove finder class from import [#1328](https://github.com/deepset-ai/haystack/pull/1328)\r\n- Update docstring for RAG [#1149](https://github.com/deepset-ai/haystack/pull/1149)\r\n- Update README.md for tutorial 13 Question Generation  [#1325](https://github.com/deepset-ai/haystack/pull/1325)\r\n- add query classifier colab and jupyter notebook [#1324](https://github.com/deepset-ai/haystack/pull/1324)\r\n- Remove pipeline eval example script [#1297](https://github.com/deepset-ai/haystack/pull/1297)\r\n- Change variable names in tutorials [#1286](https://github.com/deepset-ai/haystack/pull/1286)\r\n- Add links to tutorial 12 to readme [#1274](https://github.com/deepset-ai/haystack/pull/1274)\r\n- Encapsulate tutorial code in method [#1266](https://github.com/deepset-ai/haystack/pull/1266)\r\n- Fix Links [#1199](https://github.com/deepset-ai/haystack/pull/1199)\r\n\r\n## Misc\r\n- Improve document stores unit test parametrization [#1202](https://github.com/deepset-ai/haystack/pull/1202)\r\n- Version tag added to Haystack [#1216](https://github.com/deepset-ai/haystack/pull/1216)\r\n- Add type ignore to resolve `mypy` errors [#1427](https://github.com/deepset-ai/haystack/pull/1427)\r\n- Bump pillow from 8.2.0 to 8.3.2 [#1423](https://github.com/deepset-ai/haystack/pull/1423)\r\n- Add sentence-transformers as mandatory dependency and remove from dev\u2026 [#1387](https://github.com/deepset-ai/haystack/pull/1387)\r\n- Adjust WeaviateDocumentStore import [#1379](https://github.com/deepset-ai/haystack/pull/1379)\r\n- Update test documentation in readme [#1355](https://github.com/deepset-ai/haystack/pull/1355)\r\n- Add tests for Crawler [#1339](https://github.com/deepset-ai/haystack/pull/1339)\r\n- Suppress FAISS logs & apex warnings [#1315](https://github.com/deepset-ai/haystack/pull/1315)\r\n- Pin Weaviate version [#1306](https://github.com/deepset-ai/haystack/pull/1306)\r\n- Relax typing for meta data [#1224](https://github.com/deepset-ai/haystack/pull/1224)\r\n\r\n\r\n# \ud83d\ude4f Big thanks to all contributors! :heart:\r\nA big thank you to all the contributors for this release:  @prikmm @akkefa @MichelBartels @hammer @ramgarg102 @bishalgaire @MarkusSagen @dfhssilva @srevinsaju @demarant @mosheber @MichaelBitard @guillim @vblagoje @stefanondisponibile @cambiumproject @bobvanluijt @tanay1337 @Timoeller @annagruendler @PiffPaffM @oryx1729 @bogdankostic @brandenchan @shahrukhx01 @julian-risch @tholor\r\n\r\nWe would like to thank everyone who participated in the insightful discussions on GitHub and our [community Slack](https://www.deepset.ai/community)! \r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "dateCreated": "2021-09-16T04:49:53Z",
        "datePublished": "2021-09-16T08:31:17Z",
        "html_url": "https://github.com/deepset-ai/haystack/releases/tag/v0.10.0",
        "name": "v0.10.0",
        "tag_name": "v0.10.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/haystack/tarball/v0.10.0",
        "url": "https://api.github.com/repos/deepset-ai/haystack/releases/49647261",
        "zipball_url": "https://api.github.com/repos/deepset-ai/haystack/zipball/v0.10.0"
      },
      {
        "authorType": "User",
        "author_name": "julian-risch",
        "body": "# :star: Highlights\r\n## Long-Form Question Answering (LFQA)\r\nHaystack now provides LFQA with a Seq2SeqGenerator for generative QA and a Retribert Retriever thanks to community member @vblagoje. [#1086](https://github.com/deepset-ai/haystack/pull/1086)\r\nIf you would like to ask questions where the answer is not a short phrase explicitly given in one of the documents but a more elaborate answer than LFQA is interesting for you. These elaborate answers are generated by combining information from multiple relevant documents.\r\n\r\n## Document Re-Ranking\r\nFor pure \"semantic document search\" use cases that do not need question answering functionality but only document ranking, there is now a new type of node: Ranker. While the Retriever is a perfect fit for document retrieval, we can further improve its results with the Ranker. [#1025](https://github.com/deepset-ai/haystack/pull/1025) \r\nTo this end, the Ranker uses a pre-trained model to calculate the semantic similarity of the question and each of the top-k retrieved documents. Documents with a high semantic similarity are ranked higher. The combination of a Retriever and Ranker is especially powerful if you combine a sparse retriever, e.g., ElasticsearchRetriever based on BM25 and a dense Ranker. \r\nA pipeline with a Ranker and Retriever can be setup in just a few lines of code:\r\n```python\r\n...\r\nretriever = ElasticsearchRetriever(document_store=document_store)\r\nranker = FARMRanker(model_name_or_path=\"deepset/gbert-base-germandpr-reranking\")\r\n\r\np = Pipeline()\r\np.add_node(component=retriever, name=\"ESRetriever\", inputs=[\"Query\"])\r\np.add_node(component=ranker, name=\"Ranker\", inputs=[\"ESRetriever\"])\r\n...\r\n```\r\n\r\n## Weaviate\r\nThanks to a contribution by our community member @venuraja79 Weaviate is integrated into Haystack as another DocumentStore [#1064](https://github.com/deepset-ai/haystack/pull/1064)\r\nIt allows a combination of vector search and scalar filtering, i.e., you can filter for a certain tag and do dense retrieval on that subset. After starting a Weaviate server with docker, it's as simple as:\r\n```python\r\nfrom haystack.document_store import WeaviateDocumentStore\r\ndocument_store = WeaviateDocumentStore()\r\n```\r\nHaystack uses the most recent Weaviate version 1.4.0 and the updating of embeddings has also been optimized [#1181](https://github.com/deepset-ai/haystack/pull/1181)\r\n\r\n## Query Classifier\r\nSome search applications need to distinguish between keyword queries and longer textual questions that come in. If you only want to route longer questions to the Reader branch in order to maximize the accuracy of results and minimize computation efforts/costs and route keyword queries to a Document Retriever, you can do that now with a QueryClassifier node thanks to a contribution by @shahrukhx01. [#1099](https://github.com/deepset-ai/haystack/pull/1099)\r\nYou could use it as shown in this exemplary pipeline:\r\n![image](https://user-images.githubusercontent.com/1563902/99940511-9685c880-2d6c-11eb-8bbb-35b9b311761d.png)\r\n\r\n## New Tutorials\r\n1) Tutorial 11: Pipelines [#991](https://github.com/deepset-ai/haystack/pull/991)\r\n2) Tutorial 12: Generative QA with LFQA [#1086](https://github.com/deepset-ai/haystack/pull/1086)\r\n\r\n# \u26a0\ufe0f Breaking Changes\r\n- Remove Python 3.6 support [#1059](https://github.com/deepset-ai/haystack/pull/1059)\r\n- Refactor REST APIs to use Pipelines [#922](https://github.com/deepset-ai/haystack/pull/922)\r\n- Bump to FARM 0.8.0, torch 1.8.1 and transformers 4.6.1 [#1192](https://github.com/deepset-ai/haystack/pull/1192)\r\n\r\n# :nerd_face: Detailed Changes\r\n## Connector\r\n- Add crawler to get texts from websites [#775](https://github.com/deepset-ai/haystack/pull/775)\r\n\r\n## Preprocessor\r\n- Add white space normalization warning [#1022](https://github.com/deepset-ai/haystack/pull/1022)\r\n- Preserve whitespace during PreProcessor.split() [#1121](https://github.com/deepset-ai/haystack/pull/1121)\r\n- Fix equality check in preprocessor [#969](https://github.com/deepset-ai/haystack/pull/969)\r\n\r\n## Pipeline\r\n- Add validation for root node in Pipeline [#987](https://github.com/deepset-ai/haystack/pull/987)\r\n- Fix passing a list as parameter value in Pipeline YAML [#952](https://github.com/deepset-ai/haystack/pull/952)\r\n- Add export of Pipeline YAML config [#1003](https://github.com/deepset-ai/haystack/pull/1003)\r\n- Add config to JoinDocuments node to allow yaml export in pipelines [#1134](https://github.com/deepset-ai/haystack/pull/1134)\r\n\r\n## Document Stores\r\n- Integrate Weaviate as another DocumentStore #957 [#1064](https://github.com/deepset-ai/haystack/pull/1064)\r\n- Add OpenDistro init [#1101](https://github.com/deepset-ai/haystack/pull/1101)\r\n- Rename all document stores delete_all_documents() method to delete_documents [#1047](https://github.com/deepset-ai/haystack/pull/1047)\r\n- Fix Elasticsearch connection for non-admin users [#1028](https://github.com/deepset-ai/haystack/pull/1028)\r\n- Fix update_embeddings() for FAISSDocumentStore [#978](https://github.com/deepset-ai/haystack/pull/978)\r\n- Feature: Enable AWS Elasticsearch IAM connection [#965](https://github.com/deepset-ai/haystack/pull/965)\r\n- Fix optional FAISS import [#971](https://github.com/deepset-ai/haystack/pull/971)\r\n- Make FAISS import conditional [#970](https://github.com/deepset-ai/haystack/pull/970)\r\n- Benchmark milvus [#850](https://github.com/deepset-ai/haystack/pull/850)\r\n- Improve Milvus HNSW Performance [#1127](https://github.com/deepset-ai/haystack/pull/1127)\r\n- Update Milvus benchmarks [#1128](https://github.com/deepset-ai/haystack/pull/1128)\r\n- Upgrade milvus to 1.1.0 [#1066](https://github.com/deepset-ai/haystack/pull/1066)\r\n- Update tests for FAISSDocumentStore [#999](https://github.com/deepset-ai/haystack/pull/999)\r\n- Add L2 support for FAISS HNSW [#1138](https://github.com/deepset-ai/haystack/pull/1138)\r\n- Improve the speed of FAISSDocumentStore.delete_documents() [#1095](https://github.com/deepset-ai/haystack/pull/1095)\r\n- Add options for handling duplicate documents (skip, fail, overwrite) [#1088](https://github.com/deepset-ai/haystack/pull/1088)\r\n- Update Embeddings - Use update instead of replace [#1181](https://github.com/deepset-ai/haystack/pull/1181)\r\n- Improve the progress bar in update_embeddings() + Fix filters in update_embeddings() [#1063](https://github.com/deepset-ai/haystack/pull/1063)\r\n- Using text hash as id to prevent document duplication [#1000](https://github.com/deepset-ai/haystack/pull/1000)\r\n\r\n## Retriever\r\n- DPR Training parameter [#989](https://github.com/deepset-ai/haystack/pull/989)\r\n- Removed single_model_path; added infer_tokenizer to dpr load() [#1060](https://github.com/deepset-ai/haystack/pull/1060)\r\n- Integrate sentence transformers into benchmarks [#843](https://github.com/deepset-ai/haystack/pull/843)\r\n- added use_amp to the train method, in order to use mixed precision training [#1048](https://github.com/deepset-ai/haystack/pull/1048)\r\n\r\n## Ranker\r\n- Re-ranking component for document search without QA [#1025](https://github.com/deepset-ai/haystack/pull/1025)\r\n- Remove quickfix from reader and ranker [#1196](https://github.com/deepset-ai/haystack/pull/1196)\r\n- Distinguish labels for calculating similarity scores [#1124](https://github.com/deepset-ai/haystack/pull/1124)\r\n\r\n## Query Classifier\r\n- Fix typo in Query Classifier Exception Message [#1190](https://github.com/deepset-ai/haystack/pull/1190)\r\n- Add QueryClassifier incl. baseline models [#1099](https://github.com/deepset-ai/haystack/pull/1099)\r\n\r\n## Reader\r\n- Filtering duplicate answers [#1021](https://github.com/deepset-ai/haystack/pull/1021)\r\n- Add ONNXRuntime support [#157](https://github.com/deepset-ai/haystack/pull/157)\r\n- Remove unused function _get_pseudo_prob [#1201](https://github.com/deepset-ai/haystack/pull/1201)\r\n\r\n## Generator\r\n- Integrate LFQA with Haystack - inferencing [#1086](https://github.com/deepset-ai/haystack/pull/1086)\r\n\r\n## Evaluation Nodes\r\n- Reduce precision in pipeline eval print functions [#943](https://github.com/deepset-ai/haystack/pull/943)\r\n- Fix division by zero error in EvalRetriever [#938](https://github.com/deepset-ai/haystack/pull/938)\r\n- Add evaluation nodes for Pipelines [#904](https://github.com/deepset-ai/haystack/pull/904)\r\n- Add More top_k handling to EvalDocuments [#1133](https://github.com/deepset-ai/haystack/pull/1133)\r\n- Prevent merge of same questions on different documents during evaluation [#1119](https://github.com/deepset-ai/haystack/pull/1119)\r\n\r\n## REST API\r\n- adding root_path option [#982](https://github.com/deepset-ai/haystack/pull/982)\r\n- Add PDF converter dependencies Docker [#1107](https://github.com/deepset-ai/haystack/pull/1107)\r\n- Disable Gunicorn preload option [#960](https://github.com/deepset-ai/haystack/pull/960)\r\n\r\n## User Interface\r\n- change file-upload response to sidebar [#1018](https://github.com/deepset-ai/haystack/pull/1018)\r\n- Add File Upload Functionality in UI [#995](https://github.com/deepset-ai/haystack/pull/995)\r\n- Streamlit UI Evaluation mode [#920](https://github.com/deepset-ai/haystack/pull/920)\r\n- Fix evaluation mode in UI [#1024](https://github.com/deepset-ai/haystack/pull/1024)\r\n- Fix typo in streamlit UI  [#1106](https://github.com/deepset-ai/haystack/pull/1106)\r\n\r\n## Documentation and Tutorials\r\n- Add about sections to Tutorial 12 [#1195](https://github.com/deepset-ai/haystack/pull/1195)\r\n- Tutorial update [#1166](https://github.com/deepset-ai/haystack/pull/1166)\r\n- Documentation update [#1162](https://github.com/deepset-ai/haystack/pull/1162)\r\n- Add FAQ page [#1151](https://github.com/deepset-ai/haystack/pull/1151)\r\n- Refresh API docs [#1152](https://github.com/deepset-ai/haystack/pull/1152)\r\n- Add docu of confidence scores and calibration method [#1131](https://github.com/deepset-ai/haystack/pull/1131)\r\n- Adding indentation to markup files [#947](https://github.com/deepset-ai/haystack/pull/947)\r\n- Update preprocessing.md [#1087](https://github.com/deepset-ai/haystack/pull/1087)\r\n- Add badges to readme [#1136](https://github.com/deepset-ai/haystack/pull/1136)\r\n- Regen api docs [#1015](https://github.com/deepset-ai/haystack/pull/1015)\r\n- Docs: Add usage information detailes for aws elastic search service [#1008](https://github.com/deepset-ai/haystack/pull/1008)\r\n- Add tutorial pages [#1013](https://github.com/deepset-ai/haystack/pull/1013)\r\n- Pipelines tutorial [#991](https://github.com/deepset-ai/haystack/pull/991)\r\n- knowledge graph documentation [#979](https://github.com/deepset-ai/haystack/pull/979)\r\n- knowledge graph example [#934](https://github.com/deepset-ai/haystack/pull/934)\r\n- Add Milvus to the retriever / document store table [#931](https://github.com/deepset-ai/haystack/pull/931)\r\n- New docs version [#964](https://github.com/deepset-ai/haystack/pull/964)\r\n- Update Documentation [#976](https://github.com/deepset-ai/haystack/pull/976)\r\n- update api markdown files and add markdown file for ranker [#1198](https://github.com/deepset-ai/haystack/pull/1198)\r\n- Reformat FAQ page [#1177](https://github.com/deepset-ai/haystack/pull/1177)\r\n- Minor change with a link to the Weaviate docs [#1180](https://github.com/deepset-ai/haystack/pull/1180)\r\n- Add links to GitHub Discussion and SO [#984](https://github.com/deepset-ai/haystack/pull/984)\r\n- Update milvus links and docstrings [#959](https://github.com/deepset-ai/haystack/pull/959)\r\n- Fixed link to dpr [#962](https://github.com/deepset-ai/haystack/pull/962)\r\n- Removed comma from last item in json list [#1114](https://github.com/deepset-ai/haystack/pull/1114)\r\n- Fixing inconsistency [#926](https://github.com/deepset-ai/haystack/pull/926)\r\n\r\n## Misc\r\n- Squad tools [#1029](https://github.com/deepset-ai/haystack/pull/1029)\r\n- Bugfix setting of device by defaulting to \"cpu\" [#1182](https://github.com/deepset-ai/haystack/pull/1182)\r\n- Fixing issues caused due to mypy upgrade [#1165](https://github.com/deepset-ai/haystack/pull/1165)\r\n- Remove Duplicate Benchmark Run [#1132](https://github.com/deepset-ai/haystack/pull/1132)\r\n- Fixing grpcio-tools to version of colab's pre-installed grpcio [#1113](https://github.com/deepset-ai/haystack/pull/1113)\r\n- Update farm version [#936](https://github.com/deepset-ai/haystack/pull/936)\r\n\r\n# \ud83d\ude4f Big thanks to all contributors! :heart:\r\nA big thank you to all the contributors for this release: @PiffPaffM @oryx1729 @jacksbox @guillim @Timoeller @aantti @tholor @brandenchan @julian-risch @bhadreshpsavani @akkefa @mosheber @lalitpagaria @Avi777 @MichaelBitard @AlviseSembenico @shahrukhx01 @venuraja79 @bobvanluijt @vblagoje @cvgoudar\r\n\r\nWe would like to thank everyone who participated in the insightful discussions on GitHub and our community Slack! ",
        "dateCreated": "2021-06-21T16:39:27Z",
        "datePublished": "2021-06-21T16:50:42Z",
        "html_url": "https://github.com/deepset-ai/haystack/releases/tag/v0.9.0",
        "name": "v0.9.0",
        "tag_name": "v0.9.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/haystack/tarball/v0.9.0",
        "url": "https://api.github.com/repos/deepset-ai/haystack/releases/44734524",
        "zipball_url": "https://api.github.com/repos/deepset-ai/haystack/zipball/v0.9.0"
      },
      {
        "authorType": "User",
        "author_name": "oryx1729",
        "body": "# :star: Highlights\r\n\r\nThis is a major Haystack release with many new features.  The [release blog post](https://medium.com/deepset-ai/semantic-search-with-milvus-knowledge-graph-qa-web-crawlers-and-more-837451eae9fa) has a detailed summary. Below are the top highlights:\r\n \r\n### Milvus Document Store\r\n[Milvus](https://github.com/milvus-io/milvus) is an open-source vector database. With the `MilvusDocumentStore` contributed by @lalitpagaria, embedding based Retrievers like the `DensePassageRetriever` or EmbeddingRetriever can use production-ready Milvus servers for large-scale deployments.\r\n\r\n### Knowledge Graph\r\nAn experimental integration for KnowledgeGraphs is introduced using [GraphDB](https://graphdb.ontotext.com). The `GraphDBKnowlegeGraph` stores Triples and executes SPARQL queries. It can be integrated with `Text2SparqlRetriever` to convert natural language queries to SPARQL. \r\n\r\n### Pipeline configuration with YAML\r\nThe Pipelines can now be configured with YAML. This enables easier sharing of query & indexing configuration, reproducible setups, A/B testing of Pipelines, and moving from development to the production environment.\r\n\r\n### REST APIs\r\nThe REST APIs are revamped to use Pipelines for Query & Indexing files. The YAML configurations are in the rest_api/pipelines.YAML. The new API endpoints are more generic to accommodate custom Pipeline configurations.\r\n\r\n### Confidence Scores\r\nThe answers now have a `probability` score that is better calibrated to the model's confidence. It has a range of 0-1; 0 signifying very low confidence, while, 1 for very high confidence. \r\n\r\n### Web Crawler\r\nA Selenium based web crawler is now part of Haystack, thanks to @divya-19 for the contribution. It takes as input a list of URLs and converts extracted text to Haystack Documents.\r\n\r\n\r\n# \u26a0\ufe0f Breaking Changes\r\n\r\n### REST APIs\r\nThe REST APIs got a [major revamp](https://github.com/deepset-ai/haystack/pull/922) with this release. \r\n- `/doc-qa` & `/faq-qa` endpoints are replaced with a more generic POST `/query` endpoint. This new endpoint uses Pipelines under-the-hood, that can be configured at `rest_api/pipeline.yaml`.\r\n- The new `/query` endpoint expects a single query per request instead of a list of query strings.\r\n     The new request format is:\r\n    ```JSON\r\n    {\r\n        \"query\": \"Why did the revenue change?\"\r\n    }\r\n    ```\r\n    and the response looks like this:\r\n\r\n    ```JSON\r\n    {\r\n        \"query\": \"Why did the revenue change?\",\r\n        \"answers\": [\r\n            {\r\n                \"answer\": \"rapid technological change and evolving industry standards\",\r\n                \"question\": null,\r\n                \"score\": 0.543937623500824,\r\n                \"probability\": 0.014070278964936733,\r\n                \"context\": \"tion process. The market for our products is intensely competitive and is characterized by rapid technological change and     evolving industry standards.\",\r\n                \"offset_start\": 91,\r\n                \"offset_end\": 149,\r\n                \"offset_start_in_doc\": 511,\r\n                \"offset_end_in_doc\": 569,\r\n                \"document_id\": \"f30273b2-4d49-40d8-8824-43b3b6a0ea57\",\r\n                \"meta\": {\r\n                    \"_split_id\": \"7\"\r\n                }\r\n            },\r\n            {\r\n                 // other answers\r\n            }\r\n        ]\r\n    }\r\n    ```\r\n- The `/doc-qa-feedback` & `/faq-qa-feedback` endpoints are replaced with a new generic `/feedback` endpoint.\r\n\r\n### Created At Timestamp\r\nPreviously, all documents/labels in `SQLDocumentStore` and `FAISSDocumentStore` had a field called `created` to store the creation timestamp, while `ElasticsearchDocumentStore` did not have any timestamp field. Now, all document stores have a `created_at` field for documents and labels.\r\n\r\n### RAGenerator\r\nThe `top_k_answers` parameter in the `RAGenerator` is [renamed](https://github.com/deepset-ai/haystack/pull/873) to `top_k` for consistency across Haystack components.\r\n\r\n### Custom Query for Elasticsearch\r\nThe placeholder terms in `custom_query` should not have quotes around them. See more details [here](https://github.com/deepset-ai/haystack/pull/762).\r\n\r\n\r\n# :nerd_face: Detailed Changes\r\n\r\n## Pipeline\r\n- Fix execution of Pipelines with parallel nodes [#901](https://github.com/deepset-ai/haystack/pull/901) (@oryx1729)\r\n- Add abstract run method to basecomponent [#887](https://github.com/deepset-ai/haystack/pull/887) (@tholor)\r\n- Add support for parallel paths in Pipeline [#884](https://github.com/deepset-ai/haystack/pull/884) (@oryx1729)\r\n- Add runtime parameters to component initialization [#873](https://github.com/deepset-ai/haystack/pull/873) (@oryx1729 )\r\n- Add support for indexing pipelines [#816](https://github.com/deepset-ai/haystack/pull/816) (@oryx1729 )\r\n- Adding translator with many generic input parameter support [#782](https://github.com/deepset-ai/haystack/pull/782) (@lalitpagaria)\r\n- Fix building Pipeline with YAML [#800](https://github.com/deepset-ai/haystack/pull/800) (@oryx1729)\r\n- Load Pipeline with YAML config file [#785](https://github.com/deepset-ai/haystack/pull/785) (@oryx1729)\r\n- Add evaluation nodes for Pipelines [#904](https://github.com/deepset-ai/haystack/pull/904) (@brandenchan)\r\n- Fix passing a list as parameter value in Pipeline YAML [#952](https://github.com/deepset-ai/haystack/pull/952) (@oryx1729)\r\n\r\n\r\n## Document Store\r\n- Fixes elasticsearch auth [#871](https://github.com/deepset-ai/haystack/pull/871) (@grafke)\r\n- Allow more options for elasticsearch client (auth, multiple hosts) [#845](https://github.com/deepset-ai/haystack/pull/845) (@tholor)\r\n- Fix ElasticsearchDocumentStore.query_by_embedding() [#823](https://github.com/deepset-ai/haystack/pull/823) (@oryx1729)\r\n- Introduce incremental updates for embeddings in document stores [#812](https://github.com/deepset-ai/haystack/pull/812) (@oryx1729)\r\n- Add method to get metadata values for a key from Elasticsearch [#776](https://github.com/deepset-ai/haystack/pull/776) (@oryx1729)\r\n- Fix refresh behaviour for Elasticsearch delete [#794](https://github.com/deepset-ai/haystack/pull/794) (@oryx1729)\r\n- Milvus integration [#771](https://github.com/deepset-ai/haystack/pull/771) (@lalitpagaria)\r\n- Add flag for use of window queries in SQLDocumentStore [#768](https://github.com/deepset-ai/haystack/pull/768) (@oryx1729)\r\n- Remove quotes around placeholders in Elasticsearch custom query [#762](https://github.com/deepset-ai/haystack/pull/762) (@oryx1729)\r\n- Fix delete_all_documents for the SQLDocumentStore [#761](https://github.com/deepset-ai/haystack/pull/761) (@oryx1729)\r\n\r\n## Retriever\r\n- Improve dpr conversion [#826](https://github.com/deepset-ai/haystack/pull/826) (@Timoeller)\r\n- Fix DPR training batch size [#898](https://github.com/deepset-ai/haystack/pull/898) (@brandenchan)\r\n- Upgrade FAISS to 1.7.0 [#834](https://github.com/deepset-ai/haystack/pull/834) (@tholor)\r\n- Allow non-standard Tokenizers (e.g. CamemBERT) for DPR via new arg [#811](https://github.com/deepset-ai/haystack/pull/811)(@psorianom)\r\n\r\n## Modeling\r\n- Add model versioning support [#784](https://github.com/deepset-ai/haystack/pull/784) (@brandenchan)\r\n- Improve preprocessing and adding of eval data [#780](https://github.com/deepset-ai/haystack/pull/780) (@Timoeller)\r\n- SQuAD to DPR dataset converter [#765](https://github.com/deepset-ai/haystack/pull/765) (@psorianom)\r\n- Remove RAG todos after transformers update [#781](https://github.com/deepset-ai/haystack/pull/781) (@Timoeller)\r\n- Update farm version [#936](https://github.com/deepset-ai/haystack/pull/936) (@Timoeller)\r\n\r\n\r\n## REST API\r\n- Refactor REST APIs to use Pipelines [#922](https://github.com/deepset-ai/haystack/pull/922) (@oryx1729)\r\n- Add PDF converter in Dockerfiles [#877](https://github.com/deepset-ai/haystack/pull/877) (@oryx1729)\r\n- Update GPU Dockerimage (Cuda 11, Fix faiss) [#836](https://github.com/deepset-ai/haystack/pull/836) (@tholor)\r\n- Add API endpoint to export accuracy metrics from user feedback + created_at timestamp [#803](https://github.com/deepset-ai/haystack/pull/803)(@tholor)\r\n- Fix file upload API [#808](https://github.com/deepset-ai/haystack/pull/808) (@oryx1729)\r\n\r\n## File Converter\r\n- Add Markdown file convertor [#875](https://github.com/deepset-ai/haystack/pull/875) (@lalitpagaria)\r\n- Fix encoding for pdftotext (Russian characters, German umlauts etc). Fix version in download instructions [#813](https://github.com/deepset-ai/haystack/pull/813) (@tholor)\r\n\r\n## Crawler\r\n- Add crawler to get texts from websites [#775](https://github.com/deepset-ai/haystack/pull/775) (@DIVYA-19)\r\n\r\n## Knowledge Graph\r\n- knowledge graph example [#934](https://github.com/deepset-ai/haystack/pull/934) (@julian-risch)\r\n\r\n## Annotation Tool\r\n- Annotation Tool: data is not persisted when using local version #853 [#855](https://github.com/deepset-ai/haystack/pull/855)(@venuraja79)\r\n\r\n## Search UI\r\n- Fix UI when API returns fewer answers than expected [#828](https://github.com/deepset-ai/haystack/pull/828)(@tholor)\r\n\r\n## CI\r\n- Revamp CI [#825](https://github.com/deepset-ai/haystack/pull/825) (@oryx1729)\r\n- Fix mypy typing [#792](https://github.com/deepset-ai/haystack/pull/792) (@oryx1729)\r\n- Fix pdftotext dependency in CI [#788](https://github.com/deepset-ai/haystack/pull/788) (@tholor)\r\n\r\n## Misc Fixes\r\n- Adding indentation to markup files [#947](https://github.com/deepset-ai/haystack/pull/947) (@julian-risch)\r\n- Reduce precision in pipeline eval print functions [#943](https://github.com/deepset-ai/haystack/pull/943) (@lewtun)\r\n- Fix division by zero error in EvalRetriever [#938](https://github.com/deepset-ai/haystack/pull/938) (@lewtun)\r\n- Logged warning in Faiss and Milvus for filters [#913](https://github.com/deepset-ai/haystack/pull/913) (@peteradorjan)\r\n- fixed \"cannot allocate memory\" exception by specifying max_processes [#910](https://github.com/deepset-ai/haystack/pull/910)(@mosheber)\r\n- Fix error when is_impossible not exist [#870](https://github.com/deepset-ai/haystack/pull/870) (@voidful)\r\n- Fix validation for `split_respect_sentence_boundary` in Preprocessor [#869](https://github.com/deepset-ai/haystack/pull/869) (@oryx1729)\r\n- Fix boolean `progress_bar` for disabling tqdm progressbar [#863](https://github.com/deepset-ai/haystack/pull/863) (@tholor)\r\n- Remove conditional import of FAISS for Windows [#819](https://github.com/deepset-ai/haystack/pull/819) (@oryx1729)\r\n- Make tqdm progress bars optional (less verbose prod logs) [#796](https://github.com/deepset-ai/haystack/pull/796) (@tholor)\r\n- Fix error when is_impossible not is_impossible and json dump encoding error [#868](https://github.com/deepset-ai/haystack/pull/868 (@voidful)\r\n- fix download ntlk preprocessor [#852](https://github.com/deepset-ai/haystack/pull/852) (@mrtunguyen)\r\n\r\n## Documentation\r\n- Add Milvus to the retriever / document store table [#931](https://github.com/deepset-ai/haystack/pull/931) (@lewtun)\r\n- Fixing inconsistency [#926](https://github.com/deepset-ai/haystack/pull/926) (@guillim)\r\n- Better default value for mp chunksize [#923](https://github.com/deepset-ai/haystack/pull/923) (@Timoeller)\r\n- Run Grammarly over README.md [#890](https://github.com/deepset-ai/haystack/pull/890) (@peterdemin)\r\n- Remove tf-idf youtube link [#888](https://github.com/deepset-ai/haystack/pull/888) (@ms10596)\r\n- Add Milvus Documentation [#838](https://github.com/deepset-ai/haystack/pull/838) (@brandenchan)\r\n- Fix link to Quick Demo in ToC. [#831](https://github.com/deepset-ai/haystack/pull/831) (@aantti)\r\n- Revamp Readme [#820](https://github.com/deepset-ai/haystack/pull/820) (@brandenchan)\r\n- Update tutorials (torch versions, ES version, replace Finder with Pipeline) [#814](https://github.com/deepset-ai/haystack/pull/814) (@tholor)\r\n- Choose correct similarity fns during benchmark runs & re-run benchmarks [#773](https://github.com/deepset-ai/haystack/pull/773) (@brandenchan)\r\n- Docs v0.7.0 [#757](https://github.com/deepset-ai/haystack/pull/757) (@PiffPaffM)\r\n- Fix top_k param in RAG tutorials [#906](https://github.com/deepset-ai/haystack/pull/906) (@Timoeller)\r\n- Integrate sentence transformers into benchmarks [#843](https://github.com/deepset-ai/haystack/pull/843) (@Timoeller)\r\n\r\n\r\n# \ud83d\ude4f Thanks to our contributors\r\nA big thank you to all the contributors for this release: @aantti, @brandenchan, @DIVYA-19, @grafke, @guillim, @julian-risch, @lalitpagaria, @lewtun, @mosheber, @mrtunguyen, @ms10596, @oryx1729, @peteradorjan, @PiffPaffM, @psorianom, @tholor, @Timoeller, @venuraja79, and @voidful.\r\n\r\nWe would like to thank everyone who participated in the insightful discussions on GitHub and our community Slack! ",
        "dateCreated": "2021-04-13T14:31:48Z",
        "datePublished": "2021-04-13T15:04:29Z",
        "html_url": "https://github.com/deepset-ai/haystack/releases/tag/v0.8.0",
        "name": "v0.8.0",
        "tag_name": "v0.8.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/haystack/tarball/v0.8.0",
        "url": "https://api.github.com/repos/deepset-ai/haystack/releases/41358964",
        "zipball_url": "https://api.github.com/repos/deepset-ai/haystack/zipball/v0.8.0"
      },
      {
        "authorType": "User",
        "author_name": "tholor",
        "body": "# :star: Highlights \r\n\r\n## New Slack Channel \r\nAs many people in the community asked us for it, we decided to open a slack channel! \r\nJoin us and ask questions, show what you've built with Haystack, and simply exchange with like-minded folks!\r\n\r\n:point_right: https://haystack.deepset.ai/community/join\r\n\r\n## Optimizing Memory + CPU consumption of documentstores for large datasets (#733)\r\n\r\nInteracting with large datasets can be challenging for the local memory. Therefore, we ... \r\n1) ... add `batch_size` parameters for most methods of the document store that allow to only load smaller chunks of documents at a time\r\n2) ... add a `get_all_documents_generator()` method that \"streams\" documents one by one from your document store.\r\nBoth help to lower the memory footprint significantly- especially when calling methods like `update_embeddings()` on datasets > 1 Mio docs.\r\n\r\n## Add Simple Demo UI (#671)\r\nThanks to our community member @tanmaylaud, we now have a great and simple UI that allows you to easily try your search pipelines. Ask questions, see the results, change basic config params, debug the API response and give your colleagues a better flavor of what you are building ...\r\n\r\n![Image](https://user-images.githubusercontent.com/31733620/102084645-7b721a00-3e3b-11eb-882f-1cb65b1b885c.png)\r\n## Support for summarization models (#698)\r\nThanks to another community contribution from @lalitpagaria  we now also support summarization models like PEGASUS in Haystack. You can use them ... \r\n\r\n... standalone:  \r\n```python\r\ndocs = [Document(text=\"PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.\r\n                    The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by\r\n                    the shutoffs which were expected to last through at least midday tomorrow.\")]\r\n\r\nsummarizer = TransformersSummarizer(model_name_or_path=\"google/pegasus-xsum\")\r\nsummary = summarizer.predict(documents=docs, generate_single_summary=False)\r\n```\r\n\r\n... as a node in your pipeline:\r\n```\r\n...\r\npipeline.add_node(component=summarizer, name=\"Summarizer\", inputs=[\"Retriever\"])\r\n\r\n```\r\n... by simply calling a predefined pipeline that first retrieves and then summarizes the resulting docs: \r\n```\r\n...\r\npipe = SearchSummarizationPipeline(summarizer=summarizer, retriever=retriever)\r\npipe.run()\r\n```\r\n\r\nWe see many interesting use cases around search for it. For example, running semantic document search and displaying the summary of docs as a \"preview\" in the results.\r\n\r\n## New Tutorials\r\n1) Wonder how to train a DPR retriever on your own domain dataset? Check out [this](https://haystack.deepset.ai/docs/latest/tutorial9md) new tutorial!\r\n2) Proper preprocessing (Cleaning, Splitting etc.) of docs can have a big impact on your performance. Check out [this](https://haystack.deepset.ai/docs/latest/tutorial8md) new tutorial to learn more about it.\r\n\r\n# :warning: Breaking Changes\r\n\r\n## Dropping `index_buffer_size` from FAISSDocumentStore\r\nWe removed the arg `index_buffer_size` from the init of `FAISSDocumentStore`. \"Buffering\" is now handled via the new `batch_size` arguments that you can pass to most methods like `write_documents()`, `update_embeddings()` and `get_all_documents()`. \r\n\r\n## Renaming of Preprocessor arg\r\nOld: \r\n```\r\nPreProcessor(..., split_stride=5)\r\n```\r\n\r\nNew:\r\n```\r\nPreProcessor(..., split_overlap=5)\r\n```\r\n\r\n# :nerd_face: Detailed Changes\r\n\r\n## Preprocessing / File Conversion\r\n- Using PreProcessor functions on eval data [#751](https://github.com/deepset-ai/haystack/pull/751)\r\n\r\n## DocumentStore\r\n- Support filters for DensePassageRetriever + InMemoryDocumentStore [#754](https://github.com/deepset-ai/haystack/pull/754)\r\n- use Path class in add_eval_data of haystack.document_store.base.py [#745](https://github.com/deepset-ai/haystack/pull/745)\r\n- Make batchwise adding of evaluation data possible [#717](https://github.com/deepset-ai/haystack/pull/717)\r\n- Change signature and docstring for ca_certs parameter [#730](https://github.com/deepset-ai/haystack/pull/730)\r\n- Rename label id field for elastic & add UPDATE_EXISTING_DOCUMENTS to API config [#728](https://github.com/deepset-ai/haystack/pull/728)\r\n- Fix SQLite errors in tests [#723](https://github.com/deepset-ai/haystack/pull/723)\r\n- Add support for custom embedding field for InMemoryDocumentStore [#640](https://github.com/deepset-ai/haystack/pull/640)\r\n- Using Columns names instead of ORM to get all documents [#620](https://github.com/deepset-ai/haystack/pull/620)\r\n\r\n## Other\r\n- Generate docstrings and deploy to branches to Staging (Website) [#731](https://github.com/deepset-ai/haystack/pull/731)\r\n- Script for releasing docs [#736](https://github.com/deepset-ai/haystack/pull/736)\r\n- Increase FARM to Version 0.6.2 [#755](https://github.com/deepset-ai/haystack/pull/755)\r\n- Reduce memory consumption of fetch_archive_from_http [#737](https://github.com/deepset-ai/haystack/pull/737)\r\n- Add links to more resources [#746](https://github.com/deepset-ai/haystack/pull/746)\r\n- Fix Tutorial 9  [#734](https://github.com/deepset-ai/haystack/pull/734)\r\n- Adding a guard that prevents the tutorial from being executed in every subprocess on windows [#729](https://github.com/deepset-ai/haystack/pull/729)\r\n- Add ID to Label schema [#727](https://github.com/deepset-ai/haystack/pull/727)\r\n- Automate docstring and tutorial generation with every push to master [#718](https://github.com/deepset-ai/haystack/pull/718)\r\n- Pass custom label index name to REST API [#724](https://github.com/deepset-ai/haystack/pull/724)\r\n- Correcting pypi download badge [#722](https://github.com/deepset-ai/haystack/pull/722)\r\n- Fix GPU docker build [#703](https://github.com/deepset-ai/haystack/pull/703)\r\n- Remove sourcerer.io widget [#702](https://github.com/deepset-ai/haystack/pull/702)\r\n- Haystack logo is not visible on github mobile app [#697](https://github.com/deepset-ai/haystack/pull/697)\r\n- Update pipeline documentation and readme [#693](https://github.com/deepset-ai/haystack/pull/693)\r\n- Enable GPU args in tutorials [#692](https://github.com/deepset-ai/haystack/pull/692)\r\n- Add docs v0.6.0 [#689](https://github.com/deepset-ai/haystack/pull/689)\r\n\r\nBig thanks to all contributors :heart: !\r\n\r\n @Rob192 @antoniolanza1996 @tanmaylaud @lalitpagaria @Timoeller @tanaysoni @bogdankostic @aantti @brandenchan @PiffPaffM @julian-risch",
        "dateCreated": "2021-01-21T15:05:44Z",
        "datePublished": "2021-01-21T17:42:17Z",
        "html_url": "https://github.com/deepset-ai/haystack/releases/tag/v0.7.0",
        "name": "v0.7.0",
        "tag_name": "v0.7.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/haystack/tarball/v0.7.0",
        "url": "https://api.github.com/repos/deepset-ai/haystack/releases/36718512",
        "zipball_url": "https://api.github.com/repos/deepset-ai/haystack/zipball/v0.7.0"
      },
      {
        "authorType": "User",
        "author_name": "tholor",
        "body": "# :star: Highlights \r\n## Flexible Pipelines powered by DAGs (#596)\r\nIn order to build modern search pipelines, you need two things: powerful building blocks and a flexible way to stick them together. \r\nWhile we always had great building blocks in Haystack, we didn't have a good way to stick them together so far. That's why we put a lof thought into it in the last weeks and came up with a new `Pipeline` class that enables many new search scenarios beyond QA. The core idea: you can build a Directed Acyclic Graph (DAG) where each node is one \"building block\" (Reader, Retriever, Generator ...). Here's a simple example for a \"standard\" Open-Domain QA Pipeline: \r\n\r\n```python\r\np = Pipeline()\r\np.add_node(component=retriever, name=\"ESRetriever1\", inputs=[\"Query\"])\r\np.add_node(component=reader, name=\"QAReader\", inputs=[\"ESRetriever1\"])\r\nres = p.run(query=\"What did Einstein work on?\", top_k_retriever=1)\r\n\r\n```\r\n\r\nYou can **draw the DAG** to better inspect what you are building:\r\n```python\r\np.draw(path=\"custom_pipe.png\")\r\n```\r\n![image](https://user-images.githubusercontent.com/1563902/102451716-54813700-4039-11eb-881e-f3c01b47ca15.png)\r\n\r\n### Multiple retrievers\r\nYou can now also use multiple Retrievers and join their results: \r\n```python\r\np = Pipeline()\r\np.add_node(component=es_retriever, name=\"ESRetriever\", inputs=[\"Query\"])\r\np.add_node(component=dpr_retriever, name=\"DPRRetriever\", inputs=[\"Query\"])\r\np.add_node(component=JoinDocuments(join_mode=\"concatenate\"), name=\"JoinResults\", inputs=[\"ESRetriever\", \"DPRRetriever\"])\r\np.add_node(component=reader, name=\"QAReader\", inputs=[\"JoinResults\"])\r\nres = p.run(query=\"What did Einstein work on?\", top_k_retriever=1)\r\n```\r\n![image](https://user-images.githubusercontent.com/1563902/102451782-7bd80400-4039-11eb-9046-01b002a783f8.png)\r\n\r\n### Custom nodes\r\nYou can easily build your own custom nodes. Just respect the following requirements: \r\n\r\n1. Add a method `run(self, **kwargs)` to your class. `**kwargs` will contain the output from the previous node in your graph.\r\n2. Do whatever you want within `run()` (e.g. reformatting the query)\r\n3. Return a tuple that contains your output data (for the next node) and the name of the outgoing edge `output_dict, \"output_1`\r\n4. Add a class attribute `outgoing_edges = 1` that defines the number of output options from your node. You only need a higher number here if you have a decision node (see below).\r\n\r\n### Decision nodes\r\nOr you can add decision nodes where only one \"branch\" is executed afterwards. This allows, for example, to classify an incoming query and depending on the result routing it to different modules: \r\n![image](https://user-images.githubusercontent.com/1563902/102452199-41229b80-403a-11eb-9365-7038697e7c3e.png)\r\n```python \r\n    class QueryClassifier():\r\n        outgoing_edges = 2\r\n\r\n        def run(self, **kwargs):\r\n            if \"?\" in kwargs[\"query\"]:\r\n                return (kwargs, \"output_1\")\r\n\r\n            else:\r\n                return (kwargs, \"output_2\")\r\n\r\n    pipe = Pipeline()\r\n    pipe.add_node(component=QueryClassifier(), name=\"QueryClassifier\", inputs=[\"Query\"])\r\n    pipe.add_node(component=es_retriever, name=\"ESRetriever\", inputs=[\"QueryClassifier.output_1\"])\r\n    pipe.add_node(component=dpr_retriever, name=\"DPRRetriever\", inputs=[\"QueryClassifier.output_2\"])\r\n    pipe.add_node(component=JoinDocuments(join_mode=\"concatenate\"), name=\"JoinResults\",\r\n                  inputs=[\"ESRetriever\", \"DPRRetriever\"])\r\n    pipe.add_node(component=reader, name=\"QAReader\", inputs=[\"JoinResults\"])\r\n    res = p.run(query=\"What did Einstein work on?\", top_k_retriever=1)\r\n```\r\n\r\n### Default Pipelines (replacing the \"Finder\")\r\nLast but not least, we added some \"Default Pipelines\" that allow you to run standard patterns with very few lines of code.\r\nThis is replacing the `Finder` class which is now deprecated.\r\n\r\n```\r\nfrom haystack.pipeline import DocumentSearchPipeline, ExtractiveQAPipeline, Pipeline, JoinDocuments\r\n\r\n# Extractive QA\r\nqa_pipe = ExtractiveQAPipeline(reader=reader, retriever=retriever)\r\nres = qa_pipe.run(query=\"When was Kant born?\", top_k_retriever=3, top_k_reader=5)\r\n\r\n# Document Search\r\ndoc_pipe = DocumentSearchPipeline(retriever=retriever)\r\nres = doc_pipe.run(query=\"Physics Einstein\", top_k_retriever=1)\r\n\r\n# Generative QA\r\ndoc_pipe = GenerativeQAPipeline(generator=rag_generator, retriever=retriever)\r\nres = doc_pipe.run(query=\"Physics Einstein\", top_k_retriever=1)\r\n\r\n# FAQ based QA\r\ndoc_pipe = FAQPipeline(retriever=retriever)\r\nres = doc_pipe.run(query=\"How can I change my address?\", top_k_retriever=3)\r\n\r\n```    \r\n\r\nWe plan many more features around the new pipelines incl. parallelized execution, distributed execution, definition via YAML files, dry runs ...  \r\n\r\n## New DocumentStore for the Open Distro of Elasticsearch (#676)\r\n\r\nFrom now on we also support the [Open Distro of Elasticsearch](https://opendistro.github.io/for-elasticsearch/). This allows you to use many of the hosted Elasticsearch services (e.g. from AWS) more easily with Haystack. Usage is similar to the regular `ElasticsearchDocumentStore`: \r\n\r\n```python\r\ndocument_store = OpenDistroElasticsearchDocumentStore(host=\"localhost\", port=\"9200\", ...)\r\n```\r\n\r\n# :warning: Breaking Changes\r\nAs Haystack is extending from QA to further search types, we decided to rename all parameters from `question` to `query`.\r\nThis includes for example the `predict()` methods of the Readers but also several other places. See #614 for details. \r\n\r\n# :nerd_face: Detailed Changes\r\n\r\n## Preprocessing / File Conversion\r\n- Redone: Fix concatenation of sentences in PreProcessor. Add stride for word-based splits with sentence boundaries [#641](https://github.com/deepset-ai/haystack/pull/641)\r\n- Add needed whitespace before sentence start [#582](https://github.com/deepset-ai/haystack/pull/582)\r\n\r\n## DocumentStore\r\n- Scale dot product into probabilities [#667](https://github.com/deepset-ai/haystack/pull/667)\r\n- Add refresh_type param for Elasticsearch update_embeddings() [#630](https://github.com/deepset-ai/haystack/pull/630)\r\n- Add return_embedding parameter for get_all_documents() [#615](https://github.com/deepset-ai/haystack/pull/615)\r\n- Adding support for update_existing_documents to sql and faiss document stores [#584](https://github.com/deepset-ai/haystack/pull/584)\r\n- Add filters for delete_all_documents() [#591](https://github.com/deepset-ai/haystack/pull/591)\r\n\r\n## Retriever\r\n- Fix saving tokenizers in DPR training + unify save and load dirs [#682](https://github.com/deepset-ai/haystack/pull/682)\r\n- fix a typo, num_negatives -> num_positives [#681](https://github.com/deepset-ai/haystack/pull/681)\r\n- Refactor DensePassageRetriever._get_predictions [#642](https://github.com/deepset-ai/haystack/pull/642)\r\n- Move DPR embeddings from GPU to CPU straight away [#618](https://github.com/deepset-ai/haystack/pull/618)\r\n- Add MAP retriever metric for open-domain case [#572](https://github.com/deepset-ai/haystack/pull/572)\r\n\r\n## Reader / Generator\r\n- add GPU support for rag [#669](https://github.com/deepset-ai/haystack/pull/669)\r\n- Enable dynamic parameter updates for the FARMReader [#650](https://github.com/deepset-ai/haystack/pull/650)\r\n- Add option in API Config to configure if reader can return \"No Answer\" [#609](https://github.com/deepset-ai/haystack/pull/609)\r\n- Fix various generator issues [#590](https://github.com/deepset-ai/haystack/pull/590)\r\n\r\n## Pipeline\r\n- Add support for building custom Search Pipelines [#596](https://github.com/deepset-ai/haystack/pull/596)\r\n- Add set_node() for Pipeline [#659](https://github.com/deepset-ai/haystack/pull/659)\r\n- Add support for aggregating scores in JoinDocuments node [#683](https://github.com/deepset-ai/haystack/pull/683)\r\n- Add pipelines for GenerativeQA & FAQs [#645](https://github.com/deepset-ai/haystack/pull/645)\r\n\r\n## Other\r\n- Cleanup Pytest Fixtures [#639](https://github.com/deepset-ai/haystack/pull/639)\r\n- Add latest benchmark run [#652](https://github.com/deepset-ai/haystack/pull/652)\r\n- Fix image links in tutorial [#663](https://github.com/deepset-ai/haystack/pull/663)\r\n- Update query arg in Tutorial 7 [#656](https://github.com/deepset-ai/haystack/pull/656)\r\n- Fix benchmarks [#648](https://github.com/deepset-ai/haystack/pull/648)\r\n- Add link to FAISS Info in documentation [#643](https://github.com/deepset-ai/haystack/pull/643)\r\n- Improve User Feedback Documentation [#539](https://github.com/deepset-ai/haystack/pull/539)\r\n- Add formatting checks for shell scripts [#627](https://github.com/deepset-ai/haystack/pull/627)\r\n- Update md files for API docs [#631](https://github.com/deepset-ai/haystack/pull/631)\r\n- Clean API docs and increase coverage [#621](https://github.com/deepset-ai/haystack/pull/621)\r\n- Add boxes for recommendations [#629](https://github.com/deepset-ai/haystack/pull/629)\r\n- Automate benchmarks via CML [#518](https://github.com/deepset-ai/haystack/pull/518)\r\n- Add contributor hall of fame [#628](https://github.com/deepset-ai/haystack/pull/628)\r\n- README: Fix link to roadmap [#626](https://github.com/deepset-ai/haystack/pull/626)\r\n- Fix docstring examples [#604](https://github.com/deepset-ai/haystack/pull/604)\r\n- Cleaning the api docs [#616](https://github.com/deepset-ai/haystack/pull/616)\r\n- Fix link to DocumentStore page [#613](https://github.com/deepset-ai/haystack/pull/613)\r\n- Make more changes to documentation [#578](https://github.com/deepset-ai/haystack/pull/578)\r\n- Remove column in benchmark website [#608](https://github.com/deepset-ai/haystack/pull/608)\r\n- Make benchmarks clearer [#606](https://github.com/deepset-ai/haystack/pull/606)\r\n- Fixing defaults configs for rest_apis [#583](https://github.com/deepset-ai/haystack/pull/583)\r\n- Allow list of filter values in REST API [#568](https://github.com/deepset-ai/haystack/pull/568)\r\n- Fix CI bug due to new Elasticsearch release and new model release [#579](https://github.com/deepset-ai/haystack/pull/579)\r\n- Update Colab Torch Version [#576](https://github.com/deepset-ai/haystack/pull/576)\r\n\r\n\r\n## :heart: Big thanks to all contributors! \r\n@sadakmed @Krak91 @icy @lalitpagaria @guillim @tanaysoni @tholor @timoeller @PiffPaffM @bogdankostic ",
        "dateCreated": "2020-12-17T05:34:57Z",
        "datePublished": "2020-12-17T06:53:05Z",
        "html_url": "https://github.com/deepset-ai/haystack/releases/tag/v0.6.0",
        "name": "v0.6.0",
        "tag_name": "v0.6.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/haystack/tarball/v0.6.0",
        "url": "https://api.github.com/repos/deepset-ai/haystack/releases/35396610",
        "zipball_url": "https://api.github.com/repos/deepset-ai/haystack/zipball/v0.6.0"
      },
      {
        "authorType": "User",
        "author_name": "tholor",
        "body": "\r\n# Highlights \r\n## :speech_balloon: Generative Question Answering via RAG (#484)\r\n\r\nThanks to our community member @lalitpagaria, Haystack now also support generative QA via Retrieval Augmented Generation (\"RAG\").\r\nInstead of \"finding\" the answer within a document, these models **generate** the answer. In that sense, RAG follows a similar approach as GPT-3 but it comes with two huge advantages for real-world applications: \r\na) it has a manageable model size \r\nb) the answer generation is conditioned on retrieved documents, i.e. the model can easily adjust to domain documents even after training has finished (in contrast: GPT-3 relies on the web data seen during training) \r\n\r\n\r\nExample: \r\n\r\n```python\r\n    question = \"who got the first nobel prize in physics?\"\r\n\r\n    # Retrieve related documents from retriever\r\n    retrieved_docs = retriever.retrieve(query=question)\r\n\r\n    # Now generate answer from question and retrieved documents\r\n    predicted_result = generator.predict(\r\n        question=question,\r\n        documents=retrieved_docs,\r\n        top_k=1\r\n    )\r\n```\r\n\r\nYou already play around with it in this minimal tutorial: \r\n\r\nWe are looking forward to improve this class of models further in the next months and already plan a tighter integration into the `Finder` class. \r\n\r\n\r\n## :arrow_upper_right: Better DPR (incl. training) (#527)\r\n\r\nWe migrated the existing `DensePassageRetriever` to an own pipeline based on FARM. This allows a better modularization and most importantly simple training of DPR models! You can either train models from scratch or take an existing DPR model and fine-tune it on your own domain data. The required training data consists of queries and positive passages (i.e. passages that are related to your query / contain the answer) and the format complies with the one in the [original DPR codebase](https://github.com/facebookresearch/DPR#retriever-input-data-format). \r\n\r\nExample: \r\n\r\n```python\r\ndense_passage_retriever.train(self,\r\n                              data_dir: str,\r\n                              train_filename: str,\r\n                              dev_filename: str = None,\r\n                              test_filename: str = None,\r\n                              batch_size: int = 16,\r\n                              embed_title: bool = True,\r\n                              num_hard_negatives: int = 1,\r\n                              n_epochs: int = 3)\r\n\r\n``` \r\n\r\nFuture improvements: At the moment training is only supported on single GPUs. We will add support for Multi-GPU Training via DDP soon. \r\n\r\n##  \ud83d\udcca  New Benchmarks\r\n\r\nHappy to introduce a new benchmark section on our website!\r\nDo you wonder if you should use BERT, RoBERTa or MiniLM for your reader? Is it worth to use DPR for retrieval instead of Elastic's BM25? How would this impact speed and accuracy?\r\n\r\nSee the relevant metrics here to guide your decision: \r\n:point_right: https://haystack.deepset.ai/bm/benchmarks\r\n\r\nWe will extend this section over time with more models, metrics and key parameters.\r\n\r\n# :warning: Breaking Changes\r\n### Consistent parameter naming for TransformersReader [#510](https://github.com/deepset-ai/haystack/pull/510)\r\n```python\r\n# old: \r\nTransformersReader(model=\"distilbert-base-uncased-distilled-squad\" ..) \r\n\r\n# new\r\nTransformersReader(model=\"distilbert-base-uncased-distilled-squad\" ..) \r\nTransformersReader(model_name_or_path=\"distilbert-base-uncased-distilled-squad\" ...)\r\n```\r\n### FAISS: Remove phi normalization, support more index types [#467](https://github.com/deepset-ai/haystack/pull/467)\r\n\r\nNew default index type is \"Flat\" and params have changed slightly: \r\n\r\n```python\r\n\r\n# old \r\n FAISSDocumentStore(\r\n        sql_url: str = \"sqlite:///\",\r\n        index_buffer_size: int = 10_000,\r\n        vector_size: int = 768,\r\n        faiss_index: Optional[IndexHNSWFlat] = None,\r\n\r\n# new\r\nFAISSDocumentStore(\r\n        sql_url: str = \"sqlite:///\",\r\n        index_buffer_size: int = 10_000,\r\n        vector_dim: int = 768,\r\n        faiss_index_factory_str: str = \"Flat\",\r\n        faiss_index: Optional[faiss.swigfaiss.Index] = None,\r\n        return_embedding: Optional[bool] = True,\r\n        **kwargs,\r\n```\r\n### DPR signature\r\nSplitting `max_seq_len` into two independent params. \r\nRemoving `remove_sep_tok_from_untitled_passages` param.\r\n\r\n```python\r\n# old\r\nDensePassageRetriever(\r\n                 document_store: BaseDocumentStore,\r\n                 query_embedding_model: str = \"facebook/dpr-question_encoder-single-nq-base\",\r\n                 passage_embedding_model: str = \"facebook/dpr-ctx_encoder-single-nq-base\",\r\n                 max_seq_len: int = 256,\r\n                 use_gpu: bool = True,\r\n                 batch_size: int = 16,\r\n                 embed_title: bool = True,\r\n                 remove_sep_tok_from_untitled_passages: bool = True\r\n                 )\r\n\r\n# new \r\n DensePassageRetriever(\r\n \t\t document_store: BaseDocumentStore,\r\n                 query_embedding_model: Union[Path, str] = \"facebook/dpr-question_encoder-single-nq-base\",\r\n                 passage_embedding_model: Union[Path, str] = \"facebook/dpr-ctx_encoder-single-nq-base\",\r\n                 max_seq_len_query: int = 64,\r\n                 max_seq_len_passage: int = 256,\r\n                 use_gpu: bool = True,\r\n                 batch_size: int = 16,\r\n                 embed_title: bool = True,\r\n                 use_fast_tokenizers: bool = True,\r\n                 similarity_function: str = \"dot_product\"\r\n                 ):\r\n```\r\n# Detailed Changes\r\n\r\n# Preprocessing / File Conversion\r\n- Add preprocessing pipeline [#473](https://github.com/deepset-ai/haystack/pull/473)\r\n- Restructure checks in PreProcessor [#504](https://github.com/deepset-ai/haystack/pull/504)\r\n- Updated the example code to Indexing PDF / Docx files [#502](https://github.com/deepset-ai/haystack/pull/502)\r\n- Fix meta data = None in PreProcessor [#496](https://github.com/deepset-ai/haystack/pull/496)\r\n- add explicit encoding mode to file_converter/txt.py [#478](https://github.com/deepset-ai/haystack/pull/478)\r\n- Skip file conversion if file type is not supported [#456](https://github.com/deepset-ai/haystack/pull/456)\r\n\r\n## DocumentStore\r\n- Add support for MySQL database [#556](https://github.com/deepset-ai/haystack/pull/556)\r\n- Allow configuration of Elasticsearch Analyzer (e.g. for other languages) [#554](https://github.com/deepset-ai/haystack/pull/554)\r\n- Add support to return embedding [#514](https://github.com/deepset-ai/haystack/pull/514)\r\n- Fix scoring in Elasticsearch for dot product [#517](https://github.com/deepset-ai/haystack/pull/517)\r\n- Allow filters for get_document_count() [#512](https://github.com/deepset-ai/haystack/pull/512)\r\n- Make creation of label index optional [#490](https://github.com/deepset-ai/haystack/pull/490)\r\n- Fix update_embeddings function in FAISSDocumentStore [#481](https://github.com/deepset-ai/haystack/pull/481)\r\n- FAISS Store: allow multiple write calls and fix potential memory leak in update_embeddings [#422](https://github.com/deepset-ai/haystack/pull/422)\r\n- Enable bulk operations on vector IDs for FAISSDocumentStore [#460](https://github.com/deepset-ai/haystack/pull/460)\r\n- fixing ElasticsearchDocumentStore initialisation [#415](https://github.com/deepset-ai/haystack/pull/415)\r\n- bug: filters on a query_by_embedding [#464](https://github.com/deepset-ai/haystack/pull/464)\r\n\r\n## Retriever\r\n- DensePassageRetriever: Add Training, Refactor Inference to FARM modules [#527](https://github.com/deepset-ai/haystack/pull/527)\r\n- Fix retriever evaluation metrics [#547](https://github.com/deepset-ai/haystack/pull/547)\r\n- Add save and load method for DPR [#550](https://github.com/deepset-ai/haystack/pull/550)\r\n- Typo in dense.py comment [#545](https://github.com/deepset-ai/haystack/pull/545)\r\n- Make returning predictions in Finder & Retriever eval() possible [#524](https://github.com/deepset-ai/haystack/pull/524)\r\n- Make title info optional when evaluating on QA data [#494](https://github.com/deepset-ai/haystack/pull/494)\r\n- Make sentence-transformers usage more user-friendly [#439](https://github.com/deepset-ai/haystack/pull/439)\r\n\r\n## Reader\r\n- Fix FARMReader.eval() handling of no_answers [#531](https://github.com/deepset-ai/haystack/pull/531)\r\n- Added automatic mixed precision (AMP) support for reader training from Haystack side [#463](https://github.com/deepset-ai/haystack/pull/463)\r\n- Update ONNX conversion for FARMReader [#438](https://github.com/deepset-ai/haystack/pull/438)\r\n\r\n## Other\r\n- Fix sentencepiece dependencies in Dockerfiles [#553](https://github.com/deepset-ai/haystack/pull/553)\r\n- Update Dockerfile [#537](https://github.com/deepset-ai/haystack/pull/537)\r\n- Removing (deprecated) warnings from the Haystack codebase. [#530](https://github.com/deepset-ai/haystack/pull/530)\r\n- Pytest fix memory leak and put pytest marker on slow tests [#520](https://github.com/deepset-ai/haystack/pull/520)\r\n- [**enhancement**] Create deploy_website.yml [#450](https://github.com/deepset-ai/haystack/pull/450)\r\n- Add Docker Images & Setup for the Annotation Tool [#444](https://github.com/deepset-ai/haystack/pull/444)\r\n\r\n## REST API\r\n- Make filter value optional in REST API [#497](https://github.com/deepset-ai/haystack/pull/497)\r\n- Add Elasticsearch Query DSL compliant Query API [#471](https://github.com/deepset-ai/haystack/pull/471)\r\n- Allow configuration of log level in REST API [#541](https://github.com/deepset-ai/haystack/pull/541)\r\n- Add create_index and similarity metric to api config [#493](https://github.com/deepset-ai/haystack/pull/493)\r\n- Add deepcopy for meta dicts in answers [#485](https://github.com/deepset-ai/haystack/pull/485)\r\n- Fix windows platform installation [#480](https://github.com/deepset-ai/haystack/pull/480)\r\n- Update GPU docker & fix race condition with multiple workers [#436](https://github.com/deepset-ai/haystack/pull/436)\r\n\r\n## Documentation / Benchmarks / Tutorials\r\n- New readme [#534](https://github.com/deepset-ai/haystack/pull/534)\r\n- Add public roadmap [#432](https://github.com/deepset-ai/haystack/pull/432)\r\n- Time and performance benchmarks for all readers and retrievers [#339](https://github.com/deepset-ai/haystack/pull/339)\r\n- Added new formatting for examples in docstrings [#555](https://github.com/deepset-ai/haystack/pull/555)\r\n- Update annotation docs for website [#505](https://github.com/deepset-ai/haystack/pull/505)\r\n- Add annotation tool manual to README.md [#523](https://github.com/deepset-ai/haystack/pull/523)\r\n- Change metric to queries per second on benchmarks webpage [#529](https://github.com/deepset-ai/haystack/pull/529)\r\n- Add --ci and --update-json to CLI for benchmarks [#522](https://github.com/deepset-ai/haystack/pull/522)\r\n- Add requirement to colab notebooks [#509](https://github.com/deepset-ai/haystack/pull/509)\r\n- Update doc string for ElasticsearchDocumentStore.write_documents() & sync markdown files [#501](https://github.com/deepset-ai/haystack/pull/501)\r\n- Add versioning docs [#495](https://github.com/deepset-ai/haystack/pull/495)\r\n- READ.me for Docstring Generation [#468](https://github.com/deepset-ai/haystack/pull/468)\r\n- Separate data and view for benchmarks [#451](https://github.com/deepset-ai/haystack/pull/451)\r\n- Update DPR docstring for embed_title [#459](https://github.com/deepset-ai/haystack/pull/459)\r\n- Update Tutorial4_FAQ_style_QA.py [#416](https://github.com/deepset-ai/haystack/pull/416)\r\n\r\n## :heart: Big thanks to all contributors! \r\n\r\n@lalitpagaria @guillim  @elyase @kolk @rsanjaykamath @antoniolanza1996 @Zenahr @Futurne @tanaysoni @tholor @timoeller @PiffPaffM @bogdankostic ",
        "dateCreated": "2020-11-06T09:54:28Z",
        "datePublished": "2020-11-06T10:28:17Z",
        "html_url": "https://github.com/deepset-ai/haystack/releases/tag/v0.5.0",
        "name": "v0.5.0",
        "tag_name": "v0.5.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/haystack/tarball/v0.5.0",
        "url": "https://api.github.com/repos/deepset-ai/haystack/releases/33546535",
        "zipball_url": "https://api.github.com/repos/deepset-ai/haystack/zipball/v0.5.0"
      },
      {
        "authorType": "User",
        "author_name": "tholor",
        "body": "# Highlights\r\n## :boom: New Project Website & Documentation\r\nAs the project is growing, we have more and more content that doesn't fit in GitHub.  \r\nIn this first version of the website, we focused on documentation incl. quick start, usage guides and the API reference.\r\nIn the future, we plan to extend this with benchmarks, FAQs, use cases, and other content that helps you to build your QA system. \r\n\r\n### :point_right: https://haystack.deepset.ai \r\n\r\n## :chart_with_upwards_trend: Scalable dense retrieval: FAISSDocumentStore\r\nWith recent performance gains of dense retrieval methods (learn more about it [here](https://haystack.deepset.ai/en/docs/retrievermd#Deeper-Dive-Dense-vs-Sparse)), we need document stores that efficiently store vectors and find the most similar ones at query time. While Elasticsearch can also handle vectors, it quickly reaches its limits when dealing with larger datasets. We evaluated a couple of projects (FAISS, Scann, Milvus, Jina ...) that specialize on approximate nearest neighbour (ANN) algorithms for vector similarity. We decided to implement FAISS as it's easy to run in most environments. \r\nWe will likely add one of the heavier solutions (e.g. Jina or Milvus) later this year.        \r\n\r\nThe FAISSDocumentStore uses FAISS to handle embeddings and SQL to store the actual texts and meta data.\r\n\r\nUsage:\r\n\r\n``` python\r\n\r\ndocument_store = FAISSDocumentStore(sql_url: str = \"sqlite:///\",        # SQL DB for text + meta data\r\n                                    vector_size: int = 768)             # Dimensionality of your embeddings\r\n                                                                  \r\n\r\n```\r\n\r\n## :page_with_curl: More input file formats:  Apache Tika File Converter (#314 )\r\nThanks to @dany-nonstop you can now extract text from many file formats (docx, pptx, html, epub, odf ...) via Apache Tika.\r\n\r\nUsage: \r\n1. Start Apache Tika Server\r\n``` \r\ndocker run -d -p 9998:9998 apache/tika\r\n``` \r\n2. Do Conversion in Haystack\r\n\r\n``` \r\ntika_converter = TikaConverter(\r\n        tika_url = \"http://localhost:9998/tika\",\r\n        remove_numeric_tables = False,\r\n        remove_whitespace = False,\r\n        remove_empty_lines = False,\r\n        remove_header_footer = False,\r\n        valid_languages = None,\r\n    )\r\n>>> dict = tika_converter.convert(file_path=Path(\"test/samples/pdf/sample_pdf_1.pdf\"))\r\n>>> dict\r\n{ \r\n  \"text\": \"everything on page one \\f then page two \\f ...\"\r\n  'meta': {'Content-Type': 'application/pdf', 'Creation-Date': '2020-06-02T12:27:28Z', ...}\r\n}\r\n ```\r\n----------------------\r\n# Breaking changes\r\n\r\n## Restructuring / Renaming of modules (Breaking changes!) (#379)\r\n\r\nWe've restructured the package to make the usage more intuitive and terminology more consistent. \r\n\r\n1. Rename `database` module -> `document_store`\r\n2. Split `indexing` module into -> `file_converter` and `preprocessor`\r\n3. Move `Document`, `Label` and `Multilabel` classes into ->  `schema` and simplify import to `from haystack import Document, Label, Multilabel`  \r\n\r\n## File converters (#393)\r\nRefactoring major parts of the file converters. Not returning pages anymore, but rather adding page break symbols that can be accessed further down the pipeline.  \r\n\r\nOld:\r\n```\r\n>>> pages, meta = `Fileconverter.extract_pages(file_path=Path(\"...\"))`\r\n```\r\nNew: \r\n```\r\n>>> dict = `Fileconverter.convert(file_path=\"...\", meta={\"name\": \"some_name\", \"category\": \"news\"})`\r\n>>> dict\r\n{ \r\n  \"text\": \"everything on page one \\f then page two \\f ...\"\r\n  \"meta\": {\"name\": \"...\"}\r\n}\r\n ```\r\n\r\n## DensePassageRetriever (#308)\r\nRefactored from FB code to transformers code base and loading the models from their model hub now. \r\nSignature has therefore changed to: \r\n\r\n```\r\nretriever = DensePassageRetriever(document_store=document_store,\r\n                                  query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\r\n                                  passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\r\n                                  use_gpu=True,\r\n                                  embed_title=True,\r\n                                  remove_sep_tok_from_untitled_passages=True)\r\n```\r\n\r\n## Deprecate Tags for Document Stores (#286)\r\nWe removed the \"tags\" field that in the past could be associated with Documents and used for filtering your search. \r\nInsead, we use now the more general concept of \"meta\", where you can supply any custom fields and filter for them at runtime\r\n\r\nOld: \r\n```\r\ndict = {\"text\": \"some\", \"tags\": [\"category1\", \"category2\"]}\r\n```\r\nNew\r\n```\r\ndict =   {\"text\": \"some\", \"meta\": {\"category\": [\"1\", \"2\"] }}\r\n```\r\n-------------------------\r\n# Details\r\n\r\n**Document Stores**\r\n- Add FAISS Document Store [#253](https://github.com/deepset-ai/haystack/pull/253)\r\n- Fix type casting for vectors in FAISS [#399](https://github.com/deepset-ai/haystack/pull/399)\r\n- Fix duplicate vector ids in FAISS [#395](https://github.com/deepset-ai/haystack/pull/395)\r\n- Fix document filtering in SQLDocumentStore [#396](https://github.com/deepset-ai/haystack/pull/396)\r\n- Move retriever probability calculations to document_store [#389](https://github.com/deepset-ai/haystack/pull/389)\r\n- Add FAISS query scores [#368](https://github.com/deepset-ai/haystack/pull/368)\r\n- Raise Exception if filters used for FAISSDocumentStore query [#338](https://github.com/deepset-ai/haystack/pull/338)\r\n- Add refresh_type arg to ElasticsearchDocumentStore [#326](https://github.com/deepset-ai/haystack/pull/326)\r\n- Improve speed for SQLDocumentStore [#330](https://github.com/deepset-ai/haystack/pull/330)\r\n- Fix indexing of metadata for FAISS/SQL Document Store [#310](https://github.com/deepset-ai/haystack/pull/310)\r\n- Ensure exact match when filtering by meta in Elasticsearch [#311](https://github.com/deepset-ai/haystack/pull/311)\r\n- Deprecate Tags for Document Stores [#286](https://github.com/deepset-ai/haystack/pull/286)\r\n- Add option to update existing documents when indexing [#285](https://github.com/deepset-ai/haystack/pull/285)\r\n- Cast document_ids as strings [#284](https://github.com/deepset-ai/haystack/pull/284)\r\n- Add method to update meta fields for documents in Elasticsearch [#242](https://github.com/deepset-ai/haystack/pull/242)\r\n- Custom mapping write doc fix [#297](https://github.com/deepset-ai/haystack/pull/297)\r\n\r\n**Retriever**\r\n- DPR (Dense Retriever) for InMemoryDocumentStore #316 [#332](https://github.com/deepset-ai/haystack/pull/332)\r\n- Refactor DPR from FB to Transformers codebase [#308](https://github.com/deepset-ai/haystack/pull/308)\r\n- Restructure update embeddings [#304](https://github.com/deepset-ai/haystack/pull/304)\r\n- Added title during DPR passage embedding && ElasticsearchDocumentStore [#298](https://github.com/deepset-ai/haystack/pull/298)\r\n- Add eval for Dense Passage Retriever & Refactor handling of labels/feedback [#243](https://github.com/deepset-ai/haystack/pull/243)\r\n- Fix type of query_emb in DPR.retrieve() [#247](https://github.com/deepset-ai/haystack/pull/247)\r\n- Fix return type of EmbeddingRetriever to numpy array [#245](https://github.com/deepset-ai/haystack/pull/245)\r\n\r\n**Reader**\r\n- More robust Reader eval by limiting max answers and creating no answer labels [#331](https://github.com/deepset-ai/haystack/pull/331)\r\n- Aggregate multiple no answers in MultiLabel [#324](https://github.com/deepset-ai/haystack/pull/324)\r\n- Add \"no answer\" aggregation to Transformersreader [#259](https://github.com/deepset-ai/haystack/pull/259)\r\n- Align TransformersReader with FARMReader [#319](https://github.com/deepset-ai/haystack/pull/319)\r\n- Datasilo use all cores for preprocessing [#303](https://github.com/deepset-ai/haystack/pull/303)\r\n- Batch prediction in evaluation [#137](https://github.com/deepset-ai/haystack/pull/137)\r\n- Aggregate label objects for same questions [#292](https://github.com/deepset-ai/haystack/pull/292)\r\n- Add num_processes to reader.train() to configure multiprocessing [#271](https://github.com/deepset-ai/haystack/pull/271)\r\n- Added support for unanswerable questions in TransformersReader [#258](https://github.com/deepset-ai/haystack/pull/258)\r\n\r\n**Preprocessing**\r\n- Refactor file converter interface [#393](https://github.com/deepset-ai/haystack/pull/393)\r\n- Add Tika Converter [#314](https://github.com/deepset-ai/haystack/pull/314)\r\n\r\n**Finder**\r\n- Add index arg to Finder.get_answers() and _via_similar_questions() [#362](https://github.com/deepset-ai/haystack/pull/362)\r\n\r\n\r\n**Documentation**\r\n- Create documentation website [#272](https://github.com/deepset-ai/haystack/pull/272)\r\n- Use port 8000 in documentation [#357](https://github.com/deepset-ai/haystack/pull/357)\r\n- Documentation [#343](https://github.com/deepset-ai/haystack/pull/343)\r\n- Convert Documentation to markdown [#386](https://github.com/deepset-ai/haystack/pull/386)\r\n- Add logo to readme [#384](https://github.com/deepset-ai/haystack/pull/384)\r\n- Refactor the DPR tutorial to use FAISS [#317](https://github.com/deepset-ai/haystack/pull/317)\r\n- Make Tutorials Work on Colab GPUs [#322](https://github.com/deepset-ai/haystack/pull/322)\r\n\r\n**Other**\r\n- Exclude embedding fields from the REST API [#390](https://github.com/deepset-ai/haystack/pull/390)\r\n- Fix test suite dependency issue on MacOS [#374](https://github.com/deepset-ai/haystack/pull/374)\r\n- Add Gunicorn timeout [#364](https://github.com/deepset-ai/haystack/pull/364)\r\n- Bump FARM version to 0.4.7 [#340](https://github.com/deepset-ai/haystack/pull/340)\r\n- Add Tests for MultiLabel [#318](https://github.com/deepset-ai/haystack/pull/318)\r\n- Modified search endpoints logs to dump json [#290](https://github.com/deepset-ai/haystack/pull/290)\r\n- Add export answers to CSV function [#266](https://github.com/deepset-ai/haystack/pull/266)\r\n\r\n### Big thanks to all contributors :hearts: \r\n@antoniolanza1996, @dany-nonstop, @philipp-bode, @lalitpagaria , @PiffPaffM , @brandenchan , @tanaysoni , @Timoeller , @tholor, @bogdankostic , @maxupp, @kolk , @venuraja79 , @karimjp ",
        "dateCreated": "2020-09-21T08:54:17Z",
        "datePublished": "2020-09-21T09:01:54Z",
        "html_url": "https://github.com/deepset-ai/haystack/releases/tag/v0.4.0",
        "name": "v0.4.0",
        "tag_name": "v0.4.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/haystack/tarball/v0.4.0",
        "url": "https://api.github.com/repos/deepset-ai/haystack/releases/31592435",
        "zipball_url": "https://api.github.com/repos/deepset-ai/haystack/zipball/v0.4.0"
      },
      {
        "authorType": "User",
        "author_name": "tholor",
        "body": "## :mag: Dense Passage Retrieval\r\nGlad to introduce the new **Dense Passage Retriever** (aka DPR).\r\nUsing dense embeddings of texts is a powerful alternative to score the similarity of texts. This retriever uses two BERT models - one to embed your query, one to embed your passage. This Dual-Encoder architecture can deal much better with the different nature of query and texts (length, syntax ...). It's was published by Karpukhin et al and shows impressive performance  - especially if there's no direct overlap between tokens in your queries and your texts. \r\n\r\n``` python\r\nretriever = DensePassageRetriever(document_store=document_store,\r\n                                  embedding_model=\"dpr-bert-base-nq\",\r\n                                  do_lower_case=True, use_gpu=True)\r\nretriever.retrieve(query=\"What is cosine similarity?\")\r\n# returns: [Document, Document]\r\n```\r\nSee [Tutorial 6](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial6_Better_Retrieval_via_DPR.ipynb) for more details\r\n\r\n## :bar_chart: Evaluation\r\nWe introduce the option to evaluate your reader, retriever, and the combination of both. While there's usually a good understanding of the reader's performance, the interplay with the retriever is what really matters in practice. You want to answer: Is my retriever a bottleneck? Is it worth increasing `top_k` for the retriever? How do different retrievers compare in performance? What is the effect on speed? \r\nThe new `eval()` is a first step towards answering those questions and gives a comprehensive picture of your pipeline. Stay tuned for more enhancements here. \r\n\r\n``` python\r\ndocument_store.add_eval_data(\"../data/nq/nq_dev_subset_v2.json\")\r\n...\r\nretriever.eval(top_k=10)\r\nreader.eval(document_store=document_store, device=device)\r\nfinder.eval(top_k_retriever=10, top_k_reader=10)\r\n```\r\nSee [Tutorial 5](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial5_Evaluation.ipynb) for more details\r\n\r\n## :page_facing_up: Basic Support for PDF and Docx Files\r\nYou can now index PDF and docx files more easily to your DocumentStore. We introduce a new BaseConverter class, that offers basic cleaning functions (e.g. removing footers or tables). It's file format specific child classes (e.g. PDFToTextConverter) handle the actual extraction of the text.\r\n\r\n``` python\r\n\r\n#PDF\r\nfrom haystack.indexing.file_converters.pdf import PDFToTextConverter\r\nconverter = PDFToTextConverter(remove_header_footer=True, remove_numeric_tables=True, valid_languages=[\"de\",\"en\"])\r\npages = converter.extract_pages(file_path=file)\r\n# => list of str, one per page\r\n\r\n#DOCX\r\nfrom haystack.indexing.file_converters.docx import DocxToTextConverter\r\nconverter = DocxToTextConverter()\r\nparagraphs = converter.extract_pages(file_path=file)\r\n#  => list of str, one per paragraph (as docx has no direct notion of pages)\r\n``` \r\n\r\n------------------------------------------\r\nAnd there's much more that happened ... \r\n\r\n### Preprocessing\r\n- Added Support for Docx Files [#225](https://github.com/deepset-ai/haystack/pull/225)\r\n- Add PDF parser for indexing [#109](https://github.com/deepset-ai/haystack/pull/109)\r\n- Adjust PDF conversion subprocess for Python v3.6 [#194](https://github.com/deepset-ai/haystack/pull/194)\r\n- Fix boundary condition in detection of header/footer in file converters [#165](https://github.com/deepset-ai/haystack/pull/165)\r\n\r\n### Retriever\r\n- Refactor DPR for latest transformers version & change init arg `gpu` -> `use_gpu` for DPR and EmbeddingRetriever [#239](https://github.com/deepset-ai/haystack/pull/239)\r\n- Add dummy retriever for benchmarking / reader-only settings [#235](https://github.com/deepset-ai/haystack/pull/235)\r\n- Fix id for documents returned by the TfidfRetriever [#232](https://github.com/deepset-ai/haystack/pull/232)\r\n- Tutorial for Dense Passage Retriever [#186](https://github.com/deepset-ai/haystack/pull/186)\r\n- Fix device arg for sentence transformers [#124](https://github.com/deepset-ai/haystack/pull/124)\r\n- Fix embeddings from sentence-transformers (type cast & gpu flags) [#121](https://github.com/deepset-ai/haystack/pull/121)\r\n- Adding metadata to be returned from tfidf retreiver [#122](https://github.com/deepset-ai/haystack/pull/122)\r\n\r\n### Reader\r\n- Add ONNXRuntime support [#157](https://github.com/deepset-ai/haystack/pull/157)\r\n- Fix multi gpu training via Dataparallel [#234](https://github.com/deepset-ai/haystack/pull/234)\r\n- Fix document id missing in farm inference output [#174](https://github.com/deepset-ai/haystack/pull/174)\r\n- Add document meta for Transformer Reader [#114](https://github.com/deepset-ai/haystack/pull/114)\r\n- Fix naming of offset in answers of TransformersReader (for consistency with FARMReader) [#204](https://github.com/deepset-ai/haystack/pull/204)\r\n- Adjust to farm handling of no answer [#170](https://github.com/deepset-ai/haystack/pull/170)\r\n\r\n### DocumentStores\r\n- Move document_name attribute to meta [#217](https://github.com/deepset-ai/haystack/pull/217)\r\n- Remove meta field when writing documents in Elasticsearch [#240](https://github.com/deepset-ai/haystack/pull/240)\r\n- Harmonize meta data handling across doc stores [#214](https://github.com/deepset-ai/haystack/pull/214)\r\n- Add filtering by tags for InMemoryDocumentStore [#108](https://github.com/deepset-ai/haystack/pull/108)\r\n- Make FAQ question field customizable [#146](https://github.com/deepset-ai/haystack/pull/146)\r\n- Increase timeout for Elasticsearch bulk indexing [#119](https://github.com/deepset-ai/haystack/pull/119)\r\n- Add embedding query for InMemoryDocumentStore  [#112](https://github.com/deepset-ai/haystack/pull/112)\r\n- Increase timeout for bulk indexing in ES [#130](https://github.com/deepset-ai/haystack/pull/130)\r\n- Add custom port to ElasticsearchDocumentStore [#129](https://github.com/deepset-ai/haystack/pull/129)\r\n- Remove hard-coded embedding field [#107](https://github.com/deepset-ai/haystack/pull/107)\r\n\r\n### REST API\r\n- Move out REST API from PyPI package [#160](https://github.com/deepset-ai/haystack/pull/160)\r\n- Fix format of /export-doc-qa-feedback to comply with SQuAD [#241](https://github.com/deepset-ai/haystack/pull/241)\r\n- Create file upload directory in the REST API [#166](https://github.com/deepset-ai/haystack/pull/166)\r\n- Add API endpoint to upload files [#154](https://github.com/deepset-ai/haystack/pull/154)\r\n- Missing PORT and SCHEME for elasticsearch to run the API [#134](https://github.com/deepset-ai/haystack/pull/134)\r\n- Add EMBEDDING_MODEL_FORMAT in API config [#152](https://github.com/deepset-ai/haystack/pull/152)\r\n- Add success response for successful file upload API [#195](https://github.com/deepset-ai/haystack/pull/195)\r\n- Add response time in logs [#201](https://github.com/deepset-ai/haystack/pull/201)\r\n- Fix rest api in Docker image after refactoring [#178](https://github.com/deepset-ai/haystack/pull/178)\r\n\r\n### Other\r\n- Upgrade to new FARM / Transformers / PyTorch versions [#212](https://github.com/deepset-ai/haystack/pull/212)\r\n- Fix Evaluation Dataset [#233](https://github.com/deepset-ai/haystack/pull/233)\r\n- Remove mutation of documents in write_documents() [#231](https://github.com/deepset-ai/haystack/pull/231)\r\n- Remove mutation of results dict in print_answers() [#230](https://github.com/deepset-ai/haystack/pull/230)\r\n- Make doc name optional [#100](https://github.com/deepset-ai/haystack/pull/100)\r\n- Fix Dockerfile to build successfully without models directory [#210](https://github.com/deepset-ai/haystack/pull/210)\r\n- Docker available for TransformsReader Class [#180](https://github.com/deepset-ai/haystack/pull/180)\r\n- Fix embedding method in FAQ-QA Tutorial [#220](https://github.com/deepset-ai/haystack/pull/220)\r\n- Add more tests [#213](https://github.com/deepset-ai/haystack/pull/213)\r\n- Update docstring for embedding_field and embedding_dim [#208](https://github.com/deepset-ai/haystack/pull/208)\r\n- Make \"meta\" field generic for Document Schema [#102](https://github.com/deepset-ai/haystack/pull/102)\r\n- Update tutorials [#200](https://github.com/deepset-ai/haystack/pull/200)\r\n- Upgrade FARM version [#172](https://github.com/deepset-ai/haystack/pull/172)\r\n- Fix for installing PyTorch on Windows OS [#159](https://github.com/deepset-ai/haystack/pull/159)\r\n- Remove Literal type hint [#156](https://github.com/deepset-ai/haystack/pull/156)\r\n- Remove PyMuPDF dependency [#148](https://github.com/deepset-ai/haystack/pull/148)\r\n- Add missing type hints [#138](https://github.com/deepset-ai/haystack/pull/138)\r\n- Add a GitHub Action to start Elasticsearch instance for Build workflow [#142](https://github.com/deepset-ai/haystack/pull/142)\r\n- Correct field in evaluation tutorial [#139](https://github.com/deepset-ai/haystack/pull/139)\r\n- Update Haystack version in tutorials [#136](https://github.com/deepset-ai/haystack/pull/136)\r\n- Fix evaluation [#132](https://github.com/deepset-ai/haystack/pull/132)\r\n- Add stalebot [#131](https://github.com/deepset-ai/haystack/pull/131)\r\n- Add Reader/Retriever validations in Finder [#113](https://github.com/deepset-ai/haystack/pull/113)\r\n- Add document metadata for FAQ style QA [#106](https://github.com/deepset-ai/haystack/pull/106)\r\n- Add basic tutorial for FAQ-based QA & batch comp. of embeddings [#98](https://github.com/deepset-ai/haystack/pull/98)\r\n- Make saving more explicit in tutorial [#95](https://github.com/deepset-ai/haystack/pull/95)\r\n\r\nThanks to all contributors for working on this and shaping Haystack together: @skirdey @guillim @antoniolanza1996 @F4r1n @arthurbarros @elyase @anirbansaha96 @Timoeller @bogdankostic @tanaysoni @brandenchan \r\n",
        "dateCreated": "2020-07-16T11:30:12Z",
        "datePublished": "2020-07-16T12:30:03Z",
        "html_url": "https://github.com/deepset-ai/haystack/releases/tag/0.3.0",
        "name": "0.3.0",
        "tag_name": "0.3.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/haystack/tarball/0.3.0",
        "url": "https://api.github.com/repos/deepset-ai/haystack/releases/28627977",
        "zipball_url": "https://api.github.com/repos/deepset-ai/haystack/zipball/0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "tanaysoni",
        "body": "In our release notes, we will always highlight a few important changes first and list the detailed PRs below.\r\n\r\n## :tada: First release\r\n\r\nHappy to announce our first proper release incl. many of the features that we found absolutely crucial for a QA system. While we still have countless exciting features on our roadmap, we are confident that this version already  accelerates your development phase of QA systems significantly and it was also tested successfully in the first production deployments.  \r\nFrom now on, we will switch to a more regular release cycle.\r\n\r\n##  :scroll: ElasticsearchDocumentStore\r\nWe recommend the new [ElasticsearchDocumentStore](https://github.com/deepset-ai/haystack/blob/f96d2d2956b0d066fe739829c5032485aa28c2f5/haystack/database/elasticsearch.py#L13) for all production deployments. While we will keep more light-weight options (SQL, In-Memory) for easy prototyping, new features will be implemented first for Elasticsearch.\r\n\r\n## :rocket: New Retrievers\r\nBeside plain TF-IDF (in memory), we introduced the [ElasticsearchRetriever](https://github.com/deepset-ai/haystack/blob/f96d2d2956b0d066fe739829c5032485aa28c2f5/haystack/retriever/elasticsearch.py#L11) that supports Elasticsearch's native scoring (BM25) or custom queries (e.g. using boosting). \r\n\r\nAs a further option, we also added the [EmbeddingRetriever](https://github.com/deepset-ai/haystack/blob/f96d2d2956b0d066fe739829c5032485aa28c2f5/haystack/retriever/elasticsearch.py#L51) that encodes texts into embeddings (e.g. via Sentence-BERT) and retrieves via cosine-similarity. Especially the latter is very promising and you will likely see more features in this direction.\r\n\r\n## :interrobang: FAQ-style QA\r\nBeside extractive QA, you can now also index existing question-answer pairs (e.g. from FAQs) and find answers via matching the incoming user-question with the indexed questions and returning the related answer from that pair. This can be an interesting alternative or addition to extractive QA, if you already have huge collections of FAQs and/or need a solution that works with low computational resources.\r\n\r\n## :repeat:  Modular API based on FastAPI\r\nWe changed the basic REST API from Flask to FastAPI and modularized it. \r\nYou can now: \r\n- search answers in texts (extractive QA)\r\n- search answers by comparing user question to existing questions (FAQ-style QA)\r\n- collect & export user feedback on answers to gain domain-specific training data (feedback)\r\n- do basic monitoring of requests (currently via APM in Kibana)\r\n\r\n----------------------------------------\r\n## Detailed changes:\r\n\r\n## Document Stores\r\n-  Add Elasticsearch Datastore [#13](https://github.com/deepset-ai/haystack/pull/13)\r\n-  Refactor database layer [#10](https://github.com/deepset-ai/haystack/pull/10)\r\n-  Add test for Elasticsearch document store [#88](https://github.com/deepset-ai/haystack/pull/88)\r\n-  Make filters optional for Elasticsearch query [#80](https://github.com/deepset-ai/haystack/pull/80)\r\n-  Inmemory store [#76](https://github.com/deepset-ai/haystack/pull/76)\r\n-  Fix get_all_documents() in ElasticsearchDocumentStore [#77](https://github.com/deepset-ai/haystack/pull/77)\r\n-  Fix `get_all_documents` query for Elasticsearch [#21](https://github.com/deepset-ai/haystack/pull/21)\r\n\r\n## Retrievers\r\n-  Add FAQ-style QA [#44](https://github.com/deepset-ai/haystack/pull/44)\r\n-  added option for custom elasticsearch queries and filters [#52](https://github.com/deepset-ai/haystack/pull/52)\r\n-  More flexbile es config & support for filters [#29](https://github.com/deepset-ai/haystack/pull/29)\r\n-  Add more ES connection params [#35](https://github.com/deepset-ai/haystack/pull/35)\r\n-  Simplify Retriever query [#73](https://github.com/deepset-ai/haystack/pull/73)\r\n-  Refactor ElasticsearchRetriever into separate class [#72](https://github.com/deepset-ai/haystack/pull/72)\r\n-  Add params to create_embeddings in retriever [#45](https://github.com/deepset-ai/haystack/pull/45)\r\n-  fix scaling of pseudo probs for es scores. fix filtering of embedding retrieval [#46](https://github.com/deepset-ai/haystack/pull/46)\r\n-  Fixing doc_name for TFIDF Retriever [#33](https://github.com/deepset-ai/haystack/pull/33)\r\n\r\n\r\n## Readers\r\n-  Refactor pipeline for better generalizability & Add TransformersReader  [#1](https://github.com/deepset-ai/haystack/pull/1)\r\n-  Add method to train a reader on custom data [#5](https://github.com/deepset-ai/haystack/pull/5)\r\n-  Add no answer handling [#26](https://github.com/deepset-ai/haystack/pull/26)\r\n-  Add no_answer option to results [#24](https://github.com/deepset-ai/haystack/pull/24)\r\n-  Fix offsets in reader [#4](https://github.com/deepset-ai/haystack/pull/4)\r\n-  FARMReader.train() now takes default values from FARMReader [#47](https://github.com/deepset-ai/haystack/pull/47)\r\n-  Update inferencer args (num_processes, chunksize) to latest FARM version [#54](https://github.com/deepset-ai/haystack/pull/54)\r\n-  update readme & rename arg in TransformersReader for consistency [#86](https://github.com/deepset-ai/haystack/pull/86)\r\n-  Fixing typo in transformer. use_gpu provides ordinal of the gpu, not \u2026 [#83](https://github.com/deepset-ai/haystack/pull/83)\r\n-  Add document_id with Transformers Reader  [#60](https://github.com/deepset-ai/haystack/pull/60)\r\n-  Make eval during reader.train() more verbose [#28](https://github.com/deepset-ai/haystack/pull/28)\r\n-  Removed \"document_name\" from farm.py [#31](https://github.com/deepset-ai/haystack/pull/31)\r\n-  Add a document_name field in answers [#30](https://github.com/deepset-ai/haystack/pull/30)\r\n\r\n\r\n## REST API / Deployment\r\n-  Move API from flask to fastAPI [#3](https://github.com/deepset-ai/haystack/pull/3)\r\n-  Modularize API components [#55](https://github.com/deepset-ai/haystack/pull/55)\r\n-  Return more meta data & restructure reponse format [#66](https://github.com/deepset-ai/haystack/pull/66)\r\n-  Log API responses in APM [#70](https://github.com/deepset-ai/haystack/pull/70)\r\n-  Make Elastic-APM optional [#65](https://github.com/deepset-ai/haystack/pull/65)\r\n-  Update Python version in Dockerfile-GPU [#71](https://github.com/deepset-ai/haystack/pull/71)\r\n-  Update Dockerfiles to use Gunicorn for deployment [#69](https://github.com/deepset-ai/haystack/pull/69)\r\n-  Add limit on concurrent requests for doc-qa [#64](https://github.com/deepset-ai/haystack/pull/64)\r\n-  Add Docker Images for running Haystack [#85](https://github.com/deepset-ai/haystack/pull/85)\r\n-  Fix cyclic import of Elasticsearch client [#59](https://github.com/deepset-ai/haystack/pull/59)\r\n-  Add Feedback export API [#56](https://github.com/deepset-ai/haystack/pull/56)\r\n-  Add gpu dockerfile, improve logging, fix minor bug with filtering [#36](https://github.com/deepset-ai/haystack/pull/36)\r\n-  Improve deployment of REST API (Configs, logging, minor bugs) [#40](https://github.com/deepset-ai/haystack/pull/40)\r\n\r\n## Others\r\n-  Standardize Finder, Readers, and Retriever interfaces [#62](https://github.com/deepset-ai/haystack/pull/62)\r\n-  pin haystack version in tutorials until release [#87](https://github.com/deepset-ai/haystack/pull/87)\r\n-  Update tutorials to use Elasticsearch, new Retrievers [#79](https://github.com/deepset-ai/haystack/pull/79)\r\n-  Adding coverage reports and a few more tests [#78](https://github.com/deepset-ai/haystack/pull/78)\r\n-  Added Jupyter notebooks of Tutorials [#43](https://github.com/deepset-ai/haystack/pull/43)\r\n-  Add minimal tutorial for ES [#19](https://github.com/deepset-ai/haystack/pull/19)\r\n-  Update tutorials [#12](https://github.com/deepset-ai/haystack/pull/12)\r\n\r\nThanks to all contributors for your great work :clap:  \r\n@tanaysoni, @Timoeller , @brandenchan,  @bogdankostic , @skirdey ,  @stedomedo , @karthik19967829 , @aadil-srivastava01 , @tholor\r\n",
        "dateCreated": "2020-05-05T13:08:00Z",
        "datePublished": "2020-05-05T10:57:44Z",
        "html_url": "https://github.com/deepset-ai/haystack/releases/tag/0.2.1",
        "name": "0.2.1",
        "tag_name": "0.2.1",
        "tarball_url": "https://api.github.com/repos/deepset-ai/haystack/tarball/0.2.1",
        "url": "https://api.github.com/repos/deepset-ai/haystack/releases/26173217",
        "zipball_url": "https://api.github.com/repos/deepset-ai/haystack/zipball/0.2.1"
      },
      {
        "authorType": "User",
        "author_name": "tanaysoni",
        "body": "",
        "dateCreated": "2019-11-28T09:53:56Z",
        "datePublished": "2019-11-28T09:55:10Z",
        "html_url": "https://github.com/deepset-ai/haystack/releases/tag/0.1.0",
        "name": "Initial Release",
        "tag_name": "0.1.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/haystack/tarball/0.1.0",
        "url": "https://api.github.com/repos/deepset-ai/haystack/releases/21825370",
        "zipball_url": "https://api.github.com/repos/deepset-ai/haystack/zipball/0.1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3452,
      "date": "Wed, 22 Dec 2021 23:31:24 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "nlp",
      "question-answering",
      "bert",
      "transfer-learning",
      "language-model",
      "pytorch",
      "semantic-search",
      "neural-search",
      "squad",
      "elasticsearch",
      "dpr",
      "information-retrieval",
      "summarization",
      "search-engine",
      "transformers",
      "natural-language-processing",
      "machine-learning",
      "ai",
      "python"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "![image](https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/_src/img/concepts_haystack_handdrawn.png)\n\nFollow our [introductory tutorial](https://haystack.deepset.ai/tutorials/first-qa-system) \nto setup a question answering system using Python and start performing queries! \nExplore the rest of our tutorials to learn how to tweak pipelines, train models and perform evaluation.\n\n- Tutorial 1 - Basic QA Pipeline: [Jupyter notebook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial1_Basic_QA_Pipeline.py)\n- Tutorial 2 - Fine-tuning a model on own data: [Jupyter notebook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial2_Finetune_a_model_on_your_data.py)\n- Tutorial 3 - Basic QA Pipeline without Elasticsearch: [Jupyter notebook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.py)\n- Tutorial 4 - FAQ-style QA: [Jupyter notebook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial4_FAQ_style_QA.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial4_FAQ_style_QA.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial4_FAQ_style_QA.py)\n- Tutorial 5 - Evaluation of the whole QA-Pipeline: [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial5_Evaluation.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial5_Evaluation.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial5_Evaluation.py)\n- Tutorial 6 - Better Retrievers via \"Dense Passage Retrieval\":\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial6_Better_Retrieval_via_DPR.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial6_Better_Retrieval_via_DPR.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial6_Better_Retrieval_via_DPR.py)\n- Tutorial 7 - Generative QA via \"Retrieval-Augmented Generation\":\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial7_RAG_Generator.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial7_RAG_Generator.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial7_RAG_Generator.py)\n- Tutorial 8 - Preprocessing:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial8_Preprocessing.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial8_Preprocessing.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial8_Preprocessing.py)\n- Tutorial 9 - DPR Training:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial9_DPR_training.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial9_DPR_training.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial9_DPR_training.py)\n- Tutorial 10 - Knowledge Graph:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial10_Knowledge_Graph.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial10_Knowledge_Graph.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial10_Knowledge_Graph.py)\n- Tutorial 11 - Pipelines:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial11_Pipelines.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial11_Pipelines.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial11_Pipelines.py)\n- Tutorial 12 - Long-Form Question Answering:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial12_LFQA.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial12_LFQA.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial12_LFQA.py)\n- Tutorial 13 - Question Generation:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial13_Question_generation.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial13_Question_generation.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial13_Question_generation.py)\n- Tutorial 14 - Query Classifier:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial14_Query_Classifier.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial14_Query_Classifier.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial14_Query_Classifier.py)\n- Tutorial 15 - TableQA:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial15_TableQA.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial15_TableQA.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial15_TableQA.py)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "**Hosted**\n\nTry out our hosted [Explore The World](https://haystack-demo.deepset.ai/) live demo here!\nAsk any question on countries or capital cities and let Haystack return the answers to you. \n\n**Local**\n\nStart up a Haystack service via [Docker Compose](https://docs.docker.com/compose/).\nWith this you can begin calling it directly via the REST API or even interact with it using the included Streamlit UI.\n\n<details>\n  <summary>Click here for a step-by-step guide</summary>\n\n**1. Update/install Docker and Docker Compose, then launch Docker**\n\n```\n    apt-get update && apt-get install docker && apt-get install docker-compose\n    service docker start\n```\n\n**2. Clone Haystack repository**\n\n```\n    git clone https://github.com/deepset-ai/haystack.git\n```\n\n**3. Pull images & launch demo app**\n\n```\n    cd haystack\n    docker-compose pull\n    docker-compose up\n    \n    #: Or on a GPU machine: docker-compose -f docker-compose-gpu.yml up\n```\n\nYou should be able to see the following in your terminal window as part of the log output:\n\n```\n..\nui_1             |   You can now view your Streamlit app in your browser.\n..\nui_1             |   External URL: http://192.168.108.218:8501\n..\nhaystack-api_1   | [2021-01-01 10:21:58 +0000] [17] [INFO] Application startup complete.\n```\n\n**4. Open the Streamlit UI for Haystack by pointing your browser to the \"External URL\" from above.**\n\nYou should see the following:\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/_src/img/streamlit_ui_screenshot.png)\n\nYou can then try different queries against a pre-defined set of indexed articles related to Game of Thrones.\n\n**Note**: The following containers are started as a part of this demo:\n\n* Haystack API: listens on port 8000\n* DocumentStore (Elasticsearch): listens on port 9200\n* Streamlit UI: listens on port 8501\n\nPlease note that the demo will [publish](https://docs.docker.com/config/containers/container-networking/) the container ports to the outside world. *We suggest that you review the firewall settings depending on your system setup and the security guidelines.*\n\n</details>\n\n",
      "technique": "Header extraction"
    }
  ]
}