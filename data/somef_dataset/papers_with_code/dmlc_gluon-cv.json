{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1812.01187",
      "https://arxiv.org/abs/1902.04103",
      "https://arxiv.org/abs/2004.08955"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you feel our code or models helps in your research, kindly cite our papers:\n\n```\n@article{gluoncvnlp2020,\n  author  = {Jian Guo and He He and Tong He and Leonard Lausen and Mu Li and Haibin Lin and Xingjian Shi and Chenguang Wang and Junyuan Xie and Sheng Zha and Aston Zhang and Hang Zhang and Zhi Zhang and Zhongyue Zhang and Shuai Zheng and Yi Zhu},\n  title   = {GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing},\n  journal = {Journal of Machine Learning Research},\n  year    = {2020},\n  volume  = {21},\n  number  = {23},\n  pages   = {1-7},\n  url     = {http://jmlr.org/papers/v21/19-429.html}\n}\n\n@article{he2018bag,\n  title={Bag of Tricks for Image Classification with Convolutional Neural Networks},\n  author={He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},\n  journal={arXiv preprint arXiv:1812.01187},\n  year={2018}\n}\n\n@article{zhang2019bag,\n  title={Bag of Freebies for Training Object Detection Neural Networks},\n  author={Zhang, Zhi and He, Tong and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},\n  journal={arXiv preprint arXiv:1902.04103},\n  year={2019}\n}\n\n@article{zhang2020resnest,\n  title={ResNeSt: Split-Attention Networks},\n  author={Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Zhang, Zhi and Lin, Haibin and Sun, Yue and He, Tong and Muller, Jonas and Manmatha, R. and Li, Mu and Smola, Alexander},\n  journal={arXiv preprint arXiv:2004.08955},\n  year={2020}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhang2020resnest,\n  title={ResNeSt: Split-Attention Networks},\n  author={Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Zhang, Zhi and Lin, Haibin and Sun, Yue and He, Tong and Muller, Jonas and Manmatha, R. and Li, Mu and Smola, Alexander},\n  journal={arXiv preprint arXiv:2004.08955},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhang2019bag,\n  title={Bag of Freebies for Training Object Detection Neural Networks},\n  author={Zhang, Zhi and He, Tong and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},\n  journal={arXiv preprint arXiv:1902.04103},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{he2018bag,\n  title={Bag of Tricks for Image Classification with Convolutional Neural Networks},\n  author={He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},\n  journal={arXiv preprint arXiv:1812.01187},\n  year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{gluoncvnlp2020,\n  author  = {Jian Guo and He He and Tong He and Leonard Lausen and Mu Li and Haibin Lin and Xingjian Shi and Chenguang Wang and Junyuan Xie and Sheng Zha and Aston Zhang and Hang Zhang and Zhi Zhang and Zhongyue Zhang and Shuai Zheng and Yi Zhu},\n  title   = {GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing},\n  journal = {Journal of Machine Learning Research},\n  year    = {2020},\n  volume  = {21},\n  number  = {23},\n  pages   = {1-7},\n  url     = {http://jmlr.org/papers/v21/19-429.html}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8105854829432959
      ],
      "excerpt": "students to fast prototype products and research ideas based on these \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997228446288564,
        0.9827607999644112
      ],
      "excerpt": "| Pose Estimation: <br/>detect human pose <br/> from images. | <a href=\"https://gluon-cv.mxnet.io/model_zoo/pose.html\"><img src=\"docs/_static/pose-estimation.svg\" alt=\"pose\" height=\"200\"/></a> | <a href=\"https://gluon-cv.mxnet.io/model_zoo/pose.html#simple-pose-with-resnet\">Simple Pose</a>| \n| Video Action Recognition: <br/>recognize human actions <br/> in a video. | <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\"><img src=\"docs/_static/action-recognition.png\" alt=\"action_recognition\" height=\"200\"/></a> | MXNet: <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">TSN</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">C3D</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">I3D</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">I3D_slow</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">P3D</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">R3D</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">R2+1D</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">Non-local</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">SlowFast</a> <br/> PyTorch: <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">TSN</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">I3D</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">I3D_slow</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">R2+1D</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">Non-local</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">CSN</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">SlowFast</a>, <a href=\"https://gluon-cv.mxnet.io/model_zoo/action_recognition.html\">TPN</a> | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8034006736414253
      ],
      "excerpt": "| Person Re-ID: <br/>re-identify pedestrians across scenes | <a href=\"https://github.com/dmlc/gluon-cv/tree/master/scripts/re-id/baseline\"><img src=\"https://user-images.githubusercontent.com/3307514/46702937-f4311800-cbd9-11e8-8eeb-c945ec5643fb.png\" alt=\"re-id\" height=\"160\"/></a> |<a href=\"https://github.com/dmlc/gluon-cv/tree/master/scripts/re-id/baseline\">Market1501 baseline</a> | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": ": cuda 10.2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": ": cuda 10.2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": ": cuda 10.2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": ": cuda 10.2 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dmlc/gluon-cv",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-02-26T01:33:21Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-26T17:47:34Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9619375963416795,
        0.9764023027046526,
        0.8849545413610389
      ],
      "excerpt": "GluonCV provides implementations of the state-of-the-art (SOTA) deep learning models in computer vision. \nIt is designed for engineers, researchers, and \nstudents to fast prototype products and research ideas based on these \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8326715915662735
      ],
      "excerpt": "Community supports \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230130413365191
      ],
      "excerpt": "The latest stable version of GluonCV is 0.8 and we recommend mxnet 1.6.0/1.7.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9121491139759834
      ],
      "excerpt": "The latest stable version of GluonCV is 0.8 and we recommend PyTorch 1.6.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9629512677820836
      ],
      "excerpt": "GluonCV documentation is available at our website. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9719460717881211
      ],
      "excerpt": "For background knowledge of deep learning or CV, please refer to the open source book Dive into Deep Learning. If you are new to Gluon, please check out our 60-minute crash course. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Gluon CV Toolkit",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dmlc/gluon-cv/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1120,
      "date": "Sun, 26 Dec 2021 23:35:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dmlc/gluon-cv/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dmlc/gluon-cv",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/dmlc/gluon-cv/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/deeplab_resnest101_ade.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/deeplab_resnet101_voc.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/deeplab_resnest269_ade.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/deeplab_resnest200_ade.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/deeplab_resnet101_citys.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/deeplab_v3b_plus_wideresnet_citys.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/deeplab_resnet101_coco.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/danet_resnet50_citys.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/deeplab_resnet152_voc.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/fcn_resnet50_ade.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/deeplab_resnet101_ade.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/deeplab_resnest50_ade.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/deeplab_resnet50_citys.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/fcn_resnet101_voc.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/danet_resnet101_citys.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/deeplab_resnet50_ade.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/fcn_resnet101_coco.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/segmentation/fcn_resnet101_ade.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/ssd_300_resnet34_v1b_coco.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/ssd_300_vgg16_atrous_coco.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/ssd_300_vgg16_atrous_voc.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/yolo3_darknet53_coco.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/ssd_512_vgg16_atrous_coco.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/center_net_resnet101_v1b_coco.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/center_net_resnet101_v1b_voc.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/ssd_512_resnet50_v1_coco.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/yolo3_darknet53_voc.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/center_net_resnet18_v1b_voc.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/ssd_512_resnet50_v1_voc.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/center_net_resnet18_v1b_coco.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/yolo3_mobilenet1.0_coco.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/yolo3_mobilenet1.0_voc.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/ssd_512_vgg16_atrous_voc.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/ssd_512_mobilenet1.0_coco.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/faster_rcnn_resnet50_v1b_voc.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/ssd_512_mobilenet1.0_voc.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/center_net_resnet50_v1b_voc.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/detection/center_net_resnet50_v1b_coco.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/i3d_resnet50_v1_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/inceptionv1_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/p3d_resnet101_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/i3d_nl10_resnet101_v1_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/i3d_resnet101_v1_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/i3d_resnet50_v1_ucf101.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/i3d_resnet50_v1_sthsthv2.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/resnet50_v1b_hmdb51.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/resnet50_v1b_sthsthv2.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/i3d_nl10_resnet50_v1_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/p3d_resnet50_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/r2plus1d_resnet18_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/i3d_resnet50_v1_hmdb51.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/r2plus1d_resnet50_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/resnet152_v1b_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/slowfast_8x8_resnet50_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/i3d_nl5_resnet101_v1_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/vgg16_ucf101.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/i3d_nl5_resnet50_v1_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/resnet101_v1b_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/slowfast_8x8_resnet101_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/slowfast_4x16_resnet50_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/r2plus1d_resnet34_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/c3d_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/resnet34_v1b_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/i3d_inceptionv3_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/i3d_inceptionv1_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/inceptionv3_ucf101.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/inceptionv3_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/resnet18_v1b_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/action-recognition/resnet50_v1b_kinetics400.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/pose/simple_pose_resnet50_v1d.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/pose/simple_pose_resnet152_v1b.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/pose/simple_pose_resnet152_v1d.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/pose/alpha_pose_resnet101_v1b_coco.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/pose/mobile_pose_mobilenetv2_1.0.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/pose/mobile_pose_resnet50_v1b.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/pose/mobile_pose_mobilenetv3_small.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/pose/mobile_pose_resnet18_v1b.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/pose/simple_pose_resnet101_v1d.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/pose/mobile_pose_mobilenet1.0.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/pose/simple_pose_resnet50_v1b.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/pose/simple_pose_resnet18_v1b.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/pose/mobile_pose_mobilenetv3_large.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/pose/simple_pose_resnet101_v1b.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnext50_32x4d.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/mobilenetv3_large.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet50_v1d_0.11.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet50_v1.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet152_v1.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnest101.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/mobilenet1.0.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet101_v1.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/se_resnext101_32x4d.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnest269.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnext101_32x4d.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/xception.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet34_v2.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/mobilenetv2_1.0.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/senet_154.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/vgg16.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnest26.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet34_v1.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet152_v2.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnext101_64x4d.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet101_v2.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/mobilenetv3_small.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/googlenet.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet18_v1.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/mobilenetv2_0.5.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/squeezenet1.1.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/se_resnext50_32x4d.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet50_v2.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/vgg16_bn.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/se_resnext101_64x4d.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/vgg11.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/mobilenet0.25.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/densenet201.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/mobilenet0.5.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/vgg19.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet50_v1d_0.37.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/densenet169.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/darknet53.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/mobilenetv2_0.25.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/vgg11_bn.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnest200.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/inceptionv3.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/densenet121.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnest14.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnest50.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet18_v2.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/vgg13_bn.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet18_v1b_0.89.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/vgg13.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet101_v1d_0.73.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/mobilenet0.75.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/vgg19_bn.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/squeezenet1.0.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/alexnet.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet50_v1d_0.86.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet101_v1d_0.76.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/resnet50_v1d_0.48.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/mobilenetv2_0.75.ipynb",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/onnx/notebooks/classification/densenet161.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/instance/mask_rcnn/benchmark/ompi_bind_DGX1.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/pose/simple_pose/validate.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/pose/simple_pose/coco.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/pose/alpha_pose/coco_dpg.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/pose/alpha_pose/validate.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/pose/alpha_pose/coco.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/classification/imagenet/test.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/.github/workflows/build_docs.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/.github/workflows/gpu_test.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/tools/docker/devel_entrypoint.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/tools/docker/start_jupyter.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/tools/batch/docker/docker_deploy.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-cv/master/tools/batch/docker/gluon_cv_job.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "GluonCV supports Python 3.6 or later. The easiest way to install is via pip.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "GluonCV supports Python 3.6 or later. The easiest way to install is via pip.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "GluonCV is built on top of MXNet and PyTorch. Depending on the individual model implementation(check [model zoo](https://gluon-cv.mxnet.io/model_zoo/index.html) for the complete list), you will need to install either one of the deep learning framework. Of course you can always install both for the best coverage.\n\nPlease also check [installation guide](https://cv.gluon.ai/install.html) for a comprehensive guide to help you choose the right installation command for your environment.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8891363590692888
      ],
      "excerpt": "Supports both PyTorch and MXNet \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8278714771091701
      ],
      "excerpt": "| GAN: <br/>generate visually deceptive images | <a href=\"https://github.com/dmlc/gluon-cv/tree/master/scripts/gan\"><img src=\"https://github.com/dmlc/gluon-cv/raw/master/scripts/gan/wgan/fake_samples_400000.png\" alt=\"lsun\" height=\"200\"/></a> | <a href=\"https://github.com/dmlc/gluon-cv/tree/master/scripts/gan/wgan\">WGAN</a>, <a href=\"https://github.com/dmlc/gluon-cv/tree/master/scripts/gan/cycle_gan\">CycleGAN</a>, <a href=\"https://github.com/dmlc/gluon-cv/tree/master/scripts/gan/stylegan\">StyleGAN</a>| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9739333930886876,
        0.9948772453318471
      ],
      "excerpt": "The following commands install the stable version of GluonCV and MXNet: \npip install gluoncv --upgrade \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9805642849615029
      ],
      "excerpt": "pip install -U --pre mxnet -f https://dist.mxnet.io/python/mkl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.984901879299297
      ],
      "excerpt": "pip install -U --pre mxnet -f https://dist.mxnet.io/python/cu102mkl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9489961212994228,
        0.990191463246806
      ],
      "excerpt": "You may get access to latest features and bug fixes with the following commands which install the nightly build of GluonCV and MXNet: \npip install gluoncv --pre --upgrade \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9805642849615029
      ],
      "excerpt": "pip install -U --pre mxnet -f https://dist.mxnet.io/python/mkl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.984901879299297
      ],
      "excerpt": "pip install -U --pre mxnet -f https://dist.mxnet.io/python/cu102mkl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9801688739340758,
        0.9948772453318471
      ],
      "excerpt": "The following commands install the stable version of GluonCV and PyTorch: \npip install gluoncv --upgrade \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9807805638605454
      ],
      "excerpt": "pip install torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9699902762191287
      ],
      "excerpt": "pip install torch==1.6.0 torchvision==0.7.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9508014302000226,
        0.990191463246806
      ],
      "excerpt": "You may get access to latest features and bug fixes with the following commands which install the nightly build of GluonCV: \npip install gluoncv --pre --upgrade \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9850233711853444
      ],
      "excerpt": "pip install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9823883636962483
      ],
      "excerpt": "pip install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8677013028405017
      ],
      "excerpt": "| Depth Prediction: <br/>predict depth map <br/> from images. | <a href=\"https://gluon-cv.mxnet.io/model_zoo/depth.html\"><img src=\"docs/_static/depth.png\" alt=\"depth\" height=\"200\"/></a> | <a href=\"https://gluon-cv.mxnet.io/model_zoo/depth.html#kitti-dataset\">Monodepth2</a>| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8014795698860872
      ],
      "excerpt": "For getting started quickly, refer to notebook runnable examples at Examples. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dmlc/gluon-cv/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "C++",
      "Shell",
      "Cython",
      "CMake",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Gluon CV Toolkit",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "gluon-cv",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dmlc",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dmlc/gluon-cv/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "zhreshold",
        "body": "# Highlights \r\nGluonCV 0.10.0 release features a new `Auto Module` designed to bootstrap training tasks with less code and effort:\r\n\r\n\r\n- simpler and better custom dataset loading experience with pandas DataFrame visualization. Comparing with obsolete code based dataset composition, it allows you to load arbitrary datasets faster and more reliable.\r\n\r\n- one liner `fit` function with configuration file support(yaml configuration file)\r\n\r\n- built-in HPO support, for effortless tuning of hyper-parameters\r\n\r\n# gluoncv.auto\r\n\r\nThis release includes a new module called `gluoncv.auto`, with `gluoncv.auto` you can access many high-level APIs such as `data`, `estimators` and `tasks`.\r\n\r\n### gluoncv.auto.data\r\n\r\n`auto.data` module is designed to load arbitrary web datasets you find on the internet, such as Kaggle competition datasets.\r\nYou may refer to this [tutorial](https://cv.gluon.ai/build/examples_auto_module/demo_auto_data.html) or check out the fully compatible [d8 dataset](http://preview.d2l.ai/d8/main/image_classification/getting_started.html) for loading custom datasets.\r\n\r\n##### Loading data:\r\n<img src=\"https://user-images.githubusercontent.com/3307514/109356038-02c50c00-7835-11eb-83ba-2dfc070b4ef6.png\" height=\"150\">\r\n\r\nThe dataset has internal `DataFrame` storage for easier access and analysis\r\n\r\n<img src=\"https://user-images.githubusercontent.com/3307514/109357043-6bf94f00-7836-11eb-9db1-7628218fba43.png\" height=\"300\">\r\n\r\n\r\n##### Visualization:\r\n\r\n<img src=\"https://user-images.githubusercontent.com/3307514/109357126-8cc1a480-7836-11eb-88f5-071c9a817aac.png\" height=\"150\">\r\n\r\nsimilar for object detection:\r\n\r\n<img src=\"https://user-images.githubusercontent.com/3307514/109356385-7b2bcd00-7835-11eb-86e3-8fabc6557464.png\" height=\"200\">\r\n\r\n### gluoncv.auto.estimators\r\n\r\nIn this release, we packed the following high-level estimators for training and predicting images for image classification and object detection.\r\n\r\n- gluoncv.auto.estimators.ImageClassificationEstimator\r\n- gluoncv.auto.estimators.SSDEstimator\r\n- gluoncv.auto.estimators.CenterNetEstimator\r\n- gluoncv.auto.estimators.FasterRCNNEstimator\r\n- gluoncv.auto.estimators.YOLOv3Estimator\r\n\r\n#### Highlighted usages\r\n\r\n- `fit` function:\r\n\r\n![image](https://user-images.githubusercontent.com/3307514/109357783-a57e8a00-7837-11eb-9c99-f2a2fac99112.png)\r\n\r\n- `predict`, `predict_proba`(for image classification), `predict_feature`(for image classification)\r\n\r\n<img src=\"https://user-images.githubusercontent.com/3307514/109358005-ed9dac80-7837-11eb-811c-7690981066ab.png\" height=\"40\">\r\n<img src=\"https://user-images.githubusercontent.com/3307514/109358236-4a00cc00-7838-11eb-8256-2a6e6f51fab8.png\" height=\"150\">\r\n\r\n- `save` and `load`.\r\n\r\nYou may visit the tutorial website for more detailed [examples](https://cv.gluon.ai/build/examples_auto_module/train_image_classifier_basic.html).\r\n\r\n### gluoncv.auto.tasks\r\n\r\nIn this release, the following auto tasks are supported and have been massively tested on many datasets to ensure HPO performance:\r\n\r\n- gluoncv.auto.tasks.ImageClassification\r\n- gluoncv.auto.tasks.ObjectDetection\r\n\r\nComparing with pure algorithm-based estimators, the auto tasks provide identical APIs and functionalities but allow you to `fit` with hyper-parameter optimization(HPO) with specified `num_trials` and `time_limit`. For object detection, it allows multiple algorithms(e.g., SSDEstimator and FasterRCNNEstimator) to be tuned as a categorical search space.\r\n\r\nThe tutorial is available [here](https://cv.gluon.ai/build/examples_auto_module/demo_auto_detection.html)\r\n\r\n# Bug fixes and improvements\r\n\r\n- Improved training speed for mask-rcnn script (#1595, #1609)\r\n- Fix an issue in classification dataset (#1599)\r\n- Fix a batch-size issue for mask-rcnn validation during training (#1594)\r\n- Fix an os directory issue for model zoo folder (#1591)\r\n- Improved CI stability (#1581)",
        "dateCreated": "2021-03-02T18:37:34Z",
        "datePublished": "2021-03-09T00:20:06Z",
        "html_url": "https://github.com/dmlc/gluon-cv/releases/tag/v0.10.0",
        "name": "GluonCV 0.10.0 release",
        "tag_name": "v0.10.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-cv/tarball/v0.10.0",
        "url": "https://api.github.com/repos/dmlc/gluon-cv/releases/38084814",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-cv/zipball/v0.10.0"
      },
      {
        "authorType": "User",
        "author_name": "bryanyzhu",
        "body": "## Highlights\r\n**GluonCV v0.9.0 starts to support PyTorch!**\r\n\r\n## PyTorch Support\r\nWe want to make our toolkit agnostic to deep learning frameworks so that it is available for everyone. From this release, we start to support PyTorch. All PyTorch code and models are under `torch` folder inside `gluoncv`, arranged in the same hierarchy as before: `model`, `data`, `nn` and `utils`. `model` folder contains our model zoo with model definitions, `data` folder contains dataset definition and dataloader, `nn` defines new operators and `utils` provide utility functions to help model training, evaluation and visualization. \r\n\r\nTo get started, you can find [installation instructions](https://cv.gluon.ai/install.html), [model zoo](https://cv.gluon.ai/model_zoo/index.html) and [tutorials](https://cv.gluon.ai/tutorials_torch/index.html) on our website. In order to make our toolkit easier to use and customize, we provide model definitions separately for each method without extreme abstraction and modularization. In this manner, you can play with each model without jumping across multiple files, and you can modify individual model implementation without affecting other models. At the same time, we adopt `yaml` for easier configuration. We thrive to make our toolkit more user friendly for students and researchers.\r\n\r\n\r\n## Video Action Recognition PyTorch Model Zoo\r\nWe have **46** PyTorch models for video action recognition, with better I3D models, more recent TPN family, faster training (DDP support and multi-grid) and K700 pretrained weights. Finetuning and feature extraction can never be easier.\r\n\r\nDetails of our model zoo can be seen at [here](https://cv.gluon.ai/model_zoo/action_recognition.html). In terms of models, we cover TSN, I3D, I3D_slow, R2+1D, Non-local, CSN, TSN and TPN. In terms of datasets, we cover Kinetics400, Kinetics700 and Something-something-v2. All of our models have similar or better performance compared to numbers reported in original paper.\r\n\r\nWe provide several tutorials to get you started, including [how to make predictions using a pretrained model](https://cv.gluon.ai/build/examples_torch_action_recognition/demo_i3d_kinetics400.html), [how to extract video features from a pretrained model](https://cv.gluon.ai/build/examples_torch_action_recognition/extract_feat.html), [how to finetune a model on your dataset](https://cv.gluon.ai/build/examples_torch_action_recognition/finetune_custom.html), [how to measure a model's flops/speed](https://cv.gluon.ai/build/examples_torch_action_recognition/speed.html), and [how to use our DDP framework](https://cv.gluon.ai/build/examples_torch_action_recognition/ddp_pytorch.html).\r\n\r\nSince video models are slow to train (due to slow IO and large model), we also support distributed dataparallel (DDP) training and [multi-grid training](https://arxiv.org/abs/1912.00998). DDP can provide 2x speed up and multi-grid training can provide 3-4x speed up. Combining these two techniques can significantly shorten the training process. In addition, both techniques are provided as helper functions. You can easily add your model definitions to GluonCV (a single python file like [this](https://github.com/dmlc/gluon-cv/blob/master/gluoncv/torch/model_zoo/action_recognition/i3d_resnet.py)) and enjoy the speed brought by our framework. More details can be read in this [tutorial](https://cv.gluon.ai/build/examples_torch_action_recognition/ddp_pytorch.html).\r\n\r\n## Bug fixes and Improvements\r\n\r\n- Refactored table in csv form. (#1465 )\r\n- Added DeepLab ResNeSt200 pretrained weights (#1456 )\r\n- StyleGAN training instructions (#1446 )\r\n- More settings for Monodepth2 and bug fix (#1459 #1472 )\r\n- Fix RCNN target generator (#1508)\r\n- Revise DANet (#1507 )\r\n- New docker image is added which is ready for GluonCV applications and developments(#1474)\r\n\r\n## Acknowledgement\r\nSpecial thanks to @Arthurlxy @ECHO960 @zhreshold @yinweisu for their support in this release. Thanks to @coocoo90 for contributing the CSN and R2+1D models. And thanks to other contributors for the bug fixes and improvements.\r\n\r\n",
        "dateCreated": "2020-12-02T22:21:56Z",
        "datePublished": "2020-12-02T22:25:01Z",
        "html_url": "https://github.com/dmlc/gluon-cv/releases/tag/v0.9.0",
        "name": "GluonCV 0.9.0 Release",
        "tag_name": "v0.9.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-cv/tarball/v0.9.0",
        "url": "https://api.github.com/repos/dmlc/gluon-cv/releases/33312000",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-cv/zipball/v0.9.0"
      },
      {
        "authorType": "User",
        "author_name": "bryanyzhu",
        "body": "# GluonCV 0.8.0 Release Note\r\n\r\n## Highlights\r\n**GluonCV v0.8.0 features the popular depth estimation model Monodepth2, semantic segmentation models (DANet and FastSCNN),  StyleGAN, and multiple usability improvements.**\r\n\r\n## Monodepth2 (thanks @KuangHaofei )\r\n\r\nWe provide GluonCV implementation of [Monodepth2](https://arxiv.org/abs/1806.01260) and the results are fully reproducible. To try out on your own images, please see our [demo tutorial](https://gluon-cv.mxnet.io/build/examples_depth/demo_monodepth2.html). To train a Monodepth2 model on your own dataset, please see our [dive deep tutorial](https://gluon-cv.mxnet.io/build/examples_depth/train_monodepth2.html).\r\n\r\nFollowing table shows its performance on the KITTI dataset.\r\n| Name | Modality | Resolution | Abs. Rel. Error | delta < 1.25 | Hashtag |\r\n| -- | -- | -- | -- | -- | -- |\r\n| monodepth2_resnet18_kitti_stereo_640x192\u00a01 | Stereo | 640x192 | 0.114 | 0.856 | 92871317 |\r\n\r\n![](https://github.com/nianticlabs/monodepth2/raw/master/assets/teaser.gif)\r\n\r\n## More Semantic Segmentation Models (thanks @xdeng7 and @ytian8 )\r\n\r\nWe include two new semantic segmentation models in this release, one is [DANet](https://arxiv.org/abs/1809.02983), the other is [FastSCNN](https://arxiv.org/abs/1902.04502).\r\n\r\nFollowing table shows their performance on the Cityscapes validation set.\r\n| Model               | Pre-Trained Dataset      | Dataset | pixAcc | mIoU | \r\n|---------------------------|--------|-----|--------|-------|\r\n| danet_resnet50_citys | ImageNet  | Cityscapes |  96.3  | 78.5  |\r\n| danet_resnet101_citys | ImageNet  | Cityscapes |  96.5  | 80.1  |\r\n| fastscnn_citys | -  | Cityscapes |  95.1  | 72.3  |\r\n\r\nOur FastSCNN is an improved version from a [recent paper](https://arxiv.org/abs/2004.14960) using semi-supervised learning. To our best knowledge, `72.3` mIoU is the highest-scored implementation of FastSCNN and one of the best real-time semantic segmentation models.\r\n\r\n## StyleGAN (thanks @xdeng7 )\r\n\r\n![](https://github.com/dmlc/gluon-cv/blob/master/scripts/gan/stylegan/sample.jpg?raw=true)\r\n\r\nA GluonCV implementation of [StyleGAN](https://arxiv.org/abs/1812.04948) \"A Style-Based Generator Architecture for Generative Adversarial Networks\": https://github.com/dmlc/gluon-cv/tree/master/scripts/gan/stylegan\r\n\r\n## Bug fixes and Improvements\r\n\r\n- We now officially deprecated python2 support, the minimum required python 3 version is 3.6. (#1399)\r\n- Fixed Faster-RCNN training script (#1249)\r\n- Allow SRGAN to be hybridized (#1281) \r\n- Fix market1501 dataset (#1227)\r\n- Added Visdrone dataset (#1267)\r\n- Improved video action recognition task's `train.py` (#1339)\r\n- Added jetson object detection tutorial (#1346)\r\n- Improved guide for contributing new algorithms to GluonCV (#1354)\r\n- Fixed amp parameter that required in class ForwardBackwardTask (#1404)",
        "dateCreated": "2020-08-10T00:44:22Z",
        "datePublished": "2020-08-10T17:38:10Z",
        "html_url": "https://github.com/dmlc/gluon-cv/releases/tag/v0.8.0",
        "name": "GluonCV 0.8.0 Release",
        "tag_name": "v0.8.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-cv/tarball/v0.8.0",
        "url": "https://api.github.com/repos/dmlc/gluon-cv/releases/28719087",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-cv/zipball/v0.8.0"
      },
      {
        "authorType": "User",
        "author_name": "Jerryzcn",
        "body": "# Highlights\r\n\r\n**GluonCV 0.7 added our latest backbone network: [ResNeSt](https://arxiv.org/pdf/2004.08955.pdf), and the derived models for semantic segmentation and object detection. We achieve significant performance improvement on all three tasks.**\r\n\r\n## Image Classification\r\n\r\nGluonCV now provides the state-of-art image classification backbones that can be used by various downstream tasks. Our ResNeSt outperforms EfficientNet in accuracy-speed trade-off as shown in the following figures. You can now swap in our new ResNeSt in your research or product to get immediate performance improvement. Checkout the detail in our paper:  [ResNeSt: Split Attention Network](https://arxiv.org/pdf/2004.08955.pdf)\r\n\r\nHere is a comparison between ResNeSt and EfficientNet. The average latency is computed using a single V100 on a p3dn.24xlarge machine with a batch size of 16.\r\n\r\n<img width=\"514\" alt=\"resnest_vs_efficientnet\" src=\"https://user-images.githubusercontent.com/4907789/79623404-f6c6d480-80d0-11ea-84e5-fbbf4c1558a2.png\">\r\n\r\nModel | input size | top-1 acc (%) | avg latency (ms) | \u00a0\r\n-- | -- | -- | -- | --\r\nSENet_154 | 224x224 | 81.26 | 5.07 | previous\r\nResNeSt50 | 224x224 | 81.13 | 1.78 | v0.7\r\nResNeSt101 | 256x256 | 82.83 | 3.43 | v0.7\r\nResNeSt200 | 320x320 | 83.90 | 9.49 | v0.7\r\nResNeSt269 | 416x416 | **84.54** | 19.50 | v0.7\r\n\r\n## Object Detection\r\n\r\nWe add two new ResNeSt based Faster R-CNN model. Noted that our model is trained using 2x learning rate schedule instead of the 1x schedule used in our paper. Our two new models are 2-4% higher on COCO mAP than our previous best model \u201cfaster_rcnn_fpn_resnet101_v1d_coco\u201d.  Notebly, our ResNeSt-50 based model has a 4.1% higher mAP than our previous ResNet-101 based model.\r\n\r\nModel | Backbone | mAP | \u00a0\r\n-- | -- | -- | --\r\nFaster R-CNN | ResNet-101 | 40.8 | previous\r\nFaster R-CNN | ResNeSt-50 | 42.7 | v0.7\r\nFaster R-CNN | ResNeSt-101 | **44.9** | v0.7\r\n\r\n## Semantic Segmentation\r\n\r\nWe add ResNeSt-50 and ResNeSt-101 based DeepLabV3 for semantic segmentation task on ADE20K dataset. Our new models are 1-2.8% higher than our previous best. Similar to our detection result, ResNeSt-50 performs better than ResNet-101 based model. DeepLabV3 with ResNeSt-101 backbone achieves **a new state-of-the-art of 46.9 mIoU** on ADE20K validation set, which outperform previous best by more than 1%.\r\n\r\nModel | Backbone | pixel Accuracy | mIoU | \u00a0\r\n-- | -- | -- | -- | --\r\nDeepLabV3 | ResNet-101 | 81.1 | 44.1 | previous\r\nDeepLabV3 | ResNeSt-50 | 81.2 | 45.1 | v0.7\r\nDeepLabV3 | ResNeSt-101 | 82.1 | **46.9** | v0.7\r\n\r\n\r\n## Bug fixes and Improvements\r\n\r\n* Instructions for achieving 25.7 min Mask R-CNN training.\r\n* Fix R-CNNs export\r\n\r\n",
        "dateCreated": "2020-04-17T22:49:11Z",
        "datePublished": "2020-04-22T00:16:39Z",
        "html_url": "https://github.com/dmlc/gluon-cv/releases/tag/v0.7.0",
        "name": "GluonCV 0.7.0 Release",
        "tag_name": "v0.7.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-cv/tarball/v0.7.0",
        "url": "https://api.github.com/repos/dmlc/gluon-cv/releases/25699183",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-cv/zipball/v0.7.0"
      },
      {
        "authorType": "User",
        "author_name": "bryanyzhu",
        "body": "# GluonCV 0.6.0 Release\r\n\r\n## Highlights\r\n**GluonCV v0.6.0 added more video classification models, added pose estimation models that are suitable for mobile inference, added quantized models for video classification and pose estimation, and we also included multiple usability and code improvements.**\r\n\r\n## More video action recognition models\r\n\r\nhttps://gluon-cv.mxnet.io/model_zoo/action_recognition.html\r\n\r\nWe now provide state-of-the-art video classification networks, such as I3D, I3D-Nonlocal and SlowFast. We have a complete model zoo over several widely adopted video datasets. We provide a general video [dataloader](https://github.com/dmlc/gluon-cv/blob/master/gluoncv/data/video_custom/classification.py) (which can handle both frame format and raw video format). Users can do training, fine-tuning, prediction and feature extraction without writing complicate code. Just prepare a text file containing the video information is enough.\r\n\r\nBelow is the table of new models included in this release.\r\n\r\n\r\nName | Pretrained | Segments | Clip Length | Top-1 | Hashtag | \r\n-- | -- | -- | -- | -- | -- | \r\ninceptionv1_kinetics400 | ImageNet | 7 | 1 | 69.1 | 6dcdafb1 | \r\ninceptionv3_kinetics400 | ImageNet | 7 | 1 | 72.5 | 8a4a6946 | \r\nresnet18_v1b_kinetics400 | ImageNet | 7 | 1 | 65.5 | 46d5a985 |\r\nresnet34_v1b_kinetics400\u00a0 | ImageNet | 7 | 1 | 69.1 | 8a8d0d8d | \r\nresnet50_v1b_kinetics400\u00a0 | ImageNet | 7 | 1 | 69.9 | cc757e5c | \r\nresnet101_v1b_kinetics400\u00a0 | ImageNet | 7 | 1 | 71.3 | 5bb6098e | \r\nresnet152_v1b_kinetics400\u00a0 | ImageNet | 7 | 1 | 71.5 | 9bc70c66 | \r\ni3d_inceptionv1_kinetics400\u00a0 | ImageNet | 1 | 32 (64/2) | 71.8 | 81e0be10 | \r\ni3d_inceptionv3_kinetics400\u00a0 | ImageNet | 1 | 32 (64/2) | 73.6 | f14f8a99 | \r\ni3d_resnet50_v1_kinetics400\u00a0 | ImageNet | 1 | 32 (64/2) | 74.0 | 568a722e |\r\ni3d_resnet101_v1_kinetics400\u00a0 | ImageNet | 1 | 32 (64/2) | 75.1 | 6b69f655 | \r\ni3d_nl5_resnet50_v1_kinetics400\u00a0 | ImageNet | 1 | 32 (64/2) | 75.2 | 3c0e47ea |\r\ni3d_nl10_resnet50_v1_kinetics400\u00a0 | ImageNet | 1 | 32 (64/2) | 75.3 | bfb58c41 | \r\ni3d_nl5_resnet101_v1_kinetics400\u00a0 | ImageNet | 1 | 32 (64/2) | 76.0 | fbfc1d30 |\r\ni3d_nl10_resnet101_v1_kinetics400\u00a0 | ImageNet | 1 | 32 (64/2) | 76.1 | 59186c31 | \r\nslowfast_4x16_resnet50_kinetics400\u00a0 | ImageNet | 1 | 36 (64/1) | 75.3 | 9d650f51 | \r\nslowfast_8x8_resnet50_kinetics400\u00a0 | ImageNet | 1 | 40 (64/1) | 76.6 | d6b25339 | \r\nslowfast_8x8_resnet101_kinetics400\u00a0 | ImageNet | 1 | 40 (64/1) | 77.2 | fbde1a7c | \r\nresnet50_v1b_ucf101\u00a0 | ImageNet | 3 | 1 | 83.7 | d728ecc7 |\r\ni3d_resnet50_v1_ucf101 | ImageNet | 1 | 32 (64/2) | 83.9 | 7afc7286 |\r\ni3d_resnet50_v1_ucf101\u00a0 | Kinetics400 | 1 | 32 (64/2) | 95.4 | 760d0981 |\r\nresnet50_v1b_hmdb51\u00a0 | ImageNet | 3 | 1 | 55.2 | 682591e2 |\r\ni3d_resnet50_v1_hmdb51\u00a0 | ImageNet | 1 | 32 (64/2) | 48.5 | 0d0ad559 |\r\ni3d_resnet50_v1_hmdb51\u00a0 | Kinetics400 | 1 | 32 (64/2) | 70.9 | 2ec6bf01 |\r\nresnet50_v1b_sthsthv2\u00a0 | ImageNet | 8 | 1 | 35.5 | 80ee0c6b |\r\ni3d_resnet50_v1_sthsthv2\u00a0 | ImageNet | 1 | 16 (32/2) | 50.6 | 01961e4c |\r\n\r\n\r\nWe include tutorials for how to fine-tune a pre-trained model on users' own dataset. \r\nhttps://gluon-cv.mxnet.io/build/examples_action_recognition/finetune_custom.html\r\n\r\nWe include tutorials for introducing a new efficient video reader, Decord.\r\nhttps://gluon-cv.mxnet.io/build/examples_action_recognition/decord_loader.html\r\n\r\nWe include tutorials for how to extract features from a pre-trained model.\r\nhttps://gluon-cv.mxnet.io/build/examples_action_recognition/feat_custom.html\r\n\r\nWe include tutorials for how to make predictions from a pre-trained model.\r\nhttps://gluon-cv.mxnet.io/build/examples_action_recognition/demo_custom.html\r\n\r\nWe include tutorials for how to perform distributed training on deep video models.\r\nhttps://gluon-cv.mxnet.io/build/examples_distributed/distributed_slowfast.html\r\n\r\nWe include tutorials for how to prepare HMDB51 and Something-something-v2 dataset.\r\nhttps://gluon-cv.mxnet.io/build/examples_datasets/hmdb51.html\r\nhttps://gluon-cv.mxnet.io/build/examples_datasets/somethingsomethingv2.html\r\n\r\nWe will provide Kinetics600 and Kinetics700 pre-trained models in the next release, please stay tuned.\r\n\r\n## Mobile pose estimation models\r\n\r\nhttps://gluon-cv.mxnet.io/model_zoo/pose.html#mobile-pose-models\r\n\r\n|Model | OKS AP | OKS AP (with flip) | Hashtag | \r\n|-- | -- | -- | -- | \r\n|mobile_pose_resnet18_v1b\u00a0 | 66.2/89.2/74.3 | 67.9/90.3/75.7 | dd6644eb |\r\n|mobile_pose_resnet50_v1b\u00a0 | 71.1/91.3/78.7 | 72.4/92.3/79.8 | ec8809df |\r\n|mobile_pose_mobilenet1.0\u00a0 | 64.1/88.1/71.2 | 65.7/89.2/73.4 | b399bac7 |\r\n|mobile_pose_mobilenetv2_1.0\u00a0 | 63.7/88.1/71.0 | 65.0/89.2/72.3 | 4acdc130 | \r\n|mobile_pose_mobilenetv3_large\u00a0 | 63.7/88.9/70.8 | 64.5/89.0/72.0 | 1ca004dc |\r\n|mobile_pose_mobilenetv3_small\u00a0 | 54.3/83.7/59.4 | 55.6/84.7/61.7 | b1b148a9 | \r\n\r\nBy replacing the backbone network, and use pixel shuffle layer instead of deconvolution, we can have models that are very fast. These models are suitable for edge device applications, tutorials on deployment will come soon.\r\n\r\n## More Int8 quantized models\r\n\r\nhttps://gluon-cv.mxnet.io/build/examples_deployment/int8_inference.html\r\nBelow CPU performance is benchmarked on AWS EC2 C5.12xlarge instance with 24 physical cores. \r\nNote that you will need nightly build of MXNet to properly use these new features.\r\n\r\n![](https://user-images.githubusercontent.com/34727741/67351790-ecdc7280-f580-11e9-8b44-1b4548cb6031.png)\r\n\r\nModel | Dataset | Batch Size | Speedup (INT8/FP32) | FP32 Accuracy | INT8 Accuracy\r\n-- | -- | -- | -- | -- | --\r\nsimple_pose_resnet18_v1b | COCO Keypoint | 128 | 2.55 | 66.3 | 65.9\r\nsimple_pose_resnet50_v1b | COCO Keypoint | 128 | 3.50 | 71.0 | 70.6\r\nsimple_pose_resnet50_v1d | COCO Keypoint | 128 | 5.89 | 71.6 | 71.4\r\nsimple_pose_resnet101_v1b | COCO Keypoint | 128 | 4.07 | 72.4 | 72.2\r\nsimple_pose_resnet101_v1d | COCO Keypoint | 128 | 5.97 | 73.0 | 72.7\r\nvgg16_ucf101 | UCF101 | 64 | 4.46 | 81.86 | 81.41\r\ninceptionv3_ucf101 | UCF101 | 64 | 5.16 | 86.92 | 86.55\r\nresnet18_v1b_kinetics400 | Kinetics400 | 64 | 5.24 | 63.29 | 63.14\r\nresnet50_v1b_kinetics400 | Kinetics400 | 64 | 6.78 | 68.08 | 68.15\r\ninceptionv3_kinetics400 | Kinetics400 | 64 | 5.29 | 67.93 | 67.92\r\n\r\nFor pose-estimation models, the accuracy metric is OKS AP w/o flip. Quantized 2D video action recognition models are calibrated with num-segments=3 (7 is for ResNet-based models).\r\n\r\n## Bug fixes and Improvements\r\n\r\n- Performance of PSPNet using ResNet101 as backbone on Cityscapes (semantic segmentation) is improved from mIoU 77.1% to 79.9%, higher than the number reported in original paper.\r\n- We will deprecate Python2 support in the next release.\r\n",
        "dateCreated": "2020-01-13T18:49:23Z",
        "datePublished": "2020-01-13T23:48:21Z",
        "html_url": "https://github.com/dmlc/gluon-cv/releases/tag/v0.6.0",
        "name": "GluonCV 0.6.0 Release",
        "tag_name": "v0.6.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-cv/tarball/v0.6.0",
        "url": "https://api.github.com/repos/dmlc/gluon-cv/releases/21941761",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-cv/zipball/v0.6.0"
      },
      {
        "authorType": "User",
        "author_name": "zhreshold",
        "body": "# GluonCV 0.5.0 Release\r\n\r\n## Highlights\r\n**GluonCV v0.5.0 added Video Action Recognition models, added AlphaPose, added MobileNetV3, added VPLR semantic segmentation models for driving scenes, added more Int8 quantized models for deployment, and we also included multiple usability improvements.**\r\n\r\n## New Models released in 0.5\r\n\r\n| Model                     | Metric | 0.5 |\r\n|---------------------------|--------|-----|\r\n| vgg16_ucf101 |  UCF101 Top-1  |   83.4  |\r\n| inceptionv3_ucf101 |  UCF101 Top-1   |   88.1  |\r\n| inceptionv3_kinetics400 |    Kinetics400 Top-1    |  72.5   |\r\n| alpha_pose_resnet101_v1b_coco | OKS AP (with flip) | 76.7/92.6/82.9 |\r\n| MobileNetV3_Large\u00a0 | ImageNet  Top-1 | 75.32 |\r\n| MobileNetV3_Small\u00a0 | ImageNet  Top-1 | 67.72 |\r\n| deeplab_v3b_plus_wideresnet_citys | Cityscapes mIoU | 83.5 |\r\n\r\n\r\n## New application: Video Action Recognition\r\n\r\nhttps://gluon-cv.mxnet.io/model_zoo/action_recognition.html\r\n\r\n![](https://raw.githubusercontent.com/bryanyzhu/tiny-ucf101/master/action_basketball_anno.gif)\r\n\r\nVideo Action Recognition in GluonCV is a complete application set, including model definition, training scripts, useful loss and metric functions. We also included some pre-trained models and usage tutorials.\r\n\r\n| Model               | Pre-Trained Dataset      | Clip Length | Num of Segments |  Metric | Dataset | Accuracy |\r\n|---------------------------|--------|-----|--------|-----|---|-----|\r\n| vgg16_ucf101 | ImageNet  | 1 | 1 |  Top-1  |  UCF101 |  81.5  |\r\n| vgg16_ucf101 | ImageNet  | 1 | 3 |  Top-1  | UCF101 |  83.4  |\r\n| inceptionv3_ucf101 | ImageNet  | 1 | 1 |  Top-1  | UCF101 |  85.6  |\r\n| inceptionv3_ucf101 | ImageNet  | 1 | 3 |  Top-1  |   UCF101 | 88.1  |\r\n| inceptionv3_kinetics400 |   ImageNet | 1 | 3 |  Top-1   | Kinetics400 | 72.5   |\r\n\r\nThe tutorial for how to prepare UCF101 and Kinetics400 dataset: https://gluon-cv.mxnet.io/build/examples_datasets/ucf101.html and https://gluon-cv.mxnet.io/build/examples_datasets/kinetics400.html .\r\n\r\nThe demo for using the pre-trained model to predict human actions: https://gluon-cv.mxnet.io/build/examples_action_recognition/demo_ucf101.html. \r\n\r\nThe tutorial for how to train your own action recognition model: https://gluon-cv.mxnet.io/build/examples_action_recognition/dive_deep_ucf101.html.\r\n\r\nMore state-of-the-art models (I3D, SlowFast, etc.) are coming in the next release. Stay tuned. \r\n\r\n## New model: AlphaPose \r\n\r\nhttps://gluon-cv.mxnet.io/model_zoo/pose.html#alphapose\r\n\r\n![](https://raw.githubusercontent.com/MVIG-SJTU/AlphaPose/master/doc/pose.gif)\r\n\r\n| Model                     | Dataset |OKS AP | OKS AP (with flip) |\r\n|---------------------------|---|----|-------|\r\n| alpha_pose_resnet101_v1b_coco | COCO Keypoint  | 74.2/91.6/80.7 | 76.7/92.6/82.9 |\r\n\r\nThe demo for using the pre-trained AlphaPose model: https://gluon-cv.mxnet.io/build/examples_pose/demo_alpha_pose.html. \r\n\r\n## New model: MobileNetV3 \r\n\r\nhttps://gluon-cv.mxnet.io/model_zoo/classification.html#mobilenet\r\n\r\n![](https://raw.githubusercontent.com/bryanyzhu/tiny-ucf101/master/mobilenetv3.jpg)\r\n\r\n| Model                     | Dataset | Top-1 | Top-5 | Top-1 (original paper) |\r\n|---------------------|------|-------|-------|--------|\r\n| MobileNetV3_Large\u00a0 | ImageNet | 75.3 | 92.3 | 75.2 |\r\n| MobileNetV3_Small\u00a0 | ImageNet | 67.7 | 87.5 | 67.4 |\r\n\r\n\r\n## New model: Semantic Segmentation VPLR\r\n\r\nhttps://gluon-cv.mxnet.io/model_zoo/segmentation.html#cityscapes-dataset\r\n\r\n![](https://raw.githubusercontent.com/bryanyzhu/tiny-ucf101/master/vplr_lossy2.gif)\r\n\r\n| Model               | Pre-Trained Dataset      | Dataset | mIoU | iIoU | \r\n|---------------------------|--------|-----|--------|-------|\r\n| deeplab_v3b_plus_wideresnet_citys | ImageNet, Mapillary Vista  | Cityscapes |  83.5  | 64.4  |\r\n\r\n[Improving Semantic Segmentation via Video Propagation and Label Relaxation](https://arxiv.org/pdf/1812.01593.pdf) ported in GluonCV. State-of-the-art method on several driving semantic segmentation benchmarks (Cityscapes, CamVid and KITTI), and generalizes well to other scenes. \r\n\r\n## New model: More Int8 quantized models\r\n\r\nhttps://gluon-cv.mxnet.io/build/examples_deployment/int8_inference.html\r\nBelow CPU performance is benchmarked on AWS EC2 C5.12xlarge instance with 24 physical cores. \r\n**Note that you will need nightly build of MXNet to properly use these new features.**\r\n\r\n![](https://user-images.githubusercontent.com/34727741/64021961-a9105280-cb67-11e9-989e-76a29e58530d.png)\r\n\r\nModel | Dataset | Batch Size | C5.12xlarge FP32 | C5.12xlarge INT8 | Speedup | FP32 Acc | INT8 Acc\r\n-- | -- | -- | -- | -- | -- | -- | --\r\nFCN_resnet101 | VOC | 1 | 5.46 | 26.33 | 4.82 | 97.97% | 98.00%\r\nPSP_resnet101 | VOC | 1 | 3.96 | 10.63 | 2.68 | 98.46% | 98.45%\r\nDeeplab_resnet101 | VOC | 1 | 4.17 | 13.35 | 3.20 | 98.36% | 98.34%\r\nFCN_resnet101 | COCO | 1 | 5.19 | 26.22 | 5.05 | 91.28% | 90.96%\r\nPSP_resnet101 | COCO | 1 | 3.94 | 10.60 | 2.69 | 91.82% | 91.88%\r\nDeeplab_resnet101 | COCO | 1 | 4.15 | 13.56 | 3.27 | 91.86% | 91.98%\r\n\r\nFor segmentation models, the accuracy metric is pixAcc. Usage of int8 quantized model is identical to standard GluonCV models, simple use suffix `_int8`.\r\n\r\n## Bug fixes and Improvements\r\n\r\n- RCNN added automatic mix precision and horovod integration. Close to 4x improvements in training throughput on 8 V100 GPU.\r\n- RCNN added multi-image per device support.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "dateCreated": "2019-09-06T17:57:01Z",
        "datePublished": "2019-09-10T19:11:35Z",
        "html_url": "https://github.com/dmlc/gluon-cv/releases/tag/v0.5.0",
        "name": "GluonCV 0.5.0 Release",
        "tag_name": "v0.5.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-cv/tarball/v0.5.0",
        "url": "https://api.github.com/repos/dmlc/gluon-cv/releases/19345908",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-cv/zipball/v0.5.0"
      },
      {
        "authorType": "User",
        "author_name": "zhreshold",
        "body": "\r\n# 0.4.0 Release Note\r\n\r\n## Highlights\r\n**GluonCV v0.4 added Pose Estimation models, Int8 quantization for intel CPUs, added FPN Faster/Mask-RCNN, wide se/resnext models, and we also included multiple usability improvements.**\r\n\r\n*We highly suggest to use GluonCV 0.4.0 with MXNet>=1.4.0 to avoid some dependency issues. For some specific tasks you may need MXNet nightly build. See https://gluon-cv.mxnet.io/index.html*\r\n\r\n## New Models released in 0.4\r\n\r\n| Model                     | Metric | 0.4 |\r\n|---------------------------|--------|-----|\r\n| simple_pose_resnet152_v1b |  OKS AP*   |   74.2  |\r\n| simple_pose_resnet50_v1b |    OKS AP*    |  72.2   |\r\n| ResNext50_32x4d           |  ImageNet Top-1  |  79.32   |\r\n| ResNext101_64x4d          |  ImageNet Top-1   |  80.69  |\r\n| SE_ResNext101_32x4d       |  ImageNet Top-1   |  79.95   |\r\n| SE_ResNext101_64x4d       |  ImageNet Top-1  |  81.01   |\r\n| yolo3_mobilenet1.0_coco   |  COCO mAP      |  28.6   |\r\n\r\n\\* Using Ground-Truth person detection results\r\n\r\n## Int8 Quantization with Intel Deep Learning boost\r\n\r\nGluonCV is now integrated with Intel's vector neural network instruction(vnni) to accelerate model inference speed. \r\n**Note that you will need a capable Intel Skylake CPU to see proper speed up ratio.**\r\n\r\nModel | Dataset | Batch Size | C5.18x FP32 | C5.18x INT8 | Speedup | FP32 Acc | INT8 Acc\r\n-- | -- | -- | -- | -- | -- | -- | --\r\nresnet50_v1 | ImageNet | 128 | 122.02 | 276.72 | 2.27 | 77.21%/93.55% | 76.86%/93.46%\r\nmobilenet1.0 | ImageNet | 128 | 375.33 | 1016.39 | 2.71 | 73.28%/91.22% | 72.85%/90.99%\r\nssd_300_vgg16_atrous_voc* | VOC | 224 | 21.55 | 31.47 | 1.46 | 77.4 | 77.46\r\nssd_512_vgg16_atrous_voc* | VOC | 224 | 7.63 | 11.69 | 1.53 | 78.41 | 78.39\r\nssd_512_resnet50_v1_voc* | VOC | 224 | 17.81 | 34.55 | 1.94 | 80.21 | 80.16\r\nssd_512_mobilenet1.0_voc* | VOC | 224 | 31.13 | 48.72 | 1.57 | 75.42 | 75.04\r\n\r\n**\\*nms_thresh=0.45, nms_topk=200**\r\n\r\n![](https://user-images.githubusercontent.com/17897736/54540947-dc08c480-49d3-11e9-9a0d-a97d44f9792c.png)\r\n\r\nUsage of `int8` quantized model is identical to standard GluonCV models, simple use suffix `_int8`.\r\nFor example, use `resnet50_v1_int8` as `int8` quantized version of `resnet50_v1`.\r\n\r\n## Pruned ResNet \r\n\r\nhttps://gluon-cv.mxnet.io/model_zoo/classification.html#pruned-resnet\r\n\r\nPruning channels of convolution layers is an very effective way to reduce model redundency which aims to speed up inference without sacrificing significant accuracy. GluonCV 0.4 has included several pruned resnets from original GluonCV  SoTA ResNets for ImageNet.\r\n\r\n| Model             | Top-1 | Top-5 | Hashtag  | Speedup (to original ResNet) |\r\n|-------------------|-------|-------|----------|------------------------------|\r\n| resnet18_v1b_0.89 | 67.2  | 87.45 | 54f7742b | 2x                           |\r\n| resnet50_v1d_0.86 | 78.02 | 93.82 | a230c33f | 1.68x                        |\r\n|  resnet50_v1d_0.48    | 74.66 | 92.34 | 0d3e69bb | 3.3x  |\r\n|  resnet50_v1d_0.37    | 70.71 | 89.74 | 9982ae49 | 5.01x    |\r\n|  resnet50_v1d_0.11    | 63.22 | 84.79 | 6a25eece | 8.78x      |\r\n|  resnet101_v1d_0.76   | 79.46 | 94.69 | a872796b | 1.8x          |\r\n|   resnet101_v1d_0.73   | 78.89 | 94.48 | 712fccb1 | 2.02x     |\r\n\r\nScripts for pruning resnets will be release in the future.\r\n\r\n## More GANs(thanks @husonchen)\r\n\r\n### SRGAN\r\n\r\n![](https://github.com/dmlc/gluon-cv/blob/master/scripts/gan/srgan/pred.png?raw=true)\r\n\r\nA GluonCV SRGAN of \"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network \": https://github.com/dmlc/gluon-cv/tree/master/scripts/gan/srgan\r\n\r\n### CycleGAN\r\n\r\n![teaser](https://user-images.githubusercontent.com/3307514/54579701-efab2f80-49c1-11e9-8a90-e9170f21dc8a.jpg)\r\n\r\nImage-to-Image translation reproduced in GluonCV: https://github.com/dmlc/gluon-cv/tree/master/scripts/gan/cycle_gan\r\n\r\n## Residual Attention Network(thanks @PistonY)\r\n\r\nGluonCV implementation of https://arxiv.org/abs/1704.06904\r\n\r\n![figure2](https://user-images.githubusercontent.com/3307514/54580045-83c9c680-49c3-11e9-9f44-b2f40d337bb0.png)\r\n\r\n\r\n## New application: Human Pose Estimation\r\n\r\nhttps://gluon-cv.mxnet.io/model_zoo/pose.html\r\n\r\n![sphx_glr_demo_simple_pose_001](https://user-images.githubusercontent.com/3307514/54579196-98a45b00-49bf-11e9-9257-0a91b6240575.png)\r\n\r\nHuman Pose Estimation in GluonCV is a complete application set, including model definition, training scripts, useful loss and metric functions. We also included some pre-trained models and usage tutorials.\r\n\r\n| Model                                            | OKS AP         | OKS AP (with flip) |\r\n|--------------------------------------------------|----------------|--------------------|\r\n| simple_pose_resnet18_v1b                     | 66.3/89.2/73.4 | 68.4/90.3/75.7     |\r\n| simple_pose_resnet18_v1b    | 52.8/83.6/57.9 | 54.5/84.8/60.3     |\r\n| simple_pose_resnet50_v1b                     | 71.0/91.2/78.6 | 72.2/92.2/79.9     | \r\n| simple_pose_resnet50_v1d                   | 71.6/91.3/78.7 | 73.3/92.4/80.8     | \r\n| simple_pose_resnet101_v1b                   | 72.4/92.2/79.8 | 73.7/92.3/81.1     | \r\n| simple_pose_resnet101_v1d                  | 73.0/92.2/80.8 | 74.2/92.4/82.0     | \r\n| simple_pose_resnet152_v1b                   | 72.4/92.1/79.6 | 74.2/92.3/82.1     |\r\n| simple_pose_resnet152_v1d                   | 73.4/92.3/80.7 | 74.6/93.4/82.1     |\r\n| simple_pose_resnet152_v1d  | 74.8/92.3/82.0 | 76.1/92.4/83.2     | 2f544338 |\r\n\r\n\r\n## Feature Pyramid Network for Faster/Mask-RCNN\r\n\r\n| Model | bbox/seg mAP | Caffe bbox/seg |\r\n|--------------------------------------------------|----------------|--------------------|\r\n| faster_rcnn_fpn_resnet50_v1b_coco | **0.384**/- | 0.379 |\r\n| faster_rcnn_fpn_bn_resnet50_v1b_coco | **0.393**/- | - |\r\n| faster_rcnn_fpn_resnet101_v1d_coco | **0.412**/- | 0.398/- |\r\n| maskrcnn_fpn_resnet50_v1b_coco | **0.392**/**0.353** | 0.386/0.345 |\r\n| maskrcnn_fpn_resnet101_v1d_coco | **0.423**/**0.377** | 0.409/0.364 |\r\n\r\n## Bug fixes and Improvements\r\n\r\n- Now all resnet definitions in GluonCV support Synchronized BatchNorm\r\n- Now pretrained object detection models support `reset_class` for reuse partial category knowledge so some task may not need to finetune models anymore: https://gluon-cv.mxnet.io/build/examples_detection/skip_fintune.html#sphx-glr-build-examples-detection-skip-fintune-py\r\n- Fix some dataloader issue(need mxnet >= 1.4.0)\r\n- Fix some segmentation models that won't hybridize \r\n- Fix some detection model random Nan problems (require mxnet latest nightly build, >= 20190315)\r\n- Various other minor bug fixes\r\n\r\n",
        "dateCreated": "2019-03-19T12:18:29Z",
        "datePublished": "2019-03-26T22:07:56Z",
        "html_url": "https://github.com/dmlc/gluon-cv/releases/tag/v0.4.0",
        "name": "GluonCV toolkit v0.4.0",
        "tag_name": "v0.4.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-cv/tarball/v0.4.0",
        "url": "https://api.github.com/repos/dmlc/gluon-cv/releases/15886506",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-cv/zipball/v0.4.0"
      },
      {
        "authorType": "User",
        "author_name": "zhreshold",
        "body": "# 0.3 Release Note\r\n\r\n## Highlights\r\n\r\n### Added 5 new algorithms and updated 38 pre-trained models with improved accuracy\r\n### Compare 7 selected models\r\n\r\n| Model               | Metric                | 0.2    | 0.3    | Reference                                                    |\r\n| ------------------- | --------------------- | ------ | ------ | ------------------------------------------------------------ |\r\n| [ResNet-50](https://gluon-cv.mxnet.io/model_zoo/classification.html#resnet)           | top-1 acc on ImageNet | 77.07% | **79.15%** | 75.3% ([Caffe impl](https://github.com/KaimingHe/deep-residual-networks)) |\r\n| [ResNet-101](https://gluon-cv.mxnet.io/model_zoo/classification.html#resnet)           | top-1 acc on ImageNet | 78.81% | **80.51%** | 76.4% ([Caffe impl](https://github.com/KaimingHe/deep-residual-networks)) |\r\n| [MobileNet 1.0](https://gluon-cv.mxnet.io/model_zoo/classification.html#mobilenet) | top-1 acc on ImageNet |  N/A | **73.28%** | 70.9% ([tensorflow impl)](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md) |\r\n| [Faster-RCNN](https://gluon-cv.mxnet.io/model_zoo/detection.html#id37)         | mAP on COCO           | N/A    | **40.1%**  | 39.6% ([Detectron](https://github.com/facebookresearch/Detectron)) |\r\n| [Yolo-v3](https://gluon-cv.mxnet.io/model_zoo/detection.html#id44)             | mAP on COCO           | N/A    | **37.0%**  | 33.0% ([paper](https://pjreddie.com/media/files/papers/YOLOv3.pdf)) |\r\n| [DeepLab-v3](https://gluon-cv.mxnet.io/model_zoo/segmentation.html#semantic-segmentation)          | mIoU on VOC           | N/A    | **86.7%**  | 85.7% ([paper](https://arxiv.org/abs/1706.05587))            |\r\n| [Mask-RCNN](https://gluon-cv.mxnet.io/model_zoo/segmentation.html#instance-segmentation)   | mask AP on COCO       | N/A    | **33.1%**  | 32.8% ([Detectron](https://github.com/facebookresearch/Detectron)) |\r\n\r\n\r\n### Interactive visualizations for pre-trained models\r\n\r\nFor [image classification](https://gluon-cv.mxnet.io/model_zoo/classification.html):\r\n\r\n<a href=\"https://gluon-cv.mxnet.io/model_zoo/classification.html\"><img src=\"https://user-images.githubusercontent.com/3307514/47051128-ca3aa100-d157-11e8-8b50-08841c8cdf5f.png\" width=\"400px\" /></a>\r\n\r\nand for [object detection](https://gluon-cv.mxnet.io/model_zoo/detection.html)\r\n\r\n<a href=\"https://gluon-cv.mxnet.io/model_zoo/detection.html\"><img src=\"https://user-images.githubusercontent.com/421857/47048450-4d0b2e00-d14f-11e8-9338-bb20bb69655b.png\" width=\"400px\"/></a>\r\n\r\n### Deploy without Python\r\n\r\nAll models are hybridiziable. They can be deployed without Python. See [tutorials](https://github.com/dmlc/gluon-cv/tree/master/scripts/deployment/cpp-inference) to deploy these models in C++.\r\n\r\n\r\n## New Models with Training Scripts\r\n\r\n### DenseNet, DarkNet, SqueezeNet for [image classification](https://gluon-cv.mxnet.io/model_zoo/classification.html#imagenet)\r\n\r\nWe now provide a broader range of model families that are good for out of box usage and various research purposes.\r\n\r\n\r\n### [YoloV3](https://gluon-cv.mxnet.io/model_zoo/detection.html#id44) for object detection\r\n\r\nSignificantly more accurate than original paper. For example, we get 37.0% mAP on CoCo versus the original [paper](https://pjreddie.com/media/files/papers/YOLOv3.pdf)'s 33.0%. The techniques we used will be included in a paper to be released later.\r\n\r\n### [Mask-RCNN](https://gluon-cv.mxnet.io/model_zoo/segmentation.html#instance-segmentation) for instance segmentation\r\n\r\nAccuracy now matches Caffe2 Detectron without FPN, e.g. 38.3% box AP and 33.1% mask AP on COCO with ResNet50.\r\n\r\nFPN support will come in future versions.\r\n\r\n### [DeepLabV3](https://gluon-cv.mxnet.io/model_zoo/segmentation.html#semantic-segmentation) for semantic segmentation.\r\n\r\nSlightly more accurate than original paper. For example, we get 86.7% mIoU on voc versus the  original paper's 85.7%.\r\n\r\n### WGAN\r\n\r\nReproduced [WGAN](https://github.com/dmlc/gluon-cv/tree/master/scripts/gan/wgan) with ResNet\r\n\r\n### Person Re-identification\r\n\r\nProvide a [baseline model](https://github.com/dmlc/gluon-cv/tree/master/scripts/re-id/baseline) which achieved 93.1 best rank1 score on Market1501 dataset.\r\n\r\n## Enhanced Models with Better Accuracy\r\n\r\n### [Faster R-CNN](https://gluon-cv.mxnet.io/model_zoo/detection.html#id37)\r\n\r\n* Improved Pascal VOC model accuracy. mAP improves to 78.3% from previous version's 77.9%. VOC models with 80%+ mAP will be released with the tech paper.\r\n* Added models trained on COCO dataset.\r\n    * Now Resnet50 model achieves 37.0 mAP, out-performs Caffe2 Detectron without FPN (36.5 mAP).\r\n    * Resnet101 model achieves 40.1 mAP, out-performs Caffe2 Detectron with FPN(39.8 mAP)\r\n* FPN support will come in future versions.\r\n\r\n### [ResNet](https://gluon-cv.mxnet.io/model_zoo/classification.html#resnet), [MobileNet](https://gluon-cv.mxnet.io/model_zoo/classification.html#mobilenet), [DarkNet](https://gluon-cv.mxnet.io/model_zoo/classification.html#others), [Inception](https://gluon-cv.mxnet.io/model_zoo/classification.html#others) for image classifcation\r\n\r\n* Significantly improved accuracy for some models. For example, ResNet50_v1b gets 78.3% versus previous version's ResNet50_v1b's 77.07%.\r\n* Added models trained with mixup and distillation. For example, ResNet50_v1d has 3 versions: ResNet50_v1d_distill (78.67%), ResNet50_v1d_mixup (79.16%), ResNet50_v1d_mixup_distill (79.29%).\r\n\r\n## [Semantic Segmentation](https://gluon-cv.mxnet.io/model_zoo/segmentation.html#semantic-segmentation)\r\n\r\n* Synchronized Batch Normalization training.\r\n* Added Cityscapes dataset and pretrained models.\r\n* Added training details for reproducing state-of-the-art on Pascal VOC and Provided COCO pre-trained  models for VOC.\r\n\r\n## Dependency\r\n**GluonCV 0.3.0 now depends on [incubator-mxnet](https://github.com/apache/incubator-mxnet) >= 1.3.0, please update mxnet according to [installation guide](https://gluon-cv.mxnet.io/#install-mxnet) to avoid compatibility issues.**",
        "dateCreated": "2018-10-16T02:35:30Z",
        "datePublished": "2018-10-16T22:08:39Z",
        "html_url": "https://github.com/dmlc/gluon-cv/releases/tag/v0.3.0",
        "name": "Gluon CV Toolkit 0.3.0",
        "tag_name": "v0.3.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-cv/tarball/v0.3.0",
        "url": "https://api.github.com/repos/dmlc/gluon-cv/releases/13380335",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-cv/zipball/v0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "zhanghang1989",
        "body": "# Gluon CV Toolkit v0.2 Release Notes\r\n\r\n**Note: This release rely on some features of mxnet 1.3.0. You can early access these features by installing nightly build of mxnet.**\r\n\r\nYou can update mxnet with pip:\r\n\r\n```bash\r\npip install mxnet --upgrade --pre\r\n# or \r\npip install mxnet-cu90 --upgrade --pre\r\n```\r\n\r\n\r\n### New Features in 0.2\r\n\r\n#### Image Classification\r\nHighlight: [Much more accurate pre-trained ResNet models on ImageNet classification](https://gluon-cv.mxnet.io/model_zoo/index.html#image-classification)\r\n\r\nThese high accuracy models are updated to [Gluon Model Zoo](https://mxnet.incubator.apache.org/api/python/gluon/model_zoo.html).\r\n\r\n- ResNet50 v1b achieves over 77% accuracy, ResNet101 v1b at 78.8%, and ResNet152 v1b over 79%.\r\n- Training with large batchsize, with float16 data type\r\n- Speeding up training with ImageRecordIter interface\r\n- [ResNeXt for ImageNet and CIFAR10 classification](#resnext)\r\n- SE-ResNet(v1b) for ImageNet\r\n\r\n#### Object Detection\r\nHighlight: Faster-RCNN model with training/testing scripts\r\n\r\n- Faster-RCNN\r\n  - RPN (region proposal network)\r\n  - Region Proposal\r\n  - ROI Align operator\r\n\r\n- Train SSD on COCO dataset\r\n\r\n#### Semantic Segmentation\r\nHighlight: PSPNet for Semantic Segmentation\r\n- PSPNet\r\n- [ResNetV1b for ImageNet classification and Semantic Segmentation](#resnetv1b)\r\n  - Network `dilation` is an option\r\n\r\n#### Datasets\r\nAdded the following datasets and usage tutorials\r\n- MS COCO\r\n- ADE20k\r\n\r\n### [New Pre-trained Models in GluonCV](https://gluon-cv.mxnet.io/model_zoo/index.html)\r\n\r\n- cifar_resnext29_16x64d\r\n- resnet{18|34|50|101}_v1b\r\n- ssd_512_mobilenet1.0_voc\r\n- faster_rcnn_resnet50_v2a_voc\r\n- ssd_300_vgg16_atrous_coco\r\n- ssd_512_vgg16_atrous_coco\r\n- ssd_512_resnet50_v1_coco\r\n- psp_resnet50_ade\r\n\r\n### Breaking changes\r\n  - Rename `DilatedResnetV0` to `ResNetV1b`",
        "dateCreated": "2018-06-26T05:22:09Z",
        "datePublished": "2018-06-26T05:23:10Z",
        "html_url": "https://github.com/dmlc/gluon-cv/releases/tag/v0.2.0",
        "name": "Gluon CV Toolkit v0.2.0",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-cv/tarball/v0.2.0",
        "url": "https://api.github.com/repos/dmlc/gluon-cv/releases/10826270",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-cv/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "zhanghang1989",
        "body": "# Gluon CV Toolkit v0.1 Release Notes\r\n\r\nGluonCV provides implementations of state-of-the-art (SOTA) deep learning algorithms in computer vision. It is designed for helping engineers, researchers, and students to quickly prototype products, validate new ideas, and learning computer vision.\r\n\r\n### Table of Contents\r\n- New Features\r\n  - Tutorials\r\n    - Image Classification (CIFAR + ImageNet demo + divedeep)\r\n    - Object Detection (SSD demo + train + divedeep)\r\n    - Semantic Segmentation (FCN demo + train)\r\n\r\n  - Model Zoo\r\n    - ResNet on ImageNet and CIFAR-10\r\n    - SSD on VOC\r\n    - FCN on VOC\r\n    - Dilated ResNet\r\n  - Training Scripts\r\n    - Image Classification:\r\n      Train ResNet on ImageNet and CIFAR-10, including Mix-Up training\r\n    - Object Detection:\r\n      Train SSD on PASCAL VOC\r\n    - Semantic Segmentation\r\n      Train FCN on PASCAL VOC\r\n  - Util functions\r\n    - Image Visualization:\r\n      - plot_image\r\n      - get_color_pallete for segmentation\r\n    - Bounding Box Visualization\r\n      - plot_bbox\r\n    - Training Helpers\r\n      - PolyLRScheduler\r\n",
        "dateCreated": "2018-05-01T00:03:02Z",
        "datePublished": "2018-05-01T00:53:50Z",
        "html_url": "https://github.com/dmlc/gluon-cv/releases/tag/v0.1",
        "name": "Gluon CV Toolkit v0.1",
        "tag_name": "v0.1",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-cv/tarball/v0.1",
        "url": "https://api.github.com/repos/dmlc/gluon-cv/releases/10791370",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-cv/zipball/v0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5015,
      "date": "Sun, 26 Dec 2021 23:35:45 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "computer-vision",
      "neural-network",
      "gluon",
      "mxnet",
      "machine-learning",
      "image-classification",
      "object-detection",
      "semantic-segmentation",
      "gan",
      "person-reid",
      "action-recognition",
      "pose-estimation"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<div align=\"center\">\n    <img src=\"docs/_static/short_demo.gif\">\n</div>\n\n<br>\n\nCheck the HD video at [Youtube](https://www.youtube.com/watch?v=nfpouVAzXt0) or [Bilibili](https://www.bilibili.com/video/av55619231).\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "All tutorials are available at [our website](https://gluon-cv.mxnet.io/index.html)!\n\n- [Image Classification](http://gluon-cv.mxnet.io/build/examples_classification/index.html)\n\n- [Object Detection](http://gluon-cv.mxnet.io/build/examples_detection/index.html)\n\n- [Semantic Segmentation](http://gluon-cv.mxnet.io/build/examples_segmentation/index.html)\n\n- [Instance Segmentation](http://gluon-cv.mxnet.io/build/examples_instance/index.html)\n\n- [Video Action Recognition](https://gluon-cv.mxnet.io/build/examples_action_recognition/index.html)\n\n- [Depth Prediction](https://gluon-cv.mxnet.io/build/examples_depth/index.html)\n\n- [Generative Adversarial Network](https://github.com/dmlc/gluon-cv/tree/master/scripts/gan)\n\n- [Person Re-identification](https://github.com/dmlc/gluon-cv/tree/master/scripts/re-id/)\n\n",
      "technique": "Header extraction"
    }
  ]
}