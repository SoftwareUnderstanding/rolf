{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1502.04623",
      "https://arxiv.org/abs/1906.02691",
      "https://arxiv.org/abs/1606.05908",
      "https://arxiv.org/abs/1602.02282",
      "https://arxiv.org/abs/1502.04623",
      "https://arxiv.org/abs/1906.02691](https://arxiv.org/abs/1906.02691)\n\nCarl Doersch: Tutorial on Variational Autoencoders, [https://arxiv.org/abs/1606.05908](https://arxiv.org/abs/1606.05908)\n\nCasper Kaae S\u00f8nderby, Tapani Raiko, Lars Maal\u00f8e, S\u00f8ren Kaae S\u00f8nderby & Ole Winther, Ladder Variational Autoencoders, [https://arxiv.org/abs/1602.02282](https://arxiv.org/abs/1602.02282)\n\nKarol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende & Daan Wierstra: DRAW A Recurrent Neural Network For Image Generation, [https://arxiv.org/abs/1502.04623](https://arxiv.org/abs/1502.04623)\n\n\n",
      "https://arxiv.org/abs/1606.05908](https://arxiv.org/abs/1606.05908)\n\nCasper Kaae S\u00f8nderby, Tapani Raiko, Lars Maal\u00f8e, S\u00f8ren Kaae S\u00f8nderby & Ole Winther, Ladder Variational Autoencoders, [https://arxiv.org/abs/1602.02282](https://arxiv.org/abs/1602.02282)\n\nKarol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende & Daan Wierstra: DRAW A Recurrent Neural Network For Image Generation, [https://arxiv.org/abs/1502.04623](https://arxiv.org/abs/1502.04623)\n\n\n",
      "https://arxiv.org/abs/1602.02282](https://arxiv.org/abs/1602.02282)\n\nKarol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende & Daan Wierstra: DRAW A Recurrent Neural Network For Image Generation, [https://arxiv.org/abs/1502.04623](https://arxiv.org/abs/1502.04623)\n\n\n",
      "https://arxiv.org/abs/1502.04623](https://arxiv.org/abs/1502.04623)\n\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Diederik P. Kingma & Max Welling: An Introduction to Variational Autoencoders, [arXiv:1906.02691](https://arxiv.org/abs/1906.02691)\n\nCarl Doersch: Tutorial on Variational Autoencoders, [arXiv:1606.05908](https://arxiv.org/abs/1606.05908)\n\nCasper Kaae S\u00f8nderby, Tapani Raiko, Lars Maal\u00f8e, S\u00f8ren Kaae S\u00f8nderby & Ole Winther, Ladder Variational Autoencoders, [arXiv:1602.02282](https://arxiv.org/abs/1602.02282)\n\nKarol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende & Daan Wierstra: DRAW A Recurrent Neural Network For Image Generation, [arXiv:1502.04623](https://arxiv.org/abs/1502.04623)\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8040386622140713
      ],
      "excerpt": "<img src=\"https://latex.codecogs.com/svg.image?p_\\theta(x|z_1)&space;=&space;N(x|\\mu_{p,0},\\sigma^2_{p,0})\" title=\"p_\\theta(x|z_1) = N(x|\\mu_{p,0},\\sigma^2_{p,0})\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9751004305948343
      ],
      "excerpt": "Where <img src=\"https://latex.codecogs.com/svg.image?Z_X\" title=\"Z_X\" /> and <img src=\"https://latex.codecogs.com/svg.image?Z_Y\" title=\"Z_Y\" /> are normalisation constraints, such that  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simonamtoft/recurrence-and-attention-latent-variable-models",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-10T19:12:19Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-11T09:59:31Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9904317078472411,
        0.9878422331228325,
        0.9957235318501297,
        0.9383053872820086,
        0.8077007945740001
      ],
      "excerpt": "In recent years deep latent variable models have been widely used for image generation and representation learning. Standard approaches employ shallow inference models with restrictive mean-field assumptions.  A way to increase inference expressivity is to define a hierarchy of latent variables in space and build structured approximations. Using this approach the size of the model grows linearly in the number of layers. \nAn orthogonal approach is to define hierarchies in time using a recurrent model. This approach exploits parameter sharing and gives us the possibility to define models with infinite depth (assuming a memory-efficient learning algorithm). \nIn this project, we study recurrent latent variable models for image generation. We focus on attentive models, i.e. models that use attention to decide where to focus on and what to update, refining their output with a sequential update. This is done by implementing the DRAW model, which is described in the DRAW paper, both with basic and filterbank attention. The performance of the implemented DRAW model is then compared to both a standard VAE and a LadderVAE implementation. \nThe project is carried out by Simon Amtoft Pedersen, and supervised by Giorgio Giannone. \nVariational Autoencoders (VAEs) are a type of latent variable model that can be used for generative modelling. The VAEs consists of a decoder part and an encoder part, that is trained by optimizing the Evidence Lower Bound (ELBO). The generative model is given by <img src=\"https://latex.codecogs.com/svg.image?\\inline&space;p_\\theta(z)&space;=&space;p_\\theta(x|z)&space;p_\\theta(z)\" title=\"\\inline p_\\theta(z) = p_\\theta(x|z) p_\\theta(z)\" /> and the samples are then drawn from the distribution by <img src=\"https://latex.codecogs.com/svg.image?z\\sim&space;p_\\theta(z)\" title=\"z\\sim p_\\theta(z)\" /> and <img src=\"https://latex.codecogs.com/svg.image?x\\sim&space;p_\\theta(x|z)\" title=\"x\\sim p_\\theta(x|z)\" />, and reconstruction is drawn from <img src=\"https://latex.codecogs.com/svg.image?q_\\phi(z|x)\" title=\"q_\\phi(z|x)\" />. The objective is then to optimize <img src=\"https://latex.codecogs.com/svg.image?\\inline&space;\\sum_i&space;\\mathcal{L_{\\theta,\\phi}}(x_i)\" title=\"\\inline \\sum_i \\mathcal{L_{\\theta,\\phi}}(x_i)\" /> where ELBO is given as <img src=\"https://latex.codecogs.com/svg.image?\\inline&space;\\mathcal{L_{\\theta,\\phi}}(x)&space;=&space;\\mathbb{E}_{q_\\phi(z|x)}[\\log&space;p_\\theta&space;(x|z)]&space;&plus;&space;\\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{p_\\theta(z)}{q_\\phi(z|x)}\\right]\" title=\"\\inline \\mathcal{L_{\\theta,\\phi}}(x) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta (x|z)] + \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{p_\\theta(z)}{q_\\phi(z|x)}\\right]\" />. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9877681753071827
      ],
      "excerpt": "An extension of the standard VAE is the Ladder VAE, which adds sharing of information and parameters between the encoder and decoder by splitting the latent variables into L layers, such that the model can be described by: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8596264265622202
      ],
      "excerpt": "<img src=\"https://latex.codecogs.com/svg.image?\\inline&space;p_\\theta(z_i&space;|&space;z_{i&plus;1})&space;=&space;N(z_i|&space;\\mu_{p,i},&space;\\sigma^2_{p,i}),&space;\\;\\;\\;\\;&space;p_\\theta(z_L)&space;=&space;N(z_L|0,I)\" title=\"\\inline p_\\theta(z_i | z_{i+1}) = N(z_i| \\mu_{p,i}, \\sigma^2_{p,i}), \\;\\;\\;\\; p_\\theta(Z_L) = N(z_L|0,I)\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.983360512642494,
        0.9872861319504049,
        0.8675986599976311
      ],
      "excerpt": "A lot of the code for the Ladder VAE is taken from Wohlert semi-supervised pytorch project. \nThe Deep Recurrent Attentive Writer (DRAW) model is a VAE like model, trained with stochastic gradient descent, proposed in the original DRAW paper. The main difference is, that the DRAW model iteratively generates the final output instead of doing it in a single shot like a standard VAE. Additionally, the encoder and decoder uses recurrent networks instead of standard linear networks. \nThe model goes through T iterations, where we denote each time-step iteration by t. When using a diagonal Gaussian for the latent distribution, we have: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8478773202971223,
        0.8432135390990686
      ],
      "excerpt": "<img src=\"https://latex.codecogs.com/svg.image?\\!\\!\\!\\!\\!\\!\\!\\!\\!&space;\\hat{x}_t&space;=&space;x&space;-&space;\\sigma(c_{t-1})\\\\r_t&space;=&space;read(x_t,\\hat{x}_t,h_{t-1}^{dec})\\\\h_t^{enc}&space;=&space;RNN^{enc}(h_{t-1}^{enc},&space;[r_t,&space;h_{t-1}^{dec}]])\\\\z_t&space;\\sim&space;Q(z_t|h_t^{enc})\\\\h_t^{dec}&space;=&space;RNN^{dec}(h_{t-1}^{dec},&space;z_t)\\\\c_t&space;=&space;c_{t-1}&space;&plus;&space;write(h_t^{dec})&space;\" title=\"\\!\\!\\!\\!\\!\\!\\!\\!\\! \\hat{x}_t = x - \\sigma(c_{t-1})\\\\r_t = read(x_t,\\hat{x}_t,h_{t-1}^{dec})\\\\h_t^{enc} = RNN^{enc}(h_{t-1}^{enc}, [r_t, h_{t-1}^{dec}]])\\\\z_t \\sim Q(z_t|h_t^{enc})\\\\h_t^{dec} = RNN^{dec}(h_{t-1}^{dec}, z_t)\\\\c_t = c_{t-1} + write(h_t^{dec}) \" /> \nGenerating images from the model is then done by iteratively picking latent samples from the prior distribution, and updating the canvas with the decoder: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554222338437642
      ],
      "excerpt": "In the version without attention, the entire input image is passed to the encoder for every time-step, and the decoder modifies the entire canvas at every step. The two operations are then given by \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9926657598312918,
        0.9057416113097793,
        0.8680921591756741
      ],
      "excerpt": "In oder to use attention when reading and writing, a two-dimensional attention form is used with an array of two-dimensional Gaussian filters. For an input of size A x B, the model generates five parameters from the output of the decoder, which is used to compute the grid center, stride and mean location of the filters: \n<img src=\"https://latex.codecogs.com/svg.image?\\!\\!\\!\\!\\!\\!\\!\\!\\!(\\tilde{g}_X,&space;\\tilde{g}_Y,&space;\\log&space;\\sigma^2,&space;\\log&space;\\tilde{\\delta},&space;\\log&space;\\gamma)&space;=&space;W(h^{dec}_t)\\\\g_X&space;=&space;\\frac{A&plus;1}{2}(\\tilde{g}_X&space;&plus;&space;1)\\\\g_X&space;=&space;\\frac{A&plus;1}{2}(\\tilde{g}_X&space;&plus;&space;1)\\\\&space;\\delta&space;=&space;\\frac{\\max(A,B)&space;-&space;1}{N&space;-&space;1}&space;\\tilde{\\delta}\\\\\\mu_X^i&space;=&space;g_X&space;&plus;&space;(i&space;-&space;N/2&space;-&space;0.5)&space;\\delta\\\\\\mu_Y^j&space;=&space;g_Y&space;&plus;&space;(j&space;-&space;N/2&space;-&space;0.5)&space;\\delta&space;\" title=\"\\!\\!\\!\\!\\!\\!\\!\\!\\!(\\tilde{g}_X, \\tilde{g}_Y, \\log \\sigma^2, \\log \\tilde{\\delta}, \\log \\gamma) = W(h^{dec}_t)\\\\g_X = \\frac{A+1}{2}(\\tilde{g}_X + 1)\\\\g_X = \\frac{A+1}{2}(\\tilde{g}_X + 1)\\\\ \\delta = \\frac{\\max(A,B) - 1}{N - 1} \\tilde{\\delta}\\\\\\mu_X^i = g_X + (i - N/2 - 0.5) \\delta\\\\\\mu_Y^j = g_Y + (j - N/2 - 0.5) \\delta \" /> \nFrom this, the horizontal and veritcal filterbank matrices is defined \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9734296796932321,
        0.9901732902544464
      ],
      "excerpt": "The standard VAE, Ladder VAE and DRAW model with base attention have been trained on the standard torchvision MNIST dataset, which is transformed to be in binarized form. Below the final value of the ELBO, KL and Reconstruction metrics are reported for both the train, validation and test set. Additionally the loss plots for training and validation is shown, and finally some reconstruction and samples from the three different models are shown. \nThe three models are trained in the exact same manner, without using a lot of tricks to improve upon their results. For all models KL-annealing is used over the first 50 epochs. Additionally, every model uses learning rate decay that starts after 200 epochs and is around halved at the end of training. To check the model parameters used, inspect the config dict in the three different training files. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9962440778712769,
        0.9898291464995741
      ],
      "excerpt": "From the results it is clear that the implemented DRAW model performs better than the standard VAE. However, it is also seen that the Ladder VAE has kind of collapsed into the standard VAE, providing far worse results than in the original paper. This can be due to multiple things. First of all the model might not be exactly identical to the proposed model regarding the implementation itself and number and size of layers. Secondly, all the three models are trained in exactly the same manner, without using a lot of tricks to improve the training of the Ladder VAE, which was done in the paper. \nThe filterbank attention version of the DRAW model is somewhat of a work-in-progress. It seems to be implemented correctly using a batch size of one, but very slow computationally. Additionally when running only with a batch size of one, each epoch takes too long to make it feasible. In order to make this model able to work in practice one would have to optimize it for batch sizes larger than one and improve the computational speed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A first iteration attempt to implement the deep recurrent attentive writer (DRAW) ML model for generative modeling, and comparing this to a standard and ladder VAE.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simonamtoft/ml-library/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 07:21:48 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/simonamtoft/recurrence-and-attention-latent-variable-models/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "simonamtoft/recurrence-and-attention-latent-variable-models",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/simonamtoft/ml-library/main/notebooks/A%20Look%20at%20Attention.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/simonamtoft/ml-library/main/bsub.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8408094545071964
      ],
      "excerpt": "|Standard VAE       | -124.38   | 25.93 | 98.45 | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/simonamtoft/recurrence-and-attention-latent-variable-models/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Recurrence and Attention in Latent Variable Models",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "recurrence-and-attention-latent-variable-models",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "simonamtoft",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simonamtoft/recurrence-and-attention-latent-variable-models/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 07:21:48 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "generative-model",
      "vae",
      "draw",
      "attention"
    ],
    "technique": "GitHub API"
  }
}