{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1911.11423",
      "https://arxiv.org/abs/1801.06146"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "- https://github.com/fastai/fastai \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8664507120758091
      ],
      "excerpt": "- the fast.ai NLP-course repository: https://github.com/fastai/course-nlp \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423,
        0.9702105236373249
      ],
      "excerpt": "| German | Deutsch  | de  | 16.1 | 15k | SP | https://bit.ly/ulmfit-dewiki | \n| German | Deutsch  | de  | 18.5 | 30k | SP | https://bit.ly/ulmfit-dewiki-30k | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "| Russian | \u0420\u0443\u0441\u0441\u043a\u0438\u0439 | ru  | 29.8  | 15k | SP | https://bit.ly/ulmfit-ruwiki | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "| Vietnamese | Ti\u1ebfng Vi\u1ec7t | vi  | 18.8 | 15k | SP | https://bit.ly/ulmfit-viwiki | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625923568277878
      ],
      "excerpt": "| Hebrew | \u05e2\u05d1\u05e8\u05d9\u05ea | he  | 46.3 | 15k | SP |https://bit.ly/ulmfit-hewiki | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    \"lang\": \"de\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    \"num_epochs\": 10, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9205970089176198,
        0.8111036989382164
      ],
      "excerpt": "Compared result with: https://arxiv.org/pdf/1912.09582.pdf \nDataset https://github.com/benjaminvdb/DBRD \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479,
        0.9105368110547479
      ],
      "excerpt": "- https://github.com/laboroai/Laboro-BERT-Japanese \n- https://github.com/yoheikikuta/bert-japanese   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8111036989382164
      ],
      "excerpt": "Dataset: https://github.com/e9t/nsmc \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/floleuerer/fastai_ulmfit",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-10T15:54:38Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-07T16:04:20Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9831553596127036,
        0.9628794004802403,
        0.983504409288559
      ],
      "excerpt": "Why even bother with a non-BERT / Transformer language model? Short answer: you can train a state of the art text classifier with ULMFiT with limited data and affordable hardware. The whole process (preparing the Wikipedia dump, pretrain the language model, fine tune the language model and training the classifier) takes about 5 hours on my workstation with a RTX 3090. The training of the model with FP16 requires less than 8 GB VRAM - so you can train the model on affordable GPUs. \nI also saw this paper on the roadmap for fast.ai 2.3 Single Headed Attention RNN: Stop Thinking With Your Head which could improve the performance further.  \nThis Repo is based on:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8976954219819809
      ],
      "excerpt": ": to preserve the filenames (.tgz!) when downloading with wget use --content-disposition \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9088494084226769
      ],
      "excerpt": "The trained language models are compatible with other fastai versions! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9902083192262873,
        0.8979411005071259
      ],
      "excerpt": "\u251c\u2500\u2500 we                         Docker image for the preperation of the Wikipedia-dump / wikiextractor \n\u2514\u2500\u2500 data           \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9632974933511432
      ],
      "excerpt": "        \u2502   \u251c\u2500\u2500 sampled             sampled Wikipedia articles for language model pretraining \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "        \u2514\u2500\u2500 model  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "            \u2502   \u2514\u2500\u2500 spm             SentencePiece model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "            \u2502   \u2514\u2500\u2500 spm             SentencePiece model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8739725691396827
      ],
      "excerpt": "To get the best result, you can train two seperate language models - a forward and a backward model. You'll have to run the complete notebook twice and set the backwards parameter accordingly. The models will be saved in seperate folders (fwd / bwd). The same applies to fine-tuning and training of the classifier. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9226089260010051
      ],
      "excerpt": "lang = 'de' #: language of the Wikipedia-Dump \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9367711878238251,
        0.8058075617177362
      ],
      "excerpt": "vocab_sz = 15000 #: vocab size - 15k / 30k work fine with sentence piece \nnum_workers=18 #: num_workers for the dataloaders \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.985392109381197
      ],
      "excerpt": "model.json contains the parameters the language model was trained with and the statistics (looses and metrics) of the last epoch  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9557236345204622
      ],
      "excerpt": "To improve the performance on the downstream-task, the language model should be fine-tuned. We are using a Twitter dataset (GermEval2018/2019), so we fine-tune the LM on unlabled tweets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9888798289352169
      ],
      "excerpt": "I am not reusing the SentencePiece-Model from the language model! This could lead to slightly different tokenization but fast.ai (-> language_model_learner()) and the fine-tuning takes care of adding and training unknown tokens! This approch gave slightly better results than reusing the SP-Model from the language model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.9972009531955639
      ],
      "excerpt": "- SentencePiece-Model (spm/spm.model) \nResults with an ensemble of forward + backward model (see the inference notebook). Neither the fine-tuning of the LM, nor the training of the classifier was optimized - so there is still room for improvement. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8949751599148799
      ],
      "excerpt": "Copared results with:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "fastai ulmfit - Pretraining the Language Model, Fine-Tuning and training a Classifier",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/floleuerer/fastai_ulmfit/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Mon, 27 Dec 2021 20:42:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/floleuerer/fastai_ulmfit/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "floleuerer/fastai_ulmfit",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/floleuerer/fastai_ulmfit/main/we/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/floleuerer/fastai_ulmfit/tree/main/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/floleuerer/fastai_ulmfit/main/4_ulmfit_train_classifier.ipynb",
      "https://raw.githubusercontent.com/floleuerer/fastai_ulmfit/main/0_prepare_germeval.ipynb",
      "https://raw.githubusercontent.com/floleuerer/fastai_ulmfit/main/3_ulmfit_lm_finetuning.ipynb",
      "https://raw.githubusercontent.com/floleuerer/fastai_ulmfit/main/fastai_ulmfit_pretrained_usage.ipynb",
      "https://raw.githubusercontent.com/floleuerer/fastai_ulmfit/main/5_ulmfit_inference.ipynb",
      "https://raw.githubusercontent.com/floleuerer/fastai_ulmfit/main/2_ulmfit_lm_pretrain.ipynb",
      "https://raw.githubusercontent.com/floleuerer/fastai_ulmfit/main/lib_nbs/index.ipynb",
      "https://raw.githubusercontent.com/floleuerer/fastai_ulmfit/main/lib_nbs/00_pretrained.ipynb",
      "https://raw.githubusercontent.com/floleuerer/fastai_ulmfit/main/lib_nbs/01_embeddings.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "ULMFiT can be peretrained on relativly small datasets - 100 million tokens are sufficient to get state-of-the art classification results (compared to Transformer models as BERT, which need huge amounts of training data). The easiest way is to pretrain a language model on Wikipedia.\n\nThe code for the preperation steps is heavily inspired by / copied from the **fast.ai NLP-course**: https://github.com/fastai/course-nlp/blob/master/nlputils.py\n\nI built a docker container and script, that automates the following steps:\n1) Download Wikipedia XML-dump\n2) Extract the text from the dump\n3) Sample 160.000 documents with a minimum length of 1800 characters (results in 100m-120m tokens) both parameters can be changed - see the usage below\n\nThe whole process will take some time depending on the download speed and your hardware. For the 'dewiki' the preperation took about 45 min.\n\nRun the following commands in the current directory\n```\n#: build the wikiextractor docker file\ndocker build -t wikiextractor ./we\n\n#: run the docker container for a specific language\n#: docker run -v $(pwd)/data:/data -it wikiextractor -l <language-code> \n#: for German language-code de run:\ndocker run -v $(pwd)/data:/data -it wikiextractor -l de\n...\nsucessfully prepared dewiki - /data/dewiki/docs/sampled, number of docs 160000/160000 with 110699119 words / tokens!\n\n#: To change the number of sampled documents or the minimum length see\nusage: preprocess.py [-h] -l LANG [-n NUMBER_DOCS] [-m MIN_DOC_LENGTH] [--mirror MIRROR] [--cleanup]\n\n#: To cleanup indermediate files (wikiextractor and all splitted documents) run the following command. \n#: The Wikipedia-XML-Dump and the sampled docs will not be deleted!\ndocker run -v $(pwd)/data:/data -it wikiextractor -l <language-code> --cleanup\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8918974083095406
      ],
      "excerpt": "- https://github.com/fastai/fastai \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9967226375118565,
        0.9979947896609701
      ],
      "excerpt": "Install packages \npip install -r requirements.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8742654372449469
      ],
      "excerpt": "Dataset https://github.com/benjaminvdb/DBRD \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918974083095406,
        0.8918974083095406
      ],
      "excerpt": "- https://github.com/laboroai/Laboro-BERT-Japanese \n- https://github.com/yoheikikuta/bert-japanese   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8742654372449469
      ],
      "excerpt": "Dataset: https://github.com/e9t/nsmc \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from fastai_ulmfit.embeddings import SentenceEmbeddingCallback \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8592487823519709,
        0.8484026989226723
      ],
      "excerpt": "backwards = False #: Train backwards model? Default: False for forward model \nbs=128 #: batch size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044290599680651
      ],
      "excerpt": "    \"batch_size\": 128, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8612630503664754
      ],
      "excerpt": "    \"lr\": 0.01, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8521009991913112
      ],
      "excerpt": "To use the notebook on your own dataset, create a .csv-file containing your (unlabled) data in the text column. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8228779747048725
      ],
      "excerpt": "- Model (model.pth) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/floleuerer/fastai_ulmfit/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Makefile",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Florian Leuerer\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "fast.ai ULMFiT with SentencePiece from pretraining to deployment",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "fastai_ulmfit",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "floleuerer",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/floleuerer/fastai_ulmfit/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 22,
      "date": "Mon, 27 Dec 2021 20:42:23 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I've written a small library around this repo, to easily use the pretrained models. You don't have to bother with model, vocab and tokenizer files and paths - the following functions will take care of that. \n\nTutorial:  [fastai_ulmfit_pretrained_usage.ipynb](https://github.com/floleuerer/fastai_ulmfit/blob/main/fastai_ulmfit_pretrained_usage.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/floleuerer/fastai_ulmfit/blob/main/fastai_ulmfit_pretrained_usage.ipynb)\n\n\n**Installation**\n````\npip install fastai-ulmfit\n````\n\n**Usage**\n\n```\n#: import\nfrom fastai_ulmfit.pretrained import *\n\nurl = 'http://bit.ly/ulmfit-dewiki'\n\n#: get tokenizer - if pretrained=True, the SentencePiece Model used for language model pretraining will be used. Default: False \ntok = tokenizer_from_pretrained(url, pretrained=False)\n\n#: get language model learner for fine-tuning\nlearn = language_model_from_pretrained(dls, url=url, drop_mult=0.5).to_fp16()\n\n#: save fine-tuned model for classification\npath = learn.save_lm('tmp/test_lm')\n\n#: get text classifier learner from fine-tuned model\nlearn = text_classifier_from_lm(dls, path=path, metrics=[accuracy]).to_fp16()\n````\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Notebook: `5_ulmfit_inference.ipynb`\n\n",
      "technique": "Header extraction"
    }
  ]
}