{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2005.12741",
      "https://arxiv.org/abs/1409.1556"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9343900839872272
      ],
      "excerpt": "Authors: Mengmi Zhang, and Gabriel Kreiman \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kreimanlab/HumanIntentionInferenceZeroShot",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-31T14:06:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-20T07:54:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Can we infer intentions and goals from a person's actions? As an example of this family of problems, we consider here whether it is possible to decipher what a person is searching for by decoding their eye movement behavior. We conducted two human psychophysics experiments on object arrays and natural images where we monitored subjects' eye movements while they were looking for a target object. Using as input the pattern of \"error\" fixations on non-target objects before the target was found, we developed a model (InferNet) whose goal was to infer what the target was. \"Error\" fixations share similar features with the sought target. The Infernet model uses a pre-trained 2D convolutional architecture to extract features from the error fixations and computes a 2D similarity map between the error fixation and all locations across the search image by modulating the search image via convolution across layers. InferNet consolidates the modulated response maps across layers via max pooling to keep track of the sub-patterns highly similar to features at error fixations and integrates these maps across all error fixations. InferNet successfully identifies the subject's goal and outperforms all the competitive null models, even without any object-specific training on the inference task. \n\n[![problemintro](img/Capture.JPG)](img/Capture.JPG)\n\nWe present an example below. The figure shows the error fixations indicated by yellow number in the visual search process of one human subject (Column 1), the inference maps predicted by our InferNet based on human error fixations (Column 2) and the inferred target location denoted by green numbers (Column 3) from the inference maps. Red color denotes higher probability of being the search target.\n\n[![problemintro](img/1-teaser.gif)](img/1-teaser.gif)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9978653748322749,
        0.9849049021815734,
        0.8708238674075698
      ],
      "excerpt": "This repository contains an implementation of a zero-shot deep learning model for inferring human intentions (what the human subject is searching for) based on fixation patterns. Our paper has been accepted in EPIC-CVPR workshop, 2020. \nAn unofficial copy of our manuscript is HERE. \nWatch our 5-min presentation HERE. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kreimanlab/HumanIntentionInferenceZeroShot/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 22 Dec 2021 21:00:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kreimanlab/HumanIntentionInferenceZeroShot/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kreimanlab/HumanIntentionInferenceZeroShot",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kreimanlab/HumanIntentionInferenceZeroShot/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Lua",
      "MATLAB"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "What am I Searching for: Zero-shot Target Identity Inference in Visual Search",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "HumanIntentionInferenceZeroShot",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kreimanlab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kreimanlab/HumanIntentionInferenceZeroShot/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code has been successfully tested on Ubuntu 14.04. GPU is highly recommended (6GB GPU memory at least). \n\nIt requires the deep learning platform Torch7. Refer to [link](http://torch.ch/docs/getting-started.html) for installation.  \n\nMatio package is required (save and load matlab arrays from Torch7). Refer to [link](https://github.com/soumith/matio-ffi.torch) for installation.\n\nLoadcaffe package is required (load pre-trained caffe model to Torch7). Refer to [link](https://github.com/szagoruyko/loadcaffe) for installation.\n\nRun the commands:\n```\nluarocks install image\nluarocks install tds\n```\nDownload our repository:\n```\ngit clone https://github.com/kreimanlab/HumanIntentionInferenceZeroShot.git\n```\n\nDownload the caffe VGG16 model from [HERE](https://drive.google.com/open?id=1AEJse0liaT8uJoLmImqhyJN2y2_6mDsJ) and place it in folder ```/Models/caffevgg16/```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Wed, 22 Dec 2021 21:00:06 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Navigate to the repository folder. To pre-process fixation data, extract cropped fixation patches and save them in a folder, run ```preprocessFixation/GenerateFixationPatchObjectArray.m``` for object arrays in MATLAB. One can easily generalize the same processes to natural images by running ```preprocessFixation/GenerateFixationPatchObjectNaturalDesignWaldo.n``` .\n\nTo run our InferNet torch7 model (take fixation patches and search images as inputss, output likelihood maps for selected layer in [VGG16](https://arxiv.org/abs/1409.1556)), navigate to ```/torchMM``` and copy the following command in the command window:\n```\nth MMInferArray.lua\n```\n\nNavigate to ```/eval``` folder. Run ```ConsolidateAttentionMapArray.m``` in MATLAB to consolidate likelihood maps across layers and save them in a folder. Run ```ScoreBasedonErrorFixationsArray.m``` to generate final likelihood maps and calculate the number of guesses required given certain number of error fixations. All the evaluation results are saved in ```Mat``` folder. One can skip the steps above and run the following functions to plot the results presented in our paper.\n\n- ```PlotScoreBasedonErrorFixationNumber_cummulative.m```: plot relative improvment performance for all computational models. Toggle ```type``` variable from ```array``` to ```naturaldesign``` for different plots in two datasets respectively.\n- ```PlotAblationArray.m```: plot relative improvment performance for all ablated models on object arrays.\n- ```PlotQualatativeArray.m```: plot visualization example of final likelihood maps on object arrays.\n\n",
      "technique": "Header extraction"
    }
  ]
}