{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.00597"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@InProceedings{pmlr-v139-emami21a,\n  title = \t {Efficient Iterative Amortized Inference for Learning Symmetric and Disentangled Multi-Object Representations},\n  author =       {Emami, Patrick and He, Pan and Ranka, Sanjay and Rangarajan, Anand},\n  booktitle = \t {Proceedings of the 38th International Conference on Machine Learning},\n  pages = \t {2970--2981},\n  year = \t {2021},\n  editor = \t {Meila, Marina and Zhang, Tong},\n  volume = \t {139},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {18--24 Jul},\n  publisher =    {PMLR},\n  url = \t {http://proceedings.mlr.press/v139/emami21a.html},\n}\n\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{pmlr-v139-emami21a,\n  title =    {Efficient Iterative Amortized Inference for Learning Symmetric and Disentangled Multi-Object Representations},\n  author =       {Emami, Patrick and He, Pan and Ranka, Sanjay and Rangarajan, Anand},\n  booktitle =    {Proceedings of the 38th International Conference on Machine Learning},\n  pages =    {2970--2981},\n  year =     {2021},\n  editor =   {Meila, Marina and Zhang, Tong},\n  volume =   {139},\n  series =   {Proceedings of Machine Learning Research},\n  month =    {18--24 Jul},\n  publisher =    {PMLR},\n  url =      {http://proceedings.mlr.press/v139/emami21a.html},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{multiobjectdatasets19,\n  title={Multi-Object Datasets},\n  author={Kabra, Rishabh and Burgess, Chris and Matthey, Loic and\n          Kaufman, Raphael Lopez and Greff, Klaus and Reynolds, Malcolm and\n          Lerchner, Alexander},\n  howpublished={https://github.com/deepmind/multi-object-datasets/},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9990510104661714,
        0.8772692606136239
      ],
      "excerpt": "Official implementation of our ICML'21 paper \"Efficient Iterative Amortized Inference for Learning Symmetric and Disentangled Multi-object Representations\" Link.  \nWatch our YouTube explainer video \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9674929110560977
      ],
      "excerpt": "Please cite the original repo if you use this benchmark in your work: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266,
        0.8252756551763226
      ],
      "excerpt": "| Tetrominoes | 99.7 | 2.76 x 10^-4 | 70.7 | 5 hrs 2 min | 2x Geforce RTX 2080Ti | \n| CLEVR6 | 98.05 | 3.64 x 10^-4 | 187.1 | ~17 hours | 8x Geforce RTX 2080Ti | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "| Tetrominoes | 35 x 35 |  Gaussian       | 0.3 | -4500 (-1.224) |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8206704429237939
      ],
      "excerpt": "| Net.K | The number of object-centric latents (i.e., slots) | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9397805555443952
      ],
      "excerpt": "| Net.reverse_prior_plusplus | Trains EMORL w/ reversed prior++ (Default true), if false trains w/ reversed prior | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pemami4911/EfficientMORL",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-09T17:07:52Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-04T07:26:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9733790228849911
      ],
      "excerpt": "Official implementation of our ICML'21 paper \"Efficient Iterative Amortized Inference for Learning Symmetric and Disentangled Multi-object Representations\" Link.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9920080004936682
      ],
      "excerpt": "The motivation of this work is to design a deep generative model for learning high-quality representations of multi-object scenes. Generally speaking, we want a model that  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9889240345642256
      ],
      "excerpt": "To achieve efficiency, the key ideas were to cast iterative assignment of pixels to slots as bottom-up inference in a multi-layer hierarchical variational autoencoder (HVAE), and to use a few steps of low-dimensional iterative amortized inference to refine the HVAE's approximate posterior. We found that the two-stage inference design is particularly important for helping the model to avoid converging to poor local minima early during training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9813420561309143
      ],
      "excerpt": "See the paper for more details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8901881051409943,
        0.8354228047091593
      ],
      "excerpt": "These are processed versions of the tfrecord files available at Multi-Object Datasets in an .h5 format suitable for PyTorch. \nSee lib/datasets.py for how they are used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9549737924095948,
        0.968883017423886,
        0.9834126801129528,
        0.9814125271820844,
        0.9706387563466314,
        0.916268508343592
      ],
      "excerpt": "We found that on Tetrominoes and CLEVR in the Multi-Object Datasets benchmark, using GECO was necessary to stabilize training across random seeds and improve sample efficiency (in addition to using a few steps of lightweight iterative amortized inference). \nGECO is an excellent optimization tool for \"taming\" VAEs that helps with two key aspects: \n1. Dynamically adjusts a hyperparameter that trades off the reconstruction and KL losses, which improves training robustness to poor weight initializations from a \"bad\" random seed. The automatic schedule initially increases the relative weight of the reconstruction term to encourage the model to first achieve a high-quality image reconstruction. Following this, the relative weighting of the reconstruction term is decreased to minimize the KL. \n2. Reduces variance in the gradients of the ELBO by minimizing the distance of an exponential moving average (EMA) of the reconstruction error to a pre-specified target reconstruction error (an easier constrained minimization), instead of trying to directly minimize the error (a harder unconstrained minimization). Lower variance results in faster convergence. \nThe caveat is we have to specify the desired reconstruction target for each dataset, which depends on the image resolution and image likelihood.  \nHere are the hyperparameters we used for this paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643,
        0.9326688067690315,
        0.9513777536289676,
        0.9080168000159835
      ],
      "excerpt": "| CLEVR6      | 96 x 96 |  Mixture of Gaussians | 0.1 |  -61000 (-2.206) | \nWe show the per-pixel and per-channel reconstruction target in paranthesis. Note that we optimize unnormalized image likelihoods, which is why the values are negative. \nWe found GECO wasn't needed for Multi-dSprites to achieve stable convergence across many random seeds and a good trade-off of reconstruction and KL.  \nChoosing the reconstruction target: I have come up with the following heuristic to quickly set the reconstruction target for a new dataset without investing much effort: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8427378547816653,
        0.9180853666149608,
        0.9816242042947082
      ],
      "excerpt": "2. Start training and monitor the reconstruction error (e.g., in Tensorboard) for the first 10-20% of training steps. EMORL (and any pixel-based object-centric generative model) will in general learn to reconstruct the background first. This accounts for a large amount of the reconstruction error. \n3. Stop training, and adjust the reconstruction target so that the reconstruction error achieves the target after 10-20% of the training steps. This will reduce variance since target - EMA(recon_error) goes to 0 and allows GECO to gently increase its Lagrange parameter until foreground objects are discovered. The target should ideally be set to the reconstruction error achieved after foreground objects are discovered. \n4. Once foreground objects are discovered, the EMA of the reconstruction error should be lower than the target (in Tensorboard, geco_C_ema will be a positive value, which is target - EMA(recon_error)). Once this is positive, the GECO Lagrange parameter will decrease back to 1. This is important so that the model estimates a proper ELBO at the end of training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9956190775362053
      ],
      "excerpt": "| Net.image_likelihood | \"GMM\" is the Mixture of Gaussians, \"Gaussian\" is the deteriministic mixture | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8578728438034182
      ],
      "excerpt": "| Net.image_decoder | \"iodine\" is the (memory-intensive) decoder from the IODINE paper, \"big\" is Slot Attention's memory-efficient deconvolutional decoder, and \"small\" is Slot Attention's tiny decoder | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8061782509258978
      ],
      "excerpt": "| Net.log_scale | ln(std_dev) used in the image likelihood | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8724208719365676,
        0.8758755561224246,
        0.8450948975973963
      ],
      "excerpt": "We provide a bash script ./scripts/make_gifs.sh for creating disentanglement GIFs for individual slots. This uses moviepy, which needs ffmpeg. In eval.py, we set the IMAGEIO_FFMPEG_EXE and FFMPEG_BINARY environment variables (at the beginning of the _mask_gifs method) which is used by moviepy. You will need to make sure these env vars are properly set for your system first. \nFor each slot, the top 10 latent dims (as measured by their activeness---see paper for definition) are perturbed to make a gif. \nCheck and update the same bash variables DATA_PATH, OUT_DIR, CHECKPOINT, ENV, and JSON_FILE as you did for computing the ARI+MSE+KL. The EVAL_TYPE is make_gifs, which is already set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9617690390202659
      ],
      "excerpt": "Will create a file storing the min/max of the latent dims of the trained model, which helps with running the activeness metric and visualization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "EfficientMORL (ICML'21)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pemami4911/EfficientMORL/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 17:16:28 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pemami4911/EfficientMORL/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pemami4911/EfficientMORL",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pemami4911/EfficientMORL/main/notebooks/demo.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pemami4911/EfficientMORL/main/scripts/make_gifs.sh",
      "https://raw.githubusercontent.com/pemami4911/EfficientMORL/main/scripts/train.sh",
      "https://raw.githubusercontent.com/pemami4911/EfficientMORL/main/scripts/eval.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Install dependencies using the provided conda environment file:\n\n```bash\n$ conda env create -f environment.yml\n```\n\nTo install the conda environment in a desired directory, add a prefix to the environment file first.\n\nFor example, add this line to the end of the environment file: `prefix: /home/{YOUR_USERNAME}/.conda/envs`\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9495202768514088
      ],
      "excerpt": "Like with the training bash script, you need to set/check the following bash variables  ./scripts/eval.sh: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8005168985375024
      ],
      "excerpt": "Then, run the script: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8005168985375024,
        0.9023697225149864
      ],
      "excerpt": "Then, run the script: \n$ ./make_gifs.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9182769770577225
      ],
      "excerpt": "<img src=\"./examples/main.png\" width=\"800px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8438279774217433
      ],
      "excerpt": "| checkpoint | ARI | MSE | KL | wall clock training | hardware |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8223836778426724
      ],
      "excerpt": "1. Choose a random initial value somewhere in the ballpark of where the reconstruction error should be (e.g., for CLEVR6 128 x 128, we may guess -96000 at first). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8435341164622064,
        0.8236031036007144
      ],
      "excerpt": "| Net.bottom_up_prior | Train EMORL w/ BU prior (Default false) | \n| Net.reverse_prior_plusplus | Trains EMORL w/ reversed prior++ (Default true), if false trains w/ reversed prior | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8566726905741084,
        0.9148942369688909
      ],
      "excerpt": "| training.load_from_checkpoint | Set to true if resuming training | \n| training.checkpoint | The .pth filename for resuming training | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8721551980723856
      ],
      "excerpt": "CHECKPOINT = checkpoint.pth #: Set to the name of the .pth file saved in `/path/to/results/weights` \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8091193764678944,
        0.8068489433571217,
        0.8010525124534313
      ],
      "excerpt": "JSON_FILE = EMORL #: The code will look for the sacred config as JSON under `test/$ENV/$EVAL_TYPE/$JSON_FILE.json`. \nEVAL_TYPE = ARI_MSE_KL #: Tell `eval.py` what type of evaluation to run. `ARI_MSE_KL` will compute ARI, MSE, and KL. All eval types available can be found in `eval.py`. \nThen, run the script: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9250769088520814
      ],
      "excerpt": "Results will be stored in files ARI.txt, MSE.txt and KL.txt in folder $OUT_DIR/results/{test.experiment_name}/$CHECKPOINT-seed=$SEED. The experiment_name is specified in the sacred JSON file. This path will be printed to the command line as well. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8010525124534313
      ],
      "excerpt": "Then, run the script: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9419377296848626
      ],
      "excerpt": "A series of files with names slot_{0-#slots}_row_{0-9}.gif will be created under the results folder $OUT_DIR/results/{test.experiment_name}/$CHECKPOINT-seed=$SEED.  The experiment_name is specified in the sacred JSON file. This path will be printed to the command line as well. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9094298180633323
      ],
      "excerpt": "An array of the variance values activeness.npy will be stored in folder $OUT_DIR/results/{test.experiment_name}/$CHECKPOINT-seed=$SEED \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9115894420099588
      ],
      "excerpt": "Results will be stored in a file dci.txt in folder $OUT_DIR/results/{test.experiment_name}/$CHECKPOINT-seed=$SEED \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9026875043074183
      ],
      "excerpt": "Results will be stored in a file rinfo_{i}.pkl in folder $OUT_DIR/results/{test.experiment_name}/$CHECKPOINT-seed=$SEED where i is the sample index \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pemami4911/EfficientMORL/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Patrick E.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "EfficientMORL",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "EfficientMORL",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pemami4911",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pemami4911/EfficientMORL/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 18,
      "date": "Mon, 27 Dec 2021 17:16:28 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "vae",
      "slot-based",
      "deep-generative-model",
      "vision"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We recommend starting out getting familiar with this repo by training EfficientMORL on the Tetrominoes dataset.\nIt can finish training in a few hours with 1-2 GPUs and converges relatively quickly.\nThe following steps to start training a model can similarly be followed for CLEVR6 and Multi-dSprites.\n\nInspect the model hyperparameters we use in `./configs/train/tetrominoes/EMORL.json`, which is the Sacred config file.\nNote that `Net.stochastic_layers` is `L` in the paper and `training.refinement_curriculum` is `I` in the paper.\n \n\nThen, go to `./scripts` and edit `train.sh`. Provide values for the following variables:\n\n```bash\nNUM_GPUS = 2 #: Set this to however many GPUs you have available\nSEED = 1 #: The desired random seed for this training run\nDDP_PORT = 29500 #: The port number for torch.distributed, can be left to default\nENV = tetrominoes\nMODEL = EMORL\nDATA_PATH = /path/to/data #: Set to the absolute path of the folder where the unzipped .h5 files are\nBATCH_SIZE = 16 #: Set to 32 / NUM_GPUS\nOUT_DIR = /path/to/outputs #: Set to the absolute path of the folder where you will save tensorboard files, model weights, and (optionally) sacred runs\n```\n\nStart training:\n\n```bash\n$ ./train.sh\n```\n\nMonitor loss curves and visualize RGB components/masks:\n\n```bash\n$ tensorboard --logdir $OUT_DIR/tb\n```\n\nand open in your browser.\n\n",
      "technique": "Header extraction"
    }
  ]
}