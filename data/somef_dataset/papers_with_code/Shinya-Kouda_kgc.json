{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1511.01432",
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1810.04805"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For now, cite [the Arxiv paper](https://arxiv.org/abs/1810.04805):\n\n```\n@article{devlin2018bert,\n  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\n  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n  journal={arXiv preprint arXiv:1810.04805},\n  year={2018}\n}\n```\n\nIf we submit the paper to a conference or journal, we will update the BibTeX.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{devlin2018bert,\n  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\n  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n  journal={arXiv preprint arXiv:1810.04805},\n  year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8105206190523901
      ],
      "excerpt": "    Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8450045395698996
      ],
      "excerpt": "Sentence B: he bought a gallon of milk . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8105206190523901
      ],
      "excerpt": "    Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "export BERT_BASE_DIR=gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "  --train_batch_size=12 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030554797677082
      ],
      "excerpt": "If you fine-tune for one epoch on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "...          | 384        | 12 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "  --num_warmup_steps=10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8276578193265479
      ],
      "excerpt": "    you should use a smaller learning rate (e.g., 2e-5). \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Shinya-Kouda/kgc",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For help or issues using BERT, please submit a GitHub issue.\n\nFor personal communication related to BERT, please contact Jacob Devlin\n(`jacobdevlin@google.com`), Ming-Wei Chang (`mingweichang@google.com`), or\nKenton Lee (`kentonl@google.com`).\n",
      "technique": "Header extraction"
    }
  ],
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "How to Contribute\nBERT needs to maintain permanent compatibility with the pre-trained model files,\nso we do not plan to make any major changes to this library (other than what was\npromised in the README). However, we can accept small patches related to\nre-factoring and documentation. To submit contributes, there are just a few\nsmall guidelines you need to follow.\nContributor License Agreement\nContributions to this project must be accompanied by a Contributor License\nAgreement. You (or your employer) retain the copyright to your contribution;\nthis simply gives us permission to use and redistribute your contributions as\npart of the project. Head over to https://cla.developers.google.com/ to see\nyour current agreements on file or to sign a new one.\nYou generally only need to submit a CLA once, so if you've already submitted one\n(even if it was for a different project), you probably don't need to do it\nagain.\nCode reviews\nAll submissions, including submissions by project members, require review. We\nuse GitHub pull requests for this purpose. Consult\nGitHub Help for more\ninformation on using pull requests.\nCommunity Guidelines\nThis project follows\nGoogle's Open Source Community Guidelines.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-07T15:30:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-04T14:20:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**BERT**, or **B**idirectional **E**ncoder **R**epresentations from\n**T**ransformers, is a new method of pre-training language representations which\nobtains state-of-the-art results on a wide array of Natural Language Processing\n(NLP) tasks.\n\nOur academic paper which describes BERT in detail and provides full results on a\nnumber of tasks can be found here:\n[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).\n\nTo give a few numbers, here are the results on the\n[SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering\ntask:\n\nSQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1\n------------------------------------- | :------: | :------:\n1st Place Ensemble - BERT             | **87.4** | **93.2**\n2nd Place Ensemble - nlnet            | 86.0     | 91.7\n1st Place Single Model - BERT         | **85.1** | **91.8**\n2nd Place Single Model - nlnet        | 83.5     | 90.1\n\nAnd several natural language inference tasks:\n\nSystem                  | MultiNLI | Question NLI | SWAG\n----------------------- | :------: | :----------: | :------:\nBERT                    | **86.7** | **91.1**     | **86.3**\nOpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0\n\nPlus many other tasks.\n\nMoreover, these results were all obtained with almost no task-specific neural\nnetwork architecture design.\n\nIf you already know what BERT is and you just want to get started, you can\n[download the pre-trained models](#pre-trained-models) and\n[run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few\nminutes.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9670370874604227
      ],
      "excerpt": "This is a release of several new models which were the result of an improvement \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285349598628577
      ],
      "excerpt": "In the original pre-processing code, we randomly select WordPiece tokens to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9590334115698834,
        0.9721522251579512
      ],
      "excerpt": "The new technique is called Whole Word Masking. In this case, we always mask \nall of the the tokens corresponding to a word at once. The overall masking \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8956339792677308
      ],
      "excerpt": "Pre-trained models with Whole Word Masking are linked below. The data and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9629429494817681,
        0.8988491656083732,
        0.9528943617781297
      ],
      "excerpt": "vocab to the original models. We only include BERT-Large models. When using \nthese models, please make it clear in the paper that you are using the Whole \nWord Masking variant of BERT-Large. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8025245747663887
      ],
      "excerpt": "run_classifier_with_tfhub.py for an example of how to use the TF Hub module, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.888872449770618,
        0.8142957385377487
      ],
      "excerpt": "We uploaded a new multilingual model which does not perform any normalization \non the input (no lower casing, accent stripping, or Unicode normalization), and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8972891123127502
      ],
      "excerpt": "It is recommended to use this version for developing multilingual models, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9535234631292823,
        0.9923834188907339,
        0.8922451365192265,
        0.8139584013722183
      ],
      "excerpt": "We released code changes to reproduce our 83% F1 SQuAD 2.0 system, which is \ncurrently 1st place on the leaderboard by 3%. See the SQuAD 2.0 section of the \nREADME for details. \n***** New November 5th, 2018: Third-party PyTorch and Chainer versions of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8258402037175955,
        0.989243237034123,
        0.9015534603427343
      ],
      "excerpt": "PyTorch version of BERT available \nwhich is compatible with our pre-trained checkpoints and is able to reproduce \nour results. Sosuke Kobayashi also made a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9499725990711281,
        0.9214797454662798
      ],
      "excerpt": "(Thanks!) We were not involved in the creation or maintenance of the PyTorch \nimplementation so please direct any questions towards the authors of that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9227192430874179,
        0.8961581825078053,
        0.9389003855505633
      ],
      "excerpt": "We use character-based tokenization for Chinese, and WordPiece tokenization for \nall other languages. Both models should work out-of-the-box without any code \nchanges. We did update the implementation of BasicTokenizer in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9834869998998549
      ],
      "excerpt": "For more, see the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9260829420234794,
        0.889627016979435,
        0.8667010742680953,
        0.9052415479384156
      ],
      "excerpt": "BERT is a method of pre-training language representations, meaning that we train \na general-purpose \"language understanding\" model on a large text corpus (like \nWikipedia), and then use that model for downstream NLP tasks that we care about \n(like question answering). BERT outperforms previous methods because it is the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9890779187192537,
        0.8842040420238866
      ],
      "excerpt": "is important because an enormous amount of plain text data is publicly available \non the web in many languages. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9723139865992518,
        0.911478160601604
      ],
      "excerpt": "instead generate a representation of each word that is based on the other words \nin the sentence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "ELMo, and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.939443406301941,
        0.9370335808561889,
        0.8303468561419067,
        0.9821092939480334
      ],
      "excerpt": "\u2014 but crucially these models are all unidirectional or shallowly \nbidirectional. This means that each word is only contextualized using the words \nto its left (or right). For example, in the sentence I made a bank deposit the \nunidirectional representation of bank is only based on I made a but not \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9627140808341351
      ],
      "excerpt": "\u2014 starting from the very bottom of a deep neural network, so it is deeply \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.961212187435223
      ],
      "excerpt": "BERT uses a simple approach for this: We mask out 15% of the words in the input, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9757941675443742
      ],
      "excerpt": "Input: the man went to the [MASK1] . he bought a [MASK2] of milk. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.880703162501244
      ],
      "excerpt": "In order to learn relationships between sentences, we also train on a simple \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8881000144538649
      ],
      "excerpt": "and B, is B the actual next sentence that comes after A, or just a random \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9389485061907631
      ],
      "excerpt": "Sentence B: he bought a gallon of milk . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8235602494461995
      ],
      "excerpt": "(Wikipedia + BookCorpus) for a long time (1M \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9435232673675573,
        0.8768208426324942
      ],
      "excerpt": "Pre-training is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a \none-time procedure for each language (current models are English-only, but \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9830242257273324
      ],
      "excerpt": "Fine-tuning is inexpensive. All of the results in the paper can be \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9076606631315258,
        0.9778112548726583,
        0.9922262786684682,
        0.9670263237455218
      ],
      "excerpt": "trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of \n91.0%, which is the single system state-of-the-art. \nThe other important aspect of BERT is that it can be adapted to many types of \nNLP tasks very easily. In the paper, we demonstrate state-of-the-art results on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8450998151175391
      ],
      "excerpt": "(e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specific \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9277766945787876
      ],
      "excerpt": "TensorFlow code for the BERT model architecture (which is mostly a standard \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8371703042418694,
        0.9267525247873877
      ],
      "excerpt": "    BERT-Base and BERT-Large from the paper. \nTensorFlow code for push-button replication of the most important \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9772341422295684
      ],
      "excerpt": "All of the code in this repository works out-of-the-box with CPU, GPU, and Cloud \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9375127400876392
      ],
      "excerpt": "We are releasing the BERT-Base and BERT-Large models from the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9384507792137328
      ],
      "excerpt": "e.g., John Smith becomes john smith. The Uncased model also strips out any \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8989693486336962,
        0.8117297546189195,
        0.9968029537584643,
        0.8585344907189063
      ],
      "excerpt": "preserved. Typically, the Uncased model is better unless you know that case \ninformation is important for your task (e.g., Named Entity Recognition or \nPart-of-Speech tagging). \nThese models are all released under the same license as the source code (Apache \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9810856087116488
      ],
      "excerpt": "For information about the Multilingual and Chinese model, see the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9248931296448065
      ],
      "excerpt": "When using a cased model, make sure to pass --do_lower=False to the training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.845627915249851
      ],
      "excerpt": "The links to the models are here (right-click, 'Save link as...' on the name): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9302287581602335,
        0.94009548006442,
        0.9656783393044157,
        0.9782394787081777,
        0.9100047353230469,
        0.9396991686384055,
        0.9595258773483509,
        0.9007716629187501
      ],
      "excerpt": "    the model. \nImportant: All results on the paper were fine-tuned on a single Cloud TPU, \nwhich has 64GB of RAM. It is currently not possible to re-produce most of the \nBERT-Large results on the paper using a GPU with 12GB - 16GB of RAM, because \nthe maximum batch size that can fit in memory is too small. We are working on \nadding code to this repository which allows for much larger effective batch size \non the GPU. See the section on out-of-memory issues for \nmore details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9501634258621126,
        0.8620835320415314
      ],
      "excerpt": "Python3 (but more thoroughly with Python2, since this is what's used internally \nin Google). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8265102884185229
      ],
      "excerpt": "that has at least 12GB of RAM using the hyperparameters given. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8529383023991518
      ],
      "excerpt": "on your local machine, using a GPU like a Titan X or GTX 1080. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8335902728534769
      ],
      "excerpt": "Please see the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "\"BERT FineTuning with Cloud TPUs\". \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8441853267286736
      ],
      "excerpt": "that it's running on something other than a Cloud TPU, which includes a GPU. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207158289893998,
        0.960323208354937
      ],
      "excerpt": "The Stanford Question Answering Dataset (SQuAD) is a popular question answering \nbenchmark dataset. BERT (at the time of the release) obtains state-of-the-art \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9400534798787255,
        0.9428704252177718,
        0.8403572125810591
      ],
      "excerpt": "and post-processing to deal with (a) the variable-length nature of SQuAD context \nparagraphs, and (b) the character-level answer annotations which are used for \nSQuAD training. This processing is implemented and documented in run_squad.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.843707490652359,
        0.835980124536
      ],
      "excerpt": "SQuAD website does not seem to \nlink to the v1.1 datasets any longer, but the necessary files can be found here: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9543614526633579
      ],
      "excerpt": "The state-of-the-art SQuAD results from the paper currently cannot be reproduced \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "the output_dir: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9464034626727208
      ],
      "excerpt": "set of hyperparameters (slightly different than the paper) which consistently \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.948449000618743
      ],
      "excerpt": "This model is also implemented and documented in run_squad.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8891219678559061
      ],
      "excerpt": "./squad/predictions.json and the differences between the score of no answer (\"\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372778690978638,
        0.906410487396991
      ],
      "excerpt": "All experiments in the paper were fine-tuned on a Cloud TPU, which has 64GB of \ndevice RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likely \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8691661791673625
      ],
      "excerpt": "in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8449375585963451,
        0.9821854574878361
      ],
      "excerpt": "    up to 512, but you can fine-tune with a shorter max sequence length to save \n    substantial memory. This is controlled by the max_seq_length flag in our \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9214638880221687
      ],
      "excerpt": "train_batch_size: The memory usage is also directly proportional to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8681704457504041
      ],
      "excerpt": "Model type, BERT-Base vs. BERT-Large: The BERT-Large model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8757395539909691,
        0.9054863912051775
      ],
      "excerpt": "Optimizer: The default optimizer for BERT is Adam, which requires a lot \n    of extra memory to store the m and v vectors. Switching to a more memory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554360281760281
      ],
      "excerpt": "    results. We have not experimented with other optimizers for fine-tuning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8636009770668285
      ],
      "excerpt": "benchmarked the maximum batch size on single Titan X GPU (12GB RAM) with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9049123090814731,
        0.8872565401557901,
        0.9040977947262402
      ],
      "excerpt": "Unfortunately, these max batch sizes for BERT-Large are so small that they \nwill actually harm the model accuracy, regardless of the learning rate used. We \nare working on adding code to this repository which will allow much larger \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8983609529557343,
        0.950830463868145
      ],
      "excerpt": "    independent with respect to gradient computation (excluding batch \n    normalization, which is not used here). This means that the gradients of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8980449088214619
      ],
      "excerpt": "    intermediate activations in the forward pass that are necessary for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860844440800138,
        0.874230885421444
      ],
      "excerpt": "However, this is not implemented in the current release. \nFor sentence-level tasks (or sentence-pair) tasks, tokenization is very simple. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9093247939853658,
        0.9183060063782515,
        0.842462510524211
      ],
      "excerpt": "The basic procedure for sentence-level tasks is: \nInstantiate an instance of tokenizer = tokenization.FullTokenizer \nTokenize the raw text with tokens = tokenizer.tokenize(raw_text). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9327962082067193,
        0.8783971281161721
      ],
      "excerpt": "Add the [CLS] and [SEP] tokens in the right place. \nWord-level and span-level tasks (e.g., SQuAD and NER) are more complex, since \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8586384520214778,
        0.9432682736271463
      ],
      "excerpt": "because the input labels are character-based, and SQuAD paragraphs are often \nlonger than our maximum sequence length. See the code in run_squad.py to show \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860951600148883,
        0.8760208466645732
      ],
      "excerpt": "Before we describe the general recipe for handling word-level tasks, it's \nimportant to understand what exactly our tokenizer is doing. It has three main \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9195821794107909,
        0.9593001179567374
      ],
      "excerpt": "Text normalization: Convert all whitespace characters to spaces, and \n    (for the Uncased model) lowercase the input and strip out accent markers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8006216742491938,
        0.8592570136367486
      ],
      "excerpt": "    characters are defined as (a) Anything with a P* Unicode class, (b) any \n    non-letter/number/space ASCII character (e.g., characters like $ which are \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9213122341288262,
        0.9554025773396336
      ],
      "excerpt": "WordPiece tokenization: Apply whitespace tokenization to the output of \n    the above procedure, and apply \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9731157466740481,
        0.8906267721862793
      ],
      "excerpt": "    tokenization to each token separately. (Our implementation is directly based \n    on the one from tensor2tensor, which is linked). E.g., john johanson ' s \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9900337862972242
      ],
      "excerpt": "The advantage of this scheme is that it is \"compatible\" with most existing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9590021914118997
      ],
      "excerpt": "Johanson's house (with no space before the 's). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8380777476960225
      ],
      "excerpt": "original-to-tokenized alignment: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for orig_token in orig_tokens: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8662671694114575
      ],
      "excerpt": "Now orig_to_tok_map can be used to project labels to the tokenized \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9479174328477591,
        0.9216996404669306,
        0.871840490443295,
        0.8457142113378266
      ],
      "excerpt": "We are releasing code to do \"masked LM\" and \"next sentence prediction\" on an \narbitrary text corpus. Note that this is not the exact code that was used for \nthe paper (the original code was written in C++, and had some additional \ncomplexity), but this code does generate pre-training data as described in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.847899130758606,
        0.9765797580734363
      ],
      "excerpt": "may want to intentionally add a slight amount of noise to your input data (e.g., \nrandomly truncate 2% of input segments) to make it more robust to non-sentential \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9249587207553038
      ],
      "excerpt": "The max_predictions_per_seq is the maximum number of masked LM predictions per \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9103525487745373
      ],
      "excerpt": "pre-training from scratch. The model configuration (including vocab size) is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9626157146589479
      ],
      "excerpt": "num_train_steps to 10000 steps or more. The max_seq_length and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8304582168218955
      ],
      "excerpt": "will overfit that data in only a few steps and produce unrealistically high \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9204023944425157
      ],
      "excerpt": "    out-of-bounds access. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8029661711307563
      ],
      "excerpt": "    additional steps of pre-training on your corpus, starting from the BERT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9259645864047299,
        0.8142846854293876,
        0.8277095986859018,
        0.8567880065917519,
        0.9740042200769209,
        0.9857163122955201,
        0.819175809986933
      ],
      "excerpt": "Current BERT models are English-only, but we do plan to release a \n    multilingual model which has been pre-trained on a lot of languages in the \n    near future (hopefully by the end of November 2018). \nLonger sequences are disproportionately expensive because attention is \n    quadratic to the sequence length. In other words, a batch of 64 sequences of \n    length 512 is much more expensive than a batch of 256 sequences of \n    length 128. The fully-connected/convolutional cost is the same, but the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9242245339320695,
        0.8519722696374786,
        0.8880730747943449
      ],
      "excerpt": "    good recipe is to pre-train for, say, 90,000 steps with a sequence length of \n    128 and then for 10,000 additional steps with a sequence length of 512. The \n    very long sequences are mostly needed to learn positional embeddings, which \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9759344651972562
      ],
      "excerpt": "    data twice with different values of max_seq_length. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9322533578802127
      ],
      "excerpt": "    scratch, our recommended recipe is to pre-train a BERT-Base on a single \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9765235790841873
      ],
      "excerpt": "    takes about 2 weeks at a cost of about $500 USD (based on the pricing in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9674460061612953,
        0.8216574495684017,
        0.8719214375692368
      ],
      "excerpt": "    on a single Cloud TPU, compared to what was used in the paper. It is \n    recommended to use the largest batch size that fits into TPU memory. \nWe will not be able to release the pre-processed datasets used in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9177513925443218
      ],
      "excerpt": "any necessary cleanup to convert it into plain text. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9734398359306347
      ],
      "excerpt": "is a somewhat smaller (200M word) collection of older books that are public \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9614037741921083
      ],
      "excerpt": "Common Crawl is another very large collection of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9289483602467198,
        0.9813661240892869,
        0.8885054263357111,
        0.9278410408965626,
        0.8059450845500326,
        0.9535268535954139
      ],
      "excerpt": "This repository does not include code for learning a new WordPiece vocabulary. \nThe reason is that the code used in the paper was implemented in C++ with \ndependencies on Google's internal libraries. For English, it is almost always \nbetter to just start with our vocabulary and pre-trained models. For learning \nvocabularies of other languages, there are a number of open source options \navailable. However, keep in mind that these are not compatible with our \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9544526677778359,
        0.9567588029116127,
        0.8738301598826457
      ],
      "excerpt": "get started with the notebook \n\"BERT FineTuning with Cloud TPUs\". \nAt the time of this writing (October 31st, 2018), Colab users can access a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9122323663558507,
        0.8092093521735889,
        0.8828868086925022,
        0.9772341422295684
      ],
      "excerpt": "purchased with free credit for signing up with GCP), and this capability may not \nlonger be available in the future. Click on the BERT Colab that was just linked \nfor more information. \nYes, all of the code in this repository works out-of-the-box with CPU, GPU, and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8537305359937865
      ],
      "excerpt": "There is no official PyTorch implementation. However, NLP researchers from \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8258402037175955,
        0.989243237034123,
        0.9810078658153465,
        0.9214797454662798
      ],
      "excerpt": "PyTorch version of BERT available \nwhich is compatible with our pre-trained checkpoints and is able to reproduce \nour results. We were not involved in the creation or maintenance of the PyTorch \nimplementation so please direct any questions towards the authors of that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.989243237034123,
        0.9717539405548656,
        0.9214797454662798,
        0.9267457232337122
      ],
      "excerpt": "which is compatible with our pre-trained checkpoints and is able to reproduce \nour results. We were not involved in the creation or maintenance of the Chainer \nimplementation so please direct any questions towards the authors of that \nYes, we plan to release a multi-lingual BERT model in the near future. We cannot \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9249392977572544
      ],
      "excerpt": "be a single model which includes most of the languages which have a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8611265138420187
      ],
      "excerpt": "So far we have not attempted to train anything larger than BERT-Large. It is \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Shinya-Kouda/kgc/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "See the section on [out-of-memory issues](#out-of-memory-issues) for more\ninformation.\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 00:45:00 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Shinya-Kouda/kgc/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Shinya-Kouda/kgc",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Shinya-Kouda/kgc/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Shinya-Kouda/kgc/master/trivialmaker_1.sh",
      "https://raw.githubusercontent.com/Shinya-Kouda/kgc/master/run.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In certain cases, rather than fine-tuning the entire pre-trained model\nend-to-end, it can be beneficial to obtained *pre-trained contextual\nembeddings*, which are fixed contextual representations of each input token\ngenerated from the hidden layers of the pre-trained model. This should also\nmitigate most of the out-of-memory issues.\n\nAs an example, we include the script `extract_features.py` which can be used\nlike this:\n\n```shell\n#: Sentence A and Sentence B are separated by the ||| delimiter for sentence\n#: pair tasks like question answering and entailment.\n#: For single sentence inputs, put one sentence per line and DON'T use the\n#: delimiter.\necho 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt\n\npython extract_features.py \\\n  --input_file=/tmp/input.txt \\\n  --output_file=/tmp/output.jsonl \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --layers=-1,-2,-3,-4 \\\n  --max_seq_length=128 \\\n  --batch_size=8\n```\n\nThis will create a JSON file (one line per line of input) containing the BERT\nactivations from each Transformer layer specified by `layers` (-1 is the final\nhidden layer of the Transformer, etc.)\n\nNote that this script will produce very large output files (by default, around\n15kb for every input token).\n\nIf you need to maintain alignment between the original and tokenized words (for\nprojecting training labels), see the [Tokenization](#tokenization) section\nbelow.\n\n**Note:** You may see a message like `Could not find trained model in model_dir:\n/tmp/tmpuB5g5c, running initialization to predict.` This message is expected, it\njust means that we are using the `init_from_checkpoint()` API rather than the\nsaved model API. If you don't specify a checkpoint or specify an invalid\ncheckpoint, this script will complain.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.901378974370353
      ],
      "excerpt": "PyTorch version of BERT available \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8939543471640379
      ],
      "excerpt": "We are releasing the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8705727502454048
      ],
      "excerpt": "The links to the models are here (right-click, 'Save link as...' on the name): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.832942711007399
      ],
      "excerpt": "The fine-tuning examples which use BERT-Base should be able to run on a GPU \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9055778381242784
      ],
      "excerpt": "on your local machine, using a GPU like a Titan X or GTX 1080. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8365614543619845
      ],
      "excerpt": "might use the following flags instead: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8969836140871077
      ],
      "excerpt": "You should see output like this: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8013849216719517
      ],
      "excerpt": "Once you have trained your classifier you can use it in inference mode by using \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902691695313062
      ],
      "excerpt": "To run on SQuAD, you will first need to download the dataset. The \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8732459088273746
      ],
      "excerpt": "Download these to some directory $SQUAD_DIR. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8312752112104692
      ],
      "excerpt": "not seem to fit on a 12GB GPU using BERT-Large). However, a reasonably strong \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9026988408675386
      ],
      "excerpt": "For example, one random run with these parameters produces the following Dev \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8623730222577939
      ],
      "excerpt": "To run on SQuAD 2.0, you will first need to download the dataset. The necessary \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8732459088273746
      ],
      "excerpt": "Download these to some directory $SQUAD_DIR. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8745883380510069
      ],
      "excerpt": "We assume you have copied everything from the output directory to a local \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8002388752908777
      ],
      "excerpt": "derived threshold or alternatively you can extract the appropriate answers from \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8965978438992003,
        0.8156541010479084
      ],
      "excerpt": "effective batch sizes to be used on the GPU. The code will be based on one (or \nboth) of the following techniques: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8447287424360156
      ],
      "excerpt": "Punctuation splitting: Split all punctuation characters on both sides \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8868624236798442
      ],
      "excerpt": "    on the one from tensor2tensor, which is linked). E.g., john johanson ' s \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8013720866702825
      ],
      "excerpt": "If you have a pre-tokenized representation with word-level annotations, you can \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8010923493389237
      ],
      "excerpt": "to both scripts). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9174657358653188
      ],
      "excerpt": "    you will likely get NaNs when training on GPU or TPU due to unchecked \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8315401861310665,
        0.8228179825376202
      ],
      "excerpt": "If you want to use BERT with Colab, you can \nget started with the notebook \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.901378974370353
      ],
      "excerpt": "PyTorch version of BERT available \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8229105942931498
      ],
      "excerpt": "possible that we will release larger models if we are able to obtain significant \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8431384658842261,
        0.806369134132436
      ],
      "excerpt": "mask. For example: \nInput Text: the man jumped up , put his basket on phil ##am ##mon ' s head \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802614381703055
      ],
      "excerpt": "Whole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9358742074671855
      ],
      "excerpt": "--do_whole_word_mask=True to create_pretraining_data.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8070397835965666
      ],
      "excerpt": "run_classifier_with_tfhub.py for an example of how to use the TF Hub module, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8033173742302853,
        0.8307078900182555
      ],
      "excerpt": "Each .zip file contains three items: \nA TensorFlow checkpoint (bert_model.ckpt) containing the pre-trained \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8434778523624548,
        0.8594142235991984
      ],
      "excerpt": "the following flags to run_classifier.py or run_squad.py: \n--use_tpu=True \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8052155650180225
      ],
      "excerpt": "On Cloud TPUs, the pretrained model and the output directory will need to be on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8792739494876829
      ],
      "excerpt": "Storage folder gs://bert_models/2018_10_18. For example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.803936889558175
      ],
      "excerpt": "Before running this example you must download the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python run_classifier.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8594142235991984
      ],
      "excerpt": "  --do_train=true \\ \n  --do_eval=true \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8717657410500496,
        0.8833890096851529,
        0.9067003692325829
      ],
      "excerpt": "the --do_predict=true command. You need to have a file named test.tsv in the \ninput folder. Output will be created in file called test_results.tsv in the \noutput folder. Each line will contain output for each sample, columns are the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python run_classifier.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  --do_predict=true \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8399442795104358
      ],
      "excerpt": "To run on SQuAD, you will first need to download the dataset. The \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python run_squad.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8445146240188418,
        0.8594142235991984
      ],
      "excerpt": "  --do_train=True \\ \n  --train_file=$SQUAD_DIR/train-v1.1.json \\ \n  --do_predict=True \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8313360442840957
      ],
      "excerpt": "python $SQUAD_DIR/evaluate-v1.1.py $SQUAD_DIR/dev-v1.1.json ./squad/predictions.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python run_squad.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8445146240188418,
        0.8594142235991984
      ],
      "excerpt": "  --do_train=True \\ \n  --train_file=$SQUAD_DIR/train-v1.1.json \\ \n  --do_predict=True \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  --use_tpu=True \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8116251671288147
      ],
      "excerpt": "To run on SQuAD 2.0, you will first need to download the dataset. The necessary \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python run_squad.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8621491559118878,
        0.8594142235991984
      ],
      "excerpt": "  --do_train=True \\ \n  --train_file=$SQUAD_DIR/train-v2.0.json \\ \n  --do_predict=True \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  --use_tpu=True \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  --version_2_with_negative=True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8157897466077849,
        0.8634624883709842
      ],
      "excerpt": "python $SQUAD_DIR/evaluate-v2.0.py $SQUAD_DIR/dev-v2.0.json \n./squad/predictions.json --na-prob-file ./squad/null_odds.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python run_squad.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621491559118878,
        0.8594142235991984
      ],
      "excerpt": "  --train_file=$SQUAD_DIR/train-v2.0.json \\ \n  --do_predict=True \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  --use_tpu=True \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  --version_2_with_negative=True \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8415959090367319
      ],
      "excerpt": "Using the default training scripts (run_classifier.py and run_squad.py), we \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8610216970993526
      ],
      "excerpt": "Just follow the example code in run_classifier.py and extract_features.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8035286142206224
      ],
      "excerpt": "Tokenize the raw text with tokens = tokenizer.tokenize(raw_text). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289669050403863
      ],
      "excerpt": ":#:#: Output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    vocab_file=vocab_file, do_lower_case=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8984091520580629
      ],
      "excerpt": "Here's how to run the data generation. The input is a plain text file, with one \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9298632997506416
      ],
      "excerpt": "is a set of tf.train.Examples serialized into TFRecord file format. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8810033413874717
      ],
      "excerpt": "spaCy. The create_pretraining_data.py script will \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8256508921076318,
        0.8652506211663116
      ],
      "excerpt": "python create_pretraining_data.py \\ \n  --input_file=./sample_text.txt \\ \n  --output_file=/tmp/tf_examples.tfrecord \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  --do_lower_case=True \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811854372964597
      ],
      "excerpt": "  --masked_lm_prob=0.15 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8848236232504478
      ],
      "excerpt": "same as create_pretraining_data.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8372363289007108
      ],
      "excerpt": "python run_pretraining.py \\ \n  --input_file=/tmp/tf_examples.tfrecord \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8594142235991984
      ],
      "excerpt": "  --do_train=True \\ \n  --do_eval=True \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.824848401448175,
        0.8229325858675496
      ],
      "excerpt": "extract the text with \nWikiExtractor.py, and then apply \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Shinya-Kouda/kgc/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'\\n                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERT",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "kgc",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Shinya-Kouda",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Shinya-Kouda/kgc/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 00:45:00 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "See the section on [out-of-memory issues](#out-of-memory-issues) for more\ninformation.\n\n",
      "technique": "Header extraction"
    }
  ]
}