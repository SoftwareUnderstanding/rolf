{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* To his immense credit, my colleague, Xuanqing Liu (https://xuanqing94.github.io/), did an awesome job on the code development.  \n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        0.9986079747246374
      ],
      "excerpt": "Paper at: https://arxiv.org/pdf/2002.07971.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9848179726092176
      ],
      "excerpt": "0 qid:10 1:2 2:0 3:0 4:0 5:2 6:0.666667 7:0 8:0 9:0 10:0.666667 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sbadirli/GrowNet",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Feel free to drop me an email if you have any questions: s.badirli@gmail.com\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-11T23:36:06Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T13:53:12Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9180987644698428
      ],
      "excerpt": "Original PyTorch implementation of \"Gradient Boosting Neural Networks: GrowNet\"  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8301796048698121
      ],
      "excerpt": "Data Loading pipeline for L2R task is implemented by taking Microsoft (MSLR-WEB10K) dataset as a baseline. We also converted Yahoo into tis format (The jupyter notebook \"yahoo2mslr\" does this conversion). Thus if you want to use some other L2R datasets with GrowNet, please convert it into MSLR format. Below you can find a simple sample with just 10 features: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9540090680030336,
        0.935093375493537
      ],
      "excerpt": "The first feature is label, second is query id and the rest are data features. \nTraining and test splits of regression datasets are done in jupyter notebook \"reg_train_test_split\". The data link already contains splitted data. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sbadirli/GrowNet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 34,
      "date": "Sat, 25 Dec 2021 19:45:56 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sbadirli/GrowNet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "sbadirli/GrowNet",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/sbadirli/GrowNet/master/reg_train_test_split.ipynb",
      "https://raw.githubusercontent.com/sbadirli/GrowNet/master/yahoo2mslr.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/sbadirli/GrowNet/master/Regression/train.sh",
      "https://raw.githubusercontent.com/sbadirli/GrowNet/master/Classification/train.sh",
      "https://raw.githubusercontent.com/sbadirli/GrowNet/master/L2R/train.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run the code, You may create a conda environment (assuming you already have miniconda3 installed) by the following command on terminal:\n\n```\nconda create --name grownet --file requirements.txt\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9266325276809323,
        0.9770335174395833,
        0.8251783302224124
      ],
      "excerpt": "To reproduce the results from pape, first activate conda virtual environment \nconda activate grownet \nThen simply navigate to the task folder: Classification, L2R or Regression and execute the following command on terminal: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9040392588607754
      ],
      "excerpt": "  <img width=\"800\" src=\"Model.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8517257378551688
      ],
      "excerpt": "The original HIGGS data is splitted into train and test (same as done in XGBoost paper) using higgs2libsvm.py script. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sbadirli/GrowNet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "GrowNet",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "GrowNet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "sbadirli",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sbadirli/GrowNet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code was implemented in Python 3.6.10 and utilized the packages (full list) in requirements.txt file. The platform I used was linux-64. Most important packages you need are the followings:\n```\ncudatoolkit=10.1.243 \nnumpy=1.18.1 \npandas=1.0.0 \npython=3.6.10 \npytorch=1.4.0 \n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 135,
      "date": "Sat, 25 Dec 2021 19:45:56 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In this paper, we combine the power of gradient boosting with the flexibility and\nversatility of neural networks and introduce a new modelling paradigm called GrowNet that can\nbuild up a DNN layer by layer. Instead of decision trees, we use shallow neural networks as our\nweak learners in a general gradient boosting framework that can be applied to a wide variety of tasks\nspanning classification, regression and ranking. We introduce further innovations like adding second\norder statistics to the training process, and also including a global corrective step that has been shown,\nboth in theory and in empirical evaluation, to provide performance lift and precise fine-tuning to the specific task at hand.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}