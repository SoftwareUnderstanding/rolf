{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1606.00915",
      "https://arxiv.org/abs/1706.05587",
      "https://arxiv.org/abs/1802.02611",
      "https://arxiv.org/abs/1706.05587, 2017.\n\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, H. Adam. [Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1802.02611). https://arxiv.org/abs/1802.02611, 2018.\n\n## To-Do List\n\n- [x] Test script for new arbitrary test images.",
      "https://arxiv.org/abs/1802.02611, 2018.\n\n## To-Do List\n\n- [x] Test script for new arbitrary test images."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. [Deeplab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs](https://arxiv.org/abs/1606.00915). TPAMI, 2017.\n\nL.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. [Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587). arXiv:1706.05587, 2017.\n\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, H. Adam. [Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1802.02611). arXiv:1802.02611, 2018.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8714162992508173,
        0.8450045395698996
      ],
      "excerpt": "Lei Mao, Shengjie Lin \nUniversity of Chicago \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "\u251c\u2500\u2500 nets \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/leimao/DeepLab-V3",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-08-18T05:06:45Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-06T09:32:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "DeepLab is a series of image semantic segmentation models, whose latest version, i.e. v3+, proves to be the state-of-art. Its major contribution is the use of atrous spatial pyramid pooling (ASPP) operation at the end of the encoder. While the model works extremely well, its open source code is hard to read (at least from my personal perspective). Here we re-implemented DeepLab V3, the earlier version of v3+ (which only additionally employs the decoder architecture), in a much simpler and more understandable way.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "University of Chicago \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8244036672328247
      ],
      "excerpt": "                        Random seed for model training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9809347752696628
      ],
      "excerpt": "With learning rate of 1e-5, the mIOU could be greater 0.7 after 20 epochs, which is comparable to the test statistics of DeepLab V3 in the publication. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Google DeepLab V3 for Image Semantic Segmentation",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Download and extract VOC2012 dataset, SBD dataset, and pretrained models to designated directories.\n\n```bash\n$ python download.py --help\nusage: download.py [-h] [--downloads_dir DOWNLOADS_DIR] [--data_dir DATA_DIR]\n                   [--pretrained_models_dir PRETRAINED_MODELS_DIR]\n                   [--pretrained_models PRETRAINED_MODELS [PRETRAINED_MODELS ...]]\n\nDownload DeepLab semantic segmentation datasets and pretrained backbone\nmodels.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --downloads_dir DOWNLOADS_DIR\n                        Downloads directory\n  --data_dir DATA_DIR   Data directory\n  --pretrained_models_dir PRETRAINED_MODELS_DIR\n                        Pretrained models directory\n  --pretrained_models PRETRAINED_MODELS [PRETRAINED_MODELS ...]\n                        Pretrained models to download: resnet_50, resnet_101,\n                        mobilenet_1.0_224\n```\n\nFor example, to download and extract datasets and models into directories specified:\n\n```bash\n$ python download.py --downloads_dir ./downloads --data_dir ./data --pretrained_models_dir ./models/pretrained --pretrained_models resnet_50 resnet_101 mobilenet_1.0_224\n```\n\nFor simplicity, please just run the following command in terminal:\n\n```bash\n$ python download.py\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/leimao/DeepLab-V3/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 29,
      "date": "Mon, 27 Dec 2021 20:34:52 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/leimao/DeepLab-V3/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "leimao/DeepLab-V3",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/leimao/DeepLab-V3/master/nets/mobilenet/mobilenet_example.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/leimao/DeepLab-V3/master/install_dependencies.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To install dependencies, please run the following command to install everything required automatically:\n\n```bash\n$ chmod +x install_dependencies.sh\n$ pip install -r requirements.txt\n$ ./install_dependencies.sh\n```\nIf found permission problems, please run the following command instead:\n\n```bash\n$ chmod +x install_dependencies.sh\n$ pip install -r requirements.txt\n$ sudo ./install_dependencies.sh\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8397331684428114
      ],
      "excerpt": "                        Labels directory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8397331684428114
      ],
      "excerpt": "                        Labels augmented directory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8252946352745409
      ],
      "excerpt": "  --log_dir LOG_DIR     TensorBoard log directory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9190937605128905
      ],
      "excerpt": "For simplicity, please run the following command in terminal: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9355354798037551,
        0.9336801098518991
      ],
      "excerpt": "\u251c\u2500\u2500 download.py \n\u251c\u2500\u2500 feature_extractor.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.91892912920148,
        0.8053002471203892
      ],
      "excerpt": "\u251c\u2500\u2500 model.py \n\u251c\u2500\u2500 modules.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991,
        0.950563948951535,
        0.9586232994076559
      ],
      "excerpt": "\u251c\u2500\u2500 test_demo.py \n\u251c\u2500\u2500 test_any_image.py \n\u251c\u2500\u2500 train.py \n\u2514\u2500\u2500 utils.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9355986061254542,
        0.9632270575298659
      ],
      "excerpt": "$ python train.py --help \nusage: train.py [-h] [--network_backbone NETWORK_BACKBONE] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8610810190543553
      ],
      "excerpt": "                        Pretrained model directory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9252130783084016
      ],
      "excerpt": "                        Train dataset filename \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8771749719064245
      ],
      "excerpt": "                        Validation dataset filename \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.809096065928089
      ],
      "excerpt": "                        Images directory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8717306966045991
      ],
      "excerpt": "                        Train augmented dataset filename \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8284818608417835
      ],
      "excerpt": "                        Trained model saving directory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.825382310827872
      ],
      "excerpt": "                        Random seed for model training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "$ python train.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/leimao/DeepLab-V3/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'\\nThe MIT License (MIT)\\n\\nCopyright (c) 2018 \\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepLab V3",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepLab-V3",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "leimao",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/leimao/DeepLab-V3/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python 3.5\n* TensorFlow 1.8\n* Tqdm 4.26.0\n* Numpy 1.14\n* OpenCV 3.4.3\n* Pillow 5.3.0\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To install dependencies, please run the following command to install everything required automatically:\n\n```bash\n$ chmod +x install_dependencies.sh\n$ pip install -r requirements.txt\n$ ./install_dependencies.sh\n```\nIf found permission problems, please run the following command instead:\n\n```bash\n$ chmod +x install_dependencies.sh\n$ pip install -r requirements.txt\n$ sudo ./install_dependencies.sh\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 86,
      "date": "Mon, 27 Dec 2021 20:34:52 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "semantic-segmentation",
      "deeplab",
      "computer-vision"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To show some demos, please run the following command in terminal:\n\n```bash\n$ python test_demo.py\n```\n\nImage| Label | Prediction |\n:-------------------------:|:-------------------------:|:-------------------------:\n![](data/demos/deeplab/resnet_101_voc2012/image_0.jpg)  |  ![](data/demos/deeplab/resnet_101_voc2012/image_0_label.png) |  ![](data/demos/deeplab/resnet_101_voc2012/image_0_prediction.png)\n\nImage| Label | Prediction |\n:-------------------------:|:-------------------------:|:-------------------------:\n![](data/demos/deeplab/resnet_101_voc2012/image_1.jpg)  |  ![](data/demos/deeplab/resnet_101_voc2012/image_1_label.png) |  ![](data/demos/deeplab/resnet_101_voc2012/image_1_prediction.png)\n\nImage| Label | Prediction |\n:-------------------------:|:-------------------------:|:-------------------------:\n![](data/demos/deeplab/resnet_101_voc2012/image_2.jpg)  |  ![](data/demos/deeplab/resnet_101_voc2012/image_2_label.png) |  ![](data/demos/deeplab/resnet_101_voc2012/image_2_prediction.png)\n\nImage| Label | Prediction |\n:-------------------------:|:-------------------------:|:-------------------------:\n![](data/demos/deeplab/resnet_101_voc2012/image_3.jpg)  |  ![](data/demos/deeplab/resnet_101_voc2012/image_3_label.png) |  ![](data/demos/deeplab/resnet_101_voc2012/image_3_prediction.png)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Just put some JPG-format images into `demo_dir` and run the following command in the terminal.\n\n```bash\n$ python test_any_image.py\n```\nResults will be written into same folder. Make sure that proper model trained and a checkpoint is saved in `models_dir`. See the script for details.\n\nContributed by [pinaxe1](https://github.com/leimao/DeepLab_v3/pull/7). Will modify to accept arguments and multiple image formats.\n\n",
      "technique": "Header extraction"
    }
  ]
}