{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite as:\n\n``` bibtex\n@inproceedings{reid2021subformer,\n    title = {{S}ubformer: {E}xploring {W}eight {S}haring for {P}arameter {E}fficiency in {G}enerative {T}ransformers},\n    author = {Machel Reid and Edison Marrese-Taylor and Yutaka Matsuo},\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2021\",\n    month = nov,\n    year = \"2021\",\n    address = \"Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{reid2021subformer,\n    title = {{S}ubformer: {E}xploring {W}eight {S}haring for {P}arameter {E}fficiency in {G}enerative {T}ransformers},\n    author = {Machel Reid and Edison Marrese-Taylor and Yutaka Matsuo},\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2021\",\n    month = nov,\n    year = \"2021\",\n    address = \"Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{reid2021subformer,\n    title = {{S}ubformer: {E}xploring {W}eight {S}haring for {P}arameter {E}fficiency in {G}enerative {T}ransformers},\n    author = {Machel Reid and Edison Marrese-Taylor and Yutaka Matsuo},\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2021\",\n    month = nov,\n    year = \"2021\",\n    address = \"Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.99257753779862
      ],
      "excerpt": "If you used this code or found our work useful, please cite: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/machelreid/subformer/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/machelreid/subformer",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq)\nWe want to make contributing to this project as easy and transparent as\npossible.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nLicense\nBy contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq),\nyou agree that your contributions will be licensed under the LICENSE file in\nthe root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-24T23:29:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-10T08:08:12Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9814705686409335,
        0.9406312659133078,
        0.942207639853967
      ],
      "excerpt": "This repository contains the code for the Subformer. To help overcome this we propose the Subformer, allowing us to retain performance while reducing parameters in generative Transformers from 25% ~ 70%. The Subformer consists of the following two techniques: \nSandwich-style parameter sharing, in which we share all the layers in a block except the first and last. This allows us the use the central shared layers --\"sandwich module\" -- as a large representation learner (similar to BERT vs ALBERT) while the input and output model layers are able to focus on more specific representations for token prediction/generation while maintaining performance. \nFor our sequence to sequence tasks, we also introduce SAFE (self-attentive factorized embeddings), which help us reduce embedding parameters significantly, while still retaining performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "    --reduction-dim 320 #:for SAFE embeddings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "The code for the Subformer, from the EMNLP 2021 Findings paper: \"Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers\", by Machel Reid, Edison Marrese-Taylor, and Yutaka Matsuo",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/machelreid/subformer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sun, 26 Dec 2021 02:11:50 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/machelreid/subformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "machelreid/subformer",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/machelreid/subformer/tree/master/docs",
      "https://github.com/machelreid/subformer/tree/master/examples/simultaneous_translation/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/machelreid/subformer/master/scripts/compound_split_bleu.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/scripts/sacrebleu.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/language_model/prepare-wikitext-103.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/multilingual/finetune_multilingual_model.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/multilingual/multilingual_fairseq_gen.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/multilingual/train_multilingual_model.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/speech_recognition/datasets/prepare-librispeech.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/m2m_100/install_dependecies.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/m2m_100/tok.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/m2m_100/tokenizers/seg_ja.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/m2m_100/tokenizers/seg_ko.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/m2m_100/tokenizers/tokenizer_ar.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/criss/download_and_preprocess_tatoeba.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/criss/download_and_preprocess_flores_test.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/criss/sentence_retrieval/sentence_retrieval_tatoeba.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/criss/unsupervised_mt/eval.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/criss/mining/mine_example.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/byte_level_bpe/get_data.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/roberta/preprocess_GLUE_tasks.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/roberta/preprocess_RACE.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/roberta/commonsense_qa/download_cqa_data.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/backtranslation/sacrebleu.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/backtranslation/tokenized_bleu.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/backtranslation/prepare-wmt18en2de.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/backtranslation/prepare-de-monolingual.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/translation/prepare-wmt14en2fr.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/translation/prepare-iwslt14.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/translation/prepare-iwslt17-multilingual.sh",
      "https://raw.githubusercontent.com/machelreid/subformer/master/examples/translation/prepare-wmt14en2de.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "(As this code is based on [fairseq](https://github.com/ytorch/fairseq/), some installation instructions are taken straight from their README)\n\n* [PyTorch](http://pytorch.org/) version >= 1.5.0\n* Python version >= 3.6\n* For training new models, you'll also need an NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl)\n* To install and develop locally:\n\n``` bash\ngit clone https://github.com/machelreid/subformer\ncd subformer\npip install --e ./\n\n#: on MacOS:\n#: CFLAGS=\"-stdlib=libc++\" pip install --editable ./\n```\n\n* **For faster training** install NVIDIA's [apex](https://github.com/NVIDIA/apex) library:\n\n``` bash\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" \\\n  --global-option=\"--deprecated_fused_adam\" --global-option=\"--xentropy\" \\\n  --global-option=\"--fast_multihead_attn\" ./\n```\n\n* **For large datasets** install [PyArrow](https://arrow.apache.org/docs/python/install.html#using-pip): `pip install pyarrow`\n* If you use Docker make sure to increase the shared memory size either with `--ipc=host` or `--shm-size`\n as command line options to `nvidia-docker run` .\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py $DATA_BIN --arch transformer_wmt_en_de \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9072863254345287
      ],
      "excerpt": "python generate.py --path $CHECKPOINT --gen-subset $SPLIT --beam 5 --lenpen $LENPEN --batch-size 400 --remove-bpe \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "fairseq-train $DATA_BIN \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/machelreid/subformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Cython",
      "Lua",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) Facebook, Inc. and its affiliates.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Subformer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "subformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "machelreid",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/machelreid/subformer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "(As this code is based on [fairseq](https://github.com/ytorch/fairseq/), some installation instructions are taken straight from their README)\n\n* [PyTorch](http://pytorch.org/) version >= 1.5.0\n* Python version >= 3.6\n* For training new models, you'll also need an NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl)\n* To install and develop locally:\n\n``` bash\ngit clone https://github.com/machelreid/subformer\ncd subformer\npip install --e ./\n\n#: on MacOS:\n#: CFLAGS=\"-stdlib=libc++\" pip install --editable ./\n```\n\n* **For faster training** install NVIDIA's [apex](https://github.com/NVIDIA/apex) library:\n\n``` bash\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" \\\n  --global-option=\"--deprecated_fused_adam\" --global-option=\"--xentropy\" \\\n  --global-option=\"--fast_multihead_attn\" ./\n```\n\n* **For large datasets** install [PyArrow](https://arrow.apache.org/docs/python/install.html#using-pip): `pip install pyarrow`\n* If you use Docker make sure to increase the shared memory size either with `--ipc=host` or `--shm-size`\n as command line options to `nvidia-docker run` .\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Sun, 26 Dec 2021 02:11:50 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "nlp",
      "machine-learning"
    ],
    "technique": "GitHub API"
  }
}