{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1509.02971",
      "https://arxiv.org/abs/1509.02971](https://arxiv.org/abs/1509.02971) paper implementing a DDPG agent. It is an Actor-Critic method.\nThe algorithm helps the agent to act in an environment with a goal of solving the task defined by the environment as well as explore the environment in order to improve the agent's behaviour. The algorithm is also augmented with the fixed-Q target, double network, soft-updates and experience replay.\nThe agent exploits the initial lack of knowledge as well as Ornstein\u2013Uhlenbeck process-generated noise to explore the environment.\n\n\nThe hyperparameters selected for the demonstration are:\n\n- Actor learning rate: 0.0001\n- Critic learning rate: 0.0001\n- Update rate: 1\n- Memory size: 100000\n- Batch size: 64\n- Gamma: 0.99\n- Tau: 0.001\n- Adam weight decay: 0\n\n- Number of episodes: 200\n\nIt took the network 114 episodes to be able to perform with not less than score of 30 as an average of 100 episodes.\nTraining it different times give us lesser number of episodes to solve sometimes. \nIt can also be reduced by tuning the hyperparameters.\n\n## Plot of Rewards \n\n![](https://github.com/prajwalgatti/DRL-Continuous-Control/blob/master/plot.png)\n\nThe saved weights of the Actor and Critic networks can be found [here.](https://github.com/prajwalgatti/DRL-Continuous-Control/tree/master/savedmodels)\n\nFollow setup [here.](https://github.com/prajwalgatti/DRL-Continuous-Control/blob/master/Setup.md)\n\nTrain the network [here.](https://github.com/prajwalgatti/DRL-Continuous-Control/blob/master/Continuous_Control.ipynb)\n\n## Ideas for Future Work\n\n- Search for better hyperparameters of algorithm as well as neural network\n- Implement a state to state predictor to improve the explorative capabilities of the agent"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prajwalgatti/DRL-Continuous-Control",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-18T19:42:31Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-02-03T08:17:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "![](https://user-images.githubusercontent.com/10624937/43851024-320ba930-9aff-11e8-8493-ee547c6af349.gif \"Trained Agent\")\n\nIn this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n\nThe observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.\n\nThis projects implements DDPG for continous control for the [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9720349390018101,
        0.980267152561714,
        0.909532384498047,
        0.8737395798580185
      ],
      "excerpt": "The reinforcement learning agent implementation follows the ideas of arXiv:1509.02971 paper implementing a DDPG agent. It is an Actor-Critic method. \nThe algorithm helps the agent to act in an environment with a goal of solving the task defined by the environment as well as explore the environment in order to improve the agent's behaviour. The algorithm is also augmented with the fixed-Q target, double network, soft-updates and experience replay. \nThe agent exploits the initial lack of knowledge as well as Ornstein\u2013Uhlenbeck process-generated noise to explore the environment. \nThe hyperparameters selected for the demonstration are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9847244070534777,
        0.8442848621536944
      ],
      "excerpt": "It took the network 114 episodes to be able to perform with not less than score of 30 as an average of 100 episodes. \nTraining it different times give us lesser number of episodes to solve sometimes.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8939004018843169
      ],
      "excerpt": "The saved weights of the Actor and Critic networks can be found here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Reinforcement Learning agent using DDPG implementation for continuous control of the Reacher environment.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prajwalgatti/DRL-Continuous-Control/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 20 Dec 2021 10:41:40 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/prajwalgatti/DRL-Continuous-Control/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "prajwalgatti/DRL-Continuous-Control",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/prajwalgatti/DRL-Continuous-Control/master/Crawler.ipynb",
      "https://raw.githubusercontent.com/prajwalgatti/DRL-Continuous-Control/master/Continuous_Control.ipynb",
      "https://raw.githubusercontent.com/prajwalgatti/DRL-Continuous-Control/master/.ipynb_checkpoints/Continuous_Control-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8959483779927219
      ],
      "excerpt": "Follow setup here. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/prajwalgatti/DRL-Continuous-Control/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "ASP.NET",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Description",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DRL-Continuous-Control",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "prajwalgatti",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prajwalgatti/DRL-Continuous-Control/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 20 Dec 2021 10:41:40 GMT"
    },
    "technique": "GitHub API"
  }
}