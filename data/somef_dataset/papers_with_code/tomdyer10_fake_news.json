{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762\n\nIntegrated Gradients - https://arxiv.org/abs/1703.01365\n\nCaptum - https://captum.ai/tutorials/\n\nFastAI + transformers - https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2\n\nIntrospective Rationale (paper",
      "https://arxiv.org/abs/1703.01365\n\nCaptum - https://captum.ai/tutorials/\n\nFastAI + transformers - https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2\n\nIntrospective Rationale (paper"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8103998117777721
      ],
      "excerpt": "I have included my starter notebook on this topic, based on the github repo and research paper below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9944484218006108,
        0.9944484218006108
      ],
      "excerpt": "BERT - https://arxiv.org/abs/1706.03762 \nIntegrated Gradients - https://arxiv.org/abs/1703.01365 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9072564759784449,
        0.9698834407712852
      ],
      "excerpt": "Introspective Rationale (paper) - http://people.csail.mit.edu/tommi/papers/YCZJ_EMNLP2019.pdf \nIntrospective Ratinale (interpret-text github) - https://github.com/interpretml/interpret-text \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tomdyer10/fake_news",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-08T13:51:40Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-23T16:55:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9180569186939845,
        0.9833190140569085
      ],
      "excerpt": "This project focusses on applying a combination of BERT transformers from Hugging face, the Fast AI API library and various interpretability methods, including applying integrated gradients through Captum, to the Kaggle fake news dataset. \nModel interpretability is growing in importance in a world increasingly governed by automated decisions. It is of particular value in a field such as 'fake news', where we are applying a boolean attribute to very complex attributes such as truth and reliability. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9181371878687146,
        0.8805081003832621,
        0.9030724032151506,
        0.9922487508353706
      ],
      "excerpt": "This repository demonstrates: \nTraining models using the BERT transformer and pytorch on labelled reliable/unreliable news sources. \nCombining the Fast AI library and Transformers from the hugging face library to achieve high performance (would have placed 1st in Kaggle competition) and benefit from the ease of use of Fast AI, particularly in model training. \nModel interpretability using hooks and the Captum library, based on \"Axiomatic Attribution for Deep Networks\". \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9707504837998435
      ],
      "excerpt": "Bert Transformer applied to a FastAI learner object. Model fine tuned to performance of >99% accuracy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9466593581591229
      ],
      "excerpt": "Application of the Captum library and layered integrated gradients for model interpretability. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9939570262861003,
        0.9274690642326324
      ],
      "excerpt": "In future I am very interested in Introspective Rationale and '3 player models' for text interpretability. This requires a generator model which selects text snippets to train two classifier models on. Aiming to optimise the accuracy of the model which is fed the selected text and minimise the accuracy of models fed deselected text.  \nI have included my starter notebook on this topic, based on the github repo and research paper below. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tomdyer10/fake_news/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 23:52:00 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/tomdyer10/fake_news/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "tomdyer10/fake_news",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/tomdyer10/fake_news/master/BERT_Classifieir.ipynb",
      "https://raw.githubusercontent.com/tomdyer10/fake_news/master/FastAI%2BBERT.ipynb",
      "https://raw.githubusercontent.com/tomdyer10/fake_news/master/captum_interp.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8892618668272806
      ],
      "excerpt": "FastAI + transformers - https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8564053107084612
      ],
      "excerpt": "Introspective Ratinale (interpret-text github) - https://github.com/interpretml/interpret-text \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/tomdyer10/fake_news/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "fake_news",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "fake_news",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "tomdyer10",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tomdyer10/fake_news/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sat, 25 Dec 2021 23:52:00 GMT"
    },
    "technique": "GitHub API"
  }
}