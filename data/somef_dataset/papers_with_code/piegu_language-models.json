{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.920594315055738
      ],
      "excerpt": "Blog post: NLP nas empresas | Como criar um modelo BERT de Question-Answering (QA) de desempenho aprimorado com AdapterFusion? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920594315055738
      ],
      "excerpt": "Blog post: NLP nas empresas | Como ajustar um modelo de linguagem natural como BERT para a tarefa de Question-Answering (QA) com um Adapter? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9777880979389272
      ],
      "excerpt": "Blog post: NLP nas empresas | Como ajustar um modelo de linguagem natural como BERT para a tarefa de classifica\u00e7\u00e3o de tokens (NER) com um Adapter? \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/piegu/language-models",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-16T17:18:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-10T13:58:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9158927413927566,
        0.8387620777178207
      ],
      "excerpt": "Repository of pre-trained Language Models and NLP models. \nBlog post: NLP nas empresas | T\u00e9cnicas para acelerar modelos de Deep Learning para infer\u00eancia em produ\u00e7\u00e3o \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8210164502825567
      ],
      "excerpt": "notebook OCR_DeepLearning_Tesseract_DocTR_colab.ipynb (nbviewer of the notebook) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9882455949864069
      ],
      "excerpt": "notebook question_answering_adapter_fusion.ipynb (nbviewer of the notebook): finetuning a MLM (Masked Language Model) like BERT (base or large) with the library adapter-transformers on the Question Answering task (QA) with AdapterFusion \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9748683639918245
      ],
      "excerpt": "notebooks question_answering_adapter.ipynb (nbviewer of the notebook) and question_answering_adapter_script.ipynb (nbviewer of the notebook): finetuning a MLM (Masked Language Model) like BERT (base or large) with the library adapter-transformers on the Question Answering task (QA) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9799505261569361
      ],
      "excerpt": "notebook token_classification_adapter.ipynb (nbviewer of the notebook): finetuning a MLM (Masked Language Model) like BERT (base or large) with the library adapter-transformers on the Token Classification task (NER) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9742398874345625
      ],
      "excerpt": "notebook language_modeling_adapter.ipynb (nbviewer of the notebook): finetuning a MLM (Masked Language Model) like BERT (base or large) with the library adapter-transformers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9377080925638318
      ],
      "excerpt": "notebook question_answering_BERT_large_cased_squad_v11_pt.ipynb (nbviewer of the notebook): training code of a Portuguese BERT large cased QA (Question Answering), finetuned on SQUAD v1.1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9870188901984009,
        0.9705931723871885,
        0.8304214616760057,
        0.838919843665301,
        0.9273984782314565
      ],
      "excerpt": "Model in the Model Hub of Hugging Face: Portuguese BERT large cased QA (Question Answering), finetuned on SQUAD v1.1 \nSummary: In some cases, it may be crucial to enrich the vocabulary of an already trained natural language model with vocabulary from a specialized domain (medicine, law, etc.) in order to perform new tasks (classification, NER, summary, translation, etc.). While the Hugging Face library allows you to easily add new tokens to the vocabulary of an existing tokenizer like BERT WordPiece, those tokens must be whole words, not subwords. This article explains why and how to obtain these new tokens from a specialized corpus. \n- notebook nlp_how_to_add_a_domain_specific_vocabulary_new_tokens_to_a_subword_tokenizer_already_trained_like_BERT_WordPiece.ipynb (nbviewer of the notebook) (notebook in colab) \n- Blog post: NLP | How to add a domain-specific vocabulary (new tokens) to a subword tokenizer already trained like BERT WordPiece \nnotebook colab_question_answering_BERT_base_cased_squad_v11_pt.ipynb (nbviewer of the notebook): training code of a Portuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9873370831464894,
        0.9150382985271672,
        0.9942191852387673,
        0.9717899046901379,
        0.8929969698830342
      ],
      "excerpt": "Model in the Model Hub of Hugging Face: Portuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1 \nI trained 1 Portuguese Bidirectional Language Model (PBLM) with the MultiFit configuration with 1 NVIDIA GPU v100 on GCP. \nWARNING: a Bidirectional LM model using the MultiFiT configuration is a good model to perform text classification but with only 46 millions of parameters, it is far from being a LM that can compete with GPT-2 or BERT in NLP tasks like text generation. This my next step ;-)  \nNote: The training times shown in the tables on this page are the sum of the creation time of Fastai Databunch (forward and backward) and the training duration of the bidirectional model over 10 periods. The download time of the Wikipedia corpus and its preparation time are not counted. \nnotebook lm3-portuguese.ipynb (nbviewer of the notebook): code used to train a Portuguese Bidirectional LM on a 100 millions corpus extrated from Wikipedia by using the MultiFiT configuration. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9279094311350803,
        0.8792135048001944,
        0.9455009917986378,
        0.9752363906781627,
        0.8890695358898947
      ],
      "excerpt": "notebook lm3-portuguese-classifier-TCU-jurisprudencia.ipynb (nbviewer of the notebook): code used to fine-tune a Portuguese Bidirectional LM and a Text Classifier on \"(reduzido) TCU jurisprud\u00eancia\" dataset. \nnotebook lm3-portuguese-classifier-olist.ipynb (nbviewer of the notebook): code used to fine-tune a Portuguese Bidirectional LM and a Sentiment Classifier on \"Brazilian E-Commerce Public Dataset by Olist\" dataset.  \n[ WARNING ] The code of this notebook lm3-portuguese-classifier-olist.ipynb must be updated in order to use the SentencePiece model and vocab already trained for the Portuguese Language Model in the notebook lm3-portuguese.ipynb as it was done in the notebook lm3-portuguese-classifier-TCU-jurisprudencia.ipynb (see explanations at the top of this notebook). \nHere's an example of using the classifier to predict the category of a TCU legal text: \nI trained 3 French Bidirectional Language Models (FBLM) with 1 NVIDIA GPU v100 on GCP but the best is the one trained with the MultiFit configuration. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.808062842321781
      ],
      "excerpt": "| ULMFiT with 3 AWD-LSTM + spaCy (60 000 tokens) | forward   | 36.44%  | 25.62  | 11h | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8929969698830342
      ],
      "excerpt": "notebook lm3-french.ipynb (nbviewer of the notebook): code used to train a French Bidirectional LM on a 100 millions corpus extrated from Wikipedia by using the MultiFiT configuration. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285990114261894,
        0.97932816195008
      ],
      "excerpt": "Application: notebook lm3-french-classifier-amazon.ipynb (nbviewer of the notebook): code used to fine-tune a French Bidirectional LM and a Sentiment Classifier on \"French Amazon Customer Reviews\" dataset. \nHere's an example of using the classifier to predict the feeling of comments on an amazon product: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285990114261894
      ],
      "excerpt": "Application: notebook lm2-french-classifier-amazon.ipynb (nbviewer of the notebook): code used to fine-tune a French Bidirectional LM and a Sentiment Classifier on \"French Amazon Customer Reviews\" dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "pre-trained Language Models",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/piegu/language-models/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 26,
      "date": "Tue, 21 Dec 2021 14:44:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/piegu/language-models/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "piegu/language-models",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/piegu/language-models/master/lm3-portuguese-classifier-TCU-jurisprudencia.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/fast_inference_transformers_on_GPU.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/question_answering_BERT_large_cased_squad_v11_pt.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/lm3-portuguese-classifier-olist.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/lm2-french-classifier-amazon.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/nlp_how_to_add_a_domain_specific_vocabulary_new_tokens_to_a_subword_tokenizer_already_trained_like_BERT_WordPiece.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/lm2-french.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/colab_question_answering_BERT_base_cased_squad_v11_pt.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/lm3-french.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/lm3-portuguese.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/fast_inference_transformers_on_CPU.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/lm-french-classifier-amazon.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/OCR_DeepLearning_Tesseract_DocTR_colab.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/lm-french.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/lm-french-generator.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/lm3-french-classifier-amazon.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/adapters/language-modeling/language_modeling_adapter.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/adapters/question-answering/question_answering_adapter_script.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/adapters/question-answering/question_answering_adapter.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/adapters/question-answering/question_answering_adapter_fusion.ipynb",
      "https://raw.githubusercontent.com/piegu/language-models/master/adapters/token-classification/token_classification_adapter.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.800062575312726
      ],
      "excerpt": "| backward  | 43.67%  | 22.16  | 8h | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8464739423733262
      ],
      "excerpt": "Here's an example of using the classifier to predict the category of a TCU legal text: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/piegu/language-models/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Language Models",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "language-models",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "piegu",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/piegu/language-models/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 69,
      "date": "Tue, 21 Dec 2021 14:44:13 GMT"
    },
    "technique": "GitHub API"
  }
}