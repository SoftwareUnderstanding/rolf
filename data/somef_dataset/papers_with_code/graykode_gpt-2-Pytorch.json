{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[Jeff Wu(@WuTheFWasThat)](https://github.com/WuTheFWasThat), [Thomas Wolf(@thomwolf)](https://github.com/thomwolf) for allowing referring code.",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8227947088210703
      ],
      "excerpt": "First OpenAi-GPT Paper : Improving Language Understanding by Generative Pre-Training(2018) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9610293202239182
      ],
      "excerpt": "--unconditional : If true, unconditional generation. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/graykode/gpt-2-Pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-18T08:06:33Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-16T08:13:05Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8741289301960257,
        0.9955732840678397,
        0.9899271938463405,
        0.9551588054752006,
        0.9608716717309176
      ],
      "excerpt": "Better Language Models and Their Implications \nOur model, called GPT-2 (a successor to GPT), was trained simply to predict the next word in 40GB of Internet text. Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much smaller model for researchers to experiment with, as well as a technical paper. from openAI Blog \nThis repository is simple implementation GPT-2 about text-generator in Pytorch with compress code \nThe original repertoire is openai/gpt-2. Also You can Read Paper about gpt-2, \"Language Models are Unsupervised Multitask Learners\". To Understand more detail concept, I recommend papers about Transformer Model. \nGood implementation GPT-2 in Pytorch which I referred to, huggingface/pytorch-pretrained-BERT, You can see more detail implementation in huggingface repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9521426122636943,
        0.9917159896299618,
        0.8381244355206044
      ],
      "excerpt": "See OpenAI Blog about GPT-2 and Paper \n--text : sentence to begin with. \n--quiet : not print all of the extraneous stuff like the \"================\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8984190680453107
      ],
      "excerpt": "See more detail option about temperature and top_k in here \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Simple Text-Generator with OpenAI gpt-2 Pytorch Implementation",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/graykode/gpt-2-Pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 176,
      "date": "Tue, 21 Dec 2021 16:03:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/graykode/gpt-2-Pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "graykode/gpt-2-Pytorch",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/graykode/gpt-2-Pytorch/master/GPT2_Pytorch.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```shell\n$ python3 -m venv venv\n$ source venv/bin/activate\n$ pip install torch tqdm\n$ brew install libomp\n$ export LC_ALL=en_US.UTF-8\n$ export LANG=en_US.UTF-8\n$ pip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8058700487544376
      ],
      "excerpt": "--text : sentence to begin with. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8074337800445963
      ],
      "excerpt": "--nsamples : number of sample sampled in batch when multinomial function use \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8124508577449493
      ],
      "excerpt": "--batch_size : number of batch size \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/graykode/gpt-2-Pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 OpenAI, HugginFace Inc. team. and TaeHwan Jung\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# **GPT2-Pytorch with Text-Generator**",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "gpt-2-Pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "graykode",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/graykode/gpt-2-Pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Pytorch 0.41+\n- regex 2017.4.5\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 692,
      "date": "Tue, 21 Dec 2021 16:03:31 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "gpt-2",
      "pytorch",
      "implementation",
      "nlp",
      "text-generator",
      "story-telling",
      "gpt2",
      "natural-language-processing"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. download GPT2 pre-trained model in Pytorch which huggingface/pytorch-pretrained-BERT already made! (Thanks for sharing! it's help my problem transferring tensorflow(ckpt) file to Pytorch Model!)\n```shell\n$ git clone https://github.com/graykode/gpt-2-Pytorch && cd gpt-2-Pytorch\n#: download huggingface's pytorch model \n$ curl --output gpt2-pytorch_model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\n#: setup requirements, if using mac os, then run additional setup as descibed below\n$ pip install -r requirements.txt\n```\n\n\n2. Now, You can run like this.\n\n- Text from Book 1984, George Orwell\n\n```shell\n$ python main.py --text \"It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.\"\n```\n\n3. Also You can Quick Starting in [Google Colab](https://colab.research.google.com/github/graykode/gpt-2-Pytorch/blob/master/GPT2_Pytorch.ipynb)\n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}