{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. [Trust Region Policy Optimization](https://arxiv.org/pdf/1502.05477.pdf) (Schulman et al., 2016)\n2. [Emergence of Locomotion Behaviours in Rich Environments](https://arxiv.org/pdf/1707.02286.pdf) (Heess et al., 2017)\n3. [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/pdf/1506.02438.pdf) (Schulman et al., 2016)\n4. [GitHub Repository with several helpful implementation ideas](https://github.com/joschu/modular_rl) (Schulman)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9251858787057007
      ],
      "excerpt": "By Patrick Coady: Learning Artificial Intelligence \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pat-coady/trpo",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-07-10T15:10:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-21T06:34:54Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**NOTE:** The code has been refactored to use TensorFlow 2.0 and PyBullet (instead of MuJoCo). See the `tf1_mujoco` branch for old version.\n\nThe project's original goal was to use the same algorithm to \"solve\" [10 MuJoCo robotic control environments](https://gym.openai.com/envs/#mujoco). And, specifically, to achieve this without hand-tuning the hyperparameters (network sizes, learning rates, and TRPO settings) for each environment. This is challenging because the environments range from a simple cart pole problem with a single control input to a humanoid with 17 controlled joints and 44 observed variables. The project was successful, nabbing top spots on almost all of the AI Gym MuJoCo leaderboards.\n\nWith the release of TensorFlow 2.0, I decided to dust off this project and upgrade the code. And, while I was at it, I moved from the paid MuJoCo simulator to the free PyBullet simulator.\n\nHere are the key points:\n\n* Trust Region Policy Optimization \\[1\\] \\[2\\]\n* Value function approximated with 3 hidden-layer NN (tanh activations):\n    * hid1 size = obs_dim x 10\n    * hid2 size = geometric mean of hid1 and hid3 sizes\n    * hid3 size = 5\n* Policy is a multi-variate Gaussian parameterized by a 3 hidden-layer NN (tanh activations):\n    * hid1 size = obs_dim x 10\n    * hid2 size = geometric mean of hid1 and hid3 sizes\n    * hid3 size = action_dim x 10\n    * Diagonal covariance matrix variables are separately trained\n* Generalized Advantage Estimation (gamma = 0.995, lambda = 0.98) \\[3\\] \\[4\\]\n* ADAM optimizer used for both neural networks\n* The policy is evaluated for 20 episodes between updates, except:\n    * 50 episodes for Reacher\n    * 5 episodes for Swimmer\n    * 5 episodes for HalfCheetah\n    * 5 episodes for HumanoidStandup\n* Value function is trained on current batch + previous batch\n* KL loss factor and ADAM learning rate are dynamically adjusted during training\n* Policy and Value NNs built with TensorFlow\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8329595194445183
      ],
      "excerpt": "I ran quick checks on three of the above environments and successfully stabilized a double-inverted pendulum and taught the \"half cheetah\" to run. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Trust Region Policy Optimization with TensorFlow and OpenAI Gym",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pat-coady/trpo/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 100,
      "date": "Mon, 27 Dec 2021 23:11:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pat-coady/trpo/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pat-coady/trpo",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pat-coady/trpo/master/notebooks/env_dimension_sizes.ipynb",
      "https://raw.githubusercontent.com/pat-coady/trpo/master/trpo/view_training.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.9503189345333785,
        0.9503189345333785,
        0.9503189345333785
      ],
      "excerpt": "python train.py InvertedPendulumBulletEnv-v0 \npython train.py InvertedDoublePendulumBulletEnv-v0 -n 5000 \npython train.py HalfCheetahBulletEnv-v0 -n 5000 -b 5 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pat-coady/trpo/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 pat-coady\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Trust Region Policy Optimization with Generalized Advantage Estimation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "trpo",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pat-coady",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pat-coady/trpo/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "pat-coady",
        "body": "First release.",
        "dateCreated": "2017-10-29T16:15:27Z",
        "datePublished": "2018-02-22T23:34:43Z",
        "html_url": "https://github.com/pat-coady/trpo/releases/tag/v1.0.0",
        "name": "First release.",
        "tag_name": "v1.0.0",
        "tarball_url": "https://api.github.com/repos/pat-coady/trpo/tarball/v1.0.0",
        "url": "https://api.github.com/repos/pat-coady/trpo/releases/9798634",
        "zipball_url": "https://api.github.com/repos/pat-coady/trpo/zipball/v1.0.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python 3.6\n* The Usual Suspects: numpy, matplotlib, scipy\n* TensorFlow 2.x\n* Open AI Gym: [installation instructions](https://gym.openai.com/docs)\n* [pybullet](https://pypi.org/project/pybullet/) physics simulator\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 349,
      "date": "Mon, 27 Dec 2021 23:11:29 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning",
      "policy-gradient",
      "tensorflow",
      "machine-learning",
      "mujoco"
    ],
    "technique": "GitHub API"
  }
}