{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1902.11124>.  \nIt is an effort to combine generative model (in particular, [Variational Auto-Encoders](https://arxiv.org/abs/1312.6114",
      "https://arxiv.org/abs/1312.6114",
      "https://arxiv.org/abs/1902.11124"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Mingpan/generative_map",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-01T16:07:03Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-23T17:00:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is the code base for our work of Generative Map at <https://arxiv.org/abs/1902.11124>.  \nIt is an effort to combine generative model (in particular, [Variational Auto-Encoders](https://arxiv.org/abs/1312.6114)) and the classic Kalman filter for generation with localization.  \nFor more details, please refer to [our paper on arXiv](https://arxiv.org/abs/1902.11124).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "The code for our work in localization with a generative model.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Mingpan/generative_map/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Sat, 25 Dec 2021 18:26:16 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Mingpan/generative_map/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Mingpan/generative_map",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Mingpan/generative_map/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Mingpan\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "generative_map",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Mingpan",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Mingpan/generative_map/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Python3, and the following packages  \n```\npip install numpy scikit-image tqdm tensorflow==1.13.1\n```\n\n2. Datasets:  \n  * 7-Scenes: Download from [their website](https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/).  \n  * RobotCar: Register & Download from [here](https://robotcar-dataset.robots.ox.ac.uk/). Remark: we adopt the [preprocessing toolkit from the RobotCar authors](https://github.com/ori-mrg/robotcar-dataset-sdk) and generate data first in `.pkl` format, then load it with `data_loader_robotcar.py`. \n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 12,
      "date": "Sat, 25 Dec 2021 18:26:16 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For training, one example use can be found below. The `PATH_TO_DATA_DIR` points to the data folder, for example in 7-Scenes it is the folder where you can find `TrainSplit.txt`, `TestSplit.txt`, and folders of `seq-xx`.  \n```\npython train.py --data_dir <PATH_TO_DATA_DIR>\n```\n\nFor evaluation, one example of estimation with generation using a constant model is  \n```\npython eval.py --model_dir <PATH_TO_YOUR_MODEL_DIR_WITH_CHECKPOINTS> --generation_mode 2 --no_model\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nusage: train.py [-h] [--training_size TRAINING_SIZE]\n                [--dim_observation DIM_OBSERVATION] [--batch_size BATCH_SIZE]\n                [--num_epochs NUM_EPOCHS] [--save_every SAVE_EVERY]\n                [--model_dir MODEL_DIR] [--data_dir DATA_DIR]\n                [--reconstruct_accuracy RECONSTRUCT_ACCURACY]\n                [--dim_input DIM_INPUT DIM_INPUT DIM_INPUT]\n                [--dim_reconstruct DIM_RECONSTRUCT DIM_RECONSTRUCT DIM_RECONSTRUCT]\n                [--learning_rate LEARNING_RATE] [--use_robotcar]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --training_size TRAINING_SIZE\n                        size of the training set, sampled from the given\n                        sequences\n  --dim_observation DIM_OBSERVATION\n                        dimension of the latent representation (\"observation\")\n  --batch_size BATCH_SIZE\n                        mini-batch size for training\n  --num_epochs NUM_EPOCHS\n                        number of epochs for training\n  --save_every SAVE_EVERY\n                        intermediate saving frequency by epochs\n  --model_dir MODEL_DIR\n                        directory to save current model to\n  --data_dir DATA_DIR   directory to load the dataset\n  --reconstruct_accuracy RECONSTRUCT_ACCURACY\n                        how accurate should the reconstruction be, float,\n                        negative and positive, the smaller the more accurate\n  --dim_input DIM_INPUT DIM_INPUT DIM_INPUT\n                        the height, width, and number of channel for input\n                        image, separated by space\n  --dim_reconstruct DIM_RECONSTRUCT DIM_RECONSTRUCT DIM_RECONSTRUCT\n                        the height, width, and number of channel for\n                        reconstructed image, separated by space\n  --learning_rate LEARNING_RATE\n                        learning rate\n  --use_robotcar        if the robotcar data loader will be used, since it is\n                        stored in a different format.\n```\n  \n```\nusage: eval.py [-h] [--model_dir MODEL_DIR] [--state_var STATE_VAR]\n               [--visualize_dir VISUALIZE_DIR] [--video] [--no_model]\n               [--generation_mode GENERATION_MODE] [--all] [--seq_idx SEQ_IDX]\n               [--deviation DEVIATION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --model_dir MODEL_DIR\n                        directory to load model from. If not provided, the\n                        most recent dir in the default path will be used,\n                        raise an Error if failed.\n  --state_var STATE_VAR\n                        estimated state transition variance for the model used\n                        by extended Kalman filter, it is assumed to be\n                        independent and stay the same for all dimension\n  --visualize_dir VISUALIZE_DIR\n                        directory to save the visualization output + video (if\n                        apply)\n  --video               if a video should be recorded comparing real and\n                        estimated reconstructed images, together with\n                        localization results\n  --no_model            if the true state transition will be hidden to test\n                        localization based only on images.\n  --generation_mode GENERATION_MODE\n                        0: equidistant sample image generation; 1: Kalman\n                        filter estimation for localization; 2: perform both\n  --all                 evaluate all the validation sequences and report the\n                        performance w/o a picture.\n  --seq_idx SEQ_IDX     evaluate the given indexed sequence in the text.\n  --deviation DEVIATION\n                        deviation across all state dimension for robust test.\n```\n\n\n",
      "technique": "Header extraction"
    }
  ]
}