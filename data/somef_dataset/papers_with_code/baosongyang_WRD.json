{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite the following paper:\n```\n@inproceedings{yang2019assessing,\n  author    = {Baosong Yang  and  Longyue Wang  and  Derek F. Wong  and Lidia S. Chao and Zhaopeng Tu},\n  title     = {Assessing the Ability of Self-Attention Networks to Learn Word Order},\n  booktitle = {ACL},\n  year      = {2019}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{yang2019assessing,\n  author    = {Baosong Yang  and  Longyue Wang  and  Derek F. Wong  and Lidia S. Chao and Zhaopeng Tu},\n  title     = {Assessing the Ability of Self-Attention Networks to Learn Word Order},\n  booktitle = {ACL},\n  year      = {2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.998378858920488
      ],
      "excerpt": "Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao and Zhaopeng Tu. In ACL 2019. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/baosongyang/WRD",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-01T11:46:34Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-09T11:51:22Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The main purpose is to study how well the word order information learned by different neural networks. Specifically, we randomly move one word to another position, and examine whether a trained model can detect both the original and inserted positions. Our codes were built upon [THUMT-MT](https://github.com/THUNLP-MT/THUMT). We compare self-attention networks (SAN, [Vaswani et al., 2017](https://arxiv.org/pdf/1706.03762.pdf)) with re-implemented RNN ([Chen et al., 2018](https://www.aclweb.org/anthology/P18-1008)), as well as directional SAN (DiSAN,[Shen et al., 2018](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16126/16099)) that augments SAN with recurrence modeling.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9333143680693532,
        0.9514199868305633
      ],
      "excerpt": "This word reordering detection task (WRD) is based on the following paper: \n* Assessing the Ability of Self-Attention Networks to Learn Word Order. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This task is to study how well the word order information learned by different neural networks. ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/baosongyang/WRD/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Wed, 29 Dec 2021 05:13:48 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/baosongyang/WRD/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "baosongyang/WRD",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/baosongyang/WRD/master/word_order.sh",
      "https://raw.githubusercontent.com/baosongyang/WRD/master/word_order_MT.sh",
      "https://raw.githubusercontent.com/baosongyang/WRD/master/robustness/rob.sh"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/baosongyang/WRD/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Logos",
      "Python",
      "Yacc",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Word Reordering Detection",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "WRD",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "baosongyang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/baosongyang/WRD/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Wed, 29 Dec 2021 05:13:48 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* This program is based on [THUMT-MT](https://github.com/THUNLP-MT/THUMT). We add options for running RNN- and DiSAN-based models which are named \"**rnnp**\" and \"**transformer_di**\", respectively. To run machine translation models, you may read the documentation of the original implementation.  \n* **To examine pre-trained MT encoders** on WRD task: 1. put your model checkpoint files under the \"eval\" folder; 2. we provide an example script \"word_order_MT.sh\" to assess the ability of SAN to learn word order, you can evaluate other models by modifying the example script.\n* **To examine randomly initialized encoders** on WRD task: 1. put your well-trained MT models under the \"eval\" folder (merely use word embeddings, you can also choose other well-trained word embeddings); 2. we provide an example script \"word_order_MT.sh\" to assess the ability of SAN to learn word order, you can evaluate other models by modifying the example script. **Note that**, if you use word embeddings in pre-trained MT models, please remember to rename the scope name in the model file, making the WRD model fail to load existing parameters and re-initialize new parameters, for example: modify: ./thumt/models/transformer.py:\n```\nLine 48: \"encoder\" => \"encoder2\"\n```\n* **To assess the accuracy of models**: you can use our scripts released in ./scripts/\n* **Effect of wrong word order noises**:  we make erroneous word order noises on WMT14 En-De development set by moving one word to another position, and evaluate the drop of the translation quality of each model. The data and script can be found in \"./robustness\"\n",
      "technique": "Header extraction"
    }
  ]
}