{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "A large part of the code is borrowed from [Zhongdao/Towards-Realtime-MOT](https://github.com/Zhongdao/Towards-Realtime-MOT) and [xingyizhou/CenterNet](https://github.com/xingyizhou/CenterNet). Thanks for their wonderful works.\n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@article{zhang2021fairmot,\n  title={Fairmot: On the fairness of detection and re-identification in multiple object tracking},\n  author={Zhang, Yifu and Wang, Chunyu and Wang, Xinggang and Zeng, Wenjun and Liu, Wenyu},\n  journal={International Journal of Computer Vision},\n  volume={129},\n  pages={3069--3087},\n  year={2021},\n  publisher={Springer}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhang2021fairmot,\n  title={Fairmot: On the fairness of detection and re-identification in multiple object tracking},\n  author={Zhang, Yifu and Wang, Chunyu and Wang, Xinggang and Zeng, Wenjun and Liu, Wenyu},\n  journal={International Journal of Computer Vision},\n  volume={129},\n  pages={3069--3087},\n  year={2021},\n  publisher={Springer}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9604675413581302
      ],
      "excerpt": "A simple baseline for one-shot multi-object tracking: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997089747468236,
        0.9490753289412834
      ],
      "excerpt": "Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, Wenyu Liu,       \nIJCV2021 (arXiv 2004.01888) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ifzhang/FairMOT",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-04T07:48:48Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T08:59:27Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8549483910629022
      ],
      "excerpt": "FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking,           \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9752483642891783,
        0.9837707654345798
      ],
      "excerpt": "There has been remarkable progress on object detection and re-identification in recent years which are the core components for multi-object tracking. However, little attention has been focused on accomplishing the two tasks in a single network to improve the inference speed. The initial attempts along this path ended up with degraded results mainly because the re-identification branch is not appropriately learned. In this work, we study the essential reasons behind the failure, and accordingly present a simple baseline to addresses the problems. It remarkably outperforms the state-of-the-arts on the MOT challenge datasets at 30 FPS. We hope this baseline could inspire and help evaluate new ideas in this field. \n(2021.08.03) Our paper is accepted by IJCV! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9263401064402472,
        0.877569321397275
      ],
      "excerpt": "(2021.05.24) A light version of FairMOT using yolov5s backbone is released!  \n(2020.09.10) A new version of FairMOT is released! (73.7 MOTA on MOT17) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144965960308795
      ],
      "excerpt": "HRNetV2 ImageNet pretrained model: HRNetV2-W18 official, HRNetV2-W32 official. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.953539426964277
      ],
      "excerpt": "Our baseline FairMOT model (DLA-34 backbone) is pretrained on the CrowdHuman for 60 epochs with the self-supervised learning approach and then trained on the MIX dataset for 30 epochs. The models can be downloaded here:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9127914774016888
      ],
      "excerpt": "fairmot_dla34.pth [Google] [Baidu, code:uouv] [Onedrive]. (This is the model we get 73.7 MOTA on the MOT17 test set. ) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8916965164951427
      ],
      "excerpt": "Finetune on 2DMOT15 using the baseline model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9216400871101106
      ],
      "excerpt": "The data annotation of MOT20 is a little different from MOT17, the coordinates of the bounding boxes are all inside the image, so we need to uncomment line 313 to 316 in the dataset file src/lib/datasets/dataset/jde.py: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8079866345294066
      ],
      "excerpt": "* For ablation study, we use MIX and half of MOT17 as training data, you can use different backbones such as ResNet, ResNet-FPN, HRNet and DLA:: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9317775180756732
      ],
      "excerpt": "* Performance on the test set of MOT17 when using different training data: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.868462622210926
      ],
      "excerpt": "* We use CrowdHuman, MIX and MOT17 to train the light version of FairMOT using yolov5s as backbone: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8117087431788199,
        0.8301358753969301
      ],
      "excerpt": "The pretrained model of yolov5s on the COCO dataset can be downloaded here:  [Google] [Baidu, code:wh9h]. \nThe model of the light version 'fairmot_yolov5s' can be downloaded here:  [Google] [Baidu, code:2y3a]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9442775007968344,
        0.8303186039875602
      ],
      "excerpt": "to see the tracking results (76.5 MOTA and 79.3 IDF1 using the baseline model). You can also set save_images=True in src/track.py to save the visualization results of each frame.  \nFor ablation study, we evaluate on the other half of the training set of MOT17, you can run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.939881980408992
      ],
      "excerpt": "To get the SOTA results of 2DMOT15 and MOT20, run the tracking code: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8620463121228492
      ],
      "excerpt": "Results of the test set all need to be evaluated on the MOT challenge server. You can see the tracking results on the training set by setting --val_motxx True and run the tracking code. We set 'conf_thres' 0.4 for MOT16 and MOT17. We set 'conf_thres' 0.3 for 2DMOT15 and MOT20. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "[IJCV-2021] FairMOT: On the Fairness of Detection and Re-Identification in Multi-Object Tracking",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ifzhang/FairMOT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 785,
      "date": "Wed, 29 Dec 2021 20:12:50 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ifzhang/FairMOT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ifzhang/FairMOT",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/build/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/crowdhuman_dla34.sh",
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/mot17_dla34.sh",
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/mot17_half_yolov5s.sh",
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/mot17_half_dla34.sh",
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/mix_mot17_half_yolov5s.sh",
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/mix_mot17_half_res50.sh",
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/mix_mot17_half_res34fpn.sh",
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/mix_dla34.sh",
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/all_yolov5s.sh",
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/mix_mot17_half_dla34.sh",
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/mot15_ft_mix_dla34.sh",
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/mix_ft_ch_dla34.sh",
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/mix_mot17_half_res34.sh",
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/mot20_ft_mix_dla34.sh",
      "https://raw.githubusercontent.com/ifzhang/FairMOT/master/experiments/mix_mot17_half_hrnet18.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* **CrowdHuman**\nThe CrowdHuman dataset can be downloaded from their [official webpage](https://www.crowdhuman.org). After downloading, you should prepare the data in the following structure:\n```\ncrowdhuman\n   |\u2014\u2014\u2014\u2014\u2014\u2014images\n   |        \u2514\u2014\u2014\u2014\u2014\u2014\u2014train\n   |        \u2514\u2014\u2014\u2014\u2014\u2014\u2014val\n   \u2514\u2014\u2014\u2014\u2014\u2014\u2014labels_with_ids\n   |         \u2514\u2014\u2014\u2014\u2014\u2014\u2014train(empty)\n   |         \u2514\u2014\u2014\u2014\u2014\u2014\u2014val(empty)\n   \u2514------annotation_train.odgt\n   \u2514------annotation_val.odgt\n```\nIf you want to pretrain on CrowdHuman (we train Re-ID on CrowdHuman), you can change the paths in src/gen_labels_crowd_id.py and run:\n```\ncd src\npython gen_labels_crowd_id.py\n```\nIf you want to add CrowdHuman to the MIX dataset (we do not train Re-ID on CrowdHuman), you can change the paths in src/gen_labels_crowd_det.py and run:\n```\ncd src\npython gen_labels_crowd_det.py\n```\n* **MIX**\nWe use the same training data as [JDE](https://github.com/Zhongdao/Towards-Realtime-MOT) in this part and we call it \"MIX\". Please refer to their [DATA ZOO](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md) to download and prepare all the training data including Caltech Pedestrian, CityPersons, CUHK-SYSU, PRW, ETHZ, MOT17 and MOT16. \n* **2DMOT15 and MOT20** \n[2DMOT15](https://motchallenge.net/data/2D_MOT_2015/) and [MOT20](https://motchallenge.net/data/MOT20/) can be downloaded from the official webpage of MOT challenge. After downloading, you should prepare the data in the following structure:\n```\nMOT15\n   |\u2014\u2014\u2014\u2014\u2014\u2014images\n   |        \u2514\u2014\u2014\u2014\u2014\u2014\u2014train\n   |        \u2514\u2014\u2014\u2014\u2014\u2014\u2014test\n   \u2514\u2014\u2014\u2014\u2014\u2014\u2014labels_with_ids\n            \u2514\u2014\u2014\u2014\u2014\u2014\u2014train(empty)\nMOT20\n   |\u2014\u2014\u2014\u2014\u2014\u2014images\n   |        \u2514\u2014\u2014\u2014\u2014\u2014\u2014train\n   |        \u2514\u2014\u2014\u2014\u2014\u2014\u2014test\n   \u2514\u2014\u2014\u2014\u2014\u2014\u2014labels_with_ids\n            \u2514\u2014\u2014\u2014\u2014\u2014\u2014train(empty)\n```\nThen, you can change the seq_root and label_root in src/gen_labels_15.py and src/gen_labels_20.py and run:\n```\ncd src\npython gen_labels_15.py\npython gen_labels_20.py\n```\nto generate the labels of 2DMOT15 and MOT20. The seqinfo.ini files of 2DMOT15 can be downloaded here [[Google]](https://drive.google.com/open?id=1kJYySZy7wyETH4fKMzgJrYUrTfxKlN1w), [[Baidu],code:8o0w](https://pan.baidu.com/s/1zb5tBW7-YTzWOXpd9IzS0g).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "* Clone this repo, and we'll call the directory that you cloned as ${FAIRMOT_ROOT}\n* Install dependencies. We use python 3.8 and pytorch >= 1.7.0\n```\nconda create -n FairMOT\nconda activate FairMOT\nconda install pytorch==1.7.0 torchvision==0.8.0 cudatoolkit=10.2 -c pytorch\ncd ${FAIRMOT_ROOT}\npip install cython\npip install -r requirements.txt\n```\n* We use [DCNv2_pytorch_1.7](https://github.com/ifzhang/DCNv2/tree/pytorch_1.7) in our backbone network (pytorch_1.7 branch). Previous versions can be found in [DCNv2](https://github.com/CharlesShang/DCNv2).\n```\ngit clone -b pytorch_1.7 https://github.com/ifzhang/DCNv2.git\ncd DCNv2\n./make.sh\n```\n* In order to run the code for demos, you also need to install [ffmpeg](https://www.ffmpeg.org/).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src=\"assets/MOT15.gif\" width=\"400\"/>   <img src=\"assets/MOT16.gif\" width=\"400\"/>\n<img src=\"assets/MOT17.gif\" width=\"400\"/>   <img src=\"assets/MOT20.gif\" width=\"400\"/>\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "| Dataset    |  MOTA | IDF1 | IDS | MT | ML | FPS |\n|--------------|-----------|--------|-------|----------|----------|--------|\n|2DMOT15  | 60.6 | 64.7 |  591 | 47.6% | 11.0% | 30.5 |\n|MOT16       | 74.9 | 72.8 | 1074 | 44.7% | 15.9% | 25.9 |\n|MOT17       | 73.7 | 72.3 | 3303 | 43.2% | 17.3% | 25.9 |\n|MOT20       | 61.8 | 67.3 | 5243 | 68.8% | 7.6% | 13.2 |\n\n All of the results are obtained on the [MOT challenge](https://motchallenge.net) evaluation server under the \u201cprivate detector\u201d protocol. We rank first among all the trackers on 2DMOT15, MOT16, MOT17 and  MOT20. The tracking speed of the entire system can reach up to **30 FPS**.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9164640256136145,
        0.9164640256136145
      ],
      "excerpt": "sh experiments/crowdhuman_dla34.sh \nsh experiments/mix_ft_ch_dla34.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164640256136145
      ],
      "excerpt": "sh experiments/mix_dla34.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164640256136145
      ],
      "excerpt": "sh experiments/mot17_dla34.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164640256136145
      ],
      "excerpt": "sh experiments/mot15_ft_mix_dla34.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164640256136145,
        0.9164640256136145,
        0.9164640256136145
      ],
      "excerpt": "sh experiments/crowdhuman_dla34.sh \nsh experiments/mix_ft_ch_dla34.sh \nsh experiments/mot20_ft_mix_dla34.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164640256136145,
        0.9164640256136145,
        0.9164640256136145,
        0.9164640256136145,
        0.9164640256136145
      ],
      "excerpt": "sh experiments/mix_mot17_half_dla34.sh \nsh experiments/mix_mot17_half_hrnet18.sh \nsh experiments/mix_mot17_half_res34.sh \nsh experiments/mix_mot17_half_res34fpn.sh \nsh experiments/mix_mot17_half_res50.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164640256136145
      ],
      "excerpt": "sh experiments/all_yolov5s.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8151535396006916,
        0.9452797457628369
      ],
      "excerpt": "The default settings run tracking on the validation dataset from 2DMOT15. Using the baseline model, you can run: \ncd src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452797457628369
      ],
      "excerpt": "cd src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452797457628369
      ],
      "excerpt": "cd src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8166853986274194,
        0.9452797457628369
      ],
      "excerpt": "To run tracking using the light version of FairMOT (68.5 MOTA on the test of MOT17), you can run: \ncd src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452797457628369
      ],
      "excerpt": "cd src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9520924373537673
      ],
      "excerpt": "You can train FairMOT on custom dataset by following several steps bellow: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8245539886860519
      ],
      "excerpt": "Pretrained models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8952823871208778,
        0.8630739888074163
      ],
      "excerpt": "Download the training data \nChange the dataset root directory 'root' in src/lib/cfg/data.json and 'data_dir' in src/lib/opts.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131693795914428
      ],
      "excerpt": "The data annotation of MOT20 is a little different from MOT17, the coordinates of the bounding boxes are all inside the image, so we need to uncomment line 313 to 316 in the dataset file src/lib/datasets/dataset/jde.py: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8212727399036357,
        0.8212727399036357,
        0.8306704112157837,
        0.8306704112157837
      ],
      "excerpt": ":np.clip(xy[:, 0], 0, width, out=xy[:, 0]) \n:np.clip(xy[:, 2], 0, width, out=xy[:, 2]) \n:np.clip(xy[:, 1], 0, height, out=xy[:, 1]) \n:np.clip(xy[:, 3], 0, height, out=xy[:, 3]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8668937975765546
      ],
      "excerpt": "| Training Data    |  MOTA | IDF1 | IDS     | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629400568140789,
        0.9300244143002546
      ],
      "excerpt": "cd src \npython track.py mot --load_model ../models/fairmot_dla34.pth --conf_thres 0.6 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629400568140789,
        0.9438699978988473
      ],
      "excerpt": "cd src \npython track_half.py mot --load_model ../exp/mot/mix_mot17_half_dla34.pth --conf_thres 0.4 --val_mot17 True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629400568140789,
        0.9544536113645911,
        0.9544536113645911
      ],
      "excerpt": "cd src \npython track.py mot --test_mot17 True --load_model ../models/fairmot_dla34.pth --conf_thres 0.4 \npython track.py mot --test_mot16 True --load_model ../models/fairmot_dla34.pth --conf_thres 0.4 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629400568140789,
        0.946651956164918
      ],
      "excerpt": "cd src \npython track.py mot --test_mot17 True --load_model ../models/fairmot_yolov5s.pth --conf_thres 0.4 --arch yolo --reid_dim 64 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629400568140789,
        0.9516623759856101,
        0.9516623759856101
      ],
      "excerpt": "cd src \npython track.py mot --test_mot15 True --load_model your_mot15_model.pth --conf_thres 0.3 \npython track.py mot --test_mot20 True --load_model your_mot20_model.pth --conf_thres 0.3 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ifzhang/FairMOT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Dockerfile",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 YifuZhang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "FairMOT",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "FairMOT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ifzhang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ifzhang/FairMOT/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3211,
      "date": "Wed, 29 Dec 2021 20:12:50 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "multi-object-tracking",
      "one-shot-tracker",
      "joint-detection-and-tracking",
      "real-time"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src=\"assets/MOT15.gif\" width=\"400\"/>   <img src=\"assets/MOT16.gif\" width=\"400\"/>\n<img src=\"assets/MOT17.gif\" width=\"400\"/>   <img src=\"assets/MOT20.gif\" width=\"400\"/>\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You can input a raw video and get the demo video by running src/demo.py and get the mp4 format of the demo video:\n```\ncd src\npython demo.py mot --load_model ../models/fairmot_dla34.pth --conf_thres 0.4\n```\nYou can change --input-video and --output-root to get the demos of your own videos.\n--conf_thres can be set from 0.3 to 0.7 depending on your own videos.\n\n",
      "technique": "Header extraction"
    }
  ]
}