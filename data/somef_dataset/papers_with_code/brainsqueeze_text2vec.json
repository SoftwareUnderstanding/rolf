{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.0473",
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473",
      "https://arxiv.org/abs/1409.0473",
      "https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "---\n\n1. D. Bahdanau, K. Cho, Y. Bengio [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)\n2. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/brainsqueeze/text2vec",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-03-17T13:51:02Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-01T20:45:56Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9320024782852533
      ],
      "excerpt": "Models for contextual embedding of arbitrary texts. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.967319860396177
      ],
      "excerpt": "language processing. The technique is able to distill semantic  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8508784140730661
      ],
      "excerpt": "high-dimensional vector space. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9658847911597178,
        0.8534770788437605,
        0.9625023332456482
      ],
      "excerpt": "sentence the order of words and the use of punctuation and  \nconjugations are very important for extracting the meaning  \nof blocks of text. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9891785691729262,
        0.97964110708268,
        0.8242767819684873
      ],
      "excerpt": "[1] is  \nused to extrude the overall meaning of the input text. In the  \ncase of text2vec, we use the attention vectors found from the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9757017649068683,
        0.895360022999871
      ],
      "excerpt": "note: this is not a canonical implementation of the attention  \nmechanism, but this method was chosen intentionally to be able to  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9952669828137117
      ],
      "excerpt": "This is a tensor-to-tensor model adapted from the work in  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9865958908227589,
        0.9892320591014225
      ],
      "excerpt": "attention is applied at the end of the encoding steps and a  \ncontext-vector is learned, which in turn is used to project  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8636533101224073
      ],
      "excerpt": "The decoding steps begin as usual with the word-embedded input  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9596257651205837
      ],
      "excerpt": "and layer-normalization is applied. Before continuing, we project  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9493470918624152
      ],
      "excerpt": "This is an adapted bi-directional LSTM encoder-decoder model with  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8776916602444932
      ],
      "excerpt": "context-vectors are used to project the resulting decoded sequences  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8936027463542959,
        0.9170202277000701
      ],
      "excerpt": "The pre-built auto-encoder models inherit from tf.keras.Model, and as such they can be trained using the fit method.  \nAn example of training on Wikitext data is available in the examples folder. This uses HuggingFace tokenizers and datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8362511604607556
      ],
      "excerpt": "`embedding` and `hidden`, which referes to the dimensionality of the hidden LSTM layer. The `training` section of the YAML file can also include user-defined sentences to use as a context-angle evaluation set. This can look likeyaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8799677437994219
      ],
      "excerpt": "It can also include a `data` tag which is a list of absolute file paths for custom training data sets. This can look likeyaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9877583065516352,
        0.8810766808168947
      ],
      "excerpt": "To impose quasi-mutual orthogonality on the learned context vectors simply add the --orthogonal flag to the training command. This will add a loss term that can be thought of as a Lagrange multiplier where the constraint is self-alignment of the context vectors, and orthogonality between non-self vectors. The aim is not to impose orthogonality between all text inputs that are not the same, but rather to coerce the model to learn significantly different encodings for different contextual inputs. \nText2vec includes a Python API with convenient classes for handling attention and LSTM mechanisms. These classes can be used to create custom models and layers to address custom problems. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Contextual embedding for text blobs.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/brainsqueeze/text2vec/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 07:59:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/brainsqueeze/text2vec/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "brainsqueeze/text2vec",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "---\n\nTo get started, one should have a flavor of TensorFlow installed, with\nversion `>=2.4.1`. One can run\n```bash\npip install tensorflow>=2.4.1\n```\nIf one wishes to run the examples, some additional dependencies\nfrom HuggingFace will need to be installed. The full installation\nlooks like\n```bash\npip install tensorflow>=2.4.1 tokenizers datasets\n```\n\nTo install the core components as an import-able Python library\nsimply run\n\n```bash\npip install git+https://github.com/brainsqueeze/text2vec.git\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8986100138149027
      ],
      "excerpt": "text2vec_main --run=train --yaml_config=/path/to/config.yml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "  name: transformer_test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8986100138149027
      ],
      "excerpt": "text2vec_main --run=train --attention --yaml_config=/path/to/config.yml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9609220487369444,
        0.9993792561519651
      ],
      "excerpt": "If you have CUDA and cuDNN installed you can run  \npip install -r requirements-gpu.txt.  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8028672583512901
      ],
      "excerpt": "input text as the embedded vector representing the input.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8674346329194481
      ],
      "excerpt": "An example of training on Wikitext data is available in the examples folder. This uses HuggingFace tokenizers and datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8466926160992138
      ],
      "excerpt": "text2vec_main --run=train --yaml_config=/path/to/config.yml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "  name: transformer_test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8466926160992138,
        0.8351968613488682
      ],
      "excerpt": "text2vec_main --run=train --attention --yaml_config=/path/to/config.yml \nTo view the output of training you can then run \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/brainsqueeze/text2vec/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 2-Clause \"Simplified\" License",
      "url": "https://api.github.com/licenses/bsd-2-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 2-Clause License\\n\\nCopyright (c) 2018, Dave Hollander\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "text2vec",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "text2vec",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "brainsqueeze",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/brainsqueeze/text2vec/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "brainsqueeze",
        "body": "- Breaking changes on `ServingModel` wrapper class, now with dictionary outputs\r\n- new strings module with `SubTokenFinderMask` class which performs ragged substring searches and masking\r\n- More flexible encoder/decoder network flow",
        "dateCreated": "2021-09-27T17:32:27Z",
        "datePublished": "2021-09-27T17:42:57Z",
        "html_url": "https://github.com/brainsqueeze/text2vec/releases/tag/v1.2.0",
        "name": "v1.2.0",
        "tag_name": "v1.2.0",
        "tarball_url": "https://api.github.com/repos/brainsqueeze/text2vec/tarball/v1.2.0",
        "url": "https://api.github.com/repos/brainsqueeze/text2vec/releases/50339598",
        "zipball_url": "https://api.github.com/repos/brainsqueeze/text2vec/zipball/v1.2.0"
      },
      {
        "authorType": "User",
        "author_name": "brainsqueeze",
        "body": "* Breaking changes for some layers, some layers deprecated (in README)\r\n* Training loop via /bin/main.py is now deprecated and will be removed in future versions\r\n* Included data is removed in favor of the much richer HuggingFace datasets library\r\n* More flexible API for training auto-encoders\r\n* Leverage HuggingFace tokenizers",
        "dateCreated": "2021-06-08T19:49:01Z",
        "datePublished": "2021-06-08T19:53:51Z",
        "html_url": "https://github.com/brainsqueeze/text2vec/releases/tag/v1.0.0",
        "name": "v1.0.0",
        "tag_name": "v1.0.0",
        "tarball_url": "https://api.github.com/repos/brainsqueeze/text2vec/tarball/v1.0.0",
        "url": "https://api.github.com/repos/brainsqueeze/text2vec/releases/44303810",
        "zipball_url": "https://api.github.com/repos/brainsqueeze/text2vec/zipball/v1.0.0"
      },
      {
        "authorType": "User",
        "author_name": "brainsqueeze",
        "body": "- Improved training performance\r\n- Slimmed down `tf.saved_model` output for inference\r\n- Better documentation.",
        "dateCreated": "2020-10-27T14:53:30Z",
        "datePublished": "2020-10-27T14:57:37Z",
        "html_url": "https://github.com/brainsqueeze/text2vec/releases/tag/v0.4.3",
        "name": "v0.4.3",
        "tag_name": "v0.4.3",
        "tarball_url": "https://api.github.com/repos/brainsqueeze/text2vec/tarball/v0.4.3",
        "url": "https://api.github.com/repos/brainsqueeze/text2vec/releases/33115038",
        "zipball_url": "https://api.github.com/repos/brainsqueeze/text2vec/zipball/v0.4.3"
      },
      {
        "authorType": "User",
        "author_name": "brainsqueeze",
        "body": "Updates include:\r\n- YAML config for training to avoid long CLI inputs\r\n- More flexible method for handling external training data\r\n- convenience CLI function\r\n- PyYAML dependency added\r\n- some improvements to TensorBoard logging for scalar quantities",
        "dateCreated": "2020-02-07T23:06:41Z",
        "datePublished": "2020-02-10T17:33:10Z",
        "html_url": "https://github.com/brainsqueeze/text2vec/releases/tag/v0.2.2",
        "name": "v0.2.2",
        "tag_name": "v0.2.2",
        "tarball_url": "https://api.github.com/repos/brainsqueeze/text2vec/tarball/v0.2.2",
        "url": "https://api.github.com/repos/brainsqueeze/text2vec/releases/23541798",
        "zipball_url": "https://api.github.com/repos/brainsqueeze/text2vec/zipball/v0.2.2"
      },
      {
        "authorType": "User",
        "author_name": "brainsqueeze",
        "body": "Remove the encoding and decoding masking on the Bahdanau attention during the transformer decode pipeline.",
        "dateCreated": "2019-12-31T14:48:23Z",
        "datePublished": "2019-12-31T14:52:45Z",
        "html_url": "https://github.com/brainsqueeze/text2vec/releases/tag/v0.1.1-beta",
        "name": "v0.1.1-beta",
        "tag_name": "v0.1.1-beta",
        "tarball_url": "https://api.github.com/repos/brainsqueeze/text2vec/tarball/v0.1.1-beta",
        "url": "https://api.github.com/repos/brainsqueeze/text2vec/releases/22532861",
        "zipball_url": "https://api.github.com/repos/brainsqueeze/text2vec/zipball/v0.1.1-beta"
      },
      {
        "authorType": "User",
        "author_name": "brainsqueeze",
        "body": "Initial release of text2vec. This includes tools for creating attention-based and LSTM-based transformer models for turning sentences into vectors which encode contextual meaning.",
        "dateCreated": "2019-12-29T15:15:59Z",
        "datePublished": "2019-12-29T19:18:24Z",
        "html_url": "https://github.com/brainsqueeze/text2vec/releases/tag/v0.1",
        "name": "v0.1",
        "tag_name": "v0.1",
        "tarball_url": "https://api.github.com/repos/brainsqueeze/text2vec/tarball/v0.1",
        "url": "https://api.github.com/repos/brainsqueeze/text2vec/releases/22505019",
        "zipball_url": "https://api.github.com/repos/brainsqueeze/text2vec/zipball/v0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Fri, 24 Dec 2021 07:59:07 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "text-embedding",
      "tensorflow",
      "nlp",
      "text-summarization",
      "sequence-to-sequence",
      "attention-model",
      "transformer-encoder"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "---\n\nOnce a model is fully trained then a demo API can be run, along with a small \nUI to interact with the REST API. This demo attempts to use the trained model \nto condense long bodies of text into the most important sentences, using the \ninferred embedded context vectors.\n\nTo start the model server simply run \n```bash\ntext2vec_main --run=infer --yaml_config=/path/to/config.yml\n```\nA demonstration webpage is included in [demo](demo) at \n[context.html](demo/context.html).\n\n",
      "technique": "Header extraction"
    }
  ]
}