{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9895022137954327
      ],
      "excerpt": "paper @(https://arxiv.org/abs/1706.03762) and for detials regard the impelementation please refere to the source code here and to \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HeshamElAbd/SelfAttentionLangModel",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-19T14:33:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-14T12:38:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9766466795285416
      ],
      "excerpt": "Language model based upon the Encoder units of the transformer. For Theortical back ground please refere to Attention is all you need  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Language model based upon the Encoder units of the transformer ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HeshamElAbd/SelfAttentionLangModel/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 30 Dec 2021 00:04:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/HeshamElAbd/SelfAttentionLangModel/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "HeshamElAbd/SelfAttentionLangModel",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9311982782444204
      ],
      "excerpt": "1- build a pip package for the library.  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/HeshamElAbd/SelfAttentionLangModel/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SelfAttentionLangModel",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SelfAttentionLangModel",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "HeshamElAbd",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HeshamElAbd/SelfAttentionLangModel/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Thu, 30 Dec 2021 00:04:19 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tensorflow",
      "tensorflow2",
      "encoder-units",
      "language-model",
      "transformer"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "from SelfAttentionLangModel.Models import EncoderModels\n\ndemoModel=EncoderModels.Modeler(\n\n                                      embedding_dim=16,\n                                      vocabulary_size=28,\n                                      conditional_string_length=30,\n                                      num_encoder_layer=6,\n                                      num_heads=4,\n                                      num_neuron_pointwise=32,\n                                      rate=0.1,\\n\n                                      return_attent_weights=False\n                                         )\n                                         \n\n\n",
      "technique": "Header extraction"
    }
  ]
}