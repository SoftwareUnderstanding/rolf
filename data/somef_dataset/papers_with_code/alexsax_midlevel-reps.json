{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1812.11971"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this repository or toolkit useful, then please cite:\n\n    @inproceedings{midLevelReps2018,\n    \u2003title={Mid-Level Visual Representations Improve Generalization and Sample Efficiency for Learning Visuomotor Policies.},\n    \u2003author={Alexander Sax and Bradley Emi and Amir R. Zamir and Leonidas J. Guibas and Silvio Savarese and Jitendra Malik},\n    \u2003year={2018},\n    }\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{midLevelReps2018,\n\u2003title={Mid-Level Visual Representations Improve Generalization and Sample Efficiency for Learning Visuomotor Policies.},\n\u2003author={Alexander Sax and Bradley Emi and Amir R. Zamir and Leonidas J. Guibas and Silvio Savarese and Jitendra Malik},\n\u2003year={2018},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9307010319320057,
        0.9936810522188035,
        0.9799411683948944
      ],
      "excerpt": "Mid-Level Visual Representations Improve Generalization and Sample Efficiency for Learning Visuomotor Policies,<br> \nArxiv 2018.<br> \nAlexander Sax, Bradley Emi, Amir Zamir, Silvio Savarese, Leonidas Guibas, Jitendra Malik. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222383658450612,
        0.9979824815708863
      ],
      "excerpt": "CoRL 2019.<br> \nAlexander Sax, Jeffrey O. Zhang, Bradley Emi, Amir Zamir, Silvio Savarese, Leonidas Guibas, Jitendra Malik. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9529465667336368
      ],
      "excerpt": "Embodied Vision Toolkit \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654671031158477
      ],
      "excerpt": "action = policy(midlevel_feats) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alexsax/midlevel-reps",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-22T02:21:07Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T19:07:28Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8652790135856602
      ],
      "excerpt": "What happens when robots leverage visual priors during learning? They learn faster, generalize better, and achieve higher final performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9885145740307862,
        0.9030930511018302,
        0.991885132064527
      ],
      "excerpt": "Summary: How much does having visual priors about the world (e.g. the fact that the world is 3D) assist in learning to perform downstream motor tasks (e.g. delivering a package)? We study this question by integrating a generic perceptual skill set (mid-level vision) within a reinforcement learning framework. This skill set provides a policy with a more processed state of the world compared to raw images, conferring significant advantages over training from scratch (i.e. not leveraging priors) in navigation-oriented tasks. Agents are able to generalize to situations where the from-scratch approach fails and training becomes significantly more sample efficient. Realizing these gains requires careful selection of the mid-level perceptual skills, and we provide an efficient and generic max-coverage feature set that can be adopted in lieu of raw images. \nThis repository includes code from the paper, ready-made dockers containing pre-built environments, and commands to run our experiments. We also include instructions to install the lightweight visualpriors package, which allows you to use mid-level perception in your own code as a drop-in replacement for pixels. \nPlease see the website (http://perceptual.actor/) for more technical details. This repository is intended for distribution of the code, environments, and installation/running instructions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554484858175764,
        0.8128956835186355
      ],
      "excerpt": "Docker with all environments (Gibson, Habitat, Doom) \nUsing mid-level perception in your code \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8015590390523032
      ],
      "excerpt": "Quickly transform an image into surface normals features and then visualize the result.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.84483401248006
      ],
      "excerpt": ": Load image and rescale/resize to [-1,1] and 3x256x256 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8307689658757454
      ],
      "excerpt": ": Transform to normals feature and then visualize the readout \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8992876710978781
      ],
      "excerpt": "In addition to normals, you can use any of the following features in your transform: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9962077379880802
      ],
      "excerpt": "A description of each of the features is contained in the supplementary of Taskonomy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9184243716989653,
        0.9461949640544535,
        0.8736308510037112
      ],
      "excerpt": "In the main paper we studied how mid-level perception affects learning on various tasks. In the local planning task,  \nThe agent must direct itself to a given nonvisual target destination (specified using coordinates) using visual inputs, avoiding obstacles and walls as it navigates to the target. This task is useful for the practical skill of local planning, where an agent must traverse sparse waypoints along a desired path. The agent receives dense positive reward proportional to the progress it makes (in Euclidean distance) toward the goal. \nFurther details are contained in the paper.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9658453626465893,
        0.9578339895034146,
        0.9522238313744738,
        0.9941790327927708
      ],
      "excerpt": "An agent navigating to the goal. The goal is shown in the middle panel, in green. The agent sees only the left and right panels. \nIn addition to local_planning in Habitat, we implemented this and other tasks in Gibson and VizDoom, again finding the same phenomena (better generalization and sample efficiency). The new tasks are defined as follows: \nNavigation to a Visual Target: In this scenario the agent must locate a specific target object (Gibson: a wooden crate, Doom: a green torch) as fast as possible with only sparse rewards. Upon touching the target there is a large one-time positive reward and the episode ends. Otherwise there is a small penalty for living. The target looks the same between episodes although the location and orientation of both the agent and target are randomized. The agent must learn to identify the target during the course of training. \nVisual Exploration: The agent must visit as many new parts of the space as quickly as possible. The environment is partitioned into small occupancy cells which the agent \"unlocks\" by scanning with a myopic laser range scanner. This scanner reveals the area directly in front of the agent for up to 1.5 meters. The reward at each timestep is proportional to the number of newly revealed cells.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9670182634690809,
        0.9023974633718647,
        0.973802694472186
      ],
      "excerpt": "Local planning using surface normal features in Gibson. We also implemented other tasks; Visual-Target Navigation and Visual Exploration are included in the docker.  \nVisual navigation in Doom. The agent must navigate to the green_torch. The docker includes implementions of Visual-Target Navigation and also Visual Exploration in VizDoom. \nNote: Our original results (in a code dump form) are currently public via the docker activeperception/midlevel-training:0.3. We are currently working on a cleaner and more portable release. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9776458216042865
      ],
      "excerpt": "In addition to using our dockers, we provide a simple way to use mid-level vision in your code. We provide the lightweight visualpriors package which contains functions to upgrade your agent's state from pixels to mid-level features. The visualpriors package seeks to be a drop-in replacement for raw pixels. The remainder of this section focuses installation and usage.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.84483401248006
      ],
      "excerpt": ": Load image and rescale/resize to [-1,1] and 3x256x256 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8307689658757454
      ],
      "excerpt": ": Transform to normals feature and then visualize the readout \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9500505976926205,
        0.8558319010130258
      ],
      "excerpt": "| Diagram of the above setup in an active framework. The input image (o_t) gets encoded into representation=\\phi(o_t) which is decoded into the prediction pred. In this example, we choose to make the encoder (phi) a ResNet-50. | <img src=https://github.com/alexsax/midlevel-reps/blob/helper/img/transfer.png > |  \n2) Now let's try transforming the image into  object classification (ImageNet) features, instead of surface normals: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8377523204792117,
        0.9265438205651816
      ],
      "excerpt": "midlevel_feats = visualpriors.representation_transform(pre_transform_img, features='class_object')  #: So easy! \n3) In addition to normals and class_object, you can use any of the following features in your transform: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9962077379880802
      ],
      "excerpt": "A description of each of the features is contained in the supplementary of Taskonomy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.94906287811201
      ],
      "excerpt": "5) The obvious next question is: what's a good general-purpose choice of features? I'm glad that you asked! Our Max-Coverage Min-Distance Featureset proposes an answer, and those solver-found sets are implemented in the function max_coverage_transform. For example, if you can afford to use three features: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9992450012997853,
        0.9893521425073095
      ],
      "excerpt": "In addition to providing the lightweight visualpriors package, we provide code for our full research platform, evkit. This platform includes utilities for handling visual transforms, flexibility with the choice of RL algprothm (including our off-policy variant of PPO with replay buffer), and tools for logging and visualization. \nThis section will contain an overview of evkit, which is currently available in the evkit/ folder of this repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Mid-Level Visual Representations Improve Generalization and Sample Efficiency for Learning Visuomotor Policies",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://sacred.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alexsax/midlevel-reps/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 16,
      "date": "Wed, 29 Dec 2021 14:38:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alexsax/midlevel-reps/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "alexsax/midlevel-reps",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/alexsax/midlevel-reps/master/Dockerfile",
      "https://raw.githubusercontent.com/alexsax/midlevel-reps/master/gibson/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/alexsax/midlevel-reps/tree/master/tnt/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/alexsax/midlevel-reps/master/submission.sh",
      "https://raw.githubusercontent.com/alexsax/midlevel-reps/master/gibson/build.sh",
      "https://raw.githubusercontent.com/alexsax/midlevel-reps/master/gibson/gibson/core/render/build_cuda.sh",
      "https://raw.githubusercontent.com/alexsax/midlevel-reps/master/gibson/gibson/core/channels/external/download.sh",
      "https://raw.githubusercontent.com/alexsax/midlevel-reps/master/tnt/test/run_test.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**The simplest way** to install the `visualpriors` package is via pip: \n```bash\npip install visualpriors\n```\n\nIf you would prefer to have the source code, then you can clone this repo and install locally via:\n```bash\ngit clone --single-branch --branch visualpriors git@github.com:alexsax/midlevel-reps.git\ncd midlevel-reps\npip install -e .\n```\n\n<br>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "In a shell, pull the docker to your local machine\n``` bash\ndocker pull activeperception/habitat:1.0\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9940865749298108
      ],
      "excerpt": "Installation (using pip) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9985609906598091
      ],
      "excerpt": "Step 1) Run pip install visualpriors to install the visualpriors package. You'll need pytorch! \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8044146895679722
      ],
      "excerpt": "from PIL import Image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674,
        0.9133368656218674,
        0.8414743960509704
      ],
      "excerpt": "import visualpriors \nimport subprocess \n: Download a test image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8551653298391189
      ],
      "excerpt": "TF.to_pil_image(pred[0] / 2. + 0.5).save('test_normals_readout.png') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044146895679722
      ],
      "excerpt": "from PIL import Image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674,
        0.9133368656218674
      ],
      "excerpt": "import visualpriors \nimport subprocess \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8414743960509704
      ],
      "excerpt": ": Download a test image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8866054929387285
      ],
      "excerpt": "TF.to_pil_image(pred[0] / 2. + 0.5).save('test_{}_readout.png'.format(feature_type)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from midlevel import multi_representation_transform \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from visualpriors import max_coverage_transform \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alexsax/midlevel-reps/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C",
      "C++",
      "Cuda",
      "CMake",
      "Shell",
      "Dockerfile",
      "Batchfile",
      "Makefile",
      "HTML"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2017- Sergey Zagoruyko,\\nCopyright (c) 2017- Sasank Chilamkurthy, \\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "`Mid-Level Visual Representations` Improve Generalization and Sample Efficiency for Learning Visuomotor Policies</h1>",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "midlevel-reps",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "alexsax",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alexsax/midlevel-reps/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "alexsax",
        "body": "Fixed default segmentation output shapes ",
        "dateCreated": "2019-07-18T14:16:32Z",
        "datePublished": "2019-07-18T14:22:07Z",
        "html_url": "https://github.com/alexsax/midlevel-reps/releases/tag/visualpriors-v0.3.1",
        "name": "Minor updates",
        "tag_name": "visualpriors-v0.3.1",
        "tarball_url": "https://api.github.com/repos/alexsax/midlevel-reps/tarball/visualpriors-v0.3.1",
        "url": "https://api.github.com/repos/alexsax/midlevel-reps/releases/18706834",
        "zipball_url": "https://api.github.com/repos/alexsax/midlevel-reps/zipball/visualpriors-v0.3.1"
      },
      {
        "authorType": "User",
        "author_name": "alexsax",
        "body": "Initial release of `visualpriors` package",
        "dateCreated": "2019-06-12T15:27:04Z",
        "datePublished": "2019-06-12T15:30:15Z",
        "html_url": "https://github.com/alexsax/midlevel-reps/releases/tag/visualpriors-0.3",
        "name": "Initial visualpriors release",
        "tag_name": "visualpriors-0.3",
        "tarball_url": "https://api.github.com/repos/alexsax/midlevel-reps/tarball/visualpriors-0.3",
        "url": "https://api.github.com/repos/alexsax/midlevel-reps/releases/17946701",
        "zipball_url": "https://api.github.com/repos/alexsax/midlevel-reps/zipball/visualpriors-0.3"
      }
    ],
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Using mid-level vision, it is possible to train an agent in only a _single room_ and then generalize the training to novel spaces in different buildings. The feature-based agents learn faster and perform significantly better than their trained-from-scratch counterparts. For more extensive discussions about the benefits of visual priors and mid-level vision in particular, please see the [paper](http://perceptual.actor). This repository focuses on delivering easy-to-use experiments and code.\n\nWe provide dockers to reproduce and extend our results. Setting up these environments can be a pain, and docker provides a containerized environment with the environments already set up. If not already installed, install [Docker](https://docs.docker.com/install/) and [Nvidia-Docker](https://github.com/NVIDIA/nvidia-docker#quickstart).\n\n![environments](https://github.com/alexsax/midlevel-reps/blob/helper/img/mesh_figure_short.png?raw=true)\n\n\n<br>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Lastly, we just need to start the experiment. Let's try training an agent that uses predicted **surface normals** as inputs. We'll use only 1 training and 1 val process since we're just trying to visualize the results. \n```\npython -m scripts.train_rl /tmp/midlevel_logs/normals_agent run_training with uuid=normals cfg_habitat taskonomy_decoding  cfg.saving.log_interval=10 cfg.env.num_processes=2 cfg.env.num_val_processes=1\n```\n\nIf you want to compare this to an agent trained from **scratch**, you can swap this easily with:\n```\npython -m scripts.train_rl /tmp/midlevel_logs/scratch run_training with uuid=scratch cfg_habitat scratch  cfg.saving.log_interval=10 cfg.env.num_processes=2 cfg.env.num_val_processes=1\n```\n\nOr a **blinded** agent (no visual input)\n```\npython -m scripts.train_rl /tmp/midlevel_logs/blind run_training with uuid=blind cfg_habitat blind  cfg.saving.log_interval=10 cfg.env.num_processes=2 cfg.env.num_val_processes=1\n```\n\nOr using the **Max-Coverage Min-Distance Featureset**\n```\npython -m scripts.train_rl /tmp/midlevel_logs/max_coverage run_training with uuid=blind cfg_habitat max_coverage_perception  cfg.saving.log_interval=10 cfg.env.num_processes=2 cfg.env.num_val_processes=1\n```\n**Note**: You might see some NaNs in the first iteration. Not to worry! This is probably because the first logging occurs before any episodes have finished.\n\nYou can explore more configuration options in `configs/habitat.py`! We used [SACRED](https://sacred.readthedocs.io/en/latest/) for managing experiments, so any of these experiments can be easily modified from the command line.\n\n<br>\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 93,
      "date": "Wed, 29 Dec 2021 14:38:38 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Once the docker is installed you can start a new container. The following command will start a new container that can use ports on the host (so that visdom can be run from within the container).\n``` bash\ndocker run --runtime=nvidia -ti --rm \\\n    --network host --ipc=host \\\n    activeperception/habitat:1.0 bash\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Inside the docker container we can start a visdom server (to view videos) and a tensorboard instance (for better charts).\n```\nmkdir /tmp/midlevel_logs/\nscreen -S visdom_server -p 0 -X stuff \"visdom^M\"\nscreen -S visdom_server -p 0 -X stuff \"tensorboard --logdir .^M\"\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}