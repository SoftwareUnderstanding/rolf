{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1508.04025"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.948276842578752
      ],
      "excerpt": "This is a basic implementation of attentional neural machine translation (Bahdanau et al., 2015, Luong et al., 2015) in Pytorch. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pcyin/pytorch_basic_nmt",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-09-14T21:14:15Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T03:08:23Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8801428076902734,
        0.9587862602484994
      ],
      "excerpt": "This is a basic implementation of attentional neural machine translation (Bahdanau et al., 2015, Luong et al., 2015) in Pytorch. \nIt implements the model described in Luong et al., 2015, and supports label smoothing, beam-search decoding and random sampling. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.982872373245686,
        0.820410579150899
      ],
      "excerpt": "This codebase is used for instructional purposes in Stanford CS224N Nautral Language Processing with Deep Learning and CMU 11-731 Machine Translation and Sequence-to-Sequence Models. \nnmt.py: contains the neural machine translation model and training/testing code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A simple yet strong implementation of neural machine translation in pytorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pcyin/pytorch_basic_nmt/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 20,
      "date": "Mon, 27 Dec 2021 05:11:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pcyin/pytorch_basic_nmt/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pcyin/pytorch_basic_nmt",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pcyin/pytorch_basic_nmt/master/scripts/train.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8688236132161905
      ],
      "excerpt": "vocab.py: a script that extracts vocabulary from training data \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pcyin/pytorch_basic_nmt/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Perl",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# A Basic PyTorch Implementation of Attentional Neural Machine Translation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch_basic_nmt",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pcyin",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pcyin/pytorch_basic_nmt/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 61,
      "date": "Mon, 27 Dec 2021 05:11:02 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "neural-machine-translation",
      "pytorch",
      "deep-learning",
      "pytorch-implmention",
      "natural-language-processing"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide a preprocessed version of the IWSLT 2014 German-English translation task used in (Ranzato et al., 2015) [[script]](https://github.com/harvardnlp/BSO/blob/master/data_prep/MT/prepareData.sh). To download the dataset:\n\n```bash\nwget http://www.cs.cmu.edu/~pengchey/iwslt2014_ende.zip\nunzip iwslt2014_ende.zip\n```\n\nRunning the script will extract a`data/` folder which contains the IWSLT 2014 dataset.\nThe dataset has 150K German-English training sentences. The `data/` folder contains a copy of the public release of the dataset. Files with suffix `*.wmixerprep` are pre-processed versions of the dataset from Ranzato et al., 2015, with long sentences chopped and rared words replaced by a special `<unk>` token. You could use the pre-processed training files for training/developing (or come up with your own pre-processing strategy), but for testing you have to use the **original** version of testing files, ie., `test.de-en.(de|en)`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Each runnable script (`nmt.py`, `vocab.py`) is annotated using `dotopt`.\nPlease refer to the source file for complete usage.\n\nFirst, we extract a vocabulary file from the training data using the command:\n\n```bash\npython vocab.py \\\n    --train-src=data/train.de-en.de.wmixerprep \\\n    --train-tgt=data/train.de-en.en.wmixerprep \\\n    data/vocab.json\n```\n\nThis generates a vocabulary file `data/vocab.json`. \nThe script also has options to control the cutoff frequency and the size of generated vocabulary, which you may play with.\n\nTo start training and evaluation, simply run `scripts/train.sh`. \nAfter training and decoding, we call the official evaluation script `multi-bleu.perl` to compute the corpus-level BLEU score of the decoding results against the gold-standard.\n\n",
      "technique": "Header extraction"
    }
  ]
}