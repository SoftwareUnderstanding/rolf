{
  "citation": [
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "model_path = 'xxx/yyy/uncased_L-12_H-768_A-12' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "model_path = 'xxx/yyy/uncased_L-12_H-768_A-12' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "model_path = 'xxx/yyy/uncased_L-12_H-768_A-12' \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CyberZHG/keras-bert",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-20T01:47:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T02:03:09Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9171279516934536,
        0.8907942347715849
      ],
      "excerpt": "Implementation of the BERT. Official pre-trained models could be loaded for feature extraction and prediction. \nKashgari is a Production-ready NLP Transfer learning framework for text-labeling and text-classification \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9028517043189819,
        0.8979621745284256
      ],
      "excerpt": "In feature extraction demo, you should be able to get the same extraction results as the official model chinese_L-12_H-768_A-12. And in prediction demo, the missing word in the sentence could be predicted. \nThe Tokenizer class is used for splitting texts and generating indices: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186989955572102
      ],
      "excerpt": "texts = ['all work and no play', 'makes jack a dull boy~'] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9364906364719672,
        0.9236912104839708
      ],
      "excerpt": "The returned result is a list with the same length as texts. Each item in the list is a numpy array truncated by the length of the input. The shapes of outputs in this example are (7, 768) and (8, 768). \nWhen the inputs are paired-sentences, and you need the outputs of NSP and max-pooling of the last 4 layers: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186989955572102,
        0.9186989955572102
      ],
      "excerpt": "    ('all work and no play', 'makes jack a dull boy'), \n    ('makes jack a dull boy', 'all work and no play'), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9591985073202353,
        0.8432083120814489
      ],
      "excerpt": "There are no token features in the results. The outputs of NSP and max-pooling will be concatenated with the final shape (768 x 4 x 2,). \nThe second argument in the helper function is a generator. To extract features from file: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of BERT that could load official pre-trained models for feature extraction and prediction",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Several download urls has been added. You can get the downloaded and uncompressed path of a checkpoint by:\n\n```python\nfrom keras_bert import get_pretrained, PretrainedList, get_checkpoint_paths\n\nmodel_path = get_pretrained(PretrainedList.multi_cased_base)\npaths = get_checkpoint_paths(model_path)\nprint(paths.config, paths.checkpoint, paths.vocab)\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CyberZHG/keras-bert/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 503,
      "date": "Fri, 24 Dec 2021 23:29:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/CyberZHG/keras-bert/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "CyberZHG/keras-bert",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/CyberZHG/keras-bert/master/demo/tune/keras_bert_classification_tpu.ipynb",
      "https://raw.githubusercontent.com/CyberZHG/keras-bert/master/demo/load_model/keras_bert_load_and_extract_tpu.ipynb",
      "https://raw.githubusercontent.com/CyberZHG/keras-bert/master/demo/load_model/keras_bert_load_and_predict.ipynb",
      "https://raw.githubusercontent.com/CyberZHG/keras-bert/master/demo/load_model/keras_bert_load_and_extract.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/CyberZHG/keras-bert/master/test.sh",
      "https://raw.githubusercontent.com/CyberZHG/keras-bert/master/publish.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\npip install keras-bert\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from keras_bert import Tokenizer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8342360270817577
      ],
      "excerpt": "print(tokenizer.tokenize('unaffable'))  #: The result should be ['[CLS]', 'un', '#:#:aff', '#:#:able', '[SEP]'] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8755033686048541,
        0.9276093510169802
      ],
      "excerpt": "print(segments)  #: Should be [0, 0, 0, 0, 0] \nprint(tokenizer.tokenize(first='unaffable', second='\u94a2')) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8755033686048541
      ],
      "excerpt": "print(segments)  #: Should be [0, 0, 0, 0, 0, 1, 1, 0, 0, 0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from keras_bert import extract_embeddings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from keras_bert import extract_embeddings, POOL_NSP, POOL_MAX \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674,
        0.8801854956928516
      ],
      "excerpt": "import codecs \nfrom keras_bert import extract_embeddings \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/CyberZHG/keras-bert/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Zhao HG\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Keras BERT",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "keras-bert",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "CyberZHG",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CyberZHG/keras-bert/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The [extraction demo](https://colab.research.google.com/github/CyberZHG/keras-bert/blob/master/demo/load_model/keras_bert_load_and_extract_tpu.ipynb) shows how to convert to a model that runs on TPU.\n\nThe [classification demo](https://colab.research.google.com/github/CyberZHG/keras-bert/blob/master/demo/tune/keras_bert_classification_tpu.ipynb) shows how to apply the model to simple classification tasks.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2269,
      "date": "Fri, 24 Dec 2021 23:29:06 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "keras",
      "bert",
      "language-model"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [Load Official Pre-trained Models](#Load-Official-Pre-trained-Models)\n* [Tokenizer](#Tokenizer)\n* [Train & Use](#Train-&-Use)\n* [Use Warmup](#Use-Warmup)\n* [Download Pretrained Checkpoints](#Download-Pretrained-Checkpoints)\n* [Extract Features](#Extract-Features)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nimport keras\nfrom keras_bert import get_base_dict, get_model, compile_model, gen_batch_inputs\n\n\n#: A toy input example\nsentence_pairs = [\n    [['all', 'work', 'and', 'no', 'play'], ['makes', 'jack', 'a', 'dull', 'boy']],\n    [['from', 'the', 'day', 'forth'], ['my', 'arm', 'changed']],\n    [['and', 'a', 'voice', 'echoed'], ['power', 'give', 'me', 'more', 'power']],\n]\n\n\n#: Build token dictionary\ntoken_dict = get_base_dict()  #: A dict that contains some special tokens\nfor pairs in sentence_pairs:\n    for token in pairs[0] + pairs[1]:\n        if token not in token_dict:\n            token_dict[token] = len(token_dict)\ntoken_list = list(token_dict.keys())  #: Used for selecting a random word\n\n\n#: Build & train the model\nmodel = get_model(\n    token_num=len(token_dict),\n    head_num=5,\n    transformer_num=12,\n    embed_dim=25,\n    feed_forward_dim=100,\n    seq_len=20,\n    pos_num=20,\n    dropout_rate=0.05,\n)\ncompile_model(model)\nmodel.summary()\n\ndef _generator():\n    while True:\n        yield gen_batch_inputs(\n            sentence_pairs,\n            token_dict,\n            token_list,\n            seq_len=20,\n            mask_rate=0.3,\n            swap_sentence_rate=1.0,\n        )\n\nmodel.fit_generator(\n    generator=_generator(),\n    steps_per_epoch=1000,\n    epochs=100,\n    validation_data=_generator(),\n    validation_steps=100,\n    callbacks=[\n        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n    ],\n)\n\n\n#: Use the trained model\ninputs, output_layer = get_model(\n    token_num=len(token_dict),\n    head_num=5,\n    transformer_num=12,\n    embed_dim=25,\n    feed_forward_dim=100,\n    seq_len=20,\n    pos_num=20,\n    dropout_rate=0.05,\n    training=False,      #: The input layers and output layer will be returned if `training` is `False`\n    trainable=False,     #: Whether the model is trainable. The default value is the same with `training`\n    output_layer_num=4,  #: The number of layers whose outputs will be concatenated as a single output.\n                         #: Only available when `training` is `False`.\n)\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "`AdamWarmup` optimizer is provided for warmup and decay. The learning rate will reach `lr` in `warmpup_steps` steps, and decay to `min_lr` in `decay_steps` steps. There is a helper function `calc_train_steps` for calculating the two steps:\n\n```python\nimport numpy as np\nfrom keras_bert import AdamWarmup, calc_train_steps\n\ntrain_x = np.random.standard_normal((1024, 100))\n\ntotal_steps, warmup_steps = calc_train_steps(\n    num_example=train_x.shape[0],\n    batch_size=32,\n    epochs=10,\n    warmup_proportion=0.1,\n)\n\noptimizer = AdamWarmup(total_steps, warmup_steps, lr=1e-3, min_lr=1e-5)\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Add `TF_KERAS=1` to environment variables to use `tensorflow.python.keras`.\n",
      "technique": "Header extraction"
    }
  ]
}