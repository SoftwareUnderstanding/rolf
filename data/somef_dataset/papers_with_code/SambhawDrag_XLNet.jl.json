{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1906.08237",
      "https://arxiv.org/abs/1901.02860",
      "https://arxiv.org/abs/1906.08237"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. [**XLNet: Generalized Autoregressive Pretraining for Language Understanding** - _arxiv.org_](https://arxiv.org/abs/1906.08237)\n2. [**Understanding XLNet** - _Borealis AI_](https://www.borealisai.com/en/blog/understanding-xlnet/)\n3. [**Understanding Language using XLNet with autoregressive pre-training** - _medium.com_](https://medium.com/@zxiao2015/understanding-language-using-xlnet-with-autoregressive-pre-training-9c86e5bea443)\n4. [**Sentence-Piece Subword Tokenizer** - _Google_](https://github.com/google/sentencepiece)\n5. [**Permutation Language Modelling** - _LMU Munich_](https://compstat-lmu.github.io/seminar_nlp_ss20/transfer-learning-for-nlp-ii.html#permutation-language-modelingplm)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8028046190715653
      ],
      "excerpt": "2. XLNet-Base, Cased : 12-layer, 768-hidden, 12-heads \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SambhawDrag/XLNet.jl",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-07T06:03:18Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-06T03:37:54Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9811422066946269
      ],
      "excerpt": "A Julia based implementation of XLNet: A Generalized Autoregressive Pretraining for LU. (Flux and JuliaText) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9894623941640476,
        0.9981168094580412
      ],
      "excerpt": "XLNet is an generalized autoregressive pretraining for language understanding. The XLNet paper combines recent advances in NLP with innovative choices in how the language modelling problem is approached. When trained on a very large NLP corpus, the model achieves state-of-the-art performance for the standard NLP tasks that comprise the GLUE benchmark. \nXLNet is an auto-regressive language model which outputs the joint probability of a sequence of tokens based on the Transformer architecture with recurrence. Its training objective calculates the probability of a word token conditioned on all permutations of word tokens in a sentence, as opposed to just those to the left or just those to the right of the target token. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9707113737259628
      ],
      "excerpt": "- The autoregressive language model (e.g.GPT-2) is only trained to encode a unidirectional context and not effective at modeling deep bidirectional contexts, and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8227011237337194,
        0.9878111679585518,
        0.9125125977717988,
        0.8213431918118869
      ],
      "excerpt": "XLNet borrows ideas from the two types of objectives while avoiding their limitations. \nIt is a new objective called Permutation Language Modeling. By using a permutation operation during training time, bidirectional context information can be captured and makes it a generalized order-aware autoregressive language model. No masking is required and thus the dependency between the BERT [MASK] tokens is maintained. Besides, XLNet introduces a two-stream self-attention to solve the problem that standard parameterization will reduce the model to bag-of-words.  \nAdditionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context. \nTwo versions of the XLNet model have been released, i.e.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8921987143444392,
        0.9348511160117694
      ],
      "excerpt": "and, they include similar settings of the corresponding BERT.  \nXLNet (Paper Abstract): Empirically, XLNet outperforms BERT on 20 tasks and achieves state-of-the-art results on 18 tasks.** \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9857615345340591
      ],
      "excerpt": "Specifically, for a sequence X{...} of length T, there are T different orders to perform a valid autoregressive factorization! Intuitively, if model parameters are shared across all factorization orders, in expectation, the model will learn to gather information from all positions on both sides. Let, P<sub>T</sub> be the set of all possible permutations of a sequence [1,2,\u2026, T] and use z<sub>t</sub> and z<sub><t</sub> to denote the t-th element and the first t\u22121 elements of a permutation, p \u2208 P<sub>T</sub>.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9057905208126493,
        0.9391192115186443,
        0.9212066814757167,
        0.9154559801060825,
        0.9961901406558972,
        0.9635450856696214
      ],
      "excerpt": "In the upper left plot of the above figure, when we have a factorization order: {3, 2, 4, 1}, the probability of sequence can be expressed as follows: \nFor the third token: {my}, it cannot use the information of all other tokens, so only one arrow from the starting token points to the third token in the plot. \nIn the upper right plot of the figure, when we have a factorization order: {2, 4, 3, 1}, the probability of sequence can be expressed as follows: \nHere, for the third token: {my}, it can use the information of the second and fourth tokens because it places after these two tokens in the factorization order. Correspondingly, it cannot use the information of the first token. So in the plot, in addition to the arrow from the starting token, there are arrows from the second and fourth tokens pointing to the third token. The rest two plots in the figure have the same interpretation. \nDuring training, for a fixed factorization order, XLNet is a unidirectional language model based on the transformer decoder, which performs normal model training. But different factorization order makes the model see different order of words when traversing sentences. In this way, although the model is unidirectional, it can also learn the bidirectional information of the sentence. \nIt is noteworthy that the sequence order is not actually shuffled but only attention masks are changed to reflect factorization order. With PLM, XLNet can model bidirectional context and the dependency within each token of the sequence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9127190508711804
      ],
      "excerpt": "[ ] Transformer-XL encoder-decoder base with features essential to XLNet \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A Julia-based implementation of XLNet: A Generalized Autoregressive Pretraining for Language Understanding. < Flux | JuliaText >",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SambhawDrag/XLNet.jl/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 00:50:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/SambhawDrag/XLNet.jl/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "SambhawDrag/XLNet.jl",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/SambhawDrag/XLNet.jl/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.9115584538534853
      ],
      "excerpt": "<img src=\"doc_img/PLM.png\" width=\"100%\" height=\"100%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060092474601562
      ],
      "excerpt": "[ ] Convert pre-train weights to bson \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/SambhawDrag/XLNet.jl/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Julia"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Sambhaw Kumar, &#118;&#97;&#109;&#112;&#105;&#114;&#101;&#115;&#97;&#109;&#98;&#104;&#97;&#119;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "**XLNet**",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "XLNet.jl",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "SambhawDrag",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SambhawDrag/XLNet.jl/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 00:50:33 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "transformers",
      "natural-language-processing",
      "transformer-xl",
      "pre-trained-model"
    ],
    "technique": "GitHub API"
  }
}