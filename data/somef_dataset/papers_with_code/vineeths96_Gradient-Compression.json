{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2109.12497>View Paper</a>\n  </p>\n\n\n\n</p>\n\n> tags : distributed optimization, large-scale machine learning, gradient compression, edge learning, federated learning, deep learning, pytorch \n\n\n\n### Code for the paper [Quantization for Distributed Optimization](https://arxiv.org/abs/2109.12497",
      "https://arxiv.org/abs/2109.12497",
      "https://arxiv.org/abs/1512.03385",
      "https://arxiv.org/abs/1409.1556",
      "https://arxiv.org/abs/2109.12497"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vineeths96/Gradient-Compression",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Vineeth S - vs96codes@gmail.com\n\nProject Link: [https://github.com/vineeths96/Gradient-Compression](https://github.com/vineeths96/Gradient-Compression)\n\n\n\n\n\n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n\n[contributors-shield]: https://img.shields.io/github/contributors/vineeths96/Gradient-Compression.svg?style=flat-square\n[contributors-url]: https://github.com/vineeths96/Gradient-Compression/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/vineeths96/Gradient-Compression.svg?style=flat-square\n[forks-url]: https://github.com/vineeths96/Gradient-Compression/network/members\n[stars-shield]: https://img.shields.io/github/stars/vineeths96/Gradient-Compression.svg?style=flat-square\n[stars-url]: https://github.com/vineeths96/Gradient-Compression/stargazers\n[issues-shield]: https://img.shields.io/github/issues/vineeths96/Gradient-Compression.svg?style=flat-square\n[issues-url]: https://github.com/vineeths96/Gradient-Compression/issues\n[license-shield]: https://img.shields.io/badge/License-MIT-yellow.svg\n[license-url]: https://github.com/vineeths96/Gradient-Compression/blob/master/LICENSE\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=flat-square&logo=linkedin&colorB=555\n[linkedin-url]: https://linkedin.com/in/vineeths\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-09T15:11:25Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-06T18:51:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9740529229038164,
        0.9992410970974603,
        0.9696340467506461
      ],
      "excerpt": "<!-- ABOUT THE PROJECT --> \nMassive amounts of data have led to the training of large-scale machine learning models on a single worker inefficient. Distributed machine learning methods such as Parallel-SGD have received significant interest as a solution to tackle this problem. However, the performance of distributed systems does not scale linearly with the number of workers due to the high network communication cost for synchronizing gradients and parameters. Researchers have proposed techniques such as quantization and sparsification to alleviate this problem by compressing the gradients. Most of the compression schemes result in compressed gradients that cannot be directly aggregated with efficient protocols such as all-reduce. In this paper, we present a set of all-reduce compatible gradient compression algorithms - QSGDMaxNorm Quantization, QSGDMaxNormMultiScale Quantization, and its sparsified variants - which significantly reduce the communication overhead while maintaining the performance of vanilla SGD. We establish upper bounds on the variance introduced by the quantization schemes and prove its convergence for smooth convex functions. The proposed compression schemes can trade off between the communication costs and the rate of convergence. We empirically evaluate the performance of the compression methods by training deep neural networks on the CIFAR10 dataset. We examine the performance of training ResNet50 (computation-intensive) model and VGG16 (communication-intensive) model with and without the compression methods. We also compare the scalability of these methods with the increase in the number of workers. Our compression methods perform better than the in-built methods currently offered by the deep learning frameworks. \nThis project was built with  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9039984891541929
      ],
      "excerpt": "The environment used for developing this project is available at environment.yml. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.93408484363683
      ],
      "excerpt": "We conducted experiments on ResNet50 architecture and VGG16 architecture. Refer the original papers for more information about the models. We use publicly available implementations from GitHub for reproducing the models.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.979553882450866,
        0.996514963745079,
        0.8924212690395703
      ],
      "excerpt": "We highly recommend to read through the paper before proceeding to this section. The paper explains the different compression schemes we propose and contains many more analysis & results than what is presented here.  \nWe begin with an explanation of the notations used for the plot legends in this section. AllReduce-SGD corresponds to the default gradient aggregation provided by PyTorch. QSGD-MN and GRandK-MN corresponds to QSGDMaxNorm Quantization and GlobalRandKMaxNorm Compression respectively. The precision or number of bits used for the representation follows it. QSGD-MN-TS and GRandK-MN-TS corresponds to QSGDMaxNormMultiScale Quantization and GlobalRandKMaxNormMultiScale Compression respectively, with two scales (TS) of compression. The precision or number of bits used for the representation of the two scales follows it. For the sparsified schemes, we choose the value of K as 10000 for all the experiments. We compare our methods with a recent all-reduce compatible gradient compression scheme PowerSGD for Rank-1 compression and Rank-2 compression.  \n|                           ResNet50                           |                            VGG16                             | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8693146744390485
      ],
      "excerpt": "| Scalability with number of GPUs | Scalability with number of GPUs | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "We present a set of all-reduce compatible gradient compression algorithms which significantly reduce the communication overhead while maintaining the performance of vanilla SGD. We empirically evaluate the performance of the compression methods by training deep neural networks on the CIFAR10 dataset. ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vineeths96/Gradient-Compression/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 07:22:18 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vineeths96/Gradient-Compression/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "vineeths96/Gradient-Compression",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8837680365796365,
        0.8477062130654064
      ],
      "excerpt": "python v3.7.6 \nPyTorch v1.7.1 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vineeths96/Gradient-Compression/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 vineeths96\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## Code for the paper [Quantization for Distributed Optimization](https://arxiv.org/abs/2109.12497).",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Gradient-Compression",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "vineeths96",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vineeths96/Gradient-Compression/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Create a new conda environment and install all the libraries by running the following command\n\n```shell\nconda env create -f environment.yml\n```\n\nThe dataset used in this project (CIFAR 10) will be automatically downloaded and setup in `data` directory during execution.\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The training of the models can be performed on a distributed cluster with multiple machines and multiple worker GPUs. We make use of `torch.distributed.launch` to launch the distributed training. More information is available [here](https://pytorch.org/tutorials/beginner/dist_overview.html).\n\nTo launch distributed training on a single machine with multiple workers (GPUs), \n\n```shell\npython -m torch.distributed.launch --nproc_per_node=<num_gpus> trainer.py --local_world_size=<num_gpus> \n```\n\n To launch distributed training on multiple machine with multiple workers (GPUs), \n\n```sh\nexport NCCL_SOCKET_IFNAME=ens3\n\npython -m torch.distributed.launch --nproc_per_node=<num_gpus> --nnodes=<num_machines> --node_rank=<node_rank> --master_addr=<master_address> --master_port=<master_port> trainer.py --local_world_size=<num_gpus>\n```\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sat, 25 Dec 2021 07:22:18 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "distributed-optimization",
      "large-scale",
      "machine-learning",
      "deep-learning",
      "gradient-compression",
      "federated-learning",
      "pytorch"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Clone the repository into a local machine using,\n\n```shell\ngit clone https://github.com/vineeths96/Gradient-Compression\ncd Gradient-Compression/\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}