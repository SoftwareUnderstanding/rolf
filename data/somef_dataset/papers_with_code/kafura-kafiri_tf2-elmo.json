{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1802.05365",
      "https://arxiv.org/abs/1609.07843"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[@seolhokim](https://github.com/seolhokim)\n[@geneva0901](https://github.com/geneva0901)\n[@masepehr](https://github.com/masepehr)\n[@dilaratorunoglu](https://github.com/dilaratorunoglu)\n[@Adherer](https://github.com/Adherer)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9995108841578795,
        0.9848903265658882,
        0.9992803495973879,
        0.8853878745185478
      ],
      "excerpt": "Re-implementation of ELMo in Keras based on the tensorflow implementation presented by Allen NLP (https://github.com/allenai/bilm-tf), based on Peters et al. article in NAACL 2018 (https://arxiv.org/abs/1802.05365): \nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee and Luke Zettlemoyer. 2018. Deep contextualized word representations \nNotice: The project includes WikiText-2 datasets for experimentation as published in (https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset), presented in Merity et al. 2016 (https://arxiv.org/abs/1609.07843): \nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer Sentinel Mixture Models \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kafura-kafiri/tf2-elmo",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-13T15:00:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-13T15:06:04Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9460234044475202,
        0.9432874233583652,
        0.88574900272902
      ],
      "excerpt": "I also consider Keras as the most user-friendly and industry-ready library to work with. \nNow we are also able to integrate ELMo for practical use at Cognitiv+, where we rely on Keras for our NLP engine. \nIt was really fun! This took me more than a month, in which period I had to learn many things and vastly improve my understading and skills around Keras and Tensorflow, so be kind. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "elmo layer compatible with tf2 keras",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kafura-kafiri/tf2-elmo/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 03:58:11 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kafura-kafiri/tf2-elmo/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kafura-kafiri/tf2-elmo",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kafura-kafiri/tf2-elmo/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ELMo-keras",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "tf2-elmo",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kafura-kafiri",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kafura-kafiri/tf2-elmo/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 03:58:11 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nimport os\nimport keras.backend as K\n\nfrom data import DATA_SET_DIR\nfrom elmo.lm_generator import LMDataGenerator\nfrom elmo.model import ELMo\n\nparameters = {\n    'multi_processing': False,\n    'n_threads': 4,\n    'cuDNN': True if len(K.tensorflow_backend._get_available_gpus()) else False,\n    'train_dataset': 'wikitext-2/wiki.train.tokens',\n    'valid_dataset': 'wikitext-2/wiki.valid.tokens',\n    'test_dataset': 'wikitext-2/wiki.test.tokens',\n    'vocab': 'wikitext-2/wiki.vocab',\n    'vocab_size': 28914,\n    'num_sampled': 1000,\n    'charset_size': 262,\n    'sentence_maxlen': 100,\n    'token_maxlen': 50,\n    'token_encoding': 'word',\n    'epochs': 10,\n    'patience': 2,\n    'batch_size': 1,\n    'clip_value': 5,\n    'cell_clip': 5,\n    'proj_clip': 5,\n    'lr': 0.2,\n    'shuffle': True,\n    'n_lstm_layers': 2,\n    'n_highway_layers': 2,\n    'cnn_filters': [[1, 32],\n                    [2, 32],\n                    [3, 64],\n                    [4, 128],\n                    [5, 256],\n                    [6, 512],\n                    [7, 512]\n                    ],\n    'lstm_units_size': 400,\n    'hidden_units_size': 200,\n    'char_embedding_size': 16,\n    'dropout_rate': 0.1,\n    'word_dropout_rate': 0.05,\n    'weight_tying': True,\n}\n\n#: Set-up Generators\ntrain_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['train_dataset']),\n                                  os.path.join(DATA_SET_DIR, parameters['vocab']),\n                                  sentence_maxlen=parameters['sentence_maxlen'],\n                                  token_maxlen=parameters['token_maxlen'],\n                                  batch_size=parameters['batch_size'],\n                                  shuffle=parameters['shuffle'],\n                                  token_encoding=parameters['token_encoding'])\n\nval_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['valid_dataset']),\n                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n                                sentence_maxlen=parameters['sentence_maxlen'],\n                                token_maxlen=parameters['token_maxlen'],\n                                batch_size=parameters['batch_size'],\n                                shuffle=parameters['shuffle'],\n                                token_encoding=parameters['token_encoding'])\n\ntest_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['test_dataset']),\n                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n                                sentence_maxlen=parameters['sentence_maxlen'],\n                                token_maxlen=parameters['token_maxlen'],\n                                batch_size=parameters['batch_size'],\n                                shuffle=parameters['shuffle'],\n                                token_encoding=parameters['token_encoding'])\n\n#: Compile ELMo\nelmo_model = ELMo(parameters)\nelmo_model.compile_elmo(print_summary=True)\n\n#: Train ELMo\nelmo_model.train(train_data=train_generator, valid_data=val_generator)\n\n#: Persist ELMo Bidirectional Language Model in disk\nelmo_model.save(sampled_softmax=False)\n\n#: Evaluate Bidirectional Language Model\nelmo_model.evaluate(test_generator)\n\n#: Build ELMo meta-model to deploy for production and persist in disk\nelmo_model.wrap_multi_elmo_encoder(print_summary=True, save=True)\n\n#: Load ELMo encoder\nelmo_model.load_elmo_encoder()\n\n#: Get ELMo embeddings to feed as inputs for downstream tasks\nelmo_embeddings = elmo_model.get_outputs(test_generator, output_type='word', state='mean')\n\n#: BUILD & TRAIN NEW KERAS MODEL FOR DOWNSTREAM TASK (E.G., TEXT CLASSIFICATION)\n\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}