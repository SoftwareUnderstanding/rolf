{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{li2021focal,\n  title={{FOCAL}: Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization},\n  author={Lanqing Li and Rui Yang and Dijun Luo},\n  booktitle={International Conference on Learning Representations},\n  year={2021},\n  url={https://openreview.net/forum?id=8cpHIfgY4Dj}\n}\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{li2021focal,\n  title={{FOCAL}: Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization},\n  author={Lanqing Li and Rui Yang and Dijun Luo},\n  booktitle={International Conference on Learning Representations},\n  year={2021},\n  url={https://openreview.net/forum?id=8cpHIfgY4Dj}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/LanqingLi1993/FOCAL-ICLR",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-02T12:47:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-28T08:47:42Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9988709475605754,
        0.999107539737879
      ],
      "excerpt": "> Meta-learning for offline reinforcement learning (OMRL) is an understudied problem with tremendous potential impact by enabling RL algorithms in many real-world applications. A popular solution to the problem is to infer task identity as augmented state using a context-based encoder, for which efficient learning of task representations remains an open challenge. In this work, we improve upon one of the SOTA OMRL algorithms, FOCAL, by incorporating intra-task attention mechanism and inter-task contrastive learning objectives for more effective task inference and learning of control. Theoretical analysis and experiments are presented to demonstrate the superior performance, efficiency and robustness of our end-to-end and model-free method compared to prior algorithms across multiple meta-RL benchmarks. --> \nWe study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8116174061469957
      ],
      "excerpt": "Example of training policies and generating trajectories on multiple tasks: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8029096748007006
      ],
      "excerpt": "Generated data will be saved in ./data/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.929340074432973
      ],
      "excerpt": "edit walker_rand_params.json to add dump_eval_paths=1 and data_dir=./data/walker_rand_params \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for FOCAL Paper Published at ICLR 2021",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/FOCAL-ICLR/FOCAL-ICLR/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Sun, 26 Dec 2021 05:06:22 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/LanqingLi1993/FOCAL-ICLR/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "LanqingLi1993/FOCAL-ICLR",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/FOCAL-ICLR/FOCAL-ICLR/master/sparse-point-robot-vis.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/FOCAL-ICLR/FOCAL-ICLR/master/rand_param_envs/mujoco_py/gen_binding.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To install locally, you will need to first install [MuJoCo](https://www.roboti.us/index.html). For task distributions in which the reward function varies (Cheetah, Ant), install MuJoCo150 or plus. Set `LD_LIBRARY_PATH` to point to both the MuJoCo binaries (`/$HOME/.mujoco/mujoco200/bin`) as well as the gpu drivers (something like `/usr/lib/nvidia-390`, you can find your version by running `nvidia-smi`).\n\nFor the remaining dependencies, create conda environment by\n```\nconda env create -f environment.yaml\n```\n\n<!-- For task distributions where the transition function (dynamics)  varies  -->\n\n**For Walker environments**, MuJoCo131 is required.\nSimply install it the same way as MuJoCo200. To swtch between different MuJoCo versions:\n\n```\nexport MUJOCO_PY_MJPRO_PATH=~/.mujoco/mjpro${VERSION_NUM}\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/.mujoco/mjpro${VERSION_NUM}/bin\n``` \n\nThe environments make use of the module `rand_param_envs` which is submoduled in this repository. Add the module to your python path, `export PYTHONPATH=./rand_param_envs:$PYTHONPATH` (Check out [direnv](https://direnv.net/) for handy directory-dependent path managenement.)\n\n\nThis installation has been tested only on 64-bit CentOS 7.2. The whole pipeline consists of two stages: **data generation** and **Offline RL experiments**:\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8788217729872652
      ],
      "excerpt": "python policy_train.py --gpu 0 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9179399058988049,
        0.8469866649922542,
        0.8943096850060217
      ],
      "excerpt": "python policy_train.py --gpu 0 \nGenerate trajectories from pretrained models \npython policy_train.py --eval \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8933729386792785
      ],
      "excerpt": "python launch_experiment.py ./configs/[EXP].json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8274819048297175,
        0.8142511446291579,
        0.9153225945037594
      ],
      "excerpt": "download walker data and unzip the data to ./data/walker_rand_params \nedit walker_rand_params.json to add dump_eval_paths=1 and data_dir=./data/walker_rand_params \nrun python launch_experiment.py ./configs/walker_rand_params.json \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/LanqingLi1993/FOCAL-ICLR/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Ruby",
      "Jupyter Notebook",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Denis Yarats\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "FOCAL: Efficient Fully-Offline Meta-Reinforcement Learning Via Distance Metric Learning and Behavior Regularization",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "FOCAL-ICLR",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "LanqingLi1993",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/LanqingLi1993/FOCAL-ICLR/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 21,
      "date": "Sun, 26 Dec 2021 05:06:22 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "offline-reinforcement-learning",
      "meta-learning",
      "distance-metric-learning",
      "meta-rl",
      "multi-task-learning",
      "reinforcement-learning"
    ],
    "technique": "GitHub API"
  }
}