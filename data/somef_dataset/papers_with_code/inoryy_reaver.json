{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "A predecessor to Reaver, named simply `pysc2-rl-agent`, was developed as the practical part of\n[bachelor's thesis](https://github.com/inoryy/bsc-thesis) at the University of Tartu under the\nsupervision of [Ilya Kuzovkin](https://github.com/kuz) and [Tambet Matiisen](https://github.com/tambetm).\nYou can still access it on the [v1.0](https://github.com/inoryy/reaver-pysc2/tree/v1.0) branch.\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1708.04782",
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1806.01830",
      "https://arxiv.org/abs/1708.04782",
      "https://arxiv.org/abs/1709.06560"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you have found Reaver useful in your research, please consider citing it with the following bibtex:\n\n```\n@misc{reaver,\n  author = {Ring, Roman},\n  title = {Reaver: Modular Deep Reinforcement Learning Framework},\n  year = {2018},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/inoryy/reaver}},\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{reaver,\n  author = {Ring, Roman},\n  title = {Reaver: Modular Deep Reinforcement Learning Framework},\n  year = {2018},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/inoryy/reaver}},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9173424125938073
      ],
      "excerpt": "However, if possible please consider using Linux OS instead - due to performance and stability considerations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "gym >= 0.10.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "CollectMineralShards        |    102.8 (10.8)<br>[81, 135] |            103 |            196 |          177 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8852702150959532
      ],
      "excerpt": "DeepMind SC2LE are results published in StarCraft II: A New Challenge for Reinforcement Learning article. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "DefeatZerglingsAndBanelings |     10,496,000 |        273,463 |                15 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/inoryy/reaver",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-10-21T14:03:58Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T12:59:16Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To lead the way with reproducibility, Reaver is bundled with pre-trained weights and full Tensorboard summary logs for all six minigames. \nSimply download an experiment archive from the [releases](https://github.com/inoryy/reaver-pysc2/releases) tab and unzip onto the `results/` directory.\n\nYou can use pre-trained weights by appending `--experiment` flag to `reaver.run` command:\n\n    python reaver.run --map <map_name> --experiment <map_name>_reaver --test 2> stderr.log\n\nTensorboard logs are available if you launch `tensorboard --logidr=results/summaries`.  \nYou can also view them [directly online](https://boards.aughie.org/board/HWi4xmuvuOSuw09QBfyDD-oNF1U) via [Aughie Boards](https://boards.aughie.org/).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Reaver is a modular deep reinforcement learning framework with a focus on various StarCraft II based tasks, following in DeepMind's footsteps \nwho are pushing state-of-the-art of the field through the lens of playing a modern video game with human-like interface and limitations. \nThis includes observing visual features similar (though not identical) to what a human player would perceive and choosing actions from similar pool of options a human player would have.\nSee [StarCraft II: A New Challenge for Reinforcement Learning](https://arxiv.org/abs/1708.04782) article for more details.\n\nThough development is research-driven, the philosophy behind Reaver API is akin to StarCraft II game itself - \nit has something to offer both for novices and experts in the field. For hobbyist programmers Reaver offers all the tools\nnecessary to train DRL agents by modifying only a small and isolated part of the agent (e.g. hyperparameters).\nFor veteran researchers Reaver offers simple, but performance-optimized codebase with modular architecture: \nagent, model, and environment are decoupled and can be swapped at will.\n\nWhile the focus of Reaver is on StarCraft II, it also has full support for other popular environments, notably Atari and MuJoCo. \nReaver agent algorithms are validated against reference results, e.g. PPO agent is able to match [\nProximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347). Please see [below](#but-wait-theres-more) for more details.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8408240870563828,
        0.9377379605099296
      ],
      "excerpt": "Project status: No longer maintained! \nUnfortunately, I am no longer able to further develop or provide support to the project. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8358068271904855
      ],
      "excerpt": "Please see the wiki page for detailed instructions on setting up Reaver on Windows. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8726051871626038
      ],
      "excerpt": "If you would like to see your agent perform with full graphics enabled you can save a replay of the agent on Linux and open it on Windows. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8432929337641032,
        0.924129564676367
      ],
      "excerpt": "A companion Google Colab notebook  \nnotebook is available to try out Reaver online. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8666881153354514
      ],
      "excerpt": "Majority of open source implementations solve this task with message-based approach (e.g. Python multiprocessing.Pipe or MPI), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.956382989719527,
        0.9694368568138128,
        0.9273282178835727,
        0.894248873432467,
        0.8774101883908654
      ],
      "excerpt": "This is a valid and most likely only reasonable approach for large-scale distributed approaches that companies like DeepMind and openAI operate on.  \nHowever, for a typical researcher or hobbyist a much more common scenario is having access only to a  \nsingle machine environment, whether it is a laptop or a node on a HPC cluster. Reaver is optimized specifically  \nfor this case by making use of shared memory in a lock-free manner. This approach nets significant performance \nboost of up to 1.5x speed-up in StarCraft II sampling rate (and up to 100x speedup in general case), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8788963196928182,
        0.887711980759799
      ],
      "excerpt": "The three core Reaver modules - envs, models, and agents are almost completely detached from each other. \nThis ensures that extending functionality in one module is seamlessly integrated into the others. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8831997339434585
      ],
      "excerpt": "This includes all hyperparameters, environment arguments, and model definitions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9919138203168285,
        0.8605323809978866
      ],
      "excerpt": "When experimenting with novel ideas it is important to get feedback quickly, which is often not realistic with complex environments like StarCraft II. \nAs Reaver was built with modular architecture, its agent implementations are not actually tied to StarCraft II at all. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8722760661698656
      ],
      "excerpt": "Currently the following environments are supported by Reaver: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9443598066986328
      ],
      "excerpt": "DeepMind ReDRL refers to current state-of-the-art results, described in Relational Deep Reinforcement Learning article. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8352684946104649
      ],
      "excerpt": "Listed are the mean, standard deviation (in parentheses), and min & max (in square brackets). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8912186862868262,
        0.9786105077986513
      ],
      "excerpt": "Approx. Time is the approximate training time on a laptop with Intel i5-7300HQ CPU (4 cores) and GTX 1050 GPU. \nNote that I did not put much time into hyperparameter tuning, focusing mostly on verifying that the agent is capable of learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042066745761318,
        0.8874223454719018,
        0.9069555038699413,
        0.9691234170493925,
        0.9223597122301223
      ],
      "excerpt": "however after some playing around I was able to reduce it down all the way to 102,000 (~40x reduction) with PPO agent. \nMean episode rewards with std.dev filled in-between. Click to enlarge. \nA video recording of the agent performing on all six minigames is available online at: https://youtu.be/gEyBzcPU5-w. \nIn the video on the left is the agent acting in with randomly initialized weights and no training, whereas on the right he is trained to target scores. \nThe problem of reproducibility of research has recently become a subject of many debates in science \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.955761682154533,
        0.990061862614723,
        0.9683980271998027,
        0.9201539372575046
      ],
      "excerpt": "and Reinforcement Learning is not an exception. \nOne of the goals of Reaver as a scientific project is to help facilitate reproducible research. \nTo this end Reaver comes bundled with various tools that simplify the process: \nAll experiments are saved into separate folders with automatic model checkpoints enabled by default \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Reaver: Modular Deep Reinforcement Learning Framework. Focused on StarCraft II. Supports Gym, Atari, and MuJoCo.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/inoryy/reaver/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 92,
      "date": "Mon, 27 Dec 2021 17:33:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/inoryy/reaver/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "inoryy/reaver",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you plan to modify `Reaver` codebase you can retain its module functionality by installing from source:\n\n```\n$ git clone https://github.com/inoryy/reaver-pysc2\n$ pip install -e reaver-pysc2/\n```\n\nBy installing with `-e` flag `Python` will now look for `reaver` in the specified folder, rather than `site-packages` storage.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9614744573923605,
        0.999746712887969,
        0.8589966129428701,
        0.9700994190543226,
        0.8688367303881972
      ],
      "excerpt": "Easiest way to install Reaver is through the PIP package manager: \npip install reaver \nYou can also install additional extras (e.g. gym support) through the helper flags: \npip install reaver[gym,atari,mujoco] \nPlease see the wiki page for detailed instructions on setting up Reaver on Windows. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8921880341007349
      ],
      "excerpt": "If you would like to use Reaver with other supported environments, you must install relevant packages as well: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8806683282552588,
        0.886344732337124
      ],
      "excerpt": "StarCraft II via PySC2 (tested on all minigames) \nopenAI Gym (tested on CartPole-v0) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8477749024404176,
        0.860968659888874
      ],
      "excerpt": "atari-py >= 0.1.5 \nmujoco-py >= 1.50.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9012248701992861
      ],
      "excerpt": "import reaver as rvr \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8377621879540922
      ],
      "excerpt": "BuildMarines                |                           -- |              3 |            123 |          133 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9182186976745903
      ],
      "excerpt": "Results are gathered by running the trained agent in --test mode for 100 episodes, calculating episode total rewards. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/inoryy/reaver/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Roman Ring\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Reaver: Modular Deep Reinforcement Learning Framework",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "reaver",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "inoryy",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/inoryy/reaver/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "inoryy",
        "body": "Rewrite of the project tabula rasa. See README.md for an overview.",
        "dateCreated": "2018-11-25T16:29:35Z",
        "datePublished": "2018-11-25T16:43:36Z",
        "html_url": "https://github.com/inoryy/reaver/releases/tag/v2.0.0",
        "name": "StarCraft II Deep Reinforcement Learning Agent",
        "tag_name": "v2.0.0",
        "tarball_url": "https://api.github.com/repos/inoryy/reaver/tarball/v2.0.0",
        "url": "https://api.github.com/repos/inoryy/reaver/releases/14172562",
        "zipball_url": "https://api.github.com/repos/inoryy/reaver/zipball/v2.0.0"
      },
      {
        "authorType": "User",
        "author_name": "inoryy",
        "body": "Codebase as it is described in bachelor's thesis: [Replicating DeepMind StarCraft II Reinforcement Learning Benchmark with Actor-Critic Methods](https://github.com/inoryy/bsc-thesis) at University of Tartu, 2018.  \r\nSupervised by Ilya Kuzovkin and Tambet Matiisen.\r\n\r\n**Abstract.** Reinforcement Learning (RL) is a subfield of Artificial Intelligence (AI) that deals with agents navigating in an environment with the goal of maximizing total reward. Games are good environments to test RL algorithms as they have simple rules and clear reward signals. Theoretical part of this thesis explores some of the popular classical and modern RL approaches, which include the use of Artificial Neural Network (ANN) as a function approximator inside AI agent. In practical part of the thesis we implement Advantage Actor-Critic RL algorithm and replicate ANN based agent described in [Vinyals et al., 2017](https://arxiv.org/abs/1708.04782). We reproduce the state-of-the-art results in a modern video game StarCraft II, a game that is considered the next milestone in AI after the fall of chess and Go.\r\n\r\n**Full text:** [Digital version](https://github.com/inoryy/bsc-thesis/raw/master/ring_roman_bsc_2018.pdf)",
        "dateCreated": "2018-11-20T17:48:04Z",
        "datePublished": "2018-11-20T17:58:17Z",
        "html_url": "https://github.com/inoryy/reaver/releases/tag/v1.0.0",
        "name": "Bachelor's Thesis Codebase",
        "tag_name": "v1.0.0",
        "tarball_url": "https://api.github.com/repos/inoryy/reaver/tarball/v1.0.0",
        "url": "https://api.github.com/repos/inoryy/reaver/releases/14103856",
        "zipball_url": "https://api.github.com/repos/inoryy/reaver/zipball/v1.0.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* PySC2 >= 3.0.0\n* StarCraft II >= 4.1.2 ([instructions](https://github.com/Blizzard/s2client-proto#downloads))\n* gin-config >= 0.3.0\n* TensorFlow >= 2.0.0\n* TensorFlow Probability >= 0.9\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 522,
      "date": "Mon, 27 Dec 2021 17:33:07 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you encounter a codebase related problem then please open a ticket on GitHub and describe it in as much detail as possible. \nIf you have more general questions or simply seeking advice feel free to send me an email.\n\nI am also a proud member of an active and friendly [SC2AI](http://sc2ai.net) online community, \nwe mostly use [Discord](https://discordapp.com/invite/Emm5Ztz) for communication. People of all backgrounds and levels of expertise are welcome to join!\n\n",
      "technique": "Header extraction"
    }
  ],
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "artificial-intelligence",
      "deep-learning",
      "machine-learning",
      "reinforcement-learning",
      "actor-critic",
      "tensorflow",
      "pysc2",
      "starcraft-ii",
      "starcraft2",
      "deepmind"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can train a DRL agent with multiple StarCraft II environments running in parallel with just four lines of code!\n\n```python\nimport reaver as rvr\n\nenv = rvr.envs.SC2Env(map_name='MoveToBeacon')\nagent = rvr.agents.A2C(env.obs_spec(), env.act_spec(), rvr.models.build_fully_conv, rvr.models.SC2MultiPolicy, n_envs=4)\nagent.run(env)\n```\n\nMoreover, Reaver comes with highly configurable commandline tools, so this task can be reduced to a short one-liner!\n\n```bash\npython -m reaver.run --env MoveToBeacon --agent a2c --n_envs 4 2> stderr.log\n```\n\nWith the line above Reaver will initialize the training procedure with a set of pre-defined hyperparameters, optimized\nspecifically for the given environment and agent. After awhile you will start seeing logs with various useful statistics\nin your terminal screen.\n\n    | T    118 | Fr     51200 | Ep    212 | Up    100 | RMe    0.14 | RSd    0.49 | RMa    3.00 | RMi    0.00 | Pl    0.017 | Vl    0.008 | El 0.0225 | Gr    3.493 | Fps   433 |\n    | T    238 | Fr    102400 | Ep    424 | Up    200 | RMe    0.92 | RSd    0.97 | RMa    4.00 | RMi    0.00 | Pl   -0.196 | Vl    0.012 | El 0.0249 | Gr    1.791 | Fps   430 |\n    | T    359 | Fr    153600 | Ep    640 | Up    300 | RMe    1.80 | RSd    1.30 | RMa    6.00 | RMi    0.00 | Pl   -0.035 | Vl    0.041 | El 0.0253 | Gr    1.832 | Fps   427 |\n    ...\n    | T   1578 | Fr    665600 | Ep   2772 | Up   1300 | RMe   24.26 | RSd    3.19 | RMa   29.00 | RMi    0.00 | Pl    0.050 | Vl    1.242 | El 0.0174 | Gr    4.814 | Fps   421 |\n    | T   1695 | Fr    716800 | Ep   2984 | Up   1400 | RMe   24.31 | RSd    2.55 | RMa   30.00 | RMi   16.00 | Pl    0.005 | Vl    0.202 | El 0.0178 | Gr   56.385 | Fps   422 |\n    | T   1812 | Fr    768000 | Ep   3200 | Up   1500 | RMe   24.97 | RSd    1.89 | RMa   31.00 | RMi   21.00 | Pl   -0.075 | Vl    1.385 | El 0.0176 | Gr   17.619 | Fps   423 |\n\n\nReaver should quickly converge to about 25-26 `RMe` (mean episode rewards), which matches DeepMind results for this environment.\nSpecific training time depends on your hardware. Logs above are produced on a laptop with Intel i5-7300HQ CPU (4 cores)\nand GTX 1050 GPU, the training took around 30 minutes.\n\nAfter Reaver has finished training, you can look at how it performs by appending `--test` and `--render` flags to the one-liner.\n\n```bash\npython -m reaver.run --env MoveToBeacon --agent a2c --test --render 2> stderr.log\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}