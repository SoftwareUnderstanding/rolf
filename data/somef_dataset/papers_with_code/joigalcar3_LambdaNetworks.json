{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2102.08602",
      "https://arxiv.org/abs/2102.08602",
      "https://arxiv.org/abs/2007.14902"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9584362394001452
      ],
      "excerpt": " has published \"LambdaNetworks: Modeling long-range interactions without Attention (Paper Explained)\" on YouTube 4  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8119782802278984
      ],
      "excerpt": "  rate scheduler and imports a checkpoint if necessary. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/joigalcar3/LambdaNetworks",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-25T13:01:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-04T11:23:27Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This work builds on the foundations laid by Irwan Bello in \"LambdaNetworks: Modeling long-range interactions without attention\" \n[(Bello, 2021)](https://arxiv.org/abs/2102.08602). Bello proposes a method where long-range interactions are modeled by layers which \ntransform contexts into linear functions called lambdas, in order to avoid the use of attention maps. The great advantage \nof lambda layers is that they require much less compute than self-attention mechanisms according to the original paper \nby Bello. This is fantastic, because it does not only provide results faster, but also saves money and has a more \nfavorable carbon footprint! However, Bello still uses 32 [TPUv3s](https://cloud.google.com/tpu) and the \n200 GB sized ImageNet classification dataset. Therefore, we started this reproducibility project wondering: Could lambda \nlayers be scaled to mainstream computers while keeping its attractive properties?\n\nIn 2021 the world did not only have to deal with the [COVID-19 epidemic](https://www.who.int/emergencies/diseases/novel-coronavirus-2019) \nbut was struck by [chip shortages](https://www.cnbc.com/2021/02/10/whats-causing-the-chip-shortage-affecting-ps5-cars-and-more.html) \nas well due to increase in consumer electronics for working at home, shut down factories in China and the rising prices of crypto-currencies. \nThis has decreased supply to record lows and prices to record highs. Resulting in a situation, whereby researchers, academics, \nand students (who are all usually on a budget) are no longer able to quickly build a cluster out of COTS (commercial off-the-shelf) \nGPUs resulting in having to deal with older, less, and less efficient hardware.\n\nNo official code was released at the time of starting the project mid-March. Therefore, in order to answer the \naforementioned question, it is up to us to reproduce the paper by Bello as accurately as possible while trying to scale \nit down such that it can be run on an average consumer computer.\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.800680449889004
      ],
      "excerpt": "Lambda layers are closely related to self-attention mechanisms as they allow for modeling long-range interactions.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920098279347524
      ],
      "excerpt": "for modeling the relative importance of layer activations, which require additional compute and are hungry for RAM  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9523247584312584,
        0.8696153728939613
      ],
      "excerpt": "between each of these pixels. Therefore, it is evident that this problem should be solved in order to decrease the  \ntraining and inference times of any attention based vision task. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8238400574721391
      ],
      "excerpt": "model long-range interactions between a query and a structured set of context elements at a reduced memory  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9438981415930572
      ],
      "excerpt": "applied to the corresponding query.\" This set of context elements is consequently summarized by the lambda layer into a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.817555976908081
      ],
      "excerpt": "Linear attention mechanisms Li et al., 2020 have posed a solution to the problem of high memory usage. However,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9687373769427048
      ],
      "excerpt": "increased performance, such that it outperforms convolutions with linear attention and local relative self-attention on  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8738483098025004
      ],
      "excerpt": "The paper by Bello has been published on February 2, 2021 and has been cited 7 times at the time of this work.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9796203102963057
      ],
      "excerpt": " review of the paper.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.964432350873925
      ],
      "excerpt": " months prior to the publication of the article by Bello.  Kilcher goes over the preliminary version of the paper and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9282101587905442
      ],
      "excerpt": " published a video along the lines of Kilcher but takes more time to extensively clarify the distinction between the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.96178197886855,
        0.8499308008941087,
        0.9804729583990587
      ],
      "excerpt": "reproduced the lambda layer code, but this member of the community has also applied it to different ResNet versions \n and different datasets, as well as performing an ablation study.  \n The code from the aforementioned data scientists is not used for generating our code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.910397615939677,
        0.9573172765389707,
        0.9810489343494178,
        0.9629962971611608,
        0.972916295303839
      ],
      "excerpt": "For a complete explanation of the lambda layers, their implementation and their integration within the ResNet-50  \narchitecture, as well as our results, conclusions and recommendations derived from this work can be found in our blog.  \nBesides that, we have also designed a poster which briefly illustrates the work carried out as part of this  \nreproducibility project. \nHere follows a short description of the files that make up this repository, as well as a short description of how to get  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8888962697094328
      ],
      "excerpt": "* user_input.py: this file contains all the inputs that the user would like to modify \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8957693972092902
      ],
      "excerpt": "* model_preparation.py: this file prepares the model. It selects the right model, optimizer, the criterion, learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8335408881789573
      ],
      "excerpt": "* resnet.py: code to generate the ResNet architectures. Code borrowed from Pytorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108282936884011
      ],
      "excerpt": "* main_general.py: this file contains the main loop (as a function of the epochs) and calls some of the previous  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8686615898126459
      ],
      "excerpt": ", where yyyyy refers to the type of model being run (Lambda or Baseline) and xxxxx refers to the name of the file  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802680034823316,
        0.9546228441370707
      ],
      "excerpt": "Hope you find our work helpful for understanding the lambda layers and it will help you to integrate them in your personal  \nproject. Do not hesitate to reach out to us in the case that you have any questions: \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/joigalcar3/LambdaNetworks/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 02:02:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/joigalcar3/LambdaNetworks/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "joigalcar3/LambdaNetworks",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9843879943423856
      ],
      "excerpt": "To start using the code you can download the required Python libraries stored within requirements.txt. For that purpose, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9979947896609701
      ],
      "excerpt": "pip install -r requirements.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9879863063452118
      ],
      "excerpt": "!git clone https://github.com/joigalcar3/LambdaNetworks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8641590980005928
      ],
      "excerpt": " then run the following lines: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.850291357106684
      ],
      "excerpt": "* data_preprocessing.py: this file downloads the CIFAR-10 data and performs data augmentation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9273207620401572
      ],
      "excerpt": "* utils.py: file required by resnet.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826941221466896,
        0.8959034317274307,
        0.895440807014206
      ],
      "excerpt": "* nntrain.py: contains the training function. \n* nntest.py: contains the test function. \n* log_data.py: data logger and printer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "!python LambdaNetworks/main_general.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8238375460560197
      ],
      "excerpt": "from google.colab import files \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/joigalcar3/LambdaNetworks/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Jose Ignacio de Alvear Cardenas\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "LambdaNetworks",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "LambdaNetworks",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "joigalcar3",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/joigalcar3/LambdaNetworks/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Mon, 27 Dec 2021 02:02:17 GMT"
    },
    "technique": "GitHub API"
  }
}