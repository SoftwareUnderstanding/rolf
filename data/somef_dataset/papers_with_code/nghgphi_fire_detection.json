{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.10934\n\nMore details: [medium link](https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&sk=6039748846bbcf1d960c3061542591d7",
      "https://arxiv.org/abs/2004.10934",
      "https://arxiv.org/abs/2004.10934 \n\n\ntkDNN-TensorRT accelerates YOLOv4 **~2x** times for batch=1 and **3x-4x** times for batch=4.\n* tkDNN: https://github.com/ceccocats/tkDNN\n* OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf\n\n#### GeForce RTX 2080 Ti:\n| Network Size \t| Darknet, FPS (avg",
      "https://arxiv.org/abs/1911.11929",
      "https://arxiv.org/abs/2004.10934"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9977994744046882
      ],
      "excerpt": "Paper Yolo v4: https://arxiv.org/abs/2004.10934 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "Manual: https://github.com/AlexeyAB/darknet/wiki \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877118600674591
      ],
      "excerpt": "How to improve object detection \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977994744046882
      ],
      "excerpt": " AP50:95 / AP50 - FPS (Tesla V100) Paper: https://arxiv.org/abs/2004.10934  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479,
        0.9105368110547479
      ],
      "excerpt": "* tkDNN: https://github.com/ceccocats/tkDNN \n* OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9771565324188672
      ],
      "excerpt": "CSPNet: paper and map_fps comparison: https://github.com/WongKinYiu/CrossStagePartialNetworks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9836589059395289
      ],
      "excerpt": "Yolo v3 on MS COCO (Yolo v3 vs RetinaNet) - Figure 3: https://arxiv.org/pdf/1804.02767v1.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.901083715581408
      ],
      "excerpt": "Yolo v2 on Pascal VOC 2012 (comp4): https://hsto.org/files/3a6/fdf/b53/3a6fdfb533f34cee9b52bdd9bb0b19d9.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9310113855078828
      ],
      "excerpt": "TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9230616492713429
      ],
      "excerpt": "Netron - Visualizer for neural networks: https://github.com/lutzroeder/netron \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9179061236063182
      ],
      "excerpt": "German/Belgium/Russian/LISA/MASTIF Traffic Sign Datasets for Detection - use this parsers: https://github.com/angeligareta/Datasets2Darknet#detection-task \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8306763126980392
      ],
      "excerpt": "added example of Detection and Tracking objects: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8343628678678856
      ],
      "excerpt": "Open a shell terminal inside the cloned repository and launch: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8395688303691
      ],
      "excerpt": "in C++: https://github.com/AlexeyAB/Yolo_mark \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nghgphi/fire_detection",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-31T05:58:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-31T07:19:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9007716629187501
      ],
      "excerpt": "More details: medium link \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "YOLOv4 model zoo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.865783793959396
      ],
      "excerpt": "Improvements in this repository \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.866841361811332
      ],
      "excerpt": "How to compile on Linux \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8556303529516746
      ],
      "excerpt": "How to mark bounded boxes of objects and create annotation files \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8189301825894285
      ],
      "excerpt": "Get any .avi/.mp4 video file (preferably not more than 1920x1080 to avoid bottlenecks in CPU performance) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9737825248860232
      ],
      "excerpt": "    For YOLOv3 - convert yolov3.weights/cfg files to yolov3.ckpt/pb/meta: by using mystic123 project, and TensorFlow-lite \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9356759702677893
      ],
      "excerpt": "TVM - compilation of deep learning models (Keras, MXNet, PyTorch, Tensorflow, CoreML, DarkNet) into minimum deployable modules on diverse hardware backends (CPUs, GPUs, FPGA, and specialized accelerators): https://tvm.ai/about \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8326555599599745
      ],
      "excerpt": "added State-of-Art models: CSP, PRN, EfficientNet \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9616349963003782
      ],
      "excerpt": "added the ability for training with GPU-processing using CPU-RAM to increase the mini_batch_size and increase accuracy (instead of batch-norm sync) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826405011675686,
        0.8993789282661905
      ],
      "excerpt": "improved performance 3.5 X times of data augmentation for training (using OpenCV SSE/AVX functions instead of hand-written functions) - removes bottleneck for training on multi-GPU or GPU Volta \nimproved performance of detection and training on Intel CPU with AVX (Yolo v3 ~85%) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.803202738940659
      ],
      "excerpt": "optimized GPU initialization for detection - we use batch=1 initially instead of re-init with batch=1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9032343133017645,
        0.9330239988071134,
        0.8914828933850917
      ],
      "excerpt": "added drawing of chart of average-Loss and accuracy-mAP (-map flag) during training \nrun ./darknet detector demo ... -json_port 8070 -mjpeg_port 8090 as JSON and MJPEG server to get results online over the network by using your soft or Web-browser \nadded calculation of anchors for training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9252512406965311
      ],
      "excerpt": "many other fixes of code... \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8198950586035019
      ],
      "excerpt": "Also, you might be interested in using a simplified repository where is implemented INT8-quantization (+30% speedup and -1% mAP reduced): https://github.com/AlexeyAB/yolo2_light \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8299166424698432
      ],
      "excerpt": "Replace the address below, on shown in the phone application (Smart WebCam) and launch: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8105296482968791
      ],
      "excerpt": "OPENMP=1 to build with OpenMP support to accelerate Yolo by using multi-core CPU \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8170976728030219
      ],
      "excerpt": "If you get a Nan, then for some datasets better to decrease learning rate, for 4 GPUs set learning_rate = 0,00065 (i.e. learning_rate = 0.00261 / GPUs). In this case also increase 4x times burn_in = in your cfg-file. I.e. use burn_in = 4000 instead of 1000. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9045176944725404
      ],
      "excerpt": "change [filters=255] to filters=(classes + 5)x3 in the 3 [convolutional] before each [yolo] layer, keep in mind that it only has to be the last [convolutional] before each of the [yolo] layers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9148490921544745
      ],
      "excerpt": "(Generally filters depends on the classes, coords and number of masks, i.e. filters=(classes + coords + 1)*&lt;number of mask&gt;, where mask is indices of anchors. If mask is absence, then filters=(classes + coords + 1)*num) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.8979411005071259
      ],
      "excerpt": "  data/obj/img2.jpg \n  data/obj/img3.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.988266287233394
      ],
      "excerpt": "Do all the same steps as for the full yolo model as described above. With the exception of: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8873172444668124
      ],
      "excerpt": "my Loss is very high and mAP is very low, is training wrong? Run training with -show_imgs flag at the end of training command, do you see correct bounded boxes of objects (in windows or in files aug_...jpg)? If no - your training dataset is wrong. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.906143588917424,
        0.9068681565146853,
        0.834064540428266
      ],
      "excerpt": "What is the best way to mark objects: label only the visible part of the object, or label the visible and overlapped part of the object, or label a little more than the entire object (with a little gap)? Mark as you like - how would you like it to be detected. \nfor training with a large number of objects in each image, add the parameter max=200 or higher value in the last [yolo]-layer or [region]-layer in your cfg-file (the global maximum number of objects that can be detected by YoloV3 is 0,0615234375*(width*height) where are width and height are parameters from [net] section in cfg-file)  \nfor training for small objects (smaller than 16x16 after the image is resized to 416x416) - set layers = 23 instead of https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L895 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9166013111118131
      ],
      "excerpt": "That is, if only objects that occupied 80-90% of the image were present in the training set, then the trained network will not be able to detect objects that occupy 1-10% of the image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9741927300415745
      ],
      "excerpt": "each: model of object, side, illimination, scale, each 30 grad of the turn and inclination angles - these are different objects from an internal perspective of the neural network. So the more different objects you want to detect, the more complex network model should be used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9053512929171167
      ],
      "excerpt": "Increase network-resolution by set in your .cfg-file (height=608 and width=608) or (height=832 and width=832) or (any value multiple of 32) - this increases the precision and makes it possible to detect small objects: link \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8215478943909726
      ],
      "excerpt": "Here you can find repository with GUI-software for marking bounded boxes of objects and generating annotation files for Yolo v2 - v4: https://github.com/AlexeyAB/Yolo_mark \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8796080762191216
      ],
      "excerpt": "Different tools for marking objects in images: \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nghgphi/fire_detection/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 03:46:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nghgphi/fire_detection/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "nghgphi/fire_detection",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/json_mjpeg_streams.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/image_yolov3.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/image_yolov4.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/net_cam_v4.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/build.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/net_cam_v3.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/video_yolov4.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/video_yolov3.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/scripts/dice_label.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/scripts/setup.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/scripts/get_imagenet_train.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/scripts/gen_tactic.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/scripts/get_coco_dataset.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/scripts/imagenet_label.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/scripts/install_OpenCV4.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/scripts/windows/windows_imagenet_train.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/scripts/windows/windows_imagenet_label.sh",
      "https://raw.githubusercontent.com/nghgphi/fire_detection/master/scripts/windows/otb_get_labels.sh"
    ],
    "technique": "File Exploration"
  },
  "identifier": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://zenodo.org/badge/latestdoi/75388965",
      "technique": "Regular expression"
    }
  ],
  "installation": [
    {
      "confidence": [
        0.9140753650895566
      ],
      "excerpt": "Manual: https://github.com/AlexeyAB/darknet/wiki \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9445366597506639
      ],
      "excerpt": "Requirements (and how to install dependecies) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8861518504264562
      ],
      "excerpt": "How to compile on Linux \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8941170833864304,
        0.9230526238937559
      ],
      "excerpt": "Using make \nHow to compile on Windows \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8394967358553171
      ],
      "excerpt": "How to train with multi-GPU: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918974083095406,
        0.9218762397266433
      ],
      "excerpt": "* tkDNN: https://github.com/ceccocats/tkDNN \n* OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8120216618443424
      ],
      "excerpt": "train  = &lt;replace with your path&gt;/trainvalno5k.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9738589915327188
      ],
      "excerpt": "Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9707277520183856
      ],
      "excerpt": "* `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9201321764091015
      ],
      "excerpt": "* `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9578757234406691
      ],
      "excerpt": "You can get cfg-files by path: darknet/cfg/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8028059948572718
      ],
      "excerpt": "OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android), OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA), use yolov4.weights/cfg with: C++ example or Python example \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9322609392449874
      ],
      "excerpt": "PyTorch > ONNX:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9137395687730232
      ],
      "excerpt": "TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8732466181420652
      ],
      "excerpt": "MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8681103059674972
      ],
      "excerpt": "ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811319624300472
      ],
      "excerpt": "Start Smart WebCam on your phone \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828312948340804
      ],
      "excerpt": "The CMakeLists.txt will attempt to find installed optional dependencies like CUDA, cudnn, ZED and build against those. It will also create a shared object library file to use darknet for code development. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.976885133954891,
        0.8752615859580927
      ],
      "excerpt": "GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda) \nCUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131533231117744
      ],
      "excerpt": "ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed), then run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8390403177736346,
        0.8307586973013438,
        0.9907250308266387
      ],
      "excerpt": "This is the recommended approach to build Darknet on Windows. \nInstall Visual Studio 2017 or 2019. In case you need to download it, please go here: Visual Studio Community \nInstall CUDA (at least v10.0) enabling VS Integration during installation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9620389151658965,
        0.8717750957837743
      ],
      "excerpt": "PS Code\\&gt;              git clone https://github.com/microsoft/vcpkg \nPS Code\\&gt;              cd vcpkg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9870879348898421,
        0.8717750957837743,
        0.96026702491667,
        0.8717750957837743
      ],
      "excerpt": "PS Code\\vcpkg&gt;         .\\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base,cuda,cudnn]:x64-windows for a quicker install of dependencies \nPS Code\\vcpkg&gt;         cd .. \nPS Code\\&gt;              git clone https://github.com/AlexeyAB/darknet \nPS Code\\&gt;              cd darknet \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9217241612371774
      ],
      "excerpt": "Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511039862265771
      ],
      "excerpt": "for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.829704442551866
      ],
      "excerpt": "Also you can get result earlier than all 45000 iterations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8806801276466416,
        0.8806801276466416
      ],
      "excerpt": "in Python: https://github.com/tzutalin/labelImg \nin Python: https://github.com/Cartucho/OpenLabeling \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8194862426406093
      ],
      "excerpt": "in JavaScript: https://github.com/opencv/cvat \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8194439134960743
      ],
      "excerpt": "Download and unzip test-dev2017 dataset from MS COCO server: http://images.cocodataset.org/zips/test2017.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8070516351729637
      ],
      "excerpt": "Create /results/ folder near with ./darknet executable file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589570457713529,
        0.8203508525562374
      ],
      "excerpt": "Rename the file  /results/coco_results.json to detections_test-dev2017_yolov4_results.json and compress it to detections_test-dev2017_yolov4_results.zip \nSubmit file detections_test-dev2017_yolov4_results.zip to the MS COCO evaluation server for the test-dev2019 (bbox) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8235207535955358
      ],
      "excerpt": "    ./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -dont_show -ext_output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8240681772393719
      ],
      "excerpt": "    ./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -benchmark \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8634260778242326,
        0.8680634776119189
      ],
      "excerpt": "width=608 height=608 in cfg: 65.7% mAP@0.5 (43.5% AP@0.5:0.95) - 34(R) FPS / 62(V) FPS - 128.5 BFlops \nwidth=512 height=512 in cfg: 64.9% mAP@0.5 (43.0% AP@0.5:0.95) - 45(R) FPS / 83(V) FPS - 91.1 BFlops \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8529208122344125
      ],
      "excerpt": "width=320 height=320 in cfg:   60% mAP@0.5 (  38% AP@0.5:0.95) - 63(R) FPS / 123(V) FPS - 35.5 BFlops \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8534258244584425
      ],
      "excerpt": "Pascal VOC: use python ./scripts/voc_label.py for labeling Train/Test/Val detection datasets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8470449392762698,
        0.8503144055311906
      ],
      "excerpt": "Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137 \nThen stop and by using partially-trained model /backup/yolov4_1000.weights run training with multigpu (up to 4 GPUs): darknet.exe detector train cfg/coco.data cfg/yolov4.cfg /backup/yolov4_1000.weights -gpus 0,1,2,3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8674213441531435
      ],
      "excerpt": "For training cfg/yolov4-custom.cfg download the pre-trained weights-file (162 MB): yolov4.conv.137 (Google drive mirror yolov4.conv.137 ) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877731550916042,
        0.8202257195562564
      ],
      "excerpt": "Create file obj.names in the directory build\\darknet\\x64\\data\\, with objects names - each in new line \nCreate file obj.data in the directory build\\darknet\\x64\\data\\, containing (where classes = number of objects): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8933741375046061,
        0.8428497312187538
      ],
      "excerpt": "  train  = data/train.txt \n  valid  = data/test.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8237249625365071
      ],
      "excerpt": "Put image-files (.jpg) of your objects in the directory build\\darknet\\x64\\data\\obj\\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8584890268876774
      ],
      "excerpt": "It will create .txt-file for each .jpg-image-file - in the same directory and with the same name, but with .txt-extension, and put to file: object number and object coordinates on this image, for each object in new line:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8718673695351183
      ],
      "excerpt": "For example for img1.jpg you will be created img1.txt containing: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9410048613256292
      ],
      "excerpt": "Create file train.txt in directory build\\darknet\\x64\\data\\, with filenames of your images, each filename in new line, with path relative to darknet.exe, for example containing: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8650599171564616
      ],
      "excerpt": "Start training by using the command line: darknet.exe detector train data/obj.data yolo-obj.cfg yolov4.conv.137 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8157758741178391
      ],
      "excerpt": "8.1. For training with mAP (mean average precisions) calculation for each 4 Epochs (set valid=valid.txt or train.txt in obj.data file) and run: darknet.exe detector train data/obj.data yolo-obj.cfg yolov4.conv.137 -map \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8475017933430123
      ],
      "excerpt": "After each 100 iterations you can stop and later start training from this point. For example, after 2000 iterations you can stop training, and later just start training using: darknet.exe detector train data/obj.data yolo-obj.cfg backup\\yolo-obj_2000.weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066444378209341
      ],
      "excerpt": "* Start training: darknet.exe detector train data/obj.data yolov4-tiny-obj.cfg yolov4-tiny.conv.29 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8101921643353219
      ],
      "excerpt": "for training with a large number of objects in each image, add the parameter max=200 or higher value in the last [yolo]-layer or [region]-layer in your cfg-file (the global maximum number of objects that can be detected by YoloV3 is 0,0615234375*(width*height) where are width and height are parameters from [net] section in cfg-file)  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8330928405361905
      ],
      "excerpt": "object width in percent from Training dataset ~= object width in percent from Test dataset  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8074273915509396
      ],
      "excerpt": "darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8122973964631659
      ],
      "excerpt": "With example of: train.txt, obj.names, obj.data, yolo-obj.cfg, air1-6.txt, bird1-4.txt for 2 classes of objects (air, bird) and train_obj.cmd with example how to train this image-set with Yolo v2 - v4 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nghgphi/fire_detection/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C",
      "Cuda",
      "C++",
      "CMake",
      "Python",
      "Shell",
      "PowerShell",
      "Makefile",
      "Batchfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/nghgphi/fire_detection/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                  YOLO LICENSE\\n                             Version 2, July 29 2016\\n\\nTHIS SOFTWARE LICENSE IS PROVIDED \"ALL CAPS\" SO THAT YOU KNOW IT IS SUPER\\nSERIOUS AND YOU DON\\'T MESS AROUND WITH COPYRIGHT LAW BECAUSE YOU WILL GET IN\\nTROUBLE HERE ARE SOME OTHER BUZZWORDS COMMONLY IN THESE THINGS WARRANTIES\\nLIABILITY CONTRACT TORT LIABLE CLAIMS RESTRICTION MERCHANTABILITY. NOW HERE\\'S\\nTHE REAL LICENSE:\\n\\n0. Darknet is public domain.\\n1. Do whatever you want with it.\\n2. Stop emailing me about it!\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Yolo v4, v3 and v2 for Windows and Linux",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "fire_detection",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "nghgphi",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nghgphi/fire_detection/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Windows or Linux\n* **CMake >= 3.12**: https://cmake.org/download/\n* **CUDA >= 10.0**: https://developer.nvidia.com/cuda-toolkit-archive (on Linux do [Post-installation Actions](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#post-installation-actions))\n* **OpenCV >= 2.4**: use your preferred package manager (brew, apt), build from source using [vcpkg](https://github.com/Microsoft/vcpkg) or download from [OpenCV official site](https://opencv.org/releases.html) (on Windows set system variable `OpenCV_DIR` = `C:\\opencv\\build` - where are the `include` and `x64` folders [image](https://user-images.githubusercontent.com/4096485/53249516-5130f480-36c9-11e9-8238-a6e82e48c6f2.png))\n* **cuDNN >= 7.0** https://developer.nvidia.com/rdp/cudnn-archive (on **Linux** copy `cudnn.h`,`libcudnn.so`... as desribed here https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installlinux-tar , on **Windows** copy `cudnn.h`,`cudnn64_7.dll`, `cudnn64_7.lib` as desribed here https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installwindows )\n* **GPU with CC >= 3.0**: https://en.wikipedia.org/wiki/CUDA#GPUs_supported\n* on Linux **GCC or Clang**, on Windows **MSVC 2017/2019** https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 03:46:01 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "On Linux use `./darknet` instead of `darknet.exe`, like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`\n\nOn Linux find executable file `./darknet` in the root directory, while on Windows find it in the directory `\\build\\darknet\\x64` \n\n* Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25`\n* **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg`\n* Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4`\n* Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0`\n* Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg`\n* Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi`\n* Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4`\n* **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output`\n* Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4`\n* Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25`\n* Train on **Amazon EC2**, to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**): \n    `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map`\n* 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights`\n* Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app\n* To process a list of images `data/train.txt` and save results of detection to `result.json` file use: \n    `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt`\n* To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                             \n    `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt`\n* Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:\n    `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt`\n* To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416`\n* To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\\yolo-obj_7000.weights`\n* To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\\yolo-obj_7000.weights -iou_thresh 0.75`\n\n",
      "technique": "Header extraction"
    }
  ]
}