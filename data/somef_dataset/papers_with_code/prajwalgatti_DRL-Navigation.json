{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.06581",
      "https://arxiv.org/abs/1511.06581)](https://arxiv.org/abs/1511.06581)\n\nThe network's architecture looks like :\n```\nDuelQNet(\n  (input): Linear(in_features=37, out_features=100, bias=True)\n  (dropout): Dropout(p=0.3)\n  (hidden): ModuleList(\n    (0): Linear(in_features=100, out_features=64, bias=True)\n    (1): Linear(in_features=64, out_features=64, bias=True)\n    (2): Linear(in_features=64, out_features=64, bias=True)\n    (3): Linear(in_features=64, out_features=64, bias=True)\n  )\n  (value): ModuleList(\n    (0): Linear(in_features=64, out_features=64, bias=True)\n    (1): Linear(in_features=64, out_features=1, bias=True)\n  )\n  (advantage): ModuleList(\n    (0): Linear(in_features=64, out_features=64, bias=True)\n    (1): Linear(in_features=64, out_features=4, bias=True)\n  )\n)\n```\n\nThe hyperparameters selected for the demonstration are:\n* Learning Rate: 0.0005\n* Batch size : 64\n* Learns after every steps : 4\n* Gamma : 0.99\n* Tau : 0.003\n\nIt took the agent about 471 episodes to be able to perform with not less than score of 13 as an average of 100 episodes. \n\n## Plot of Rewards\n\n![](https://github.com/prajwalgatti/DRL-Navigation/raw/master/plot.png)\n\nThe goal set of the agent is to reach +13 points as average reward of the last 100 episodes.\nThe current solution manages to reach the goal after 400-500 episodes and keep improving over 17 points.\n\nThe saved weights can be found [here.](https://github.com/prajwalgatti/DRL-Navigation/tree/master/saved_weights)\n\nFollow the training notebook [here.](https://github.com/prajwalgatti/DRL-Navigation/blob/master/Navigation.ipynb)\n\n## Some ideas for future work\n\n* Search for better hyperparameters of algorithm as well as neural network\n* Implement prioritized experience replay mechanism"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prajwalgatti/DRL-Navigation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-15T17:43:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-02-03T15:55:48Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[image1]: https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif \"Trained Agent\"\n\n![Trained Agent][image1]\n\nThe RL agent is allowed to traverse across a two dimensional grid with blue and yellow bananas placed across it. The agent is expected to collect the yellow bananas while avoiding the blue ones. The agent receives a positive reward for every yellow banana it collects and a negative reward for every blue banana collected. The size of the state space is 37. The agent is able to move forwards and backwards as well as turn left and right, thus the size of the action space is 4. The minimal expected performance of the agent after training is a score of +13 over 100 consecutive episodes.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9647878918473066
      ],
      "excerpt": "The current solution implements a Dueling DQN algorithm with Prioritized Experience Replay as described in the \"Dueling Network Architectures for Deep Reinforcement Learning\" paper (arXiv:1511.06581) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737395798580185
      ],
      "excerpt": "The hyperparameters selected for the demonstration are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.991246093625049,
        0.9845697696282703,
        0.8910464589509932
      ],
      "excerpt": "It took the agent about 471 episodes to be able to perform with not less than score of 13 as an average of 100 episodes. \nThe goal set of the agent is to reach +13 points as average reward of the last 100 episodes. \nThe current solution manages to reach the goal after 400-500 episodes and keep improving over 17 points. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9475548212161615
      ],
      "excerpt": "Search for better hyperparameters of algorithm as well as neural network \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "An RL agent that implements DDQN algorithm ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prajwalgatti/DRL-Navigation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 21:53:44 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/prajwalgatti/DRL-Navigation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "prajwalgatti/DRL-Navigation",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/prajwalgatti/DRL-Navigation/master/Navigation.ipynb",
      "https://raw.githubusercontent.com/prajwalgatti/DRL-Navigation/master/.ipynb_checkpoints/Navigation-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8731562459058029
      ],
      "excerpt": "  (value): ModuleList( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8238142003613946
      ],
      "excerpt": "Follow the training notebook here. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/prajwalgatti/DRL-Navigation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "ASP.NET",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Description",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DRL-Navigation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "prajwalgatti",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prajwalgatti/DRL-Navigation/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 21:53:44 GMT"
    },
    "technique": "GitHub API"
  }
}