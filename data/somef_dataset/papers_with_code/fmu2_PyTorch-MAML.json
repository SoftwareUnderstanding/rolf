{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1703.03400",
      "https://arxiv.org/abs/1909.09157",
      "https://arxiv.org/abs/2003.03284",
      "https://arxiv.org/abs/2003.03284"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{finn2017model,\n  title={Model-agnostic meta-learning for fast adaptation of deep networks},\n  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},\n  booktitle={International Conference on Machine Learning (ICML)},\n  year={2017}\n}\n\n@inproceedings{raghu2019rapid,\n  title={Rapid learning or feature reuse? towards understanding the effectiveness of maml},\n  author={Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},\n  booktitle={International Conference on Learning Representations (ICLR)},\n  year={2019}\n}\n\n@article{Bronskill2020tasknorm,\n  title={Tasknorm: rethinking batch normalization for meta-learning},\n  author={Bronskill, John and Gordon, Jonathan and Requeima, James and Nowozin, Sebastian and Turner, Richard E.},\n  journal={arXiv preprint arXiv:2003.03284},\n  year={2020}\n}\n```",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@misc{pytorch_maml,\n  title={maml in pytorch - re-implementation and beyond},\n  author={Mu, Fangzhou},\n  howpublished={\\url{https://github.com/fmu2/PyTorch-MAML}},\n  year={2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Bronskill2020tasknorm,\n  title={Tasknorm: rethinking batch normalization for meta-learning},\n  author={Bronskill, John and Gordon, Jonathan and Requeima, James and Nowozin, Sebastian and Turner, Richard E.},\n  journal={arXiv preprint arXiv:2003.03284},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{raghu2019rapid,\n  title={Rapid learning or feature reuse? towards understanding the effectiveness of maml},\n  author={Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},\n  booktitle={International Conference on Learning Representations (ICLR)},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{finn2017model,\n  title={Model-agnostic meta-learning for fast adaptation of deep networks},\n  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},\n  booktitle={International Conference on Machine Learning (ICML)},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{pytorch_maml,\n  title={maml in pytorch - re-implementation and beyond},\n  author={Mu, Fangzhou},\n  howpublished={\\url{https://github.com/fmu2/PyTorch-MAML}},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8101227425953824
      ],
      "excerpt": "* maml (the official implementation) https://github.com/cbfinn/maml \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fmu2/PyTorch-MAML",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[Fangzhou Mu](http://pages.cs.wisc.edu/~fmu/) (fmu2@wisc.edu)\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-01T16:16:51Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T13:17:39Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9894083295849746,
        0.9983675625156666,
        0.9931404700314959,
        0.918656381229455,
        0.9876226162317495,
        0.8871522101338288,
        0.9554079721496035,
        0.9768830763565661,
        0.8252085594880761
      ],
      "excerpt": "A PyTorch implementation of Model Agnostic Meta-Learning (MAML). We faithfully reproduce the official Tensorflow implementation while incorporating a number of additional features that may ease further study of this very high-profile meta-learning framework. \nThis repository contains code for training and evaluating MAML on the mini-ImageNet and tiered-ImageNet datasets most commonly used for few-shot image classification. To the best of our knowledge, this is the only PyTorch implementation of MAML to date that fully reproduces the results in the original paper without applying tricks such as data augmentation, evaluation on multiple crops, and ensemble of multiple models. Other existing PyTorch implementations typically see a ~3% gap in accuracy for the 5-way-1-shot and 5-way-5-shot classification tasks on mini-ImageNet. \nBeyond reproducing the results, our implementation comes with a few extra bits that we believe can be helpful for further development of the framework. We highlight the improvements we have built into our code, and discuss our observations that warrent some attention. \nBatch normalization with per-episode running statistics. Our implementation provides flexibility of tracking global and/or per-episode running statistics, hence supporting both transductive and inductive inference. \nBetter data pre-processing. The official implementation does not normalize and augment data. We support data normalization and a variety of data augmentation techniques. We also implement data batching and support/query-set splitting more efficiently. \nMore datasets. We support mini-ImageNet, tiered-ImageNet and more. \nMore options for outer-loop optimization. We support mutiple optimizers and learning-rate schedulers for the outer-loop optimization. \nMore powerful inner-loop optimization. The official implementation uses vanilla gradient descent in the inner loop. We support momentum and weight decay. \nMore options for encoder architecture. We support the standard four-layer ConvNet as well as ResNet-12 and ResNet-18 as the encoder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9481414696751961,
        0.9764318135900244,
        0.9906828174634624,
        0.9222218639559038,
        0.994324064634775,
        0.9565008970138856,
        0.830170089220548,
        0.9068765710445814,
        0.9022074186289749,
        0.9107705072400815,
        0.9846593978896014
      ],
      "excerpt": "Meta-learning with zero-initialized classifier head. The official implementation learns a meta-initialization for both the encoder and the classifier head. This prevents one from varying the number of categories at training or test time. With our implementation, one may opt to learn a meta-initialization for the encoder while initializing the classifier head at zero. \nDistributed training and gradient checkpointing. MAML is very memory-intensive because it buffers all tensors generated throughout the inner-loop adaptation steps. Gradient checkpointing trades compute for memory, effectively bringing the memory cost from O(N) down to O(1), where N is the number of inner-loop steps. In our experiments, gradient checkpointing saved up to 80% of GPU memory at the cost of running the forward pass more than once (a moderate 20% increase in running time). \nThe official implementation assumes transductive learning. The batch normalization layers do not track running statistics at training time, and they use mini-batch statistics at test time. The implicit assumption here is that test data come in mini-batches and are perhaps balanced across categories. This is a very restrictive assumption and does not land MAML directly comparable with the vast majority of meta-learning and few-shot learning methods. Unfortunately, this is not immediately obvious from the paper, and our findings suggest that the performance of MAML is hugely overestimated. \nAccuracy is very sensitive to the size of query set in the transductive setting. For example, the result for 5-way-1-shot classification on miniImageNet from the paper (48.70%) was obtained on five queries, one per category. We found that the accuracy dropped by ~1.5% given five queries per category, and by ~2.5% given 15 queries per category. \nThe paper reports mean accuracy over 600 independently sampled tasks, or trials. We found that 600 trials, again in the transductive setting, are insufficient for an unbiased estimate of model performance. The mean accuracy from 6,000 trials is more stable, and is always ~2% lower than that from the first 600 trials. We conjecture that the distribution of per-trial accuracy is highly skewed towards the high end. \nWe found that MAML performs a lot worse in the inductive setting. Given the same model configuration, inductive accuracy is always much lower (~4%) than the corrected transductive accuracy, which is already a few percentage points behind the reported number. \nHence, one should be extremely cautious when comparing MAML with its competitors as is evident from the discussion above. \nUnfortunately, some insights discussed in the original paper and its follow-up works do not appear to hold in the inductive setting.  \nFOMAML (i.e. the first-order approximation of MAML) performs as well as MAML in transductive learning, but fails completely in the inductive setting.  \nCompletely freezing the encoder during inner-loop adaption as was done in this work results in dramatic decrease in accuracy. \nA recent work proposes TaskNorm, a test-time enhancement of batch normalization, noting that the small batch sizes during training may leave batch normalization less effective. We did not have much success with this method. We observed marginal improvement most of the time, and found that it hurts performance occationally. That said, we do believe that batch normalization is hard to deal with in MAML. TaskNorm attempts to attack the problem of small batch sizes, which we conjecture is just one among the three main causes (i.e., extremely scarse training data, extremely small batch sizes, and extremely small number of inner-loop updates) of the ineffectiveness of batch normalization in MAML. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.90937583566575,
        0.967695097207194
      ],
      "excerpt": "Template configuration files as well as those for reproducing the results in the original paper can be found in configs/. The hyperparameters are self-explanatory. \nHere is the command for single-GPU training of MAML with ConvNet4 backbone for 5-way-1-shot classification on mini-ImageNet to reproduce the result in the original paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9363662687754477
      ],
      "excerpt": "Our implementation is inspired by the following repositories. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A PyTorch implementation of Model Agnostic Meta-Learning (MAML) that faithfully reproduces the results from the original paper.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fmu2/PyTorch-MAML/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Wed, 29 Dec 2021 02:34:39 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/fmu2/PyTorch-MAML/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "fmu2/PyTorch-MAML",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8301362378811463
      ],
      "excerpt": "Hence, one should be extremely cautious when comparing MAML with its competitors as is evident from the discussion above. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9388304363791192,
        0.9549319778953697
      ],
      "excerpt": "Python 3.6.8 (or any Python 3 distribution) \nPyTorch 1.3.1 (or any PyTorch > 1.0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8338256005539654
      ],
      "excerpt": "python train.py --config=configs/convnet4/mini-imagenet/train_reproduce.yaml --gpu=0,1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9725002728977397,
        0.8918974083095406
      ],
      "excerpt": "* MAML-Pytorch https://github.com/dragen1860/MAML-Pytorch \n* HowToTrainYourMAMLPytorch https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8856287707023407
      ],
      "excerpt": "python train.py --config=configs/convnet4/mini-imagenet/train_reproduce.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.899389389748537
      ],
      "excerpt": "python train.py --config=configs/convnet4/mini-imagenet/train_reproduce.yaml --gpu=0,1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8541543541248668
      ],
      "excerpt": "python train.py --config=configs/convnet4/mini-imagenet/train_reproduce.yaml --efficient \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8863424073673143
      ],
      "excerpt": "python test.py --config=configs/convnet4/mini-imagenet/test_reproduce.yaml \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/fmu2/PyTorch-MAML/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "MAML in PyTorch - Re-implementation and Beyond",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PyTorch-MAML",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "fmu2",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fmu2/PyTorch-MAML/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 65,
      "date": "Wed, 29 Dec 2021 02:34:39 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "meta-learning",
      "few-shot-learning",
      "deep-learning",
      "maml"
    ],
    "technique": "GitHub API"
  }
}