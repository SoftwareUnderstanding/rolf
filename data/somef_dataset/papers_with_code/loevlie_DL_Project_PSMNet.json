{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<a id=\"1\">[1]</a> \nJia.-Ren Chang and Yong.-Sheng Chen (2018). \nPyramid Stereo Matching Network\nCoRR, abs/1803.08669, http://arxiv.org/abs/1803.08669\n\n<a id=\"2\">[2]</a> \nChristian Szegedy and\n               Vincent Vanhoucke and\n               Sergey Ioffe and\n               Jonathon Shlens and\n               Zbigniew Wojna (2015). \nRethinking the Inception Architecture for Computer Vision \nCoRR, abs/1512.00567, http://arxiv.org/abs/1512.00567\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9001257786168221
      ],
      "excerpt": "2D and 3D asymmetric convolutions \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/loevlie/DL_Project_PSMNet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-11T01:35:35Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-24T17:39:13Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9093559471117095
      ],
      "excerpt": "Other Modified Model Architectures \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9389297003358453
      ],
      "excerpt": "A PSMNet model was developed based on the literature [1].  This was used to generate disparity maps and they were tested based on the training L1 loss and validation 3-pixel accuracy.  The PSMNet architecture from [1] is shown in Figure 1.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8322798282788103
      ],
      "excerpt": "We use the 3 pixel disparity error to evaluate our models and compare them against the original PSMNet [1]performance.  A comparison of each model\u2019s total number ofparameters used, error on the RGB dataset, and error on the IR dataset can be seen in Table \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9763223159032857
      ],
      "excerpt": "| Our Model         | 3.1 mil  | 6.9 %    | 31.2 %   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8549837316462923
      ],
      "excerpt": "Three main modification to the architecture of the model were also tested.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9393613754316538
      ],
      "excerpt": "2D and 3D asymmetric convolutions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9882561026382415
      ],
      "excerpt": "These modifications to the literature PSMNet model all reached a close final loss/accuracy with the Final model being the one that achieved a higher accuracy then the PSMNet architecture and leading to our decision of proposing that model for the use on IR datasets.  Figures for the changes in loss and accuracy for RGB are shown below in Figure 4 and Figure 5.  Figures for IR are shown in Figure 6 and 7. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9432302094701996,
        0.8469940910826049
      ],
      "excerpt": "The asymmetric convolutions idea was based on the paper \"Rethinking the Inception Architecture for Computer Vision\" [2].  The inception paper has shown that for example using a 3x1 convolution followed by a 1x3 convolution is equivalent to sliding a two layer network with the same receptive field as in a 3x3 convolution.  This is shown in Figure 8.  [2] has stated that the asymmetric convolutions are equivilant to sliding a two layer network with the same receptive field as in a 3x3 convolution.  This is illustrated in Figure 8.  The change to the basic block in the PSMNet architecture is shown in figure 9.  3D convolutions can be approximated by asymmetric convolutions in a similar manor as shown in figure 10.   \nAsymmetric Convolutions                                                                                                     |  Change in Basic Block Model Architectures  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8684025418565658,
        0.9706299244879044,
        0.9856602072802273
      ],
      "excerpt": "Figure 10: Approximation of 3D convolution with 3 asymmetric convolutions \nUsing the insight gained from the aforementioned IR experiments, we redesigned the SPP module of PSMNet using residual blocks as shown in Figure 11 such that performance could be improved on IR images. The modifications described in this section, while tested primarily on IR images, may be applicable to RGB images as well. However, for the sake of this work we consider the architecture\u2019s performance on the more challenging problem of IR disparity estimation. \nSimilar to PSMNet, we first perform spatial pooling at scales4\u00d74,8\u00d78,16\u00d716, and32\u00d732. Theoutputs of each spatial pooling operation are sent to a convolutional block (CB) whose architecture isprovided in Figure 12a. Specifically CB1 accepts 3 feature maps from the provided image and outputs 32 feature maps. The outputs from CB1 are passed to a series of 4 identity blocks. The design of each identity block (IB) is shown in Figure 12b. Note that the number of feature maps is unchanged by the identity block. The outputs of the identity block are passed through another set of convolutional (CB2) and identity (IB2) blocks. In the figure, CB2 accepts 32 feature maps and outputs 64 maps.  The outputs from each spatial pooling branch are upsampled to a common size, concatenated, and passed through a final set of convolutional and identity modules.  In Figure 10, CB3 takes in 512 feature maps and outputs 128 maps, while CB4 contains 64 filters. The final Conv layer contains 32 filters and performs a convolution with kernel size and stride both set to 1\u00d71. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PSMNet Replication and Modification",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/loevlie/DL_Project_PSMNet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 28 Dec 2021 11:30:25 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/loevlie/DL_Project_PSMNet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "loevlie/DL_Project_PSMNet",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/loevlie/DL_Project_PSMNet/main/Models/Baseline/11785_ProjMidterm_Baseline.ipynb",
      "https://raw.githubusercontent.com/loevlie/DL_Project_PSMNet/main/Models/Modified/11785_ProjMidterm_IncreasedParams.ipynb",
      "https://raw.githubusercontent.com/loevlie/DL_Project_PSMNet/main/Models/Modified/11785_ProjMidterm_Modified_param_reduction.ipynb",
      "https://raw.githubusercontent.com/loevlie/DL_Project_PSMNet/main/Models/Modified/11785_ProjMidterm_Parameter_Reduction.ipynb",
      "https://raw.githubusercontent.com/loevlie/DL_Project_PSMNet/main/Models/Modified/11785_ProjMidterm_ReducedParams.ipynb",
      "https://raw.githubusercontent.com/loevlie/DL_Project_PSMNet/main/Models/Modified/11785_ProjMidterm_Conv3D_Modified.ipynb",
      "https://raw.githubusercontent.com/loevlie/DL_Project_PSMNet/main/Models/Final/Final_Model_IR.ipynb",
      "https://raw.githubusercontent.com/loevlie/DL_Project_PSMNet/main/Models/Final/Final_Model_RGB.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8259329713403077
      ],
      "excerpt": "| Name              |  Params. |RGB Error | IR Error | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8200308744647624
      ],
      "excerpt": "| Final model       | 1.77 mil | 8.4 %    | 23.7 %   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9227741113294464,
        0.9227741113294464
      ],
      "excerpt": "    <td><img src=\"./Images/Ref_err.png\" width=600 height=400></td> \n    <td><img src=\"./Images/Model_err.png\" width=600 height=400></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9227741113294464,
        0.9227741113294464
      ],
      "excerpt": "    <td><img src=\"./Images/lerr.png\" width=300 height=400></td> \n    <td><img src=\"./Images/hierr.png\" width=300 height=400></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8213457800676237
      ],
      "excerpt": "             Training                                              |                                        Validation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8213457800676237
      ],
      "excerpt": "|            Training                                              |                                        Validation| \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/loevlie/DL_Project_PSMNet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Introduction to Deep Learning Project Repo",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DL_Project_PSMNet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "loevlie",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/loevlie/DL_Project_PSMNet/blob/main/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Running the code is simplified by use of a python notebook. All that is required is to run each cell in the [Final Model IR](Models/Final/Final_Model_IR.ipynb) for IR data and [Final Model RGB](Models/Final/Final_Model_RGB.ipynb) for RGB data. The training should take about 4 hours for 100 epochs for the final model and 11 and a half hours for the baseline model. The accuracies and model will be saved automatically every 10 epochs.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 28 Dec 2021 11:30:25 GMT"
    },
    "technique": "GitHub API"
  }
}