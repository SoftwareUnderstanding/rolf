{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1801.06146",
      "https://arxiv.org/abs/1801.06146 (2018).](https://arxiv.org/abs/1801.06146)\n\n\n\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[[1] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems. 2017.](https://arxiv.org/abs/1706.03762)\n\n[[2] Howard, Jeremy, and Sebastian Ruder. \"Universal language model fine-tuning for text classification.\" arXiv preprint arXiv:1801.06146 (2018).](https://arxiv.org/abs/1801.06146)\n\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "| 10 | 0.882431 | 0.765422 | 0.901345 | 4:21:53 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PrideLee/sentiment-analysis",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-17T10:40:09Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-08T02:08:52Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "&emsp;&emsp;This project is about movive reviews sentiment analysis based on Transformer and ULMFiT model. \n\n**You can browse the full report from [here](https://github.com/PrideLee/sentiment-analysis/blob/master/Different%20Deep%20Learning%20Models%20Applied%20to%20Sentiment%20Analysis.pdf).**\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9850948773494088
      ],
      "excerpt": "&emsp;&emsp;To solve the long-term independence and reduce the computation, Google designs a new model in ML tasks, named Transformer. About this model's detalied introduction please refer my reporters. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8522396939506078
      ],
      "excerpt": "&emsp;&emsp;The loss mean at each epoch (50 epoches total) in training data and the accuracy of verification data (every 5 epoch) will be saved in \"root/results\" path. The best training model will also be saved. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9361028235126705
      ],
      "excerpt": "&emsp;&emsp;ULMFiT model is introduced the pre-training and fine-tuning strategy to text classification tasks, we pre-train a general model in wiki-103 dataset and fine-tuning it on IMDB dataset, then training a classfication about sentiment analysis. There are some pre-training tricks presented in this paper. More detail about ULMFiT please refer my notebook. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "IMDB sentiment analysis, Transformer, ULMFiT,  attention, TextCNN.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PrideLee/sentiment-analysis/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Thu, 23 Dec 2021 20:47:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PrideLee/sentiment-analysis/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "PrideLee/sentiment-analysis",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8411257538933686
      ],
      "excerpt": "&emsp;&emsp;Python3.6.0 + Pytorch 1.0.1. (Some other python and pytorch version should also be used, but some fuctions and libraries may have a little bit difference, we coding in python 3.6 and pytorch 1.0.1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9708119013170028
      ],
      "excerpt": "&emsp;&emsp;Python 3.6.0 + Pytorch 1.0.1 + Fastai 1.0.51. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8091008019322253
      ],
      "excerpt": "&emsp;&emsp;The loss mean at each epoch (50 epoches total) in training data and the accuracy of verification data (every 5 epoch) will be saved in \"root/results\" path. The best training model will also be saved. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PrideLee/sentiment-analysis/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Sentiment analysis",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sentiment-analysis",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "PrideLee",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PrideLee/sentiment-analysis/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 13,
      "date": "Thu, 23 Dec 2021 20:47:58 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Please run train.py.\n  \n  You can also change the parameters, e.g. batch_size=64, learning_rate=0.001, epoches=50, etc. The resultes, training model and processing data will be saved in the folder, you can assign the path by changing the saye_path parameter.\n\n- IMDB dataset and GloVe wording vectory will be download in the \"root\\\" path.\n- dataload.py will processing (wmbedding, tokenize, etc.) raw data.\n- model.py define and design the transformer netwoek.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Run ULMFiT_slim.py, you can assign the path to save the trained model and IMDB dataset. The processing data.csv (train and test set will be created randomly) will also be saved. You also can assign the batch_size, learning_rate, dropout etc.\n- There is an other version to reliaze this model we can refere [here](https://github.com/fastai/fastai/tree/master/courses/dl2/imdb_scripts).\n\n",
      "technique": "Header extraction"
    }
  ]
}