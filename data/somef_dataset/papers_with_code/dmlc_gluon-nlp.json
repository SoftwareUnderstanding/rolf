{
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dmlc/gluon-nlp",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-04-04T20:57:13Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T16:53:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9136938752592462,
        0.860059181823877
      ],
      "excerpt": "Easy-to-use Text Processing Tools and Modular APIs \nPretrained Model Zoo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9203397236726403,
        0.9795576947980673
      ],
      "excerpt": "downloading and processing the NLP datasets. For more details, you may refer to \n GluonNLP Datasets and GluonNLP Data Processing Tools. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9081988138028817
      ],
      "excerpt": ": CLI for accessing some common data processing scripts \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "NLP made easy",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dmlc/gluon-nlp/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 522,
      "date": "Wed, 29 Dec 2021 09:17:56 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dmlc/gluon-nlp/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dmlc/gluon-nlp",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/dmlc/gluon-nlp/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/machine_translation/evaluate_epochs_wmt2014_ende.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/machine_translation/wmt2014_back_translation.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/datasets/machine_translation/wmt2017_zhen.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/datasets/machine_translation/wmt2014_ende.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/conversion_toolkits/convert_mt5.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/conversion_toolkits/convert_gpt2.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/conversion_toolkits/convert_xlmr.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/conversion_toolkits/convert_mobilebert.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/conversion_toolkits/convert_t5.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/conversion_toolkits/convert_bert.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/conversion_toolkits/convert_roberta.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/conversion_toolkits/convert_albert.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/conversion_toolkits/convert_electra.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/conversion_toolkits/convert_bert_torch.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/conversion_toolkits/convert_bart.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/question_answering/commands/run_squad2_albert_base.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/question_answering/commands/run_squad2_albert_large.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/question_answering/commands/run_squad2_electra_large.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/question_answering/commands/run_squad2_electra_small.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/question_answering/commands/run_squad2_roberta_large.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/question_answering/commands/run_squad2_electra_base.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/question_answering/commands/run_squad2_gluon_en_cased_bert_base_v1.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/question_answering/commands/run_squad2_albert_xlarge.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/question_answering/commands/run_squad2_albert_xxlarge.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/question_answering/commands/run_squad2_uncased_bert_base.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/question_answering/commands/run_squad2_mobilebert.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/question_answering/commands/run_squad2_uncased_bert_wwm_large.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/question_answering/commands/run_squad2_uncased_bert_large.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/benchmarks/run_backbone_benchmark.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/benchmarks/benchmark_gluonnlp_fp16.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/benchmarks/benchmark_gluonnlp.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/scripts/benchmarks/benchmark_gluonnlp_tvm.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/docker/devel_entrypoint.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/docker/start_jupyter.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/docker/gluon_nlp_job.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/docker/install/install_openmpi.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/docker/install/install_tvm_cpu.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/docker/install/install_horovod.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/docker/install/install_python_packages.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/docker/install/install_ubuntu18.04_core.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/docker/install/install_tvm_gpu.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/docker/install/install_llvm.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/docker/install/install_jupyter_lab.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/batch/run_batch_conversion.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/batch/sync_batch_result.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/batch/backbone_benchmark/run_batch_backbone_benchmark.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/batch/question_answering/run_batch_squad.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/batch/batch_states/test.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/batch/batch_states/test_data_pipeline.sh",
      "https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/batch/batch_states/compile_notebooks.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "First of all, install the MXNet 2 release such as MXNet 2 Alpha. You may use the\nfollowing commands:\n\n```bash\n#: Install the version with CUDA 10.2\npython3 -m pip install -U --pre \"mxnet-cu102>=2.0.0a\"\n\n#: Install the version with CUDA 11\npython3 -m pip install -U --pre \"mxnet-cu110>=2.0.0a\"\n\n#: Install the cpu-only version\npython3 -m pip install -U --pre \"mxnet>=2.0.0a\"\n```\n\n\nTo install GluonNLP, use\n\n```bash\npython3 -m pip install -U -e .\n\n#: Also, you may install all the extra requirements via\npython3 -m pip install -U -e .\"[extras]\"\n```\n\nIf you find that you do not have the permission, you can also install to the user folder:\n\n```bash\npython3 -m pip install -U -e . --user\n```\n\nFor Windows users, we recommend to use the [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/about).\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8790170645557265
      ],
      "excerpt": ": Also, you can use python -m to access the toolkits \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dmlc/gluon-nlp/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Cuda",
      "C++",
      "Dockerfile",
      "Makefile",
      "C"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "history\"><img src=\"https://img.shields.io/pypi/v/gluonnlp.svg\"></a>",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "gluon-nlp",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dmlc",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dmlc/gluon-nlp/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "szha",
        "body": "This release includes the following fixes:\r\n- [BUGFIX] remove wd from squad (#1223)\r\n- Fix deprecation warnings due to invalid escape sequences. (#1219)\r\n- Fix layer_norm_eps in BERTEncoder (#1214)\r\n- [BUGFIX] Fix vocab determinism in py35 (#1166) (#1167)\r\n\r\nAs we prepare for the NumPy-based GluonNLP development, we are making the following adjustments to the branch usage:\r\n- master (old) -> v0.x: this branch will be used for maintenance of GluonNLP 0.x versions.\r\n- numpy -> master: the new master branch will be used for GluonNLP 1.0 onward with NumPy-compatible interface, based on the upcoming MXNet 2.0.",
        "dateCreated": "2020-08-13T01:33:22Z",
        "datePublished": "2020-08-13T19:16:27Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.10.0",
        "name": "v0.10.0 Maintenance Release",
        "tag_name": "v0.10.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.10.0",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/29669259",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.10.0"
      },
      {
        "authorType": "User",
        "author_name": "szha",
        "body": "This patch release includes the following bug fix:\r\n- [BUGFIX] remove wd from squad (#1223)\r\n- Fix deprecation warnings due to invalid escape sequences. (#1219)\r\n- Fix layer_norm_eps in BERTEncoder (#1214)",
        "dateCreated": "2020-08-05T02:29:57Z",
        "datePublished": "2020-08-13T17:26:39Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.9.2",
        "name": "v0.9.2: Bug Fix",
        "tag_name": "v0.9.2",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.9.2",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/29664981",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.9.2"
      },
      {
        "authorType": "User",
        "author_name": "szha",
        "body": "This release includes the bug fix for https://github.com/dmlc/gluon-nlp/pull/1158 (#1167). It affects the determinism of the instantiated vocabulary object on the order of special tokens on Python 3.5. Users of Python 3.5 are strongly encouraged to upgrade to this version.",
        "dateCreated": "2020-03-03T04:47:59Z",
        "datePublished": "2020-03-03T04:54:02Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.9.1",
        "name": "v0.9.1: Bug Fix",
        "tag_name": "v0.9.1",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.9.1",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/24155650",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.9.1"
      },
      {
        "authorType": "User",
        "author_name": "leezu",
        "body": "News\r\n====\r\n- GluonNLP was featured in EMNLP 2019 Hong Kong! [Check out the code accompanying the tutorial](https://github.com/leezu/EMNLP19-D2L).\r\n- \"[GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing](http://jmlr.org/papers/volume21/19-429/19-429.pdf)\" has been published in the Journal of Machine Learning Research.\r\n\r\n\r\nModels and Scripts in v0.9\r\n==============================================\r\n\r\n### BERT ###\r\n\r\nINT8 Quantization for BERT Sentence Classification and Question Answering\r\n(#1080)! Also Check out the [blog post](https://medium.com/apache-mxnet/optimization-for-bert-inference-performance-on-cpu-3bb2413d376c).\r\n\r\nEnhancements to the pretraining script (#1121, #1099) and faster tokenizer for\r\nBERT (#921, #1024) as well as multi-GPU support for SQuAD fine-tuning (#1079).\r\n\r\nMake BERT a HybridBlock (#877).\r\n\r\n\r\n### XLNet ###\r\n\r\nThe XLNet model introduced by *Yang, Zhilin, et. al* in\r\n\"[XLNet: Generalized Autoregressive Pretraining for Language Understanding](\r\nhttps://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf)\".\r\nThe model was converted from the [original repository](https://github.com/zihangdai/xlnet/) (#866).\r\n\r\nGluonNLP further provides scripts for finetuning XLNet on the Glue (#995) and\r\nSQuAD datasets (#1130) that reproduce the authors results. [Check out the usage](https://gluon-nlp.mxnet.io/v0.9.x/model_zoo/language_model/index.html#xlnet-generalized-autoregressive-pretraining-for-language-understanding).\r\n\r\n### DistilBERT ###\r\n\r\nThe DistilBERT model introduced by *Sanh, Victor, et. al* in\r\n\"[DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](\r\nhttps://arxiv.org/pdf/1910.01108.pdf)\" (#922).\r\n\r\n\r\n### Transformer ###\r\n\r\nAdd a separate Transformer inference script to make inference easy and make it\r\nconvenient to analysis the performance of transformer inference (#852).\r\n\r\n### Korean BERT ###\r\n\r\n[Pre-trained Korean BERT](https://github.com/SKTBrain/KoBERT) is available as part of GluonNLP (#1057)\r\n\r\n\r\n### RoBERTa ###\r\nGluonNLP now provides scripts for finetuning RoBERTa (#931).\r\n\r\n\r\n### GPT2 ###\r\n\r\nGPT2 is now a HybridBlock the model can be exported for running from other MXNet\r\nlanguage bindings (#1010).\r\n\r\n\r\nNew Features\r\n============\r\n- Add NamedTuple + Dict batchify (#959)\r\n- Add even_size option to split sampler (#1028)\r\n- Add length normalized metrics for machine translation tasks (#1095)\r\n- Add raw attention scores to the AttentionCell #951 (#964)\r\n- Add round_to feature to BERT & XLNet finetuning scripts (#1133)\r\n- Add stratified train_valid_split similar to sklearn.model_selection.train_test_split (#933)\r\n- Add SuperGlue dataset API (#858)\r\n- Add Multi Model Server deployment code example for developers (#1140)\r\n- Allow custom dropout, number of layers/units for BERT (#950)\r\n- Avoid race condition when downloading vocab (#1078)\r\n- Deprecate specifying Vocab padding, bos and eos_token as positional arguments (#945)\r\n- Fast multitensor adam optimizer (#1111)\r\n- Faster grad_global_norm for clipping (#1115)\r\n- Hybridizable AWDRNN/StandardRNN (#911)\r\n- Padding seq length to multiple of 8 in BERT model (#909)\r\n- Scripts for producing the figures that explain the bucketing strategy (#908)\r\n- Split up Seq2SeqDecoder in Seq2SeqDecoder and Seq2SeqOneStepDecoder (#976)\r\n- Switch CI to Python 3.5 and declare Python 3.5 support (#1009)\r\n- Try to use the new None feature in MXNet + Drop support for MXNet 1.5 (#967)\r\n- Use fused gelu operator (#1082)\r\n- Use softmax with length, and interleaved matmul for BERT (#1136) \r\n- Documentation of Model Conversion Scripts at https://gluon-nlp.mxnet.io/v0.9.x/model_zoo/conversion_tools/index.html (#922)\r\n\r\nBug Fixes and code cleanup\r\n==========================\r\n- Add version checker to all scripts  (#930)\r\n- Add version checker to all tutorials (#934)\r\n- Add 'packaging' to requirements (#1143)\r\n- Adjust code owner (#923)\r\n- Avoid using dict for attention cell parameter creation (#1050)\r\n- Bump version in preparation for 0.9 release (#987)\r\n- Change SimVerb3500 URL to aclweb hosted version (#979)\r\n- Correct propagation of error codes in GluonNLP-py3-master-gpu-doc (#971)\r\n- Corrected np.random.randint upper limit in data.stream.py (#935)\r\n- Declare Python version requirement in setup.py (#927)\r\n- Declare more optional dependencies (#958)\r\n- Declare pytest seed marker in pytest.ini (#940)\r\n- Disable HybridBeamSearch (#1021)\r\n- Drop LAMB optimizer from GluonNLP in favor of MXNet version (#1116)\r\n- Drop unused compatibility helpers and fix doc (#928)\r\n- Fix #905 (#906)\r\n- Fix a SQuAD 2.0 evaluation bug (#907)\r\n- Fix argument `analogy-max-vocab-size` (#904)\r\n- Fix broken multi-head attention cell (#878)\r\n- Fix bugs in BERT export script (#944)\r\n- Fix chnsenticorp dataset download link (#873)\r\n- Fix file sampler for BERT (#977)\r\n- Fix index.rst and gpu flag in machine translation (#952)\r\n- Fix log in finetune_squad.py (#1001)\r\n- Fix parameter sharing of WeightDropParameter (#1083)\r\n- Fix scripts/question_answering/data_pipeline.py requiring optional package (#1013)\r\n- Fix the weight tie and weight sharing for AWDRNN (#1087)\r\n- Fix training command in Language Modeling index.rst (#1100)\r\n- Fix version check in train_gnmt.py and train_transformer.py (#1003)\r\n- Fix standard rnn weight sharing error (#1122)\r\n- Glue data preprocessing pipeline and bert & xlnet scripts (#1031)\r\n- Improve Vocab.__repr__ if reserved_tokens or unknown_token is None (#989)\r\n- Improve readability (#975)\r\n- Improve test robustness (#960)\r\n- Improve the readability of the training script. This fix replaces magic numbers with the name (#1006)\r\n- Make EmbeddingCenterContextBatchify returned dtype robust to empty sentences (#954)\r\n- Modify the log average loss (#1103)\r\n- Move ICSL script out of BERT folder (#1131)\r\n- Move NER script out of bert folder  (#1090)\r\n- Move ParallelBigRNN into nlp.model namespace (#1118)\r\n- Move get_rnn_cell out of seq2seq_encoder_decoder (#1073)\r\n- Mxnet version check (#1063)\r\n- Refactor BERT with new data preprocessing (#1124)\r\n- Remove NLTKMosesTokenizer in favor of SacreMosesTokenizer (#942)\r\n- Remove extra dropout in BERT/RoBERTa (#1022)\r\n- Remove outdated comment (#943)\r\n- Remove padding warning (#916)\r\n- Replace unicode comma with ascii comma (#1056)\r\n- Split up inheritance structure of TransformerEncoder and BERTEncoder (#988)\r\n- Support int32 for sampled blocks (#1106)\r\n- Switch batch jobs to use G4dn.2x instance (#1041)\r\n- TransformerXL LayerNorm eps and XLNet pretrained model config (#1005)\r\n- Unify BERT horovod and kvstore pre-training script (#889)\r\n- Update README.rst (#884)\r\n- Update data_api.rst (#893)\r\n- Update embedding script  (#1046)\r\n- Update fp16_utils.py (#1037)\r\n- Update index.rst (#876)\r\n- Update index.rst (#891)\r\n- Update navbar install (#983)\r\n- Update numba dependency in setup.py (#941)\r\n- Update outdated contributor list (#963)\r\n- Update prepare_clean_env.sh (#998)\r\n\r\nDocumentation\r\n=============\r\n- Add comment to BERT notebook (#1026)\r\n- Add missing docs for nlp.utils (#936)\r\n- Add more documentation to XLNet scripts (#985)\r\n- Add section for \"Clone the master branch for development\" (#1075)\r\n- Add to toc tree depth to enable multiple level menu (#1108)\r\n- Cite source of pretrained parameters for bert_12_768_12 (#915)\r\n- Doc fix for vocab.subwords (#885)\r\n- Enhance vocab not found err msg (#917)\r\n- Fix command line examples for text classification (#874)\r\n- Fix math formula in docs (#920)\r\n- More detailed doc for CorpusBPTTBatchify (#888)\r\n- Release checklist (#890)\r\n- Remove non-existent arguments for BERT and Transformer (#946)\r\n- Remove py3 usage from the doc (#1077)\r\n- Update installation guide with selectors (#966)\r\n- Update mxnet version in installation doc (#1072)\r\n- Update pre-trained model link (#1117)\r\n- Update Installation instructions for source (#1146) \r\n\r\nContinuous Integration\r\n======================\r\n- Disable SimVerb test for 14 days (#953)\r\n- Disable horovod test temporarily (#1030)\r\n- Disable known bad mxnet nightly version (#997)\r\n- Enable integration tests on CPU (#957)\r\n- Enable testing warnings with pytest and update deprecated API invocations (#980)\r\n- Enable timestamp in CI (#925)\r\n- Enable type checks and inference with pytype (#1018)\r\n- Fix CI (#875)\r\n- Preserve stderr and stdout streams in doc CI stage for Cloudwatch (#882)\r\n- Remove skip_master feature (#1017)\r\n- Switch source of MXNet nightly build (#1058)\r\n- Test MXNet 1.6 pre-release as part of CI pipeline (#1023)\r\n- Update MXNet master version tested on CI (#1113)\r\n- Update numba (#1096)\r\n- Use Cuda 10.0 MXNet build (#991)\r\n",
        "dateCreated": "2020-02-10T18:17:37Z",
        "datePublished": "2020-02-10T18:52:59Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.9.0",
        "name": "v0.9.0: BERT Inference Time Cut by Half and 90% Scaling Efficiency for Distributed Training",
        "tag_name": "v0.9.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.9.0",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/23351980",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.9.0"
      },
      {
        "authorType": "User",
        "author_name": "eric-haibin-lin",
        "body": "- Add int32 support for importance sampling (`model.ISDense`) and noise contrastive estimation (`model.NCEDense`). ",
        "dateCreated": "2020-01-14T21:06:31Z",
        "datePublished": "2020-01-14T21:09:44Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.8.3",
        "name": "v0.8.3: Minor Bug Fixes",
        "tag_name": "v0.8.3",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.8.3",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/22840261",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.8.3"
      },
      {
        "authorType": "User",
        "author_name": "eric-haibin-lin",
        "body": "This release covers a few fixes for the bugs reported:\r\n- Fixed argument passing in the `bert/embedding.py` script \r\n- Updated `SimVerb3500` dataset URL to the aclweb hosted version\r\n- Removed multi-processing in DataLoader from in `bert/pretraining_utils.py` which potentially causes crash when horovod mpi is used for training\r\n- Before MXNet 1.6.0, Gluon `Trainer` assumes **deterministic parameter creation order** for distributed traiing. The attention cell for BERT and transformer has a non-deterministic parameter creation order in v0.8.1 and v0.8.0, which will cause divergence during distributed training. It is now fixed. \r\n\r\nNote that since v0.8.2, the default branch of gluon-nlp github will be switched to the latest stable branch, instead of the master branch under development. ",
        "dateCreated": "2019-12-21T10:24:54Z",
        "datePublished": "2019-12-21T10:28:24Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.8.2",
        "name": "v0.8.2: Bug Fixes",
        "tag_name": "v0.8.2",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.8.2",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/22403852",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.8.2"
      },
      {
        "authorType": "User",
        "author_name": "eric-haibin-lin",
        "body": "News\r\n====\r\n- GluonNLP was featured in KDD 2019 Alaska! Check out our tutorial: [From Shallow to Deep Language Representations: Pre-training, Fine-tuning, and Beyond](http://kdd19.mxnet.io).\r\n- GluonNLP 0.8.1 will no longer support Python 2. (#721, #838)\r\n- Interested in BERT int8 quantization for deployment? Check out the blog post [here](https://medium.com/apache-mxnet/optimization-for-bert-inference-performance-on-cpu-3bb2413d376c).\r\n\r\nModels and Scripts\r\n==================\r\n### RoBERTa\r\n- The RoBERTa model introduced by *Yinhan Liu, et. al* in \"[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)\". The model checkpoints are converted from the [original repository](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md). Check out the usage [here](http://gluon-nlp.mxnet.io/model_zoo/bert/index.html). (#870)\r\n\r\n### Transformer-XL\r\n- The Transformer-XL model introduced by *Zihang Dai, et. al* in \"[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)\". (#846) \r\n\r\nBug Fixes\r\n=========\r\n- Fixed hybridization for the BERT model (#877)\r\n- Change the variable model to bert_classifier (#828) thank you @LindenLiu \r\n- Revert \"Add axis argument to squeeze()\" (#857)\r\n- [BUGFIX] Remove incorrect vocab.padding_token requirement in CorpusBPTTBatchify\r\n- [BUGFIX] Fix Vocab with unknown_token remapped to != 0 via token_to_idx arg (#862)\r\n- [BUGFIX] Fix AMP in finetune_classifier.py (#848)\r\n- [BUGFIX] fix broken multi-head attention cell (#878) @ZiyueHuang \r\n- [FIX] fix chnsenticorp dataset download link (#873)\r\n- fix the usage of pad in bert (#850)\r\n\r\n\r\nDocumentation\r\n=============\r\n- Clarify Bert does not require MXNet nightly anymore (#860)\r\n- [DOC] fix broken links (#833)\r\n- [DOC] Update BERT index.rst (#844)\r\n- [DOC] Add GluonCV/NLP archive (#823) \r\n- [DOC] add missing dataset document (#832) \r\n- [DOC] remove wrong tutorial header level (#826)\r\n- [DOC] Fix a typo in attention_cell's docstring (#841) thank you @shenfei\r\n- [DOC] Upgrade mxnet dependency to 1.5.0 and use Cuda 10.1 on CI (#842)\r\n- Remove Py2 icon from Readme. Add 3.7 (#856)\r\n- [DOC] Improve help message (#855) thank you @apeforest\r\n- Update index.rst (#853)\r\n- [DOC] Fix Machine Translation with Transformers example (#865)\r\n- update button style (#869)\r\n- [DOC] doc fix for vocab.subwords (#885) thank you @liusy182\r\n\r\nContinuous Integration\r\n======================\r\n- [CI] Support py3-master_gpu_doc CI run on arbitrary branches (#829)\r\n- Enforce AWS Batch jobName rules (#836)\r\n- dump linkcheck errors to comments (#827)\r\n- Enable Sphinx Autodoc typehints (#830)\r\n- [CI] Preserve stderr and stdout streams in doc CI stage for Cloudwatch",
        "dateCreated": "2019-08-21T06:16:32Z",
        "datePublished": "2019-08-21T06:17:43Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.8.1",
        "name": "v0.8.1",
        "tag_name": "v0.8.1",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.8.1",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/19363961",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.8.1"
      },
      {
        "authorType": "User",
        "author_name": "szha",
        "body": "News\r\n====\r\n- GluonNLP is featured in KDD 2019 Alaska! Check out our tutorial: [From Shallow to Deep Language Representations: Pre-training, Fine-tuning, and Beyond](https://kdd19.mxnet.io).\r\n- GluonNLP 0.8.0 will no longer support Python 2. #721 \r\n\r\nModels\r\n======\r\n### RoBERTa\r\n- [RoBERTa](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/) is now available in GluonNLP BERT model zoo. #870\r\n\r\n### Transformer-XL\r\n- [Transformer-XL](https://arxiv.org/abs/1901.02860) is now available in GluonNLP language model zoo. #846 ",
        "dateCreated": "2019-08-08T17:42:30Z",
        "datePublished": "2019-08-08T17:42:32Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.8.0",
        "name": "v0.8.0",
        "tag_name": "v0.8.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.8.0",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/19172152",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.8.0"
      },
      {
        "authorType": "User",
        "author_name": "eric-haibin-lin",
        "body": "News\r\n====\r\n- GluonNLP will be featured in KDD 2019 Alaska! Check out our tutorial: [From Shallow to Deep Language Representations: Pre-training, Fine-tuning, and Beyond](https://www.kdd.org/kdd2019/hands-on-tutorials).\r\n- GluonNLP was featured in JSALT 2019 in Montreal, 2019-6-14! Checkout https://jsalt19.mxnet.io.\r\n- This is the last release in GluonNLP that will officially support Python 2. #721 \r\n\r\nModels and Scripts\r\n==================\r\n### BERT\r\n- a BERT BASE model pre-trained on a large corpus including [OpenWebText Corpus](https://skylion007.github.io/OpenWebTextCorpus/), BooksCorpus, and English Wikipedia, which has comparable performance with the BERT large model from Google. The test score on GLUE Benchmark is reported below. Also improved usability of the BERT pre-training script: on-the-fly training data generation, sentencepiece, horovod, etc. (#799, #687, #806, #669, #665). Thank you @davisliang @vanyacohen @Skylion007\r\n\r\n| Source    | GluonNLP                                | google-research/bert       | google-research/bert       |\r\n|-----------|-----------------------------------------|-----------------------------|-----------------------------|\r\n| Model     | bert_12_768_12                          | bert_12_768_12              | bert_24_1024_16             |\r\n| Dataset   | `openwebtext_book_corpus_wiki_en_uncased` | `book_corpus_wiki_en_uncased` | `book_corpus_wiki_en_uncased` |\r\n| SST-2     | **95.3**                                    | 93.5                        | 94.9                        |\r\n| RTE       | **73.6**                                    | 66.4                        | 70.1                        |\r\n| QQP       | **72.3**                                    | 71.2                        | 72.1                        |\r\n| SQuAD 1.1     | **91.0/84.4**                               | 88.5/80.8                   | 90.9/84.1                   |\r\n| STS-B     | **87.5**                                    | 85.8                        | 86.5                        |\r\n| MNLI-m/mm | 85.3/84.9                               | 84.6/83.4                   | **86.7/85.9**                   |\r\n\r\n- The SciBERT model introduced by *Iz Beltagy and Arman Cohan and Kyle Lo* in \"[SciBERT: Pretrained Contextualized Embeddings for Scientific Text](https://arxiv.org/abs/1903.10676)\". The model checkpoints are converted from the [original repository](https://github.com/allenai/scibert) from AllenAI with the following datasets (#735):\r\n    - `scibert_scivocab_uncased`\r\n    - `scibert_scivocab_cased`\r\n    - `scibert_basevocab_uncased`\r\n    - `scibert_basevocab_cased`\r\n\r\n- The BioBERT model introduced by *Lee, Jinhyuk, et al.* in \"[BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://arxiv.org/abs/1901.08746)\". The model checkpoints are converted from the [original repository](https://github.com/naver/biobert-pretrained) with the following datasets (#735):\r\n    - `biobert_v1.0_pmc_cased`\r\n    - `biobert_v1.0_pubmed_cased`\r\n    - `biobert_v1.0_pubmed_pmc_cased`\r\n    - `biobert_v1.1_pubmed_cased`\r\n\r\n- The ClinicalBERT model introduced by *Kexin Huang and Jaan Altosaar and Rajesh Ranganath* in \"[ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://arxiv.org/abs/1904.05342)\". The model checkpoints are converted from the [original repository](https://github.com/kexinhuang12345/clinicalBERT) with the `clinicalbert_uncased` dataset (#735)\r\n\r\n- The ERNIE model introduced by *Sun, Yu, et al.* in \"[ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223)\". You can get the model checkpoints converted from the [original repository](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE) with `model.get_model(\"ernie_12_768_12\", \"baidu_ernie_uncased\")` (#759) thanks @paperplanet\r\n\r\n\r\n- BERT fine-tuning script for named entity recognition on CoNLL2003 with test F1 92.2 (#612).\r\n\r\n- BERT fine-tuning script for Chinese XNLI dataset with 78.3% validation accuracy. (#759) thanks @paperplanet\r\n\r\n- BERT fine-tuning script for intent classification and slot labelling on ATIS (95.9 F1) and SNIPS (95.9 F1). (#817)\r\n\r\n### GPT-2\r\n- The GPT-2 language model introduced by *Radford, Alec, et al.* in \"[Language Models are Unsupervised Multitask Learners](https://www.techbooky.com/wp-content/uploads/2019/02/Better-Language-Models-and-Their-Implications.pdf)\". The model checkpoints are converted from the [original repository](https://github.com/openai/gpt-2), with a script to generate text from GPT-2 model (`gpt2_117m`, `gpt2_345m`) trained on the `openai_webtext` dataset (#761).\r\n\r\n### ESIM\r\n- The ESIM model for text matching introduced by *Chen, Qian, et al.* in \"[Enhanced LSTM for Natural Language Inference](https://arxiv.org/abs/1609.06038)\". (#689)\r\n\r\n\r\nData\r\n====\r\n- Natural language understanding with datasets from the GLUE benchmark: CoLA, SST-2, MRPC, STS-B, MNLI, QQP, QNLI, WNLI, RTE (#682)\r\n- Sentiment analysis datasets: CR, MPQA (#663)\r\n- Intent classification and slot labeling datasets: ATIS and SNIPS (#816)\r\n\r\nNew Features\r\n============\r\n- [Feature] support save model / trainer states to S3 (#700)\r\n- [Feature] support load model/trainer states from s3 (#702)\r\n- [Feature] Add SentencePieceTokenizer for BERT (#669)\r\n- [FEATURE] Flexible vocabulary (#732)\r\n- [API] Moving MaskedSoftmaxCELoss and LabelSmoothing to model API (#754) thanks @ThomasDelteil \r\n- [Feature] add the List batchify function (#812) thanks @ThomasDelteil\r\n- [FEATURE] Add LAMB optimizer (#733) \r\n\r\nBug Fixes\r\n=========\r\n- [BUGFIX] Fixes for BERT embedding, pretraining scripts (#640) thanks @Deseaus\r\n- [BUGFIX] Update hash of wiki_cn_cased and wiki_multilingual_cased vocab (#655)\r\n- fix bert forward call parameter mismatch (#695) thanks @paperplanet\r\n- [BUGFIX] Fix mlm_loss reporting for eval dataset (#696)\r\n- Fix _get_rnn_cell (#648) thanks @MarisaKirisame\r\n- [BUGFIX] fix mrpc dataset idx (#708)\r\n- [bugfix] fix hybrid beam search sampler(#710)\r\n- [BUGFIX] [DOC] Update nlp.model.get_model documentation and get_model API (#734)\r\n- [BUGFIX] Fix handling of duplicate special tokens in Vocabulary (#749)\r\n- [BUGFIX] Fix TokenEmbedding serialization with `emb[emb.unknown_token] != 0` (#763)\r\n- [BUGFIX] Fix glue test result serialization (#773)\r\n- [BUGFIX] Fix init bug for multilevel BiLMEncoder (#783) thanks @Ishitori\r\n\r\n\r\n\r\nAPI Changes\r\n===========\r\n- [API] Dropping support for wiki_multilingual and wiki_cn (#764)\r\n- [API] Remove get_bert_model from the public API list (#767) \r\n\r\nEnhancements\r\n============\r\n- [FEATURE] offer load_w2v_binary method to load w2v binary file (#620)\r\n- [Script] Add inference function for BERT classification (#639) thanks @TaoLv\r\n- [SCRIPT] - Add static BERT base export script (for use with MXNet Module API) (#672)\r\n- [Enhancement] One script to export bert for classification/regression/QA (#705)\r\n- [enhancement] refactor bert finetuning script (#692) \r\n- [Enhancement] only use the best model for inference for bert classification (#716)\r\n- [Dataset] redistribute conll2004 (#719)\r\n- [Enhancement] add periodic evaluation for BERT pre-training (#720)\r\n- [FEATURE]add XNLI task (#717)\r\n- [refactor] Refactor BERT script folder (#744) \r\n- [Enhancement] BERT pre-training data generation from sentencepiece vocab (#743)\r\n- [REFACTOR] Refactor TokenEmbedding to reduce number of places that initialize internals (#750)\r\n- [Refactor] Refactor BERT SQuAD inference code (#758)\r\n- [Enhancement] Fix dtype conversion, add sentencepiece support for SQuAD (#766)\r\n- [Dataset] Move MRPC dataset to API (#780) \r\n- [BiDAF-QANet] Common data processing logic for BiDAF and QANet (#739) thanks @Ishitori \r\n- [DATASET] add LCQMC, ChnSentiCorp dataset (#774)  thanks @paperplanet\r\n- [Improvement] Implement parser evaluation in Python (#772)\r\n- [Enhancement] Add whole word masking for BERT (#770) thanks @basicv8vc\r\n- [Enhancement] Mix precision support for BERT finetuning (#793)\r\n-  Generate BERT training samples in compressed format (#651)\r\n\r\n\r\nMinor Fixes\r\n===========\r\n- Various documentation fixes: #635, #637, #647, #656, #664, #667, #670, #676, #678, #681, #698, #704, #731, #745, #762, #771, #746, #778, #800, #810, #807 #814 thanks @rongruosong @crcrpar @mrchypark @xwind-h \r\n- Fix BERT multiprocessing data creation bug which causes unnecessary dispatching to single worker (#649)\r\n- [BUGFIX] Update BERT test and pre-train script (#661)\r\n- update url for ws353 (#701)\r\n- bump up version (#742)\r\n- [DOC] Update textCNN results (#737) \r\n- padding value warning (#747)\r\n- [TUTORIAL][DOC] Tutorial Updates (#802) thanks @faramarzmunshi\r\n\r\nContinuous Integration\r\n======================\r\n- skip failing tests in mxnet master (#685)\r\n- [CI] update nodes for CI (#686)\r\n- [CI] CI refactoring to speed up tests (#566) \r\n- [CI] fix codecov (#693)\r\n- use fixture for squad dataset tests (#699)\r\n- [CI] create zipped notebooks for link check (#712)\r\n- Fix test infrastructure for pytest > 4 and bump CI pytest version (#728)\r\n- [CI] set root in BERT tests (#738)\r\n- Fix conftest.py function_scope_seed (#748)\r\n- [CI] Fix links in contribute.rst (#752) \r\n- [CI] Update CI dependencies (#756) \r\n- Revert \"[CI] Update CI dependencies (#756)\" (#769) \r\n- [CI] AWS Batch serverless CI Pipeline for parallel notebook execution during website build step (#791)\r\n- [CI] Don't exit pipeline before displaying AWS Batch logfiles (#801)\r\n- [CI] Fix for \"Don't exit pipeline before displaying AWS Batch logfile (#803)\r\n- add license checker (#804)\r\n- enable timeout (#813)\r\n- Fix website build on master branch (#819) ",
        "dateCreated": "2019-07-17T23:03:49Z",
        "datePublished": "2019-07-17T23:05:35Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.7.1",
        "name": "v0.7.1",
        "tag_name": "v0.7.1",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.7.1",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/18688595",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.7.1"
      },
      {
        "authorType": "User",
        "author_name": "eric-haibin-lin",
        "body": "News\r\n====\r\n- GluonNLP will be featured in KDD 2019 Alaska! Check out our tutorial: [From Shallow to Deep Language Representations: Pre-training, Fine-tuning, and Beyond](https://www.kdd.org/kdd2019/hands-on-tutorials).\r\n- GluonNLP was featured in JSALT 2019 in Montreal, 2019-6-14! Checkout https://jsalt19.mxnet.io.\r\n\r\nModels and Scripts\r\n==================\r\n### BERT\r\n- BERT model pre-trained on [OpenWebText Corpus](https://skylion007.github.io/OpenWebTextCorpus/), BooksCorpus, and English Wikipedia. The test score on GLUE Benchmark is reported below. Also improved usability of the BERT pre-training script: on-the-fly training data generation, sentencepiece, horovod, etc. (#799, #687, #806, #669, #665). Thank you @davisliang\r\n\r\n| Source    | GluonNLP                                | google-research/bert       | google-research/bert       |\r\n|-----------|-----------------------------------------|-----------------------------|-----------------------------|\r\n| Model     | bert_12_768_12                          | bert_12_768_12              | bert_24_1024_16             |\r\n| Dataset   | `openwebtext_book_corpus_wiki_en_uncased` | `book_corpus_wiki_en_uncased` | `book_corpus_wiki_en_uncased` |\r\n| SST-2     | **95.3**                                    | 93.5                        | 94.9                        |\r\n| RTE       | **73.6**                                    | 66.4                        | 70.1                        |\r\n| QQP       | **72.3**                                    | 71.2                        | 72.1                        |\r\n| SQuAD 1.1     | **91.0/84.4**                               | 88.5/80.8                   | 90.9/84.1                   |\r\n| STS-B     | **87.5**                                    | 85.8                        | 86.5                        |\r\n| MNLI-m/mm | 85.3/84.9                               | 84.6/83.4                   | **86.7/85.9**                   |\r\n\r\n- The SciBERT model introduced by *Iz Beltagy and Arman Cohan and Kyle Lo* in \"[SciBERT: Pretrained Contextualized Embeddings for Scientific Text](https://arxiv.org/abs/1903.10676)\". The model checkpoints are converted from the [original repository](https://github.com/allenai/scibert) from AllenAI with the following datasets (#735):\r\n    - `scibert_scivocab_uncased`\r\n    - `scibert_scivocab_cased`\r\n    - `scibert_basevocab_uncased`\r\n    - `scibert_basevocab_cased`\r\n\r\n- The BioBERT model introduced by *Lee, Jinhyuk, et al.* in \"[BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://arxiv.org/abs/1901.08746)\". The model checkpoints are converted from the [original repository](https://github.com/naver/biobert-pretrained) with the following datasets (#735):\r\n    - `biobert_v1.0_pmc_cased`\r\n    - `biobert_v1.0_pubmed_cased`\r\n    - `biobert_v1.0_pubmed_pmc_cased`\r\n    - `biobert_v1.1_pubmed_cased`\r\n\r\n- The ClinicalBERT model introduced by *Kexin Huang and Jaan Altosaar and Rajesh Ranganath* in \"[ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://arxiv.org/abs/1904.05342)\". The model checkpoints are converted from the [original repository](https://github.com/kexinhuang12345/clinicalBERT) with the `clinicalbert_uncased` dataset (#735)\r\n\r\n- The ERNIE model introduced by *Sun, Yu, et al.* in \"[ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223)\". You can get the model checkpoints converted from the [original repository](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE) with `model.get_model(\"ernie_12_768_12\", \"baidu_ernie_uncased\")` (#759) thanks @paperplanet\r\n\r\n\r\n- BERT fine-tuning script for named entity recognition on CoNLL2003 with test F1 92.2 (#612).\r\n\r\n- BERT fine-tuning script for Chinese XNLI dataset with 78.3% validation accuracy. (#759) thanks @paperplanet\r\n\r\n- BERT fine-tuning script for intent classification and slot labelling on ATIS (95.9 F1) and SNIPS (95.9 F1). (#817)\r\n\r\n### GPT-2\r\n- The GPT-2 language model introduced by *Radford, Alec, et al.* in \"[Language Models are Unsupervised Multitask Learners](https://www.techbooky.com/wp-content/uploads/2019/02/Better-Language-Models-and-Their-Implications.pdf)\". The model checkpoints are converted from the [original repository](https://github.com/openai/gpt-2), with a script to generate text from GPT-2 model (`gpt2_117m`, `gpt2_345m`) trained on the `openai_webtext` dataset (#761).\r\n\r\n### ESIM\r\n- The ESIM model for text matching introduced by *Chen, Qian, et al.* in \"[Enhanced LSTM for Natural Language Inference](https://arxiv.org/abs/1609.06038)\". (#689)\r\n\r\n\r\nData\r\n====\r\n- Natural language understanding with datasets from the GLUE benchmark: CoLA, SST-2, MRPC, STS-B, MNLI, QQP, QNLI, WNLI, RTE (#682)\r\n- Sentiment analysis datasets: CR, MPQA (#663)\r\n- Intent classification and slot labeling datasets: ATIS and SNIPS (#816)\r\n\r\nNew Features\r\n============\r\n- [Feature] support save model / trainer states to S3 (#700)\r\n- [Feature] support load model/trainer states from s3 (#702)\r\n- [Feature] Add SentencePieceTokenizer for BERT (#669)\r\n- [FEATURE] Flexible vocabulary (#732)\r\n- [API] Moving MaskedSoftmaxCELoss and LabelSmoothing to model API (#754) thanks @ThomasDelteil \r\n- [Feature] add the List batchify function (#812) thanks @ThomasDelteil\r\n- [FEATURE] Add LAMB optimizer (#733) \r\n\r\nBug Fixes\r\n=========\r\n- [BUGFIX] Fixes for BERT embedding, pretraining scripts (#640) thanks @Deseaus\r\n- [BUGFIX] Update hash of wiki_cn_cased and wiki_multilingual_cased vocab (#655)\r\n- fix bert forward call parameter mismatch (#695) thanks @paperplanet\r\n- [BUGFIX] Fix mlm_loss reporting for eval dataset (#696)\r\n- Fix _get_rnn_cell (#648) thanks @MarisaKirisame\r\n- [BUGFIX] fix mrpc dataset idx (#708)\r\n- [bugfix] fix hybrid beam search sampler(#710)\r\n- [BUGFIX] [DOC] Update nlp.model.get_model documentation and get_model API (#734)\r\n- [BUGFIX] Fix handling of duplicate special tokens in Vocabulary (#749)\r\n- [BUGFIX] Fix TokenEmbedding serialization with `emb[emb.unknown_token] != 0` (#763)\r\n- [BUGFIX] Fix glue test result serialization (#773)\r\n- [BUGFIX] Fix init bug for multilevel BiLMEncoder (#783) thanks @Ishitori\r\n\r\n\r\n\r\nAPI Changes\r\n===========\r\n- [API] Dropping support for wiki_multilingual and wiki_cn (#764)\r\n- [API] Remove get_bert_model from the public API list (#767) \r\n\r\nEnhancements\r\n============\r\n- [FEATURE] offer load_w2v_binary method to load w2v binary file (#620)\r\n- [Script] Add inference function for BERT classification (#639) thanks @TaoLv\r\n- [SCRIPT] - Add static BERT base export script (for use with MXNet Module API) (#672)\r\n- [Enhancement] One script to export bert for classification/regression/QA (#705)\r\n- [enhancement] refactor bert finetuning script (#692) \r\n- [Enhancement] only use the best model for inference for bert classification (#716)\r\n- [Dataset] redistribute conll2004 (#719)\r\n- [Enhancement] add periodic evaluation for BERT pre-training (#720)\r\n- [FEATURE]add XNLI task (#717)\r\n- [refactor] Refactor BERT script folder (#744) \r\n- [Enhancement] BERT pre-training data generation from sentencepiece vocab (#743)\r\n- [REFACTOR] Refactor TokenEmbedding to reduce number of places that initialize internals (#750)\r\n- [Refactor] Refactor BERT SQuAD inference code (#758)\r\n- [Enhancement] Fix dtype conversion, add sentencepiece support for SQuAD (#766)\r\n- [Dataset] Move MRPC dataset to API (#780) \r\n- [BiDAF-QANet] Common data processing logic for BiDAF and QANet (#739) thanks @Ishitori \r\n- [DATASET] add LCQMC, ChnSentiCorp dataset (#774)  thanks @paperplanet\r\n- [Improvement] Implement parser evaluation in Python (#772)\r\n- [Enhancement] Add whole word masking for BERT (#770) thanks @basicv8vc\r\n- [Enhancement] Mix precision support for BERT finetuning (#793)\r\n-  Generate BERT training samples in compressed format (#651)\r\n\r\n\r\nMinor Fixes\r\n===========\r\n- Various documentation fixes: #635, #637, #647, #656, #664, #667, #670, #676, #678, #681, #698, #704, #731, #745, #762, #771, #746, #778, #800, #810, #807 #814 thanks @rongruosong @crcrpar @mrchypark @xwind-h \r\n- Fix BERT multiprocessing data creation bug which causes unnecessary dispatching to single worker (#649)\r\n- [BUGFIX] Update BERT test and pre-train script (#661)\r\n- update url for ws353 (#701)\r\n- bump up version (#742)\r\n- [DOC] Update textCNN results (#737) \r\n- padding value warning (#747)\r\n- [TUTORIAL][DOC] Tutorial Updates (#802) thanks @faramarzmunshi\r\n\r\nContinuous Integration\r\n======================\r\n- skip failing tests in mxnet master (#685)\r\n- [CI] update nodes for CI (#686)\r\n- [CI] CI refactoring to speed up tests (#566) \r\n- [CI] fix codecov (#693)\r\n- use fixture for squad dataset tests (#699)\r\n- [CI] create zipped notebooks for link check (#712)\r\n- Fix test infrastructure for pytest > 4 and bump CI pytest version (#728)\r\n- [CI] set root in BERT tests (#738)\r\n- Fix conftest.py function_scope_seed (#748)\r\n- [CI] Fix links in contribute.rst (#752) \r\n- [CI] Update CI dependencies (#756) \r\n- Revert \"[CI] Update CI dependencies (#756)\" (#769) \r\n- [CI] AWS Batch serverless CI Pipeline for parallel notebook execution during website build step (#791)\r\n- [CI] Don't exit pipeline before displaying AWS Batch logfiles (#801)\r\n- [CI] Fix for \"Don't exit pipeline before displaying AWS Batch logfile (#803)\r\n- add license checker (#804)\r\n- enable timeout (#813)\r\n- Fix website build on master branch (#819) ",
        "dateCreated": "2019-07-09T18:33:46Z",
        "datePublished": "2019-07-09T18:36:15Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.7.0",
        "name": "v0.7.0",
        "tag_name": "v0.7.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.7.0",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/18505249",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.7.0"
      },
      {
        "authorType": "User",
        "author_name": "eric-haibin-lin",
        "body": "News\r\n====\r\n- Tutorial proposal for GluonNLP is accepted at [EMNLP 2019](https://www.emnlp-ijcnlp2019.org/), Hong Kong, and [KDD 2019](https://www.kdd.org/kdd2019/), Anchorage.\r\n\r\nModels and Scripts\r\n==================\r\n- BERT pre-training on BooksCorpus and English Wikipedia with mixed precision and gradient accumulation on GPUs. We achieved the following fine-tuning results based on the produced checkpoint on validation sets(#482, #505, #489). Thank you @haven-jeon\r\n    - |  Dataset   |      MRPC      | SQuAD 1.1      |   SST-2     | MNLI-mm |\r\n      |:----------:|:--------------:|:--------------:|:-----------:|:-------:|\r\n      |  Score     |      87.99%    |  80.99/88.60   |    93%      | 83.6%   |\r\n\r\n- BERT fine-tuning on various sentence classification datasets with checkpoints converted from the official repository(#600, #571, #481). Thank you @kenjewu @haven-jeon \r\n    - |  Dataset  |      MRPC      |       RTE      |      SST-2     |    MNLI-m/mm   |\r\n      |:---------:|:--------------:|:--------------:|:--------------:|:--------------:|\r\n      | Score     |      88.7%     |      70.8%     |       93%      | 84.55%, 84.66% |\r\n- BERT fine-tuning on question answering datasets with checkpoints converted from the official repository(#493). Thank you @fiercex \r\n    - |  Dataset  |      SQuAD 1.1  |      SQuAD 1.1  |   SQuAD 2.0   |\r\n      |:---------:|:---------------:|:---------------:|:-------------:|\r\n      | Model     |   bert_12_768_12| bert_24_1024_16 |bert_24_1024_16|\r\n      | F1/EM     |   88.53/80.98   |    90.97/84.05  |   77.96/81.02 |\r\n- BERT model convertion scripts for checkpoints from the original tensorflow repository, and more converted models(#456, #461, #449). Thank you @fiercex:\r\n    - Multilingual Wikipedia (cased, BERT Base)\r\n    - Chinese Wikipedia (cased, BERT Base)\r\n    - Books Corpus & English Wikipedia (uncased, BERT Large)\r\n- Scripts and command line interface for BERT embedding of raw sentences(#587, #618). Thank you @imgarylai\r\n- Scripts for exporting BERT model for deployment (#624)\r\n\r\nNew Features\r\n============\r\n- [API] Add BERTVocab (#509) thanks @kenjewu\r\n- [API] Add Transforms for BERT (#526) thanks @kenjewu\r\n- [API] add data parallel for transformer (#387)\r\n- [FEATURE] Add squad2.0 Dataset (#551) thanks @fiercex\r\n- [FEATURE] Add NumpyDataset (#498)\r\n- [FEATURE] Add TruncNorm initializer for BERT (#548) thanks @Ishitori\r\n- [FEATURE] Add split sampler for distributed training (#494)\r\n- [FEATURE] Custom metric for masked accuracy (#503)\r\n- [FEATURE] Support custom sampler in SimpleDatasetStream (#507)\r\n- [FEATURE] clip gradient norm by parameter (#470)\r\n\r\n \r\nBug Fixes\r\n=========\r\n- [BUGFIX] Fix Data Preprocessing for Translation Data (#568)\r\n- [FIX] fix parameter clip (#527)\r\n- [FIX] Fix divergence of the training of transformer (#543)\r\n- [FIX] Fix documentation and a bug in NCE Block (#558)\r\n- [FIX] Fix hashing single ngrams in NGramHashes (#450)\r\n- [FIX] Fix weight dying in BERTModel.decoder for BERT pre-training (#500)\r\n- [BUGFIX] Modifying the FastText Classification training for accurate mean pooling (#529) thanks @sravanbabuiitm \r\n\r\nAPI Changes\r\n===========\r\n- [API] BERT return intermediate encodings per layer (#606) thanks @Ishitori\r\n- [API] Better handle case when backoff is not possible in TokenEmbedding (#459)\r\n- [FIX] Rename wiki_cn/wiki_multilingual to wiki_cn_cased/wiki_multilingual_uncased (#594) thanks @kenjewu\r\n- [FIX] Update default value of BERTAdam epsilon to 1e-6 (#601)\r\n- [FIX] Fix BERT decoder API for masked language model prediction (#501)\r\n- [FIX] Remove bias correction term in BERTAdam (#499)\r\n\r\nEnhancements\r\n============\r\n- [BUGFIX] use glove.840B.300d for NLI experiments (#567)\r\n- [API] Add debug option for parallel (#584)\r\n- [FEATURE] Skip dropout layer in Transformer when rate=0 (#597) thanks @TaoLv\r\n- [FEATURE] update sharded loader (#468)\r\n- [FIX] Update BERTLayerNorm Implementation (#485)\r\n- [TUTORIAL] Use FixedBucketSampler in BERT tutorial for better performance (#506) thanks @Ishitori\r\n- [API] Add Bert tokenizer to transforms.py (#464) thanks @fiercex\r\n- [FEATURE] Add data parallel to big rnn lm script (#564)\r\n\r\nMinor Fixes\r\n===========\r\n- Various documentation fixes: #484, #613, #614, #438, #448, #550, #563, #611, #605, #440, #554, #445, #556, #603, #483, #576, #610, #547, #458, #574, #510, #447, #465, #436, #622, #583 thanks @anuragsarkar97 @brettkoonce \r\n- [FIX] fix repeated unzipping in squad dataset (#553)\r\n- [FIX] web fixes (#453)\r\n- [FIX] Remove unused argument in fasttext_word_ngram.py (#486) thanks @kurtjanssensai \r\n- [FIX] Remove unused code (#528)\r\n- [FIX] Remove unused code in text_classification script (#442)\r\n- [MISC] Bump up version (#454)\r\n- [BUGFIX] fix pylint error (#549)\r\n- [FIX] Simplify the data preprocessing code for the sentiment analysis script (#462)\r\n- [FEATURE] BERT doc fixes and script usability enhancements (#444)\r\n- [FIX] Fix Py2 compatibility of machine_translation/dataprocessor.py (#541) thanks @ymjiang\r\n- [BUGFIX] Fix GluonNLP MXNet dependency (#555)\r\n- [BUGFIX] Fix Weight Drop and Test (#546)\r\n- [CI] Add version upper bound to doc.yml (#467)\r\n- [CI] speed up tests (#582)\r\n- [CI] upgrade mxnet to 1.4.0 (#617)\r\n- [FIX] Revert an unintended change (#525)\r\n- [BUGFIX] update paths and imports in bert scripts (#634)",
        "dateCreated": "2019-03-18T06:52:05Z",
        "datePublished": "2019-03-18T21:19:52Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.6.0",
        "name": "v0.6.0",
        "tag_name": "v0.6.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.6.0",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/16190583",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.6.0"
      },
      {
        "authorType": "User",
        "author_name": "eric-haibin-lin",
        "body": "Highlights\r\n=================\r\n\r\n- Featured in [AWS re:invent](https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=88736) 2018\r\n\r\n\r\nModels\r\n======\r\n\r\n- **BERT**\r\n  - The [Bidirectional Encoder Representations from Transformers](http://gluon-nlp.mxnet.io/model_zoo/bert/index.html) model as introduced by *Devlin, Jacob, et al. \"[Bert: Pre-training of deep bidirectional transformers for language understanding.](https://arxiv.org/abs/1802.05365)\" arXiv preprint arXiv:1810.04805 (2018)* (#409).\r\n  Model parameters are converted from the [original model checkpoints from Google research](https://github.com/google-research/bert/), including:\r\n    - BERT BASE model trained on\r\n      - Book Corpus & English Wikipedia (cased)\r\n      - Book Corpus & English Wikipedia (uncased)\r\n      - multilingual Wikipedia (uncased)\r\n    - BERT LARGE model trained on Book Corpus & English Wikipedia (uncased)\r\n- **ELMo**\r\n  - The [Embeddings from Language Models](http://gluon-nlp.mxnet.io/api/modules/model.html#elmo) as introduced by *Peters, Matthew E., et al. \"[Deep contextualized word representations](https://arxiv.org/abs/1802.05365).\" arXiv preprint arXiv:1802.05365 (2018)* (#227, #428).\r\n    Model parameters are converted from the [original model checkpoints in AllenNLP](https://allennlp.org/elmo), including the small, medium, original models trained on 1 billion words dataset, and the original model trained on 5.5B tokens consisting of Wikipedia & monolingual news crawl data from WMT 2008-2012.\r\n- **Word Embedding**\r\n    - The [GloVe](http://gluon-nlp.mxnet.io/model_zoo/word_embeddings/index.html) model as introduced by *Pennington, Jeffrey, Richard Socher, and Christopher Manning. \"[Glove: Global vectors for word representation](http://www.aclweb.org/anthology/D14-1162).\"\u00a0Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014* (#359). \r\n- **Natural Language Inference**\u00a0\r\n    - The [Decomposable Attention Model](http://gluon-nlp.mxnet.io/model_zoo/natural_language_inference/index.html) as introduced by *Parikh, Ankur P., et al. \"[A decomposable attention model for natural language inference](https://arxiv.org/abs/1606.01933).\" arXiv preprint arXiv:1606.01933 (2016).* (#404). On the SNLI test set, it achieves 84.6% accuracy (without intra-sentence attention) and 84.4% accuracy (with intra-sentence attention). Thank you @linmx0130 @hhexiy!\r\n- **Dependency Parsing**\r\n  - The [Deep Biaffine Attention Dependency Parser](http://gluon-nlp.mxnet.io/model_zoo/parsing/index.html) as introduced by *Dozat, Timothy, and Christopher D. Manning. \"[Deep biaffine attention for neural dependency parsing](https://arxiv.org/pdf/1611.01734.pdf).\"\u00a0arXiv preprint arXiv:1611.01734\u00a0(2016).* (#408). It achieved 96% UAS on the Penn Treebank dataset. Thank you @hankcs!\r\n  \r\n- **Text Classification**\r\n  - The [Text CNN](http://gluon-nlp.mxnet.io/model_zoo/sentiment_analysis/index.html) model as introduced by *Kim, Yoon. \"[Convolutional neural networks for sentence classification](https://arxiv.org/abs/1408.5882).\" arXiv preprint arXiv:1408.5882 (2014)*. (#391) Thank you @xiaotinghe!\r\n\r\n\r\nNew Tutorials\r\n============\r\n\r\n- **ELMo**\r\n  - A [tutorial](http://gluon-nlp.mxnet.io/examples/sentence_embedding/elmo_sentence_representation.html) on generating contextualized representation with the pre-trained ELMo model, as introduced by *Peters, Matthew E., et al. \"[Deep contextualized word representations](https://arxiv.org/abs/1802.05365).\" arXiv preprint arXiv:1802.05365 (2018)* (#227, #428).\r\n- **BERT**\r\n  - A [tutorial](http://gluon-nlp.mxnet.io/model_zoo/bert/index.html) on fine-tuning the BERT model for sentence pair classification, as introduced by *Devlin, Jacob, et al. \"[Bert: Pre-training of deep bidirectional transformers for language understanding.](https://arxiv.org/abs/1802.05365)\" arXiv preprint arXiv:1810.04805 (2018)* (#437)\r\n\r\nNew Datasets\r\n============\r\n- **Sentiment Analysis**\r\n  - [MR](https://www.cs.cornell.edu/people/pabo/movie-review-data), a movie-review data set of 10,662 sentences labeled with respect to their overall sentiment polarity (positive or negative). (#391)\r\n  - [SST_1](http://nlp.stanford.edu/sentiment), an extension of the MR data set with fine-grained labels (#391)\r\n  - [SST_2](http://nlp.stanford.edu/sentiment), an extension of the MR data set with binary sentiment polarity labels (#391)\r\n  - [SUBJ](https://www.cs.cornell.edu/people/pabo/movie-review-data), a subjectivity data set for sentiment analysis (#391)\r\n  - [TREC](http://cogcomp.org/page/resource_view/49), a movie-review data set of 10,000 sentences labeled with respect to their subjectivity status (subjective or objective). (#391)\r\n\r\n\r\nAPI Updates\r\n===========\r\n- Changed Vocab constructor from staticmethod to classmethod to handle inheritance (#386)\r\n- Added Transformer Encoder APIs (#409) \r\n- Added pre-trained ELMo model to model.get_model API (#227)\r\n- Added pre-trained BERT model to model.get_model API (#409)\r\n- Added unknown_lookup setter to TokenEmbedding (#429)\r\n- Added dtype support to EmbeddingCenterContextBatchify (#416)\r\n- Propagated exceptions from PrefetchingStream (#406)\r\n- Added sentencepiece tokenizer detokenizer (#380)\r\n- Added CSR format for variable length data in embedding training (#384)\r\n\r\n\r\nFixes & Small Changes\r\n=================\r\n- Included output of nlp.embedding.list_sources() in API docs (#421)\r\n- Supported symlinks in examples and scripts (#403)\r\n- Fixed weight tying in GNMT and Transformer (#413)\r\n- Simplified transformer notebook (#400) \r\n- Fixed LazyTransformDataStream prefetching (#397)\r\n- Adopted src/gluonnlp folder layout (#390)\r\n- Fixed text8 archive file name for downloads from S3 (#388) Thanks @bkktimber! \r\n- Fixed ppl reporting for training on multi gpu in the language model notebook (#365). Thanks @ThomasDelteil! \r\n- Fixed a spelling mistake in QA script. (#379) Thanks @qyhfbqz!\r\n",
        "dateCreated": "2018-11-27T05:01:07Z",
        "datePublished": "2018-11-27T06:52:05Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.5.0",
        "name": "v0.5.0",
        "tag_name": "v0.5.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.5.0",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/14199991",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.5.0"
      },
      {
        "authorType": "User",
        "author_name": "cgraywang",
        "body": "Highlights\r\n=================\r\n\r\n- Hands-on Tutorial at KDD 2018 ([website](https://kdd18.mxnet.io/)) ([code](https://github.com/szha/KDD18-Gluon))\r\n\r\nModels\r\n======\r\n\r\n- **Language Model**\r\n  - The [Large Scale Word Language Model](http://gluon-nlp.mxnet.io/scripts/index.html#large-scale-word-language-model) as introduced by *Jozefowicz, Rafal, et al. \u201cExploring the limits of language modeling\u201d. arXiv preprint arXiv:1602.02410 (2016)* achieved test PPL *43.62* on GBW dataset (#179 #270 #277 #278 #286 #294)\r\n  - The [NT-ASGD based Language Model](http://gluon-nlp.mxnet.io/master/model_zoo/language_model/index.html) as introduced by *Merity, S., et al. \u201cRegularizing and optimizing LSTM language models\u201d. ICLR 2018* achieved test PPL *65.62* on WikiText-2 dataset (#170)\r\n- **Document Classification**\r\n  - The [Classification Model](http://gluon-nlp.mxnet.io/scripts/index.html#document-classification) as introduced by *Joulin, Armand, et al. \u201cBag of tricks for efficient text classification\u201d* achieved validation accuracy validation accuracy *98* on Yelp review dataset (#258 #297)\r\n- **Question Answering**\r\n  - The [QANet](https://github.com/dmlc/gluon-nlp/blob/qanet/scripts/question_answering/train.py) as introduced by *Jozefowicz, Rafal, et al. \u201c\r\nQANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension\u201d. ICLR 2018* achieved F1 score *79.5* on SQuAD 1.1 dataset (#339) (coming soon to master branch)\r\n\r\nNew Tutorials\r\n============\r\n\r\n- **Machine Translation**\r\n  - The [Google NMT](http://gluon-nlp.mxnet.io/examples/machine_translation/gnmt.html) as introduced by *Wu, Yonghui, et al. \u201cGoogle's neural machine translation system:\r\n   Bridging the gap between human and machine translation\u201d. arXiv preprint arXiv:1609.08144 (2016)* is introduced as part of the gluonnlp tutorial (#261)\r\n  - The [Transformer based Machine Translation](http://gluon-nlp.mxnet.io/examples/machine_translation/transformer.html) by *Vaswani, Ashish, et al. \u201cAttention is all you need.\u201d Advances in Neural Information Processing Systems. 2017* is introduced as part of the gluonnlp tutorial (#279)\r\n- **Sentence Embedding**\r\n  - [A Structured Self-attentive Sentence Embedding](http://gluon-nlp.mxnet.io/examples/sentence_embedding/self_attentive_sentence_embedding.html) (#366) by *Z. Lin, M. Feng, C. Santos, M. Yu, B. Xiang, B. Zhou, Y. Bengio, \"A Structured Self-attentive Sentence Embedding\" ICLR 2017* is introduced in gluonnlp tutorial (#366)\r\n\r\nNew Datasets\r\n============\r\n\r\n- **Word Embedding**\r\n  - [Wikipedia](https://github.com/dmlc/gluon-nlp/blob/master/scripts/word_embeddings/data.py) (#218)\r\n  - [Fil9 dataset](https://github.com/dmlc/gluon-nlp/blob/master/gluonnlp/data/corpora/large_text_compression_benchmark.py)(#363)\r\n  - FastText [crawl-300d-2M-subword](https://github.com/dmlc/gluon-nlp/blob/master/gluonnlp/_constants.py)(#336), [wiki-news-300d-1M-subword](https://github.com/dmlc/gluon-nlp/blob/master/gluonnlp/_constants.py)(#368), [cc.en.300](https://github.com/dmlc/gluon-nlp/blob/master/gluonnlp/_constants.py)(#373)\r\n\r\nAPI updates\r\n===========\r\n\r\n- Added dataloader that allows multi-shard sampling (#237 #280 #285)\r\n- Simplified DataStream, added DatasetStream, refactored and extended PrefetchingStream (#235)\r\n- Unified BPTT batchify for dataset and stream (#246)\r\n- Added symbolic beam search (#233)\r\n- Added SequenceSampler (#272) \r\n- Refactored Transform APIs (#282) \r\n- Reorganized index of the repo and model zoo page (#357)\r\n\r\nFixes & Small Changes\r\n=================\r\n\r\n- Fixed module name in batchify.py example (#239)\r\n- Improved imports structure (#248)\r\n- Added test for nmt scripts (#234)\r\n- Speeded up batchify.Pad (#249)\r\n- Fixed LanguageModelDataset.bptt_batchify (#243)\r\n- Fixed weight drop and add tests (#268)\r\n- Fixed relative links that pypi doesn't handle (#293)\r\n- Updated notebook build logic (#309)\r\n- Added community link (#313)\r\n- Enabled run tests in parallel (#317)\r\n- Enabled word embedding scripts tests (#321)\r\n\r\n[See all commits](https://github.com/dmlc/gluon-nlp/compare/v0.3.3...v0.4.1)",
        "dateCreated": "2018-10-24T17:35:04Z",
        "datePublished": "2018-10-24T17:35:59Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.4.1",
        "name": "v0.4.1",
        "tag_name": "v0.4.1",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.4.1",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/13441331",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.4.1"
      },
      {
        "authorType": "User",
        "author_name": "leezu",
        "body": "GluonNLP v0.3 contains many exciting new features.\r\n(depends on MXNet 1.3.0b20180725)\r\n\r\nModels\r\n======\r\n\r\n- **Language Models**\r\n  - The [Cache Language Model](https://gluon-nlp.mxnet.io/api/model.train.html#gluonnlp.model.train.CacheCell) as introduced by *Grave, E., et al. \u201cImproving neural language models with a continuous cache\u201d. ICLR 2017* is introduced as part of gluonnlp.model.train (#110)\r\n  - The [Activation Regularizer and Temporal Activation Regularizer](https://gluon-nlp.mxnet.io/api/loss.html#activation-regularizers) as introduced by *Merity, S., et al. \"Regularizing and optimizing LSTM language models\". ICLR 2018* is introduced as part of gluonnlp.loss (#110)\r\n- **Machine Translation**\r\n  - The [Transformer Model](https://gluon-nlp.mxnet.io/api/scripts/index.html#machine-translation) as introduced by Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems. 2017* is introduced as part of the gluonnlp nmt scripts (#133)\r\n- **Word embeddings**\r\n  - Trainable word embedding models are introduced as part of gluonnlp.model.train (#136)\r\n    - Word2Vec by *Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119).*\r\n    - FastText models by *Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching Word Vectors with Subword Information. Transactions of the Association for Computational Linguistics, 5, 135-146.*\r\n\r\n\r\nNew Datasets\r\n============\r\n- **Machine Translation**\r\n  - [WMT2014BPE](https://gluon-nlp.mxnet.io/api/data.html#gluonnlp.data.WMT2016BPE) (#135) (#177) (#180)\r\n- **Question Answering**\r\n  - [Stanford Question Answering Dataset (SQuAD)](https://gluon-nlp.mxnet.io/api/data.html#gluonnlp.data.SQuAD) *Rajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016). SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (pp. 2383-2392).* (#113)\r\n- **Word Embeddings**\r\n  - [Text8](https://gluon-nlp.mxnet.io/api/data.html#gluonnlp.data.Text8) (#136)\r\n\r\nAPI changes\r\n===========\r\n\r\n- The download directory for datasets and other artifacts can now be specified\r\n  via the MXNET_HOME environment variable. (#106)\r\n- TokenEmbedding class now exposes the Inverse Vocab as well (#123)\r\n- SortedSampler now supports use_average_length option (#135)\r\n- Add more strategies for bucket creation (#145)\r\n- Add tokenizer to bleu (#154)\r\n- Add Convolutional Encoder and Highway Layer (#129) (#186)\r\n- Add plain text of translation data. (#158)\r\n- Use Sherlock Holmes dataset instead of PTB for language model notebook (#174)\r\n- Add classes JiebaToknizer and NLTKStanfordSegmenter for Chinese Word Segmentation (#164)\r\n- Allow toggling output and prompt in documentation website (#184)\r\n- Add shape assertion statements for better user experience to some attention cells (#201)\r\n- Add support for computation of word embeddings for unknown words in `TokenEmbedding` class (#185) \r\n- Distribute subword vectors for pretrained fastText embeddings enabling embeddings for unknown words (#185) \r\n\r\nFixes & Small Changes\r\n=================\r\n- fixed bptt_batchify sometimes returned an invalid last batch (#120) \r\n- Fixed wrong PPL calculation in word language model script for multi-GPU (#150)\r\n- Fix split compound words and wmt16 results (#151)\r\n- Adapt pretrained word embeddings example notebook for nd.topk change in mxnet 1.3 (#153)\r\n- Fix beam search script (#175) \r\n- Fix small bugs in parser (#183) \r\n- TokenEmbedding: Skip lines with invalid bytes instead of crashing (#188)\r\n- Fix overly large memory use in TokenEmbedding serialization/deserialization if some tokens are overly large (eg. 50k characters) (#187) \r\n- Remove duplicates in WordSim353 when combining segments (#192)\r\n\r\n\r\n[See all commits](https://github.com/dmlc/gluon-nlp/compare/v0.2.0...v0.3.0)",
        "dateCreated": "2018-07-24T18:18:53Z",
        "datePublished": "2018-06-13T05:26:53Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.3.3",
        "name": "v0.3.3",
        "tag_name": "v0.3.3",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.3.3",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/11453566",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.3.3"
      },
      {
        "authorType": "User",
        "author_name": "leezu",
        "body": "Features\r\n========\r\n\r\nGluonNLP provides its users with easy access to\r\n\r\n- State of the art models\r\n- Pre-trained word embeddings\r\n- Many public datasets for different tasks\r\n- Examples friendly to users that are new to the task\r\n- Reproducible training scripts\r\n\r\nModels\r\n------\r\n\r\nGluon NLP Toolkit supplies model definitions for common NLP tasks. These can be\r\nadapted for the users requirements or taken as blueprint for new developments.\r\nAll of these are implemented using [Gluon Blocks](https://mxnet.incubator.apache.org/gluon/index.html) \r\nallowing easy reuse as plug-and-play neural network building blocks.\r\n\r\n- **Language Models**\r\n  - [Standard RNN language model](https://gluon-nlp.mxnet.io/api/model.html#gluonnlp.model.StandardRNN)\r\n  - [AWD language model by salesforce](https://gluon-nlp.mxnet.io/api/model.html#gluonnlp.model.AWDRNN)\r\n- **Attention Cells**\r\n  - [Multi-head attention cell](https://gluon-nlp.mxnet.io/api/model.html#gluonnlp.model.MultiHeadAttentionCell)\r\n  - [MLP attention cell](https://gluon-nlp.mxnet.io/api/model.html#gluonnlp.model.MLPAttentionCell)\r\n  - [Dot-product attention cell](https://gluon-nlp.mxnet.io/api/model.html#gluonnlp.model.DotProductAttentionCell)\r\n- **Beam Search**\r\n  - [Beam Search Sampler](https://gluon-nlp.mxnet.io/api/model.html#gluonnlp.model.BeamSearchSampler)\r\n  - [Beam Search Scorer](https://gluon-nlp.mxnet.io/api/model.html#gluonnlp.model.BeamSearchScorer)\r\n\r\n\r\nData\r\n----\r\n\r\nGluon NLP Toolkit provides tools for building efficient data pipelines for NLP\r\ntasks by defining a Dataset class interface and utilities for transforming them.\r\nSeveral datasets are included by default and will be automatically downloaded\r\nwhen used.\r\n\r\n- **Language modeling** with WikiText\r\n  - WikiText is a popular language modeling dataset from Salesforce. It is a\r\n    collection of over 100 million tokens extracted from the set of verified\r\n    Good and Featured articles on Wikipedia.\r\n- **Sentiment Analysis** with IMDB\r\n  - IMDB: IMDB is a popular dataset for binary sentiment classification. It\r\n    provides a set of 25,000 highly polar movie reviews for training, 25,000 for\r\n    testing, and additional unlabeled data.\r\n- **CoNLL** datasets\r\n  - These datasets include data for the shared tasks, such as part-of-speech\r\n    (POS) tagging, chunking, named entity recognition (NER), semantic role\r\n    labeling (SRL), etc.\r\n  - We provide built in support for CoNLL 2000 \u2013 2002, 2004, as well as the\r\n    Universal Dependencies dataset which is used in the 2017 and 2018\r\n    competitions.\r\n- **Word embedding evaluation** datasets\r\n  - There are a number of commonly used datasets for intrinsic evaluation for\r\n    word embeddings. We provide commonly used datasets for the similarity and\r\n    analogy evaluation tasks.\r\n\r\nGluon NLP further ships with common datasets data transformation functions,\r\ndataset samplers to determine how to iterate through datasets as well as\r\nfunctions to generate data batches.\r\n\r\n[A complete and up-to-date list of supplied datasets and utilities is available\r\nin the API documentation](https://gluon-nlp.mxnet.io/api/data.html).\r\n\r\nOther features\r\n--------------\r\n- [Vocabulary](https://gluon-nlp.mxnet.io/api/vocab.html)\r\n- [Pre-trained embeddings](https://gluon-nlp.mxnet.io/api/embedding.html)\r\n\r\nExamples and scripts\r\n--------------------\r\n\r\n[The Gluon NLP toolkit also provides scripts that use the functionality of the\r\ntoolkit for various tasks](https://gluon-nlp.mxnet.io/scripts/index.html)\r\n\r\n- Word Embedding Evaluation\r\n- Beam Search Generator\r\n- Word language modeling\r\n- Sentiment Analysis through Fine-tuning, w/ Bucketing\r\n- Machine Translation\r\n\r\n",
        "dateCreated": "2018-04-25T03:41:35Z",
        "datePublished": "2018-05-04T18:24:58Z",
        "html_url": "https://github.com/dmlc/gluon-nlp/releases/tag/v0.2.0",
        "name": "",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/dmlc/gluon-nlp/tarball/v0.2.0",
        "url": "https://api.github.com/repos/dmlc/gluon-nlp/releases/10858527",
        "zipball_url": "https://api.github.com/repos/dmlc/gluon-nlp/zipball/v0.2.0"
      }
    ],
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You may go to [tests](tests) to see how to run the unittests.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2349,
      "date": "Wed, 29 Dec 2021 09:17:56 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "mxnet",
      "gluonnlp",
      "nlp",
      "gluon",
      "deep-learning",
      "machine-learning",
      "natural-language-processing",
      "numpy",
      "nlu",
      "natural-language-understanding",
      "nlg",
      "natural-language-generation",
      "natural-language-inference"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can use Docker to launch a JupyterLab development environment with GluonNLP installed.\n\n```\n#: GPU Instance\ndocker pull gluonai/gluon-nlp:gpu-latest\ndocker run --gpus all --rm -it -p 8888:8888 -p 8787:8787 -p 8786:8786 --shm-size=2g gluonai/gluon-nlp:gpu-latest\n\n#: CPU Instance\ndocker pull gluonai/gluon-nlp:cpu-latest\ndocker run --rm -it -p 8888:8888 -p 8787:8787 -p 8786:8786 --shm-size=2g gluonai/gluon-nlp:cpu-latest\n``` \n\nFor more details, you can refer to the guidance in [tools/docker](tools/docker).\n",
      "technique": "Header extraction"
    }
  ]
}