{
  "citation": [
    {
      "confidence": [
        0.9818894004866677
      ],
      "excerpt": "Author: Laura H. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108221188045336
      ],
      "excerpt": "machine learning and natural language processing tasks.[2] The term word em- \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9649288001163268
      ],
      "excerpt": "title/ID of article the sentence belongs to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9388293684180229
      ],
      "excerpt": "    from language corpora contain human-like biases,\u201d in Science, vol. 356, pp. 183\u2013186. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9561938791452179,
        0.8422862053358879
      ],
      "excerpt": "    Advances in Neural Information Processing Systems 29, D. D. Lee, M. Sugiyama, \n    U. V. Luxburg, I. Guyon, and R. Garnett, Eds. Curran Associates, Inc., pp. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9666675638198374
      ],
      "excerpt": "5. T. Mikolov, K. Chen, G. Corrado, and J. Dean, \u201cEfficient estimation of word \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BaconCookie/PlagiarismDetection",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-23T14:43:36Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-22T08:53:53Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.974951672042779
      ],
      "excerpt": "An explorative / examplary implementation of a plagiarism detection using the word2vec implementation of spaCy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.907066301745948,
        0.9353074320189483,
        0.966002800107032,
        0.8165221771472836,
        0.8867917402524567,
        0.9022803162580649,
        0.8993282668885418,
        0.8813156596349904
      ],
      "excerpt": "Word2Vec utilizes vector representations of words, \"word embeddings\". \nWord embedding is a popular framework to represent each word as a vector in \na vector space of many (for example 300) dimensions, based on the semantic \ncontext in which the word is found.[1] This technique has been used in many \nmachine learning and natural language processing tasks.[2] The term word em- \nbedding is being used to describe multiple word vectors in a data set as well as \nfor a single word vector in such a word embedding. \nImagining such high-dimensional vectors and word embeddings is challeng- \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9371900740282094,
        0.896703934924298,
        0.9111542662431249
      ],
      "excerpt": "interactive visualization of high-dimensional data by rendering them in two or \nthree dimensions. The example in Figure 1 is based on input data from Word2Vec \nAll, which means this word embedding consists of roughly 70.000 vectors with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9774975318159508,
        0.9247434075112909
      ],
      "excerpt": "the collection of models in the Embedding Projector by Google. To reduce the \ndimensionality of this data set, the t-SNE (T-distributed stochastic neighbor \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9814508583405828,
        0.8087331337248408
      ],
      "excerpt": "tries to preserve local neighborhoods in the data, meaning that vectors of words \nwith similar semantics are being projected in close approximation.[4] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9792941598221547
      ],
      "excerpt": "the computer vector in the original space are highlighted in the graph and listed on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.86772255856194
      ],
      "excerpt": "As we can see in Figure 1, words which convey the same meaning / are about the same \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908986614452937,
        0.9494902008118046
      ],
      "excerpt": "by calculating the cosine similarity (direction) of these vectors.  \nA cosine similarity of 1 means it is the same word. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9230461061021119,
        0.9122817512620413,
        0.8748721083139436
      ],
      "excerpt": "Yet, plagiarisms are more about sentences and paragraphs rather than words. \nTherefore, we should rather calculate the mean vectors of sentences and compare these. \nDuring explorative testing I noticed that a cosine similarity > 0.9 is rather reliable  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9825189618334049
      ],
      "excerpt": "A table in the Cassandra database with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016,
        0.9933670504615798
      ],
      "excerpt": "Possibly TF-IDF for speedup \nSplit the text which is to analyse (TextToAnalyse) into sentences  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9770817035708729
      ],
      "excerpt": "Compare the TextToAnalyse sentences with all the sentences from Wiki (see sentence table as described above) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8871749851503786
      ],
      "excerpt": "(see vec.similarity(\"word1\",\"word2\") in dl4j Word2Vec Tutorial) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8648298943622836
      ],
      "excerpt": "There is no need to implement Word2Vec and the necessary functions from scratch, since  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9168448653822624
      ],
      "excerpt": "I found that the idea worked and that the quality of the Word2Vec models used made quite  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8407081577257538,
        0.934531985170486,
        0.8125095519067613
      ],
      "excerpt": "models to see what works best. \nFor Java/Scala, there is a library deeplearning4j (dl4j) and nd4j  \n(dl4j is based on nd4j and there are some functions you might need from here). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9814124935988958,
        0.9511418986997683
      ],
      "excerpt": "since the dependencies of dl4j and nd4j are not compatible with the current project (sbt). \nComparing all sentences from the text to analyse with all sentences in the wiki database requires  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9594810544180647
      ],
      "excerpt": "Begin with calculating the inverse document frequency: how often is a word used in the whole dataset? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9825164610425159
      ],
      "excerpt": "(For the table with all wikipedia articles (~2 Milion), the spark memory needs to be adjusted or the table  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9736856509205459,
        0.9764885427217085,
        0.91204106957346,
        0.9775539260210144
      ],
      "excerpt": "Then, for each new text to analyse, count the frequency of each word in the text to analyse. \nThe words with are rare in the complete dataset, but are more frequently used in the text to analyse,  \nwill very likely reveal the subject of the text to analyse. Each word gets a TF-IDF score. \nFilter the wiki data for articles about these subjects.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9539860566960954,
        0.9846475592643649
      ],
      "excerpt": "of words with the highest TF-IDF scores,  \nand continue with step 1 of the algorithm as described above. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8479066148594504,
        0.9983877099508093,
        0.9160580892619492
      ],
      "excerpt": "    from language corpora contain human-like biases,\u201d in Science, vol. 356, pp. 183\u2013186. \n2. T. Bolukbasi, K.-W. Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai, \u201cMan is to \n    computer programmer as woman is to homemaker? debiasing word embeddings,\u201d in5. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9177573181742293
      ],
      "excerpt": "   visualization of high-dimensional data. Online, Available: http://projector.tensorflow.org \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8781587138813978
      ],
      "excerpt": "5. T. Mikolov, K. Chen, G. Corrado, and J. Dean, \u201cEfficient estimation of word \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "An explorative / examplary implementation of a plagiarism detection using the word2vec implementation of spaCy",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BaconCookie/PlagiarismDetection/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 15:53:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/BaconCookie/PlagiarismDetection/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "BaconCookie/PlagiarismDetection",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8005493794581892
      ],
      "excerpt": "Compare the TextToAnalyse sentences with all the sentences from Wiki (see sentence table as described above) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8452981288204973
      ],
      "excerpt": "We can do that by using TF-IDF  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8222916621553601
      ],
      "excerpt": "a difference in outcome for, for example, th similarity function.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.817315971367293
      ],
      "excerpt": "will very likely reveal the subject of the text to analyse. Each word gets a TF-IDF score. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/BaconCookie/PlagiarismDetection/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "PlagiarismDetection with a WORD2VEC implementation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PlagiarismDetection",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "BaconCookie",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BaconCookie/PlagiarismDetection/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Sat, 25 Dec 2021 15:53:06 GMT"
    },
    "technique": "GitHub API"
  }
}