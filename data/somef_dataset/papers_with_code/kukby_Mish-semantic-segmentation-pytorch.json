{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1612.01105](https://arxiv.org/abs/1612.01105",
      "https://arxiv.org/abs/1612.01105",
      "https://arxiv.org/abs/1807.10221](https://arxiv.org/abs/1807.10221",
      "https://arxiv.org/abs/1807.10221",
      "https://arxiv.org/abs/1904.04514](https://arxiv.org/abs/1904.04514",
      "https://arxiv.org/abs/1904.04514",
      "https://arxiv.org/abs/1807.10221",
      "https://arxiv.org/abs/1908.08681, 2019.(https://github.com/digantamisra98/Mish)\r\n\r\n```\r\n@misc{misra2019mish,\r\n    title={Mish: A Self Regularized Non-Monotonic Neural Activation Function"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nIf you find the code or pre-trained models useful, please cite the following papers:\r\n\r\nSemantic Understanding of Scenes through ADE20K Dataset. B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso and A. Torralba. International Journal on Computer Vision (IJCV), 2018. (https://arxiv.org/pdf/1608.05442.pdf)\r\n\r\n    @article{zhou2018semantic,\r\n      title={Semantic understanding of scenes through the ade20k dataset},\r\n      author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Xiao, Tete and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},\r\n      journal={International Journal on Computer Vision},\r\n      year={2018}\r\n    }\r\n\r\nScene Parsing through ADE20K Dataset. B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso and A. Torralba. Computer Vision and Pattern Recognition (CVPR), 2017. (http://people.csail.mit.edu/bzhou/publication/scene-parse-camera-ready.pdf)\r\n\r\n    @inproceedings{zhou2017scene,\r\n        title={Scene Parsing through ADE20K Dataset},\r\n        author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},\r\n        booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\r\n        year={2017}\r\n    }\r\n\r\nMisra D. Mish: A Self Regularized Non-Monotonic Neural Activation Function[J]. arXiv preprint arXiv:1908.08681, 2019.(https://github.com/digantamisra98/Mish)\r\n\r\n```\r\n@misc{misra2019mish,\r\n    title={Mish: A Self Regularized Non-Monotonic Neural Activation Function},\r\n    author={Diganta Misra},\r\n    year={2019},\r\n    eprint={1908.08681},\r\n    archivePrefix={arXiv},\r\n    primaryClass={cs.LG}\r\n}\r\n```\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{misra2019mish,\n    title={Mish: A Self Regularized Non-Monotonic Neural Activation Function},\n    author={Diganta Misra},\n    year={2019},\n    eprint={1908.08681},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{zhou2017scene,\n    title={Scene Parsing through ADE20K Dataset},\n    author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},\n    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n    year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhou2018semantic,\n  title={Semantic understanding of scenes through the ade20k dataset},\n  author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Xiao, Tete and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},\n  journal={International Journal on Computer Vision},\n  year={2018}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kukby/Mish-semantic-segmentation-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-14T12:23:05Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-28T06:33:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8466946815449463,
        0.9249784267634462
      ],
      "excerpt": "This is a PyTorch implementation of semantic segmentation models on MIT ADE20K scene parsing dataset. \nIn 2019, Google proposed a new activation function-Mish for the machine learning. The discovery of this function makes it possible to replace the commonly used ReLU function in semantic segmentation tasks. We use MIT's ADE20k data set for experiments, and write the semantic segmentation network models according to MIT's GitHub project https://github.com/csailvision/semantic-segmentation-python. At the same time we mainly modify the basement of the semantic segmentation network models such as the ResNet, MobilNet and HRNet because these three networks was generally been used in the machine learning of the semantic segmentation.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9062051288653844,
        0.9076674626918839
      ],
      "excerpt": "The Mish function is created for the task of the image classification. In this task it got the Top-1 Accuracy while for Generative Networks and Image Segmentation the Loss Metric. Therefore, we considered that if we use this function in the Semantic Segmentation task, we will get such a good result. \nIMPORTANT: The base ResNet in our repository is a customized (different from the one in torchvision). The base models will be automatically downloaded when needed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9307884222426569
      ],
      "excerpt": "The training is benchmarked on a server with 4 NVIDIA Tesla V100 GPUs (16GB GPU memory). The inference speed is benchmarked a single NVIDIA Pascal 1080ti GPU, with visualization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8968481219178137,
        0.8635956400593181
      ],
      "excerpt": "Mish is a Self Regularized Non-Monotonic Neural Activation Function. Activation Function serves a core functionality in the training process of a Neural Network Architecture and is represented by the basic mathematical representation:. \nFor the task of semantic segmentation. we usually found that many activation functions in the machine learning have been constructed with the  most popular amongst them being ReLU(Rectified Linear Unit;f(x)=max(0,x)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9657177378937875,
        0.9570537275585462,
        0.9296261312196197,
        0.9808687115421855,
        0.9769865190111006,
        0.8962519581911883
      ],
      "excerpt": "For the task of semantic segmentation, it is good to keep aspect ratio of images during training. So we re-implement the DataParallel module, and make it support distributing data to multiple GPUs in python dict, so that each gpu can process images of different sizes. At the same time, the dataloader also operates differently.  \n<sup>Now the batch size of a dataloader always equals to the number of GPUs, each element will be sent to a GPU. It is also compatible with multi-processing. Note that the file index for the multi-processing dataloader is stored on the master process, which is in contradict to our goal that each worker maintains its own file list. So we use a trick that although the master process still gives dataloader an index for __getitem__ function, we just ignore such request and send a random batch dict. Also, the multiple workers forked by the dataloader all have the same seed, you will find that multiple workers will yield exactly the same data, if we use the above-mentioned trick directly. Therefore, we add one line of code which sets the defaut seed for numpy.random before activating multiple worker in dataloader.</sup> \nPSPNet is scene parsing network that aggregates global representation with Pyramid Pooling Module (PPM). It is the winner model of ILSVRC'16 MIT Scene Parsing Challenge. Please refer to https://arxiv.org/abs/1612.01105 for details. \nUPerNet is a model based on Feature Pyramid Network (FPN) and Pyramid Pooling Module (PPM). It doesn't need dilated convolution, an operator that is time-and-memory consuming. Without bells and whistles, it is comparable or even better compared with PSPNet, while requiring much shorter training time and less GPU memory. Please refer to https://arxiv.org/abs/1807.10221 for details. \nHRNet is a recently proposed model that retains high resolution representations throughout the model, without the traditional bottleneck design. It achieves the SOTA performance on a series of pixel labeling tasks. Please refer to https://arxiv.org/abs/1904.04514 for details. \nWe split our models into encoder and decoder, where encoders are usually modified directly from classification networks, and decoders consist of final convolutions and upsampling. We have provided some pre-configured models in the config folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8962519581911883
      ],
      "excerpt": "We split our models into encoder and decoder, where encoders are usually modified directly from classification networks, and decoders consist of final convolutions and upsampling. We have provided some pre-configured models in the config folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8895307568905328
      ],
      "excerpt": "PPM (Pyramid Pooling Module, see PSPNet paper for details.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665325938766169,
        0.8588032913438295
      ],
      "excerpt": "UPerNet (Pyramid Pooling + FPN head, see UperNet for details.) \nThe code is developed under the following configurations. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kukby/Mish-semantic-segmentation-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 13:19:15 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kukby/Mish-semantic-segmentation-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kukby/Mish-semantic-segmentation-pytorch",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/kukby/Mish-semantic-segmentation-pytorch/master/demo_test.sh",
      "https://raw.githubusercontent.com/kukby/Mish-semantic-segmentation-pytorch/master/download_ADE20K.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.895313969889522,
        0.9760137078151593
      ],
      "excerpt": "Software: Ubuntu 18.04.3 LTS, CUDA>=8.0, Python>=3.5, PyTorch>=1.0.0 \nDependencies: numpy, scipy, opencv, yacs, tqdm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8468902673898102
      ],
      "excerpt": "chmod +x download_ADE20K.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8187378156987922
      ],
      "excerpt": "\u200b       For example, you can start with our provided configurations:  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8127787733679802
      ],
      "excerpt": "Hardware: >=4 GPUs for training, >=1 GPU for testing (set [--gpus GPUS] accordingly) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382411321719964
      ],
      "excerpt": "Train a model by selecting the GPUs ($GPUS) and configuration file ($CFG) to use. During training, checkpoints by default are saved in folder ckpt. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8849067143471592
      ],
      "excerpt": "python3 train.py --gpus $GPUS --cfg $CFG \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "Train MobileNetV2dilated + C1_deepsup \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8761935741854626,
        0.8589534893990137
      ],
      "excerpt": "python3 train.py --gpus GPUS --cfg config/ade20k-mobilenetv2dilated-c1_deepsup.yaml \nTrain ResNet50dilated + PPM_deepsup \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8761935741854626,
        0.8589534893990137
      ],
      "excerpt": "python3 train.py --gpus GPUS --cfg config/ade20k-resnet50dilated-ppm_deepsup.yaml \nTrain UPerNet101 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8761935741854626,
        0.9165849299223386,
        0.8702104592069103,
        0.8216270093103228
      ],
      "excerpt": "python3 train.py --gpus GPUS --cfg config/ade20k-resnet101-upernet.yaml \nYou can also override options in commandline, for example  python3 train.py TRAIN.num_epoch 10. \nEvaluate a trained model on the validation set. Add VAL.visualize True in argument to output visualizations as shown in teaser. \nFor example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8338227104242915
      ],
      "excerpt": "python3 eval_multipro.py --gpus GPUS --cfg config/ade20k-mobilenetv2dilated-c1_deepsup.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8338227104242915
      ],
      "excerpt": "python3 eval_multipro.py --gpus GPUS --cfg config/ade20k-resnet50dilated-ppm_deepsup.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8338227104242915
      ],
      "excerpt": "python3 eval_multipro.py --gpus GPUS --cfg config/ade20k-resnet101-upernet.yaml \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kukby/Mish-semantic-segmentation-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2019, MIT CSAIL Computer Vision\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Mish-Semantic Segmentation on MIT ADE20K dataset in PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Mish-semantic-segmentation-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kukby",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kukby/Mish-semantic-segmentation-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Wed, 29 Dec 2021 13:19:15 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n1. Here is a simple demo to do inference on a single image:\r\n\r\n```bash\r\nchmod +x demo_test.sh\r\n./demo_test.sh\r\n```\r\n\r\nThis script downloads a trained model (ResNet50dilated + PPM_deepsup) and a test image, runs the test script, and saves predicted segmentation (.png) to the working directory.\r\n\r\n2. To test on an image or a folder of images (```$PATH_IMG```), you can simply do the following:\r\n\r\n```\r\npython3 -u test.py --imgs $PATH_IMG --gpu $GPU --cfg $CFG\r\n```\r\n\r\n",
      "technique": "Header extraction"
    }
  ]
}