{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2101.11075\">Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization</a> (Aaron Defazio and Samy Jelassi, 2021"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```citation\n@misc{defazio2021adaptivity,\n      title={Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization}, \n      author={Aaron Defazio and Samy Jelassi},\n      year={2021},\n      eprint={2101.11075},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n\n\n\n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n[contributors-shield]: https://img.shields.io/badge/CONTRIBUTORS-1-orange?style=for-the-badge\n[contributors-url]: https://github.com/othneildrew/Best-README-Template/graphs/contributors\n[license-shield]: https://img.shields.io/badge/LICENSE-MIT-brightgreen?style=for-the-badge\n[license-url]: https://github.com/DarshanDeshpande/tf-madgrad/blob/master/LICENSE.txt\n[version-shield]: https://img.shields.io/badge/VERSION-1.0.0-orange?style=for-the-badge\n[python-shield]: https://img.shields.io/badge/PYTHON-3.6%7C3.7%7C3.8-blue?style=for-the-badge\n[release-shield]: https://img.shields.io/badge/Build-Stable-yellow?style=for-the-badge\n[code-style]: https://img.shields.io/badge/Code_Style-Black-black?style=for-the-badge\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{defazio2021adaptivity,\n      title={Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization}, \n      author={Aaron Defazio and Samy Jelassi},\n      year={2021},\n      eprint={2101.11075},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DarshanDeshpande/tf-madgrad",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Feel free to reach out for any issues or requests related to this implementation\n\nDarshan Deshpande - [Email](https://mail.google.com/mail/u/0/?view=cm&fs=1&to=darshan1504@gmail.com&tf=1) | [LinkedIn](https://www.linkedin.com/in/darshan-deshpande/)\n\n\n\n<!-- ACKNOWLEDGEMENTS -->\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-21T18:59:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-20T06:41:17Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9857799562082408
      ],
      "excerpt": "The MadGrad algorithm of optimization uses Dual averaging of gradients along with momentum based adaptivity to attain results that match or outperform Adam or SGD + momentum based algorithms. This project offers a Tensorflow implementation of the algorithm along with a few usage examples and tests. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A tf.keras implementation of Facebook AI's MadGrad optimization algorithm",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DarshanDeshpande/tf-madgrad/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Sat, 25 Dec 2021 11:26:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DarshanDeshpande/tf-madgrad/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "DarshanDeshpande/tf-madgrad",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project is built with Python 3 and can be `pip` installed directly\n\n```sh\npip install tf-madgrad\n```\n\n<!-- USAGE EXAMPLES -->\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8296138449975318
      ],
      "excerpt": "<img src=\"https://i.imgur.com/czLMClK.jpg\" height=\"50%\" width=\"90%\"></img> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DarshanDeshpande/tf-madgrad/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Darshan Deshpande\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "about-the-project\">About The Project</a>",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "tf-madgrad",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "DarshanDeshpande",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DarshanDeshpande/tf-madgrad/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Prerequisites can be installed separately through the `requirements.txt` file as below\n\n```sh\npip install -r requirements.txt\n```\n\n\n \n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 17,
      "date": "Sat, 25 Dec 2021 11:26:59 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "optimization-algorithms",
      "tensorflow",
      "facebookai"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Tq6mH4ULsj7PzuOuMN13lOxSr_IbXgYq?usp=sharing)\n\nTo use the optimizer in any tf.keras model, you just need to import and instantiate the ```MadGrad``` optimizer from the `tf_madgrad` package.\n```python\nfrom madgrad import MadGrad\n\n#: Create the architecture\ninp = tf.keras.layers.Input(shape=shape)\n...\nop = tf.keras.layers.Dense(classes, activation=activation)\n\n#: Instantiate the model\nmodel = tf.keras.models.Model(inp, op)\n\n#: Pass the MadGrad optimizer to the compile function\nmodel.compile(optimizer=MadGrad(lr=0.01), loss=loss)\n\n#: Fit the keras model as normal\nmodel.fit(...)\n```\nThis implementation is also supported for distributed training using ```tf.strategy```\n\nSee a MNIST example <a href=\"https://github.com/DarshanDeshpande/tf-madgrad/blob/master/examples/mnist_example.py\">here</a> \n\n<!-- CONTRIBUTING -->\n",
      "technique": "Header extraction"
    }
  ]
}