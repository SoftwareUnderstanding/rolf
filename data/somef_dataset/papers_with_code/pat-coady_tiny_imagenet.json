{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pat-coady/tiny_imagenet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-05-10T15:38:51Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-13T05:42:32Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[ImageNet](http://www.image-net.org/) and Alex Krizhevsky's [\"AlexNet\"](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks) sparked a revolution in machine learning. AlexNet marked the end of the era of mostly hand-crafted features for visual recognition problems. In just the few years that followed AlexNet, \"deep learning\" found great success in natural language processing, speech recognition, and reinforcement learning.\n\nAny aspiring machine learning engineer should construct and train a deep convnet \"from scratch.\"  Of course, there are varying degrees of \"from scratch.\" I had already implemented many of the neural network primitives using NumPy (e.g. fully connected layers, cross-entropy loss, batch normalization, LSTM / GRU cells, and convolutional layers). So, here I use TensorFlow so the focus is on training a deep network on a large dataset.\n\nAmazingly, with only 2 hours of GPU time (about $0.50 using an Amazon EC2 spot instance), it was not difficult to reach 50% top-1 accuracy and almost 80% top-5 accuracy. At this accuracy, I was also making mistakes on the images that the model got wrong (and I even made mistakes on some that it got correct).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8053198613912965,
        0.9178362217428556
      ],
      "excerpt": "The images are down-sampled to 64x64 pixels vs. 256x256 for the original ImageNet. The full ImageNet dataset also has 1000 classes.  \nTiny ImageNet is large enough to be a challenging and realistic problem. But not so big as to require days of training before you see results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9791210897113218
      ],
      "excerpt": "Implement saliency (i.e. Where in the image is the model focused?) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.968691966009016
      ],
      "excerpt": "For more details, see my blog: Learning Artificial Intelligence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.866248656728121
      ],
      "excerpt": "Another simple baseline. A neural net with a single hidden layer: 1024 hidden units with ReLU activations. Reaches about 8% accuracy with minimal tuning effort. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9807105077686499
      ],
      "excerpt": "This paper by Karen Simonyan and Andrew Zisserman introduced the VGG-16 architecture. The authors reached state-of-the-art performance using only a deep stack of 3x3xC filters and max-pooling layers. Because Tiny ImageNet has much lower resolution than the original ImageNet data, I removed the last max-pool layer and the last three convolution layers. With a little tuning, this model reaches 52% top-1 accuracy and 77% top-5 accuracy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9351609551523384,
        0.897707707888433
      ],
      "excerpt": "Trains models and monitors validation accuracy. The training loop has learning rate control and terminates training when progress stops. I take full advantage of TensorBoard by saving histograms of all weights, activations, and also learning curves. \nTraining is built to run fast on GPU by running the data pipeline on the CPU and model training on the GPU. It is straightforward to train a different model by changing 'model_name' in TrainConfig class. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8790316167637482
      ],
      "excerpt": "Smoothed cross-entropy loss (add small, non-zero, probability to all classes) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9602367918086961
      ],
      "excerpt": "This short notebook randomly selects ten images from the validation set and displays the top-5 predictions vs. the \"gold\" label. The notebook also displays saliency maps next to each image so you can see where the model is \"looking\" as it makes decisions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8512109387544236
      ],
      "excerpt": "Visualize input kernels (aka filters) of first two conv layers. The receptive field is only 7x7 after two 3x3 layers, but the results are still interesting. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9726228138916391
      ],
      "excerpt": "This notebook loads a model and calculates the validation set accuracy. It also computes the accuracy when predictions from 5 different crops x 2 flips are averaged: about a 3% accuracy improvement. This notebook runs slowly because it loops through the validation images one-by-one: It was not worth the extra effort to write efficiently. Premature optimization is the root of all evil. -Donald Knuth \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "TensorFlow Deep CNN for Tiny ImageNet Problem",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pat-coady/tiny_imagenet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 18,
      "date": "Sun, 26 Dec 2021 05:51:46 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pat-coady/tiny_imagenet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pat-coady/tiny_imagenet",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pat-coady/tiny_imagenet/master/src/image_distort.ipynb",
      "https://raw.githubusercontent.com/pat-coady/tiny_imagenet/master/src/val_accuracy.ipynb",
      "https://raw.githubusercontent.com/pat-coady/tiny_imagenet/master/src/kernel_viz_conv4.ipynb",
      "https://raw.githubusercontent.com/pat-coady/tiny_imagenet/master/src/predict_and_saliency.ipynb",
      "https://raw.githubusercontent.com/pat-coady/tiny_imagenet/master/src/kernel_viz.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.959796155394199
      ],
      "excerpt": "QueueRunner to feed GPU \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8704294612002438
      ],
      "excerpt": "Stanford prepared the Tiny ImageNet dataset for their CS231n course. The dataset spans 200 image classes with 500 training examples per class. The dataset also has 50 validation and 50 test examples per class. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pat-coady/tiny_imagenet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 pat-coady\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Tiny ImageNet",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "tiny_imagenet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pat-coady",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pat-coady/tiny_imagenet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 52,
      "date": "Sun, 26 Dec 2021 05:51:46 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "deep-neural-networks",
      "convolutional-neural-networks",
      "imagenet",
      "machine-learning",
      "tensorflow"
    ],
    "technique": "GitHub API"
  }
}