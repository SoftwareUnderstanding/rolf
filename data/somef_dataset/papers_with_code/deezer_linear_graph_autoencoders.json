{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**1** - Please cite the following paper(s) if you use linear graph AE/VAE code in your own work.\n\nNeurIPS 2019 workshop version:\n\n```BibTeX\n@misc{salha2019keep,\n  title={Keep It Simple: Graph Autoencoders Without Graph Convolutional Networks},\n  author={Salha, Guillaume and Hennequin, Romain and Vazirgiannis, Michalis},\n  howpublished={Workshop on Graph Representation Learning, 33rd Conference on Neural Information Processing Systems (NeurIPS)},\n  year={2019}\n}\n```\n\nand/or the extended conference version:\n\n```BibTeX\n@inproceedings{salha2020simple,\n  title={Simple and Effective Graph Autoencoders with One-Hop Linear Models},\n  author={Salha, Guillaume and Hennequin, Romain and Vazirgiannis, Michalis},\n  booktitle={European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)},\n  year={2020}\n}\n```\n\n**2** - Please cite the following paper if you use the k-core framework for scalability in your own work.\n\n```BibTeX\n@inproceedings{salha2019degeneracy,\n  title={A Degeneracy Framework for Scalable Graph Autoencoders},\n  author={Salha, Guillaume and Hennequin, Romain and Tran, Viet Anh and Vazirgiannis, Michalis},\n  booktitle={28th International Joint Conference on Artificial Intelligence (IJCAI)},\n  year={2019}\n}\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{salha2019degeneracy,\n  title={A Degeneracy Framework for Scalable Graph Autoencoders},\n  author={Salha, Guillaume and Hennequin, Romain and Tran, Viet Anh and Vazirgiannis, Michalis},\n  booktitle={28th International Joint Conference on Artificial Intelligence (IJCAI)},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{salha2020simple,\n  title={Simple and Effective Graph Autoencoders with One-Hop Linear Models},\n  author={Salha, Guillaume and Hennequin, Romain and Vazirgiannis, Michalis},\n  booktitle={European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{salha2019keep,\n  title={Keep It Simple: Graph Autoencoders Without Graph Convolutional Networks},\n  author={Salha, Guillaume and Hennequin, Romain and Vazirgiannis, Michalis},\n  howpublished={Workshop on Graph Representation Learning, 33rd Conference on Neural Information Processing Systems (NeurIPS)},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9191213955345262
      ],
      "excerpt": "| task | string |Name of the Machine Learning evaluation task, among: <br> - link_prediction: Link Prediction <br> - node_clustering: Node Clustering <br> <br> See section 4 and supplementary material of NeurIPS 2019 workshop paper for details about tasks| link_prediction| \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deezer/linear_graph_autoencoders",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-02T10:08:31Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-25T02:36:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We release Tensorflow implementations of the following **two graph embedding models** from the paper:\n - Linear Graph Autoencoders\n - Linear Graph Variational Autoencoders\n\ntogether with standard Graph Autoencoders (AE) and Graph Variational Autoencoders (VAE) models (with 2-layer or 3-layer Graph Convolutional Networks encoders) from [Kipf and Welling (2016)](https://arxiv.org/pdf/1611.07308.pdf). \n\nWe evaluate all models on the **link prediction** and **node clustering** tasks introduced in the paper. We provide the **Cora**, **Citeseer** and **Pubmed** datasets in the `data` folder, and refer to section 4 of the paper for direct link to the additional datasets used in our experiments.\n\nOur code builds upon Thomas Kipf's [original Tensorflow implementation](https://github.com/tkipf/gae) of standard Graph AE/VAE.\n\n![Linear AE and VAE](figures/linearsummary.png)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8429500731043907,
        0.8384634706934768,
        0.9824433905556061,
        0.9914649685359456,
        0.9986846393297237
      ],
      "excerpt": "This repository provides Python (Tensorflow) code to reproduce experiments from the article Keep It Simple: Graph Autoencoders Without Graph Convolutional Networks presented at the NeurIPS 2019 Workshop on Graph Representation Learning.  \nUpdate*: an extended conference version of this article is now available here: Simple and Effective Graph Autoencoders with One-Hop Linear Models (accepted at ECML-PKDD 2020*). \nUpdate 2*: do you prefer PyTorch*? An implementation of Linear Graph AE and VAE is now available in the pytorch_geometric project! See the example here. \nStandard Graph AE and VAE models suffer from scalability issues. In order to scale them to large graphs with millions of nodes and egdes, we also provide an implementation of our framework from the article A Degeneracy Framework for Scalable Graph Autoencoders (IJCAI 2019). In this paper, we propose to train the graph AE/VAE only from a dense subset of nodes, namely the k-core or k-degenerate subgraph. Then, we propagate embedding representations to the remaining nodes using faster heuristics. \nUpdate*: in this other repository, we provide an implementation of FastGAE*, a new (and more effective) method from our group to scale Graph AE and VAE. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507585768547158
      ],
      "excerpt": "| model     | string | Name of the model, among:<br> - gcn_ae: Graph AE from Kipf and Welling (2016), with 2-layer GCN encoder and inner product decoder<br> - gcn_vae: Graph VAE from Kipf and Welling (2016), with Gaussian distributions, 2-layer GCN encoders for mu and sigma, and inner product decoder <br> - linear_ae: Linear Graph AE, as introduced in section 3 of NeurIPS workshop paper, with linear encoder, and inner product decoder <br> - linear_vae: Linear Graph VAE, as introduced in section 3 of NeurIPS workshop paper, with Gaussian distributions, linear encoders for mu and sigma, and inner product decoder <br> - deep_gcn_ae: Deeper version of Graph AE, with 3-layer GCN encoder, and inner product decoder <br> - deep_gcn_vae: Deeper version of Graph VAE, with Gaussian distributions, 3-layer GCN encoders for mu and sigma, and inner product decoder| gcn_ae | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8981278488684359
      ],
      "excerpt": "| task | string |Name of the Machine Learning evaluation task, among: <br> - link_prediction: Link Prediction <br> - node_clustering: Node Clustering <br> <br> See section 4 and supplementary material of NeurIPS 2019 workshop paper for details about tasks| link_prediction| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.911470751854319
      ],
      "excerpt": "| features| boolean | Whether to include node features in encoder | False | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8898758497629712
      ],
      "excerpt": "| prop_val| float | Proportion of edges in validation set (for Link Prediction) | 5. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.974571139173977
      ],
      "excerpt": "Cora - with features \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.974571139173977
      ],
      "excerpt": "Citeseer - with features \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.974571139173977
      ],
      "excerpt": "Pubmed - with features \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9677413102932843
      ],
      "excerpt": " - Set --task=node_clustering with same hyperparameters to evaluate models on node clustering (as in Table 4) instead of link prediction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Source code from the NeurIPS 2019 workshop article \"Keep It Simple: Graph Autoencoders Without Graph Convolutional Networks\" (G. Salha, R. Hennequin, M. Vazirgiannis) + k-core framework implementation from IJCAI 2019 article \"A Degeneracy Framework for Scalable Graph Autoencoders\" (G. Salha, R. Hennequin, V.A. Tran, M. Vazirgiannis)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deezer/linear_graph_autoencoders/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 15,
      "date": "Tue, 28 Dec 2021 06:40:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/deezer/linear_graph_autoencoders/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "deezer/linear_graph_autoencoders",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\npython setup.py install\n```\n\nRequirements: tensorflow (1.X), networkx, numpy, scikit-learn, scipy\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8605842440898621
      ],
      "excerpt": "| epoch| int | Number of epochs in model training | 200 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8366862911707245
      ],
      "excerpt": "| nb_run| integer | Number of model runs + tests | 1 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.80263904314244,
        0.887970848988341
      ],
      "excerpt": "| validation| boolean | Whether to report validation results  at each epoch (for Link Prediction) | False | \n| verbose| boolean | Whether to print full comments details | True | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9378931907624595,
        0.9378931907624595,
        0.9279632610924644,
        0.9279632610924644,
        0.9279632610924644,
        0.9279632610924644
      ],
      "excerpt": "python train.py --dataset=cora --model=linear_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --dimension=16 --nb_run=5 \npython train.py --dataset=cora --model=linear_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --dimension=16 --nb_run=5 \npython train.py --dataset=cora --model=gcn_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=cora --model=gcn_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=cora --model=deep_gcn_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=cora --model=deep_gcn_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9455007844260451,
        0.9455007844260451,
        0.9375114227177753,
        0.9375114227177753,
        0.9375114227177753,
        0.9375114227177753
      ],
      "excerpt": "python train.py --dataset=cora --features=True --model=linear_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --dimension=16 --nb_run=5 \npython train.py --dataset=cora --features=True --model=linear_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --dimension=16 --nb_run=5 \npython train.py --dataset=cora --features=True --model=gcn_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=cora --features=True --model=gcn_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=cora --features=True --model=deep_gcn_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=cora --features=True --model=deep_gcn_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9378931907624595,
        0.9378931907624595,
        0.9279632610924644,
        0.9279632610924644,
        0.9279632610924644,
        0.9279632610924644
      ],
      "excerpt": "python train.py --dataset=citeseer --model=linear_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --dimension=16 --nb_run=5 \npython train.py --dataset=citeseer --model=linear_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --dimension=16 --nb_run=5 \npython train.py --dataset=citeseer --model=gcn_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=citeseer --model=gcn_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=citeseer --model=deep_gcn_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=citeseer --model=deep_gcn_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9455007844260451,
        0.9455007844260451,
        0.9375114227177753,
        0.9375114227177753,
        0.9375114227177753,
        0.9375114227177753
      ],
      "excerpt": "python train.py --dataset=citeseer --features=True --model=linear_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --dimension=16 --nb_run=5 \npython train.py --dataset=citeseer --features=True --model=linear_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --dimension=16 --nb_run=5 \npython train.py --dataset=citeseer --features=True --model=gcn_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=citeseer --features=True --model=gcn_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=citeseer --features=True --model=deep_gcn_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=citeseer --features=True --model=deep_gcn_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9378931907624595,
        0.9378931907624595,
        0.9279632610924644,
        0.9279632610924644,
        0.9279632610924644,
        0.9279632610924644
      ],
      "excerpt": "python train.py --dataset=pubmed --model=linear_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --dimension=16 --nb_run=5 \npython train.py --dataset=pubmed --model=linear_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --dimension=16 --nb_run=5 \npython train.py --dataset=pubmed --model=gcn_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=pubmed --model=gcn_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=pubmed --model=deep_gcn_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=pubmed --model=deep_gcn_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9455007844260451,
        0.9455007844260451,
        0.9375114227177753,
        0.9375114227177753,
        0.9375114227177753,
        0.9375114227177753
      ],
      "excerpt": "python train.py --dataset=pubmed --features=True --model=linear_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --dimension=16 --nb_run=5 \npython train.py --dataset=pubmed --features=True --model=linear_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --dimension=16 --nb_run=5 \npython train.py --dataset=pubmed --features=True --model=gcn_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=pubmed --features=True --model=gcn_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=pubmed --features=True --model=deep_gcn_ae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \npython train.py --dataset=pubmed --features=True --model=deep_gcn_vae --task=link_prediction --epochs=200 --learning_rate=0.01 --hidden=32 --dimension=16 --nb_run=5 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/deezer/linear_graph_autoencoders/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Linear Graph Autoencoders",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "linear_graph_autoencoders",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "deezer",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deezer/linear_graph_autoencoders/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\ncd linear_gae\npython train.py --model=gcn_vae --dataset=cora --task=link_prediction\npython train.py --model=linear_vae --dataset=cora --task=link_prediction\n```\n\nThe above commands will train a *standard Graph VAE with 2-layer GCN encoders (line 2)* and a *Linear Graph VAE (line 3)* on *Cora dataset* and will evaluate embeddings on the *Link Prediction* task, with all parameters set to default values.\n\n```bash\npython train.py --model=gcn_vae --dataset=cora --task=link_prediction --kcore=True --k=2\npython train.py --model=gcn_vae --dataset=cora --task=link_prediction --kcore=True --k=3\npython train.py --model=gcn_vae --dataset=cora --task=link_prediction --kcore=True --k=4\n```\n\nBy adding `--kcore=True`, the model will only be trained on the k-core subgraph instead of using the entire graph. Here, k is a parameter (from 0 to the maximal core number of the graph) to specify using the `--k` flag.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 98,
      "date": "Tue, 28 Dec 2021 06:40:06 GMT"
    },
    "technique": "GitHub API"
  }
}