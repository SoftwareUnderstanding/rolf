{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Mikolov T, Chen K, Corrado G, Dean J. Efficient Estimation of Word Representations in Vector Space. January 2013. http://arxiv.org/abs/1301.3781.\n2. Mikolov T, Sutskever I, Chen K, Corrado G, Dean J. Distributed Representations of Words and Phrases and their Compositionality. October 2013. http://arxiv.org/abs/1310.4546.\n3. Qi Y, Oja M, Weston J, Noble WS. A unified multitask architecture for predicting local protein properties. PLoS One. 2012;7(3). doi:10.1371/journal.pone.0032235.\n4. Y. Kim, \u201cConvolutional Neural Networks for Sentence Classification,\u201d Proc. 2014 Conf. Empir. Methods Nat. Lang. Process., pp. 1746\u20131751, 2014.",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mari756h/The_unemployed_cells",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-30T06:54:56Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-02T07:35:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9976082906550755,
        0.9872018928130021,
        0.8613298262237992,
        0.9506599406934269,
        0.9578645441825773,
        0.9730060388404792
      ],
      "excerpt": "Language of life essentially refers all of the information that is encoded by our genetic sequences, and we propose to view these sequences as the sentences that makes up the vocabulary of life. The language of life is built by protein sequences, and the biological alphabet consists of 21 amino acids denoted by their designated letters. \nThis project aims to predict amino acids in a protein sequence, where two word embedding architectures are used: Continuous Bag-of-Words and  Skip-gram. The models are implemented in Pytorch and the archicture can be seen in the figure below. \nAll the results can be seen in this notebook. \nUpdate January 2019: This project has further been expanded to the use of convolutional neural networks (CNNs). The results of the CNNs are visualized in this notebook. \nThe CNN model architecture is as follows \nThe goal of CBoW is to predict a amino acid at a position t given the surrounding words (contexts), meaning that we try to optimize the probabilities of each amino acid at position t. [1] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8257720650987505,
        0.8754412061359714
      ],
      "excerpt": "- -window_size WINDOW_SIZE: Size of the context window. \n- -batch_size BATCH_SIZE: Size of neural network batches. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.940531700406483
      ],
      "excerpt": "Instead of predicting the amino acid at the center, the Skip-gram model instead optimizes the prediction of the context words given a center word at position t. [1, 2] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9207160742802687
      ],
      "excerpt": "- --batchsize BATCHSIZE: size of batches \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8263061289974142
      ],
      "excerpt": "The optional flags for testing this model are:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8257720650987505,
        0.8754412061359714
      ],
      "excerpt": "- -window_size WINDOW_SIZE: Size of the context window. \n- -batch_size BATCH_SIZE: Size of neural network batches. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9207160742802687
      ],
      "excerpt": "- --batchsize BATCHSIZE: size of batches \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.940386234055534,
        0.8725668455126201
      ],
      "excerpt": "Here the goal is to find a vector for a word (or amino acid) w4, so that it will be closed to three other given amino acids: vec(x) = vec(w1) - vec(w2) + vec(w3) according to the cosine distance. [2] \nCompare the top k most similar words to a given word given by one of the embedding model with the k most similar words given by the BLOSUM62 score matrix. [3] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8063194376668539
      ],
      "excerpt": "- --in_channel: number of in channels \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8999531193718416
      ],
      "excerpt": "- --kernel_sizes: sizes of kernels (list) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9207160742802687
      ],
      "excerpt": "- --window: size of window (n-gram) \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mari756h/The_unemployed_cells/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 15:48:46 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mari756h/The_unemployed_cells/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mari756h/The_unemployed_cells",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mari756h/The_unemployed_cells/master/Main%20results.ipynb",
      "https://raw.githubusercontent.com/mari756h/The_unemployed_cells/master/notebooks/test_CBoW.ipynb",
      "https://raw.githubusercontent.com/mari756h/The_unemployed_cells/master/notebooks/cbow_analogical%20reasoning.ipynb",
      "https://raw.githubusercontent.com/mari756h/The_unemployed_cells/master/notebooks/Visualize.ipynb",
      "https://raw.githubusercontent.com/mari756h/The_unemployed_cells/master/notebooks/aa_freqs.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8814072183764774
      ],
      "excerpt": "- -wkdir WKDIR: Path to directory for output files. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8124904568355875
      ],
      "excerpt": "Testing the CBoW network can also be done by running the below script in terminal \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8404749582358655
      ],
      "excerpt": "The required arguments for this command are: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8759490517845565
      ],
      "excerpt": "python main_CBoW_aa.py -train_data TRAIN_DATA -val_data VAL_DATA [OPTIONAL FLAGS] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8348813576870169
      ],
      "excerpt": "- -train_data TRAIN_DATA: Path to training data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8083704752461224
      ],
      "excerpt": "- -f POST_FIXPost_fix for output files.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391557463703154
      ],
      "excerpt": "- -wkdir WKDIR: Path to directory for output files. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8257642882178484,
        0.9331142729799173,
        0.9432294936998961,
        0.9015160354924104
      ],
      "excerpt": "- --datadir DATADIR: data directory \n- --traindata TRAINDATA: name for training data file \n- --testdata TESTDATA: name for test data file \n- --validdata VALIDDATA: name for validation data file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8295040894493668,
        0.8051368525398265
      ],
      "excerpt": "- --tSNE: boolean, should tSNE plot be created? \n- --save: boolean, save model each epoch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9309685797325766
      ],
      "excerpt": "python utils/test_cbow.py -model MODEL -test_data TEST_DATA [OPTIONAL FLAGS] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9387946870706627,
        0.8351850456578634,
        0.8257642882178484,
        0.9331142729799173,
        0.9432294936998961
      ],
      "excerpt": "python utils/test_network_sg.py --model MODEL --tSNE  \nArguments for test_network_sg.py are: \n- --datadir DATADIR: data directory \n- --traindata TRAINDATA: name for training data file \n- --testdata TESTDATA: name for test data file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8349259879386134
      ],
      "excerpt": "- --tSNE: if specified, creates tSNE plot \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python main_baseline.py --datafile DATAFILE --word2idxfile WORD2IDXFILE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.812284971685824,
        0.8797529601964764
      ],
      "excerpt": "- --datafile: Path + name to formatted data file with sentences if --convert is not specified \n- --word2idxfile: Path + name to word2idx file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8790626198569066
      ],
      "excerpt": "- --file: Path + name to data file with sentences if --convert is given \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8027780498063171
      ],
      "excerpt": "- --probfile: Path + name to formatted data file with sentences if if --predict is given \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246628806267027,
        0.8998489188263171
      ],
      "excerpt": "- --train_file: Path+name to file with training data \n- --valid_file: Path+name to file with validation data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621882743635256
      ],
      "excerpt": "- --word2idx: word2idx file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8304498210440928
      ],
      "excerpt": "- --embedding_matrix: file with previously trained embeddings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806231503790735
      ],
      "excerpt": "- --batch_size: Batch size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8732149407160328
      ],
      "excerpt": "- --test_file: Path+name to file with test data if --evaluate is given \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mari756h/The_unemployed_cells/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Predicting the language of life",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "The_unemployed_cells",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mari756h",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mari756h/The_unemployed_cells/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Both analogical reasoning and BLOSUM62 comparisons can be run with\n```console\npython utils/analogical_reasoning.py --model MODEL --file FILE --model_type {CBOW, SG} --blosum62\n```\n\nArguments for this command are:\n- `--idx2word IDX2WORD`: Path + name to idx2word file\n- `--word2idx WORD2IDX`: Path + name to word2idx file\n- `--file FILE`: Path + name to file containing analogies to test\n- `--model_type {CBOW,SG}`: specifies whether the model to be tested is Skip-gram (SG) or continouos bag of words (CBOW)\n- `--model MODEL`: Path + name to saved model\n- `--emb_dim EMB_DIM`: Embedding dimension\n- `--vocab_size VOCAB_SIZE`: size of vocabulary\n- `--verbose`: Print all results\n- `--blosum62`: Compare with blosum62 scores (requires biopython)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Sat, 25 Dec 2021 15:48:46 GMT"
    },
    "technique": "GitHub API"
  }
}