{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  \n***KoSpeech***, an open-source software, is modular and extensible end-to-end Korean automatic speech recognition (ASR) toolkit based on the deep learning library PyTorch. Several automatic speech recognition open-source toolkits have been released, but all of them deal with non-Korean languages, such as English (e.g. ESPnet, Espresso). Although AI Hub opened 1,000 hours of Korean speech corpus known as KsponSpeech, there is no established preprocessing method and baseline model to compare model performances. Therefore, we propose preprocessing methods for KsponSpeech corpus and a several models (Deep Speech 2, LAS, Transformer, Jasper, Conformer). By KoSpeech, we hope this could be a guideline for those who research Korean speech recognition.  \n  \n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1512.02595",
      "https://arxiv.org/abs/1508.01211",
      "https://arxiv.org/abs/2005.08100",
      "https://arxiv.org/abs/1409.3215",
      "https://arxiv.org/abs/1409.0473",
      "https://arxiv.org/abs/1506.07503",
      "https://arxiv.org/abs/1508.01211",
      "https://arxiv.org/abs/1512.02595",
      "https://arxiv.org/abs/1706.02737",
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1712.01769",
      "https://arxiv.org/abs/1712.01996",
      "https://arxiv.org/abs/1904.08779",
      "https://arxiv.org/abs/1906.02629",
      "https://arxiv.org/abs/1912.05533",
      "https://arxiv.org/abs/2004.09367",
      "https://arxiv.org/abs/2005.08100",
      "https://arxiv.org/abs/2009.03092",
      "https://arxiv.org/abs/2009.03092},\n  journal   = {ArXiv e-prints},\n  eprint    = {2009.03092}\n}\n```",
      "https://arxiv.org/abs/ 1409.3215*  \n  \n*Dzmitry Bahdanau et al. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) https://arxiv.org/abs/ 1409.0473*   \n  \n*Jan Chorowski et al. [Attention Based Models for Speech Recognition](https://arxiv.org/abs/1506.07503) https://arxiv.org/abs/ 1506.07503*    \n  \n*Wiliam Chan et al. [Listen, Attend and Spell](https://arxiv.org/abs/1508.01211) https://arxiv.org/abs/ 1508.01211*   \n   \n*Dario Amodei et al. [Deep Speech2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/abs/1512.02595) https://arxiv.org/abs/ 1512.02595*   \n   \n*Takaaki Hori et al. [Advances in Joint CTC-Attention based E2E Automatic Speech Recognition with a Deep CNN Encoder and RNN-LM](https://arxiv.org/abs/1706.02737) https://arxiv.org/abs/ 1706.02737*   \n  \n*Ashish Vaswani et al. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) https://arxiv.org/abs/ 1706.03762*     \n  \n*Chung-Cheng Chiu et al. [State-of-the-art Speech Recognition with Sequence-to-Sequence Models](https://arxiv.org/abs/1712.01769) https://arxiv.org/abs/ 1712.01769*   \n  \n*Anjuli Kannan et al. [An Analysis Of Incorporating An External LM Into A Sequence-to-Sequence Model](https://arxiv.org/abs/1712.01996) https://arxiv.org/abs/ 1712.01996*  \n  \n*Daniel S. Park et al. [SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779) https://arxiv.org/abs/ 1904.08779*     \n    \n*Rafael Muller et al. [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629) https://arxiv.org/abs/ 1906.02629*   \n  \n*Daniel S. Park et al. [SpecAugment on large scale datasets](https://arxiv.org/abs/1912.05533) https://arxiv.org/abs/ 1912.05533* \n    \n*Jung-Woo Ha et al. [ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers](https://arxiv.org/abs/2004.09367) https://arxiv.org/abs/ 2004.09367*  \n  \n*Jason Li et al. [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/pdf/1904.03288.pdf) https://arxiv.org/abs/ 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang",
      "https://arxiv.org/abs/ 1409.0473*   \n  \n*Jan Chorowski et al. [Attention Based Models for Speech Recognition](https://arxiv.org/abs/1506.07503) https://arxiv.org/abs/ 1506.07503*    \n  \n*Wiliam Chan et al. [Listen, Attend and Spell](https://arxiv.org/abs/1508.01211) https://arxiv.org/abs/ 1508.01211*   \n   \n*Dario Amodei et al. [Deep Speech2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/abs/1512.02595) https://arxiv.org/abs/ 1512.02595*   \n   \n*Takaaki Hori et al. [Advances in Joint CTC-Attention based E2E Automatic Speech Recognition with a Deep CNN Encoder and RNN-LM](https://arxiv.org/abs/1706.02737) https://arxiv.org/abs/ 1706.02737*   \n  \n*Ashish Vaswani et al. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) https://arxiv.org/abs/ 1706.03762*     \n  \n*Chung-Cheng Chiu et al. [State-of-the-art Speech Recognition with Sequence-to-Sequence Models](https://arxiv.org/abs/1712.01769) https://arxiv.org/abs/ 1712.01769*   \n  \n*Anjuli Kannan et al. [An Analysis Of Incorporating An External LM Into A Sequence-to-Sequence Model](https://arxiv.org/abs/1712.01996) https://arxiv.org/abs/ 1712.01996*  \n  \n*Daniel S. Park et al. [SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779) https://arxiv.org/abs/ 1904.08779*     \n    \n*Rafael Muller et al. [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629) https://arxiv.org/abs/ 1906.02629*   \n  \n*Daniel S. Park et al. [SpecAugment on large scale datasets](https://arxiv.org/abs/1912.05533) https://arxiv.org/abs/ 1912.05533* \n    \n*Jung-Woo Ha et al. [ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers](https://arxiv.org/abs/2004.09367) https://arxiv.org/abs/ 2004.09367*  \n  \n*Jason Li et al. [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/pdf/1904.03288.pdf) https://arxiv.org/abs/ 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang",
      "https://arxiv.org/abs/ 1506.07503*    \n  \n*Wiliam Chan et al. [Listen, Attend and Spell](https://arxiv.org/abs/1508.01211) https://arxiv.org/abs/ 1508.01211*   \n   \n*Dario Amodei et al. [Deep Speech2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/abs/1512.02595) https://arxiv.org/abs/ 1512.02595*   \n   \n*Takaaki Hori et al. [Advances in Joint CTC-Attention based E2E Automatic Speech Recognition with a Deep CNN Encoder and RNN-LM](https://arxiv.org/abs/1706.02737) https://arxiv.org/abs/ 1706.02737*   \n  \n*Ashish Vaswani et al. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) https://arxiv.org/abs/ 1706.03762*     \n  \n*Chung-Cheng Chiu et al. [State-of-the-art Speech Recognition with Sequence-to-Sequence Models](https://arxiv.org/abs/1712.01769) https://arxiv.org/abs/ 1712.01769*   \n  \n*Anjuli Kannan et al. [An Analysis Of Incorporating An External LM Into A Sequence-to-Sequence Model](https://arxiv.org/abs/1712.01996) https://arxiv.org/abs/ 1712.01996*  \n  \n*Daniel S. Park et al. [SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779) https://arxiv.org/abs/ 1904.08779*     \n    \n*Rafael Muller et al. [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629) https://arxiv.org/abs/ 1906.02629*   \n  \n*Daniel S. Park et al. [SpecAugment on large scale datasets](https://arxiv.org/abs/1912.05533) https://arxiv.org/abs/ 1912.05533* \n    \n*Jung-Woo Ha et al. [ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers](https://arxiv.org/abs/2004.09367) https://arxiv.org/abs/ 2004.09367*  \n  \n*Jason Li et al. [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/pdf/1904.03288.pdf) https://arxiv.org/abs/ 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang",
      "https://arxiv.org/abs/ 1508.01211*   \n   \n*Dario Amodei et al. [Deep Speech2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/abs/1512.02595) https://arxiv.org/abs/ 1512.02595*   \n   \n*Takaaki Hori et al. [Advances in Joint CTC-Attention based E2E Automatic Speech Recognition with a Deep CNN Encoder and RNN-LM](https://arxiv.org/abs/1706.02737) https://arxiv.org/abs/ 1706.02737*   \n  \n*Ashish Vaswani et al. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) https://arxiv.org/abs/ 1706.03762*     \n  \n*Chung-Cheng Chiu et al. [State-of-the-art Speech Recognition with Sequence-to-Sequence Models](https://arxiv.org/abs/1712.01769) https://arxiv.org/abs/ 1712.01769*   \n  \n*Anjuli Kannan et al. [An Analysis Of Incorporating An External LM Into A Sequence-to-Sequence Model](https://arxiv.org/abs/1712.01996) https://arxiv.org/abs/ 1712.01996*  \n  \n*Daniel S. Park et al. [SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779) https://arxiv.org/abs/ 1904.08779*     \n    \n*Rafael Muller et al. [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629) https://arxiv.org/abs/ 1906.02629*   \n  \n*Daniel S. Park et al. [SpecAugment on large scale datasets](https://arxiv.org/abs/1912.05533) https://arxiv.org/abs/ 1912.05533* \n    \n*Jung-Woo Ha et al. [ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers](https://arxiv.org/abs/2004.09367) https://arxiv.org/abs/ 2004.09367*  \n  \n*Jason Li et al. [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/pdf/1904.03288.pdf) https://arxiv.org/abs/ 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang",
      "https://arxiv.org/abs/ 1512.02595*   \n   \n*Takaaki Hori et al. [Advances in Joint CTC-Attention based E2E Automatic Speech Recognition with a Deep CNN Encoder and RNN-LM](https://arxiv.org/abs/1706.02737) https://arxiv.org/abs/ 1706.02737*   \n  \n*Ashish Vaswani et al. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) https://arxiv.org/abs/ 1706.03762*     \n  \n*Chung-Cheng Chiu et al. [State-of-the-art Speech Recognition with Sequence-to-Sequence Models](https://arxiv.org/abs/1712.01769) https://arxiv.org/abs/ 1712.01769*   \n  \n*Anjuli Kannan et al. [An Analysis Of Incorporating An External LM Into A Sequence-to-Sequence Model](https://arxiv.org/abs/1712.01996) https://arxiv.org/abs/ 1712.01996*  \n  \n*Daniel S. Park et al. [SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779) https://arxiv.org/abs/ 1904.08779*     \n    \n*Rafael Muller et al. [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629) https://arxiv.org/abs/ 1906.02629*   \n  \n*Daniel S. Park et al. [SpecAugment on large scale datasets](https://arxiv.org/abs/1912.05533) https://arxiv.org/abs/ 1912.05533* \n    \n*Jung-Woo Ha et al. [ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers](https://arxiv.org/abs/2004.09367) https://arxiv.org/abs/ 2004.09367*  \n  \n*Jason Li et al. [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/pdf/1904.03288.pdf) https://arxiv.org/abs/ 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang",
      "https://arxiv.org/abs/ 1706.02737*   \n  \n*Ashish Vaswani et al. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) https://arxiv.org/abs/ 1706.03762*     \n  \n*Chung-Cheng Chiu et al. [State-of-the-art Speech Recognition with Sequence-to-Sequence Models](https://arxiv.org/abs/1712.01769) https://arxiv.org/abs/ 1712.01769*   \n  \n*Anjuli Kannan et al. [An Analysis Of Incorporating An External LM Into A Sequence-to-Sequence Model](https://arxiv.org/abs/1712.01996) https://arxiv.org/abs/ 1712.01996*  \n  \n*Daniel S. Park et al. [SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779) https://arxiv.org/abs/ 1904.08779*     \n    \n*Rafael Muller et al. [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629) https://arxiv.org/abs/ 1906.02629*   \n  \n*Daniel S. Park et al. [SpecAugment on large scale datasets](https://arxiv.org/abs/1912.05533) https://arxiv.org/abs/ 1912.05533* \n    \n*Jung-Woo Ha et al. [ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers](https://arxiv.org/abs/2004.09367) https://arxiv.org/abs/ 2004.09367*  \n  \n*Jason Li et al. [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/pdf/1904.03288.pdf) https://arxiv.org/abs/ 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang",
      "https://arxiv.org/abs/ 1706.03762*     \n  \n*Chung-Cheng Chiu et al. [State-of-the-art Speech Recognition with Sequence-to-Sequence Models](https://arxiv.org/abs/1712.01769) https://arxiv.org/abs/ 1712.01769*   \n  \n*Anjuli Kannan et al. [An Analysis Of Incorporating An External LM Into A Sequence-to-Sequence Model](https://arxiv.org/abs/1712.01996) https://arxiv.org/abs/ 1712.01996*  \n  \n*Daniel S. Park et al. [SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779) https://arxiv.org/abs/ 1904.08779*     \n    \n*Rafael Muller et al. [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629) https://arxiv.org/abs/ 1906.02629*   \n  \n*Daniel S. Park et al. [SpecAugment on large scale datasets](https://arxiv.org/abs/1912.05533) https://arxiv.org/abs/ 1912.05533* \n    \n*Jung-Woo Ha et al. [ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers](https://arxiv.org/abs/2004.09367) https://arxiv.org/abs/ 2004.09367*  \n  \n*Jason Li et al. [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/pdf/1904.03288.pdf) https://arxiv.org/abs/ 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang",
      "https://arxiv.org/abs/ 1712.01769*   \n  \n*Anjuli Kannan et al. [An Analysis Of Incorporating An External LM Into A Sequence-to-Sequence Model](https://arxiv.org/abs/1712.01996) https://arxiv.org/abs/ 1712.01996*  \n  \n*Daniel S. Park et al. [SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779) https://arxiv.org/abs/ 1904.08779*     \n    \n*Rafael Muller et al. [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629) https://arxiv.org/abs/ 1906.02629*   \n  \n*Daniel S. Park et al. [SpecAugment on large scale datasets](https://arxiv.org/abs/1912.05533) https://arxiv.org/abs/ 1912.05533* \n    \n*Jung-Woo Ha et al. [ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers](https://arxiv.org/abs/2004.09367) https://arxiv.org/abs/ 2004.09367*  \n  \n*Jason Li et al. [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/pdf/1904.03288.pdf) https://arxiv.org/abs/ 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang",
      "https://arxiv.org/abs/ 1712.01996*  \n  \n*Daniel S. Park et al. [SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779) https://arxiv.org/abs/ 1904.08779*     \n    \n*Rafael Muller et al. [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629) https://arxiv.org/abs/ 1906.02629*   \n  \n*Daniel S. Park et al. [SpecAugment on large scale datasets](https://arxiv.org/abs/1912.05533) https://arxiv.org/abs/ 1912.05533* \n    \n*Jung-Woo Ha et al. [ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers](https://arxiv.org/abs/2004.09367) https://arxiv.org/abs/ 2004.09367*  \n  \n*Jason Li et al. [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/pdf/1904.03288.pdf) https://arxiv.org/abs/ 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang",
      "https://arxiv.org/abs/ 1904.08779*     \n    \n*Rafael Muller et al. [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629) https://arxiv.org/abs/ 1906.02629*   \n  \n*Daniel S. Park et al. [SpecAugment on large scale datasets](https://arxiv.org/abs/1912.05533) https://arxiv.org/abs/ 1912.05533* \n    \n*Jung-Woo Ha et al. [ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers](https://arxiv.org/abs/2004.09367) https://arxiv.org/abs/ 2004.09367*  \n  \n*Jason Li et al. [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/pdf/1904.03288.pdf) https://arxiv.org/abs/ 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang",
      "https://arxiv.org/abs/ 1906.02629*   \n  \n*Daniel S. Park et al. [SpecAugment on large scale datasets](https://arxiv.org/abs/1912.05533) https://arxiv.org/abs/ 1912.05533* \n    \n*Jung-Woo Ha et al. [ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers](https://arxiv.org/abs/2004.09367) https://arxiv.org/abs/ 2004.09367*  \n  \n*Jason Li et al. [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/pdf/1904.03288.pdf) https://arxiv.org/abs/ 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang",
      "https://arxiv.org/abs/ 1912.05533* \n    \n*Jung-Woo Ha et al. [ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers](https://arxiv.org/abs/2004.09367) https://arxiv.org/abs/ 2004.09367*  \n  \n*Jason Li et al. [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/pdf/1904.03288.pdf) https://arxiv.org/abs/ 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang",
      "https://arxiv.org/abs/ 2004.09367*  \n  \n*Jason Li et al. [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/pdf/1904.03288.pdf) https://arxiv.org/abs/ 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang",
      "https://arxiv.org/abs/ 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang",
      "https://arxiv.org/abs/ 2005.08100*\n    \n### Github References\n  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n### License\nThis project is licensed under the Apache-2.0 LICENSE - see the [LICENSE.md](https://github.com/sooftware/kospeech/blob/master/LICENSE) file for details\n  \n## Citation\n  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  \nA [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000026) on KoSpeech is available. If you use the system for academic work, please cite:\n  \n```\n@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang},\n  title     = {KoSpeech: Open-Source Toolkit for End-to-End Korean Speech Recognition},\n  url       = {https://www.sciencedirect.com/science/article/pii/S2665963821000026},\n  month     = {February},\n  year      = {2021},\n  publisher = {ELSEVIER},\n  journal   = {SIMPAC},\n  pages     = {Volume 7, 100054}\n}\n```\nA [technical report](https://arxiv.org/abs/2009.03092) on KoSpeech in available.  \n  \n```\n@TECHREPORT{2020-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang},\n  title     = {KoSpeech: Open-Source Toolkit for End-to-End Korean Speech Recognition},\n  month     = {September},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2009.03092},\n  journal   = {ArXiv e-prints},\n  eprint    = {2009.03092}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "  \n*[IBM/Pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)*  \n  \n*[SeanNaren/deepspeech.pytorch](https://github.com/SeanNaren/deepspeech.pytorch)*  \n  \n*[kaituoxu/Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)*  \n  \n*[OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)*  \n  \n*[clovaai/ClovaCall](https://github.com/clovaai/ClovaCall)*  \n  \n*[LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)*\n  \n*[NVIDIA/DeepLearningExample](https://github.com/NVIDIA/DeepLearningExamples)*\n  \n*[espnet/espnet](https://github.com/espnet/espnet)*\n   \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "  \n*Ilya Sutskever et al. [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) arXiv: 1409.3215*  \n  \n*Dzmitry Bahdanau et al. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) arXiv: 1409.0473*   \n  \n*Jan Chorowski et al. [Attention Based Models for Speech Recognition](https://arxiv.org/abs/1506.07503) arXiv: 1506.07503*    \n  \n*Wiliam Chan et al. [Listen, Attend and Spell](https://arxiv.org/abs/1508.01211) arXiv: 1508.01211*   \n   \n*Dario Amodei et al. [Deep Speech2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/abs/1512.02595) arXiv: 1512.02595*   \n   \n*Takaaki Hori et al. [Advances in Joint CTC-Attention based E2E Automatic Speech Recognition with a Deep CNN Encoder and RNN-LM](https://arxiv.org/abs/1706.02737) arXiv: 1706.02737*   \n  \n*Ashish Vaswani et al. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) arXiv: 1706.03762*     \n  \n*Chung-Cheng Chiu et al. [State-of-the-art Speech Recognition with Sequence-to-Sequence Models](https://arxiv.org/abs/1712.01769) arXiv: 1712.01769*   \n  \n*Anjuli Kannan et al. [An Analysis Of Incorporating An External LM Into A Sequence-to-Sequence Model](https://arxiv.org/abs/1712.01996) arXiv: 1712.01996*  \n  \n*Daniel S. Park et al. [SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779) arXiv: 1904.08779*     \n    \n*Rafael Muller et al. [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629) arXiv: 1906.02629*   \n  \n*Daniel S. Park et al. [SpecAugment on large scale datasets](https://arxiv.org/abs/1912.05533) arXiv: 1912.05533* \n    \n*Jung-Woo Ha et al. [ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers](https://arxiv.org/abs/2004.09367) arXiv: 2004.09367*  \n  \n*Jason Li et al. [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/pdf/1904.03288.pdf) arXiv: 1902.03288* \n    \n*Anmol Gulati et al. [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) arXiv: 2005.08100*\n    \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@TECHREPORT{2020-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang},\n  title     = {KoSpeech: Open-Source Toolkit for End-to-End Korean Speech Recognition},\n  month     = {September},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2009.03092},\n  journal   = {ArXiv e-prints},\n  eprint    = {2009.03092}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@ARTICLE{2021-kospeech,\n  author    = {Kim, Soohwan and Bae, Seyoung and Won, Cheolhwang},\n  title     = {KoSpeech: Open-Source Toolkit for End-to-End Korean Speech Recognition},\n  url       = {https://www.sciencedirect.com/science/article/pii/S2665963821000026},\n  month     = {February},\n  year      = {2021},\n  publisher = {ELSEVIER},\n  journal   = {SIMPAC},\n  pages     = {Volume 7, 100054}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9586571307183652,
        0.9143258237372004,
        0.9986994287937864
      ],
      "excerpt": "|Deep Speech 2|2D-invariant convolution & RNN & CTC|Dario Amodei et al., 2015|  \n|Listen Attend Spell (LAS)|Attention based RNN sequence to sequence|William Chan et al., 2016| \n|Joint CTC-Attention LAS|Joint CTC-Attention LAS|Suyoun Kim et al., 2017| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9793806207210615,
        0.9994143240148727,
        0.8491520732036005
      ],
      "excerpt": "|Speech Transformer|Convolutional extractor & transformer|Linhao Dong et al., 2018| \n|Jasper|Fully convolutional & dense residual connection & CTC|Jason Li et al., 2019| \n|Conformer|Convolution-augmented-Transformer|Anmol Gulati et al., 2020|   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8260022124206593
      ],
      "excerpt": "Joint CTC-Attention Listen, Attend and Spell Training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9634767388187392,
        0.8028553231447091
      ],
      "excerpt": "If you have any questions, bug reports, and feature requests, please open an issue on Github.  \nFor live discussions, please go to our gitter or Contacts sh951011@gmail.com please. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sooftware/kospeech",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-30T08:43:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-30T02:22:48Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  \nEnd-to-end (E2E) automatic speech recognition (ASR) is an emerging paradigm in the field of neural network-based speech recognition that offers multiple benefits. Traditional \u201chybrid\u201d ASR systems, which are comprised of an acoustic model, language model, and pronunciation model, require separate training of these components, each of which can be complex.   \n  \nFor example, training of an acoustic model is a multi-stage process of model training and time alignment between the speech acoustic feature sequence and output label sequence. In contrast, E2E ASR is a single integrated approach with a much simpler training pipeline with models that operate at low audio frame rates. This reduces the training time, decoding time, and allows joint optimization with downstream processing such as natural language understanding.  \n  \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8341589559722501
      ],
      "excerpt": "Febuary 2021: Add RNN-Transducer model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9274516444386742
      ],
      "excerpt": "- Not long ago, I modified a lot of the code, but I was personally busy, so I couldn't test all the cases. If there is an error, please feel free to give me a feedback. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9897892502623081,
        0.8990904167640333
      ],
      "excerpt": "It is based on the above papers, but there may be other parts of the model implementation. \nSo far, serveral models are implemented: Deep Speech 2, Listen Attend and Spell (LAS), RNN-Transducer, Speech Transformer, Jasper, Conformer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9781345207356589
      ],
      "excerpt": "Deep Speech 2 showed faster and more accurate performance on ASR tasks with Connectionist Temporal Classification (CTC) loss. This model has been highlighted for significantly increasing performance compared to the previous end- to-end models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9368505986452693
      ],
      "excerpt": "We follow the architecture previously proposed in the \"Listen, Attend and Spell\", but some modifications were added to improve performance. We provide four different attention mechanisms, scaled dot-product attention, additive attention, location aware attention, multi-head attention. Attention mechanisms much affect the performance of models.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702619888283621
      ],
      "excerpt": "RNN-Transducer are a form of sequence-to-sequence models that do not employ attention mechanisms. Unlike most sequence-to-sequence models, which typically need to process the entire input sequence (the waveform in our case) to produce an output (the sentence), the RNN-T continuously processes input samples and streams output symbols, a property that is welcome for speech dictation. In our implementation, the output symbols are the characters of the alphabet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9634868264374066
      ],
      "excerpt": "Transformer is a powerful architecture in the Natural Language Processing (NLP) field. This architecture also showed good performance at ASR tasks. In addition, as the research of this model continues in the natural language processing field, this model has high potential for further development. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9908742037184528
      ],
      "excerpt": "With the proposed architecture to take advantage of both the CTC-based model and the attention-based model. It is a structure that makes it robust by adding CTC to the encoder. Joint CTC-Attention can be trained in combination with LAS and Speech Transformer.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9903925953142626
      ],
      "excerpt": "Jasper (Just Another SPEech Recognizer) is a end-to-end convolutional neural acoustic model. Jasper showed powerful performance with only CNN \u2192 BatchNorm \u2192 ReLU \u2192 Dropout block and residential connection.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9891289955224295
      ],
      "excerpt": "Conformer combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "   |  +-- model.pt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Open-Source Toolkit for End-to-End Korean Automatic Speech Recognition leveraging PyTorch and Hydra.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sooftware/End-to-end-Speech-Recognition/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 109,
      "date": "Thu, 30 Dec 2021 07:54:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sooftware/kospeech/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "sooftware/kospeech",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/sooftware/End-to-end-Speech-Recognition/tree/latest/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/sooftware/End-to-end-Speech-Recognition/latest/dataset/kspon/preprocess.sh",
      "https://raw.githubusercontent.com/sooftware/End-to-end-Speech-Recognition/latest/dataset/libri/prepare-libri.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  \nDownload from [here](https://github.com/sooftware/kospeech#pre-processed-transcripts) or refer to the following to preprocess.\n  \n- KsponSpeech : [Check this page](https://github.com/sooftware/kospeech/tree/master/dataset/kspon)\n- LibriSpeech : [Check this page](https://github.com/sooftware/kospeech/tree/master/dataset/libri)\n  \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Currently we only support installation from source code using setuptools. Checkout the source code and run the   \nfollowing commands:  \n```\npip install -e .\n```\n  \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This project recommends Python 3.7 or higher.   \nWe recommend creating a new virtual environment for this project (using virtual env or conda).  \n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9546132831597937
      ],
      "excerpt": "python ./bin/main.py model=ds2 train=ds2_train train.dataset_path=$DATASET_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9642649990290614
      ],
      "excerpt": "python ./bin/main.py model=las train=las_train train.dataset_path=$DATASET_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9459665375341321,
        0.8174540907975313,
        0.9546132831597937,
        0.8174540907975313,
        0.9546132831597937
      ],
      "excerpt": "python ./bin/main.py model=joint-ctc-attention-las train=las_train train.dataset_path=$DATASET_PATH \nRNN Transducer Training \npython ./bin/main.py model=rnnt train=rnnt_train train.dataset_path=$DATASET_PATH \nSpeech Transformer Training \npython ./bin/main.py model=transformer train=transformer_train train.dataset_path=$DATASET_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9301541201785521,
        0.8174540907975313,
        0.9546132831597937,
        0.8174540907975313,
        0.936956748005879
      ],
      "excerpt": "python ./bin/main.py model=joint-ctc-attention-transformer train=transformer_train train.dataset_path=$DATASET_PATH \nJasper Training \npython ./bin/main.py model=jasper train=jasper_train train.dataset_path=$DATASET_PATH \nConformer Training \npython ./bin/main.py model=conformer-large train=conformer_large_train train.dataset_path=$DATASET_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.849780341768748
      ],
      "excerpt": "python ./bin/eval.py eval.dataset_path=$DATASET_PATH eval.transcripts_path=$TRANSCRIPTS_PATH eval.model_path=$MODEL_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8449837841215025
      ],
      "excerpt": "$ python3 ./bin/inference.py --model_path $MODEL_PATH --audio_path $AUDIO_PATH --device $DEVICE \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sooftware/kospeech/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Soohwan Kim\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "introduction\">Introduction</a> \u2022",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "kospeech",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "sooftware",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sooftware/kospeech/blob/latest/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "sooftware",
        "body": "### What's New\r\n\r\n- Add [Jasper Model](https://arxiv.org/pdf/1904.03288.pdf)\r\n- Add [Conformer Model](https://arxiv.org/abs/2005.08100)\r\n- Add Transformer Learning Rate Scheduler\r\n",
        "dateCreated": "2021-01-25T20:42:26Z",
        "datePublished": "2021-01-25T20:44:50Z",
        "html_url": "https://github.com/sooftware/kospeech/releases/tag/v1.3",
        "name": "KoSpeech v1.3",
        "tag_name": "v1.3",
        "tarball_url": "https://api.github.com/repos/sooftware/kospeech/tarball/v1.3",
        "url": "https://api.github.com/repos/sooftware/kospeech/releases/36881451",
        "zipball_url": "https://api.github.com/repos/sooftware/kospeech/zipball/v1.3"
      },
      {
        "authorType": "User",
        "author_name": "sooftware",
        "body": "### What's New\r\n- January 2021: Joint CTC-Attention Transformer model passing\r\n- January 2021: Speech Transformer model passing\r\n- January 2021: Apply Hydra: framework for elegantly configuring complex applications",
        "dateCreated": "2021-01-04T19:59:54Z",
        "datePublished": "2021-01-05T06:47:55Z",
        "html_url": "https://github.com/sooftware/kospeech/releases/tag/v1.2",
        "name": "KoSpeech v1.2",
        "tag_name": "v1.2",
        "tarball_url": "https://api.github.com/repos/sooftware/kospeech/tarball/v1.2",
        "url": "https://api.github.com/repos/sooftware/kospeech/releases/35981910",
        "zipball_url": "https://api.github.com/repos/sooftware/kospeech/zipball/v1.2"
      },
      {
        "authorType": "User",
        "author_name": "sooftware",
        "body": "- Add Joint CTC-Attention Architecture (Currently, Not Support Multi-GPU training)\r\n- Add Deep Speech 2 Architecture",
        "dateCreated": "2020-12-14T03:20:26Z",
        "datePublished": "2020-12-14T16:37:07Z",
        "html_url": "https://github.com/sooftware/kospeech/releases/tag/v1.1",
        "name": "KoSpeech v1.1",
        "tag_name": "v1.1",
        "tarball_url": "https://api.github.com/repos/sooftware/kospeech/tarball/v1.1",
        "url": "https://api.github.com/repos/sooftware/kospeech/releases/35246002",
        "zipball_url": "https://api.github.com/repos/sooftware/kospeech/zipball/v1.1"
      },
      {
        "authorType": "User",
        "author_name": "sooftware",
        "body": "KoSpeech v1.0",
        "dateCreated": "2020-11-30T19:50:08Z",
        "datePublished": "2020-12-02T06:16:59Z",
        "html_url": "https://github.com/sooftware/kospeech/releases/tag/v1.0",
        "name": "KoSpeech v1.0",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/sooftware/kospeech/tarball/v1.0",
        "url": "https://api.github.com/repos/sooftware/kospeech/releases/34672939",
        "zipball_url": "https://api.github.com/repos/sooftware/kospeech/zipball/v1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  \n* Numpy: `pip install numpy` (Refer [here](https://github.com/numpy/numpy) for problem installing Numpy).\n* Pytorch: Refer to [PyTorch website](http://pytorch.org/) to install the version w.r.t. your environment.   \n* Pandas: `pip install pandas` (Refer [here](https://github.com/pandas-dev/pandas) for problem installing Pandas)  \n* Matplotlib: `pip install matplotlib` (Refer [here](https://github.com/matplotlib/matplotlib) for problem installing Matplotlib)\n* librosa: `conda install -c conda-forge librosa` (Refer [here](https://github.com/librosa/librosa) for problem installing librosa)\n* torchaudio: `pip install torchaudio==0.6.0` (Refer [here](https://github.com/pytorch/pytorch) for problem installing torchaudio)\n* tqdm: `pip install tqdm` (Refer [here](https://github.com/tqdm/tqdm) for problem installing tqdm)\n* sentencepiece: `pip install sentencepiece` (Refer [here](https://github.com/google/sentencepiece) for problem installing sentencepiece)\n* warp-rnnt: `pip install warp_rnnt` (Refer [here](https://github.com/1ytic/warp-rnnt)) for problem installing warp-rnnt)\n* hydra: `pip install hydra-core --upgrade` (Refer [here](https://github.com/facebookresearch/hydra) for problem installing hydra)\n  \n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 331,
      "date": "Thu, 30 Dec 2021 07:54:01 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  \nDownload from [here](https://github.com/sooftware/kospeech#pre-processed-transcripts) or refer to the following to preprocess.\n  \n- KsponSpeech : [Check this page](https://github.com/sooftware/kospeech/tree/master/dataset/kspon)\n- LibriSpeech : [Check this page](https://github.com/sooftware/kospeech/tree/master/dataset/libri)\n  \n",
      "technique": "Header extraction"
    }
  ],
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "speech-recognition",
      "asr",
      "korean-speech",
      "end-to-end",
      "las-models",
      "ksponspeech",
      "pytorch",
      "seq2seq",
      "e2e-asr",
      "las",
      "transformer",
      "attention-is-all-you-need",
      "jasper",
      "conformer"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  \nWe use [Hydra](https://github.com/facebookresearch/hydra) to control all the training configurations. If you are not familiar with Hydra we recommend visiting the [Hydra website](https://hydra.cc/). Generally, Hydra is an open-source framework that simplifies the development of research applications by providing the ability to create a hierarchical configuration dynamically.\n  \n",
      "technique": "Header extraction"
    }
  ]
}