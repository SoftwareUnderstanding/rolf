{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [PyTorch-Encoding](https://github.com/zhanghang1989/PyTorch-Encoding)\n- [Pytorch-Template](https://github.com/victoresque/pytorch-template/blob/master/README.m)\n- [Synchronized-BatchNorm-PyTorch](https://github.com/vacancy/Synchronized-BatchNorm-PyTorch)\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1802.02611",
      "https://arxiv.org/abs/1703.02719",
      "https://arxiv.org/abs/1807.10221",
      "https://arxiv.org/abs/1702.08502",
      "https://arxiv.org/abs/1606.02147",
      "https://arxiv.org/abs/1505.04597",
      "https://arxiv.org/abs/1705.08790",
      "https://arxiv.org/abs/1708.07120"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9502226402959315
      ],
      "excerpt": "(U-Net) Convolutional Networks for Biomedical Image Segmentation (2015): [Paper] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9152404117042999
      ],
      "excerpt": "(FCN) Fully Convolutional Networks for Semantic Segmentation (2015): [Paper] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9349701480894177
      ],
      "excerpt": "        \"rotate\": true,       // Random rotation between 10 and -10 degrees \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9591125186287817
      ],
      "excerpt": "    \"save_period\": 10,            // Saving chechpoint each 10 epochs \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yassouali/pytorch-segmentation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-14T23:00:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T11:59:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "Data augmentation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9667533344206685
      ],
      "excerpt": "This repo contains a PyTorch an implementation of different semantic segmentation models for different datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8082827378026995,
        0.8743996677779584
      ],
      "excerpt": "A json config file with a lot of possibilities for parameter tuning, \nSupports various models, losses, Lr schedulers, data augmentations and datasets, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9221493863900956,
        0.8053321243847325,
        0.9493763243856513,
        0.9788572443603879,
        0.9617452551044147,
        0.8571485198139109,
        0.992292834759273,
        0.8854634475603502,
        0.9792324855630049
      ],
      "excerpt": "COCO Stuff: For COCO, there is two partitions, CocoStuff10k with only 10k that are used for training the evaluation, note that this dataset is outdated, can be used for small scale testing and training, and can be downloaded here. For the official dataset with all of the training 164k examples, it can be downloaded from the official website.\\ \nNote that when using COCO dataset, 164k version is used per default, if 10k is prefered, this needs to be specified with an additionnal parameter partition = 'CocoStuff164k' in the config file with the corresponding path. \nIn addition to the Cross-Entorpy loss, there is also \n- Dice-Loss, which measures of overlap between two samples and can be more reflective of the training objective (maximizing the mIoU), but is highly non-convexe and can be hard to optimize. \n- CE Dice loss, the sum of the Dice loss and CE, CE gives smooth optimization while Dice loss is a good indicator of the quality of the segmentation results. \n- Focal Loss, an alternative version of the CE, used to avoid class imbalance where the confident predictions are scaled down. \n- Lovasz Softmax lends it self as a good alternative to the Dice loss, where we can directly optimization for the mean intersection-over-union based on the convex Lov\u00e1sz extension of submodular losses (for more details, check the paper: The Lov\u00e1sz-Softmax loss). \nPoly learning rate, where the learning rate is scaled down linearly from the starting value down to zero during training. Considered as the go to scheduler for semantic segmentaion (see Figure below). \nOne Cycle learning rate, for a learning rate LR, we start from LR / 10 up to LR for 30% of the training time, and we scale down to LR / 25 for remaining time, the scaling is done in a cos annealing fashion (see Figure bellow), the momentum is also modified but in the opposite manner starting from 0.95 down to 0.85 and up to 0.95, for more detail see the paper: Super-Convergence.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.949071994144764
      ],
      "excerpt": "All of the data augmentations are implemented using OpenCV in \\base\\base_dataset.py, which are: rotation (between -10 and 10 degrees), random croping between 0.5 and 2 of the selected crop_size, random h-flip and blurring \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9321716074570245
      ],
      "excerpt": "For inference, we need a PyTorch trained model, the images we'd like to segment and the config used in training (to load the correct model and other parameters),  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.906540580372868
      ],
      "excerpt": "Here are the parameters availble for inference: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9636721609549131
      ],
      "excerpt": "The code structure is based on pytorch-template \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8613856100054982
      ],
      "excerpt": "  \u2502   \u251c\u2500\u2500 base_dataset.py - All the data augmentations are implemented here \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8881798750879455
      ],
      "excerpt": "  \u251c\u2500\u2500 dataloader/ - loading the data for different segmentation datasets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8285214242989304,
        0.8600519902153052
      ],
      "excerpt": "        \"freeze_bn\": false,         // When fine tuning the model this can be used \n        \"freeze_backbone\": false    // In this case only the decoder is trained \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8789983862234464
      ],
      "excerpt": "        \"crop_size\": 380,     // Size of the random crop after rescaling \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.932845912652147
      ],
      "excerpt": "        \"base_size\": 400,     // The image is resized to base_size, then randomly croped \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "        \"data_dir\": \"data/\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8229213265799199
      ],
      "excerpt": "    \"save_dir\": \"saved/\",         // Checkpoints are saved in save_dir/models/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": ":art: Semantic segmentation models, datasets and losses implemented in PyTorch.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/y-ouali/pytorch_segmentation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 258,
      "date": "Tue, 21 Dec 2021 13:19:44 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yassouali/pytorch-segmentation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "yassouali/pytorch-segmentation",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.867295084442139
      ],
      "excerpt": "Config file format \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8270575600439654
      ],
      "excerpt": "CityScapes: First download the images and the annotations (there is two types of annotations, Fine gtFine_trainvaltest.zip and Coarse gtCoarse.zip annotations, and the images leftImg8bit_trainvaltest.zip) from the official website cityscapes-dataset.com, extract all of them in the same folder, and use the location of this folder in config.json for training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8835528982191526
      ],
      "excerpt": "To train a model, first download the dataset to be used to train the model, then choose the desired architecture, add the correct path to the dataset and set the desired hyperparameters (the config file is detailed below), then simply run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9109426385148628
      ],
      "excerpt": "python train.py --config config.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9012264334006812
      ],
      "excerpt": "python inference.py --config config.json --model best_model.pth --images images_folder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8116483713253355
      ],
      "excerpt": "--output       The folder where the results will be saved (default: outputs). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8235051233746071
      ],
      "excerpt": "--config       The config file used for training the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8225730320181891
      ],
      "excerpt": "| Model     | Backbone     | PascalVoc val mIoU | PascalVoc test mIoU | Pretrained Model | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554293434323748
      ],
      "excerpt": "  \u251c\u2500\u2500 train.py - main script to start training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8866020860566906
      ],
      "excerpt": "  \u251c\u2500\u2500 trainer.py - the main trained \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991,
        0.8470941049043132,
        0.9336801098518991
      ],
      "excerpt": "  \u2502   \u251c\u2500\u2500 base_data_loader.py \n  \u2502   \u251c\u2500\u2500 base_model.py \n  \u2502   \u251c\u2500\u2500 base_dataset.py - All the data augmentations are implemented here \n  \u2502   \u2514\u2500\u2500 base_trainer.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8095923088625622,
        0.8325703251392461
      ],
      "excerpt": "  \u2514\u2500\u2500 utils/ - small utility functions \n      \u251c\u2500\u2500 losses.py - losses used in training the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8400741364782979
      ],
      "excerpt": "Config files are in .json format: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838913169919834
      ],
      "excerpt": "  \"name\": \"PSPNet\",         // training session name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8133597408422605
      ],
      "excerpt": "  \"use_synch_bn\": true,     // Using Synchronized batchnorm (for multi-GPU usage) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8510138543763953
      ],
      "excerpt": "    \"type\": \"PSPNet\", // name of model architecture to train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8119332972433224
      ],
      "excerpt": "        \"data_dir\": \"data/\",  // dataset path \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8195589480380796
      ],
      "excerpt": "        \"augment\": true,      // Use data augmentation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "        \"shuffle\": true, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8195177320051209
      ],
      "excerpt": "        \"val\": true, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021747485411023
      ],
      "excerpt": "\"loss\": \"CrossEntropyLoss2d\",     // Loss (see utils/losses.py) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8195177320051209
      ],
      "excerpt": "    \"val\": true, \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yassouali/pytorch-segmentation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Yassine\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Semantic Segmentation in PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-segmentation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "yassouali",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yassouali/pytorch-segmentation/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "yassouali",
        "body": "",
        "dateCreated": "2020-06-30T10:06:31Z",
        "datePublished": "2020-06-30T10:09:30Z",
        "html_url": "https://github.com/yassouali/pytorch-segmentation/releases/tag/v0.1",
        "name": "",
        "tag_name": "v0.1",
        "tarball_url": "https://api.github.com/repos/yassouali/pytorch-segmentation/tarball/v0.1",
        "url": "https://api.github.com/repos/yassouali/pytorch-segmentation/releases/28060874",
        "zipball_url": "https://api.github.com/repos/yassouali/pytorch-segmentation/zipball/v0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "PyTorch and Torchvision needs to be installed before running the scripts, together with `PIL` and `opencv` for data-preprocessing and `tqdm` for showing the training progress. PyTorch v1.1 is supported (using the new supported tensoboard); can work with ealier versions, but instead of using tensoboard, use tensoboardX.\n\n```bash\npip install -r requirements.txt\n```\n\nor for a local installation\n\n```bash\npip install --user -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 939,
      "date": "Tue, 21 Dec 2021 13:19:44 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "semantic-segmentation",
      "computer-vision",
      "deep-learning"
    ],
    "technique": "GitHub API"
  }
}