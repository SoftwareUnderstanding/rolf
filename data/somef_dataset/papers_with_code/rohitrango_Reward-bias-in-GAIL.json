{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2009.09467",
      "https://arxiv.org/abs/1710.11248",
      "https://arxiv.org/abs/1606.03476",
      "https://arxiv.org/abs/2009.09467"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you like this work and want to use it in your research, consider citing us:\n```\n@article{jena2020addressing,\n  title={Addressing reward bias in Adversarial Imitation Learning with neutral reward functions},\n  author={Jena, Rohit and Agrawal, Siddharth and Sycara, Katia},\n  journal={arXiv preprint arXiv:2009.09467},\n  year={2020}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{jena2020addressing,\n  title={Addressing reward bias in Adversarial Imitation Learning with neutral reward functions},\n  author={Jena, Rohit and Agrawal, Siddharth and Sycara, Katia},\n  journal={arXiv preprint arXiv:2009.09467},\n  year={2020}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rohitrango/Reward-bias-in-GAIL",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-29T20:17:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-13T04:06:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9407788504809341
      ],
      "excerpt": "Implementation of the paper Addressing reward bias in Adversarial Imitation Learning with neutral reward functions in Tensorflow.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9695707099378236,
        0.9735091824666242,
        0.9743574873988353
      ],
      "excerpt": "This project aims to provide clean implementations of imitation learning algorithms. \nCurrently we have implementations of AIRL and  \nGAIL, and intend to add more in the future. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.896875427363036
      ],
      "excerpt": "Add units tests covering any new features, or bugs that are being fixed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833245197029026
      ],
      "excerpt": "Code coverage is automatically enforced by CodeCov. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Official implementation of the paper `Addressing reward bias in Adversarial Imitation Learning with neutral reward functions` in Tensorflow.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rohitrango/Reward-bias-in-GAIL/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 20:45:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rohitrango/Reward-bias-in-GAIL/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rohitrango/Reward-bias-in-GAIL",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/rohitrango/Reward-bias-in-GAIL/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/Discriminator%20rewards%20to%20expert%20.ipynb",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/notebooks/density_baseline_demo.ipynb",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/_experiments/mce_irl.ipynb",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/_experiments/bc_demo.ipynb",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/_experiments/plot_training.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/experiment1_doorkey.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/biasplot.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/experiment1_walker.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/experiment2_hopper.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/experiment1_empty.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/experiment1_lava.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/experiment2_doorkey.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/emptyv2.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/experiment1_hopper.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/experiment2_empty.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/experiment2_lava.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/experiment2.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/experiment1.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/experiment2_redblue.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/experiment1_redblue.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/_experiments/download_models.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/_experiments/imit_benchmark.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/_experiments/transfer_learn_benchmark.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/_experiments/train_experts.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/_experiments/rollouts_from_policies.sh",
      "https://raw.githubusercontent.com/rohitrango/Reward-bias-in-GAIL/master/tests/generate_test_data.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nconda create -n imitation python=3.8  #: python 3.7 and virtualenv are also okay.\nconda activate imitation\npip install -e '.[dev]'  #: install `imitation` in developer mode\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9264893834884709,
        0.9752117098950219,
        0.8004138995996513
      ],
      "excerpt": "To run the experiments, do the following: \n1. Install the repository (instructions below). \n2. Save trajectories using PPO agents. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356738970915627
      ],
      "excerpt": "    The exact coverage required by CodeCov depends on the previous \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rohitrango/Reward-bias-in-GAIL/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Justin Fu\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Addressing reward bias in GAIL",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Reward-bias-in-GAIL",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rohitrango",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rohitrango/Reward-bias-in-GAIL/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Follow instructions to install [mujoco_py v1.5 here](https://github.com/openai/mujoco-py/tree/498b451a03fb61e5bdfcb6956d8d7c881b1098b5#install-mujoco).\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n#: Train PPO2 agent on cartpole and collect expert demonstrations\npython -m imitation.scripts.expert_demos with cartpole\n#: Train AIRL on from demonstrations\npython -m imitation.scripts.train_adversarial with cartpole airl\n```\nView Tensorboard with `tensorboard --logdir output/`.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 20:45:17 GMT"
    },
    "technique": "GitHub API"
  }
}