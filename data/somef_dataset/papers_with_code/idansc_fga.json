{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1904.05880"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{schwartz2019factor,\n  title={Factor graph attention},\n  author={Schwartz, Idan and Yu, Seunghak and Hazan, Tamir and Schwing, Alexander G},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={2039--2048},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8132852241448368
      ],
      "excerpt": "(Appeared in CVPR'19) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.995502336505211
      ],
      "excerpt": "Part of 2020 visual dialog challenge winning submission (https://github.com/idansc/mrr-ndcg) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "             --epochs 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9104388306336967,
        0.9104388306336967
      ],
      "excerpt": "             --initialization \"he\" \\ \n             --lstm-initialization \"he\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "             --epochs 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9104388306336967,
        0.9104388306336967
      ],
      "excerpt": "             --initialization \"he\" \\ \n             --lstm-initialization \"he\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "             --epochs 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9104388306336967,
        0.9104388306336967
      ],
      "excerpt": "             --initialization \"he\" \\ \n             --lstm-initialization \"he\" \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/idansc/fga",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-05T11:37:01Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-07T01:49:08Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.901276300605054,
        0.9926316630817625
      ],
      "excerpt": "Achieves a state-of-the-art performance (MRR) on visual dialog task. \nThis repository is the official implementation of Factor Graph Attention. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8930901044020226,
        0.9322521585276207,
        0.8655538868538862
      ],
      "excerpt": "Pretrained features: \n- VGG A grid image features based on the VGG model pretrained on ImageNet (Faster). Note, the h5 databases has slightly different dataset keys, therfore the code needs to be adapted accordingly.  \n- F-RCNN based on object detector with ResNetx101 backbone, 37 proposals, fine-tuned on Visual Genome. Achives SOTA. The file includes boxes and classes information.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9807010915017343
      ],
      "excerpt": "See the original paper for performance differences. I recommand using the FRCNN features, mainly because it is finetuned on the relevant VisualGenome dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "             --image_data \"data/frcnn_features_new\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "             --image_data \"data/frcnn_features_new\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327268779510595
      ],
      "excerpt": "replace only_val, with submission arg, i.e.: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "             --image_data \"data/frcnn_features_new\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8238102548336088
      ],
      "excerpt": "Our model achieves the following performance on the validation set, and similar results on test-std/test-challenge. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9056412878087964
      ],
      "excerpt": "Note, the paper results may slightly vary from the results of this repo, since it is a refactored version. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/idansc/fga/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Thu, 30 Dec 2021 06:52:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/idansc/fga/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "idansc/fga",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8365814243644211
      ],
      "excerpt": "* Spatial navigation,  can be found here (https://github.com/barmayo/spatial_attention)  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9136230884094254
      ],
      "excerpt": "Note: You can use CurlWget to easily download the features on your server. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9321422000519974
      ],
      "excerpt": "You can download pertained models here: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9030543674490512,
        0.8974481152384285
      ],
      "excerpt": "  <img src=\"imgs/fga.png\" height=\"40%\" width=\"40%\"/> \n  <img src=\"/imgs/model.png\"\" height=\"50%\" width=\"50%\"/>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9427559858262275
      ],
      "excerpt": "python train.py --batch-size  128 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8572243551016527
      ],
      "excerpt": "             --test-batch-size 64 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8420542273012395
      ],
      "excerpt": "The path should contain a model file, best_model_mrr.pth.tar. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9427559858262275
      ],
      "excerpt": "python train.py --batch-size  128 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8572243551016527
      ],
      "excerpt": "             --test-batch-size 64 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8451401342496009,
        0.9427559858262275
      ],
      "excerpt": "test eval \npython train.py --batch-size  128 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8572243551016527
      ],
      "excerpt": "             --test-batch-size 64 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8088512944595196
      ],
      "excerpt": "You can download pertained models here: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8645544707693019
      ],
      "excerpt": "| Model name                   | R@1 |  MRR  | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/idansc/fga/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Factor Graph Attention",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "fga",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "idansc",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/idansc/fga/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The model can easily run on a single GPU :)\n\nTo install requirements:\n\n```setup\nconda env create -f fga.yml\n```\n\nfollows with:\n```\n conda activate fga\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 20,
      "date": "Thu, 30 Dec 2021 06:52:06 GMT"
    },
    "technique": "GitHub API"
  }
}