{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.3215",
      "https://arxiv.org/abs/1409.0473",
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215), Ilya Sutskever, Oriol Vinyals, Quoc V. Le\n- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473), Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio\n- [Attention Is All You Need](https://arxiv.org/abs/1706.03762), Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cosmoquester/seq2seq",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-30T14:36:51Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-06T05:21:47Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9712196747837957,
        0.9098081405535577,
        0.816832691749279
      ],
      "excerpt": "This is seq2seq model structures with Tensorflow 2. \nThere are three model architectures, RNNSeq2Seq, RNNSeq2SeqWithAttention, TransformerSeq2Seq. \nThis repository contains train, evaulate, inference, converting to savedmodel format scripts. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "                        pretrained model checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9843194314561768
      ],
      "excerpt": "                        Policy for sequences of which length is over the max \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8972436173187118
      ],
      "excerpt": "    model training to predict sequence B when we inputs sequence A. However, when we useauto-encodingoption, dataset format is just lines of text. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9697730900260492
      ],
      "excerpt": "-disable-mixed-precisionis to disable fp16 mixed precision. Mixed precision is on as default. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "                        pretrained model checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9257353068050027
      ],
      "excerpt": "-beam-size` is beam search parameter. When this is less than two or not given, use greedy search. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "                        pretrained model checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8485142211768171
      ],
      "excerpt": "- When usesave-pair` option, save with original sentence and generated sentence with tsv format. If not, save only generated sentences. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "                        pretrained model checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192494304231617
      ],
      "excerpt": "  --pad-id PAD_ID       Pad token id when tokenize with sentencepiece \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192494304231617
      ],
      "excerpt": "  --pad-id PAD_ID       Pad token id when tokenize with sentencepiece \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This is seq2seq model structures with Tensorflow 2",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cosmoquester/seq2seq/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sat, 25 Dec 2021 10:07:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cosmoquester/seq2seq/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "cosmoquester/seq2seq",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8254640689056041
      ],
      "excerpt": "  --use-tfrecord        train using tfrecord dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8847517208231404
      ],
      "excerpt": "  --device {CPU,GPU,TPU} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057675898656085
      ],
      "excerpt": "                        Savedmodel path \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.806561330359993,
        0.8289959638904407,
        0.8645544707693019
      ],
      "excerpt": "File Paths: \n  --model-name MODEL_NAME \n                        Seq2seq model name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8549644838882401
      ],
      "excerpt": "                        model config file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8795864982803193,
        0.8230431577528401,
        0.8583411594076191,
        0.8375833831969657,
        0.8443501873685468
      ],
      "excerpt": "                        a text file or multiple files ex) *.txt \n  --pretrained-model-path PRETRAINED_MODEL_PATH \n                        pretrained model checkpoint \n  --output-path OUTPUT_PATH \n                        output directory to save log and model checkpoints \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806231503790735
      ],
      "excerpt": "  --batch-size BATCH_SIZE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8295865281211637,
        0.8295865281211637
      ],
      "excerpt": "  --shuffle-buffer-size SHUFFLE_BUFFER_SIZE \n  --prefetch-buffer-size PREFETCH_BUFFER_SIZE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8570031560710254
      ],
      "excerpt": "  --auto-encoding       train by auto encoding with text lines dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8576591754281959
      ],
      "excerpt": "                        device to train model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8214424486961576,
        0.8886380009747077
      ],
      "excerpt": "-model-nameis seq2seq model class name, so one of (RNNSeq2Seq, RNNSeq2SeqWithAttention, TransformerSeq2Seq) \n-model-config-pathis model config file path. model config file describe model parameter. There are default model configs inresources/configs-dataset-pathis dataset file glob expression. dataset file format is tsv file without header having two columns sequence A, sequenceB \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8427596960772852
      ],
      "excerpt": "-sp-model-pathis sentencepiece model path to tokenize text. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8452063773921183
      ],
      "excerpt": "When ending of training, the model checkpoints and tensorboard log files are saved to output directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806561330359993,
        0.8289959638904407,
        0.8645544707693019
      ],
      "excerpt": "File Paths: \n  --model-name MODEL_NAME \n                        Seq2seq model name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8549644838882401
      ],
      "excerpt": "                        model config file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.823719961071482,
        0.8583411594076191
      ],
      "excerpt": "  --model-path MODEL_PATH \n                        pretrained model checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806231503790735,
        0.8295865281211637
      ],
      "excerpt": "  --batch-size BATCH_SIZE \n  --prefetch-buffer-size PREFETCH_BUFFER_SIZE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8106046147638122
      ],
      "excerpt": "  --header              use this flag if dataset (tsv file) has header \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8170417332836992
      ],
      "excerpt": "                        this value as beam size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8576591754281959
      ],
      "excerpt": "  --device DEVICE       device to train model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806561330359993,
        0.8289959638904407,
        0.8645544707693019
      ],
      "excerpt": "File Paths: \n  --model-name MODEL_NAME \n                        Seq2seq model name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8549644838882401
      ],
      "excerpt": "                        model config file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8795864982803193,
        0.823719961071482,
        0.8583411594076191,
        0.8375833831969657,
        0.8334015255615093
      ],
      "excerpt": "                        a text file or multiple files ex) *.txt \n  --model-path MODEL_PATH \n                        pretrained model checkpoint \n  --output-path OUTPUT_PATH \n                        output file path to save generated sentences \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806231503790735,
        0.8295865281211637
      ],
      "excerpt": "  --batch-size BATCH_SIZE \n  --prefetch-buffer-size PREFETCH_BUFFER_SIZE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8170417332836992
      ],
      "excerpt": "                        this value as beam size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8576591754281959
      ],
      "excerpt": "  --device DEVICE       device to train model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806561330359993,
        0.8289959638904407,
        0.8645544707693019
      ],
      "excerpt": "File Paths: \n  --model-name MODEL_NAME \n                        Seq2seq model name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8549644838882401,
        0.823719961071482,
        0.8583411594076191
      ],
      "excerpt": "                        model config file \n  --model-path MODEL_PATH \n                        pretrained model checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806231503790735,
        0.8295865281211637
      ],
      "excerpt": "  --batch-size BATCH_SIZE \n  --prefetch-buffer-size PREFETCH_BUFFER_SIZE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8170417332836992
      ],
      "excerpt": "                        this value as beam size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8576591754281959
      ],
      "excerpt": "  --device DEVICE       device to train model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289959638904407,
        0.8645544707693019
      ],
      "excerpt": "  --model-name MODEL_NAME \n                        Seq2seq model name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8549644838882401
      ],
      "excerpt": "                        model config file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8389059080630952
      ],
      "excerpt": "                        Model weight file path saved in training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8375833831969657
      ],
      "excerpt": "  --output-path OUTPUT_PATH \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cosmoquester/seq2seq/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "seq2seq",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "seq2seq",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "cosmoquester",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cosmoquester/seq2seq/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 10:07:13 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "seq2seq",
      "tensorflow2",
      "nlp"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can start training by running script like below\n```sh\n$ python -m scripts.train \\\n\t--dataset-path \"data/*.txt\" \\\n\t--batch-size 2048 --dev-batch-size 2048 \\\n\t--epoch 90 --steps-per-epoch 250 --auto-encoding \\\n\t--learning-rate 2e-4 \\\n\t--device gpu \\\n\t--tensorboard-update-freq 50 --model-name TransformerSeq2Seq --model-config-path resources/configs/transformer.yml\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You can start training by running script like below\n```sh\n$ python -m scripts.evaluate \\\n    --model-path ~/Downloads/output/models/model-50epoch-nanloss_0.3870acc.ckpt \\\n    --model-config-path ~/Downloads/output/model_config.yml  \\\n    --dataset-path test.txt \\\n    --auto-encoding \\\n    --beam-size 2 \\\n    --disable-mixed-precision\n\n[skip some messy logs...]\n[2020-12-20 01:42:28,308] RNN `implementation=2` is not supported when `recurrent_dropout` is set. Using `implementation=1`.\nDEBUG:tensorflow:RNN `implementation=2` is not supported when `recurrent_dropout` is set. Using `implementation=1`.\n[2020-12-20 01:42:28,311] RNN `implementation=2` is not supported when `recurrent_dropout` is set. Using `implementation=1`.\nDEBUG:tensorflow:RNN `implementation=2` is not supported when `recurrent_dropout` is set. Using `implementation=1`.\n[2020-12-20 01:42:28,315] RNN `implementation=2` is not supported when `recurrent_dropout` is set. Using `implementation=1`.\n[2020-12-20 01:42:30,963] Loaded weights of model\nPerplexity: 17.618855794270832, BLEU: 0.07615733809469007: : 1it [00:04,  4.38s/it]\n[2020-12-20 01:42:35,347] Finished evalaution!\n[2020-12-20 01:42:35,348] Perplexity: 17.618855794270832, BLEU: 0.07615733809469007\n```\nResults is ppl and BLEU.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You can start training by running script like below\n```sh\n$ python -m scripts.inference \\\n    --dataset-path test.txt \\\n    --model-path ~/Downloads/output/models/model-50epoch-nanloss_0.3870acc.ckpt \\\n    --output-path out.txt \\\n    --save-pair\n\n[skip some messy logs...]\n[2020-12-20 01:52:27,856] Loaded weights of model\n[2020-12-20 01:52:27,857] Start Inference\n[2020-12-20 01:52:35,629] Ended Inference, Start to save...\n[2020-12-20 01:52:35,631] Saved (original sentence,decoded sentence) pairs to out.txt\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You can test your trained model interactively. If you want to finish, just enter to put empty input.\n```sh\n$ python -m scripts.interactive \\\n    --model-name TransformerSeq2Seq \\\n    --model-path ~/model-28epoch-0.1396loss_0.9812acc.ckpt \\\n    --model-config-path resources/configs/transformer.yml\n\n[2021-04-30 00:27:00,037] Loaded weights of model\nPlease Input Text: \ub108 \uc774\ub984\uc774 \ubb50\uc57c?\n2021-04-30 00:27:23.437004: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n2021-04-30 00:27:23.705647: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\nOutput: \ub108 \uc774\ub984\uc774 \ubb50\uc57c?, Perplexity: 1.0628\nPlease Input Text: \uadfc\ub370 \uc5b4\uca4c\ub77c\ub294 \uac78\uae4c\nOutput: \uadfc\ub370 \uc5b4\uca4c\ub77c\ub294 \uac78\uae4c, Perplexity: 1.0594\nPlease Input Text:  \ud5e4\ud5e4\ud5e4\ud5e4\nOutput: \ud5e4\ud5e4\ud5e4\ud5e4, Perplexity: 1.4151\nPlease Input Text:\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You can simply convert model checkpoint to savedmodel format.\n```sh\n$ python -m scripts.convert_to_savedmodel \\\n    --model-name RNNSeq2SeqWithAttention \\\n    --model-config-path ~/Downloads/output/model_config.yml \\\n    --model-weight-path ~/Downloads/output/models/model-50epoch-nanloss_0.3870acc.ckpt \\\n    --output-path seq2seq-model/1\n\n[skip some messy logs...]\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\nINFO:tensorflow:Assets written to: seq2seq-model/1/assets\n[2020-12-20 01:58:49,424] Assets written to: seq2seq-model/1/assets\n[2020-12-20 01:58:51,285] Saved model to seq2seq-model/1\n```\nIf you make savedmodel by using this script, tokenize is included in savedmodel so you can use generate sequence without tokenizer or vocab.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```sh\n$ docker run -v `pwd`/seq2seq-model:/models/seq2seq -e MODEL_NAME=seq2seq -p 8501:8501 -dt tensorflow/serving\n```\nYou can open tensorflow serving server.\n\n```sh\n$ curl -XPOST localhost:8501/v1/models/seq2seq:predict -d '{\"inputs\":[\"\uc548\ub155\ud558\uc138\uc694\", \"\ub098\ub294 \uc624\ub298 \ubc25\uc744 \uba39\uc5c8\ub2e4\", \"\uc544\ub2c8 \uc9c0\uae08 \ubb50\ub77c\uace0\uc694?, \uadf8\uac8c \ub300\uccb4 \ubb34\uc2a8 \ub9d0\uc774\uc5d0\uc694!!\"]}'\n{\n    \"outputs\": {\n        \"perplexity\": [\n            1.00468457,\n            1.06678605,\n            1.04327798\n        ],\n        \"sentences\": [\n            \"\uc548\ub155\ud558\uc138\uc694\",\n            \"\ub098\ub294 \uc624\ub298 \ubc25\uc744 \uba39\uc5c8\ub2e4\",\n            \"\uc544\ub2c8 \uc9c0\uae08 \ubb50\ub77c\uace0\uc694?, \uadf8\uac8c \ub300\uccb4 \ubb34\uc2a8 \ub9d0\uc774\uc5d0\uc694!!\"\n        ]\n    }\n}\n```\n- By default, signature function is greedy search. Like above example, you can send texts then receice ppl and gernerated texts.\n\n```sh\n$ curl -XPOST localhost:8501/v1/models/seq2seq:predict -d '{\"inputs\":{\"texts\":[\"\ubc18\uac11\uc2b5\ub2c8\ub2e4\", \"\ud559\uad50\uac00\uae30 \uc2eb\ub2e4\"], \"beam_size\":3}, \"signature_name\":\"beam_search\"}'\n{\n    \"outputs\": {\n        \"sentences\": [\n            [\n                \"\ubc18\uac11\uc2b5\ub2c8\ub2e4\",\n                \"\ubc18\uac11\uc2b5\ub2c8\ub2e4\",\n                \"\ubc18\uac11\uc2b5\ub2c8\ub2e4\"\n            ],\n            [\n                \"\ud559\uad50\uac00\uae30 \uc2eb\ub2e4\",\n                \"\ud559\uad50\uac00\uae30\ub194\",\n                \"\ud559\uad50\uac00\uae30 \ucc59\uaca8\"\n            ]\n        ],\n        \"perplexity\": [\n            [\n                1.0299294,\n                1.0299294,\n                1.0299294\n            ],\n            [\n                1.22807097,\n                1.54527545,\n                1.56684875\n            ]\n        ]\n    }\n}\n```\n- If you want to inference by beam searching, set `signature_name` as beam_search and request with beam_size.\n- Response also contains beam size number of texts per an example.\n\n",
      "technique": "Header extraction"
    }
  ]
}