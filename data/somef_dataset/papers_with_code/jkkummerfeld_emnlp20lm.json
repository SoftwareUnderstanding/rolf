{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1708.02182",
      "https://arxiv.org/abs/1708.02182"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{merityRegOpt,\n  title={{Regularizing and Optimizing LSTM Language Models}},\n  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},\n  journal={arXiv preprint arXiv:1708.02182},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9995457175598671
      ],
      "excerpt": "If you use this code or our results in your research, please cite: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jkkummerfeld/emnlp20lm",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-21T18:40:48Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-05T18:58:53Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9697749044729251
      ],
      "excerpt": "This repository contains code for language modeling experiments, as described in: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "  Charles Welch, Rada Mihalcea \\and Jonathan K. Kummerfeld \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9867660348944765,
        0.8732200038302981,
        0.8935099005830129
      ],
      "excerpt": "It is based on the original version of the AWD-LSTM language model (later versions of the code had slightly different performance, so we used the original to match the original paper). \nMost of this readme file is taken from that code. \nThe model code has been modified to support the experiments described in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9207160742802687
      ],
      "excerpt": "--emsize EMSIZE       size of word embeddings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9502691437764413,
        0.9821220487466656
      ],
      "excerpt": "This repository contains the code used for Salesforce Research's Regularizing and Optimizing LSTM Language Models paper, originally forked from the PyTorch word level language modeling example. \nThe model comes with instructions to train a word level language model over the Penn Treebank (PTB) and WikiText-2 (WT2) datasets, though the model is likely extensible to many other datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8111396861526387,
        0.9417042596013939
      ],
      "excerpt": "Finetune the model using finetune.py \nApply the continuous cache pointer to the finetuned model using pointer.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9378103597262917
      ],
      "excerpt": "The codebase was modified during the writing of the paper, preventing exact reproduction due to minor differences in random seeds or similar. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9687972562703343,
        0.9870690753066664
      ],
      "excerpt": "This is proper experimental practice and is especially important when tuning hyperparameters, such as those used by the pointer. \nThe instruction below trains a PTB model that without finetuning achieves perplexities of 61.2 / 58.9 (validation / testing), with finetuning achieves perplexities of 58.8 / 56.6, and with the continuous cache pointer augmentation achieves perplexities of 53.5 / 53.0. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808372561373473
      ],
      "excerpt": "To then fine-tune that model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9743682343541357,
        0.9938284419804937
      ],
      "excerpt": "Note that the model in the paper was trained for 500 epochs and the batch size was 40, in comparison to 300 and 20 for the model above. \nThe window size for this pointer is chosen to be 500 instead of 2000 as in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.972770809996921
      ],
      "excerpt": "The instruction below train a WT2 model that without finetuning achieves perplexities of 69.1 / 66.1 (validation / testing), with finetuning achieves perplexities of 68.7 / 65.8, and with the continuous cache pointer augmentation achieves perplexities of 53.6 / 52.0 (51.95 specifically). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9700412311787676
      ],
      "excerpt": "All the augmentations to the LSTM, including our variant of DropConnect (Wan et al. 2013) termed weight dropping which adds recurrent dropout, allow for the use of NVIDIA's cuDNN LSTM implementation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8729426075784217,
        0.8807177354719142
      ],
      "excerpt": "This ensures the model is fast to train even when convergence may take many hundreds of epochs. \nThe default speeds for the model during training on an NVIDIA Quadro GP100: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for the paper \"Improving Low Compute Language Modeling with In-Domain Embedding Initialisation\" at EMNLP 2020",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jkkummerfeld/emnlp20lm/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 22 Dec 2021 22:39:05 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jkkummerfeld/emnlp20lm/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jkkummerfeld/emnlp20lm",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jkkummerfeld/emnlp20lm/master/do-check.sh",
      "https://raw.githubusercontent.com/jkkummerfeld/emnlp20lm/master/example-run.sh",
      "https://raw.githubusercontent.com/jkkummerfeld/emnlp20lm/master/getdata.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository contains the code we used to pre-process the data. There are files for extraction:\n\n- `data-preprocessing/gigaword-extract.py`\n- `data-preprocessing/cord-extract.py`\n- `data-preprocessing/wiki-extract.py`\n- `data-preprocessing/nanc-extract.py`\n- `data-preprocessing/irc-extract.py`\n\nAnd files for tokenising with Stanza and converting numbers:\n\n- `data-preprocessing/tokenise.py`\n- `data-preprocessing/convert_nums.py`\n\nWe also include a script that reads the LDC PTB tgz file and produces our version of the PTB:\n\n- `data-preprocessing/make-non-unk-ptb.py`\n\nFor example, to prepare the data in the same way we did, run these two commands (where `treebank_3_LDC99T42.tgz` must be downloaded from the LDC).\n\n```\n./data-preprocessing/make-non-unk-ptb.py --prefix ptb.std. treebank_3_LDC99T42.tgz\n./data-preprocessing/make-non-unk-ptb.py --prefix ptb.rare. --no-unks treebank_3_LDC99T42.tgz\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9986563398845941
      ],
      "excerpt": "Install PyTorch 0.1.12_2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8671526817437554
      ],
      "excerpt": "For data setup, run ./getdata.sh. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559614939205502
      ],
      "excerpt": "PyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.819289735255173
      ],
      "excerpt": "Specifying separate input and output embeddings properties (e.g. size). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030504587953663,
        0.8717294828563956
      ],
      "excerpt": "Train the base model using main.py \nFinetune the model using finetune.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9136064441154114,
        0.9237887017789442,
        0.8269734602855762
      ],
      "excerpt": "First, train the model: \npython main.py --batch_size 20 --data data/penn --dropouti 0.4 --seed 28 --epoch 300 --save PTB.pt \nThe first epoch should result in a validation perplexity of 308.03. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9012320187125533,
        0.8235179047624954
      ],
      "excerpt": "python finetune.py --batch_size 20 --data data/penn --dropouti 0.4 --seed 28 --epoch 300 --save PTB.pt \nThe validation perplexity after the first epoch should be 60.85. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9022285019093377
      ],
      "excerpt": "python pointer.py --data data/penn --save PTB.pt --lambdasm 0.1 --theta 1.0 --window 500 --bptt 5000  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9232916405646634,
        0.871984590917679,
        0.896422779876524
      ],
      "excerpt": "python main.py --seed 20923 --epochs 750 --data data/wikitext-2 --save WT2.pt \nThe first epoch should result in a validation perplexity of 629.93. \npython -u finetune.py --seed 1111 --epochs 750 --data data/wikitext-2 --save WT2.pt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9022285019093377
      ],
      "excerpt": "python pointer.py --save WT2.pt --lambdasm 0.1279 --theta 0.662 --window 3785 --bptt 2000 --data data/wikitext-2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8965962714895965,
        0.8883517052676217
      ],
      "excerpt": "Penn Treebank: approximately 45 seconds per epoch for batch size 40, approximately 65 seconds per epoch with batch size 20 \nWikiText-2: approximately 105 seconds per epoch for batch size 80 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jkkummerfeld/emnlp20lm/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2017, \\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Improving Low Compute Language Modeling with In-Domain Embedding Initialisation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "emnlp20lm",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jkkummerfeld",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This codebase requires Python 3 and PyTorch 0.1.12_2.\nIf you are using Anaconda, this can be achieved via:\n`conda install pytorch=0.1.12 -c soumith`.\n\nNote the older version of PyTorch - upgrading to later versions would require minor updates and would prevent the exact reproductions of the results below.\nPull requests which update to later PyTorch versions are welcome, especially if they have baseline numbers to report too :)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 22 Dec 2021 22:39:05 GMT"
    },
    "technique": "GitHub API"
  }
}