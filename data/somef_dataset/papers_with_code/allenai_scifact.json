{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.14974",
      "https://arxiv.org/abs/2004.14974",
      "https://arxiv.org/abs/1910.01108"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@inproceedings{Wadden2020FactOF,\n  title={Fact or Fiction: Verifying Scientific Claims},\n  author={David Wadden and Shanchuan Lin and Kyle Lo and Lucy Lu Wang and Madeleine van Zuylen and Arman Cohan and Hannaneh Hajishirzi},\n  booktitle={EMNLP},\n  year={2020},\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{Wadden2020FactOF,\n  title={Fact or Fiction: Verifying Scientific Claims},\n  author={David Wadden and Shanchuan Lin and Kyle Lo and Lucy Lu Wang and Madeleine van Zuylen and Arman Cohan and Hannaneh Hajishirzi},\n  booktitle={EMNLP},\n  year={2020},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9910677733074283
      ],
      "excerpt": "This repository contains data and code for the paper Fact or Fiction: Verifying Scientific Claims by David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/allenai/scifact",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Email: `davidw@allenai.org`.\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-27T20:18:36Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-12T13:07:07Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9755970614680706
      ],
      "excerpt": "This repository contains data and code for the paper Fact or Fiction: Verifying Scientific Claims by David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9127477814344492,
        0.8955124028891104
      ],
      "excerpt": "You can also check out our COVID-19 claim verification demo. For a heavier-weight COVID claim verifier, see the section on verifying COVID-19 claims. \nUpdate (Dec 2020): SciFact will be used for the SciVer shared task to be featured at the SDP workshop at NAACL 2021.  Registration is open! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9195911464108429
      ],
      "excerpt": "UPDATE (Jan 2021): We now have an official AI2 leaderboard with automated evaluation! For information on the submission file format and evaluation metrics, see evaluation.md. Or, check out the getting started page on the leaderboard. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9380491988974099
      ],
      "excerpt": "Due to the relatively small size of the dataset, we also provide a 5-fold cross-validation split that may be useful for model development. After unzipping the tarball, the data will organized like this: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9587445086624279,
        0.8896330303687506
      ],
      "excerpt": "See data.md for descriptions of the schemas for each file type. \nWe also make available the collection of claims together with the documents and citation contexts they are based on. We hope that these data will facilitate the training of \"claim generation\" models that can summarize a citation context into atomic claims. Click here to download the file, or enter \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9942646769631192
      ],
      "excerpt": "For more information on the data, see claims-with-citances.md \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9966828654077782
      ],
      "excerpt": "While the project website features a COVID-19 fact-checking demo, it is not configurable and uses a \"light-weight\" version of VeriSci based on DistilBERT. We provide a more configurable fact-checking script that uses the full model. Like the web demo, it uses covidex for document retrieval.  Usage is as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Data and models for the SciFact verification task.",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "All \"BERT-to-BERT\"-style models as described in the paper are stored in a public AWS S3 bucket. You can download the models models using the script:\n```bash\n./script/download-model.sh [model-component] [bert-variant] [training-dataset]\n```\n- `[model-component]` options: `rationale`, `label`\n- `[bert-variant]` options: `roberta_large`, `roberta_base`, `scibert`, `biomed_roberta_base`\n- `[training-dataset]` options: `scifact`, `scifact_only_claim`, `scifact_only_rationale`, `fever_scifact`, `fever`, `snopes`\n\nThe script checks to make sure the downloaded model doesn't already exist before starting new downloads.\n\nThe best-performing pipeline reported in [paper](https://arxiv.org/abs/2004.14974) uses:\n- `rationale`: `roberta_large` + `scifact`\n- `label`: `roberta_large` + `fever_scifact`\n\nFor `fever` and `fever_scifact`, there are models available for all 4 BERT variants. For `snopes`, only `roberta_large` is available for download (but you can train your own model).\n\nAfter downloading the pretrained-model, you can follow instruction [model.md](doc/model.md) to run individual model components.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/allenai/scifact/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 16,
      "date": "Fri, 24 Dec 2021 19:27:25 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/allenai/scifact/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "allenai/scifact",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/allenai/scifact/master/script/pipeline.sh",
      "https://raw.githubusercontent.com/allenai/scifact/master/script/download-data.sh",
      "https://raw.githubusercontent.com/allenai/scifact/master/script/rationale-selection.sh",
      "https://raw.githubusercontent.com/allenai/scifact/master/script/download-model.sh",
      "https://raw.githubusercontent.com/allenai/scifact/master/script/label-prediction.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8064385178958192
      ],
      "excerpt": "\u2b07\ufe0fDownload the dataset here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8829158690321498
      ],
      "excerpt": "For a description of the optional arguments, run python script/verify_covid.py -h. The script generates either a pdf or markdown report. The pdf version requires pandoc and wkhtmltopdf, both of which can be installed with conda. A usage example might be: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8656447306675842
      ],
      "excerpt": "\u2b07\ufe0fDownload the dataset here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8351477930697216
      ],
      "excerpt": "Download pre-trained models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8036198486371166
      ],
      "excerpt": "Download with script: The data will be downloaded and stored in the data directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9326946791402703
      ],
      "excerpt": "python script/verify_covid.py [claim-text] [report-file] [optional-arguments]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8949152021148753
      ],
      "excerpt": "python script/verify_covid.py \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/allenai/scifact/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'# License\\n\\n## Dataset\\n\\nThe SciFact dataset is released under the CC BY-NC 2.0. By using the SciFact data, you are agreeing to its usage terms.\\n\\n## Code\\n\\nThe code in this repository is licensed under the Apache 2.0 license.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SciFact",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "scifact",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "allenai",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/allenai/scifact/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We recommend you create an anaconda environment:\n```bash\nconda create --name scifact python=3.7 conda-build\n```\nThen, from the `scifact` project root, run\n```\nconda develop .\n```\nwhich will add the scifact code to your `PYTHONPATH`.\n\nThen, install Python requirements:\n```\npip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide scripts let you easily run our models and re-create the dev set metrics published in paper. The script will automatically download the dataset and pre-trained models. You should be able to reproduce our dev set results from the paper by following these instructions (we are not releasing test set labels at this point). Please post an issue if you're unable to do this.\n\nTo recreate Table 3 rationale selection metrics:\n```bash\n./script/rationale-selection.sh [bert-variant] [training-dataset] [dataset]\n```\nTo recreate Table 3 label prediction metrics:\n```bash\n./script/label-prediction.sh [bert-variant] [training-dataset] [dataset]\n```\n- `[bert-variant]` options: `roberta_large`, `roberta_base`, `scibert`, `biomed_roberta_base`\n- `[training-dataset]` options: `scifact`, `scifact_only_claim`, `scifact_only_rationale`, `fever_scifact`, `fever`, `snopes`\n- `[dataset]` options: `dev`.\n\n\nTo make full-pipeline predictions, you can use:\n```bash\n./script/pipeline.sh [retrieval] [model] [dataset]\n```\n- `[retrieval]` options: `oracle`, `open`\n- `[model]` options: `oracle-rationale`, `zero-shot`, `verisci`\n- `[dataset]` options: `dev`, `test`.\n\nTwo notes on this:\n- For the dev set, this script will also compute performance metrics. For the test set the \"gold\" labels are not public, so the script will just make predictions without evaluating.\n- `oracle` retrieval will break on the `test` set, since it requires access to the gold evidence documents. But `open` retrieval will work on both `dev` and `test`.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 116,
      "date": "Fri, 24 Dec 2021 19:27:25 GMT"
    },
    "technique": "GitHub API"
  }
}