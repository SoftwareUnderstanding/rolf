{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805\n\n\n## Overview\n\n### Pytorh Template\n\n![](https://github.com/moemen95/Pytorch-Project-Template/raw/master/utils/assets/class_diagram.png"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9997706017152777
      ],
      "excerpt": "Paper URL : https://arxiv.org/abs/1810.04805 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/YongWookHa/BERT-on-Pytorch-Template",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-05T00:36:36Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-17T23:53:53Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9649072394739601,
        0.8451720178895493
      ],
      "excerpt": "Pytorch implementation of Google AI's 2018 BERT on moemen95's Pytorch-Project-Template. \nBERT 2018 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9892067854468352,
        0.9981119653165355
      ],
      "excerpt": "moemen95's Pytorch-Project-Template has a specific structure represented above. It's proposing a baseline for any Pytorch project so that we can only focus on the model implementation. It provides some examples as well. So click the link and see what it is. \nThis repository is a reconstruction result of dhlee347's Pytorchic BERT and codertimo's BERT-pytorch on Pytorch template. The purpose of this is to learn how pytorch and bert work. So in this repository, pretraining and validating are only available.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047,
        0.8490037945672047,
        0.9467301360573722,
        0.9024770786958278
      ],
      "excerpt": "* The Annotated Transformer \n* The illustrated Transformer \n* Intuitive Understanding of Attention Mechanism in Deep Learning \n* Attention and memory in deep learning and nlp \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "* The illustrated transformer (translated) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9557406302337748
      ],
      "excerpt": "In the paper, authors use masked language model and predict next sentence tasks for pretraining. Here's short explanation of those two (copied from codertimo's BERT-Pytorch). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9266906915093617,
        0.8293732102061997
      ],
      "excerpt": "Randomly 15% of input token will be changed into something, based on under sub-rules \nRandomly 80% of tokens, gonna be a [MASK] token \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.924807497248963,
        0.823206699190029
      ],
      "excerpt": "Input : [CLS] the man went to the store [SEP] he bought a gallon of milk [SEP] \nLabel : Is Next \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9045441841131522
      ],
      "excerpt": "understanding the relationship, between two text sentences, which is not directly captured by language modeling \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.977876869437683
      ],
      "excerpt": "In pretraining with Korean corpus(sejong corpus), 300k iteration with 32 batch size, I was able to get 78% of accuracy in Next Sentence Prediction task. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9876928681888606,
        0.9616925417632722
      ],
      "excerpt": "With Korean corpus, the result of using 32 batch size is better than using 96. It seems that more frequent parameter updating leads to the optima. Pictures below are loss graphs of Language Model Loss and Next Sentence Prediction Classification Loss.  \nThe result shows that the model had been learning about NSP task after language model because of the difference of magnitude of loss values. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Google BERT implementation on pytorch-template",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/YongWookHa/BERT-on-Pytorch-Template/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 14:28:42 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/YongWookHa/BERT-on-Pytorch-Template/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "YongWookHa/BERT-on-Pytorch-Template",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/YongWookHa/BERT-on-Pytorch-Template/master/run.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Basically, your corpus should be prepared with two sentences in one line with tab(\\t) separator\n```\nWelcome to the \\t the jungle\\n\nI can stay \\t here all night\\n\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/YongWookHa/BERT-on-Pytorch-Template/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERT-on-Pytorch-Template",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERT-on-Pytorch-Template",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "YongWookHa",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/YongWookHa/BERT-on-Pytorch-Template/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Run `run.sh`.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 14:28:42 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "bert",
      "bert-model",
      "korean-bert",
      "korean-nlp",
      "transformer",
      "pytorch",
      "pytorch-implementation",
      "pytorch-template"
    ],
    "technique": "GitHub API"
  }
}